{"year": "2021", "forum": "RDpTZpubOh7", "title": "Safety Aware Reinforcement Learning (SARL)", "decision": "Reject", "meta_review": "Based on the paper, reviewers' comments and discussions, and the responses, the meta-reviewer would like to suggest the authors to improve the paper and resubmit.", "reviews": [{"review_id": "RDpTZpubOh7-0", "review_text": "* * Update after authors ' response * * I want to thank the authors for their responses . My responses to the authors ' comments are in the respective threads . * * Summary * * The paper addresses the timely and important problem of how to train RL agents such that they solve desired tasks while not engaging in undesired behavior that is not explicitly specified via the reward function . In particular , the paper focuses on training agents that learn to avoid unnecessary side effects , that is ( irreversible ) alterations to the environment which are not necessary to solve the task at hand . Experiments are performed on SafeLife , which provides a suite of tasks in an environment ( potentially with rich intrinsic dynamics ) , along with a quantitative measure of the strength of undesired side effects . The main idea of the paper is to co-train an RL policy on this side effect measure with the aim of minimizing side effects . This policy is used for regularizing the reward-optimizing agent during training , such that the trained agent learns to bias its actions towards avoiding side effects when the task allows for multiple viable actions . The paper compares against a strong , previously reported baseline , both in static and dynamic SafeLife environments/tasks . Additionally , the generalization of the side-effect-avoiding policy is tested , by using it for training a reward-optimizing agent on task-versions that the side-effect-avoiding policy has not been trained on . * * Contributions , Novelty , Impact * * 1 ) Incorporation of the two ( sometimes conflicting ) objectives of maximizing reward and avoiding side-effects into a single training objective , where purely reward maximizing actions are regularized by action-distributions from a side-effect-minimizing policy . This is an interesting idea that turns trading off avoiding side effects against reward maximization into a learning problem . I think this is a promising way forward . What I \u2019 d like to see in the paper for even greater impact is a clear discussion of the requirements ( the objective of avoiding side-effects must be specified as a trajectory-dependent , quantitative function , similar to a reward function ) , and the current limitations ( unclear how to assess \u201c how much \u201d of the task-relevant state-space is well covered by the side-effect-avoiding policy , particularly in the zero-shot setting ) . 2 ) Experimental evaluation of the proposed method on * the * state-of-the-art benchmark suite , and comparison against a strong , previously proposed baseline . The results are promising , though it \u2019 s hard to distill a very clear message in favor of the method from the results shown . I personally think that \u2019 s fine ( and to some degree expected when discussing solutions that solve a particular trade-off in a different fashion ) , but I \u2019 d like to see even more of a multi-faceted evaluation and discussion in the paper . 3 ) The idea of learning a side-effect-avoiding regularizer that generalizes well , e.g.to different tasks under the same environment dynamics . This is very interesting and a promising step towards tackling the side effects problem at scale . It is very nice to see the zero-shot results . To make the paper even stronger and more impactful it would be nice to evaluate the generalization of the trained side-effect-avoiding police in more detail . * * Score and reasons for score * * I am ( currently ) in favor of accepting the paper , though I think that some additional work could improve the strength and potential impact of the work . The topic addressed is timely and very important , and the approach taken is interesting and sensible . Results look promising , and the paper does a great job at presenting the work . To further strengthen the paper it would be nice to discuss results in more detail and potentially perform additional experiments to highlight certain aspects that are \u201c buried \u201d in the current results . Additionally it would be good to say something more substantial about the generalization properties of the side-effect-avoiding policy . While the latter two issues are probably beyond what \u2019 s easily possible in the rebuttal phase , I want to strongly encourage the authors to add a short paragraph that clearly states the assumptions/requirements ( the strongest assumption is perhaps the presence of a quantitative side effect measure which can be used directly as a reinforcement signal ) , and current limitations . I am looking forward to the other reviews and authors \u2019 response , and will update my final verdict accordingly . * * Strengths * * 1 ) Empirically promising results on a timely and important problem , including the comparison against a strong baseline method . 2 ) Evaluation of proposed method by : ( i ) multiple runs to assess statistical significance , ( ii ) ablation studies regarding the \u201c distance \u201d metric used by the method , ( iii ) control-experiments regarding the ( zero-shot ) generalization performance of the side-effect-avoiding policy . 3 ) Well written paper , with good introduction to the problem and discussion of related literature ( given the limited space of a conference-format publication ) . * * Weaknesses * * 1 ) The experimental results shown are interesting and promising , but it \u2019 s hard to distill a clear message from the results other than : \u201c the proposed method seems to work on par with a previously proposed method but often makes the trade-offs ( between high reward and low side-effects ) differently , which makes comparison more difficult \u201d . Drilling down on some of the findings and trying to control for more factors to get a clearer picture would strengthen the results . 2 ) The generalization of the side-effect-avoiding policy is a very interesting aspect of the work , however the current analysis of how well that generalization behaves is a bit crude . It is unclear to which degree the previously trained side-effect-avoiding policy in the zero-shot regime covers the state-space encountered when solving a particular task . It is also unclear whether the side-effect-avoiding policy in the generalization setting \u201c behaves mostly well overall \u201d or whether it has some severe and potentially even systematic shortcomings ( leading to undesired policies ) in particular situations of the generalization regime . Addressing this in full generality is of course beyond the scope of this paper , but some more analysis into this issue would be very nice to see ( e.g.comparing the zero-shot vs the trained side-effect-policies in isolation , and potentially drilling in on some of the differences encountered ) . * * Correctness * * The construction of the algorithm and training scheme presented in the paper seems correct to me . * * Clarity * * The paper is mostly well written , and the method is clearly described . Perhaps two things to improve : ( i ) the discussion of results could be expanded a bit more , there \u2019 s a lot going on in the plot and unfortunately there \u2019 s no intuitive message that one can easily take away visually . ( ii ) To facilitate the flow of the manuscript to readers unfamiliar with SafeLife it would be nice to include a short section describing the side-effect penalty . * * Improvements / major issues * * 1 ) The results currently shown are interesting but it \u2019 s hard to distill a clear message ( which is understandable to some degree , as the paper also points out , because different solutions to a multi-objective optimization can not be easily compared ) . It might be worthwhile though to expand the discussion ( and perhaps even presentation ) of the results a bit more . 2 ) One of the most interesting aspects of the work is the potential to train a task-agnostic side-effects-avoiding policy that generalizes to a broad range of tasks . The paper demonstrates that this works by applying said policy in a zero-shot setting and analyzing the resulting policy . It would be nice to also do some more comparison of the side-effects-avoiding policies directly ( e.g.what is the side effect score when directly comparing a zero-shot Z vs a Z trained on the current task/environment - are there any systematic deviations between the two , do certain biases get baked into the zero-shot Z that can be explained by the tasks/environment-variants it \u2019 s been trained on ) . 3 ) A clear discussion of the requirements ( the objective of avoiding side-effects must be specified as a trajectory-dependent , quantitative function , similar to a reward function ) , and the current limitations ( unclear how to assess \u201c how much \u201d of the task-relevant state-space is well covered by the side-effect-avoiding policy , particularly in the zero-shot setting ) . 4 ) Please clarify : why are there separate zero-shot agents shown in prune-still and append-still - shouldn \u2019 t they be the same SARL JS/WD since the zero-shot agents have been trained on these two tasks respectively ? 5 ) Please clarify and potentially discuss in the paper : perhaps the main requirement for the method is having a side-effect-strength signal s. This signal must be suitable for a reinforcement learning algorithm to train a side-effects-minimizing policy Z . But if such a signal is available , why not simply combine it with the task-specific reward function r to create a \u201c safe reward function \u201d to train a reward-optimizing agent that avoids side-effects ? Would the solution obtained this way be qualitatively different ( in some aspects ) compared to the solution obtained by the proposed scheme ? It \u2019 s fine to simply comment on this - the strongest version would include actual control experiments ( but I understand that this might not be easily doable ) . 6 ) Please comment and potentially discuss in the paper : What is the advantage of co-training Z with A ( lines 11-15 of Algorithm 1 ) ? Why not train Z first ( e.g.would that improve training stability ) ? * * Minor comments * * A ) Please give a few details for the side effect metric that \u2019 s used by the experiment ( fine to refer to the SafeLife paper for full details , but the rough idea should be in the paper to improve readability ) . B ) How exactly is it ensured that Z sees the same parts of the state-space that A does ( i.e.how is it ensured that Z \u201c explores \u201d similarly to A , which is solving some tasks ) ? I assume that the actions actually taken ( which lead to a certain state on which A and Z are evaluated in line 5 and 6 in Algo 1 ) are driven by A ? C ) P4 : \u201c In this formulation , policy characteristics are converted to distributions in a latent space of behavioral embeddings on which the Wasserstein Distance is then computed. \u201d . I have a hard time following this sentence , please consider unpacking it a bit .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your very thorough review , we hope that we can clarify some of your concerns through these discussions . In this comment , we would like to focus on the Improvements you outlined and address them directly : 1 . We agree that a clearer discussion on the key takeaways would be useful . Our primary goal was to train a portable safe-agent that could generalize across multiple tasks that share common dynamics . Our results on zero shot transfer show that the transferred safe agents performed similarly compared to training SARL agents from scratch on the same environment . The primary distinctions from prior work are : a. the use of a safe-agent to abstract the concept of safety . b. the portability of the safe agent across multiple tasks . c. using probabilistic distances to capture differences between policies with different objectives on the same environment We did not cast this problem as a multi-objective optimization problem in this paper - however , that is a natural next step for this line of work in the future . As mentioned in our general comment , proper reframing of the framework as a multi-objective setup requires significant future work , which we believe this is out of scope for this paper . 2.Based on our understanding of your comment , we already performed those experiments . We call them \u201c from scratch training \u201d and show them in Section 5 in the lighter shade of color . Here , the task agent and safety are trained concurrently on the same environment , and we observe similar behavior to using safe policies that are zero-shot generalized from other environments . Does this address the scenario you are suggesting ? 3.We have updated the details pertaining to the side effect metric in Section 2 of our new draft . We take the side effect metric from SafeLife as is and use it to train our safety agent . We would like to highlight that we designed SARL to be agnostic to the safety metric used , as the distance function formulation allows for that flexibility . 4.In the prune-still environment we take the safety agent trained on the append-dynamic environment , and in the append-still case we take the safety agent trained on the prune-dynamic environment . As we discuss in Section 5 , we wanted to take the safety agent trained on the environment that is most dissimilar to the task environment since we believe that would be the most difficult to generalize from . It is not strictly necessary to do that , as we could have one zero-shot agent that is generalized to all environments except for the one it is trained on . 5.We already train a baseline policy on a shaped reward that combines the primary objective and side-effect penalty . Since this was the method applied in the original SafeLife paper , we use this method as the \u201c baseline \u201d in our experiments . This method takes the frame-by-frame side effect from SafeLife and subtracts that from the frame-by-frame reward ( with a scaling factor ) . We searched for a good scaling factor as a hyperparameter in our experiments to support the side effect baseline . 6.Thank you for this suggestion . Co-training the task and safety agents on the same environment was primarily a practical choice , but we will add experiments that work with a safety agent that is previously trained . Intuitively , we do not expect major changes and agree with you that it should improve training stability . Regarding the Minor Comments : 1 . We have added more details on the side effect metric in our new draft . 2.The current environment includes a fully visible state space ; we can therefore pass the entire state to A and Z when we train A . In the current setup ( using an on policy method like PPO ) , it is difficult to ensure exploration of the same states . If we were to use an off-policy algorithm , such as DQN , we can use the replay buffer to ensure that A and Z are drawing from the same distribution of states during their training . 3.This sentence refers to how the Wasserstein distance is computed using the method described in Pacciano et al.They construct a space of test functions in a latent behavioral space and compute in that space . Essentially what happens is : 1 . A trajectory ( defined by the user ) is given 2 . A function transforms that trajectory to latent embedding space 3 . The WD distance is computed in that space iteratively via test functions . The original paper provides a lot more detail on this process , which essentially allows one to take any definition of a trajectory and compute a distance on it . Please let us know if we addressed your points and if you want to continue to discuss more ."}, {"review_id": "RDpTZpubOh7-1", "review_text": "This paper proposes a safety-aware reinforcement learning algorithm that learns to perform tasks with minimal side-effects . The key idea is that a safety policy is learned independent of the task reward . When learning the task , this safety policy is incorporated by minimizing the distance between the task agent and the safety agent . In this way , the paper claims that the safety agent can be generalized to different tasks . The method is tested on SafeLife Suite , and its performance can match task-specific safe learning baselines . Safe reinforcement learning is an extremely important research area , when we need to apply reinforcement learning to real-world applications , such as robotics , recommendation system , power grid , etc . This paper works in this direction and addresses the key challenges , including how to learn generalizable safety agents . While I think that the paper is promising , I have the following three major concerns : 1 ) The `` side effect metric '' is not clear to me . The description in Section 2.1 is high-level and vague . More rigorous mathematical definition is preferred here . For a safe learning paper , it is extremely important to clearly define what safety means . Is the `` side effect metric '' the same as the `` safety metric '' in Line 12 of Algorithm 1 ? Reading from the text , it seems that the side effect metric is calculated per episode , while the safety metric is per step . 2 ) Section 3.4 seems to leak the testing set into training . One claim of this paper is that the learned safety agent is generalizable : Z ( \\psi ) can be taken zero-shot from previous trained environments . However , during training , by tracking the Champion policy , decisions are made based on the performance on the testing environments , by retaining the policy that performs the best in the testing environments . If my understanding is right , this makes any claim about generalization less convincing because the training directly optimizes the policy in the testing environments . 3 ) Intuitively , I do not understand how Algorithm 1 could work . According to Algorithm 1 , the training of the safety agent Z ( \\psi ) is totally independent of the task , whose only objective is to be safe . If it is the case , the learned safety agent would not move or take action at all . The action distribution P_Z_\\psi would be concentrated on the zero action . This would make optimizing A ( \\theta ) using the loss ( eq . ( 1 ) ) extremely difficult . This might explain why the paper observes that the hyperparameter \\beta is difficult to tune . Here are some minor suggestions about writing : 1 ) A brief description of the 4 tasks is needed ( prune-still , prune-dynamic , append-still , append-dynamic ) to make this paper more self-contained . If the page-limit is a concern , this description could be added to the Appendix . Otherwise , it is difficult for readers to understand the difficulties and the usefulness of these tasks . 2 ) `` Line 13-17 from Algorithm 1 '' : The pseudo-code ends at Line 16 . For the above reasons , I would not recommend acceptance at this time .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and comments . We would like to address the concerns you identified directly : 1 . We have updated our description of the side effect metric in Section 2 of our new draft . There we describe the difference between the frame-by-frame metric used for training the SARL agent and the episodic metric we used to report our results in Section 5 . 2.The training and testing sets refer to different environment configurations within a given task . Let \u2019 s take prune-still as an example : Within prune-still there is a large set of different environment configurations that lead to different maps in SafeLife for which the agent will attempt to solve the prune-still task . Our process takes out a test set of configurations that are not used during training and tracks the champion policy ( of the primary task agent ) within them purely to report test performance . The safety agent never interacts with the test environments in any of the training runs , and the generalization claim pertains to transferring the safety agent across different environments , meaning that a safety agent trained on prune-still will be applied in append-dynamic , etc . We hope this clarifies the process . 3.Your intuition that the safety agent will converge towards a \u201c no action \u201d policy is what we often observed . We also believe this a major reason why finding a good \\beta is difficult to achieve . The \u201c no action \u201d policy , however , does not apply to all states that the task actor visits due to the complexity of the SafeLife environment . One potential mitigation for this problem is to modify the reward signal the safety agent is trained on , such as incentivizing it to reach only the level exit as safely as possible . We would also like to add that since task performance and safety often come at a trade-off , it is difficult to define an optimal solution . As we discuss in Section 6 and in our general comments , we plan to extend this work in the future to cast it as a multi-objective RL problem where such trade-offs can be better analyzed . 4.We have added more detailed descriptions about the different tasks . We have currently placed them in the appendix due to space concerns . 5.Thanks for pointing out this typo ."}, {"review_id": "RDpTZpubOh7-2", "review_text": "The paper aims to reduce the unwanted side effects of the actions of a reward-maximizing reinforcement-learning ( RL ) agent . The authors study a framework in which the environment issues a metric that measures the total side effects of the agent 's actions at the end of each episode . The work 's proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects . The authors then empirically investigate the effectiveness of combining the two agents via a distance measure between the policies that unifies them into one . I find the following items strong points in the submission : * The posed problem is relevant in the context of safety in AI . * The chosen testbed for the experiments matches the goals and premises of the posed problem . * The empirical results suggest that the proposed method is effective . * The discussion regarding the choice of the distance measure is thorough and makes sense . On the other hand , I find the following issues as weaknesses in the submitted manuscript : * There are no theoretical developments to demonstrate whether the reported results generalize beyond the adopted environment settings and the value assigned to the parameter beta or not . * The paper dives right into introducing the loss function in Section 3 without establishing the required notation and preliminary materials . A brief summary of the task agent and the virtual safe agent descriptions is currently provided in the caption of Figure 2 . In my opinion , Section 3 would read better if the authors append a preliminaries section wherein they establish the notations and the descriptions of the task agent and the virtual safe agent . * The loss function adopted in equation ( 3 ) provides little room for theoretical developments . The original paper that introduces the PPO algorithm ( Schulman et al. , 2017 ) offers multiple loss function choices . In my opinion , the combination of the Jenson-Shannon distance with the loss function that incorporates the KL divergences enables the authors to study their proposed algorithm beyond empirical results . I find the posed problem relevant in the context of safety in AI and the suggested method well-motivated and intuitive . I believe the submission is far from theoretically solving the posed problem ; however , the methodology alongside the promising empirical results that the manuscript offers may be of interest .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and comments . We would like to address the weaknesses you identified : 1 . It is true that we primarily focused our work on the SafeLife suite and can not make any statements beyond the environments in SafeLife . We believe , however , that SafeLife provides a rich set of different settings that allow us to demonstrate the different notions we discuss in the paper . We believe that a thorough theoretical framework for generalization outside of the SafeLife suite is beyond the scope of the paper . A theoretical framework for generalization of RL agents , as well generalization neural networks overall , are broader fields of research and open problems in the deep learning community . 2.Thank you for this feedback . We have updated our notation and further descriptions in our new draft . 3.Our current framework assumes that the task agent employs a loss function given from an established RL algorithm . Based on your feedback , we will add a deeper discussion on further development of different loss functions that involve probabilistic distances . We believe conducting this study would not be feasible within the scope of this paper , but we agree that a more thorough theoretical discussion is important ."}, {"review_id": "RDpTZpubOh7-3", "review_text": "This paper aims to address the issue of mitigating side effects in policy learning . The authors propose an algorithm SARL , which uses a safe policy to define a regularization term for penalizing the agent 's actions deviating from the safe agent in policy learning . In the experiments , four variations for SARL are shown and compared with a baseline method based on reward penalty . The proposed algorithm is competitive across the experiments presented in the paper . I think side effects and safety in reinforcement learning is an important issue . However , this paper does a bad job in describing the problem it wishes to address and , therefore , it 's unclear whether the proposed algorithm really achieves that goal . 1.The main motivation of this paper is to mitigate the side effects in learning . However , the definition of side effects were never given . It 's only until Algorithm 1 is presented where the paper mentions a safety metric that the safe agent aims to optimize ( is this the same s appearing in A ( s|theta ) and Z ( s|psi ) in Sec 3.2 ? ) , which however is not defined . Therefore , I do not fully understand what the objective of this learning algorithm wants to achieve . From the paper 's vague description , it seems like the goal is that the learner should have high performance in the original reward while not causing high side effects . This is a multi-objective MDP problem or at least can be framed as a constrained MDP . However , the proposed algorithm , based on simple regularization with a constant weight , can address neither of these two criteria . I am wondering if the authors consider to more explicitly outline the solution concept they wish to obtain . Current hand-wavy description makes me difficult to judge whether the proposed algorithm actually solves the problem they wish to solve . 2.In Algorithm 1 , since the safe agent Z is updated independently of the progress of the learner agent A , when there 's only a single environment , there is no point of distinguishing the so-called `` zero-shot '' and the online version , as in high level this dependency allows us to pretrain the safety agent alone beforehand and get the same results . Or do the authors mean zero-shot in the sense that the safe agent is trained on a different set of environments and the online version means they 're trained on the same environment ? 3.In the paper , the authors write multiple times that a difficulty in this problem setup is that the side effects are difficulty to define . But it seems that the proposed algorithm assumes some safety metric . How are the two related precisely ? And what is that used in the experiments ? 4.What is S [ \\pi_theta ] in ( 3 ) ? Overall , I think the paper is rather incomplete and therefore I do not recommend acceptance .", "rating": "3: Clear rejection", "reply_text": "Thank you for your review . We would like to address your main concerns directly : 1. a . We agree that the description of side-effects was not completely clear in the paper and have updated the description in our new draft as mentioned in our response to all reviewers . For SARL training , we assume that the environment provides a definition of the side effect . As we describe in Section , SafeLife calculates a side effect signal by taking the deviation between a baseline state and the current state . This is the frame-by-frame metric we use to train our agents . We also use the episodic side-effect to report our results where this difference is calculated between a distribution of states at the beginning and end of an episode to account for environment dynamics . b.Thank you for pointing out the inconsistent notations . We have updated the paper , including Algorithm 1 , to correct this instance as well as a few others we noticed . In the current version , s refers solely to the state of the environment . In the old notation , the \u2018 s \u2019 in Algorithm 1 , line 1-2 , \u2018 s \u2019 referred to the state of the environment and in Algorithm 1 , line 12-14 we referred to the side effect metric as \u2018 s \u2019 in green color . c. You are correct that the multi-objective MDP formulation is very applicable here . As we discuss in Section 6 , this is a future direction that we are highly interested in since we believe it would be a more effective way to understand the trade-offs between task performance and safe behavior . Our primary goal in this paper was to encapsulate the concept of safety into a safe actor - that could then be used to modulate the behavior of a primary agent on different tasks in the same dynamic environment . This mitigates the need to define or learn a penalty factor for every task that shares similar dynamics . 2.In the zero-shot experiment , the safety agent is trained on a different environment and then transferred over to a new environment without re-training . In the case of online co-training , the safety agent is trained in conjunction with the regular task agent on the same environment . 3.In this paper , the side-effect metric is defined by the environment , as we previously mentioned in Point # 1 . Side-effect metric and safety metric are used interchangeably - we have updated the manuscript to keep the terminology consistent . Our discussion in Section 1 and Section 6 focused on the general difficulty in developing an effective side effect metric which continues to be an active area of research . 4.S [ \\pi_theta ] in Equation 3 refers to the entropy of the policy as described in the original PPO paper . Please let us know if we addressed your points and if you want to continue to discuss more ."}], "0": {"review_id": "RDpTZpubOh7-0", "review_text": "* * Update after authors ' response * * I want to thank the authors for their responses . My responses to the authors ' comments are in the respective threads . * * Summary * * The paper addresses the timely and important problem of how to train RL agents such that they solve desired tasks while not engaging in undesired behavior that is not explicitly specified via the reward function . In particular , the paper focuses on training agents that learn to avoid unnecessary side effects , that is ( irreversible ) alterations to the environment which are not necessary to solve the task at hand . Experiments are performed on SafeLife , which provides a suite of tasks in an environment ( potentially with rich intrinsic dynamics ) , along with a quantitative measure of the strength of undesired side effects . The main idea of the paper is to co-train an RL policy on this side effect measure with the aim of minimizing side effects . This policy is used for regularizing the reward-optimizing agent during training , such that the trained agent learns to bias its actions towards avoiding side effects when the task allows for multiple viable actions . The paper compares against a strong , previously reported baseline , both in static and dynamic SafeLife environments/tasks . Additionally , the generalization of the side-effect-avoiding policy is tested , by using it for training a reward-optimizing agent on task-versions that the side-effect-avoiding policy has not been trained on . * * Contributions , Novelty , Impact * * 1 ) Incorporation of the two ( sometimes conflicting ) objectives of maximizing reward and avoiding side-effects into a single training objective , where purely reward maximizing actions are regularized by action-distributions from a side-effect-minimizing policy . This is an interesting idea that turns trading off avoiding side effects against reward maximization into a learning problem . I think this is a promising way forward . What I \u2019 d like to see in the paper for even greater impact is a clear discussion of the requirements ( the objective of avoiding side-effects must be specified as a trajectory-dependent , quantitative function , similar to a reward function ) , and the current limitations ( unclear how to assess \u201c how much \u201d of the task-relevant state-space is well covered by the side-effect-avoiding policy , particularly in the zero-shot setting ) . 2 ) Experimental evaluation of the proposed method on * the * state-of-the-art benchmark suite , and comparison against a strong , previously proposed baseline . The results are promising , though it \u2019 s hard to distill a very clear message in favor of the method from the results shown . I personally think that \u2019 s fine ( and to some degree expected when discussing solutions that solve a particular trade-off in a different fashion ) , but I \u2019 d like to see even more of a multi-faceted evaluation and discussion in the paper . 3 ) The idea of learning a side-effect-avoiding regularizer that generalizes well , e.g.to different tasks under the same environment dynamics . This is very interesting and a promising step towards tackling the side effects problem at scale . It is very nice to see the zero-shot results . To make the paper even stronger and more impactful it would be nice to evaluate the generalization of the trained side-effect-avoiding police in more detail . * * Score and reasons for score * * I am ( currently ) in favor of accepting the paper , though I think that some additional work could improve the strength and potential impact of the work . The topic addressed is timely and very important , and the approach taken is interesting and sensible . Results look promising , and the paper does a great job at presenting the work . To further strengthen the paper it would be nice to discuss results in more detail and potentially perform additional experiments to highlight certain aspects that are \u201c buried \u201d in the current results . Additionally it would be good to say something more substantial about the generalization properties of the side-effect-avoiding policy . While the latter two issues are probably beyond what \u2019 s easily possible in the rebuttal phase , I want to strongly encourage the authors to add a short paragraph that clearly states the assumptions/requirements ( the strongest assumption is perhaps the presence of a quantitative side effect measure which can be used directly as a reinforcement signal ) , and current limitations . I am looking forward to the other reviews and authors \u2019 response , and will update my final verdict accordingly . * * Strengths * * 1 ) Empirically promising results on a timely and important problem , including the comparison against a strong baseline method . 2 ) Evaluation of proposed method by : ( i ) multiple runs to assess statistical significance , ( ii ) ablation studies regarding the \u201c distance \u201d metric used by the method , ( iii ) control-experiments regarding the ( zero-shot ) generalization performance of the side-effect-avoiding policy . 3 ) Well written paper , with good introduction to the problem and discussion of related literature ( given the limited space of a conference-format publication ) . * * Weaknesses * * 1 ) The experimental results shown are interesting and promising , but it \u2019 s hard to distill a clear message from the results other than : \u201c the proposed method seems to work on par with a previously proposed method but often makes the trade-offs ( between high reward and low side-effects ) differently , which makes comparison more difficult \u201d . Drilling down on some of the findings and trying to control for more factors to get a clearer picture would strengthen the results . 2 ) The generalization of the side-effect-avoiding policy is a very interesting aspect of the work , however the current analysis of how well that generalization behaves is a bit crude . It is unclear to which degree the previously trained side-effect-avoiding policy in the zero-shot regime covers the state-space encountered when solving a particular task . It is also unclear whether the side-effect-avoiding policy in the generalization setting \u201c behaves mostly well overall \u201d or whether it has some severe and potentially even systematic shortcomings ( leading to undesired policies ) in particular situations of the generalization regime . Addressing this in full generality is of course beyond the scope of this paper , but some more analysis into this issue would be very nice to see ( e.g.comparing the zero-shot vs the trained side-effect-policies in isolation , and potentially drilling in on some of the differences encountered ) . * * Correctness * * The construction of the algorithm and training scheme presented in the paper seems correct to me . * * Clarity * * The paper is mostly well written , and the method is clearly described . Perhaps two things to improve : ( i ) the discussion of results could be expanded a bit more , there \u2019 s a lot going on in the plot and unfortunately there \u2019 s no intuitive message that one can easily take away visually . ( ii ) To facilitate the flow of the manuscript to readers unfamiliar with SafeLife it would be nice to include a short section describing the side-effect penalty . * * Improvements / major issues * * 1 ) The results currently shown are interesting but it \u2019 s hard to distill a clear message ( which is understandable to some degree , as the paper also points out , because different solutions to a multi-objective optimization can not be easily compared ) . It might be worthwhile though to expand the discussion ( and perhaps even presentation ) of the results a bit more . 2 ) One of the most interesting aspects of the work is the potential to train a task-agnostic side-effects-avoiding policy that generalizes to a broad range of tasks . The paper demonstrates that this works by applying said policy in a zero-shot setting and analyzing the resulting policy . It would be nice to also do some more comparison of the side-effects-avoiding policies directly ( e.g.what is the side effect score when directly comparing a zero-shot Z vs a Z trained on the current task/environment - are there any systematic deviations between the two , do certain biases get baked into the zero-shot Z that can be explained by the tasks/environment-variants it \u2019 s been trained on ) . 3 ) A clear discussion of the requirements ( the objective of avoiding side-effects must be specified as a trajectory-dependent , quantitative function , similar to a reward function ) , and the current limitations ( unclear how to assess \u201c how much \u201d of the task-relevant state-space is well covered by the side-effect-avoiding policy , particularly in the zero-shot setting ) . 4 ) Please clarify : why are there separate zero-shot agents shown in prune-still and append-still - shouldn \u2019 t they be the same SARL JS/WD since the zero-shot agents have been trained on these two tasks respectively ? 5 ) Please clarify and potentially discuss in the paper : perhaps the main requirement for the method is having a side-effect-strength signal s. This signal must be suitable for a reinforcement learning algorithm to train a side-effects-minimizing policy Z . But if such a signal is available , why not simply combine it with the task-specific reward function r to create a \u201c safe reward function \u201d to train a reward-optimizing agent that avoids side-effects ? Would the solution obtained this way be qualitatively different ( in some aspects ) compared to the solution obtained by the proposed scheme ? It \u2019 s fine to simply comment on this - the strongest version would include actual control experiments ( but I understand that this might not be easily doable ) . 6 ) Please comment and potentially discuss in the paper : What is the advantage of co-training Z with A ( lines 11-15 of Algorithm 1 ) ? Why not train Z first ( e.g.would that improve training stability ) ? * * Minor comments * * A ) Please give a few details for the side effect metric that \u2019 s used by the experiment ( fine to refer to the SafeLife paper for full details , but the rough idea should be in the paper to improve readability ) . B ) How exactly is it ensured that Z sees the same parts of the state-space that A does ( i.e.how is it ensured that Z \u201c explores \u201d similarly to A , which is solving some tasks ) ? I assume that the actions actually taken ( which lead to a certain state on which A and Z are evaluated in line 5 and 6 in Algo 1 ) are driven by A ? C ) P4 : \u201c In this formulation , policy characteristics are converted to distributions in a latent space of behavioral embeddings on which the Wasserstein Distance is then computed. \u201d . I have a hard time following this sentence , please consider unpacking it a bit .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your very thorough review , we hope that we can clarify some of your concerns through these discussions . In this comment , we would like to focus on the Improvements you outlined and address them directly : 1 . We agree that a clearer discussion on the key takeaways would be useful . Our primary goal was to train a portable safe-agent that could generalize across multiple tasks that share common dynamics . Our results on zero shot transfer show that the transferred safe agents performed similarly compared to training SARL agents from scratch on the same environment . The primary distinctions from prior work are : a. the use of a safe-agent to abstract the concept of safety . b. the portability of the safe agent across multiple tasks . c. using probabilistic distances to capture differences between policies with different objectives on the same environment We did not cast this problem as a multi-objective optimization problem in this paper - however , that is a natural next step for this line of work in the future . As mentioned in our general comment , proper reframing of the framework as a multi-objective setup requires significant future work , which we believe this is out of scope for this paper . 2.Based on our understanding of your comment , we already performed those experiments . We call them \u201c from scratch training \u201d and show them in Section 5 in the lighter shade of color . Here , the task agent and safety are trained concurrently on the same environment , and we observe similar behavior to using safe policies that are zero-shot generalized from other environments . Does this address the scenario you are suggesting ? 3.We have updated the details pertaining to the side effect metric in Section 2 of our new draft . We take the side effect metric from SafeLife as is and use it to train our safety agent . We would like to highlight that we designed SARL to be agnostic to the safety metric used , as the distance function formulation allows for that flexibility . 4.In the prune-still environment we take the safety agent trained on the append-dynamic environment , and in the append-still case we take the safety agent trained on the prune-dynamic environment . As we discuss in Section 5 , we wanted to take the safety agent trained on the environment that is most dissimilar to the task environment since we believe that would be the most difficult to generalize from . It is not strictly necessary to do that , as we could have one zero-shot agent that is generalized to all environments except for the one it is trained on . 5.We already train a baseline policy on a shaped reward that combines the primary objective and side-effect penalty . Since this was the method applied in the original SafeLife paper , we use this method as the \u201c baseline \u201d in our experiments . This method takes the frame-by-frame side effect from SafeLife and subtracts that from the frame-by-frame reward ( with a scaling factor ) . We searched for a good scaling factor as a hyperparameter in our experiments to support the side effect baseline . 6.Thank you for this suggestion . Co-training the task and safety agents on the same environment was primarily a practical choice , but we will add experiments that work with a safety agent that is previously trained . Intuitively , we do not expect major changes and agree with you that it should improve training stability . Regarding the Minor Comments : 1 . We have added more details on the side effect metric in our new draft . 2.The current environment includes a fully visible state space ; we can therefore pass the entire state to A and Z when we train A . In the current setup ( using an on policy method like PPO ) , it is difficult to ensure exploration of the same states . If we were to use an off-policy algorithm , such as DQN , we can use the replay buffer to ensure that A and Z are drawing from the same distribution of states during their training . 3.This sentence refers to how the Wasserstein distance is computed using the method described in Pacciano et al.They construct a space of test functions in a latent behavioral space and compute in that space . Essentially what happens is : 1 . A trajectory ( defined by the user ) is given 2 . A function transforms that trajectory to latent embedding space 3 . The WD distance is computed in that space iteratively via test functions . The original paper provides a lot more detail on this process , which essentially allows one to take any definition of a trajectory and compute a distance on it . Please let us know if we addressed your points and if you want to continue to discuss more ."}, "1": {"review_id": "RDpTZpubOh7-1", "review_text": "This paper proposes a safety-aware reinforcement learning algorithm that learns to perform tasks with minimal side-effects . The key idea is that a safety policy is learned independent of the task reward . When learning the task , this safety policy is incorporated by minimizing the distance between the task agent and the safety agent . In this way , the paper claims that the safety agent can be generalized to different tasks . The method is tested on SafeLife Suite , and its performance can match task-specific safe learning baselines . Safe reinforcement learning is an extremely important research area , when we need to apply reinforcement learning to real-world applications , such as robotics , recommendation system , power grid , etc . This paper works in this direction and addresses the key challenges , including how to learn generalizable safety agents . While I think that the paper is promising , I have the following three major concerns : 1 ) The `` side effect metric '' is not clear to me . The description in Section 2.1 is high-level and vague . More rigorous mathematical definition is preferred here . For a safe learning paper , it is extremely important to clearly define what safety means . Is the `` side effect metric '' the same as the `` safety metric '' in Line 12 of Algorithm 1 ? Reading from the text , it seems that the side effect metric is calculated per episode , while the safety metric is per step . 2 ) Section 3.4 seems to leak the testing set into training . One claim of this paper is that the learned safety agent is generalizable : Z ( \\psi ) can be taken zero-shot from previous trained environments . However , during training , by tracking the Champion policy , decisions are made based on the performance on the testing environments , by retaining the policy that performs the best in the testing environments . If my understanding is right , this makes any claim about generalization less convincing because the training directly optimizes the policy in the testing environments . 3 ) Intuitively , I do not understand how Algorithm 1 could work . According to Algorithm 1 , the training of the safety agent Z ( \\psi ) is totally independent of the task , whose only objective is to be safe . If it is the case , the learned safety agent would not move or take action at all . The action distribution P_Z_\\psi would be concentrated on the zero action . This would make optimizing A ( \\theta ) using the loss ( eq . ( 1 ) ) extremely difficult . This might explain why the paper observes that the hyperparameter \\beta is difficult to tune . Here are some minor suggestions about writing : 1 ) A brief description of the 4 tasks is needed ( prune-still , prune-dynamic , append-still , append-dynamic ) to make this paper more self-contained . If the page-limit is a concern , this description could be added to the Appendix . Otherwise , it is difficult for readers to understand the difficulties and the usefulness of these tasks . 2 ) `` Line 13-17 from Algorithm 1 '' : The pseudo-code ends at Line 16 . For the above reasons , I would not recommend acceptance at this time .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and comments . We would like to address the concerns you identified directly : 1 . We have updated our description of the side effect metric in Section 2 of our new draft . There we describe the difference between the frame-by-frame metric used for training the SARL agent and the episodic metric we used to report our results in Section 5 . 2.The training and testing sets refer to different environment configurations within a given task . Let \u2019 s take prune-still as an example : Within prune-still there is a large set of different environment configurations that lead to different maps in SafeLife for which the agent will attempt to solve the prune-still task . Our process takes out a test set of configurations that are not used during training and tracks the champion policy ( of the primary task agent ) within them purely to report test performance . The safety agent never interacts with the test environments in any of the training runs , and the generalization claim pertains to transferring the safety agent across different environments , meaning that a safety agent trained on prune-still will be applied in append-dynamic , etc . We hope this clarifies the process . 3.Your intuition that the safety agent will converge towards a \u201c no action \u201d policy is what we often observed . We also believe this a major reason why finding a good \\beta is difficult to achieve . The \u201c no action \u201d policy , however , does not apply to all states that the task actor visits due to the complexity of the SafeLife environment . One potential mitigation for this problem is to modify the reward signal the safety agent is trained on , such as incentivizing it to reach only the level exit as safely as possible . We would also like to add that since task performance and safety often come at a trade-off , it is difficult to define an optimal solution . As we discuss in Section 6 and in our general comments , we plan to extend this work in the future to cast it as a multi-objective RL problem where such trade-offs can be better analyzed . 4.We have added more detailed descriptions about the different tasks . We have currently placed them in the appendix due to space concerns . 5.Thanks for pointing out this typo ."}, "2": {"review_id": "RDpTZpubOh7-2", "review_text": "The paper aims to reduce the unwanted side effects of the actions of a reward-maximizing reinforcement-learning ( RL ) agent . The authors study a framework in which the environment issues a metric that measures the total side effects of the agent 's actions at the end of each episode . The work 's proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects . The authors then empirically investigate the effectiveness of combining the two agents via a distance measure between the policies that unifies them into one . I find the following items strong points in the submission : * The posed problem is relevant in the context of safety in AI . * The chosen testbed for the experiments matches the goals and premises of the posed problem . * The empirical results suggest that the proposed method is effective . * The discussion regarding the choice of the distance measure is thorough and makes sense . On the other hand , I find the following issues as weaknesses in the submitted manuscript : * There are no theoretical developments to demonstrate whether the reported results generalize beyond the adopted environment settings and the value assigned to the parameter beta or not . * The paper dives right into introducing the loss function in Section 3 without establishing the required notation and preliminary materials . A brief summary of the task agent and the virtual safe agent descriptions is currently provided in the caption of Figure 2 . In my opinion , Section 3 would read better if the authors append a preliminaries section wherein they establish the notations and the descriptions of the task agent and the virtual safe agent . * The loss function adopted in equation ( 3 ) provides little room for theoretical developments . The original paper that introduces the PPO algorithm ( Schulman et al. , 2017 ) offers multiple loss function choices . In my opinion , the combination of the Jenson-Shannon distance with the loss function that incorporates the KL divergences enables the authors to study their proposed algorithm beyond empirical results . I find the posed problem relevant in the context of safety in AI and the suggested method well-motivated and intuitive . I believe the submission is far from theoretically solving the posed problem ; however , the methodology alongside the promising empirical results that the manuscript offers may be of interest .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and comments . We would like to address the weaknesses you identified : 1 . It is true that we primarily focused our work on the SafeLife suite and can not make any statements beyond the environments in SafeLife . We believe , however , that SafeLife provides a rich set of different settings that allow us to demonstrate the different notions we discuss in the paper . We believe that a thorough theoretical framework for generalization outside of the SafeLife suite is beyond the scope of the paper . A theoretical framework for generalization of RL agents , as well generalization neural networks overall , are broader fields of research and open problems in the deep learning community . 2.Thank you for this feedback . We have updated our notation and further descriptions in our new draft . 3.Our current framework assumes that the task agent employs a loss function given from an established RL algorithm . Based on your feedback , we will add a deeper discussion on further development of different loss functions that involve probabilistic distances . We believe conducting this study would not be feasible within the scope of this paper , but we agree that a more thorough theoretical discussion is important ."}, "3": {"review_id": "RDpTZpubOh7-3", "review_text": "This paper aims to address the issue of mitigating side effects in policy learning . The authors propose an algorithm SARL , which uses a safe policy to define a regularization term for penalizing the agent 's actions deviating from the safe agent in policy learning . In the experiments , four variations for SARL are shown and compared with a baseline method based on reward penalty . The proposed algorithm is competitive across the experiments presented in the paper . I think side effects and safety in reinforcement learning is an important issue . However , this paper does a bad job in describing the problem it wishes to address and , therefore , it 's unclear whether the proposed algorithm really achieves that goal . 1.The main motivation of this paper is to mitigate the side effects in learning . However , the definition of side effects were never given . It 's only until Algorithm 1 is presented where the paper mentions a safety metric that the safe agent aims to optimize ( is this the same s appearing in A ( s|theta ) and Z ( s|psi ) in Sec 3.2 ? ) , which however is not defined . Therefore , I do not fully understand what the objective of this learning algorithm wants to achieve . From the paper 's vague description , it seems like the goal is that the learner should have high performance in the original reward while not causing high side effects . This is a multi-objective MDP problem or at least can be framed as a constrained MDP . However , the proposed algorithm , based on simple regularization with a constant weight , can address neither of these two criteria . I am wondering if the authors consider to more explicitly outline the solution concept they wish to obtain . Current hand-wavy description makes me difficult to judge whether the proposed algorithm actually solves the problem they wish to solve . 2.In Algorithm 1 , since the safe agent Z is updated independently of the progress of the learner agent A , when there 's only a single environment , there is no point of distinguishing the so-called `` zero-shot '' and the online version , as in high level this dependency allows us to pretrain the safety agent alone beforehand and get the same results . Or do the authors mean zero-shot in the sense that the safe agent is trained on a different set of environments and the online version means they 're trained on the same environment ? 3.In the paper , the authors write multiple times that a difficulty in this problem setup is that the side effects are difficulty to define . But it seems that the proposed algorithm assumes some safety metric . How are the two related precisely ? And what is that used in the experiments ? 4.What is S [ \\pi_theta ] in ( 3 ) ? Overall , I think the paper is rather incomplete and therefore I do not recommend acceptance .", "rating": "3: Clear rejection", "reply_text": "Thank you for your review . We would like to address your main concerns directly : 1. a . We agree that the description of side-effects was not completely clear in the paper and have updated the description in our new draft as mentioned in our response to all reviewers . For SARL training , we assume that the environment provides a definition of the side effect . As we describe in Section , SafeLife calculates a side effect signal by taking the deviation between a baseline state and the current state . This is the frame-by-frame metric we use to train our agents . We also use the episodic side-effect to report our results where this difference is calculated between a distribution of states at the beginning and end of an episode to account for environment dynamics . b.Thank you for pointing out the inconsistent notations . We have updated the paper , including Algorithm 1 , to correct this instance as well as a few others we noticed . In the current version , s refers solely to the state of the environment . In the old notation , the \u2018 s \u2019 in Algorithm 1 , line 1-2 , \u2018 s \u2019 referred to the state of the environment and in Algorithm 1 , line 12-14 we referred to the side effect metric as \u2018 s \u2019 in green color . c. You are correct that the multi-objective MDP formulation is very applicable here . As we discuss in Section 6 , this is a future direction that we are highly interested in since we believe it would be a more effective way to understand the trade-offs between task performance and safe behavior . Our primary goal in this paper was to encapsulate the concept of safety into a safe actor - that could then be used to modulate the behavior of a primary agent on different tasks in the same dynamic environment . This mitigates the need to define or learn a penalty factor for every task that shares similar dynamics . 2.In the zero-shot experiment , the safety agent is trained on a different environment and then transferred over to a new environment without re-training . In the case of online co-training , the safety agent is trained in conjunction with the regular task agent on the same environment . 3.In this paper , the side-effect metric is defined by the environment , as we previously mentioned in Point # 1 . Side-effect metric and safety metric are used interchangeably - we have updated the manuscript to keep the terminology consistent . Our discussion in Section 1 and Section 6 focused on the general difficulty in developing an effective side effect metric which continues to be an active area of research . 4.S [ \\pi_theta ] in Equation 3 refers to the entropy of the policy as described in the original PPO paper . Please let us know if we addressed your points and if you want to continue to discuss more ."}}