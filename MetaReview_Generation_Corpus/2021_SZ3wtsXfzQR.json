{"year": "2021", "forum": "SZ3wtsXfzQR", "title": "Theoretical bounds on estimation error for meta-learning", "decision": "Accept (Poster)", "meta_review": "The authors study the theoretical performance of a meta-learning in two settings. In the first one the overall number of possible tasks is limited and tasks are close in KL-divergence. The second setting is MAP estimation (in a hierarchical Bayesian framework) for a family of linear regression tasks. Lower and upper bounds are provided on minimax parameter estimation error.\nThis paper has spurred a lot of discussion among reviewers and (competent) external commentators. Most of these criticisms were right on target, but the authors managed to convince the reviewers and myself that there was simply an issue of presentation of the main results. I suggest the authors to take into serious considerations all the aspects raised by the reviewers that has generated misinterpretations of the presented results.", "reviews": [{"review_id": "SZ3wtsXfzQR-0", "review_text": "Pros : 1 - The paper seems to be well written , have a good review of the references and necessaries for understanding the problem . 2 - Some of the results found in the paper seem to be interesting . For instance , the asymptotic analysis provided in paper gives some insight on the performance of meta learning algorithms with the number shots , ratio of observation noise to the sampling noise and the number of tasks . Cons : 1 - It seems that most of the results of the paper are based on Loh ( 2017 ) . It is expected that the author differentiate their contributions with those of that reference more clearly . 2 - Some of the notations are misleading . This is a minor issue and up to the authors to change it or not but it may help with the readability of the paper . Some notations like $ \\theta $ are usually used for parameters in models . In this paper , $ \\theta $ is used as a function . I understand why the authors decided to use it as a function but it may be a bit misleading . 3 - Some of the assumptions in the paper can be very restrictive . For instance , it is assumed that the distributions are $ 2\\delta $ -separated while close in the KL divergence sense . Is n't this too restrictive ? Maybe it is good for the authors to try to talk more about the implications of such assumptions . How does this restrict the space of interested probability distributions ? 4 - Another example of a restrictive assumption is bounded minimum singular values . How does such a restriction affect the space of considered solutions ? 5 - There has been no effort in comparing with any bounds that are available currently .", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # Contribution relative to Loh 2017 We certainly utilize some of the techniques presented in Loh 2017 , and elsewhere in the minimax risk literature . We extend these results on several fronts : addressing generalization to novel tasks , multiple sources of training data , and novel local-packing bounds under product and mixture sampling procedures . We will make these contributions clearer in our paper . # # # $ \\theta $ as a functional We follow the notation introduced by Loh , 2017 and others within the minimax risk literature . We acknowledge that this notation does not agree with the standard notation used in machine learning and related fields . But it does have the advantage of being succinct . We introduce short-hand notation ( $ \\theta_P = \\theta ( P ) $ ) , which aligns with the standard ML notation . # # # Local-packing is a strong constraint This is indeed a strong constraint , but is a typical one in the minimax risk setting . Further , these constraints are natural and relevant in FSL . For example , metric learning approaches to FSL such as Prototypical Networks learn an embedding space in which classes of images must be discriminable ( meaning separated in the embedding space ) , but share features , so that novel class data are embedded within the bounds of the space . Notably , in this paper we do not introduce any additional constraints relative to the standard minimax setting . Therefore , existing packing sets can be utilized in our setting . Additionally , please see our response to R3 ( `` Local-packing examples '' ) . # # # Minimum singular values assumption This assumption is potentially strong , but can be verified for large random matrices ( Raskutti et.al , 2011 ) . Note that our upper bound depends explicitly on the singular values lower-bound , and so the effect of weakening this assumption on our results can be measured . # # # No comparison to existing bounds This is true , though we argue that our bounds are not immediately comparable to existing ones as , to our knowledge , these are the first minimax bounds in this meta-learning setting . We can directly compare our bounds to existing IID bounds in the minimax literature ( and do so within our work ) . If you have any particular bounds in mind , we would be happy to consider how they compare to our results ."}, {"review_id": "SZ3wtsXfzQR-1", "review_text": "The paper studies the information-theoretic lower bounds in the minimax setting of meta-learning . The paper also discusses upper and lower bounds in the hierarchical Bayesian framework of meta linear regression . The novelty of the paper is two-fold : a ) it proves a novel meta-learning local packing result to compute the conditional information between training task samples and the novel task data distribution and b ) it compares the lower bound of the risk to the risk of posterior estimate in meta linear regression . In addition , the authors verify the dependence of risk on various parameters in 2 different experiments . My major concern is that in theorem 3 , $ Mn $ needs to grow exponentially with $ d $ to be significantly better than the case when there are no training tasks available ( theorem 2 ) . However , theorem 4 states that $ Mn $ just needs to depend polynomially on the dimension $ d $ . Hence , the upper bound and the lower bound on the risk have a gap on the parameter $ d $ . Thus , it is not clear if the lower bound in Theorem 1 is tight . It would be great to have a discussion on this and the assumptions that one needs to take to improve upon this gap . Few other concerns : 1 . In the hierarchical regression setting , the assumptions for theorem 3 and theorem 4 are different . A discussion on how the lower bounds will change , if we assume different variance of task parameters $ \\sigma_ { \\theta_1 } , \\cdots , \\sigma_ { \\theta_ { M+1 } } $ and task-specific observed samples $ \\sigma_1 , \\cdots , \\sigma_ { M+1 } $ , will be helpful . 2.The proof of theorem 3 focuses on packing the parameter space $ \\theta $ by an $ \\epsilon $ -net on $ B_2 ( 4\\delta ) $ , where $ \\delta $ is fixed later on . However , the model assumes $ \\theta $ coming from ball $ B_2 ( 1 ) $ . There seems to be some mismatch in the theorem statement and the proof . 3.The risk in theorem 4 involves few more parameters whose effect on the risk has n't been checked in experiments e.g.the ratio of variance in observed samples of novel task v/s variance in task parameters . Also , it will be interesting to see how the results change when the data distribution is changed since the risk depends on the singular values of the data matrix . I have read the proofs . They are easy to read and understand . My scores are slightly on the lower side because I am concerned about the tightness of the lower bound in Theorem 1 . I am happy to discuss this with the authors and other reviewers during the discussion period . * * After Rebuttal * * I have read the reviews of other reviewers and the responses of the authors to the questions posed by the reviewers and Ahmad Beirami . I understand that the authors have taken a pessimistic approach to compute the lower bounds of risk in the minimax setup of meta-learning . However , I believe the paper is an important step in this direction . Theorem 3 and Theorem 4 are important additions to the paper , which show a margin between the pessimistic lower bound of the risk and the actual risk with the introduction of structure to the learning setup . Overall , I enjoyed reading the paper , and I have increased my score after the rebuttal .", "rating": "7: Good paper, accept", "reply_text": "# # # Tightness of Theorem 3 vs Theorem 4 There is indeed a gap in the derived lower and upper bounds . We discuss this gap in Section 5.3 but will provide some additional discussion . We expect that our upper-bounds are asymptotically tight , and that the looseness is due to the relatedness assumptions used in Theorem 1 , which are likely too weak for meta-learning in the hierarchical Gaussian model . We also discuss the inverse exponential scaling w.r.t dimension $ d $ at the end of Section 5.1 . We aim to ( partially ) address this gap through Corollary 2 , which provides tighter lower bounds in the setting where the environment is only partially observed . # # # Different variance assumptions Theorem 4 has a more general variance assumption ( allowing different variances for the training tasks and novel task ) . We could introduce more general variance assumptions in Theorem~3 , and need only derive a new upper-bound on the pairwise KL-divergences . # # # Smaller bounding ball in proof This is a standard technique that is typically hidden in the proof of these minimax results ( see e.g.Raskutti et al.2011 ) .While this step looks to be suboptimal , existing minimax results using this approach have provably optimal rates . Introducing a larger packing set makes the derivation much less clean ; however , we are exploring this approach to see if this improves the bounds . # # # Parameters in empirical evaluation Thank you for the suggestion . The settings we explored probe at the prominent model parameters that appear in our bound . However , we did not explore varying the data distribution . We will explore this and hope to include these results before the end of the rebuttal period ."}, {"review_id": "SZ3wtsXfzQR-2", "review_text": "This paper provides a minimax novel-task risk lower bound for meta learning via information-theoretical techniques , showing the fundamental limits of meta learning . The novel-task minimax risk depends on the number of samples from the meta-training set and novel task , as well as the task similarity . The authors further investigate the meta learning problem on a hierarchical Bayesian model , discuss the lower bound and upper bound with maximum-a-posterior estimator . Overall I feel this paper study an important problem of the meta-learning and the derivation all looks correct . The main drawback is that the presented minimax lower bound is quite pessimistic that may not fully exploit the potential task similarity in practice , but I feel it is still acceptable to first study the minimax lower bound , and leave the case with more structure as future work . I would like to ask if there is a gap between the description at the beginning of Section 5 and the analysis starting from Section 5.1 , as the bayesian model described at the beginning of Section 5 does not assume theta have ell_2 norm smaller or equal than 1 , and in fact , can not be universally hold as theta is sampled from a d dimensional Gaussian ( can have high probability arguments instead ) . However , the analysis throughout Section 5 all have this constraint . I don \u2019 t see strong dependency on the norm of theta ( e.g.in the covering number , KL bound etc ) but I hope the authors check and clarify it in the next version . One small tip : lower bound is better represented via big-Omega notation .", "rating": "7: Good paper, accept", "reply_text": "# # # Gap between 5 and 5.1 We acknowledge that this is a potential source of confusion in the current write-up , and will clarify it in the next version . Here we present an alternative view-point . We have a meta-learner in Section 5 defined by Eq 28-30 , that utilizes data from the training tasks and novel task . This meta-learner is derived by taking an empirical Bayes estimate in the hierarchical Gaussian model . The bounds we derive apply in the minimax setting that we study in Sections 3 and 4 , even if the model is mis-specified for the actual data distribution seen . Natural extensions of our results may introduce high-probability arguments as you suggested , to bridge the gap . # # # No strong dependency on the norm of $ \\theta $ We assume that $ \\theta $ is bounded in a 1-norm ball . The norm of $ \\theta $ plays a role in the computation of the KL-divergence bounds in the lower-bounds , and in the bias computation in the upper-bounds . But due to the unit-factor this is n't visible in the presented results themselves ."}, {"review_id": "SZ3wtsXfzQR-3", "review_text": "The authors study the performance of a meta-learner theoretically in two settings . In the first one the overall number of possible tasks is limited and tasks are close in KL-divergence . The second setting is MAP estimation for a family of linear regression tasks . Lower and upper bounds are provided on minimax parameter estimation error . The bounds solidify common sense knowledge about the role of the number of training tasks , number of samples , and task relatedness and diversity on the performance of meta-learner and are supported by numerical examples . I am in favor of accepting this paper . I 'll provide suggestions for improving presentation . 1.The assumption in the first setting is that the tasks are close in distribution space but far away in parameter space . Could you mention some examples of such set of tasks ? It does not seem like a weak assumption to me , as mentioned in the paper , but then this is the nature of lower bounds . The point is not to model a typical scenario but to design hard cases that lead to high error . But in any way an example is necessary to show that this situation is possible . 2.It seems to me that the lower bound in the first setting has less to do with the performance of the meta-learner than suitability of parameter estimation error as a measure of performance . At the end of the day , it is accurate predictions , and not accurate parameters , that matter . The setting is designed so that tasks have similar distributions but quite different optimal parameters , which results in a high parameter estimation error for the meta-learner . It could be that the meta-learner in fact makes quite accurate predictions ( since the tasks are actually close in distribution space ) despite its high parameter error . 3.The first setting is worded generally ( novel task generalization ) while the second setting is presented as a more specific scenario ( hierarchical Bayesian linear regression ) . The first time I was reading the paper I assumed that the first part describes general results on meta-learning and the second part shows a particular instance of the previous section , which was confusing as the second setting does not satisfy the assumptions in the first setting . I suggest presenting these two sections as two particular and different settings as they are . Minor comments : 1 . The statement of Theorem 1 has undefined notation . I assume I ( . ; . ) is mutual information but it is not defined . The text defines W|\\pi and Z|\\pi but the equation uses \\pi_ { M+1 } and W and Z separately . What do \\pi_ { M+1 } , W , and Z ( without the vertical bar ) mean ? 2.The paragraph above Corollary 1 is hard to understand . Do `` training task '' and `` meta-training task '' have the same meaning ? Also , if the number of previous tasks is M \\leq J-1 , how can there be J previous tasks that are close ? 3.What is B_2 ( 1 ) in 5.1 ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Local-packing examples Our paper includes such a set of tasks in the space of linear regression models that we consider ( Section 5.1 ) . In general , it is difficult to construct realistic spaces of distributions that satisfy these constraints . Fortunately , by design , these assumptions align with those used in the standard minimax framework . Therefore , we may use our results to study meta-learners in spaces of distributions studied in those works . For example , Raskutti et al.provide several compelling examples , focused on $ \\ell_q $ high-dimensional linear regression . Yang \\ & Barron analyze local-packing for minimax rates , and provide several examples including data compression and non-parametric regression . # # # Parameter estimation vs. accuracy We follow the typical statistical minimax risk framework where parameter estimation error is the focus . You bring up a good point , that these parametric estimation bounds do not speak to prediction performance in general . However , there are standard methods to extend the symmetric parameter loss to objectives like cross entropy . The clearest example we \u2019 re aware of is given in Duchi \u2019 s 2016 lecture notes , in Question 7.5 . In short , the $ \\delta $ -packing can be applied in loss space directly to allow more general forms of risk . # # # Discrepancy between Section 4 and Section 5 Our bounds in Section 5 are valid bounds on the minimax risk object that we introduce in Section 3 . We agree that there are some discrepancies here that can be clarified significantly ( see response to R2 , `` Gap between 5 and 5.1 '' ) . We are not certain what particular assumptions you are referring to , and so would appreciate clarification if necessary . # # # # Minor comments 1 . Thank you , we will make these clear in the paper . 2.Yes , these have the same meaning . We will make these consistent . $ M $ tasks are visible to the learner , but the proof requires $ J $ distributions in the local-packing . In the setting $ M < J-1 $ , we assume that the learner only sees tasks from some incomplete subset of the full packing . 3.This is a ball of radius 1 ( measured in 2-norm ) , centered at the origin ."}], "0": {"review_id": "SZ3wtsXfzQR-0", "review_text": "Pros : 1 - The paper seems to be well written , have a good review of the references and necessaries for understanding the problem . 2 - Some of the results found in the paper seem to be interesting . For instance , the asymptotic analysis provided in paper gives some insight on the performance of meta learning algorithms with the number shots , ratio of observation noise to the sampling noise and the number of tasks . Cons : 1 - It seems that most of the results of the paper are based on Loh ( 2017 ) . It is expected that the author differentiate their contributions with those of that reference more clearly . 2 - Some of the notations are misleading . This is a minor issue and up to the authors to change it or not but it may help with the readability of the paper . Some notations like $ \\theta $ are usually used for parameters in models . In this paper , $ \\theta $ is used as a function . I understand why the authors decided to use it as a function but it may be a bit misleading . 3 - Some of the assumptions in the paper can be very restrictive . For instance , it is assumed that the distributions are $ 2\\delta $ -separated while close in the KL divergence sense . Is n't this too restrictive ? Maybe it is good for the authors to try to talk more about the implications of such assumptions . How does this restrict the space of interested probability distributions ? 4 - Another example of a restrictive assumption is bounded minimum singular values . How does such a restriction affect the space of considered solutions ? 5 - There has been no effort in comparing with any bounds that are available currently .", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # Contribution relative to Loh 2017 We certainly utilize some of the techniques presented in Loh 2017 , and elsewhere in the minimax risk literature . We extend these results on several fronts : addressing generalization to novel tasks , multiple sources of training data , and novel local-packing bounds under product and mixture sampling procedures . We will make these contributions clearer in our paper . # # # $ \\theta $ as a functional We follow the notation introduced by Loh , 2017 and others within the minimax risk literature . We acknowledge that this notation does not agree with the standard notation used in machine learning and related fields . But it does have the advantage of being succinct . We introduce short-hand notation ( $ \\theta_P = \\theta ( P ) $ ) , which aligns with the standard ML notation . # # # Local-packing is a strong constraint This is indeed a strong constraint , but is a typical one in the minimax risk setting . Further , these constraints are natural and relevant in FSL . For example , metric learning approaches to FSL such as Prototypical Networks learn an embedding space in which classes of images must be discriminable ( meaning separated in the embedding space ) , but share features , so that novel class data are embedded within the bounds of the space . Notably , in this paper we do not introduce any additional constraints relative to the standard minimax setting . Therefore , existing packing sets can be utilized in our setting . Additionally , please see our response to R3 ( `` Local-packing examples '' ) . # # # Minimum singular values assumption This assumption is potentially strong , but can be verified for large random matrices ( Raskutti et.al , 2011 ) . Note that our upper bound depends explicitly on the singular values lower-bound , and so the effect of weakening this assumption on our results can be measured . # # # No comparison to existing bounds This is true , though we argue that our bounds are not immediately comparable to existing ones as , to our knowledge , these are the first minimax bounds in this meta-learning setting . We can directly compare our bounds to existing IID bounds in the minimax literature ( and do so within our work ) . If you have any particular bounds in mind , we would be happy to consider how they compare to our results ."}, "1": {"review_id": "SZ3wtsXfzQR-1", "review_text": "The paper studies the information-theoretic lower bounds in the minimax setting of meta-learning . The paper also discusses upper and lower bounds in the hierarchical Bayesian framework of meta linear regression . The novelty of the paper is two-fold : a ) it proves a novel meta-learning local packing result to compute the conditional information between training task samples and the novel task data distribution and b ) it compares the lower bound of the risk to the risk of posterior estimate in meta linear regression . In addition , the authors verify the dependence of risk on various parameters in 2 different experiments . My major concern is that in theorem 3 , $ Mn $ needs to grow exponentially with $ d $ to be significantly better than the case when there are no training tasks available ( theorem 2 ) . However , theorem 4 states that $ Mn $ just needs to depend polynomially on the dimension $ d $ . Hence , the upper bound and the lower bound on the risk have a gap on the parameter $ d $ . Thus , it is not clear if the lower bound in Theorem 1 is tight . It would be great to have a discussion on this and the assumptions that one needs to take to improve upon this gap . Few other concerns : 1 . In the hierarchical regression setting , the assumptions for theorem 3 and theorem 4 are different . A discussion on how the lower bounds will change , if we assume different variance of task parameters $ \\sigma_ { \\theta_1 } , \\cdots , \\sigma_ { \\theta_ { M+1 } } $ and task-specific observed samples $ \\sigma_1 , \\cdots , \\sigma_ { M+1 } $ , will be helpful . 2.The proof of theorem 3 focuses on packing the parameter space $ \\theta $ by an $ \\epsilon $ -net on $ B_2 ( 4\\delta ) $ , where $ \\delta $ is fixed later on . However , the model assumes $ \\theta $ coming from ball $ B_2 ( 1 ) $ . There seems to be some mismatch in the theorem statement and the proof . 3.The risk in theorem 4 involves few more parameters whose effect on the risk has n't been checked in experiments e.g.the ratio of variance in observed samples of novel task v/s variance in task parameters . Also , it will be interesting to see how the results change when the data distribution is changed since the risk depends on the singular values of the data matrix . I have read the proofs . They are easy to read and understand . My scores are slightly on the lower side because I am concerned about the tightness of the lower bound in Theorem 1 . I am happy to discuss this with the authors and other reviewers during the discussion period . * * After Rebuttal * * I have read the reviews of other reviewers and the responses of the authors to the questions posed by the reviewers and Ahmad Beirami . I understand that the authors have taken a pessimistic approach to compute the lower bounds of risk in the minimax setup of meta-learning . However , I believe the paper is an important step in this direction . Theorem 3 and Theorem 4 are important additions to the paper , which show a margin between the pessimistic lower bound of the risk and the actual risk with the introduction of structure to the learning setup . Overall , I enjoyed reading the paper , and I have increased my score after the rebuttal .", "rating": "7: Good paper, accept", "reply_text": "# # # Tightness of Theorem 3 vs Theorem 4 There is indeed a gap in the derived lower and upper bounds . We discuss this gap in Section 5.3 but will provide some additional discussion . We expect that our upper-bounds are asymptotically tight , and that the looseness is due to the relatedness assumptions used in Theorem 1 , which are likely too weak for meta-learning in the hierarchical Gaussian model . We also discuss the inverse exponential scaling w.r.t dimension $ d $ at the end of Section 5.1 . We aim to ( partially ) address this gap through Corollary 2 , which provides tighter lower bounds in the setting where the environment is only partially observed . # # # Different variance assumptions Theorem 4 has a more general variance assumption ( allowing different variances for the training tasks and novel task ) . We could introduce more general variance assumptions in Theorem~3 , and need only derive a new upper-bound on the pairwise KL-divergences . # # # Smaller bounding ball in proof This is a standard technique that is typically hidden in the proof of these minimax results ( see e.g.Raskutti et al.2011 ) .While this step looks to be suboptimal , existing minimax results using this approach have provably optimal rates . Introducing a larger packing set makes the derivation much less clean ; however , we are exploring this approach to see if this improves the bounds . # # # Parameters in empirical evaluation Thank you for the suggestion . The settings we explored probe at the prominent model parameters that appear in our bound . However , we did not explore varying the data distribution . We will explore this and hope to include these results before the end of the rebuttal period ."}, "2": {"review_id": "SZ3wtsXfzQR-2", "review_text": "This paper provides a minimax novel-task risk lower bound for meta learning via information-theoretical techniques , showing the fundamental limits of meta learning . The novel-task minimax risk depends on the number of samples from the meta-training set and novel task , as well as the task similarity . The authors further investigate the meta learning problem on a hierarchical Bayesian model , discuss the lower bound and upper bound with maximum-a-posterior estimator . Overall I feel this paper study an important problem of the meta-learning and the derivation all looks correct . The main drawback is that the presented minimax lower bound is quite pessimistic that may not fully exploit the potential task similarity in practice , but I feel it is still acceptable to first study the minimax lower bound , and leave the case with more structure as future work . I would like to ask if there is a gap between the description at the beginning of Section 5 and the analysis starting from Section 5.1 , as the bayesian model described at the beginning of Section 5 does not assume theta have ell_2 norm smaller or equal than 1 , and in fact , can not be universally hold as theta is sampled from a d dimensional Gaussian ( can have high probability arguments instead ) . However , the analysis throughout Section 5 all have this constraint . I don \u2019 t see strong dependency on the norm of theta ( e.g.in the covering number , KL bound etc ) but I hope the authors check and clarify it in the next version . One small tip : lower bound is better represented via big-Omega notation .", "rating": "7: Good paper, accept", "reply_text": "# # # Gap between 5 and 5.1 We acknowledge that this is a potential source of confusion in the current write-up , and will clarify it in the next version . Here we present an alternative view-point . We have a meta-learner in Section 5 defined by Eq 28-30 , that utilizes data from the training tasks and novel task . This meta-learner is derived by taking an empirical Bayes estimate in the hierarchical Gaussian model . The bounds we derive apply in the minimax setting that we study in Sections 3 and 4 , even if the model is mis-specified for the actual data distribution seen . Natural extensions of our results may introduce high-probability arguments as you suggested , to bridge the gap . # # # No strong dependency on the norm of $ \\theta $ We assume that $ \\theta $ is bounded in a 1-norm ball . The norm of $ \\theta $ plays a role in the computation of the KL-divergence bounds in the lower-bounds , and in the bias computation in the upper-bounds . But due to the unit-factor this is n't visible in the presented results themselves ."}, "3": {"review_id": "SZ3wtsXfzQR-3", "review_text": "The authors study the performance of a meta-learner theoretically in two settings . In the first one the overall number of possible tasks is limited and tasks are close in KL-divergence . The second setting is MAP estimation for a family of linear regression tasks . Lower and upper bounds are provided on minimax parameter estimation error . The bounds solidify common sense knowledge about the role of the number of training tasks , number of samples , and task relatedness and diversity on the performance of meta-learner and are supported by numerical examples . I am in favor of accepting this paper . I 'll provide suggestions for improving presentation . 1.The assumption in the first setting is that the tasks are close in distribution space but far away in parameter space . Could you mention some examples of such set of tasks ? It does not seem like a weak assumption to me , as mentioned in the paper , but then this is the nature of lower bounds . The point is not to model a typical scenario but to design hard cases that lead to high error . But in any way an example is necessary to show that this situation is possible . 2.It seems to me that the lower bound in the first setting has less to do with the performance of the meta-learner than suitability of parameter estimation error as a measure of performance . At the end of the day , it is accurate predictions , and not accurate parameters , that matter . The setting is designed so that tasks have similar distributions but quite different optimal parameters , which results in a high parameter estimation error for the meta-learner . It could be that the meta-learner in fact makes quite accurate predictions ( since the tasks are actually close in distribution space ) despite its high parameter error . 3.The first setting is worded generally ( novel task generalization ) while the second setting is presented as a more specific scenario ( hierarchical Bayesian linear regression ) . The first time I was reading the paper I assumed that the first part describes general results on meta-learning and the second part shows a particular instance of the previous section , which was confusing as the second setting does not satisfy the assumptions in the first setting . I suggest presenting these two sections as two particular and different settings as they are . Minor comments : 1 . The statement of Theorem 1 has undefined notation . I assume I ( . ; . ) is mutual information but it is not defined . The text defines W|\\pi and Z|\\pi but the equation uses \\pi_ { M+1 } and W and Z separately . What do \\pi_ { M+1 } , W , and Z ( without the vertical bar ) mean ? 2.The paragraph above Corollary 1 is hard to understand . Do `` training task '' and `` meta-training task '' have the same meaning ? Also , if the number of previous tasks is M \\leq J-1 , how can there be J previous tasks that are close ? 3.What is B_2 ( 1 ) in 5.1 ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Local-packing examples Our paper includes such a set of tasks in the space of linear regression models that we consider ( Section 5.1 ) . In general , it is difficult to construct realistic spaces of distributions that satisfy these constraints . Fortunately , by design , these assumptions align with those used in the standard minimax framework . Therefore , we may use our results to study meta-learners in spaces of distributions studied in those works . For example , Raskutti et al.provide several compelling examples , focused on $ \\ell_q $ high-dimensional linear regression . Yang \\ & Barron analyze local-packing for minimax rates , and provide several examples including data compression and non-parametric regression . # # # Parameter estimation vs. accuracy We follow the typical statistical minimax risk framework where parameter estimation error is the focus . You bring up a good point , that these parametric estimation bounds do not speak to prediction performance in general . However , there are standard methods to extend the symmetric parameter loss to objectives like cross entropy . The clearest example we \u2019 re aware of is given in Duchi \u2019 s 2016 lecture notes , in Question 7.5 . In short , the $ \\delta $ -packing can be applied in loss space directly to allow more general forms of risk . # # # Discrepancy between Section 4 and Section 5 Our bounds in Section 5 are valid bounds on the minimax risk object that we introduce in Section 3 . We agree that there are some discrepancies here that can be clarified significantly ( see response to R2 , `` Gap between 5 and 5.1 '' ) . We are not certain what particular assumptions you are referring to , and so would appreciate clarification if necessary . # # # # Minor comments 1 . Thank you , we will make these clear in the paper . 2.Yes , these have the same meaning . We will make these consistent . $ M $ tasks are visible to the learner , but the proof requires $ J $ distributions in the local-packing . In the setting $ M < J-1 $ , we assume that the learner only sees tasks from some incomplete subset of the full packing . 3.This is a ball of radius 1 ( measured in 2-norm ) , centered at the origin ."}}