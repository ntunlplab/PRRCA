{"year": "2021", "forum": "YTWGvpFOQD-", "title": "Differentially Private Learning Needs Better Features (or Much More Data)", "decision": "Accept (Spotlight)", "meta_review": "This paper presents a very interesting investigation. While deep neural networks are typically best in non-private settings, the authors show that linear models with handcrafted features (ScatterNets) perform better in certain settings of the privacy parameter. The reviewers all found this to be important and insightful, with a thorough investigation, and I tend to agree, recommending acceptance.", "reviews": [{"review_id": "YTWGvpFOQD--0", "review_text": "The paper presents an analysis of differential privacy in machine learning , with a focus on neural networks trained via differentially private stochastic gradient descent ( DPSGD ) . The main focus and the message in the paper is that the handcrafted features work better compared to learned features during training of NNs and having more training data results in better outcomes ( i.e.a better privacy-utility trade-off ) . Starting with the latter , this is apparent from the noise formulation in DPSGD , where the noise is reduced via sampling probability , which decreases as the data size grows . Hence , I do not consider this as a new insight or a contribution . Unless , I have misunderstood something , in which case , please do explain . For the former , as the final model used ( Table 3 and Figure 1 ) is a linear classifier , it outperforming an end-to-end CNN based model is intuitive , as it has far fewer number of parameters ( which improve the noise scale , due to smaller gradient norm ) . This is slightly touched upon in subsection `` Training CNNs on handcrafted features '' , where the comparison is made using CNNs on the handcrafted features , however there are no detailed results presented in the paper , I would have liked to see a similar table and figures as earlier . The presentation of results ( Table 3 ) is a bit strange . I would have further liked to see the comparison of both models on the * same * set of hyperparameters . Also , instead of stating that hyperparameter search 's privacy budget was not accounted for as in prior works , it would have been nice to see some analysis , such as the section D ( Appendix ) in Abadi et al .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * ... having more training data results in better outcomes [ ... ] I do not consider this as a new insight or a contribution . Unless , I have misunderstood something , in which case , please do explain . * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data , we provide a detailed response to these concerns in a meta-comment above . * In summary , while the tradeoff we show is indeed expected qualitatively , we are interested in a quantitative assessment : how much more private data is needed for private end-to-end deep learning to outperform handcrafted features ? * Our result shows that improving private deep learning simply by collecting more data might be very expensive for canonical vision tasks like CIFAR-10 : we need about one order of magnitude more data before the end-to-end CNN outperforms our handcrafted baselines . This motivates the design of better private learning algorithms in low data regimes . * * 2 ) Q : * * * as the final model used ( Table 3 and Figure 1 ) is a linear classifier , it outperforming an end-to-end CNN based model is intuitive , as it has far fewer number of parameters * * * A : * * * * The claim that our linear classifiers have fewer parameters than end-to-end CNNs is incorrect ! * * This is the point of our analysis in Section 4 : \u201c Smaller models are not easier to train privately. \u201d We find that ScatterNet models outperform end-to-end CNNs * despite having more trainable parameters ! * As the reviewer correctly notes , the noise analysis of DP-SGD would suggest the opposite , * * so this result is surprising . * * We have clarified this in our introduction . As we show in Section 4 , handcrafted features give rise to an \u201c easier \u201d learning task , where convergence occurs much faster * despite the higher noise . * * * 3 ) Q : * * * there are no detailed results presented in the paper , I would have liked to see a similar table and figures as earlier * * * A : * * It is unclear to us what the reviewer means by a lack of \u201c detailed results \u201d for CNNs trained on ScatterNet features . Figure 2 is analogous to the earlier Figure 1 and directly compares the ScatterNet+linear , ScatterNet+CNN and end-to-end CNN models for all privacy budgets we consider . We have also added results with ScatterNet+CNN models to Table 1 and Table 3 , which in particular includes some improved results for the ScatterNet+CNN model on CIFAR-10 , following a suggestion of reviewer 3 . Are there other results that we could add to the paper to better compare the ScatterNet and end-to-end results ? * * 4 ) Q : * * * The presentation of results ( Table 3 ) is a bit strange . I would have further liked to see the comparison of both models on the same set of hyperparameters * * * A : * * We are not sure what the reviewer means by a \u201c comparison of both models on the same set of hyperparameters \u201d . It seems to us that this is exactly what is shown in Figure 1 , which shows the best performance achieved by both linear ScattterNet models and end-to-end CNNs for each privacy budget , across the entire hyper-parameter set . If the reviewer meant comparing these models for a * single * assignment of hyper-parameters , this is somewhat tricky to do in a fair way . It is unclear how to choose those hyper-parameters so as not to favor one of the two models . We note that for CIFAR-10 this issue is moot anyhow : * the best hyper-parameter assignment for end-to-end CNNs is outperformed by the worst assignment of hyper-parameters for linear ScatterNet models ( Table 3 ) . * * * 4 ) Q : * * * instead of stating that hyper-parameter search 's privacy budget was not accounted for as in prior works , it would have been nice to see some analysis , such as the section D ( Appendix ) in Abadi et al . * * * A : * * We can definitely perform an analysis of the cost of hyper-parameter search as in Abadi et al.Ultimately though , none of the prior works that we compare against in Table 1 do this , so it is hard to perform a fair comparison that accounts for this cost . If we solely consider our comparison of end-to-end CNNs and ScatterNet models , then the hyper-parameter sets are identical so the analysis of Abadi et al.would yield the same cost in both cases . Moreover , as we show in Table 2 and Appendices D.1 and D.2 , our hyper-parameter search was anyhow somewhat \u201c excessive \u201d , especially for linear ScatterNets . We would obtain similar results by considering a much smaller set of parameters ( e.g. , we find that contrary to what was claimed in prior work , the choice of batch size is not very important in DP-SGD , as long as the learning rate is set adequately ) ."}, {"review_id": "YTWGvpFOQD--1", "review_text": "The paper shows that linear model on top of ScatterNet can outperform CNN for DPSGD training on a few generic image classification tasks . It analyzed the results , provides hypotheses to explain it , and concludes that more data / better feature is needed for DPSGD training . People have been searching for good models for DPSGD training , so it is nice to see a new baseline ( ScatterNet + linear model ) that is simple but performs better . This can be pretty valuable for researchers and practitioners in the field . The paper also did some quite interesting experiments to explain the advantage of ScatterNet + linear model and to suggest directions to improve DPSGD training . The experiments and discussions on the learning rate are quite interesting and inspiring to me . However , I feel like the results for having more data / transfer learning is not so surprising , though the experiments with models different from previous work are valuable . More detailed comments : - In Sec 4 `` smaller models are not easier to train privately '' , you mentioned that the CNN is smaller than the linear model , so dimensionality is not an explanation for ScatterNet + linear 's better performance . But I guess convex model ( or maybe shallow model ) might have some fundamental difference from nonconvex ( or maybe deeper model ) . Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between . - In Sec 4 `` Models with handcrafted features converge faster without privacy '' , I guess the results can be explained by the fact that simpler model ( linear ) has a lower capacity than more complicated model ( CNN ) so requires less training time even with lower learning rate . So maybe again it would worth trying something in between linear and CNN . - In Sec 5.2 , you showed results that are much better than previous results with transfer learning . It seems like the main difference is the model architecture . Is that the case ? Do you have any comment on that ? The presentation is clear in general . I would love to see more details about ScatterNet as that is the important component of the proposed method .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * However , I feel like the results for having more data / transfer learning is not so surprising , though the experiments with models different from previous work are valuable . * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data or with transfer learning , we provide a detailed response to these concerns in a meta-comment above . In summary , while the data tradeoff we show is indeed expected * qualitatively * , we are interested in a * quantitative * assessment : how much more private data is needed for private end-to-end deep learning to work ? Similarly , our experiments with transfer learning aim to rectify confusion in the literature about the quantitative limitations of DP in this setting , which seem largely due to prior work only evaluating private transfer learning with very weak public source models ( as the reviewer notes , the only difference in our approach is the use of a state-of-the-art source model.It is unclear to us why prior work has not followed this approach ) . * * 2 ) Q : * * * Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between . * * * A : * * We agree with the reviewer that the non-convexity or larger capacity of neural networks are likely a cause for their slower convergence ( with or without ScatterNet features ) . Following the reviewers ' great suggestion , we experimented with shallower CNN architectures on MNIST , Fashion-MNIST and CIFAR-10 , by varying the number of convolutional layers and dense layers . We observe no improvement for the end-to-end CNNs ( this is not very surprising , as the original CNNs we use were the result of an architecture search tailored to DP-SGD performed by Papernot et al . ) * * However , for the Scat+CNN model on CIFAR-10 , we do see some improvements with shallower architectures ! * * Specifically , our best Scat+CNN model slightly outperforms our best linear model on CIFAR-10 ( we get 69 % accuracy at eps=3 ) . This is exciting ! We have added these improved results to our submission , and we plan to perform a more thorough architecture search for ScatterNet models . For MNIST and Fashion-MNIST , we could not improve the Scat+CNN results further by varying the model depth . The architectures we use are already very shallow ( 2 convolutions followed by 2 dense layers ) and further reducing the model size reduces private and non-private accuracy alike . The main reason for this discrepancy between ( Fashion ) -MNIST and CIFAR-10 is that on the simpler MNIST tasks , the linear ScatterNet model achieves near optimal accuracy in the non-private setting . Therefore , deeper architectures have little room for improvement , and since they converge more slowly , they perform worse in the private setting . On CIFAR-10 however , the linear ScatterNet model achieves about 71 % non-private accuracy , which is far from state-of-the-art . Our shallow Scat+CNN model achieves about 74 % non-private accuracy , so even though it converges a bit slower , the extra base accuracy is sufficient to slightly outperform the linear model in the private setting . The end-to-end CNN is at the other end of this spectrum : it achieves over 80 % accuracy in the non-private setting , but converges too slowly to compete in the private setting . These results are included in our updated submission . Our main conclusion remains that handcrafted features are highly beneficial for private learning in low data regimes . * * 3 ) Q : * * * In Sec 5.2 , you showed results that are much better than previous results with transfer learning . It seems like the main difference is the model architecture . Is that the case ? Do you have any comment on that ? * * * A : * * This indeed seems to be the main difference ( e.g. , the model used by Papernot et al.achieves 75 % accuracy in the non-private setting ) . Our results thus show that as in the non-private case , the performance of private transfer learning is highly dependent on the choice of a good source model . A thorough analysis of differentially private transfer learning is out of scope of our paper , but our results provide strong baselines to inform such a study in the future . * * 4 ) Q : * * * I would love to see more details about ScatterNet * * * A : * * We do provide some details on ScatterNets in Appendix C.1 . Are there additional aspects of these networks that would be helpful for us to include there ?"}, {"review_id": "YTWGvpFOQD--2", "review_text": "This article is about a topical issue : performance degradation of of deep learning models trained with differential privacy ( DP ) . Clipping of the gradients and addition of the noise , required to obtain DP guarantees , blur the models such that for moderate privacy guarantees ( eps~7.0 ) CIFAR-10 test accuracy baseline is currently ~66 % ( Papernot et al. , 2020b ) . Lower bounds for this degradation have been shown theoretically ( e.g.Bassily et al. , 2014 ) , and there has recently been also work on circumventing this issue , see e.g.Kairouz , P. , Ribero , M. , Rush , K. and Thakurta , A. , 2020 . Dimension Independence in Unconstrained Private ERM via Adaptive Preconditioning . arXiv preprint arXiv:2008.06570 , Yingxue Zhou , Zhiwei Steven Wu , and Arindam Banerjee . Bypassing the ambient dimension : Private sgd with gradient subspace identification . arXiv preprint arXiv:2007.03813 , 2020 ( these references are not included in the paper ) . Although this paper does not introduce fundamentally anything new for DP learning ( DP-SGD + R\u00e9nyi DP accountant for obtaining eps , delta-guarantees are used ) , it does clearly beat the state-of-the-art for small epsilon values ( eps up to 3.0 ) for MNIST , Fashion-MNIST and CIFAR-10 . This is obtained by using so called Scattering Networks ( Oyallon and Mallat , 2015 ) , which have the property of converging very fast without privacy . This phenomenon is transferred to DP learning and thus high accuracies for shorter DP-SGD runs ( i.e.smaller epsilons ) are obtained . As expected , these 'handcrafted data-independent feature extractors ' of Scatter Networks can not beat CNN+DP-SGD when more private data is available , or when features can be extracted from public image data . All in all , although I think the gist of the paper is simply combining these handcrafted feature extractors ( ScatterNets ) and DP-SGD , it does improve the baseline for DP CIFAR-10 for small / moderate eps-values ( up to 3.0 ) and does provide new ideas / questions on how to improve DP learning ( e.g.by accelerated convergence ) also outside of image domain ( handcrafted feature extraction for non-vision tasks ) . The paper is very well written . A tiny remark : You write `` Gaussian noise of variance sigma^2 C^2 is added to the mean gradient . '' Notice that sigma^2 C^2 - noise is added to the summed gradients , and sigma^2 C^2 / B^2 to the mean .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * Lower bounds for this degradation have been shown theoretically ( e.g.Bassily et al. , 2014 ) , and there has recently been also work on circumventing this issue * * * A : * * Thank you for pointing us to the works of Kairouz et al.and Banerjee et al.We added a reference to these works in our paper . There has indeed been a lot of work on trying to improve the performance of DP-SGD ( e.g. , all the works listed in Table 1 ) . As our results show however , proposed improvements have typically been evaluated in sub-par settings , which makes it hard to convincingly argue which techniques will generalize to better models . For example , Banerjee et al.achieve 85 % accuracy on MNIST , which is far below ours and prior results . We hope that our work can provide simple and strong baselines for private learning , which can be used to assess future proposed improvements to DP-SGD . * * 2 ) Q : * * * As expected , these 'handcrafted data-independent feature extractors ' of Scatter Networks can not beat CNN+DP-SGD when more private data is available , or when features can be extracted from public image data * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data or with transfer learning , we provide a detailed response to these concerns in a meta-comment above . In summary , while the data tradeoff we show is indeed expected * qualitatively * , we are interested in a * quantitative * assessment : how much more private data is needed for private end-to-end deep learning to work ? Similarly , our experiments with transfer learning aim to rectify confusion in the literature about the quantitative limitations of DP in this setting , which seem largely due to prior work only evaluating private transfer learning with very weak public source models ."}, {"review_id": "YTWGvpFOQD--3", "review_text": "The paper considers ways of improving private versions of SGD in the context of image classification . The main finding is that providing `` hand crafted '' features can significantly improve the privacy/accuracy trade-off . In some cases , even a linear model built on top of such features ( like those produced by ScatterNet ) , can improve over differentially private SGD . A plausible explanation for this phenomenon is that extra features can reduce the number of iterations required in SGD , resulting in better privacy and/or less noise . ( It is also argued that having much more data similarly improves the trade-off , but this is unsurprising and , it seems , has been observed before by McMahan et al . ) The paper is quite well-written , and I found it easy to follow even though this is not my area of expertise . I also like that it presents a number of possible directions for further improving private SGD , including transfer learning from related , public data sets , and second-order optimization . A possible criticism is that in principle the `` hand crafted '' features may have been built based on empirical work on MNIST and CIFAR-10 , and the same goes for the architecture choices , so in theory there could be some privacy leakage from these choices . It would have been more impressive to demonstrate effectiveness of a newer data set , not known when ScatterNet and the used CNN architectures were proposed . Two final comments : - `` Unlearned '' usually means that you have ( deliberately ) forgotten something , so it is not the same as `` not learned '' . - It would be interesting to consider the setting where just the image * label * is private . Has DP SGD been considered in that setting ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * A possible criticism is that in principle the `` hand crafted '' features may have been built based on empirical work on MNIST and CIFAR-10 * * * A : * * The design of ScatterNet may indeed have been partially influenced by existing vision datasets . However , the core scattering transform originates from a purely theoretical work of Mallat . The architecture proposed by Oyallon and Mallat was indeed originally evaluated on CIFAR-10 , but not on MNIST or Fashion-MNIST . The latter two datasets are qualitatively very different from CIFAR-10 , so this speaks to the generality of the approach . Note that the Fashion-MNIST dataset ( 2017 ) was indeed proposed after the ScatterNet paper ( 2014 ) . We agree that the choice of CNN architectures might lead to some privacy leakage , but this only strengthens our main result that linear models with handcrafted features outperform deep neural nets for moderate privacy budgets . * * 2 ) Q : * * * It is also argued that having much more data similarly improves the trade-off , but this is unsurprising and , it seems , has been observed before by McMahan et al . * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data , we provide a detailed response to these concerns in a meta-comment above . In summary , while the tradeoff we show is indeed expected * qualitatively * , we are interested in a * quantitative * assessment : how much more private data is needed for private end-to-end deep learning to work ? * * 3 ) Q : * * * Two final comments : * * * A : * * - Yes , thank you for pointing out this misnomer about \u201c unlearning \u201d - The label-private setting is indeed very interesting to consider . We are unaware of any work in this direction . The standard DP-SGD algorithm does not seem well suited to exploit such a weaker privacy model . Indeed , DP-SGD provides privacy at the level of gradients , which depend on both the input and label ."}], "0": {"review_id": "YTWGvpFOQD--0", "review_text": "The paper presents an analysis of differential privacy in machine learning , with a focus on neural networks trained via differentially private stochastic gradient descent ( DPSGD ) . The main focus and the message in the paper is that the handcrafted features work better compared to learned features during training of NNs and having more training data results in better outcomes ( i.e.a better privacy-utility trade-off ) . Starting with the latter , this is apparent from the noise formulation in DPSGD , where the noise is reduced via sampling probability , which decreases as the data size grows . Hence , I do not consider this as a new insight or a contribution . Unless , I have misunderstood something , in which case , please do explain . For the former , as the final model used ( Table 3 and Figure 1 ) is a linear classifier , it outperforming an end-to-end CNN based model is intuitive , as it has far fewer number of parameters ( which improve the noise scale , due to smaller gradient norm ) . This is slightly touched upon in subsection `` Training CNNs on handcrafted features '' , where the comparison is made using CNNs on the handcrafted features , however there are no detailed results presented in the paper , I would have liked to see a similar table and figures as earlier . The presentation of results ( Table 3 ) is a bit strange . I would have further liked to see the comparison of both models on the * same * set of hyperparameters . Also , instead of stating that hyperparameter search 's privacy budget was not accounted for as in prior works , it would have been nice to see some analysis , such as the section D ( Appendix ) in Abadi et al .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * ... having more training data results in better outcomes [ ... ] I do not consider this as a new insight or a contribution . Unless , I have misunderstood something , in which case , please do explain . * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data , we provide a detailed response to these concerns in a meta-comment above . * In summary , while the tradeoff we show is indeed expected qualitatively , we are interested in a quantitative assessment : how much more private data is needed for private end-to-end deep learning to outperform handcrafted features ? * Our result shows that improving private deep learning simply by collecting more data might be very expensive for canonical vision tasks like CIFAR-10 : we need about one order of magnitude more data before the end-to-end CNN outperforms our handcrafted baselines . This motivates the design of better private learning algorithms in low data regimes . * * 2 ) Q : * * * as the final model used ( Table 3 and Figure 1 ) is a linear classifier , it outperforming an end-to-end CNN based model is intuitive , as it has far fewer number of parameters * * * A : * * * * The claim that our linear classifiers have fewer parameters than end-to-end CNNs is incorrect ! * * This is the point of our analysis in Section 4 : \u201c Smaller models are not easier to train privately. \u201d We find that ScatterNet models outperform end-to-end CNNs * despite having more trainable parameters ! * As the reviewer correctly notes , the noise analysis of DP-SGD would suggest the opposite , * * so this result is surprising . * * We have clarified this in our introduction . As we show in Section 4 , handcrafted features give rise to an \u201c easier \u201d learning task , where convergence occurs much faster * despite the higher noise . * * * 3 ) Q : * * * there are no detailed results presented in the paper , I would have liked to see a similar table and figures as earlier * * * A : * * It is unclear to us what the reviewer means by a lack of \u201c detailed results \u201d for CNNs trained on ScatterNet features . Figure 2 is analogous to the earlier Figure 1 and directly compares the ScatterNet+linear , ScatterNet+CNN and end-to-end CNN models for all privacy budgets we consider . We have also added results with ScatterNet+CNN models to Table 1 and Table 3 , which in particular includes some improved results for the ScatterNet+CNN model on CIFAR-10 , following a suggestion of reviewer 3 . Are there other results that we could add to the paper to better compare the ScatterNet and end-to-end results ? * * 4 ) Q : * * * The presentation of results ( Table 3 ) is a bit strange . I would have further liked to see the comparison of both models on the same set of hyperparameters * * * A : * * We are not sure what the reviewer means by a \u201c comparison of both models on the same set of hyperparameters \u201d . It seems to us that this is exactly what is shown in Figure 1 , which shows the best performance achieved by both linear ScattterNet models and end-to-end CNNs for each privacy budget , across the entire hyper-parameter set . If the reviewer meant comparing these models for a * single * assignment of hyper-parameters , this is somewhat tricky to do in a fair way . It is unclear how to choose those hyper-parameters so as not to favor one of the two models . We note that for CIFAR-10 this issue is moot anyhow : * the best hyper-parameter assignment for end-to-end CNNs is outperformed by the worst assignment of hyper-parameters for linear ScatterNet models ( Table 3 ) . * * * 4 ) Q : * * * instead of stating that hyper-parameter search 's privacy budget was not accounted for as in prior works , it would have been nice to see some analysis , such as the section D ( Appendix ) in Abadi et al . * * * A : * * We can definitely perform an analysis of the cost of hyper-parameter search as in Abadi et al.Ultimately though , none of the prior works that we compare against in Table 1 do this , so it is hard to perform a fair comparison that accounts for this cost . If we solely consider our comparison of end-to-end CNNs and ScatterNet models , then the hyper-parameter sets are identical so the analysis of Abadi et al.would yield the same cost in both cases . Moreover , as we show in Table 2 and Appendices D.1 and D.2 , our hyper-parameter search was anyhow somewhat \u201c excessive \u201d , especially for linear ScatterNets . We would obtain similar results by considering a much smaller set of parameters ( e.g. , we find that contrary to what was claimed in prior work , the choice of batch size is not very important in DP-SGD , as long as the learning rate is set adequately ) ."}, "1": {"review_id": "YTWGvpFOQD--1", "review_text": "The paper shows that linear model on top of ScatterNet can outperform CNN for DPSGD training on a few generic image classification tasks . It analyzed the results , provides hypotheses to explain it , and concludes that more data / better feature is needed for DPSGD training . People have been searching for good models for DPSGD training , so it is nice to see a new baseline ( ScatterNet + linear model ) that is simple but performs better . This can be pretty valuable for researchers and practitioners in the field . The paper also did some quite interesting experiments to explain the advantage of ScatterNet + linear model and to suggest directions to improve DPSGD training . The experiments and discussions on the learning rate are quite interesting and inspiring to me . However , I feel like the results for having more data / transfer learning is not so surprising , though the experiments with models different from previous work are valuable . More detailed comments : - In Sec 4 `` smaller models are not easier to train privately '' , you mentioned that the CNN is smaller than the linear model , so dimensionality is not an explanation for ScatterNet + linear 's better performance . But I guess convex model ( or maybe shallow model ) might have some fundamental difference from nonconvex ( or maybe deeper model ) . Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between . - In Sec 4 `` Models with handcrafted features converge faster without privacy '' , I guess the results can be explained by the fact that simpler model ( linear ) has a lower capacity than more complicated model ( CNN ) so requires less training time even with lower learning rate . So maybe again it would worth trying something in between linear and CNN . - In Sec 5.2 , you showed results that are much better than previous results with transfer learning . It seems like the main difference is the model architecture . Is that the case ? Do you have any comment on that ? The presentation is clear in general . I would love to see more details about ScatterNet as that is the important component of the proposed method .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * However , I feel like the results for having more data / transfer learning is not so surprising , though the experiments with models different from previous work are valuable . * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data or with transfer learning , we provide a detailed response to these concerns in a meta-comment above . In summary , while the data tradeoff we show is indeed expected * qualitatively * , we are interested in a * quantitative * assessment : how much more private data is needed for private end-to-end deep learning to work ? Similarly , our experiments with transfer learning aim to rectify confusion in the literature about the quantitative limitations of DP in this setting , which seem largely due to prior work only evaluating private transfer learning with very weak public source models ( as the reviewer notes , the only difference in our approach is the use of a state-of-the-art source model.It is unclear to us why prior work has not followed this approach ) . * * 2 ) Q : * * * Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between . * * * A : * * We agree with the reviewer that the non-convexity or larger capacity of neural networks are likely a cause for their slower convergence ( with or without ScatterNet features ) . Following the reviewers ' great suggestion , we experimented with shallower CNN architectures on MNIST , Fashion-MNIST and CIFAR-10 , by varying the number of convolutional layers and dense layers . We observe no improvement for the end-to-end CNNs ( this is not very surprising , as the original CNNs we use were the result of an architecture search tailored to DP-SGD performed by Papernot et al . ) * * However , for the Scat+CNN model on CIFAR-10 , we do see some improvements with shallower architectures ! * * Specifically , our best Scat+CNN model slightly outperforms our best linear model on CIFAR-10 ( we get 69 % accuracy at eps=3 ) . This is exciting ! We have added these improved results to our submission , and we plan to perform a more thorough architecture search for ScatterNet models . For MNIST and Fashion-MNIST , we could not improve the Scat+CNN results further by varying the model depth . The architectures we use are already very shallow ( 2 convolutions followed by 2 dense layers ) and further reducing the model size reduces private and non-private accuracy alike . The main reason for this discrepancy between ( Fashion ) -MNIST and CIFAR-10 is that on the simpler MNIST tasks , the linear ScatterNet model achieves near optimal accuracy in the non-private setting . Therefore , deeper architectures have little room for improvement , and since they converge more slowly , they perform worse in the private setting . On CIFAR-10 however , the linear ScatterNet model achieves about 71 % non-private accuracy , which is far from state-of-the-art . Our shallow Scat+CNN model achieves about 74 % non-private accuracy , so even though it converges a bit slower , the extra base accuracy is sufficient to slightly outperform the linear model in the private setting . The end-to-end CNN is at the other end of this spectrum : it achieves over 80 % accuracy in the non-private setting , but converges too slowly to compete in the private setting . These results are included in our updated submission . Our main conclusion remains that handcrafted features are highly beneficial for private learning in low data regimes . * * 3 ) Q : * * * In Sec 5.2 , you showed results that are much better than previous results with transfer learning . It seems like the main difference is the model architecture . Is that the case ? Do you have any comment on that ? * * * A : * * This indeed seems to be the main difference ( e.g. , the model used by Papernot et al.achieves 75 % accuracy in the non-private setting ) . Our results thus show that as in the non-private case , the performance of private transfer learning is highly dependent on the choice of a good source model . A thorough analysis of differentially private transfer learning is out of scope of our paper , but our results provide strong baselines to inform such a study in the future . * * 4 ) Q : * * * I would love to see more details about ScatterNet * * * A : * * We do provide some details on ScatterNets in Appendix C.1 . Are there additional aspects of these networks that would be helpful for us to include there ?"}, "2": {"review_id": "YTWGvpFOQD--2", "review_text": "This article is about a topical issue : performance degradation of of deep learning models trained with differential privacy ( DP ) . Clipping of the gradients and addition of the noise , required to obtain DP guarantees , blur the models such that for moderate privacy guarantees ( eps~7.0 ) CIFAR-10 test accuracy baseline is currently ~66 % ( Papernot et al. , 2020b ) . Lower bounds for this degradation have been shown theoretically ( e.g.Bassily et al. , 2014 ) , and there has recently been also work on circumventing this issue , see e.g.Kairouz , P. , Ribero , M. , Rush , K. and Thakurta , A. , 2020 . Dimension Independence in Unconstrained Private ERM via Adaptive Preconditioning . arXiv preprint arXiv:2008.06570 , Yingxue Zhou , Zhiwei Steven Wu , and Arindam Banerjee . Bypassing the ambient dimension : Private sgd with gradient subspace identification . arXiv preprint arXiv:2007.03813 , 2020 ( these references are not included in the paper ) . Although this paper does not introduce fundamentally anything new for DP learning ( DP-SGD + R\u00e9nyi DP accountant for obtaining eps , delta-guarantees are used ) , it does clearly beat the state-of-the-art for small epsilon values ( eps up to 3.0 ) for MNIST , Fashion-MNIST and CIFAR-10 . This is obtained by using so called Scattering Networks ( Oyallon and Mallat , 2015 ) , which have the property of converging very fast without privacy . This phenomenon is transferred to DP learning and thus high accuracies for shorter DP-SGD runs ( i.e.smaller epsilons ) are obtained . As expected , these 'handcrafted data-independent feature extractors ' of Scatter Networks can not beat CNN+DP-SGD when more private data is available , or when features can be extracted from public image data . All in all , although I think the gist of the paper is simply combining these handcrafted feature extractors ( ScatterNets ) and DP-SGD , it does improve the baseline for DP CIFAR-10 for small / moderate eps-values ( up to 3.0 ) and does provide new ideas / questions on how to improve DP learning ( e.g.by accelerated convergence ) also outside of image domain ( handcrafted feature extraction for non-vision tasks ) . The paper is very well written . A tiny remark : You write `` Gaussian noise of variance sigma^2 C^2 is added to the mean gradient . '' Notice that sigma^2 C^2 - noise is added to the summed gradients , and sigma^2 C^2 / B^2 to the mean .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * Lower bounds for this degradation have been shown theoretically ( e.g.Bassily et al. , 2014 ) , and there has recently been also work on circumventing this issue * * * A : * * Thank you for pointing us to the works of Kairouz et al.and Banerjee et al.We added a reference to these works in our paper . There has indeed been a lot of work on trying to improve the performance of DP-SGD ( e.g. , all the works listed in Table 1 ) . As our results show however , proposed improvements have typically been evaluated in sub-par settings , which makes it hard to convincingly argue which techniques will generalize to better models . For example , Banerjee et al.achieve 85 % accuracy on MNIST , which is far below ours and prior results . We hope that our work can provide simple and strong baselines for private learning , which can be used to assess future proposed improvements to DP-SGD . * * 2 ) Q : * * * As expected , these 'handcrafted data-independent feature extractors ' of Scatter Networks can not beat CNN+DP-SGD when more private data is available , or when features can be extracted from public image data * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data or with transfer learning , we provide a detailed response to these concerns in a meta-comment above . In summary , while the data tradeoff we show is indeed expected * qualitatively * , we are interested in a * quantitative * assessment : how much more private data is needed for private end-to-end deep learning to work ? Similarly , our experiments with transfer learning aim to rectify confusion in the literature about the quantitative limitations of DP in this setting , which seem largely due to prior work only evaluating private transfer learning with very weak public source models ."}, "3": {"review_id": "YTWGvpFOQD--3", "review_text": "The paper considers ways of improving private versions of SGD in the context of image classification . The main finding is that providing `` hand crafted '' features can significantly improve the privacy/accuracy trade-off . In some cases , even a linear model built on top of such features ( like those produced by ScatterNet ) , can improve over differentially private SGD . A plausible explanation for this phenomenon is that extra features can reduce the number of iterations required in SGD , resulting in better privacy and/or less noise . ( It is also argued that having much more data similarly improves the trade-off , but this is unsurprising and , it seems , has been observed before by McMahan et al . ) The paper is quite well-written , and I found it easy to follow even though this is not my area of expertise . I also like that it presents a number of possible directions for further improving private SGD , including transfer learning from related , public data sets , and second-order optimization . A possible criticism is that in principle the `` hand crafted '' features may have been built based on empirical work on MNIST and CIFAR-10 , and the same goes for the architecture choices , so in theory there could be some privacy leakage from these choices . It would have been more impressive to demonstrate effectiveness of a newer data set , not known when ScatterNet and the used CNN architectures were proposed . Two final comments : - `` Unlearned '' usually means that you have ( deliberately ) forgotten something , so it is not the same as `` not learned '' . - It would be interesting to consider the setting where just the image * label * is private . Has DP SGD been considered in that setting ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful comments . * * 1 ) Q : * * * A possible criticism is that in principle the `` hand crafted '' features may have been built based on empirical work on MNIST and CIFAR-10 * * * A : * * The design of ScatterNet may indeed have been partially influenced by existing vision datasets . However , the core scattering transform originates from a purely theoretical work of Mallat . The architecture proposed by Oyallon and Mallat was indeed originally evaluated on CIFAR-10 , but not on MNIST or Fashion-MNIST . The latter two datasets are qualitatively very different from CIFAR-10 , so this speaks to the generality of the approach . Note that the Fashion-MNIST dataset ( 2017 ) was indeed proposed after the ScatterNet paper ( 2014 ) . We agree that the choice of CNN architectures might lead to some privacy leakage , but this only strengthens our main result that linear models with handcrafted features outperform deep neural nets for moderate privacy budgets . * * 2 ) Q : * * * It is also argued that having much more data similarly improves the trade-off , but this is unsurprising and , it seems , has been observed before by McMahan et al . * * * A : * * As many reviewers raised a similar concern about expectedness of our results when training with more data , we provide a detailed response to these concerns in a meta-comment above . In summary , while the tradeoff we show is indeed expected * qualitatively * , we are interested in a * quantitative * assessment : how much more private data is needed for private end-to-end deep learning to work ? * * 3 ) Q : * * * Two final comments : * * * A : * * - Yes , thank you for pointing out this misnomer about \u201c unlearning \u201d - The label-private setting is indeed very interesting to consider . We are unaware of any work in this direction . The standard DP-SGD algorithm does not seem well suited to exploit such a weaker privacy model . Indeed , DP-SGD provides privacy at the level of gradients , which depend on both the input and label ."}}