{"year": "2019", "forum": "rJl-HsR9KX", "title": "Discriminative Active Learning", "decision": "Reject", "meta_review": "This paper proposes a novel and interesting active learning approach, that  trains a classifier to discriminate between the examples in the labeled and unlabeled data at each iteration. The top few samples that are most likely to be from the unlabeled set as per this classifier are selected to be labeled by an oracle, and are moved to the labeled training examples bin in the next iteration. The idea is simple and clear and is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. Experiments performed on CIFAR-10 and MNIST benchmarks demonstrate good results in comparison to baselines. \nDuring the review period, authors considered most of the suggestions by the reviewers and updated the paper. Although the proposed method is similar to density-based active learning methods, as also suggested by the reviewers, baselines do not include such approaches in the comparison experiments.", "reviews": [{"review_id": "rJl-HsR9KX-0", "review_text": "The paper is proposing a distribution matching as a metric for active learning. Basic intuition is: if we can make the distribution of labelled and unlabelled examples similar to each other, training error in one will approximate the training error in the other. Hence, a model learned using labelled ones will do well in unlabelled ones. The main tool to enforce this distributional distance is using adversarial learning similar to GANs or gradient reversal network for domain adaptation. The idea is definitely interesting. I am not sure about why should it work (I explained in detail later), but it does work well empirically. Moreover, it is very easy to implement. Given any learned or hard-coded features, learning a simple binary classifier is sufficient to implement the method. The mini-queries idea in 4.1 is especially interesting. Handling large batches in active learning is always a problem but this neat trick make it much easier. I think the proposed method is counter intuitive as the discussion does not explain why should it work better than random sampling. Clearly if labelled samples are randomly sampled, labelled and unlabelled data is coming from the exactly same distribution. Hence, the distance (H-divergence, TV-distance etc.) between them is 0. My main question to authors is why does this method work better than random sampling? A similar question is; since they are coming from the exact same distribution, what is the meaning of minimizing empirical H-divergence? I think a more detailed study on a toy problem could potentially explain this. Authors can generate 1-D or 2D samples from a well defined distribution (eg. Gaussians with different means/variances for each class) and visualize what is the algorithm actually doing. Considering my point that these data points are actually coming from the same distribution, discussion in Section 3.1 is rather unjustified. Most of the entities discussed in that section are probabilistic entities (generally speaking expected values) and does not differ between labelled and unlabelled case since they have same underlying distribution. Their empirical values are different but this is beyond the study of Ben-David(2010). Therefore, I am not sure does the Section 3.1 is contributing to the paper without any explicit connection to the empirical divergence minimization. More importantly a much similar work from domain adaptation is [Unsupervised domain adaptation by backpropagation, ICML 2015] and it should also be discussed in the paper. Some minor issues: - Are the hyper-parameters kept fixed for all experiments. In other words, does the training size of 5k and 15k share hyperparameters? Which might be sub-optimal. - The experiments use very large batch sizes. A smaller batch sizes might separate the algorithms better. - References in the text have some issues. There are missing commas between references in the text. There are also some cases where \\citep should have been used but \\citet is used. A careful pass over them might be beneficial. In summary, I think the paper is interesting, easy to implement and possibly useful to the large part of the community since active learning is very important problem. I think the major weakness of the paper is the fact that authors did not give a clear explain why does it actually work. I think it is crucial for authors to provide a theoretical or an empirical study which answer this question. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful comments regarding our paper . We 'll first address your comments about using the H-divergence as a motivation for our method and the expectation that DAL should n't improve upon random sampling . We agree that when considering the underlying distributions of the labeled and unlabeled sets , the TV distance between them is zero and that random sampling allows claiming that they are indeed the same distribution . That being said , we work under the assumption that the labeled set is very small compared to the unlabeled pool , and so it can be viewed as coming from a different , empirical distribution ( for instance , one which contains only some of the modes of the original distribution ) . When represented in this way , for a fixed labeled set , we claim that we have a distribution shift that we must correct for using active learning . We could either sample an example randomly from the real distribution , which will give us in expectation a good solution ( assuming we sample enough examples ) , or actively choose the examples which minimize this distribution drift which is caused by the small size of the labeled set . So , while in expectation over large labeled sets we should expect the domain shift to be zero , in essence when we are dealing with small labeled sets it is more accurate to treat the distributions as different and attempt to minimize their distance . This is mentioned shortly in the second paragraph of section 3.1 . A simple example to motivate our reasoning can be to look at a finite sample space with n possible values for x , and P ( x ) being some multinomial distribution over the values of x . We also have an estimate of the probabilities , P * ( x ) , which comes from the relative frequency of every x in out labeled set . These two distributions are different and we want to make them more similar . Our method will choose the sample to label which maximizes P ( x ) / ( P ( x ) +P * ( x ) ) , which is the one where P ( x ) /P * ( x ) is largest . This is a good heuristic as opposed to picking randomly , which might even push the distributions farther apart . We can also motivate this in a similar way to the Core-Set approach , in which we care about representing all of the modes of the distribution ( having all possible examples of x in our labeled set ) . If we use DAL for the above example , assuming P ( x ) is non zero for every x , then we will represent all of the modes after n samples . Contrary to DAL , random sampling will capture all of the modes in ~nlog ( n ) samples , which is a big difference if n is large and we want to have as few labeled samples as we can . Finally , we did run experiments on toy data when trying to compare our method to the Core-Set approach . We used two Gaussians , one having a large variance and the other having a smaller one . While the Core-Set approach favored labeling samples from the large variance Gaussian , DAL labeled samples more equally from the two Gaussians . We felt this experiment , along with other ones we ran on low dimensional data , did n't add enough to justify getting into the final paper under the ICLR page constraints , and chose to let the experiments on more realistic data be the main message . As for your comment about the similar work from domain adaptation , we thank you for bringing this paper to our attention . We discuss it briefly in section 3.1.1 of the revised submission . As for the minor issues you raise : The hyper-parameters are indeed kept fixed , and we agree this could be sub-optimal . Using cross validation in every iteration of the active learning experiment for every AL method for the amount of experiments we ran was computationally too expensive for us . Still , since all algorithms were tested in the same playing field with the same parameters , we think our results can be trusted . The batch sizes were indeed quite large , as we are addressing active learning for neural networks where using small batch sizes is often impractical since we are trying to amass a relatively large dataset . We agree that in the domain of small batch sizes there could be stronger differences between the methods , specifically a bigger advantage to uncertainty based methods compared to DAL and the Core-Set approach ."}, {"review_id": "rJl-HsR9KX-1", "review_text": "Thank you for this enjoyable paper. Summary: The authors propose a novel approach to active learning as follows. At each iteration they develop a classifier that can discriminate between the samples in the labeled and unlabeled sets; they select the top few samples that are most likely to be from the unlabeled set as per this classifier, and request the oracle to provide labels for this batch next. This simple idea is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. They provide clear algorithms and open source code for easy verification, and public testing. They provide good experimental verification on CIFAR-10 and MNIST benchmarks. I personally look at new papers more for novel ideas and good intuition/theoretical justification than an immediate improvement in benchmark results, so I enjoyed this paper thoroughly. Results: Among other things they show that their algorithms ranks the samples to be next labeled quite differently than uncertainty sampling based approaches; that their method is at least as accurate/sample-efficient as the state of the art ; and that some previously published experimental results are incorrect(!). As the authors will probably agree I am not convinced the proposed method is better than previous algorithms in any statistically significant way, but the novel idea itself is worth publishing even if it is just as good as the state of the art. Novelty: I liked the paper very much because it provides quite an innovative new approach to look at active learning, which resembles GANs and Core set ideas in some ways, yet differs in significant ways that are critical for active learning. I've been working and publishing in related areas for a long time so I genuinely found your central idea refreshing and new. Relevance: The paper is very relevant to the ICLR community and addresses critical questions. Question: My intuition as a Bayesian is that we most need to find labels that maximize the mutual information I(y,w) where w are the weights of the neural net. In practice this corresponds to the samples x which have the maximum class uncertainty, but for which the parameters under the posterior disagree about the outcome the most, eg see discussion below equation 2 for Bayesian Active Learning by Disagreement (BALD) in this paper https://arxiv.org/pdf/1112.5745.pdf . In essence: The above means that the labels that provide most information about the classification model are most valuable for active learning. However, your approach intuitively ignores the conditional distribution(ie py(|x)), and instead tries to make the original unconditional distribution p(x) between the labeled and unlabeled sets similar. Yet, it works beautifully. So: Why does this work? What is the intuition? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate the time and effort of reviewing our paper , and thank the reviewer for the kind words . As for an intuition for why our method should work while ignoring the conditional probability P ( y|x ) , we would not view the problem we are trying to solve as one which is tailored only to classification , and so we do not need access to the probability over the labels . As you say , we are trying to find a subset of the distribution which represents the true distribution as much as possible . Under the assumption that the distribution of labels is strongly dependent on the inputs ( a reasonable assumption in our view ) , then getting a labeled set which correctly captures P ( x ) should reasonably capture P ( x , y ) as well , which is what we need for classification . A similar notion can be found in the Core-Set approach , where Lipschitz assumptions on the labeling function allows for a bound on the test loss of the classifier , and so the objective becomes to cover the representation of the data ( which ignores the labels as well ) . We would also note that while we do ignore P ( y|x ) in DAL , there is still information from the labels that is used during the entire active learning process , since we are running DAL on a representation learned by a classifier that uses the labels in the labeled set . Having said all of that , it is quite reasonable to think that a method which combines DAL with other methods which do use the label information could improve on our results ."}, {"review_id": "rJl-HsR9KX-2", "review_text": "This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled. The paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting. However, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? Besides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational. Next, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable. Finally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. Questions: - Could you elaborate why DAL strategy does not end up doing just random sampling? - Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers? - How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case? - How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible? Small comments: - I think in many cases citep command should be used instead of cite. - Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network? - Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)? - It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed. - I am not sure \"discriminative\" is a good name for this algorithm. It suggested that is it opposite to \"generative\" (query synthesis?), but then all AL that rank datapoints with some scoring function are \"discriminative\".", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed comments and questions regarding our paper and the time and effort spent reviewing it . You are correct in saying that DAL in essence chooses the under-represented regions of the data and acts similarly to a density based approach ( this can be seen in experiments on toy data as well ) . Indeed , for low-dimensional data there is no reason to choose to use DAL over classical density-based approaches which directly estimate the density . DAL is relevant for high dimensional data , where density-based approaches break down due to the difficulty of estimating probability densities in high dimensions . While estimating densities is difficult , we have great tools for binary classification in high dimensional data and so DAL remains effective in these domains . We briefly mention this in the first paragraph of section 3 . As for having DAL be supplementary to uncertainty based methods , we agree that such a combination could be beneficial and improve the results of both DAL and uncertainty separately , but combining the methods in a non-naive way is n't straightforward and was outside the scope of the current work . Regarding the use of information about the labels , we would only comment that the labels are n't completely ignored in the entire active learning process of DAL . While it is true that DAL only uses the representation to pick the next sample to label , the representation itself is learned using a classifier which takes the labels of the labeled set into account . This is important , since running DAL on the raw data gives results which are much worse . The same can be said regarding the Core-Set approach , where it ignores the labels of the representation for choosing the examples to label , but uses a representation learned using the labels . As for the experimental results , the point is well taken that the results are n't convincing enough to choose DAL over the other methods in practice . Even worse , our results show that for the benchmarks of MNIST and CIFAR-10 , there is n't enough information for a practitioner to decide about any of the existing methods . This speaks to a general issue in benchmarking modern AL methods , which we admittedly did not address in our paper fully by exploring other , larger datasets . Exploring the problem of benchmarking AL methods is important , but tangential to a paper that aims to introduce a new method . Still , we feel that our method is interesting and has merit even if it only performs comparably to the existing methods and does not improve upon them ."}], "0": {"review_id": "rJl-HsR9KX-0", "review_text": "The paper is proposing a distribution matching as a metric for active learning. Basic intuition is: if we can make the distribution of labelled and unlabelled examples similar to each other, training error in one will approximate the training error in the other. Hence, a model learned using labelled ones will do well in unlabelled ones. The main tool to enforce this distributional distance is using adversarial learning similar to GANs or gradient reversal network for domain adaptation. The idea is definitely interesting. I am not sure about why should it work (I explained in detail later), but it does work well empirically. Moreover, it is very easy to implement. Given any learned or hard-coded features, learning a simple binary classifier is sufficient to implement the method. The mini-queries idea in 4.1 is especially interesting. Handling large batches in active learning is always a problem but this neat trick make it much easier. I think the proposed method is counter intuitive as the discussion does not explain why should it work better than random sampling. Clearly if labelled samples are randomly sampled, labelled and unlabelled data is coming from the exactly same distribution. Hence, the distance (H-divergence, TV-distance etc.) between them is 0. My main question to authors is why does this method work better than random sampling? A similar question is; since they are coming from the exact same distribution, what is the meaning of minimizing empirical H-divergence? I think a more detailed study on a toy problem could potentially explain this. Authors can generate 1-D or 2D samples from a well defined distribution (eg. Gaussians with different means/variances for each class) and visualize what is the algorithm actually doing. Considering my point that these data points are actually coming from the same distribution, discussion in Section 3.1 is rather unjustified. Most of the entities discussed in that section are probabilistic entities (generally speaking expected values) and does not differ between labelled and unlabelled case since they have same underlying distribution. Their empirical values are different but this is beyond the study of Ben-David(2010). Therefore, I am not sure does the Section 3.1 is contributing to the paper without any explicit connection to the empirical divergence minimization. More importantly a much similar work from domain adaptation is [Unsupervised domain adaptation by backpropagation, ICML 2015] and it should also be discussed in the paper. Some minor issues: - Are the hyper-parameters kept fixed for all experiments. In other words, does the training size of 5k and 15k share hyperparameters? Which might be sub-optimal. - The experiments use very large batch sizes. A smaller batch sizes might separate the algorithms better. - References in the text have some issues. There are missing commas between references in the text. There are also some cases where \\citep should have been used but \\citet is used. A careful pass over them might be beneficial. In summary, I think the paper is interesting, easy to implement and possibly useful to the large part of the community since active learning is very important problem. I think the major weakness of the paper is the fact that authors did not give a clear explain why does it actually work. I think it is crucial for authors to provide a theoretical or an empirical study which answer this question. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful comments regarding our paper . We 'll first address your comments about using the H-divergence as a motivation for our method and the expectation that DAL should n't improve upon random sampling . We agree that when considering the underlying distributions of the labeled and unlabeled sets , the TV distance between them is zero and that random sampling allows claiming that they are indeed the same distribution . That being said , we work under the assumption that the labeled set is very small compared to the unlabeled pool , and so it can be viewed as coming from a different , empirical distribution ( for instance , one which contains only some of the modes of the original distribution ) . When represented in this way , for a fixed labeled set , we claim that we have a distribution shift that we must correct for using active learning . We could either sample an example randomly from the real distribution , which will give us in expectation a good solution ( assuming we sample enough examples ) , or actively choose the examples which minimize this distribution drift which is caused by the small size of the labeled set . So , while in expectation over large labeled sets we should expect the domain shift to be zero , in essence when we are dealing with small labeled sets it is more accurate to treat the distributions as different and attempt to minimize their distance . This is mentioned shortly in the second paragraph of section 3.1 . A simple example to motivate our reasoning can be to look at a finite sample space with n possible values for x , and P ( x ) being some multinomial distribution over the values of x . We also have an estimate of the probabilities , P * ( x ) , which comes from the relative frequency of every x in out labeled set . These two distributions are different and we want to make them more similar . Our method will choose the sample to label which maximizes P ( x ) / ( P ( x ) +P * ( x ) ) , which is the one where P ( x ) /P * ( x ) is largest . This is a good heuristic as opposed to picking randomly , which might even push the distributions farther apart . We can also motivate this in a similar way to the Core-Set approach , in which we care about representing all of the modes of the distribution ( having all possible examples of x in our labeled set ) . If we use DAL for the above example , assuming P ( x ) is non zero for every x , then we will represent all of the modes after n samples . Contrary to DAL , random sampling will capture all of the modes in ~nlog ( n ) samples , which is a big difference if n is large and we want to have as few labeled samples as we can . Finally , we did run experiments on toy data when trying to compare our method to the Core-Set approach . We used two Gaussians , one having a large variance and the other having a smaller one . While the Core-Set approach favored labeling samples from the large variance Gaussian , DAL labeled samples more equally from the two Gaussians . We felt this experiment , along with other ones we ran on low dimensional data , did n't add enough to justify getting into the final paper under the ICLR page constraints , and chose to let the experiments on more realistic data be the main message . As for your comment about the similar work from domain adaptation , we thank you for bringing this paper to our attention . We discuss it briefly in section 3.1.1 of the revised submission . As for the minor issues you raise : The hyper-parameters are indeed kept fixed , and we agree this could be sub-optimal . Using cross validation in every iteration of the active learning experiment for every AL method for the amount of experiments we ran was computationally too expensive for us . Still , since all algorithms were tested in the same playing field with the same parameters , we think our results can be trusted . The batch sizes were indeed quite large , as we are addressing active learning for neural networks where using small batch sizes is often impractical since we are trying to amass a relatively large dataset . We agree that in the domain of small batch sizes there could be stronger differences between the methods , specifically a bigger advantage to uncertainty based methods compared to DAL and the Core-Set approach ."}, "1": {"review_id": "rJl-HsR9KX-1", "review_text": "Thank you for this enjoyable paper. Summary: The authors propose a novel approach to active learning as follows. At each iteration they develop a classifier that can discriminate between the samples in the labeled and unlabeled sets; they select the top few samples that are most likely to be from the unlabeled set as per this classifier, and request the oracle to provide labels for this batch next. This simple idea is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. They provide clear algorithms and open source code for easy verification, and public testing. They provide good experimental verification on CIFAR-10 and MNIST benchmarks. I personally look at new papers more for novel ideas and good intuition/theoretical justification than an immediate improvement in benchmark results, so I enjoyed this paper thoroughly. Results: Among other things they show that their algorithms ranks the samples to be next labeled quite differently than uncertainty sampling based approaches; that their method is at least as accurate/sample-efficient as the state of the art ; and that some previously published experimental results are incorrect(!). As the authors will probably agree I am not convinced the proposed method is better than previous algorithms in any statistically significant way, but the novel idea itself is worth publishing even if it is just as good as the state of the art. Novelty: I liked the paper very much because it provides quite an innovative new approach to look at active learning, which resembles GANs and Core set ideas in some ways, yet differs in significant ways that are critical for active learning. I've been working and publishing in related areas for a long time so I genuinely found your central idea refreshing and new. Relevance: The paper is very relevant to the ICLR community and addresses critical questions. Question: My intuition as a Bayesian is that we most need to find labels that maximize the mutual information I(y,w) where w are the weights of the neural net. In practice this corresponds to the samples x which have the maximum class uncertainty, but for which the parameters under the posterior disagree about the outcome the most, eg see discussion below equation 2 for Bayesian Active Learning by Disagreement (BALD) in this paper https://arxiv.org/pdf/1112.5745.pdf . In essence: The above means that the labels that provide most information about the classification model are most valuable for active learning. However, your approach intuitively ignores the conditional distribution(ie py(|x)), and instead tries to make the original unconditional distribution p(x) between the labeled and unlabeled sets similar. Yet, it works beautifully. So: Why does this work? What is the intuition? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate the time and effort of reviewing our paper , and thank the reviewer for the kind words . As for an intuition for why our method should work while ignoring the conditional probability P ( y|x ) , we would not view the problem we are trying to solve as one which is tailored only to classification , and so we do not need access to the probability over the labels . As you say , we are trying to find a subset of the distribution which represents the true distribution as much as possible . Under the assumption that the distribution of labels is strongly dependent on the inputs ( a reasonable assumption in our view ) , then getting a labeled set which correctly captures P ( x ) should reasonably capture P ( x , y ) as well , which is what we need for classification . A similar notion can be found in the Core-Set approach , where Lipschitz assumptions on the labeling function allows for a bound on the test loss of the classifier , and so the objective becomes to cover the representation of the data ( which ignores the labels as well ) . We would also note that while we do ignore P ( y|x ) in DAL , there is still information from the labels that is used during the entire active learning process , since we are running DAL on a representation learned by a classifier that uses the labels in the labeled set . Having said all of that , it is quite reasonable to think that a method which combines DAL with other methods which do use the label information could improve on our results ."}, "2": {"review_id": "rJl-HsR9KX-2", "review_text": "This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled. The paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting. However, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? Besides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational. Next, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable. Finally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. Questions: - Could you elaborate why DAL strategy does not end up doing just random sampling? - Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers? - How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case? - How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible? Small comments: - I think in many cases citep command should be used instead of cite. - Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network? - Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)? - It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed. - I am not sure \"discriminative\" is a good name for this algorithm. It suggested that is it opposite to \"generative\" (query synthesis?), but then all AL that rank datapoints with some scoring function are \"discriminative\".", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed comments and questions regarding our paper and the time and effort spent reviewing it . You are correct in saying that DAL in essence chooses the under-represented regions of the data and acts similarly to a density based approach ( this can be seen in experiments on toy data as well ) . Indeed , for low-dimensional data there is no reason to choose to use DAL over classical density-based approaches which directly estimate the density . DAL is relevant for high dimensional data , where density-based approaches break down due to the difficulty of estimating probability densities in high dimensions . While estimating densities is difficult , we have great tools for binary classification in high dimensional data and so DAL remains effective in these domains . We briefly mention this in the first paragraph of section 3 . As for having DAL be supplementary to uncertainty based methods , we agree that such a combination could be beneficial and improve the results of both DAL and uncertainty separately , but combining the methods in a non-naive way is n't straightforward and was outside the scope of the current work . Regarding the use of information about the labels , we would only comment that the labels are n't completely ignored in the entire active learning process of DAL . While it is true that DAL only uses the representation to pick the next sample to label , the representation itself is learned using a classifier which takes the labels of the labeled set into account . This is important , since running DAL on the raw data gives results which are much worse . The same can be said regarding the Core-Set approach , where it ignores the labels of the representation for choosing the examples to label , but uses a representation learned using the labels . As for the experimental results , the point is well taken that the results are n't convincing enough to choose DAL over the other methods in practice . Even worse , our results show that for the benchmarks of MNIST and CIFAR-10 , there is n't enough information for a practitioner to decide about any of the existing methods . This speaks to a general issue in benchmarking modern AL methods , which we admittedly did not address in our paper fully by exploring other , larger datasets . Exploring the problem of benchmarking AL methods is important , but tangential to a paper that aims to introduce a new method . Still , we feel that our method is interesting and has merit even if it only performs comparably to the existing methods and does not improve upon them ."}}