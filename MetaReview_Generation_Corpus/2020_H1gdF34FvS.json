{"year": "2020", "forum": "H1gdF34FvS", "title": "Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning", "decision": "Reject", "meta_review": "This paper caused a lot of discussions before and after the rebuttal. The concerns are related to the novelty of this paper, which seems to be relatively limited. Since we do not have a champion among positive reviewers, and the overall score is not high enough, I cannot recommend its acceptance at this stage.\n", "reviews": [{"review_id": "H1gdF34FvS-0", "review_text": "[Note: I wrote this review after John Schulman's first comment, before any reply, and before Gehrard Neumann's comment] The authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm. I basically subscribe to John Schulman's comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature. The paper does not cite or discuss the one below, though it looks VERY close: @inproceedings{neumann2009fitted, title={Fitted Q-iteration by advantage weighted regression}, author={Neumann, Gerhard and Peters, Jan R}, booktitle={Advances in neural information processing systems}, pages={1177--1184}, year={2009} } This paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper. Less importantly, the authors may also want to have a look at : @article{zimmer2019exploiting, title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains}, author={Zimmer, Matthieu and Weng, Paul}, journal={arXiv preprint arXiv:1906.04556}, year={2019} } which also uses ideas along the same line. To me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function. ", "rating": "6: Weak Accept", "reply_text": "Thank you for the feedback , we will aim to conduct additional experiments to address your questions . But first , could you clarify what you mean by the stability of the value function ? Value functions are very commonly used for actor-critic algorithms , such as policy gradient methods , while Q-functions are more commonly used for off-policy methods . To the best of our knowledge , neither one is necessarily easier to learn than the other ."}, {"review_id": "H1gdF34FvS-1", "review_text": "This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. -- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. -- The proposed method doesn't outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough. -- There are significant overlaps between this paper, \"Fitted Q-iteration by Advantage Weighted Regression\", \"Model-Free Preference-Based Reinforcement Learning \", and \"Reinforcement learning by reward-weighted regression for operational space control\" which makes the contribution of this paper very incremental. -- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds. Questions: 1) Shouldn't be an importance sampling ratio between \\pi and \\mu in the equations? starting from eq.5. 2) Does the algorithm optimize the respect to $w$ as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since $d(s)$ is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. \\hat{V} = \\sum(w_i V_i), wouldn't be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3) 3) Is this the same code used to report results in this paper? if yes, I didn't see any seed assignment in the code?! and what is \"action_std\" in the code? There are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them. ", "rating": "3: Weak Reject", "reply_text": "Initial response to R1 Thank you for your feedback , we will aim to run the following additional experiments to address your questions : 1 ) We will perform experiments that directly compare AWR with REPS and FQI by AWR [ Neumann & Peters ] . 2 ) We will include additional tasks with discrete action spaces such as Cartpole-v1 , which is more commonly used . Please let us know if there are any additional experiments that you would find helpful . Re : Fitted Q-iteration by Advantage Weighted Regression Thank you for the pointer , We have revised the paper to include a discussion of \u201c Fitted Q-iteration by Advantage Weighted Regression \u201d [ Neumann & Peters ] in Section 4 ( highlighted in red ) . Neumann & Peters proposed a kernel-based fitted Q-iteration algorithm . Though their method also uses exponentiated advantages as weights , their definition of the policy is different from ours : https : //imgur.com/a/0DuRH7o The key difference is that in our method , the likelihood of an action is determined by both the likelihood of the sampling policy \\mu and the exponentiated advantage , while the policy in Neumann & Peters depends only on the advantage . Therefore , the policy update from Neumann & Peters does not enforce a trust region penalty that ensures the new policy is similar to the sampling policy , which is crucial for obtaining a good estimate of the objective using off-policy data collected from the sampling policy . Our method is simpler , does not perform fitted Q-iteration , and incorporates experience for off-policy learning . Furthermore , we provide a principled derivation of the advantage weights from a conservative policy improvement perspective . We further extend this analysis to AWR with experience replay and demonstrate its effectiveness for batch RL , both of which were not presented in these prior works . `` Model-Free Preference-Based Reinforcement Learning '' uses REPS as its learning algorithm , and a discussion of REPS is available in Section 4 . We will perform additional experiments to directly compare AWR with REPS and FQI by AWR . Re : Not outperforming previous methods While AWR does not outperform previous methods on all tasks , which we acknowledge in the paper , we would like to emphasize that our goal is to show that a simple off-policy method that uses supervised regression as subroutines can in fact be competitive with a number of current state-of-the-art algorithms . We believe that simple and effective RL algorithms are of interest to the ICLR community , and beating all state-of-the-art methods is not a prerequisite . We also demonstrate AWR \u2019 s effectiveness in the fully off-policy batch RL setting , where previous methods such DDPG and SAC perform poorly . Furthermore , AWR does not require the additional complexity of batch RL methods such as BCQ and BEAR . Re : Benchmarks We will also include additional tasks with discrete action spaces such as Cartpole-v1 , which is a more commonly used benchmark . Note , our focus is primarily on continuous control tasks , and the addition of the discrete tasks is mainly to show that AWR can also be easily applied to discrete actions . The motivation for the tasks in section 5.3 is to show that AWR is also effective for controlling complex agents with larger numbers of degrees-of-freedom . Since the agents in standard benchmark tasks are relatively simple , we opt to use new environments to better demonstrate this capability . Re : additional random seeds We have ran additional experiments for AWR with 10 different seeds . Here are learning curves comparing the results with 5 seeds and 10 seeds . The results are similar . We will update the paper to include results with 10 seeds . https : //imgur.com/a/yLntOzf Re : Q1 Importance sampling in Equation 5 Importance sampling ( IS ) is not required in Equation 5 . The objective itself does not require IS . For policy gradient methods like TRPO and PPO , IS is used to estimate the policy gradient from samples . In the AWR formulation , the solution of the Lagrangian in Equation 8 yields an update that does not require importance sampling , but it does require that \\mu and \\pi are similar , which is enforced by the trust region constraint ( Equation 6 ) . Re : Q2 Optimize w and uniform d ( s ) The algorithm does not optimize with respect to w. w_i represents the probability of selecting samples from a particular policy \\pi_i from the replay buffer , which is a constant . d ( s ) is also not a uniform distribution over states , it represents the state-marginal of the sampling distribution . In our implementation , we sample uniformly from the replay buffer , which does not result in a uniform d ( s ) , since some states might be visited more frequently than others . Re : code This is the same code used in the experiments . We do not manually assign random seeds to each run , instead we use python \u2019 s default random seed initialization , which assigns a different random seed to each execution of the problem . \u201c action_std \u201d specifies the standard deviation of the gaussian action distribution ."}, {"review_id": "H1gdF34FvS-2", "review_text": " # Summary The paper shows that good old reward weighted regression (RWR) with value-function baseline is still state-of-the-art algorithm. # Decision The paper is well-written and provides many evaluations. The contribution should be articulated more carefully, though, taking into account that most algorithmic ideas are present in prior work (https://openreview.net/forum?id=H1gdF34FvS&noteId=Bkxi11nsdr). Perhaps, the experience replay part is somewhat novel. It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea. Provided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution. ", "rating": "6: Weak Accept", "reply_text": "Thank you for the feedback , we will improve the writing to more clearly articulate the contribution of this work and include a more thorough discussion to contrast our method with related techniques . We have revised the paper to include a discussion of these prior works at the end of Section 4 ( highlighted in red ) . We will also perform experiments that directly compare AWR with REPS and FQI by AWR [ Neumann & Peters ] . While exponentiated-advantage weights have been used in a number of prior work , we present a principled derivation of AWR from a conservative policy improvement perspective , and also provide an analysis of AWR when combined with experience replay . We show that a number of simple design decisions , such as the use of TD-lambda and experience replay , enables AWR to achieve competitive performance with state-of-the-art algorithms both for RL and batch RL settings ."}], "0": {"review_id": "H1gdF34FvS-0", "review_text": "[Note: I wrote this review after John Schulman's first comment, before any reply, and before Gehrard Neumann's comment] The authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm. I basically subscribe to John Schulman's comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature. The paper does not cite or discuss the one below, though it looks VERY close: @inproceedings{neumann2009fitted, title={Fitted Q-iteration by advantage weighted regression}, author={Neumann, Gerhard and Peters, Jan R}, booktitle={Advances in neural information processing systems}, pages={1177--1184}, year={2009} } This paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper. Less importantly, the authors may also want to have a look at : @article{zimmer2019exploiting, title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains}, author={Zimmer, Matthieu and Weng, Paul}, journal={arXiv preprint arXiv:1906.04556}, year={2019} } which also uses ideas along the same line. To me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function. ", "rating": "6: Weak Accept", "reply_text": "Thank you for the feedback , we will aim to conduct additional experiments to address your questions . But first , could you clarify what you mean by the stability of the value function ? Value functions are very commonly used for actor-critic algorithms , such as policy gradient methods , while Q-functions are more commonly used for off-policy methods . To the best of our knowledge , neither one is necessarily easier to learn than the other ."}, "1": {"review_id": "H1gdF34FvS-1", "review_text": "This paper proposes an off-policy reinforcement learning method in which the model parameters have been updated using the regression style loss function. Specifically, this method uses two regression update steps: one update value function and another one update policy using weighted regression. To compare the proposed method with others [main comprasion], 6 MuJoCo tasks are used for continuous control and LunarLander-v2 for discrete space. -- Even though this paper has done a good job in terms of running different experiments, the selection of some of the benchmarks seems arbitrary. For example, for discrete action space, this paper uses LunarLander which is rarely used in any papers so it makes very difficult to draw a conclusion based on these results. Common 49 Atari-2600 games should have been used for comparison. The same thing about experiments in section 5.3 is true too as those tasks are not that well-known. -- The proposed method doesn't outperform previous off-policy methods on Mujoco task (Table 1). Since the main claim of this paper is a new off-policy method, outperforming the previous off-policy methods is a fair game. The current results are not convincing enough. -- There are significant overlaps between this paper, \"Fitted Q-iteration by Advantage Weighted Regression\", \"Model-Free Preference-Based Reinforcement Learning \", and \"Reinforcement learning by reward-weighted regression for operational space control\" which makes the contribution of this paper very incremental. -- The authors used only 5 seeds to run Mujoco experiments. Given the sensitivity of Mujoco for different starting points, the experiments should have been run at least with 10 different seeds. Questions: 1) Shouldn't be an importance sampling ratio between \\pi and \\mu in the equations? starting from eq.5. 2) Does the algorithm optimize the respect to $w$ as well? (eq. 15) if yes, why it is not mentioned in algorithm 1? Plus, since $d(s)$ is uniform dist. (at least this is assumed for implementation), eq. 14,15,and 16 (wherever there is d(s)), those can be simplified, e.g. \\hat{V} = \\sum(w_i V_i), wouldn't be better just introduced simplified version rather than current ones? (referring to the only equation above section 3.3) 3) Is this the same code used to report results in this paper? if yes, I didn't see any seed assignment in the code?! and what is \"action_std\" in the code? There are a couple of recent works in merging on-policy with off-policy updates which you might want to cite them. ", "rating": "3: Weak Reject", "reply_text": "Initial response to R1 Thank you for your feedback , we will aim to run the following additional experiments to address your questions : 1 ) We will perform experiments that directly compare AWR with REPS and FQI by AWR [ Neumann & Peters ] . 2 ) We will include additional tasks with discrete action spaces such as Cartpole-v1 , which is more commonly used . Please let us know if there are any additional experiments that you would find helpful . Re : Fitted Q-iteration by Advantage Weighted Regression Thank you for the pointer , We have revised the paper to include a discussion of \u201c Fitted Q-iteration by Advantage Weighted Regression \u201d [ Neumann & Peters ] in Section 4 ( highlighted in red ) . Neumann & Peters proposed a kernel-based fitted Q-iteration algorithm . Though their method also uses exponentiated advantages as weights , their definition of the policy is different from ours : https : //imgur.com/a/0DuRH7o The key difference is that in our method , the likelihood of an action is determined by both the likelihood of the sampling policy \\mu and the exponentiated advantage , while the policy in Neumann & Peters depends only on the advantage . Therefore , the policy update from Neumann & Peters does not enforce a trust region penalty that ensures the new policy is similar to the sampling policy , which is crucial for obtaining a good estimate of the objective using off-policy data collected from the sampling policy . Our method is simpler , does not perform fitted Q-iteration , and incorporates experience for off-policy learning . Furthermore , we provide a principled derivation of the advantage weights from a conservative policy improvement perspective . We further extend this analysis to AWR with experience replay and demonstrate its effectiveness for batch RL , both of which were not presented in these prior works . `` Model-Free Preference-Based Reinforcement Learning '' uses REPS as its learning algorithm , and a discussion of REPS is available in Section 4 . We will perform additional experiments to directly compare AWR with REPS and FQI by AWR . Re : Not outperforming previous methods While AWR does not outperform previous methods on all tasks , which we acknowledge in the paper , we would like to emphasize that our goal is to show that a simple off-policy method that uses supervised regression as subroutines can in fact be competitive with a number of current state-of-the-art algorithms . We believe that simple and effective RL algorithms are of interest to the ICLR community , and beating all state-of-the-art methods is not a prerequisite . We also demonstrate AWR \u2019 s effectiveness in the fully off-policy batch RL setting , where previous methods such DDPG and SAC perform poorly . Furthermore , AWR does not require the additional complexity of batch RL methods such as BCQ and BEAR . Re : Benchmarks We will also include additional tasks with discrete action spaces such as Cartpole-v1 , which is a more commonly used benchmark . Note , our focus is primarily on continuous control tasks , and the addition of the discrete tasks is mainly to show that AWR can also be easily applied to discrete actions . The motivation for the tasks in section 5.3 is to show that AWR is also effective for controlling complex agents with larger numbers of degrees-of-freedom . Since the agents in standard benchmark tasks are relatively simple , we opt to use new environments to better demonstrate this capability . Re : additional random seeds We have ran additional experiments for AWR with 10 different seeds . Here are learning curves comparing the results with 5 seeds and 10 seeds . The results are similar . We will update the paper to include results with 10 seeds . https : //imgur.com/a/yLntOzf Re : Q1 Importance sampling in Equation 5 Importance sampling ( IS ) is not required in Equation 5 . The objective itself does not require IS . For policy gradient methods like TRPO and PPO , IS is used to estimate the policy gradient from samples . In the AWR formulation , the solution of the Lagrangian in Equation 8 yields an update that does not require importance sampling , but it does require that \\mu and \\pi are similar , which is enforced by the trust region constraint ( Equation 6 ) . Re : Q2 Optimize w and uniform d ( s ) The algorithm does not optimize with respect to w. w_i represents the probability of selecting samples from a particular policy \\pi_i from the replay buffer , which is a constant . d ( s ) is also not a uniform distribution over states , it represents the state-marginal of the sampling distribution . In our implementation , we sample uniformly from the replay buffer , which does not result in a uniform d ( s ) , since some states might be visited more frequently than others . Re : code This is the same code used in the experiments . We do not manually assign random seeds to each run , instead we use python \u2019 s default random seed initialization , which assigns a different random seed to each execution of the problem . \u201c action_std \u201d specifies the standard deviation of the gaussian action distribution ."}, "2": {"review_id": "H1gdF34FvS-2", "review_text": " # Summary The paper shows that good old reward weighted regression (RWR) with value-function baseline is still state-of-the-art algorithm. # Decision The paper is well-written and provides many evaluations. The contribution should be articulated more carefully, though, taking into account that most algorithmic ideas are present in prior work (https://openreview.net/forum?id=H1gdF34FvS&noteId=Bkxi11nsdr). Perhaps, the experience replay part is somewhat novel. It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea. Provided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution. ", "rating": "6: Weak Accept", "reply_text": "Thank you for the feedback , we will improve the writing to more clearly articulate the contribution of this work and include a more thorough discussion to contrast our method with related techniques . We have revised the paper to include a discussion of these prior works at the end of Section 4 ( highlighted in red ) . We will also perform experiments that directly compare AWR with REPS and FQI by AWR [ Neumann & Peters ] . While exponentiated-advantage weights have been used in a number of prior work , we present a principled derivation of AWR from a conservative policy improvement perspective , and also provide an analysis of AWR when combined with experience replay . We show that a number of simple design decisions , such as the use of TD-lambda and experience replay , enables AWR to achieve competitive performance with state-of-the-art algorithms both for RL and batch RL settings ."}}