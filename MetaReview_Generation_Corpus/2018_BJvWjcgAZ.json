{"year": "2018", "forum": "BJvWjcgAZ", "title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update", "decision": "Reject", "meta_review": "The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this.", "reviews": [{"review_id": "BJvWjcgAZ-0", "review_text": "This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a \u00ab backward \u00bb update (i.e. from end to start of episode). The targets\u2019 update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation). This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5. The intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL. Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay (\u00ab Programming Robots Using Reinforcement Learning and Teaching \u00bb, Lin, 1991), something that is not mentioned here. That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin\u2019s algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments). In the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation. Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time? Or maybe the average is a bad performance measure due to outliers? I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method). Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here). The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm. Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance. A few additional small remarks and questions: - \u00ab Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. \u00bb: should \u00ab unless \u00bb be replaced by \u00ab if \u00bb? - In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze? - Typo in eq. 3: the - in the max should be a comma - There is a good amount of typos and grammar errors, though they do not harm the readability of the paper - Citations for \u00ab Deep Reinforcement Learning with Double Q-learning \u00bb and \u00ab Dueling Network Architectures for Deep Reinforcement Learning \u00bb could refer to their conference versions - \u00ab epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner \u00bb: please specify the exact formula - Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed feedback and questions . I 'd like to answer some of questions and share our plan to revise the paper with regards to your feedback . 1.Limited comparison We strongly agree that we need more baseline algorithms to show the effectiveness of our algorithm . As other reviewers have suggested , we will include the performance of prioritized experience replay and retrace algorithm in the revised version . 2.Idea of replaying experiences in backward Thank you for the reference , we will mention the relationship between Lin 's idea and our methods in the revised version . 3.Poor performance in MNIST DQN Learning curve tends to converge so fast for all algorithms when we used simple 2D maze , so it was difficult to compare different algorithms . So we used MNIST images as the state representation to make the learning process of general state transitions harder . We trained the agents for 200,000 steps , and all three algorithms ( backward DQN , vanilla DQN , n-step DQN ) converge to 1 . In the paper , we showed the plots over 100,000 steps to show the effectiveness of our method in the early stages of training . To avoid any confusion , we will show the results until 200,000 steps in the revised version . Note that the vanilla DQN is trained for 50M steps ( 200M frames ) in the Atari domain . Since the MNIST DQN environment is much simple , it is reasonable that the training is done for 0.2M steps . 4.A few more comments on MNIST DQN : We terminated the episode when the agent stays in the maze for more than 1000 time steps . We trained 50 different independent agents each in a different random maze and reported the mean score . But as you suggested , mean may be a bad measure due to outliers . So we will show both mean and median of 50 agents ' scores as the result in the revised version . 5.Running time compared to RAINBOW Running time may vary a lot depending on which device and distributed method you use . We used a single GPU to train an agent . As reported in the paper , it took 152 hours to train 490M frames ( 49 games x 10M frames ) . RAINBOW takes 10 days to train 200M frames . We will mention that the training time is not the 'mean ' training time of 49 games but the 'sum ' of training time in the revised version . 6.The last figure We apologize for the confusion . The first column and fourth rows of initialization and recursive updates part should be changed as `` s_1 '' - > `` s_2 '' . The beta is applied only for the positions where the actions were taken in the replay memory , as the update is done from right to left . a_T = A_2 , a_ ( T-1 ) = A_1 in the example . We will make this clear in the revised version . 7.Typos and Citations We will correct the typos and citations as your suggestions . Thank you so much for your ideas and suggestions . Any further comments are appreciated ."}, {"review_id": "BJvWjcgAZ-1", "review_text": "The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update. The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards. This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.). Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max. The paper is written in a clear way. The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there. I think this should have been one of the baselines to compare to for that reason. The experimental results seem promising in the illustrative MNIST domain. Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time and suggestions . As you mentioned , we guess there may be some relation between prioritized experience replay and our method . As all the reviewers have mentioned , we will add prioritized experience replay and retrace algorithm as the baseline to compare in the revised version . Any further suggestions are appreciated ."}, {"review_id": "BJvWjcgAZ-2", "review_text": "This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode. The paper is interesting, but it lacks the proper comparisons to previously published techniques. The results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA. In the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case. There has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case. In the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your time and suggestions . As you and other reviewers have mentioned , we strongly agree that we lack the comparisons to other related methods . We will try to compare our results and those of prioritized experience replay and retrace algorithm in the revised version . Also we will try to add some theoretical analysis to compare our algorithm to others . Any further comments and thoughts are appreciated ."}], "0": {"review_id": "BJvWjcgAZ-0", "review_text": "This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a \u00ab backward \u00bb update (i.e. from end to start of episode). The targets\u2019 update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation). This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5. The intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL. Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay (\u00ab Programming Robots Using Reinforcement Learning and Teaching \u00bb, Lin, 1991), something that is not mentioned here. That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin\u2019s algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments). In the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation. Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time? Or maybe the average is a bad performance measure due to outliers? I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method). Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here). The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm. Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance. A few additional small remarks and questions: - \u00ab Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. \u00bb: should \u00ab unless \u00bb be replaced by \u00ab if \u00bb? - In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze? - Typo in eq. 3: the - in the max should be a comma - There is a good amount of typos and grammar errors, though they do not harm the readability of the paper - Citations for \u00ab Deep Reinforcement Learning with Double Q-learning \u00bb and \u00ab Dueling Network Architectures for Deep Reinforcement Learning \u00bb could refer to their conference versions - \u00ab epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner \u00bb: please specify the exact formula - Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed feedback and questions . I 'd like to answer some of questions and share our plan to revise the paper with regards to your feedback . 1.Limited comparison We strongly agree that we need more baseline algorithms to show the effectiveness of our algorithm . As other reviewers have suggested , we will include the performance of prioritized experience replay and retrace algorithm in the revised version . 2.Idea of replaying experiences in backward Thank you for the reference , we will mention the relationship between Lin 's idea and our methods in the revised version . 3.Poor performance in MNIST DQN Learning curve tends to converge so fast for all algorithms when we used simple 2D maze , so it was difficult to compare different algorithms . So we used MNIST images as the state representation to make the learning process of general state transitions harder . We trained the agents for 200,000 steps , and all three algorithms ( backward DQN , vanilla DQN , n-step DQN ) converge to 1 . In the paper , we showed the plots over 100,000 steps to show the effectiveness of our method in the early stages of training . To avoid any confusion , we will show the results until 200,000 steps in the revised version . Note that the vanilla DQN is trained for 50M steps ( 200M frames ) in the Atari domain . Since the MNIST DQN environment is much simple , it is reasonable that the training is done for 0.2M steps . 4.A few more comments on MNIST DQN : We terminated the episode when the agent stays in the maze for more than 1000 time steps . We trained 50 different independent agents each in a different random maze and reported the mean score . But as you suggested , mean may be a bad measure due to outliers . So we will show both mean and median of 50 agents ' scores as the result in the revised version . 5.Running time compared to RAINBOW Running time may vary a lot depending on which device and distributed method you use . We used a single GPU to train an agent . As reported in the paper , it took 152 hours to train 490M frames ( 49 games x 10M frames ) . RAINBOW takes 10 days to train 200M frames . We will mention that the training time is not the 'mean ' training time of 49 games but the 'sum ' of training time in the revised version . 6.The last figure We apologize for the confusion . The first column and fourth rows of initialization and recursive updates part should be changed as `` s_1 '' - > `` s_2 '' . The beta is applied only for the positions where the actions were taken in the replay memory , as the update is done from right to left . a_T = A_2 , a_ ( T-1 ) = A_1 in the example . We will make this clear in the revised version . 7.Typos and Citations We will correct the typos and citations as your suggestions . Thank you so much for your ideas and suggestions . Any further comments are appreciated ."}, "1": {"review_id": "BJvWjcgAZ-1", "review_text": "The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update. The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards. This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.). Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max. The paper is written in a clear way. The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there. I think this should have been one of the baselines to compare to for that reason. The experimental results seem promising in the illustrative MNIST domain. Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time and suggestions . As you mentioned , we guess there may be some relation between prioritized experience replay and our method . As all the reviewers have mentioned , we will add prioritized experience replay and retrace algorithm as the baseline to compare in the revised version . Any further suggestions are appreciated ."}, "2": {"review_id": "BJvWjcgAZ-2", "review_text": "This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode. The paper is interesting, but it lacks the proper comparisons to previously published techniques. The results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA. In the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case. There has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case. In the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your time and suggestions . As you and other reviewers have mentioned , we strongly agree that we lack the comparisons to other related methods . We will try to compare our results and those of prioritized experience replay and retrace algorithm in the revised version . Also we will try to add some theoretical analysis to compare our algorithm to others . Any further comments and thoughts are appreciated ."}}