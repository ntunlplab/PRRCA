{"year": "2017", "forum": "rJbPBt9lg", "title": "Neural Code Completion", "decision": "Reject", "meta_review": "The paper extends existing code completion methods over discrete symbols with an LSTM-based neural network. This constitutes a novel application of neural networks to this domain, but is rather incremental. Alone, I don't think this would be a bad thing as good work can be incremental and make a useful contribution, but the scores do not show an amazingly significant improvement over the baseline. There are many design choices that could be better justified empirically with the introduction of neural benchmarks or an ablation study. We encourage the authors to further refine this work and re-submit.", "reviews": [{"review_id": "rJbPBt9lg-0", "review_text": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods. Comments: - Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation. - In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the useful comments . We have updated the related work section to revise the imprecise claims . We have worked on updated analysis for UNK tokens and deny prediction experiments . We are still waiting for the results , and we will update them within one day ."}, {"review_id": "rJbPBt9lg-1", "review_text": "Pros: using neural network on a new domain. Cons: It is not clear how it is guaranteed that the network generates syntactically correct code. Questions, comments: How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications? Were all combinations of NT2N decision with all possible NTN2T considered? Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the lexicon, thus limited the possible predictions. The question should then more likely be what is the optimal value of alpha for UNK. See also my previous comment on estimating and using UNK. Section 5.5, second paragraph, compares numbers which are not comparable. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the useful comments . The goal of this work is to enable code completion . With this target application scenario in mind , our latest results ( Section 5.3 ) have shown that over 96 % of the time when prediction is restricted to the top 50,000 most frequent tokens in the training set , our approach is able to provide a candidate list so that users can select from the list rather than inputting manually . Indeed , we agree with the reviewer that our approach does not guarantee the generated code is syntactically correct , but we would like to argue that 96 % accuracy indicates that our approach is effective enough for our target application . In the current version , we have removed the top5 accuracy for Table 5 , since these results are lower priority . We will update these results , later , by maximizing the joint posterior probability , and only top5 non-terminals from NT2N and top5 terminals from NTN2T will be considered . We will update the analysis about UNK tokens and alpha in one to two days , as well as Section 5.5 ( which is now removed ) ."}, {"review_id": "rJbPBt9lg-2", "review_text": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next. Empirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance? Overall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR. Detailed comments: * I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state. * The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the useful comments . We have tuned the model , and re-run all experiments . The current results suggest that our approaches can beat the state-of-the-art on both non-terminal and terminal prediction tasks . We have added a paragraph in Section 3.3 to explain why we think the next node prediction task is more meaningful than the next token prediction task . Also , the performance of these two tasks are not directly comparable , and recent works have focused more on the node prediction task . Thus we chose the former rather than the latter . For NT2NT , we have added an observation in Section 5.4 to show that , even though the model seems to predict the next non-terminal and the next terminal independently ( conditioned on the hidden states ) , our experiment results show that this is not the case . We have revised the related work section ."}], "0": {"review_id": "rJbPBt9lg-0", "review_text": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods. Comments: - Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation. - In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the useful comments . We have updated the related work section to revise the imprecise claims . We have worked on updated analysis for UNK tokens and deny prediction experiments . We are still waiting for the results , and we will update them within one day ."}, "1": {"review_id": "rJbPBt9lg-1", "review_text": "Pros: using neural network on a new domain. Cons: It is not clear how it is guaranteed that the network generates syntactically correct code. Questions, comments: How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications? Were all combinations of NT2N decision with all possible NTN2T considered? Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the lexicon, thus limited the possible predictions. The question should then more likely be what is the optimal value of alpha for UNK. See also my previous comment on estimating and using UNK. Section 5.5, second paragraph, compares numbers which are not comparable. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the useful comments . The goal of this work is to enable code completion . With this target application scenario in mind , our latest results ( Section 5.3 ) have shown that over 96 % of the time when prediction is restricted to the top 50,000 most frequent tokens in the training set , our approach is able to provide a candidate list so that users can select from the list rather than inputting manually . Indeed , we agree with the reviewer that our approach does not guarantee the generated code is syntactically correct , but we would like to argue that 96 % accuracy indicates that our approach is effective enough for our target application . In the current version , we have removed the top5 accuracy for Table 5 , since these results are lower priority . We will update these results , later , by maximizing the joint posterior probability , and only top5 non-terminals from NT2N and top5 terminals from NTN2T will be considered . We will update the analysis about UNK tokens and alpha in one to two days , as well as Section 5.5 ( which is now removed ) ."}, "2": {"review_id": "rJbPBt9lg-2", "review_text": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next. Empirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance? Overall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR. Detailed comments: * I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state. * The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the useful comments . We have tuned the model , and re-run all experiments . The current results suggest that our approaches can beat the state-of-the-art on both non-terminal and terminal prediction tasks . We have added a paragraph in Section 3.3 to explain why we think the next node prediction task is more meaningful than the next token prediction task . Also , the performance of these two tasks are not directly comparable , and recent works have focused more on the node prediction task . Thus we chose the former rather than the latter . For NT2NT , we have added an observation in Section 5.4 to show that , even though the model seems to predict the next non-terminal and the next terminal independently ( conditioned on the hidden states ) , our experiment results show that this is not the case . We have revised the related work section ."}}