{"year": "2017", "forum": "S1xh5sYgx", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "decision": "Reject", "meta_review": "The paper proposes a ConvNet architecture (\"SqueezeNet\") and a building block (\"Fire module\") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.", "reviews": [{"review_id": "S1xh5sYgx-0", "review_text": "Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules. Pros: Achieves x50 less memory usage than AlexNet while keeping similar accuracy. Cons & Questions: Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet? ", "rating": "7: Good paper, accept", "reply_text": "Good questions . I think we address most of your questions in an earlier comment thread . Search this page for `` SqueezeNet versus other models than AlexNet . ''"}, {"review_id": "S1xh5sYgx-1", "review_text": "Strengths \uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules. \uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \uf06e-- strong experimental results Weaknesses \uf06e--Would be nice to test Sqeezenet on multiple tasks \uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback and encouragement . Let me take a moment to discuss the weaknesses/questions that you mentioned . 1.SqueezeNet on other tasks . We recently added a detection layer at the end of SqueezeNet and fine-tuned it on KITTI object detection . We identified a variant of SqueezeNet that is simultaneously the fastest , smallest , and most accurate ( in terms of mean-average precision ) model on the KITTI detection task , compared to previous reported results . We will be posting this to the KITTI leaderboard soon , but meanwhile we have released some details in this paper : https : //arxiv.org/abs/1612.01051 2 . Theoretical analysis . We certainly appreciate the theoretical aspects of deep neural network research . If you ask a more specific question , we will do our best to answer it . 3.Placing SqueezeNet in the context of GoogLeNet and ResNet . Take a look at our response to the earlier comment thread , `` SqueezeNet versus other models than AlexNet . ''"}, {"review_id": "S1xh5sYgx-2", "review_text": "The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing. Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home. On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work. Oh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the encouraging and positive feedback ."}], "0": {"review_id": "S1xh5sYgx-0", "review_text": "Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules. Pros: Achieves x50 less memory usage than AlexNet while keeping similar accuracy. Cons & Questions: Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet? ", "rating": "7: Good paper, accept", "reply_text": "Good questions . I think we address most of your questions in an earlier comment thread . Search this page for `` SqueezeNet versus other models than AlexNet . ''"}, "1": {"review_id": "S1xh5sYgx-1", "review_text": "Strengths \uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules. \uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \uf06e-- strong experimental results Weaknesses \uf06e--Would be nice to test Sqeezenet on multiple tasks \uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback and encouragement . Let me take a moment to discuss the weaknesses/questions that you mentioned . 1.SqueezeNet on other tasks . We recently added a detection layer at the end of SqueezeNet and fine-tuned it on KITTI object detection . We identified a variant of SqueezeNet that is simultaneously the fastest , smallest , and most accurate ( in terms of mean-average precision ) model on the KITTI detection task , compared to previous reported results . We will be posting this to the KITTI leaderboard soon , but meanwhile we have released some details in this paper : https : //arxiv.org/abs/1612.01051 2 . Theoretical analysis . We certainly appreciate the theoretical aspects of deep neural network research . If you ask a more specific question , we will do our best to answer it . 3.Placing SqueezeNet in the context of GoogLeNet and ResNet . Take a look at our response to the earlier comment thread , `` SqueezeNet versus other models than AlexNet . ''"}, "2": {"review_id": "S1xh5sYgx-2", "review_text": "The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing. Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home. On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work. Oh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the encouraging and positive feedback ."}}