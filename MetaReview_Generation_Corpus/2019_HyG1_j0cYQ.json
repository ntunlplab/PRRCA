{"year": "2019", "forum": "HyG1_j0cYQ", "title": "Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels", "decision": "Reject", "meta_review": "The paper presents an approach to mitigate the presence of noisy labels during\ntraining by trying to forget wrong labels. Reviewers pointed out a few\nconcerns, including lack of novelty, lack of enough experimental support, and\nlack of theoretical support. Authors have added some experiments and details\nabout the experimental section, but reviewers still think it's not enough\nfor acceptance. I concur with the reviewers to reject the paper.", "reviews": [{"review_id": "HyG1_j0cYQ-0", "review_text": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels. The idea is to squeeze out the negative effects of noisy labels actively. The paper trains deep neural networks by stochastic gradient descent on \u201cfitting\u201d labels; while trains deep neural networks by scaled stochastic gradient ascent on \u201cnot-fitting\u201d labels. Experimental results show the improvement on robustness. The good things of the paper are clear. 1. Technical sound with reasonable idea 2. Problem is well motivated 3. Paper is general well written. Some comments 1. The idea using instance selection is not new. The novelty could be improved. If the paper could make more insight from either theoretical or application value, would be more interesting. 2. Experiments are too standard. More divers and various data sets would be more convincing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your suggestion . Recently , in the area of deep learning with noisy labels , training on selected instances is a totally new direction , which attracts a lot of attention [ 1-3 ] . However , once some false-positive instances ( real noisy data ) are selected during training , DNNs will memorize them finally , which inevitably degrades the generalization performance ( i.e. , test accuracy ) in the test phase . The key novelty of this paper is how to actively mitigate memorizing the negative effects of noisy labels , instead of following the path of training on selected instances . Specifically , to address such an issue , we introduce a meta approach called Pumpout . Intuitively , it squeezes the negative effects of noise labels actively by scaled stochastic gradient ascent . We can leverage Pumpout to upgrade orthogonal methods , such as MentorNet [ 1 ] ( training on selected instances ) and Backward Correction [ 4 ] ( estimating the noise transition matrix ) . To the best of our knowledge , it is the first attempt in deep learning that studies `` how to forget memorized information in an active manner '' , which is critical to improve the performance of some existing state-of-the-art methods . Nevertheless , we agree that some theoretical justification for Pumpout can be useful to understand and improve our method . We leave this in future works . References : [ 1 ] Jiang L , Zhou Z , Leung T , et al.MentorNet : Learning data-driven curriculum for very deep neural networks on corrupted labels . In ICML , 2018 . [ 2 ] Ren M , Zeng W , Yang B , Urtasun R. Learning to reweight examples for robust deep learning . In ICML , 2018 . [ 3 ] Han B , Yao Q , Yu X , Niu G , et al.Co-teaching : Robust training of deep Neural networks with extremely noisy labels . In NeurIPS , 2018 . [ 4 ] Patrini G , Rozza A , Menon A , et al.Making deep neural networks robust to label noise : A loss correction approach . In CVPR , 2017 ."}, {"review_id": "HyG1_j0cYQ-1", "review_text": "A new method for defending against label noise during training of deep neural networks is presented. The main idea is to \u201cforget\u201d about wrongly labeled examples during training by an ascending step in the gradient direction. This is a meta-approach that can be combined with other methods for noise robustness; the detection of the noisy examples is specific of the base method utilised. Experimental results are promising. In general, I find the idea original, and of potential practical use, but I believe the contribution of paper to be limited and not well supported by experiments. Moreover, I think that some claims made in this work are poorly justified. == Method The paper would greatly benefit from some theoretical backing of the proposed optimization scheme, even on a simplified scenario. An idea would be to prove that, given a dataset with noisy labels, PumpOut converges close to the best model (= the one learned without noise), for certain hyperparameters. I think this would be new and interesting. A result of similar fashion was proven in [A]. I found the following arguments not well or only heuristically supported: * section 2: the scaling factor \\gamma. Why using \\gamma=1 is suboptimal? One could claim that as much as you want to memorize the true image-label patterns, you also want to forget the image-noise ones. * Why MentorNet + PumpOut does not suffer from the selection bias effect of CoTraining? This is unclear to me * The non-negative version of the BackwardCorrection, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded. In fact, for a minimization problem to be well-defined, all you need is a lower bounded objective. Then, adding the lower bound makes your loss non-negative. Notice that Patrini et al 17 did not state that BC is unbounded, but only that it can be negative. Can you show more that that -- maybe, at least experimentally? The statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .). I suggest to remove this part. I have notice later that the non-negative version of BC is used as a baseline in the experiment, but I think that is the only use. Regarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut. == Experiments Table 1 can be removed as these are extremely common datasets. The experimental results look very promising for applications. As a side effect of this analysis, I can also notice an improvement over BC given to the nnBC, which is nice per se. Although, I would have strengthen the empirics as follow. * SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \u201cdid it work on CIFAR10?\" * A much harder instance of noise, for instance open set [B] or from a real dataset [Xiao et al 15] would have more clearly supported the use of PumpOut for real applications. * Can the authors elaborate on \u201cthe choices of \\beta and \\gamma follows Kirkyo et al 2017\u201d ? And how assuming their knowledge gives a fair comparison to BC which does not require them? I believe this is a critical point for the validity of the experiments. Minor: * \u201cLRELU active function\u2019 -> activation function. What is a LRELU? LeakyReLU? [A] Malach, Eran, and Shai Shalev-Shwartz. \"Decoupling\" when to update\" from\" how to update\".\" Advances in Neural Information Processing Systems. 2017. [B] Veit, Andreas, et al. \"Learning From Noisy Large-Scale Datasets With Minimal Supervision.\" CVPR. 2017.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We conduct the experiments on several benchmark datasets , including vision and text . We empirically find that the best performance is usually chosen when $ \\gamma $ is in-between 0 and 1 by using the validation set ."}, {"review_id": "HyG1_j0cYQ-2", "review_text": "The paper proposes a meta algorithm to train a network with noisy labels. It is not a general algorithm but a simple modification of two proposed methods. It is presented as a heuristics and it would be helpful to derive a theoretical framework or motivation for the proposed algorithm. My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise. In case of 20% symmetric error (figure 2c) the performance degradation using standard training should be very small. Hence it is difficult to evaluate to performance of the proposed method. At the beginning of the experiment section you mentioned several algorithms for training with noisy labels. I expect to compare your results to at least one of them. ", "rating": "3: Clear rejection", "reply_text": "We are sorry your concerns . However , the degradation of the test accuracy in the early stage of training is due to the memorization effects of deep networks . Namely , the model first learns the simple and general patterns of the real data before over-fitting and memorizing the noise ( which results in decreasing test accuracy ) . This observation has been well demonstrated by a lot of researchers recently ( e.g . [ 1-3 ] ) , and this observation has become a common sense . From Figure 7 ( b ) of [ 2 ] and Figures 3 , 5 , 6 of [ 3 ] , we can clearly see the same phenomenon of memorization effects of deep networks on noisy labels . Actually , the degradation of Figure 2 ( c ) is small . Please note that the lowest Y-axis is 0.88 . The performance degradation of this case is about 0.1 , which is similar to [ 2 ] and [ 3 ] . By the way , to the best of our knowledge , we are not aware of a top conference/journal paper in this area claiming that `` Standard training of neural network is very robust to label noise . '' Could you please point out a paper for our reference ? References : [ 1 ] Zhang C , et al.Understanding deep learning requires rethinking generalization . In ICLR , 2017 . [ 2 ] Arpit D , et al.A closer look at memorization in deep networks . In ICML , 2017 . [ 3 ] Han B , et al.Co-teaching : Robust training of deep neural networks with extremely noisy labels . In NeurIPS , 2018 ."}], "0": {"review_id": "HyG1_j0cYQ-0", "review_text": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels. The idea is to squeeze out the negative effects of noisy labels actively. The paper trains deep neural networks by stochastic gradient descent on \u201cfitting\u201d labels; while trains deep neural networks by scaled stochastic gradient ascent on \u201cnot-fitting\u201d labels. Experimental results show the improvement on robustness. The good things of the paper are clear. 1. Technical sound with reasonable idea 2. Problem is well motivated 3. Paper is general well written. Some comments 1. The idea using instance selection is not new. The novelty could be improved. If the paper could make more insight from either theoretical or application value, would be more interesting. 2. Experiments are too standard. More divers and various data sets would be more convincing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your suggestion . Recently , in the area of deep learning with noisy labels , training on selected instances is a totally new direction , which attracts a lot of attention [ 1-3 ] . However , once some false-positive instances ( real noisy data ) are selected during training , DNNs will memorize them finally , which inevitably degrades the generalization performance ( i.e. , test accuracy ) in the test phase . The key novelty of this paper is how to actively mitigate memorizing the negative effects of noisy labels , instead of following the path of training on selected instances . Specifically , to address such an issue , we introduce a meta approach called Pumpout . Intuitively , it squeezes the negative effects of noise labels actively by scaled stochastic gradient ascent . We can leverage Pumpout to upgrade orthogonal methods , such as MentorNet [ 1 ] ( training on selected instances ) and Backward Correction [ 4 ] ( estimating the noise transition matrix ) . To the best of our knowledge , it is the first attempt in deep learning that studies `` how to forget memorized information in an active manner '' , which is critical to improve the performance of some existing state-of-the-art methods . Nevertheless , we agree that some theoretical justification for Pumpout can be useful to understand and improve our method . We leave this in future works . References : [ 1 ] Jiang L , Zhou Z , Leung T , et al.MentorNet : Learning data-driven curriculum for very deep neural networks on corrupted labels . In ICML , 2018 . [ 2 ] Ren M , Zeng W , Yang B , Urtasun R. Learning to reweight examples for robust deep learning . In ICML , 2018 . [ 3 ] Han B , Yao Q , Yu X , Niu G , et al.Co-teaching : Robust training of deep Neural networks with extremely noisy labels . In NeurIPS , 2018 . [ 4 ] Patrini G , Rozza A , Menon A , et al.Making deep neural networks robust to label noise : A loss correction approach . In CVPR , 2017 ."}, "1": {"review_id": "HyG1_j0cYQ-1", "review_text": "A new method for defending against label noise during training of deep neural networks is presented. The main idea is to \u201cforget\u201d about wrongly labeled examples during training by an ascending step in the gradient direction. This is a meta-approach that can be combined with other methods for noise robustness; the detection of the noisy examples is specific of the base method utilised. Experimental results are promising. In general, I find the idea original, and of potential practical use, but I believe the contribution of paper to be limited and not well supported by experiments. Moreover, I think that some claims made in this work are poorly justified. == Method The paper would greatly benefit from some theoretical backing of the proposed optimization scheme, even on a simplified scenario. An idea would be to prove that, given a dataset with noisy labels, PumpOut converges close to the best model (= the one learned without noise), for certain hyperparameters. I think this would be new and interesting. A result of similar fashion was proven in [A]. I found the following arguments not well or only heuristically supported: * section 2: the scaling factor \\gamma. Why using \\gamma=1 is suboptimal? One could claim that as much as you want to memorize the true image-label patterns, you also want to forget the image-noise ones. * Why MentorNet + PumpOut does not suffer from the selection bias effect of CoTraining? This is unclear to me * The non-negative version of the BackwardCorrection, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded. In fact, for a minimization problem to be well-defined, all you need is a lower bounded objective. Then, adding the lower bound makes your loss non-negative. Notice that Patrini et al 17 did not state that BC is unbounded, but only that it can be negative. Can you show more that that -- maybe, at least experimentally? The statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .). I suggest to remove this part. I have notice later that the non-negative version of BC is used as a baseline in the experiment, but I think that is the only use. Regarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut. == Experiments Table 1 can be removed as these are extremely common datasets. The experimental results look very promising for applications. As a side effect of this analysis, I can also notice an improvement over BC given to the nnBC, which is nice per se. Although, I would have strengthen the empirics as follow. * SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder \u201cdid it work on CIFAR10?\" * A much harder instance of noise, for instance open set [B] or from a real dataset [Xiao et al 15] would have more clearly supported the use of PumpOut for real applications. * Can the authors elaborate on \u201cthe choices of \\beta and \\gamma follows Kirkyo et al 2017\u201d ? And how assuming their knowledge gives a fair comparison to BC which does not require them? I believe this is a critical point for the validity of the experiments. Minor: * \u201cLRELU active function\u2019 -> activation function. What is a LRELU? LeakyReLU? [A] Malach, Eran, and Shai Shalev-Shwartz. \"Decoupling\" when to update\" from\" how to update\".\" Advances in Neural Information Processing Systems. 2017. [B] Veit, Andreas, et al. \"Learning From Noisy Large-Scale Datasets With Minimal Supervision.\" CVPR. 2017.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We conduct the experiments on several benchmark datasets , including vision and text . We empirically find that the best performance is usually chosen when $ \\gamma $ is in-between 0 and 1 by using the validation set ."}, "2": {"review_id": "HyG1_j0cYQ-2", "review_text": "The paper proposes a meta algorithm to train a network with noisy labels. It is not a general algorithm but a simple modification of two proposed methods. It is presented as a heuristics and it would be helpful to derive a theoretical framework or motivation for the proposed algorithm. My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise. In case of 20% symmetric error (figure 2c) the performance degradation using standard training should be very small. Hence it is difficult to evaluate to performance of the proposed method. At the beginning of the experiment section you mentioned several algorithms for training with noisy labels. I expect to compare your results to at least one of them. ", "rating": "3: Clear rejection", "reply_text": "We are sorry your concerns . However , the degradation of the test accuracy in the early stage of training is due to the memorization effects of deep networks . Namely , the model first learns the simple and general patterns of the real data before over-fitting and memorizing the noise ( which results in decreasing test accuracy ) . This observation has been well demonstrated by a lot of researchers recently ( e.g . [ 1-3 ] ) , and this observation has become a common sense . From Figure 7 ( b ) of [ 2 ] and Figures 3 , 5 , 6 of [ 3 ] , we can clearly see the same phenomenon of memorization effects of deep networks on noisy labels . Actually , the degradation of Figure 2 ( c ) is small . Please note that the lowest Y-axis is 0.88 . The performance degradation of this case is about 0.1 , which is similar to [ 2 ] and [ 3 ] . By the way , to the best of our knowledge , we are not aware of a top conference/journal paper in this area claiming that `` Standard training of neural network is very robust to label noise . '' Could you please point out a paper for our reference ? References : [ 1 ] Zhang C , et al.Understanding deep learning requires rethinking generalization . In ICLR , 2017 . [ 2 ] Arpit D , et al.A closer look at memorization in deep networks . In ICML , 2017 . [ 3 ] Han B , et al.Co-teaching : Robust training of deep neural networks with extremely noisy labels . In NeurIPS , 2018 ."}}