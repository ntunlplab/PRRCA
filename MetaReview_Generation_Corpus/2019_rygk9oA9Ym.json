{"year": "2019", "forum": "rygk9oA9Ym", "title": "3D-RelNet: Joint Object and Relational Network for 3D Prediction", "decision": "Reject", "meta_review": "With ratings of 6, 5 & 3 the numerical scores are just not strong enough to warrant acceptance.\nThe author rebuttal was not able to sway opinions.\n", "reviews": [{"review_id": "rygk9oA9Ym-0", "review_text": "The paper is well-written with a few figures to illustrate the ideas and components of the proposed method. However, one of the main components in the proposed method is based on Tulsiani et al. CVPR'18. The remaining components of the proposed method are not very new. Hence, I am not very sure whether the novelty of the paper is significant. Nevertheless, the performance of the proposed method is fairly good outperforming all baseline methods. I also have a few questions: 1. How did you get the instance boxes, union boxes, and binary masks in testing? 2. What are the training and inference time? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful for your feedback . We hope that the above discussion assuaged the reviewer \u2019 s concerns regarding novelty and some unclear details . We briefly address the two questions regarding the setup : During testing , in the setting with known GT boxes ( Sec 4.2 ) , we assume that the 2D instance boxes are given . In the detection setting , the 2D instance boxes are the result of the learned detector . Given the ( detected or known ) instance boxes , the union boxes and binary masks can be easily computed - the union box is just the larger box containing both instance boxes , and the mask highlights these instance boxes in the union box . Training and Testing Inference Time on a single GPU ( Maxwell Titan X ) 1 . Train time : 65 hrs 2 . Test time : 0.55s per image"}, {"review_id": "rygk9oA9Ym-1", "review_text": "This paper proposed a 3D scene parsing that takes both objects and their relations into account, extending the Factor3D model proposed by Tulsiani et al 18. Results are demonstrated on both synthetic and real datasets. The paper is in general well written and clear. The approach is new, the results are good, the experiments are complete. However, I am still lukewarm about the paper and cannot champion it. I feel the paper interesting but not exciting, and it\u2019s unclear what we can really learn from it. Approach-wise, the idea of using pair-wise relationship as an inductive bias is getting popular. This paper demonstrated that it can be used for scene parsing, too, within a neural net. This is good to know, but not surprising given what have been demonstrated in the extensive literature in the computer graphics and vision community. In particular, the authors should discuss many related papers from Pat Hanrahan\u2019s group and Song-Chun Zhu\u2019s group (see some examples below). Apart from that, this paper doesn\u2019t have an obvious technical innovation that can inspire future work. This is different from Factor3D, which is the first voxel-based semantic scene parsing model from a single color image, with modern neural architecture. The results are good, but are on either synthetic data, or using ground truth bounding boxes. Requiring ground truth boxes greatly restricts the usage of these models. Would that be possible to include results under the detection setting on NYU-D or Matterport 3D? The authors claimed that the gain of 6 points is significant; however, a simple interaction net achieves a gain of 5 points, so the technical contribution of the proposed model is not too impressive. In general, I\u2019m on the border but leaning slightly toward rejection, because this paper is very similar to Tulsiani et al, and the proposed innovation has been explored in various forms in other papers. A minor issue: - In fig 5. The object colors are not matched for GT and Factor3D and ours. Related work Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image. ECCV\u201918. Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. IJCV\u201918. Characterizing Structural Relationships in Scenes Using Graph Kernels. SIGGRAPH\u201911. Example-based Synthesis of 3D Object Arrangements. SIGGRAPH Asia\u201912. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments and additional references - we will include these in related work . The reviewer raised concerns regarding the lessons learned from the paper in the context of previous pairwise relation modeling work in vision/graphics , suggested additional experiments in detection setting on real data , and pointed out a relatively small improvement over a particular baseline . Regarding the last concern , we note that apart from mAP scores we show significant improvements on errors in translations and scales as compared to all other baselines and that the difference between interaction net and our approach on these errors is significant . We hope that the discussion and results presented above in the reply to all reviewers addressed the first two concerns mentioned . We would again like to emphasize that we agree with the reviewer that previous work has examined similar inductive biases but would point out that our contributions regarding how these biases should be incorporated are novel , and would be useful for future attempts ."}, {"review_id": "rygk9oA9Ym-2", "review_text": "<Summary>: This paper presented a method for incorporating binary relationship between objects (relative location, rotation and scale) into single object 3d prediction. It is built on top of previously published work of [a] and used same network architecture and loss as of [a] and only added the binary relations between objects for object 3d estimation. The results are shown on SUNCG synthetic dataset and only *4 image* instances of NYUv2 dataset which is very small for a computer vision task. [a] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A Efros, and Jitendra Malik. Factoring shape, pose, and layout from the 2d image of a 3d scene. In CVPR, 2018. <Pros>: The paper tackles a problem of obvious interest to computer vision research community. It shows better results compared to previous similar work of [a] without considering binary relation between objects. <Cons>: *Technical details are missing: The set of known and unknown variables are not clear throughout the paper: -The extrinsic camera parameters are known or estimated by the method? -The intrinsic camera parameters are known or estimated by the method? -What are the properties of ground truth bounding boxes in 2D camera frame and 3D space? -What is the coordinate of translation? is it in camera coordinate or world coordinate? -What are the variations of camera poses in training and testing for synthetic dataset and how are the samples generated? Are the train/test images generated or are rendered images from previously published work of [b] used? [b] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. In CVPR, 2017. *The proposed method is trained on synthetic dataset of SUNCG and their object relations have biases from scene creators. While using binary relation between objects increase the recall in prediction it can also make the predictions bias to the most dominant relations and decrease the precision of detection in rare cases in synthetic dataset. Also, such bias can decrease prediction precision in images of real scenes. *One of the main issues in this paper is that the result of fully automated pipeline versus having ground-truth annotation at test time are mixed up. For example, in the teaser figure (Figure 1-b), does the proposed method use ground truth bounding boxes or not? It is mentioned in figure caption: \u201c(b) Output: An example result of our method that takes as input the 2D image and generates the 3D layout.\u201d. Is the input only 2D image or 2D image + ground truth object bounding boxes? In order to make sure that reader understands each qualitative result, there should be a column showing the \u201cInput\u201d to the pipeline (Not \u201cImage\u201d). For example, in Figure 3 and Figure 4, the image overlaid with input ground-truth bounding boxes should be shown as input to the algorithm. *The experiments and results does not convey the effectiveness of the proposed approach. There are major issues with the quality of the experiments and results. Here are several examples: - Missing baseline: Comparison with the CRF-based baseline is missing. This statement is not convincing in the introduction: \u201cOne classical approach is to use graphical models such as CRFs. However, these classical approaches have usually provided little improvements over object-based approaches.\u201d For a fair comparison with prior works, reporting results on a CRF-based baseline using similar unary predictions is necessary. -The experimental results are heavily based on ground truth boxes for the objects, but it is not clear how/where the ground truth boxes are given at the test time and which part is actually predicted. -If the ground truth boxes are given at the test time, it means that the ground truth binary relations between objects are given and it makes the problem trivial. -It is not clear what is the ground truth box in experimental setup. Is it amodal object box or the ground truth box contains only the visible part of the object? -The qualitative results shown in Figure 4 have full objects in voxel space with predicted rotation, scale and translation. In the qualitative result of Figure 3 and Figure 5 the voxel prediction is shown as final output. Why the result of full object in voxel space with predicted (rotation, scale and translation) is not shown in Figure 3 and Figure 5 and why it is shown in Figure 4? *Very limited results on real images: -Quantitative result on a dataset of real images is missing. The results on synthetic datasets is not a good proxy for the actual performance of the algorithm in real use cases and applications. - The paper only shows few results of NYUv2 on known ground truth boxes. The errors in object detection can be propagated to the 3D estimation therefore these qualitative results are not representative of the actual qualitative performance of the proposed algorithm. Several randomly selected qualitative results on a dataset of real images \u201cwithout ground-truth boxes\u201d are needed for evaluating the performance of the proposed method on real images. -Reporting variation in all parameters of scale, rotation and translation is necessary in order to find the difficulty of the problem. For example, what is the distribution of object scale in different object categories. What is the error of scale prediction of we use mean object scale for each object category for all object instance at test set? *Unclear statements and presentation: - It is mentioned in the paper: \u201cWhile the incorporation of unary and relative predictions can be expressed via linear constraints in the case of translation and scale, a similar closed form update does not apply for rotation because of the framing as a classification task and non-linearity of the manifold.\u201d -Is it necessary for the relative rotation to be formulated to classification task? -If not the comparison of modeling relative rotation via linear constraints is missing. - In some of the tables and figures the \u201cknow ground-truth boxes/detection setting\u201d are in bold face and in some cases are not. This should be consistent throughout the paper. ", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for the comments . Below we address specific concerns regarding the work . [ Missing details ] We point out relevant sections in the original submission where we already had mentioned the details . - Translation in camera vs. world frame : In Section 3.1 , we outline the parameterization of the 3D pose in terms of translation , rotation , and scale in the camera frame . - Rendered images and pose variations : As we mentioned in Section 4.1 ( Experimental Setup ) . We use the rendered images provided by Zhang et al . [ 1 ] Their work generated these images with a variety of different camera poses . We randomly partition this set of images into three splits from different houses - 70 % ( train ) , 10 % ( val ) and 20 % ( test ) . - Extrinsic vs. Intrinsic camera parameters : We estimate the pose of objects in the camera frame we can say that the extrinsics are Identity . Also , we make no assumptions about Intrinsics of the camera . This is outlined in Section 3.1 ( page 3 ) . - Properties of ground truth boxes in 2D : In Section 4.2 , we explain that we only input the ground truth boxes in 2D . We are unclear on what the reviewer means by \u201c properties \u201d of bounding boxes in 3D space but would be happy to clarify if the reviewer can elaborate . - What part is predicted at test time : In Section 4.2 , the ground-truth 2D boxes and the image are given as input and the method predicts the 3D pose and shape of each object . In Section 4.3 , the image is given as input and the method first detects the objects and then predicts the 3D pose of each object . We predict the voxel of the object for the experiments on the SUNCG dataset . - No voxel output in Figure 4 : As we mention in the Figure 4 caption - `` We use the ground-truth meshes for visualization due to lack of variability in shape annotations for learning . '' ( We have also described this in section 4.1 , NYUv2 setup ) - `` object relations have biases from scene creators '' ( in SUNCG ) : We appreciate the concerns about the biases in the synthetic data , but note that we do show results on the real images from the NYUv2 dataset . Unclear presentation : We thank the reviewer for these suggestions . They will surely help us improve the quality of the paper . - Set of knowns vs. Unknowns : We will clarify this in the paper . Your suggestion of using `` Input '' in the figures as opposed to `` Image '' is a great one and will be incorporated . - Figure 1 ( b ) : The input to the method is the 2D image and the associated ground-truth object bounding boxes . The output is the 3D pose and shape for each object . - Amodal box vs. full box : We use 2D bounding boxes as is standard in the object detection literature . These boxes are * not * amodal . - Relative rotation as classification , and ( possible ) missing comparison to linear baseline : As quaternion algebra is non-commutative the quaternion `` vector '' space is not linear . Thus , we formulate relative rotation as a classification problem , and hence there is no linear baseline . Also , note that previous works have also modeled rotation as a classification problem . Incorrect statements by the reviewer - Ground truth boxes are given implies relations are given : This is incorrect . The boxes are in 2D while the relations we use and predict in the paper are in 3D coordinates . - `` Quantitative result on a dataset of real images is missing '' : Please see Table 1 that shows quantitative results on the NYUv2 dataset . - `` * 4 image * instances of NYUv2 dataset '' : This statement is a mis-characterization of our work . We only visualize results of a few images but note that Table 1 reports evaluations using * all * ( 654 ) test set images on NYUv2 . This is explained in Section 4.1 . Missing baseline - CRF baseline : While this is an excellent suggestion , there are many practical reasons why it is not straightforward for us to do this . CRF based methods typically rely on class-specific pairwise potentials , and while these can be designed for specific cases ( e.g.chair-table ) , there are numerous non-trivial design decisions that make this not scalable for generic classes e.g.what is the form of potential function , do all ( or only nearby ) object pairs have an edge between them , do all classes pairs have potential functions etc . However , if the reviewer has any specific suggestions regarding a CRF baseline that can generically handle 3D pose across classes , we would be happy to compare . We would also like to point out the GCN baseline , which also does message passing , can be considered as an implicitly learned CRF ."}], "0": {"review_id": "rygk9oA9Ym-0", "review_text": "The paper is well-written with a few figures to illustrate the ideas and components of the proposed method. However, one of the main components in the proposed method is based on Tulsiani et al. CVPR'18. The remaining components of the proposed method are not very new. Hence, I am not very sure whether the novelty of the paper is significant. Nevertheless, the performance of the proposed method is fairly good outperforming all baseline methods. I also have a few questions: 1. How did you get the instance boxes, union boxes, and binary masks in testing? 2. What are the training and inference time? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful for your feedback . We hope that the above discussion assuaged the reviewer \u2019 s concerns regarding novelty and some unclear details . We briefly address the two questions regarding the setup : During testing , in the setting with known GT boxes ( Sec 4.2 ) , we assume that the 2D instance boxes are given . In the detection setting , the 2D instance boxes are the result of the learned detector . Given the ( detected or known ) instance boxes , the union boxes and binary masks can be easily computed - the union box is just the larger box containing both instance boxes , and the mask highlights these instance boxes in the union box . Training and Testing Inference Time on a single GPU ( Maxwell Titan X ) 1 . Train time : 65 hrs 2 . Test time : 0.55s per image"}, "1": {"review_id": "rygk9oA9Ym-1", "review_text": "This paper proposed a 3D scene parsing that takes both objects and their relations into account, extending the Factor3D model proposed by Tulsiani et al 18. Results are demonstrated on both synthetic and real datasets. The paper is in general well written and clear. The approach is new, the results are good, the experiments are complete. However, I am still lukewarm about the paper and cannot champion it. I feel the paper interesting but not exciting, and it\u2019s unclear what we can really learn from it. Approach-wise, the idea of using pair-wise relationship as an inductive bias is getting popular. This paper demonstrated that it can be used for scene parsing, too, within a neural net. This is good to know, but not surprising given what have been demonstrated in the extensive literature in the computer graphics and vision community. In particular, the authors should discuss many related papers from Pat Hanrahan\u2019s group and Song-Chun Zhu\u2019s group (see some examples below). Apart from that, this paper doesn\u2019t have an obvious technical innovation that can inspire future work. This is different from Factor3D, which is the first voxel-based semantic scene parsing model from a single color image, with modern neural architecture. The results are good, but are on either synthetic data, or using ground truth bounding boxes. Requiring ground truth boxes greatly restricts the usage of these models. Would that be possible to include results under the detection setting on NYU-D or Matterport 3D? The authors claimed that the gain of 6 points is significant; however, a simple interaction net achieves a gain of 5 points, so the technical contribution of the proposed model is not too impressive. In general, I\u2019m on the border but leaning slightly toward rejection, because this paper is very similar to Tulsiani et al, and the proposed innovation has been explored in various forms in other papers. A minor issue: - In fig 5. The object colors are not matched for GT and Factor3D and ours. Related work Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image. ECCV\u201918. Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. IJCV\u201918. Characterizing Structural Relationships in Scenes Using Graph Kernels. SIGGRAPH\u201911. Example-based Synthesis of 3D Object Arrangements. SIGGRAPH Asia\u201912. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments and additional references - we will include these in related work . The reviewer raised concerns regarding the lessons learned from the paper in the context of previous pairwise relation modeling work in vision/graphics , suggested additional experiments in detection setting on real data , and pointed out a relatively small improvement over a particular baseline . Regarding the last concern , we note that apart from mAP scores we show significant improvements on errors in translations and scales as compared to all other baselines and that the difference between interaction net and our approach on these errors is significant . We hope that the discussion and results presented above in the reply to all reviewers addressed the first two concerns mentioned . We would again like to emphasize that we agree with the reviewer that previous work has examined similar inductive biases but would point out that our contributions regarding how these biases should be incorporated are novel , and would be useful for future attempts ."}, "2": {"review_id": "rygk9oA9Ym-2", "review_text": "<Summary>: This paper presented a method for incorporating binary relationship between objects (relative location, rotation and scale) into single object 3d prediction. It is built on top of previously published work of [a] and used same network architecture and loss as of [a] and only added the binary relations between objects for object 3d estimation. The results are shown on SUNCG synthetic dataset and only *4 image* instances of NYUv2 dataset which is very small for a computer vision task. [a] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A Efros, and Jitendra Malik. Factoring shape, pose, and layout from the 2d image of a 3d scene. In CVPR, 2018. <Pros>: The paper tackles a problem of obvious interest to computer vision research community. It shows better results compared to previous similar work of [a] without considering binary relation between objects. <Cons>: *Technical details are missing: The set of known and unknown variables are not clear throughout the paper: -The extrinsic camera parameters are known or estimated by the method? -The intrinsic camera parameters are known or estimated by the method? -What are the properties of ground truth bounding boxes in 2D camera frame and 3D space? -What is the coordinate of translation? is it in camera coordinate or world coordinate? -What are the variations of camera poses in training and testing for synthetic dataset and how are the samples generated? Are the train/test images generated or are rendered images from previously published work of [b] used? [b] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. In CVPR, 2017. *The proposed method is trained on synthetic dataset of SUNCG and their object relations have biases from scene creators. While using binary relation between objects increase the recall in prediction it can also make the predictions bias to the most dominant relations and decrease the precision of detection in rare cases in synthetic dataset. Also, such bias can decrease prediction precision in images of real scenes. *One of the main issues in this paper is that the result of fully automated pipeline versus having ground-truth annotation at test time are mixed up. For example, in the teaser figure (Figure 1-b), does the proposed method use ground truth bounding boxes or not? It is mentioned in figure caption: \u201c(b) Output: An example result of our method that takes as input the 2D image and generates the 3D layout.\u201d. Is the input only 2D image or 2D image + ground truth object bounding boxes? In order to make sure that reader understands each qualitative result, there should be a column showing the \u201cInput\u201d to the pipeline (Not \u201cImage\u201d). For example, in Figure 3 and Figure 4, the image overlaid with input ground-truth bounding boxes should be shown as input to the algorithm. *The experiments and results does not convey the effectiveness of the proposed approach. There are major issues with the quality of the experiments and results. Here are several examples: - Missing baseline: Comparison with the CRF-based baseline is missing. This statement is not convincing in the introduction: \u201cOne classical approach is to use graphical models such as CRFs. However, these classical approaches have usually provided little improvements over object-based approaches.\u201d For a fair comparison with prior works, reporting results on a CRF-based baseline using similar unary predictions is necessary. -The experimental results are heavily based on ground truth boxes for the objects, but it is not clear how/where the ground truth boxes are given at the test time and which part is actually predicted. -If the ground truth boxes are given at the test time, it means that the ground truth binary relations between objects are given and it makes the problem trivial. -It is not clear what is the ground truth box in experimental setup. Is it amodal object box or the ground truth box contains only the visible part of the object? -The qualitative results shown in Figure 4 have full objects in voxel space with predicted rotation, scale and translation. In the qualitative result of Figure 3 and Figure 5 the voxel prediction is shown as final output. Why the result of full object in voxel space with predicted (rotation, scale and translation) is not shown in Figure 3 and Figure 5 and why it is shown in Figure 4? *Very limited results on real images: -Quantitative result on a dataset of real images is missing. The results on synthetic datasets is not a good proxy for the actual performance of the algorithm in real use cases and applications. - The paper only shows few results of NYUv2 on known ground truth boxes. The errors in object detection can be propagated to the 3D estimation therefore these qualitative results are not representative of the actual qualitative performance of the proposed algorithm. Several randomly selected qualitative results on a dataset of real images \u201cwithout ground-truth boxes\u201d are needed for evaluating the performance of the proposed method on real images. -Reporting variation in all parameters of scale, rotation and translation is necessary in order to find the difficulty of the problem. For example, what is the distribution of object scale in different object categories. What is the error of scale prediction of we use mean object scale for each object category for all object instance at test set? *Unclear statements and presentation: - It is mentioned in the paper: \u201cWhile the incorporation of unary and relative predictions can be expressed via linear constraints in the case of translation and scale, a similar closed form update does not apply for rotation because of the framing as a classification task and non-linearity of the manifold.\u201d -Is it necessary for the relative rotation to be formulated to classification task? -If not the comparison of modeling relative rotation via linear constraints is missing. - In some of the tables and figures the \u201cknow ground-truth boxes/detection setting\u201d are in bold face and in some cases are not. This should be consistent throughout the paper. ", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for the comments . Below we address specific concerns regarding the work . [ Missing details ] We point out relevant sections in the original submission where we already had mentioned the details . - Translation in camera vs. world frame : In Section 3.1 , we outline the parameterization of the 3D pose in terms of translation , rotation , and scale in the camera frame . - Rendered images and pose variations : As we mentioned in Section 4.1 ( Experimental Setup ) . We use the rendered images provided by Zhang et al . [ 1 ] Their work generated these images with a variety of different camera poses . We randomly partition this set of images into three splits from different houses - 70 % ( train ) , 10 % ( val ) and 20 % ( test ) . - Extrinsic vs. Intrinsic camera parameters : We estimate the pose of objects in the camera frame we can say that the extrinsics are Identity . Also , we make no assumptions about Intrinsics of the camera . This is outlined in Section 3.1 ( page 3 ) . - Properties of ground truth boxes in 2D : In Section 4.2 , we explain that we only input the ground truth boxes in 2D . We are unclear on what the reviewer means by \u201c properties \u201d of bounding boxes in 3D space but would be happy to clarify if the reviewer can elaborate . - What part is predicted at test time : In Section 4.2 , the ground-truth 2D boxes and the image are given as input and the method predicts the 3D pose and shape of each object . In Section 4.3 , the image is given as input and the method first detects the objects and then predicts the 3D pose of each object . We predict the voxel of the object for the experiments on the SUNCG dataset . - No voxel output in Figure 4 : As we mention in the Figure 4 caption - `` We use the ground-truth meshes for visualization due to lack of variability in shape annotations for learning . '' ( We have also described this in section 4.1 , NYUv2 setup ) - `` object relations have biases from scene creators '' ( in SUNCG ) : We appreciate the concerns about the biases in the synthetic data , but note that we do show results on the real images from the NYUv2 dataset . Unclear presentation : We thank the reviewer for these suggestions . They will surely help us improve the quality of the paper . - Set of knowns vs. Unknowns : We will clarify this in the paper . Your suggestion of using `` Input '' in the figures as opposed to `` Image '' is a great one and will be incorporated . - Figure 1 ( b ) : The input to the method is the 2D image and the associated ground-truth object bounding boxes . The output is the 3D pose and shape for each object . - Amodal box vs. full box : We use 2D bounding boxes as is standard in the object detection literature . These boxes are * not * amodal . - Relative rotation as classification , and ( possible ) missing comparison to linear baseline : As quaternion algebra is non-commutative the quaternion `` vector '' space is not linear . Thus , we formulate relative rotation as a classification problem , and hence there is no linear baseline . Also , note that previous works have also modeled rotation as a classification problem . Incorrect statements by the reviewer - Ground truth boxes are given implies relations are given : This is incorrect . The boxes are in 2D while the relations we use and predict in the paper are in 3D coordinates . - `` Quantitative result on a dataset of real images is missing '' : Please see Table 1 that shows quantitative results on the NYUv2 dataset . - `` * 4 image * instances of NYUv2 dataset '' : This statement is a mis-characterization of our work . We only visualize results of a few images but note that Table 1 reports evaluations using * all * ( 654 ) test set images on NYUv2 . This is explained in Section 4.1 . Missing baseline - CRF baseline : While this is an excellent suggestion , there are many practical reasons why it is not straightforward for us to do this . CRF based methods typically rely on class-specific pairwise potentials , and while these can be designed for specific cases ( e.g.chair-table ) , there are numerous non-trivial design decisions that make this not scalable for generic classes e.g.what is the form of potential function , do all ( or only nearby ) object pairs have an edge between them , do all classes pairs have potential functions etc . However , if the reviewer has any specific suggestions regarding a CRF baseline that can generically handle 3D pose across classes , we would be happy to compare . We would also like to point out the GCN baseline , which also does message passing , can be considered as an implicitly learned CRF ."}}