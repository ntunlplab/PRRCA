{"year": "2018", "forum": "r1lUOzWCW", "title": "Demystifying MMD GANs", "decision": "Accept (Poster)", "meta_review": "This paper does an excellent job at helping to clarify the relationship between various, recently proposed GAN models. The empirical contribution is small, but the KID metric will hopefully be a useful one for researchers. It would be really useful to show that it maintains its advantage when the dimensionality of the images increases (e.g., on Imagenet 128x128).", "reviews": [{"review_id": "r1lUOzWCW-0", "review_text": "This paper claims to demystify MMD-GAN, a generative adversarial network with the maximum mean discrepancy (MMD) as a critic, by showing that the usual estimator for MMD yields unbiased gradient estimates (Theorem 1). It was noted by the authors that biased gradient estimate can cause problem when performing stochastic gradient descent, as also noted previously by Bellemare et al. The authors also proposed a kernel inception distance (KID) as a quantitative evaluation metric for GAN. The KID is defined to be the squared MMD between inception representation of the distributions. In experiments, the authors compared the quality of samples generated by MMD-GAN with various kernels with the ones generated from WGAN-GP (Gulrajani et al., 2017) and Cramer GAN (Bellemare et al., 2017). The empirical results show the benefits of using the MMD on top of deep convolutional features. The major flaw of this paper is that its contribution is not really clear. Showing that the expectation and gradient can be interchanged (Theorem 1) does not seem to provide sufficient significance. Unbiasedness of the gradient alone does not guarantee that training will be successful and that the resulting models will better reflect the underlying data distribution, as evident by other successful variants of GANs, e.g., WGAN, which employ biased estimate. Indeed, since the training process relies on a small mini-batch, a small bias could help counteract the potentially high variance of the gradient estimate. The key is rather a good balance of both bias and variance during the training process and a guarantee that the estimate is asymptotically unbiased wrt the training iterations. Lastly, I do not see how the empirical results would demystify MMD-GANs, as claimed by the paper. The paper is clearly written. Some minor comments: - The proof of the main result, Theorem 1, should be placed in the main paper. - Page 7, 2nd paragraph: later --> layer", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments , and please also see our comments about improvements in the new revision above . You are certainly correct that an unbiased gradient does not guarantee that training will succeed ; our recent revision also substantially clarifies the bias situation . However , in SGD the bias-variance tradeoff is somewhat different than the situation in e.g.ridge regression , where the regularization procedure adds some bias but also reduces variance enough that it is worthwhile . There does n't seem to be any reason to think that the gradient variance is any higher for MMD GANs than for WGANs , and so a direct analogy does n't quite apply . Also , when performing SGD , the biases of each step might add up over time , and so \u2013 as in Bellemare et al . 's example \u2013 following biased gradients is worth at least some level of concern . With regards to the rest of the contribution : the title `` demystifying '' was intended more for the earlier parts of the paper , which elucidate the relationship of MMD GANs to other models and ( especially in the revision ) clarify the nature of the bias argument of Bellemare et al.The empirical results perhaps do not directly `` demystify , '' but rather bring another level of understanding of these models ."}, {"review_id": "r1lUOzWCW-1", "review_text": "The quality and clarity of this work are very good. The introduction of the kernel inception metric is well-motivated and novel, to my knowledge. With the mention of a bit more related work (although this is already quite good), I believe that this could be a significant resource for understanding MMD GANs and how they fit into the larger model zoo. Pros - best description of MMD GANs that I have encountered - good contextualization of related work and descriptions of relationships, at least among the works surveyed - reasonable proposed metric (KID) and comparison with other scores - proof of unbiased gradient estimates is a solid contribution Cons - although the review of related work is very good, it does focus on ~3 recent papers. As a review, it would be nice to see mention (even just in a list with citations) of how other models in the zoo fit in - connection between IPMs and MMD gets a bit lost; a figure (e.g. flow chart) would help - wavers a bit between proposing/proving novel things vs. reviewing and lacks some overall structure/storyline - Figure 1 is a bit confusing; why is KID tested without replacement, and FID with? Why 100 vs 10 samples? The comparison is good to have, but it's hard to draw any insight with these differences in the subfigures. The figure caption should also explain what we are supposed to get out of looking at this figure. Specific comments: - I suggest bolding terms where they are defined; this makes it easy for people to scan/find (e.g. Jensen-Shannon divergence, Integral Probability Metrics, witness functions, Wasserstein distance, etc.) - Although they are common knowledge in the field, because this is a review it could be helpful to provide references or brief explanations of e.g. JSD, KL, Wasserstein distance, RKHS, etc. - a flow chart (of GANs, IPMs, MMD, etc., mentioning a few more models than are discussed in depth here, would be *very* helpful. - page 2, middle paragraph, you mention \"...constraints to ensure the kernel distribution embeddings remained injective\"; it would be helpful to add a sentence here to explain why that's a good thing. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments . We 've posted a new revision addressing most of them ; see also our separate comment describing other significant improvements . - Review of related work : thanks for the suggestion . We have added a brief section 2.4 with some more related work ; we would be happy to add more if you have some other suggestions . - We have attempted to slightly clarify the description of IPMs in this revision , and will further consider better ways to do this . - KID/FID comparison figure : We agree that this difference is confusing . It was done because the standard KID estimator becomes biased when there are repeated points due to sampling with replacement , but of course when sampling 10,000 / 10,000 points without replacement , it is unsurprising that there is no variance in the estimate , so it made more sense for the point we were trying to make to evaluate FID with replacement . The difference in number of samples was due to the relatively higher computational expense of the FID ( which requires the SVD of a several thousand dimensional-matrix ) , but we have increased that to the same number of samples as well . The figures look essentially identical changing either of these issues ; we have changed to using a variant of the KID estimator which is still unbiased for samples with replacement and clarified the caption . - We have added a footnote on why injectivity of the distribution embeddings is desirable ."}, {"review_id": "r1lUOzWCW-2", "review_text": "The main contribution of the paper is that authors extend some work of Bellemare: they show that MMD GANs [which includes the Cramer GAN as a subset] do possess unbiased gradients. They provide a lot of context for the utility of this claim, and in the experiments section they provide a few different metrics for comparing GANs [as this is a known tricky problem]. The authors finally show that an MMD GAN can achieve comparable performance with a much smaller network used in the discriminator. As previously mentioned, the big contribution of the paper is the proof that MMD GANs permit unbiased gradients. This is a useful result; however, given the lack of other outstanding theoretical or empirical results, it almost seems like this paper would be better shaped as a theory paper for a journal. I could be swayed to accept this paper however if others feel positive about it. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . We do feel that this paper has contributions outside just the proof of unbiased gradients , in particular clarifying the relationship among various slightly-different GAN models , the KID score , and the new experimental results about success with smaller critic networks , which are of interest to the ICLR community . Please also see our general comments about the new revision above , which includes substantial improvements ."}], "0": {"review_id": "r1lUOzWCW-0", "review_text": "This paper claims to demystify MMD-GAN, a generative adversarial network with the maximum mean discrepancy (MMD) as a critic, by showing that the usual estimator for MMD yields unbiased gradient estimates (Theorem 1). It was noted by the authors that biased gradient estimate can cause problem when performing stochastic gradient descent, as also noted previously by Bellemare et al. The authors also proposed a kernel inception distance (KID) as a quantitative evaluation metric for GAN. The KID is defined to be the squared MMD between inception representation of the distributions. In experiments, the authors compared the quality of samples generated by MMD-GAN with various kernels with the ones generated from WGAN-GP (Gulrajani et al., 2017) and Cramer GAN (Bellemare et al., 2017). The empirical results show the benefits of using the MMD on top of deep convolutional features. The major flaw of this paper is that its contribution is not really clear. Showing that the expectation and gradient can be interchanged (Theorem 1) does not seem to provide sufficient significance. Unbiasedness of the gradient alone does not guarantee that training will be successful and that the resulting models will better reflect the underlying data distribution, as evident by other successful variants of GANs, e.g., WGAN, which employ biased estimate. Indeed, since the training process relies on a small mini-batch, a small bias could help counteract the potentially high variance of the gradient estimate. The key is rather a good balance of both bias and variance during the training process and a guarantee that the estimate is asymptotically unbiased wrt the training iterations. Lastly, I do not see how the empirical results would demystify MMD-GANs, as claimed by the paper. The paper is clearly written. Some minor comments: - The proof of the main result, Theorem 1, should be placed in the main paper. - Page 7, 2nd paragraph: later --> layer", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments , and please also see our comments about improvements in the new revision above . You are certainly correct that an unbiased gradient does not guarantee that training will succeed ; our recent revision also substantially clarifies the bias situation . However , in SGD the bias-variance tradeoff is somewhat different than the situation in e.g.ridge regression , where the regularization procedure adds some bias but also reduces variance enough that it is worthwhile . There does n't seem to be any reason to think that the gradient variance is any higher for MMD GANs than for WGANs , and so a direct analogy does n't quite apply . Also , when performing SGD , the biases of each step might add up over time , and so \u2013 as in Bellemare et al . 's example \u2013 following biased gradients is worth at least some level of concern . With regards to the rest of the contribution : the title `` demystifying '' was intended more for the earlier parts of the paper , which elucidate the relationship of MMD GANs to other models and ( especially in the revision ) clarify the nature of the bias argument of Bellemare et al.The empirical results perhaps do not directly `` demystify , '' but rather bring another level of understanding of these models ."}, "1": {"review_id": "r1lUOzWCW-1", "review_text": "The quality and clarity of this work are very good. The introduction of the kernel inception metric is well-motivated and novel, to my knowledge. With the mention of a bit more related work (although this is already quite good), I believe that this could be a significant resource for understanding MMD GANs and how they fit into the larger model zoo. Pros - best description of MMD GANs that I have encountered - good contextualization of related work and descriptions of relationships, at least among the works surveyed - reasonable proposed metric (KID) and comparison with other scores - proof of unbiased gradient estimates is a solid contribution Cons - although the review of related work is very good, it does focus on ~3 recent papers. As a review, it would be nice to see mention (even just in a list with citations) of how other models in the zoo fit in - connection between IPMs and MMD gets a bit lost; a figure (e.g. flow chart) would help - wavers a bit between proposing/proving novel things vs. reviewing and lacks some overall structure/storyline - Figure 1 is a bit confusing; why is KID tested without replacement, and FID with? Why 100 vs 10 samples? The comparison is good to have, but it's hard to draw any insight with these differences in the subfigures. The figure caption should also explain what we are supposed to get out of looking at this figure. Specific comments: - I suggest bolding terms where they are defined; this makes it easy for people to scan/find (e.g. Jensen-Shannon divergence, Integral Probability Metrics, witness functions, Wasserstein distance, etc.) - Although they are common knowledge in the field, because this is a review it could be helpful to provide references or brief explanations of e.g. JSD, KL, Wasserstein distance, RKHS, etc. - a flow chart (of GANs, IPMs, MMD, etc., mentioning a few more models than are discussed in depth here, would be *very* helpful. - page 2, middle paragraph, you mention \"...constraints to ensure the kernel distribution embeddings remained injective\"; it would be helpful to add a sentence here to explain why that's a good thing. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments . We 've posted a new revision addressing most of them ; see also our separate comment describing other significant improvements . - Review of related work : thanks for the suggestion . We have added a brief section 2.4 with some more related work ; we would be happy to add more if you have some other suggestions . - We have attempted to slightly clarify the description of IPMs in this revision , and will further consider better ways to do this . - KID/FID comparison figure : We agree that this difference is confusing . It was done because the standard KID estimator becomes biased when there are repeated points due to sampling with replacement , but of course when sampling 10,000 / 10,000 points without replacement , it is unsurprising that there is no variance in the estimate , so it made more sense for the point we were trying to make to evaluate FID with replacement . The difference in number of samples was due to the relatively higher computational expense of the FID ( which requires the SVD of a several thousand dimensional-matrix ) , but we have increased that to the same number of samples as well . The figures look essentially identical changing either of these issues ; we have changed to using a variant of the KID estimator which is still unbiased for samples with replacement and clarified the caption . - We have added a footnote on why injectivity of the distribution embeddings is desirable ."}, "2": {"review_id": "r1lUOzWCW-2", "review_text": "The main contribution of the paper is that authors extend some work of Bellemare: they show that MMD GANs [which includes the Cramer GAN as a subset] do possess unbiased gradients. They provide a lot of context for the utility of this claim, and in the experiments section they provide a few different metrics for comparing GANs [as this is a known tricky problem]. The authors finally show that an MMD GAN can achieve comparable performance with a much smaller network used in the discriminator. As previously mentioned, the big contribution of the paper is the proof that MMD GANs permit unbiased gradients. This is a useful result; however, given the lack of other outstanding theoretical or empirical results, it almost seems like this paper would be better shaped as a theory paper for a journal. I could be swayed to accept this paper however if others feel positive about it. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . We do feel that this paper has contributions outside just the proof of unbiased gradients , in particular clarifying the relationship among various slightly-different GAN models , the KID score , and the new experimental results about success with smaller critic networks , which are of interest to the ICLR community . Please also see our general comments about the new revision above , which includes substantial improvements ."}}