{"year": "2020", "forum": "r1glygHtDB", "title": "A multi-task U-net for segmentation with lazy labels", "decision": "Reject", "meta_review": "The paper proposes an architecture for semantic instance segmentation learnable from coarse annotations and evaluates it on two microscopy image datasets, demonstrating its advantage over baseline. While the reviewers appreciate the details of the architecture, they note the lack of evaluation on any of popular datasets and the lack of comparisons with baselines that would be more close to state-of-the-art. The authors do not address this criticism convincingly. It is not clear, why e.g. the Cityscapes or VOC Pascal datasets, which both have reasonably accurate annotations, cannot be used for the validation of the idea. If the focus is on the precision of the result near the boundaries, then one can always report the error near boundaries (this is a standard thing to do). Note that the performance of the baseline models is far from saturated near boundaries (i.e. the errors are larger than mistakes of annotation).\n\nAt this stage, the paper lacks convincing evaluation and comparison with prior art. Given that this is first and foremost application paper, lacking some very novel ideas (as pointed out by e.g. Rev1), better evaluation is needed for acceptance.", "reviews": [{"review_id": "r1glygHtDB-0", "review_text": "Summary: This paper addresses the problem of learning segmentation models in the presence of weak and strong labels of different types. This is an important problem that arises while learning in data-scarce settings. The presented approach optimizes jointly over the various types of labels by treating each one as a task. They extend the U-net architecture to incorporate the different tasks. Prior work: There has been other work on incorporating multi-resolution or different types of labels. Here is one that can be cited: Label super-resolution networks (https://openreview.net/forum?id=rkxwShA9Ym) Major comments: - The motivation for the specific structure of the multi-task blocks is not clear - The object boundaries labels can be noisy (i.e s(2) can have noise). How does model deal with this? - Is it the case that every image in I_3 is completely labeled - i.e all segments/classes marked? - The assumption that s(3) is independent of s(1) and s(2) is not true. Instead of constraining the model to learn masks that respect the various types of labels, it seems they learn from each source independently. It is not clear how the sharing of parameters in the multitask block helps. - Can they comment on the applicability of the prior work suggested above? Minor comments: - How do the rough labeling tools work on biomedical data where the objects are more heterogenous patterns where different labels can have very different distribution of pixels. How well will their method generalize in such settings? - Can this work be used for segmentation and prediction on crop data? Results: - It seems as if the improvement over the PL baseline (pseudo labels) is incremental? Can the authors provide error bars so the reader knows what the significance of the results is? - Can they give a more thorough comparison in terms of human effort? It is interesting to note that only 2 images give 0.82. Would 3 images give 0.94? They need to show the trade-off between additional effort vs gains in performance. - What is the performance of MT U-net without the SL images (i.e without task-3)? Table-2 does give some intuition, but authors should add another row with multitask WL - Table-3: How well does MDUnet do with 9.4% SL data? ", "rating": "6: Weak Accept", "reply_text": "> > Can they comment on the applicability of the prior work suggested above ? The Label super-resolution networks assume that the label is given at the low-resolution level ( in the form of one label per block of the image ) and known distribution of high-resolution labels is conditioned on the label of the low-resolution block . Our work also introduces a certain kind of low resolution labels , in the sense that the boundaries of the WL are noisy and inaccurate . We make use of the observation that the pixels that are in the partial mask ( WL ) set a strict constraint for the values of the SL , and do not assume that the distribution of the SL given the WL is known . In practice , the annotation strategy is very different for this method compared to ours . Our WL gives more information as it includes details on separating and touching objects . This is not the case with the low-resolution level labels used by the Label super-resolution network . Therefore the mentioned prior work is not directly comparable to our approach . > > How do the rough labeling tools work on biomedical data where the objects are more heterogenous patterns Indeed , in the ice crystal and air bubble problem , the distributions of pixels are similar in the different classes , which makes this problem difficult to handle . Since our method provides accurate results on this problem , we believe the segmentation task should adapt easily to different distributions of pixels . Given the results we show on the Gland dataset ( Subsection 4.2 ) , we believe that our method should also generalize well to textured objects . At the same time , the detection should not be affected by heterogeneous patterns , but more extensive experiments would have to be conducted to validate this . > > Can this work be used for segmentation and prediction on crop data ? We are not familiar with prediction problems associated to crop data . Our work could be relevant in the case of crop image datasets containing small training sets . > > It seems as if the improvement over the PL baseline ( pseudo labels ) is incremental ? Can the authors provide error bars so the reader knows what the significance of the results is ? We have added error bars to compare the two methods ( see Figure 4 ) . Thanks for the suggestion . In addition to the comparisons provided in Table 1 and Figure 4 , we have demonstrated qualitative results in Figure 5 , which shows that our approach gives accurate predictions on the contours of the objects , and this is one of our main objectives in the work . > > Can they give a more thorough comparison in terms of human effort ? It is interesting to note that only 2 images give 0.82 . Would 3 images give 0.94 ? We included a comparison of the supervision from different amounts of SL in Table 2 . The $ 20\\ % $ SL ( $ 4 $ images ) , $ 30\\ % $ SL ( $ 6 $ images ) and $ 75\\ % $ SL ( $ 15 $ images ) give scores of $ 0.882 $ , $ 0.913 $ , $ 0.940 $ respectively . Given 6x speedup on WL annotation time , the creation of $ 10\\ % $ SL + WL is $ 3 $ times faster than $ 75\\ % $ SL which gives similar accuracy . > > What is the performance of MT U-net without the SL images ( i.e without task-3 ) ? The MT U-net without SL ( i.e. , without task 3 ) degenerates into the U-net with WL . In fact , according to the loss function ( 5 ) we used , the loss on task 3 is always zero provided no SL available . This contributes to nothing in backpropagation during the network training , and therefore MT U-net degenerates into the U-net with WL . We have provided the results on U-net with WL ( second row of table 1 ) , which therefore covers the results of MT U-net without task 3 . > > Table-3 : How well does MDUnet do with 9.4 % SL data ? The MDUnet is proposed in a fully supervised setting , and it is limited by the amount of SL , so is the single task U-net . We have demonstrated the relatively lower performance of the single task U-net ( in Table 3 , and also in Table 1 ) , and similarly a significant performance reduction could be expected for the MDUnet if one reduces the SL from $ 100\\ % $ to only $ 9.4\\ % $ ."}, {"review_id": "r1glygHtDB-1", "review_text": "The submission presents a neural network for multi-task learning on sets of labeled data that are largely weakly supervised (in this case, partially segmented instances), augmented by comparatively fewer fully supervised annotations. The multiple tasks are designed to make use of both the weak as well as as full (\u2018strong\u2019) labels, such that performance on fully annotated machine-generated output is improved. As noted in the related work section (Section 2), multi-task methods aim to use benefits from underlying common information that may be ignored in a single-task setting. The network presented here is quite similar to most of these multi-task approaches: a common feature encoder, and partially distinct feature decoding and classification parts. The (minor) novelty mainly comes from the distinct types of weak/strong annotation data fed here: instance scribbles, boundary scribbles, and (some or few) full segmentations. The submission is overall well written and provides sufficient clarity and a good overview of the approach. Section 3 presents a probabilistic decomposition of the proposed architecture. With some fairly standard assumptions and simplifications, the loss in Eq. 3 becomes rather straightforward (weighted cross entropy) The actual network architecture described in Section 3.2 takes a standard U-Net as a starting point and modifies it in a fairly targeted way for the different expected types of annotations. These annotations (Section 3.3) are cheaper than full labels on a same-size dataset; it is not completely clear, however, if the mentioned scribbles need to capture each instance in the training set, or if some can also be left out. Without this being explicitly mentioned, I will assume the former. The experimental evaluation is done reasonably well, although I am not familiar with any of the presented data sets. The SES set seems to be specific to the submission, while the H&E data set has been used at least one other relevant publication (Zhang et al.). My main issue here is that at least on the SES set, which does not seem to be that large, the score difference is not that big, so dataset bias could play some part (which is unproven, but so is the opposite). Experimental evaluation does not leave the low-number-of-classes regime, and I\u2019m left wondering how the method might compare on a semantically much richer data set, e.g. Cityscapes. Finally, unmodified U-Net is by now a rather venerable baseline, so I\u2019m also wondering how the proposed multi-task learning could be used in other (more recent) architectures, i.e. whether the idea can be generalized sufficiently. While I think the ideas per se have relatively minor novelty, the combination seems novel to me, and that might warrant publication.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments and feedback . > > it is not completely clear , however , if the mentioned scribbles need to capture each instance in the training set , or if some can also be left out . We used different subsets of images ( i.e. , $ \\mathcal { I } _1 $ , $ \\mathcal { I } _2 $ and $ \\mathcal { I } _3 $ ) labeled for different tasks . Therefore , the scribbles ( WL ) do not need to capture every instance in the training sets . However , we do have an assumption that most of the images are weakly labelled . This is motivated by the fact that the WL are collected in a much cheaper way than the SL . As we mentioned in Section 3 , the missing labels ( either SL or WL ) are incorporated using a weighted loss function ( given by Equation ( 5 ) ) . > > Experimental evaluation does not leave the low-number-of-classes regime , and I \u2019 m left wondering how the method might compare on a semantically much richer data set , e.g.Cityscapes The successful examples we have shown make the proposed model relevant for a wide range of applications with a low number of classes ( for example biomedical imaging data ) and namely the practical problems we are working on . It would indeed be interesting to try the approach for a problem with a larger number of classes , but it would require specific data . Our method has been designed to address problems in which datasets have fewer images , not highly contrasted images and where finding very accurate contours is essential . Most semantically rich and large datasets , such as Cityscapes , have better contrast and the annotations are not always accurate enough . > > Finally , unmodified U-Net is by now a rather venerable baseline , so I \u2019 m also wondering how the proposed multi-task learning could be used in other ( more recent ) architectures , i.e.whether the idea can be generalized sufficiently . We have not explored all possible architectures in our main segmentation network , but the idea of decomposition of the segmentation problem into the mentioned smaller tasks is to a certain extent agnostic to the underlying architecture and readily applicable to a lot of different settings . The choice of network architectures has some flexibility depending on the problem at hand , and the multi-task learning block could be used in other architectures that have multiple levels of resolution design ( such as SegNet ) ."}, {"review_id": "r1glygHtDB-2", "review_text": "This paper proposes a method for semantic segmentation using \"lazy\" segmentation labels. Lazy labels are defined as coarse labels of the segmented objects. The proposed method is a UNET trained in a multitask fashion whit 3 tasks: object detection, object separation, and object segmentation. The method is trained on 2 datasets: air bubbles, and ice crystals. The proposed method performs better than the same method using only the weakly supervised labels and the one that only uses the sparse labels. The novelty of the method is very limited. It is a multitask UNET. The method is compared with one method using pseudo labels. However, this method is not SOTA. Authors should compare with current methods such as: - Where are the Masks: Instance Segmentation with Image-level Supervision - Instance Segmentation with Point Supervision - Object counting and instance segmentation with image-level supervision - Weakly supervised instance segmentation using class peak response - Soft proposal networks for weakly supervised object localization - Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation These methods can use much less supervision (point-level, count-level or image-level) and may work even better. The method should be compared on standard and challenging datasets like Cityscapes, PASCAL VOC 2012, COCO, KITTI... ", "rating": "1: Reject", "reply_text": "> > The method should be compared on standard and challenging datasets like Cityscapes , PASCAL VOC 2012 , COCO , KITTI\u2026 There are various reasons why we think the current datasets are more suitable for this work . 1 ) .The mentioned standard datasets are about images from natural scenes and they generally have better contrast between the objects and the background compared to the datasets that we used . Indeed , one key objective of our work is to obtain accurate object contours for images in poor contrast ( for instance.images in Figure 9 ) . 2 ) .We wish to develop methods that are applicable for application domains where only relatively small training sets are available , as the ones demonstrated in the paper . 3 ) .We are focusing on problems where finding a precise segmentation is important . On the large datasets mentioned by the reviewer , the annotations are not always accurate enough : - Cityscapes Dataset : The legs of the man on the motorcycle in Example Weimar 1 in https : //www.cityscapes-dataset.com/examples/ - COCO 2019 : Legs of the tennis player in http : //cocodataset.org/ # detection-2019 - In KITTI , cars are often not segmented when touching image boundaries ( second line of Fig.1 page 16 of https : //arxiv.org/pdf/1604.05096.pdf , Fig 9 ( e ) in https : //arxiv.org/pdf/1908.11656.pdf )"}], "0": {"review_id": "r1glygHtDB-0", "review_text": "Summary: This paper addresses the problem of learning segmentation models in the presence of weak and strong labels of different types. This is an important problem that arises while learning in data-scarce settings. The presented approach optimizes jointly over the various types of labels by treating each one as a task. They extend the U-net architecture to incorporate the different tasks. Prior work: There has been other work on incorporating multi-resolution or different types of labels. Here is one that can be cited: Label super-resolution networks (https://openreview.net/forum?id=rkxwShA9Ym) Major comments: - The motivation for the specific structure of the multi-task blocks is not clear - The object boundaries labels can be noisy (i.e s(2) can have noise). How does model deal with this? - Is it the case that every image in I_3 is completely labeled - i.e all segments/classes marked? - The assumption that s(3) is independent of s(1) and s(2) is not true. Instead of constraining the model to learn masks that respect the various types of labels, it seems they learn from each source independently. It is not clear how the sharing of parameters in the multitask block helps. - Can they comment on the applicability of the prior work suggested above? Minor comments: - How do the rough labeling tools work on biomedical data where the objects are more heterogenous patterns where different labels can have very different distribution of pixels. How well will their method generalize in such settings? - Can this work be used for segmentation and prediction on crop data? Results: - It seems as if the improvement over the PL baseline (pseudo labels) is incremental? Can the authors provide error bars so the reader knows what the significance of the results is? - Can they give a more thorough comparison in terms of human effort? It is interesting to note that only 2 images give 0.82. Would 3 images give 0.94? They need to show the trade-off between additional effort vs gains in performance. - What is the performance of MT U-net without the SL images (i.e without task-3)? Table-2 does give some intuition, but authors should add another row with multitask WL - Table-3: How well does MDUnet do with 9.4% SL data? ", "rating": "6: Weak Accept", "reply_text": "> > Can they comment on the applicability of the prior work suggested above ? The Label super-resolution networks assume that the label is given at the low-resolution level ( in the form of one label per block of the image ) and known distribution of high-resolution labels is conditioned on the label of the low-resolution block . Our work also introduces a certain kind of low resolution labels , in the sense that the boundaries of the WL are noisy and inaccurate . We make use of the observation that the pixels that are in the partial mask ( WL ) set a strict constraint for the values of the SL , and do not assume that the distribution of the SL given the WL is known . In practice , the annotation strategy is very different for this method compared to ours . Our WL gives more information as it includes details on separating and touching objects . This is not the case with the low-resolution level labels used by the Label super-resolution network . Therefore the mentioned prior work is not directly comparable to our approach . > > How do the rough labeling tools work on biomedical data where the objects are more heterogenous patterns Indeed , in the ice crystal and air bubble problem , the distributions of pixels are similar in the different classes , which makes this problem difficult to handle . Since our method provides accurate results on this problem , we believe the segmentation task should adapt easily to different distributions of pixels . Given the results we show on the Gland dataset ( Subsection 4.2 ) , we believe that our method should also generalize well to textured objects . At the same time , the detection should not be affected by heterogeneous patterns , but more extensive experiments would have to be conducted to validate this . > > Can this work be used for segmentation and prediction on crop data ? We are not familiar with prediction problems associated to crop data . Our work could be relevant in the case of crop image datasets containing small training sets . > > It seems as if the improvement over the PL baseline ( pseudo labels ) is incremental ? Can the authors provide error bars so the reader knows what the significance of the results is ? We have added error bars to compare the two methods ( see Figure 4 ) . Thanks for the suggestion . In addition to the comparisons provided in Table 1 and Figure 4 , we have demonstrated qualitative results in Figure 5 , which shows that our approach gives accurate predictions on the contours of the objects , and this is one of our main objectives in the work . > > Can they give a more thorough comparison in terms of human effort ? It is interesting to note that only 2 images give 0.82 . Would 3 images give 0.94 ? We included a comparison of the supervision from different amounts of SL in Table 2 . The $ 20\\ % $ SL ( $ 4 $ images ) , $ 30\\ % $ SL ( $ 6 $ images ) and $ 75\\ % $ SL ( $ 15 $ images ) give scores of $ 0.882 $ , $ 0.913 $ , $ 0.940 $ respectively . Given 6x speedup on WL annotation time , the creation of $ 10\\ % $ SL + WL is $ 3 $ times faster than $ 75\\ % $ SL which gives similar accuracy . > > What is the performance of MT U-net without the SL images ( i.e without task-3 ) ? The MT U-net without SL ( i.e. , without task 3 ) degenerates into the U-net with WL . In fact , according to the loss function ( 5 ) we used , the loss on task 3 is always zero provided no SL available . This contributes to nothing in backpropagation during the network training , and therefore MT U-net degenerates into the U-net with WL . We have provided the results on U-net with WL ( second row of table 1 ) , which therefore covers the results of MT U-net without task 3 . > > Table-3 : How well does MDUnet do with 9.4 % SL data ? The MDUnet is proposed in a fully supervised setting , and it is limited by the amount of SL , so is the single task U-net . We have demonstrated the relatively lower performance of the single task U-net ( in Table 3 , and also in Table 1 ) , and similarly a significant performance reduction could be expected for the MDUnet if one reduces the SL from $ 100\\ % $ to only $ 9.4\\ % $ ."}, "1": {"review_id": "r1glygHtDB-1", "review_text": "The submission presents a neural network for multi-task learning on sets of labeled data that are largely weakly supervised (in this case, partially segmented instances), augmented by comparatively fewer fully supervised annotations. The multiple tasks are designed to make use of both the weak as well as as full (\u2018strong\u2019) labels, such that performance on fully annotated machine-generated output is improved. As noted in the related work section (Section 2), multi-task methods aim to use benefits from underlying common information that may be ignored in a single-task setting. The network presented here is quite similar to most of these multi-task approaches: a common feature encoder, and partially distinct feature decoding and classification parts. The (minor) novelty mainly comes from the distinct types of weak/strong annotation data fed here: instance scribbles, boundary scribbles, and (some or few) full segmentations. The submission is overall well written and provides sufficient clarity and a good overview of the approach. Section 3 presents a probabilistic decomposition of the proposed architecture. With some fairly standard assumptions and simplifications, the loss in Eq. 3 becomes rather straightforward (weighted cross entropy) The actual network architecture described in Section 3.2 takes a standard U-Net as a starting point and modifies it in a fairly targeted way for the different expected types of annotations. These annotations (Section 3.3) are cheaper than full labels on a same-size dataset; it is not completely clear, however, if the mentioned scribbles need to capture each instance in the training set, or if some can also be left out. Without this being explicitly mentioned, I will assume the former. The experimental evaluation is done reasonably well, although I am not familiar with any of the presented data sets. The SES set seems to be specific to the submission, while the H&E data set has been used at least one other relevant publication (Zhang et al.). My main issue here is that at least on the SES set, which does not seem to be that large, the score difference is not that big, so dataset bias could play some part (which is unproven, but so is the opposite). Experimental evaluation does not leave the low-number-of-classes regime, and I\u2019m left wondering how the method might compare on a semantically much richer data set, e.g. Cityscapes. Finally, unmodified U-Net is by now a rather venerable baseline, so I\u2019m also wondering how the proposed multi-task learning could be used in other (more recent) architectures, i.e. whether the idea can be generalized sufficiently. While I think the ideas per se have relatively minor novelty, the combination seems novel to me, and that might warrant publication.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments and feedback . > > it is not completely clear , however , if the mentioned scribbles need to capture each instance in the training set , or if some can also be left out . We used different subsets of images ( i.e. , $ \\mathcal { I } _1 $ , $ \\mathcal { I } _2 $ and $ \\mathcal { I } _3 $ ) labeled for different tasks . Therefore , the scribbles ( WL ) do not need to capture every instance in the training sets . However , we do have an assumption that most of the images are weakly labelled . This is motivated by the fact that the WL are collected in a much cheaper way than the SL . As we mentioned in Section 3 , the missing labels ( either SL or WL ) are incorporated using a weighted loss function ( given by Equation ( 5 ) ) . > > Experimental evaluation does not leave the low-number-of-classes regime , and I \u2019 m left wondering how the method might compare on a semantically much richer data set , e.g.Cityscapes The successful examples we have shown make the proposed model relevant for a wide range of applications with a low number of classes ( for example biomedical imaging data ) and namely the practical problems we are working on . It would indeed be interesting to try the approach for a problem with a larger number of classes , but it would require specific data . Our method has been designed to address problems in which datasets have fewer images , not highly contrasted images and where finding very accurate contours is essential . Most semantically rich and large datasets , such as Cityscapes , have better contrast and the annotations are not always accurate enough . > > Finally , unmodified U-Net is by now a rather venerable baseline , so I \u2019 m also wondering how the proposed multi-task learning could be used in other ( more recent ) architectures , i.e.whether the idea can be generalized sufficiently . We have not explored all possible architectures in our main segmentation network , but the idea of decomposition of the segmentation problem into the mentioned smaller tasks is to a certain extent agnostic to the underlying architecture and readily applicable to a lot of different settings . The choice of network architectures has some flexibility depending on the problem at hand , and the multi-task learning block could be used in other architectures that have multiple levels of resolution design ( such as SegNet ) ."}, "2": {"review_id": "r1glygHtDB-2", "review_text": "This paper proposes a method for semantic segmentation using \"lazy\" segmentation labels. Lazy labels are defined as coarse labels of the segmented objects. The proposed method is a UNET trained in a multitask fashion whit 3 tasks: object detection, object separation, and object segmentation. The method is trained on 2 datasets: air bubbles, and ice crystals. The proposed method performs better than the same method using only the weakly supervised labels and the one that only uses the sparse labels. The novelty of the method is very limited. It is a multitask UNET. The method is compared with one method using pseudo labels. However, this method is not SOTA. Authors should compare with current methods such as: - Where are the Masks: Instance Segmentation with Image-level Supervision - Instance Segmentation with Point Supervision - Object counting and instance segmentation with image-level supervision - Weakly supervised instance segmentation using class peak response - Soft proposal networks for weakly supervised object localization - Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation These methods can use much less supervision (point-level, count-level or image-level) and may work even better. The method should be compared on standard and challenging datasets like Cityscapes, PASCAL VOC 2012, COCO, KITTI... ", "rating": "1: Reject", "reply_text": "> > The method should be compared on standard and challenging datasets like Cityscapes , PASCAL VOC 2012 , COCO , KITTI\u2026 There are various reasons why we think the current datasets are more suitable for this work . 1 ) .The mentioned standard datasets are about images from natural scenes and they generally have better contrast between the objects and the background compared to the datasets that we used . Indeed , one key objective of our work is to obtain accurate object contours for images in poor contrast ( for instance.images in Figure 9 ) . 2 ) .We wish to develop methods that are applicable for application domains where only relatively small training sets are available , as the ones demonstrated in the paper . 3 ) .We are focusing on problems where finding a precise segmentation is important . On the large datasets mentioned by the reviewer , the annotations are not always accurate enough : - Cityscapes Dataset : The legs of the man on the motorcycle in Example Weimar 1 in https : //www.cityscapes-dataset.com/examples/ - COCO 2019 : Legs of the tennis player in http : //cocodataset.org/ # detection-2019 - In KITTI , cars are often not segmented when touching image boundaries ( second line of Fig.1 page 16 of https : //arxiv.org/pdf/1604.05096.pdf , Fig 9 ( e ) in https : //arxiv.org/pdf/1908.11656.pdf )"}}