{"year": "2019", "forum": "ryxDjjCqtQ", "title": "Deconfounding Reinforcement Learning in Observational Settings", "decision": "Reject", "meta_review": "The paper studies RL based on data with confounders, where the confounders can affect both rewards and actions.  The setting is relevant in many problems and can have much potential.  This work is an interesting and useful attempt.  However, reviewers raised many questions regarding the problem setup and its comparison to related areas like causal inference.  While the author response provided further helpful details, the questions remained among the reviewers.  Therefore, the paper is not recommended for acceptance in its current stage; more work is needed to better motivate the setting and clarify its relation to other areas.\n\nFurthermore, the paper should probably discuss its relation to (1) partially observable MDP; and (2) off-policy RL.", "reviews": [{"review_id": "ryxDjjCqtQ-0", "review_text": "I have read the discussion from the authors. my evaluation stays the same. -------- this paper studies an interesting question of how to learn causal effects from observational data generated from reinforcement learning. they work with a very challenging setting where an unobserved confounder exists at each time step that affects actions, rewards and the confounder at next time step. the authors fit latent variables models to the observational data and perform experiments. the major concern is on the causal inference side, where it is not easy to claim anything causal in such a complicated system with unobserved confounders. causal inference with unobserved confounders cannot be simply solved by fitting a latent variable model. there exists negative examples even in the simplest setting that two distinct causal structure can lead to the same observational distribution. for example here, https://www.alexdamour.com/blog/public/2018/05/18/non-identification-in-latent-confounder-models/ it could be helpful if the authors can lay out the identification assumptions for causal effects. before claiming anything causal and justifying experimental results.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments . We have updated the paper and please refer to Section 2.4 , Section 3.3 , and Appendix A of the new version for the solution to identification ."}, {"review_id": "ryxDjjCqtQ-1", "review_text": "The paper addresses an important and often overlooked issue in off-policy reinforcement learning - the possibility of confounding between the agent's actions and the rewards. This is a subject which has been exhaustively explored in the causal inference literature, and the authors are very correct in suggesting that it should be incorporated into the world of reinforcement learning. Specifically they propose a generative model with a global latent confounder that is inferred using a variational autoencoder architecture. The paper is generally well-written, though some points could be made clearer in my opinion, as detailed below. The experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but I am not entirely sure whether the given architecture is necessary, see comments below. High-level comments: (1) Classic RL deals with confounders all the time. The state is a confounder between the action and the reward. The issue of confounding becomes less trivial when one is performing off-policy RL when the original policy is *unknown*. This is exactly the case that the authors mention when they cite the recent work by Gottesman et al. (2018) who deal with using RL to learn from the actions of physicians in a hospital. While I am sure the authors are aware of these distinctions, I think the paper would be better if this is spelled out very explicitly. This includes explaining why this issue doesn't come up in classic RL. (2) Assuming the case above - off-policy RL with unknown confounders - one would usually assume \"no unmeasured confounding\", i.e. that the observed actions are an unknown but learnable function of the observed states. That is basically the scenario of most off-policy RL. (3) However, the authors strive to go one step beyond the case (2), to a situation where there is an *unmeasured* confounder affecting both observed actions and rewards. If nothing is known about this unmeasured confounder, then it is generally impossible to learn effective policies, as the causal effects of actions are not identifiable from the observed data. In this paper, the authors make an implicit assumption that while the confounder is unmeasured, it can still be inferred from the data. This is an intermediate step between \"no unmeasured confounding\" and \"complete unmeasured confounding\". This is related to work on using proxy variables e.g. Kuroki & Pearl (2014) and even more closely related to the work cited by Louizos et al. (2017). Again, I think the paper would be much improved if all this is addressed explicitly. (4) An important consequence of point (3) above is that in fact adding the single global latent-confounder U is not, in itself, very important from a causal perspective. The sequence of variables Z_1... Z_T are already latent confounders that are assumed to be inferrable from data. It is true that the addition of the global U might change the statistical and optimization properties of the model. This leads to a very important conclusion: the authors should test their model with and without U. I think this specific ablation experiment is crucial. In many cases I am sure that the assumption of a global latent confounder is a good one and is especially useful in the VAE case where it will make optimization more stable. However, in principle, all of U's roles could be taken within the sequence of Z's, and I am curious to see in practice how big of an effect it has. (5) I wish to add that even if the U variable turns out to not add much empirically, this work is still valid since the sequence of Z's can themselves be considered inferred latent confounders. Specific comments: (1) 2.3: there are more than 2 ways of computing the do-operator. RCTs and backdoor are the best known approaches, but not the only ones, e.g. there is frontdoor adjustment. (2) I think the paper would be easier to follow if there was one concrete example used throughout. This will make it easier to understand and possibly verify/criticize the assumptions of the generative model. (3) Related to \"higher-level point (4)\" above, in eqs. 17 & 18 note that Z_t is unknown, same as U. Both are inferred. This also leads to the question which Z_t is actually used in practice? Is it the mean, or is it also sampled from the approximate posterior q? (4) Below eq. 19, it would be very useful for the readers if you could explain exactly when would there be a difference between the two versions p(r_{t+1}|z_t,a_t) and p(r_{t+1}|z_t, do(a_t=a)) (5) In the description of all the experiments I was missing a crucial point: how does the introduced confounder affect the reward? Is it only through the different actions? The way it is currently explained, it seems like the added variable introduces lack of *overlap*, but not strictly confounding. (6) The description of the experiment in 4.3 could be more detailed. What exactly was the training and test? What RL method was used? What did the baseline optimize for? I would like to see an ablation experiment where U is not included in the model. (7) In 4.5, what is the \"vanilla\" method? And as mentioned above, I would like to see an ablation experiment where U is not included in the model. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comment . We have updated the paper and solved all the issues you are concerned about . In the following , we will point out which part in our new draft answers your question , respectively . Nevertheless , we still recommend you re-read the full updated paper to get some refreshed stuff . Regarding High-level Comments : Re ( 1 ) : Refer to Abstract , Section 1 , and especially to the last paragraph of Appendix A in which we explained the reason why we consider only adjusting for the confounder u . Re ( 2 ) - ( 3 ) : Refer to Section 2.4 , Section 3.3 , and Appendix A for the solution to identification . Re ( 4-5 ) : Refer to the last paragraph of Appendix A for the difference between Z and u , and to Section 4.3 and Section 4.4 for both experiments with and without u . It is worth noting that , as shown in Figure 3 ( c ) , in each episode our deconfounding algorithm considering u almost chooses the optimal action at each time step , whilst the vanilla algorithm not considering u makes a wrong decision for more than half time . Regarding Specific Comments : Re ( 1 ) : Refer to Section 2.3 and Section 2.4 . Re ( 2 ) : The kidney stone example is used throughout the paper , referring to Section 1 , Section 2.1 , Section 2.2 , Section 3.1 , Footnote 2 , Appendix F.2 , and Appendix H.3 . Re ( 3 ) : Z , sampled using Equation ( 6 ) , has to be used in reinforcement learning algorithms , because we need the state transition when generating trajectories/rollout . Refer to Section 4.1 , Section 4.4 , and Appendix E. Re ( 4 ) : Refer to Appendix F.2 for an intuition of the difference , and to Section 4.4 in which , as shown in Figure 3 ( c ) , in each episode our deconfounding algorithm using p ( r_ { t+1 } |z_t , do ( a_t=a ) ) almost chooses the optimal action at each time step , whilst the vanilla algorithm using p ( r_ { t+1 } |z_t , a_t ) makes a wrong decision for more than half time . Re ( 5 ) : Refer to Section 4.2 and Appendix H.3 in which a straightforward analogy is provided as well . Re ( 6 ) : Refer to Section 4.3 . Re ( 7 ) : Refer to Section 4.4 ."}, {"review_id": "ryxDjjCqtQ-2", "review_text": "This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. First, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldn\u2019t make sense: in RL you get to choose the policy so it doesn\u2019t make sense to assume that the choice of action is confounded while you\u2019re doing RL. To get around this, the authors assume that they\u2019re working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they don\u2019t explain why you would want to do RL at all when you could just do causal inference directly. If you can\u2019t experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). # Method The authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. Quibbles: - Page 3: the authors claim the model is \u201cwithout loss of generality\u201d but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc. - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldn\u2019t all be factorized normal distributions. Real data isn\u2019t made up of factorized normals. # Experiments The authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so I\u2019ll only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. \u201cConfounding\u201d is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isn\u2019t affected directly by the latent variable. Because of this, there isn\u2019t actually a confounding problem - the \u201cconfounder\u201d simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). The RL evaluations aren\u2019t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions. # Summary This work studies a setting in which the correct baselines would be causal inference algorithms (but they aren\u2019t considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. ", "rating": "2: Strong rejection", "reply_text": "Thanks for your comments . We have updated the paper and made the paper much clearer . We recommend you re-read the full updated paper to get some refreshed stuff . Re motivation : Refer to Section 1 . Re the method : Refer to Section 2.4 , Section 3.3 , and Appendix A . Re quibbles : Refer to Section 3.1 and Section 3.4 . We have to emphasize that the factorization assumption in terms of Gaussian is a widely used technique in machine learning communities . I agree that the real data is not made up of factorized Gaussians , but the assumption of factorized Gaussian is the first step and also the easiest way to deeply understand how the proposed approach works . It is not necessary to get ourselves lost in the complicated distributions , which is not beneficial to capturing some insights about the nature of the model . More importantly , even with such simplified assumption , experiments presented in Section 4.3 and Section 4.4 show that our model work much better . Especially as shown in Figure 3 ( c ) , in each episode our deconfounding algorithm almost chooses the optimal action at each time step , whilst the vanilla algorithm makes a wrong decision for more than half time . Re experiments : Refer to the whole Section 4 and the corresponding appendices . Note that , in this new draft we include all the details about the experiments , especially about how to design a reasonable reward and Appendix H.3 also provides a straightforward analogy to help readers understand the design . As mentioned above , Figure 3 ( c ) demonstrated that the confounder u plays an extremely important role in reinforcement learning algorithms , because in that experiment , in each episode our deconfounding RL algorithm almost chooses the optimal action at each time step , whilst the vanilla RL algorithm makes a wrong decision for more than half time ."}], "0": {"review_id": "ryxDjjCqtQ-0", "review_text": "I have read the discussion from the authors. my evaluation stays the same. -------- this paper studies an interesting question of how to learn causal effects from observational data generated from reinforcement learning. they work with a very challenging setting where an unobserved confounder exists at each time step that affects actions, rewards and the confounder at next time step. the authors fit latent variables models to the observational data and perform experiments. the major concern is on the causal inference side, where it is not easy to claim anything causal in such a complicated system with unobserved confounders. causal inference with unobserved confounders cannot be simply solved by fitting a latent variable model. there exists negative examples even in the simplest setting that two distinct causal structure can lead to the same observational distribution. for example here, https://www.alexdamour.com/blog/public/2018/05/18/non-identification-in-latent-confounder-models/ it could be helpful if the authors can lay out the identification assumptions for causal effects. before claiming anything causal and justifying experimental results.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments . We have updated the paper and please refer to Section 2.4 , Section 3.3 , and Appendix A of the new version for the solution to identification ."}, "1": {"review_id": "ryxDjjCqtQ-1", "review_text": "The paper addresses an important and often overlooked issue in off-policy reinforcement learning - the possibility of confounding between the agent's actions and the rewards. This is a subject which has been exhaustively explored in the causal inference literature, and the authors are very correct in suggesting that it should be incorporated into the world of reinforcement learning. Specifically they propose a generative model with a global latent confounder that is inferred using a variational autoencoder architecture. The paper is generally well-written, though some points could be made clearer in my opinion, as detailed below. The experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but I am not entirely sure whether the given architecture is necessary, see comments below. High-level comments: (1) Classic RL deals with confounders all the time. The state is a confounder between the action and the reward. The issue of confounding becomes less trivial when one is performing off-policy RL when the original policy is *unknown*. This is exactly the case that the authors mention when they cite the recent work by Gottesman et al. (2018) who deal with using RL to learn from the actions of physicians in a hospital. While I am sure the authors are aware of these distinctions, I think the paper would be better if this is spelled out very explicitly. This includes explaining why this issue doesn't come up in classic RL. (2) Assuming the case above - off-policy RL with unknown confounders - one would usually assume \"no unmeasured confounding\", i.e. that the observed actions are an unknown but learnable function of the observed states. That is basically the scenario of most off-policy RL. (3) However, the authors strive to go one step beyond the case (2), to a situation where there is an *unmeasured* confounder affecting both observed actions and rewards. If nothing is known about this unmeasured confounder, then it is generally impossible to learn effective policies, as the causal effects of actions are not identifiable from the observed data. In this paper, the authors make an implicit assumption that while the confounder is unmeasured, it can still be inferred from the data. This is an intermediate step between \"no unmeasured confounding\" and \"complete unmeasured confounding\". This is related to work on using proxy variables e.g. Kuroki & Pearl (2014) and even more closely related to the work cited by Louizos et al. (2017). Again, I think the paper would be much improved if all this is addressed explicitly. (4) An important consequence of point (3) above is that in fact adding the single global latent-confounder U is not, in itself, very important from a causal perspective. The sequence of variables Z_1... Z_T are already latent confounders that are assumed to be inferrable from data. It is true that the addition of the global U might change the statistical and optimization properties of the model. This leads to a very important conclusion: the authors should test their model with and without U. I think this specific ablation experiment is crucial. In many cases I am sure that the assumption of a global latent confounder is a good one and is especially useful in the VAE case where it will make optimization more stable. However, in principle, all of U's roles could be taken within the sequence of Z's, and I am curious to see in practice how big of an effect it has. (5) I wish to add that even if the U variable turns out to not add much empirically, this work is still valid since the sequence of Z's can themselves be considered inferred latent confounders. Specific comments: (1) 2.3: there are more than 2 ways of computing the do-operator. RCTs and backdoor are the best known approaches, but not the only ones, e.g. there is frontdoor adjustment. (2) I think the paper would be easier to follow if there was one concrete example used throughout. This will make it easier to understand and possibly verify/criticize the assumptions of the generative model. (3) Related to \"higher-level point (4)\" above, in eqs. 17 & 18 note that Z_t is unknown, same as U. Both are inferred. This also leads to the question which Z_t is actually used in practice? Is it the mean, or is it also sampled from the approximate posterior q? (4) Below eq. 19, it would be very useful for the readers if you could explain exactly when would there be a difference between the two versions p(r_{t+1}|z_t,a_t) and p(r_{t+1}|z_t, do(a_t=a)) (5) In the description of all the experiments I was missing a crucial point: how does the introduced confounder affect the reward? Is it only through the different actions? The way it is currently explained, it seems like the added variable introduces lack of *overlap*, but not strictly confounding. (6) The description of the experiment in 4.3 could be more detailed. What exactly was the training and test? What RL method was used? What did the baseline optimize for? I would like to see an ablation experiment where U is not included in the model. (7) In 4.5, what is the \"vanilla\" method? And as mentioned above, I would like to see an ablation experiment where U is not included in the model. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comment . We have updated the paper and solved all the issues you are concerned about . In the following , we will point out which part in our new draft answers your question , respectively . Nevertheless , we still recommend you re-read the full updated paper to get some refreshed stuff . Regarding High-level Comments : Re ( 1 ) : Refer to Abstract , Section 1 , and especially to the last paragraph of Appendix A in which we explained the reason why we consider only adjusting for the confounder u . Re ( 2 ) - ( 3 ) : Refer to Section 2.4 , Section 3.3 , and Appendix A for the solution to identification . Re ( 4-5 ) : Refer to the last paragraph of Appendix A for the difference between Z and u , and to Section 4.3 and Section 4.4 for both experiments with and without u . It is worth noting that , as shown in Figure 3 ( c ) , in each episode our deconfounding algorithm considering u almost chooses the optimal action at each time step , whilst the vanilla algorithm not considering u makes a wrong decision for more than half time . Regarding Specific Comments : Re ( 1 ) : Refer to Section 2.3 and Section 2.4 . Re ( 2 ) : The kidney stone example is used throughout the paper , referring to Section 1 , Section 2.1 , Section 2.2 , Section 3.1 , Footnote 2 , Appendix F.2 , and Appendix H.3 . Re ( 3 ) : Z , sampled using Equation ( 6 ) , has to be used in reinforcement learning algorithms , because we need the state transition when generating trajectories/rollout . Refer to Section 4.1 , Section 4.4 , and Appendix E. Re ( 4 ) : Refer to Appendix F.2 for an intuition of the difference , and to Section 4.4 in which , as shown in Figure 3 ( c ) , in each episode our deconfounding algorithm using p ( r_ { t+1 } |z_t , do ( a_t=a ) ) almost chooses the optimal action at each time step , whilst the vanilla algorithm using p ( r_ { t+1 } |z_t , a_t ) makes a wrong decision for more than half time . Re ( 5 ) : Refer to Section 4.2 and Appendix H.3 in which a straightforward analogy is provided as well . Re ( 6 ) : Refer to Section 4.3 . Re ( 7 ) : Refer to Section 4.4 ."}, "2": {"review_id": "ryxDjjCqtQ-2", "review_text": "This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. First, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldn\u2019t make sense: in RL you get to choose the policy so it doesn\u2019t make sense to assume that the choice of action is confounded while you\u2019re doing RL. To get around this, the authors assume that they\u2019re working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they don\u2019t explain why you would want to do RL at all when you could just do causal inference directly. If you can\u2019t experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). # Method The authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. Quibbles: - Page 3: the authors claim the model is \u201cwithout loss of generality\u201d but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc. - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldn\u2019t all be factorized normal distributions. Real data isn\u2019t made up of factorized normals. # Experiments The authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so I\u2019ll only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. \u201cConfounding\u201d is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isn\u2019t affected directly by the latent variable. Because of this, there isn\u2019t actually a confounding problem - the \u201cconfounder\u201d simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). The RL evaluations aren\u2019t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions. # Summary This work studies a setting in which the correct baselines would be causal inference algorithms (but they aren\u2019t considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. ", "rating": "2: Strong rejection", "reply_text": "Thanks for your comments . We have updated the paper and made the paper much clearer . We recommend you re-read the full updated paper to get some refreshed stuff . Re motivation : Refer to Section 1 . Re the method : Refer to Section 2.4 , Section 3.3 , and Appendix A . Re quibbles : Refer to Section 3.1 and Section 3.4 . We have to emphasize that the factorization assumption in terms of Gaussian is a widely used technique in machine learning communities . I agree that the real data is not made up of factorized Gaussians , but the assumption of factorized Gaussian is the first step and also the easiest way to deeply understand how the proposed approach works . It is not necessary to get ourselves lost in the complicated distributions , which is not beneficial to capturing some insights about the nature of the model . More importantly , even with such simplified assumption , experiments presented in Section 4.3 and Section 4.4 show that our model work much better . Especially as shown in Figure 3 ( c ) , in each episode our deconfounding algorithm almost chooses the optimal action at each time step , whilst the vanilla algorithm makes a wrong decision for more than half time . Re experiments : Refer to the whole Section 4 and the corresponding appendices . Note that , in this new draft we include all the details about the experiments , especially about how to design a reasonable reward and Appendix H.3 also provides a straightforward analogy to help readers understand the design . As mentioned above , Figure 3 ( c ) demonstrated that the confounder u plays an extremely important role in reinforcement learning algorithms , because in that experiment , in each episode our deconfounding RL algorithm almost chooses the optimal action at each time step , whilst the vanilla RL algorithm makes a wrong decision for more than half time ."}}