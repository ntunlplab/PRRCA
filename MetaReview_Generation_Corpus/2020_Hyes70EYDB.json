{"year": "2020", "forum": "Hyes70EYDB", "title": "Visual Interpretability Alone Helps Adversarial Robustness", "decision": "Reject", "meta_review": "This work focuses on how one can design models with robustness of interpretations. While this is an interesting direction, the paper would benefit from a more careful treatment of its technical claims.\n\n", "reviews": [{"review_id": "Hyes70EYDB-0", "review_text": "In summary, this paper studies if interpretation robustness (i.e., similar examples should have similar interpretation) can help enhance the robustness of the model, especially in terms of adversarial attacks. The study direction itself is interesting and very useful for the interpretation and adversarial attack community. Moreover, some promising results can be observed in part of the empirical study. However, this paper can be improved a lot as follows. 1. This paper states several times that \"adversarial examples can be hidden from neural network interpretability\". It is not clear on the definition of \"hidden\" in terms of \"interpretability\". Therefore, how this \"hidden\" is related and why this \"hidden\" is important are unclear too. 2. Many details are missing, which makes the proposal suspicious. For example, the proposed method has a tradeoff parameter \\lambda. However, the settings and affects are not discussed at all. Without a clear setup, the reproducibility and applicability is in doubt. 3. Some empirical results are overstated. For example, why 0.790 vs 0.890 and 0.270 vs 0.170 are comparable results? These results show the weakness of the proposed method. Further explanations can be provided. From the reported results, it could be useful to see results when the perturbation is even higher to check the limitation of the proposed method. 4. Besides the clarification in the writing mentioned above, some typos or errors should be fixed, e.g., f_t'(x') - - f_t(x') >=0 in the proof of proposition 1.", "rating": "3: Weak Reject", "reply_text": "# Question : 1 . This paper states several times that `` adversarial examples can be hidden from neural network interpretability '' . It is not clear on the definition of `` hidden '' in terms of `` interpretability '' . Therefore , how this `` hidden '' is related and why this `` hidden '' is important are unclear too. # Response : The definition of \u201c hidden \u201d was borrowed from ( Zhang et al. , 2018 ; Subramanya et al. , 2018 ) , which refers to the interpretation map of an adversarial example generated from Interpretability Sneaking Attack ( ISA ) being visually similar to the original interpretation map of the benign example . We quantify this \u201c hidden \u201d effect with the Kendall \u2019 s Tau order rank correlation between interpretation maps before and after performing ISA . As a visual example , Figure 2c shows that the conventional ISA generated under 1-class interpretability discrepancy ( namely , $ \\ell_1 $ 1-class ISA ) minimizes the interpretability discrepancy w.r.t.the true label $ t $ only , supported by a high correlation value 0.7107 . If ISA is a real threat , then it could create confusion between the model interpreter and the classifier , and it could further confuse AI systems which use network interpretations in down-stream actions , e.g. , medical recommendation ( G Quellec , et al. , 2017 ) and transfer learning ( Shafahi et al. , 2019 ) . Thus , we think that studying the \u201c hidden \u201d effect of ISA is an important aspect to explore the relationship between network interpretability and robust classification . However , the main theme of this paper is not to make a claim that adversarial examples are able to be hidden from network interpretability checkers . Instead , we would like to have a deeper understanding on the plausibility of ISA . We found that ISA can fail when the 2-class interpretability discrepancy was examined ; e.g. , Figure 2c column 3 versus column 1 ( compared to column 4 versus column 1 ) . Our main point for ISA is that hiding adversarial attack from network interpretation is actually challenging . Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations . Xinyang Zhang , et al. , Interpretable deep learning under fire . Akshayvarun Subramanya , et al. , Towards hiding adversarial examples from network interpretation . arXiv preprint arXiv:1812.02843 , 2018 . G Quellec , et al.Deep image mining for diabetic retinopathy screening . Medical image analysis , 39:178 , 2017 . Shafahi , Ali , et al . `` Adversarially robust transfer learning . '' arXiv preprint arXiv:1905.08232 ( 2019 ) . # Question : 2 . Many details are missing , which makes the proposal suspicious . For example , the proposed method has a tradeoff parameter \\lambda . However , the settings and effects are not discussed at all . Without a clear setup , the reproducibility and applicability is in doubt. # Response : We apologize for the missing details such that you felt our proposal 'suspicious \u2019 . In Sec.5 and Appendix B-D , we have tried our best to present a clear experiment setup . The parameter $ \\lambda $ used during finding ISA controls the tradeoff between an attack changing the network classification and minimizing the interpretability discrepancy . As explained in section 3.1 , $ \\lambda $ is chosen using the bisection method to find a successful attack with the minimum interpretability discrepancy . And we have conducted additional experiments on the training regularization parameter $ \\gamma $ . As we can see in Appendix C , Figure A1 , $ \\gamma $ controls the tradeoff between clean accuracy and adversarial test accuracy . The value of $ \\gamma $ chosen for our experiments ( $ \\gamma=0.01 $ ) is the value yielding the highest robust accuracy on the model tested . Our results indicate that $ \\gamma $ can be chosen to smoothly interpolate between normal training and maximally robust interpretability-aware training . We have also released our code for reviewer \u2019 s reproducibility and applicability check ."}, {"review_id": "Hyes70EYDB-1", "review_text": "Interesting work and good contribution #Summary: The paper demonstrated that by having an l1-norm based 2-class interpretability discrepancy measure, it can be shown both empirically and theoretically that it is actually difficult to hide adversarial examples. Furthermore, the authors propose an interpretability-aware robust training method and show it can be used to successfully defend adversarially attacks and can result in comparable performance compared to adversarial training. #Strength The paper is well written and structured, with a clear demonstration of technical details. Compared with other works that tried to use model interpretation to help improve the model\u2019s robustness, the authors not only consider the saliency map computed for the actual target label but also the label that corresponds to the adversarial example. The proposed interpretability discrepancy measure is novel and has been proven effective to defend interpretability sneaking attacks that aiming to fool both classifiers and detectors and against interpretability-only attacks. Furthermore, extensive experiments have been done to prove the effectiveness of interpretability-aware training, which strengthens the claims of the entire paper. #Presentation Good coverage of the literature in both adversarial robustness and model interpretation. Some minor typos need to be fixed. For example, in the second last line of the caption of Figure. 2, one L(x\u2019,i) should be L(x,i) if I understand correctly. ", "rating": "6: Weak Accept", "reply_text": "We thank reviewer # 2 very much for their positive comments and for accurately summarizing our key contributions . We are very glad to learn that the reviewer found the topic interesting and our study meaningful . Thanks for carefully checking our presentation . We have fixed the typo in Figure 2 : Yes , the first $ \\mathbf x^\\prime $ should be $ \\mathbf x $ . We have also carefully revised our paper for a better version ."}, {"review_id": "Hyes70EYDB-2", "review_text": "The present work considers adversarial attacks that also yield similar outputs for \"interpretability methods\", which are methods that output some vector corresponding to a given classification (usually the vector is e.g. an image or a similar object). It also shows that by regularizing nearby inputs to have similar interpetations (instead of similar classifications), robustness can be achieved similar to adversarial training. I did not understand the motivation of the paper. Why is it important for adversarial attacks to yield similar interpretations? A human would need to assess the interpretations to detect the attack, but it would already be trivial for an attack to be detected given human oversight (just check whether the classification of the image matches the human-assigned label). It also wasn't clear how this was related to the other observation that regularizing based on interpretability yields robustness; these seem like two fairly separate results. Finally, I found the claim that \"interpretability alone helps robustness\" to be misleading and not substantiated by the paper. The purported justification is that regularizing nearby inputs to have the same interpretation yields robustness. But a better summary of this observation is that \"robustness of interpretability implies robustness of classification\", which is not surprising, and is in fact a trivial corollary of the fact that the metric on interpretations dominates the classification error metric (an observation which is made in the paper). More minor, but I found it hard to follow the writing in the paper (this is related to the motivation being unclear). This is exacerbated by the paper being longer than unusual (10 pages instead of 8).", "rating": "3: Weak Reject", "reply_text": "We thank reviewer # 3 for the valuable comments . # General response on the motivation of our work # In what follows , we would like to further clarify the motivation of our work . The manuscript is updated to make our points clearer . The primary motivation of our work is to investigate the relationship between network interpretability and adversarial robustness . Based on previous literature , there are seemingly two possible hypotheses for how robust interpretability affects adversarial robustness of classification : ( a ) From an attack perspective ( Zhang et al. , 2018 ; Subramanya et al. , 2018 ) , robust interpretability does not significantly help robust classification since empirically adversarial examples with a small interpretation discrepancy may have a large classification error . ( b ) From a defense perspective ( Chen et al. , 2019 ) , robust interpretability helps robust classification since robust interpretations make aspects of the network less sensitive to small perturbations . We aim to make a unified answer to the debate between ( a ) and ( b ) . We found that the choice of interpretability discrepancy matters when drawing conclusions . We showed that with an appropriate choice of interpretability discrepancy ( namely , the proposed $ \\ell_1 $ norm based 2-class measure ) , the claim ( a ) may NOT be correct since hiding adversarial examples from network interpretation could be difficult ( see Prop.1 and the corresponding detailed examples and analysis in Sec.3.1 ) .Moreover , our interpretability-aware robust training results provide a positive answer to ( b ) : robust interpretability does help robust classification by solely penalizing the proposed interpretability discrepancy ( see Sec.4 ) during training but the choice of interpretability discrepancy again matters . References : Xinyang Zhang , et al. , Interpretable deep learning under fire . Akshayvarun Subramanya , et al. , Towards hiding adversarial examples from network interpretation . arXiv preprint arXiv:1812.02843 , 2018 . J. Chen , et al. , Robust attribution regularization , NeurIPS , 2019 . # Question : \u201c Why is it important for adversarial attacks to yield similar interpretations ? \u201d # Response : Adversarial attacks that yield similar interpretations is a recent threat model ( we call interpretability sneaking attack ( ISA ) ) proposed by ( Zhang et al. , 2018 ; Subramanya et al. , 2018 ) , which showed that there exist adversarially chosen perturbations that fool an image classifier but minimize discrepancy of corresponding interpretation maps . If these perturbations are possible , then it could create confusion between the model interpreter and the classifier , and it could further confuse AI systems which use network interpretations in down-stream actions , e.g. , transfer learning ( Shafahi et al.2019 ) and medical recommendation ( G Quellec , et al . ) . Thus , we believe that ISA is a practical attacking scenario to study the relationship between network interpretation and classification . However , the goal of this paper is not to make a claim that ISA is a strong attack . Instead , we would like to have a deeper understanding on the plausibility of ISA . We found that ISA can fail when the 2-class interpretability discrepancy was examined ; see Figure 2 and more results in experiments . This showed that hiding adversarial attack from network interpretation is challenging . Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations . G Quellec , et al.Deep image mining for diabetic retinopathy screening . Medical image analysis , 39:178 , 2017 . Shafahi , Ali , et al . `` Adversarially robust transfer learning . '' arXiv preprint arXiv:1905.08232 ( 2019 ) ."}], "0": {"review_id": "Hyes70EYDB-0", "review_text": "In summary, this paper studies if interpretation robustness (i.e., similar examples should have similar interpretation) can help enhance the robustness of the model, especially in terms of adversarial attacks. The study direction itself is interesting and very useful for the interpretation and adversarial attack community. Moreover, some promising results can be observed in part of the empirical study. However, this paper can be improved a lot as follows. 1. This paper states several times that \"adversarial examples can be hidden from neural network interpretability\". It is not clear on the definition of \"hidden\" in terms of \"interpretability\". Therefore, how this \"hidden\" is related and why this \"hidden\" is important are unclear too. 2. Many details are missing, which makes the proposal suspicious. For example, the proposed method has a tradeoff parameter \\lambda. However, the settings and affects are not discussed at all. Without a clear setup, the reproducibility and applicability is in doubt. 3. Some empirical results are overstated. For example, why 0.790 vs 0.890 and 0.270 vs 0.170 are comparable results? These results show the weakness of the proposed method. Further explanations can be provided. From the reported results, it could be useful to see results when the perturbation is even higher to check the limitation of the proposed method. 4. Besides the clarification in the writing mentioned above, some typos or errors should be fixed, e.g., f_t'(x') - - f_t(x') >=0 in the proof of proposition 1.", "rating": "3: Weak Reject", "reply_text": "# Question : 1 . This paper states several times that `` adversarial examples can be hidden from neural network interpretability '' . It is not clear on the definition of `` hidden '' in terms of `` interpretability '' . Therefore , how this `` hidden '' is related and why this `` hidden '' is important are unclear too. # Response : The definition of \u201c hidden \u201d was borrowed from ( Zhang et al. , 2018 ; Subramanya et al. , 2018 ) , which refers to the interpretation map of an adversarial example generated from Interpretability Sneaking Attack ( ISA ) being visually similar to the original interpretation map of the benign example . We quantify this \u201c hidden \u201d effect with the Kendall \u2019 s Tau order rank correlation between interpretation maps before and after performing ISA . As a visual example , Figure 2c shows that the conventional ISA generated under 1-class interpretability discrepancy ( namely , $ \\ell_1 $ 1-class ISA ) minimizes the interpretability discrepancy w.r.t.the true label $ t $ only , supported by a high correlation value 0.7107 . If ISA is a real threat , then it could create confusion between the model interpreter and the classifier , and it could further confuse AI systems which use network interpretations in down-stream actions , e.g. , medical recommendation ( G Quellec , et al. , 2017 ) and transfer learning ( Shafahi et al. , 2019 ) . Thus , we think that studying the \u201c hidden \u201d effect of ISA is an important aspect to explore the relationship between network interpretability and robust classification . However , the main theme of this paper is not to make a claim that adversarial examples are able to be hidden from network interpretability checkers . Instead , we would like to have a deeper understanding on the plausibility of ISA . We found that ISA can fail when the 2-class interpretability discrepancy was examined ; e.g. , Figure 2c column 3 versus column 1 ( compared to column 4 versus column 1 ) . Our main point for ISA is that hiding adversarial attack from network interpretation is actually challenging . Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations . Xinyang Zhang , et al. , Interpretable deep learning under fire . Akshayvarun Subramanya , et al. , Towards hiding adversarial examples from network interpretation . arXiv preprint arXiv:1812.02843 , 2018 . G Quellec , et al.Deep image mining for diabetic retinopathy screening . Medical image analysis , 39:178 , 2017 . Shafahi , Ali , et al . `` Adversarially robust transfer learning . '' arXiv preprint arXiv:1905.08232 ( 2019 ) . # Question : 2 . Many details are missing , which makes the proposal suspicious . For example , the proposed method has a tradeoff parameter \\lambda . However , the settings and effects are not discussed at all . Without a clear setup , the reproducibility and applicability is in doubt. # Response : We apologize for the missing details such that you felt our proposal 'suspicious \u2019 . In Sec.5 and Appendix B-D , we have tried our best to present a clear experiment setup . The parameter $ \\lambda $ used during finding ISA controls the tradeoff between an attack changing the network classification and minimizing the interpretability discrepancy . As explained in section 3.1 , $ \\lambda $ is chosen using the bisection method to find a successful attack with the minimum interpretability discrepancy . And we have conducted additional experiments on the training regularization parameter $ \\gamma $ . As we can see in Appendix C , Figure A1 , $ \\gamma $ controls the tradeoff between clean accuracy and adversarial test accuracy . The value of $ \\gamma $ chosen for our experiments ( $ \\gamma=0.01 $ ) is the value yielding the highest robust accuracy on the model tested . Our results indicate that $ \\gamma $ can be chosen to smoothly interpolate between normal training and maximally robust interpretability-aware training . We have also released our code for reviewer \u2019 s reproducibility and applicability check ."}, "1": {"review_id": "Hyes70EYDB-1", "review_text": "Interesting work and good contribution #Summary: The paper demonstrated that by having an l1-norm based 2-class interpretability discrepancy measure, it can be shown both empirically and theoretically that it is actually difficult to hide adversarial examples. Furthermore, the authors propose an interpretability-aware robust training method and show it can be used to successfully defend adversarially attacks and can result in comparable performance compared to adversarial training. #Strength The paper is well written and structured, with a clear demonstration of technical details. Compared with other works that tried to use model interpretation to help improve the model\u2019s robustness, the authors not only consider the saliency map computed for the actual target label but also the label that corresponds to the adversarial example. The proposed interpretability discrepancy measure is novel and has been proven effective to defend interpretability sneaking attacks that aiming to fool both classifiers and detectors and against interpretability-only attacks. Furthermore, extensive experiments have been done to prove the effectiveness of interpretability-aware training, which strengthens the claims of the entire paper. #Presentation Good coverage of the literature in both adversarial robustness and model interpretation. Some minor typos need to be fixed. For example, in the second last line of the caption of Figure. 2, one L(x\u2019,i) should be L(x,i) if I understand correctly. ", "rating": "6: Weak Accept", "reply_text": "We thank reviewer # 2 very much for their positive comments and for accurately summarizing our key contributions . We are very glad to learn that the reviewer found the topic interesting and our study meaningful . Thanks for carefully checking our presentation . We have fixed the typo in Figure 2 : Yes , the first $ \\mathbf x^\\prime $ should be $ \\mathbf x $ . We have also carefully revised our paper for a better version ."}, "2": {"review_id": "Hyes70EYDB-2", "review_text": "The present work considers adversarial attacks that also yield similar outputs for \"interpretability methods\", which are methods that output some vector corresponding to a given classification (usually the vector is e.g. an image or a similar object). It also shows that by regularizing nearby inputs to have similar interpetations (instead of similar classifications), robustness can be achieved similar to adversarial training. I did not understand the motivation of the paper. Why is it important for adversarial attacks to yield similar interpretations? A human would need to assess the interpretations to detect the attack, but it would already be trivial for an attack to be detected given human oversight (just check whether the classification of the image matches the human-assigned label). It also wasn't clear how this was related to the other observation that regularizing based on interpretability yields robustness; these seem like two fairly separate results. Finally, I found the claim that \"interpretability alone helps robustness\" to be misleading and not substantiated by the paper. The purported justification is that regularizing nearby inputs to have the same interpretation yields robustness. But a better summary of this observation is that \"robustness of interpretability implies robustness of classification\", which is not surprising, and is in fact a trivial corollary of the fact that the metric on interpretations dominates the classification error metric (an observation which is made in the paper). More minor, but I found it hard to follow the writing in the paper (this is related to the motivation being unclear). This is exacerbated by the paper being longer than unusual (10 pages instead of 8).", "rating": "3: Weak Reject", "reply_text": "We thank reviewer # 3 for the valuable comments . # General response on the motivation of our work # In what follows , we would like to further clarify the motivation of our work . The manuscript is updated to make our points clearer . The primary motivation of our work is to investigate the relationship between network interpretability and adversarial robustness . Based on previous literature , there are seemingly two possible hypotheses for how robust interpretability affects adversarial robustness of classification : ( a ) From an attack perspective ( Zhang et al. , 2018 ; Subramanya et al. , 2018 ) , robust interpretability does not significantly help robust classification since empirically adversarial examples with a small interpretation discrepancy may have a large classification error . ( b ) From a defense perspective ( Chen et al. , 2019 ) , robust interpretability helps robust classification since robust interpretations make aspects of the network less sensitive to small perturbations . We aim to make a unified answer to the debate between ( a ) and ( b ) . We found that the choice of interpretability discrepancy matters when drawing conclusions . We showed that with an appropriate choice of interpretability discrepancy ( namely , the proposed $ \\ell_1 $ norm based 2-class measure ) , the claim ( a ) may NOT be correct since hiding adversarial examples from network interpretation could be difficult ( see Prop.1 and the corresponding detailed examples and analysis in Sec.3.1 ) .Moreover , our interpretability-aware robust training results provide a positive answer to ( b ) : robust interpretability does help robust classification by solely penalizing the proposed interpretability discrepancy ( see Sec.4 ) during training but the choice of interpretability discrepancy again matters . References : Xinyang Zhang , et al. , Interpretable deep learning under fire . Akshayvarun Subramanya , et al. , Towards hiding adversarial examples from network interpretation . arXiv preprint arXiv:1812.02843 , 2018 . J. Chen , et al. , Robust attribution regularization , NeurIPS , 2019 . # Question : \u201c Why is it important for adversarial attacks to yield similar interpretations ? \u201d # Response : Adversarial attacks that yield similar interpretations is a recent threat model ( we call interpretability sneaking attack ( ISA ) ) proposed by ( Zhang et al. , 2018 ; Subramanya et al. , 2018 ) , which showed that there exist adversarially chosen perturbations that fool an image classifier but minimize discrepancy of corresponding interpretation maps . If these perturbations are possible , then it could create confusion between the model interpreter and the classifier , and it could further confuse AI systems which use network interpretations in down-stream actions , e.g. , transfer learning ( Shafahi et al.2019 ) and medical recommendation ( G Quellec , et al . ) . Thus , we believe that ISA is a practical attacking scenario to study the relationship between network interpretation and classification . However , the goal of this paper is not to make a claim that ISA is a strong attack . Instead , we would like to have a deeper understanding on the plausibility of ISA . We found that ISA can fail when the 2-class interpretability discrepancy was examined ; see Figure 2 and more results in experiments . This showed that hiding adversarial attack from network interpretation is challenging . Its difficulty relies on how one measures the interpretability discrepancy caused by input perturbations . G Quellec , et al.Deep image mining for diabetic retinopathy screening . Medical image analysis , 39:178 , 2017 . Shafahi , Ali , et al . `` Adversarially robust transfer learning . '' arXiv preprint arXiv:1905.08232 ( 2019 ) ."}}