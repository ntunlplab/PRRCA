{"year": "2020", "forum": "r1g6MCEtwr", "title": "Zero-Shot Out-of-Distribution Detection with Feature Correlations", "decision": "Reject", "meta_review": "The paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. This is an important problem that is of general interest in our community.\n\nReviewer 2 found the paper to be clear, provided a set of weaknesses relating to lack of explanations of performance and more careful ablations, along with a set of strategies to address them. Reviewer 1 recognized the importance of being useful for pretrained networks but also raised questions of explanation and theoretical motivations. Reviewer 3 was extremely supportive, used the authors' code to highlight the difference between far-from-distribution behaviour versus near-distribution OOD examples. The authors provided detailed responses to all points raised and provided additional eidence. There was  no convergence of the review recommendations.\n\nThe review added much more clarity to the paper and it is no a better paper. The paper demonstrates all the features of a good paper, but unfortunately didn't yet reach the level for acceptance for the next conference. ", "reviews": [{"review_id": "r1g6MCEtwr-0", "review_text": "This paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. The method only requires in-distribution data for tuning its hyper-parameters and can use a pre-trained classifier directly. Its performance is evaluated with small image datasets, which are commonly used in this line of work. Two additional experiments are performed to analyze the effect of two factors (the layer of the feature and the order of Gram matrix) in the method. The overall vote for this paper is a weak reject. The clarity of the paper is good. The primary strength of the work is the very strong performance given the setting, which does not require OOD data for tuning. However, we give a rating slightly below the borderline, because of lack of in-depth explanation of why pairwise feature correlation is helpful for the problem. The explanation could be either theoretical or empirical, while the latter can be a set of carefully designed ablation studies. We do not see sufficient arguments or experiments to ensure the performance gain comes from the pairwise feature and is not from other parts of the design. The above weakness could be addressed by answering the following questions: 1. If the pairwise feature (G) is replaced by the unary feature (F) while keeping the other parts of the pipeline the same (including the use of p-th power), would it reach a similar performance or much worse? 2. Why does the implementation use the statistics of min/max values instead of mean and variance? The latter can also calculate the deviation for the method using a Gaussian model, which is usually a more natural choice. 3. What is the motivation for using p-th power, and why does it help? We present the additional feedbacks in the form of questions which could further strengthen the work if answered: 4. Based on Figure 2, having p=10 is sufficient to give a good result. Why does the method still use all p-th power (p=1..10)? 5. The study in Figure 1 only presents specific OOD data (Tiny Imagenet). Does the trend still hold with other OOD data, such as SVHN or LSUN? 6. Why are the networks used in figure 1(a) and (b) different? Does the trend still hold when the networks in both cases are changed? 7. The statistics (Mins/Maxs) to be saved can be very large. The paper provides a strategy to reduce that by using the row-wise sums of G. However, the summation has an effect of mixing the features, causing a weaker signal from the deviation. Why does the method still works with the row-wise sums? 8. Would the method work if the networks have no batch normalization layer? ", "rating": "3: Weak Reject", "reply_text": "We thank R2 for their careful reading , thoughtful comments , and also for phrasing their review in such a way that gives us the opportunity to answer some of these questions . We further appreciate R2 \u2019 s openness to either empirical or theoretical justification . Our general approach is to provide considerably more empirical results at the moment , along with some intuition and interpretation of those results . In particular , R2 provided us with 3 focused questions to understand better where the performance gain might be coming from , and the ( most welcome ) suggestion that these questions might be addressed with a set of carefully designed ablation studies , and we have aimed to do exactly that . We thus compare , as described below : ( 1 ) strictly pairwise vs unary features vs both ( complete Gram matrix ) ; ( 2 ) min/max vs mean/var ; and ( 3 ) with/without normalization . During our research up to this point , we have found that the latter makes a significant difference , and therefore we include it as part of this set of experiments . We have included new appendices in the revised submission that include these , which we will refer to in our detailed response below . ==================================== 1 . If the pairwise feature ( G ) is replaced by the unary feature ( F ) while keeping the other parts of the pipeline the same ( including the use of p-th power ) , would it reach a similar performance or much worse ? We would like to clarify that we use both diagonal elements ( unary features ) and off-diagonal elements ( pairwise features ) of the Gram Matrix . In this ablation study , we considered 2 additional settings : strictly unary features and strictly pairwise features ( i.e.only the off-diagonal elements ) . From the detailed ablation study , we observe that the Min/Max metric can work equally well with both unary and pairwise features ; in some cases , the unary features are marginally better ( Ex : ResNet/CIFAR-10 vs SVHN ) and in some cases , the pairwise features are marginally better ( Ex : ResNet/CIFAR-100 vs iSUN/LSUN/TinyImgNet ) . Interestingly , the behavior of the Mean/Var metric is different : the performance with pairwise features are significantly higher than with unary features in 19 out of 28 tested cases . For example , the TNR at TPR95 for ResNet/CIFAR-100 vs TinyImgNet is 68.0 with unary features and 84.2 with pairwise features . This might suggest that the distribution of pairwise statistics are more \u201c normal \u201d and natural as compared to unary statistics ; the pairwise statistics can be indicative ( at least , partly ) of unary statistics , but the converse isn \u2019 t true . Nevertheless , an overall message of our experiments is that it is worthwhile to consider all elements of the gram matrix ( gives an ensemble effect ) . ==================================== 2 . Why does the implementation use the statistics of min/max values instead of mean and variance ? The latter can also calculate the deviation for the method using a Gaussian model , which is usually a more natural choice . Mean/Variance was indeed the first option that we tried . In fact , computing class-conditional mean/variance for each component of the statistic derived from the Gram Matrix is equivalent to building a probability distribution over the gram matrices : Effectively , each input example can be represented by a big vector ( say , $ Z $ ) derived from the Gram Matrices computed across various layers , which can then be used to construct class-conditional distributions of $ Z $ ( assuming that each component of $ Z $ is normally distributed and independent of the other components ) ; this , in turn , can be used to compute the probability of an unseen $ Z $ . To begin with , our hypothesis was that the distribution over gram matrices can be useful ; for details of our motivation , and to avoid duplication , please refer to our answer to Q1 of R1 for the motivation . In early research , we noticed the following problems with the Mean/Var estimate : ( a ) The individual components of gram matrices do not follow normal distribution strictly and Mean/Var assigns lower probabilities to the in-distribution images as well . ( b ) The total deviation $ \\Delta $ -- computed by simply summing across the layerwise deviations , $ \\delta_l $ -- was not able to accurately summarize the information contained in the different $ \\delta_l $ s . Specifically , information about the layer where the input example had a higher deviation was lost when a simple sum was taken . The proposed Min/Max idea solves problem ( a ) by employing a weaker metric : deviation from extrema instead of the mean . It can also be said that the Min/Max metric considers a uniform probability density between the extrema . Problem ( b ) , which exists even for this newer metric , is solved by the normalization scheme described in the main paper for computing the sum total deviation . We refer the reader to Appendix C for a comprehensive summary of challenges with the Mean/Var metric , and also to the computed means and variances ( in the last row of Table 4 ) ."}, {"review_id": "r1g6MCEtwr-1", "review_text": "This paper uses Gram matrices for OOD detection. This enables the reliable detection of far-from-distribution examples, which is a long-standing and surprisingly difficult problem in OOD detection. This is the best paper in my batch due to the strength of the results. However, this paper should more accurately reflect the contribution: this helps with far-from-distribution examples, not near-distribution yet OOD examples. For instance, I used their code and found that their technique leads to an AUROC of 79.01% when using CIFAR-10 as the in-distribution and CIFAR-100 as the OOD set. Likewise, with their code I found having CIFAR-100 as in and CIFAR-10 as out gives an AUROC of 67.95%. This is much worse than currently existing techniques. Hence the paper should re-frame or qualify their results as helping with far-from-distribution detection or the detection of obvious anomalies. This paper makes a solid stride in improving the detection of garbage inputs, but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks. Small comments: > Lee et al. (2018b)\u2014to the best of our knowledge, the current SOTA technique by a significant margin This should be again qualified and expanded. If you assume access to knowledge of the test distribution, then Mahalanobis is easily the best. If not, the Outlier Exposure is best. If you assume access to no extra data during training, then the maximum softmax probability + rotation prediction is best [1]. > However, while the OE method is able to generalize across different non-training distributions, it does not achieve the SOTA rates of Lee et al. (2018b) on most cases. There are different senses of state-of-the-art and these should be qualified. Does this technique do better if you do 5th and 9th percentile instead of min and max? Is it important to do the min and max with training examples instead of validation examples? (Not a pressing question.) > Can work without access to OOD validation examples? Table 1 is deceptive. OE does not need the \"validation\" examples. I suggest two columns instead of one. \"Can this work without knowledge of OOD test examples? Does this use OOD training examples?\" Show AUROC and AUPR. Detection accuracy is an unusual metric relative to AUPR (OOD as positive). Show CIFAR-10 vs CIFAR-100 in the tables or I'll downgrade my rating, since otherwise the paper is not leaving an accurate impression. Since OE is complementary, perhaps this technique can be combined to tackle these near-distribution cases? The title is confusing. \"Zero-Shot\" could be applied to various techniques in this space. Perhaps emphasize Gram matrices? In their code: validation_indices = random.sample(range(len(all_test_deviations)),int(0.1*len(all_test_deviations))) test_indices = sorted(list(set(range(len(all_test_deviations)))-set(validation_indices))) These indices change with every power, which is unrealistic. Please fix the sets beforehand. Since neural style transfer, which uses Gram matrices, works much better with VGG architectures than ResNets, does this technique work better with VGG architectures? [1] Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. Hendrycks et al. NeurIPS 2019.", "rating": "8: Accept", "reply_text": "We are grateful to R3 for their incredibly detailed review , and we are delighted that they took the time and effort to run our code . This is remarkable , and greatly appreciated . Thank you ! \u201c This paper uses Gram matrices for OOD detection . This enables the reliable detection of far-from-distribution examples , which is a long-standing and surprisingly difficult problem in OOD detection . This is the best paper in my batch due to the strength of the results . However , this paper should more accurately reflect the contribution : this helps with far-from-distribution examples , not near-distribution yet OOD examples. \u201d We thank R3 for recognizing the strength of our results , and also for pointing out this important distinction regarding the contribution . We have taken initial steps to reflect this distinction , as described below , and our future revisions will do so further . \u201c For instance , I used their code and found that their technique leads to an AUROC of 79.01 % when using CIFAR-10 as the in-distribution and CIFAR-100 as the OOD set . Likewise , with their code I found having CIFAR-100 as in and CIFAR-10 as out gives an AUROC of 67.95 % . This is much worse than currently existing techniques . Hence the paper should re-frame or qualify their results as helping with far-from-distribution detection or the detection of obvious anomalies . This paper makes a solid stride in improving the detection of garbage inputs , but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks. \u201d Thank you for bringing this result to our attention . In all new ablation experiments and graphs , we have reported the results for these pairs . As of now , we have included a paragraph in the results section that notes the weakness of the metric , and places it within the context of other current results . We will obtain the results for other baseline methods and add the table as part of the main paper . Small comments : ========================= \u201c Lee et al . ( 2018b ) [ ... ] There are different senses of state-of-the-art and these should be qualified. \u201d We agree that it is important to qualify these , and we will do so in our next revision . ========================= \u201c Does this technique do better if you do 5th and 9th percentile instead of min and max ? Is it important to do the min and max with training examples instead of validation examples ? ( Not a pressing question . ) \u201d Sometimes , there is no change , but sometimes the result is adversely affected . If the validation examples are used to compute the min and max , there is a good chance that the test examples themselves will get a higher deviation value . In general , it is good to compute the min and max on a more representative distribution like the training examples . ========================= `` Can work without access to OOD validation examples ? Table 1 is deceptive . OE does not need the `` validation '' examples . I suggest two columns instead of one . `` Can this work without knowledge of OOD test examples ? Does this use OOD training examples ? '' We have fixed this ! ========================= \u201c Show AUROC and AUPR . Detection accuracy is an unusual metric relative to AUPR ( OOD as positive ) . \u201d We will add AUPR results for our metric and the baselines in a later revision . ========================= Show CIFAR-10 vs CIFAR-100 in the tables or I 'll downgrade my rating , since otherwise the paper is not leaving an accurate impression . As mentioned above , we have incorporated this into all new tables and graphs . Once we obtain results for the baselines , we will add it to Table 2 . ========================= Since OE is complementary , perhaps this technique can be combined to tackle these near-distribution cases ? This seems to be a promising future research direction , that we would be very interested in exploring more systematically ! ========================= \u201c In their code : validation_indices = random.sample ( range ( len ( all_test_deviations ) ) , int ( 0.1 * len ( all_test_deviations ) ) ) test_indices = sorted ( list ( set ( range ( len ( all_test_deviations ) ) ) -set ( validation_indices ) ) ) These indices change with every power , which is unrealistic . Please fix the sets beforehand. \u201d We apologize if the code was confusing ; the loop above this extracts the detection performance using 10 different random samples of the validation data . The loop over powers is done in the main ResNet/DenseNet class . We will comment the code . ========================= `` Since neural style transfer , which uses Gram matrices , works much better with VGG architectures than ResNets , does this technique work better with VGG architectures ? '' This is another very good question ; we will extract the results on VGG architectures . ========================= If there are any outstanding concerns that you would like us to address , we would be very happy to do so ."}, {"review_id": "r1g6MCEtwr-2", "review_text": "The paper proposes a strategy for detecting out-of-distribution samples based on feature representations obtained via neural networks. Given a test sample, the proposed strategy checks whether the correlation values between the features of the test sample obtained at different channels of the same layer are coherent with those of the training samples known to belong to the estimated class of the test sample. The proposed strategy can be applied to pretrained networks, as it only requires the channel activation values to determine whether a sample is out of distribution or not. The studied problem is an important problem and the experimental results show that the proposed strategy leads to some performance gains in comparison to reference methods. However, in my view the main drawback of the study is that it is based on an ad-hoc methodology whose theoretical foundation is not quite clear. In particular, it would be good to provide some more explanations on the following issues: - What exactly motivates the assumption that the correlation values between different channels of the same layer provides a discriminative characteristics of the classes from each other? It would be natural to assume that different classes will have different activation levels at a certain channel of a certain layer. However, the idea of the paper is to look at how different channels correlate with each other. I cannot entirely grasp the motivation for this, as looking at the feature correlations is a bit more indirect than looking at the features themselves. It would be good to provide the justification of this choice. - What is the theoretical motivation behind using the p-th order Gram matrix, instead of using the original Gram matrix (e.g. p=1)? Some experimental justification is given, but it is also important to provide some theoretical insight if possible.", "rating": "3: Weak Reject", "reply_text": "We thank R1 for their careful reading of the paper and for their thoughtful and helpful questions . ==================================== 1 . `` What exactly motivates the assumption that the correlation values between different channels of the same layer provides a discriminative characteristics of the classes from each other ? It would be natural to assume that different classes will have different activation levels at a certain channel of a certain layer . However , the idea of the paper is to look at how different channels correlate with each other . I can not entirely grasp the motivation for this , as looking at the feature correlations is a bit more indirect than looking at the features themselves . It would be good to provide the justification of this choice . '' We believe that our response to R2 \u2019 s Q1 , together with new ablation tests and graphs in Appendix C in the revised submission , provide justification for our method . We might also add that our work is motivated by the Mahalanobis algorithm : apart from the means , the core constituent of the Mahalanobis distance is the covariance between channels of a layer ; while the Mahalanobis algorithm used a single covariance matrix independent of class and represented each channel by its mean , we decided to investigate if computing the gram matrix using all features of a channel would be useful . Roughly , the idea was to build a class-conditional distribution of gram matrices , which could later be used to compute the probability of an unseen gram-matrix at test time ; also , the distribution over gram matrices would implicitly consider the distribution over the mean activations of each channel ( which is essentially the Mahalanobis algorithm ) . ==================================== 2 . `` What is the theoretical motivation behind using the p-th order Gram matrix , instead of using the original Gram matrix ( e.g.p=1 ) ? Some experimental justification is given , but it is also important to provide some theoretical insight if possible . '' As this is exactly the same question as asked by R2 Q3 , please refer to our reply to that question . ==================================== We believe that the additional material prompted by these questions has strengthened the overall paper . We believe we have addressed all of your concerns , but if there are any outstanding ones , we would love to have the opportunity to address them ."}], "0": {"review_id": "r1g6MCEtwr-0", "review_text": "This paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. The method only requires in-distribution data for tuning its hyper-parameters and can use a pre-trained classifier directly. Its performance is evaluated with small image datasets, which are commonly used in this line of work. Two additional experiments are performed to analyze the effect of two factors (the layer of the feature and the order of Gram matrix) in the method. The overall vote for this paper is a weak reject. The clarity of the paper is good. The primary strength of the work is the very strong performance given the setting, which does not require OOD data for tuning. However, we give a rating slightly below the borderline, because of lack of in-depth explanation of why pairwise feature correlation is helpful for the problem. The explanation could be either theoretical or empirical, while the latter can be a set of carefully designed ablation studies. We do not see sufficient arguments or experiments to ensure the performance gain comes from the pairwise feature and is not from other parts of the design. The above weakness could be addressed by answering the following questions: 1. If the pairwise feature (G) is replaced by the unary feature (F) while keeping the other parts of the pipeline the same (including the use of p-th power), would it reach a similar performance or much worse? 2. Why does the implementation use the statistics of min/max values instead of mean and variance? The latter can also calculate the deviation for the method using a Gaussian model, which is usually a more natural choice. 3. What is the motivation for using p-th power, and why does it help? We present the additional feedbacks in the form of questions which could further strengthen the work if answered: 4. Based on Figure 2, having p=10 is sufficient to give a good result. Why does the method still use all p-th power (p=1..10)? 5. The study in Figure 1 only presents specific OOD data (Tiny Imagenet). Does the trend still hold with other OOD data, such as SVHN or LSUN? 6. Why are the networks used in figure 1(a) and (b) different? Does the trend still hold when the networks in both cases are changed? 7. The statistics (Mins/Maxs) to be saved can be very large. The paper provides a strategy to reduce that by using the row-wise sums of G. However, the summation has an effect of mixing the features, causing a weaker signal from the deviation. Why does the method still works with the row-wise sums? 8. Would the method work if the networks have no batch normalization layer? ", "rating": "3: Weak Reject", "reply_text": "We thank R2 for their careful reading , thoughtful comments , and also for phrasing their review in such a way that gives us the opportunity to answer some of these questions . We further appreciate R2 \u2019 s openness to either empirical or theoretical justification . Our general approach is to provide considerably more empirical results at the moment , along with some intuition and interpretation of those results . In particular , R2 provided us with 3 focused questions to understand better where the performance gain might be coming from , and the ( most welcome ) suggestion that these questions might be addressed with a set of carefully designed ablation studies , and we have aimed to do exactly that . We thus compare , as described below : ( 1 ) strictly pairwise vs unary features vs both ( complete Gram matrix ) ; ( 2 ) min/max vs mean/var ; and ( 3 ) with/without normalization . During our research up to this point , we have found that the latter makes a significant difference , and therefore we include it as part of this set of experiments . We have included new appendices in the revised submission that include these , which we will refer to in our detailed response below . ==================================== 1 . If the pairwise feature ( G ) is replaced by the unary feature ( F ) while keeping the other parts of the pipeline the same ( including the use of p-th power ) , would it reach a similar performance or much worse ? We would like to clarify that we use both diagonal elements ( unary features ) and off-diagonal elements ( pairwise features ) of the Gram Matrix . In this ablation study , we considered 2 additional settings : strictly unary features and strictly pairwise features ( i.e.only the off-diagonal elements ) . From the detailed ablation study , we observe that the Min/Max metric can work equally well with both unary and pairwise features ; in some cases , the unary features are marginally better ( Ex : ResNet/CIFAR-10 vs SVHN ) and in some cases , the pairwise features are marginally better ( Ex : ResNet/CIFAR-100 vs iSUN/LSUN/TinyImgNet ) . Interestingly , the behavior of the Mean/Var metric is different : the performance with pairwise features are significantly higher than with unary features in 19 out of 28 tested cases . For example , the TNR at TPR95 for ResNet/CIFAR-100 vs TinyImgNet is 68.0 with unary features and 84.2 with pairwise features . This might suggest that the distribution of pairwise statistics are more \u201c normal \u201d and natural as compared to unary statistics ; the pairwise statistics can be indicative ( at least , partly ) of unary statistics , but the converse isn \u2019 t true . Nevertheless , an overall message of our experiments is that it is worthwhile to consider all elements of the gram matrix ( gives an ensemble effect ) . ==================================== 2 . Why does the implementation use the statistics of min/max values instead of mean and variance ? The latter can also calculate the deviation for the method using a Gaussian model , which is usually a more natural choice . Mean/Variance was indeed the first option that we tried . In fact , computing class-conditional mean/variance for each component of the statistic derived from the Gram Matrix is equivalent to building a probability distribution over the gram matrices : Effectively , each input example can be represented by a big vector ( say , $ Z $ ) derived from the Gram Matrices computed across various layers , which can then be used to construct class-conditional distributions of $ Z $ ( assuming that each component of $ Z $ is normally distributed and independent of the other components ) ; this , in turn , can be used to compute the probability of an unseen $ Z $ . To begin with , our hypothesis was that the distribution over gram matrices can be useful ; for details of our motivation , and to avoid duplication , please refer to our answer to Q1 of R1 for the motivation . In early research , we noticed the following problems with the Mean/Var estimate : ( a ) The individual components of gram matrices do not follow normal distribution strictly and Mean/Var assigns lower probabilities to the in-distribution images as well . ( b ) The total deviation $ \\Delta $ -- computed by simply summing across the layerwise deviations , $ \\delta_l $ -- was not able to accurately summarize the information contained in the different $ \\delta_l $ s . Specifically , information about the layer where the input example had a higher deviation was lost when a simple sum was taken . The proposed Min/Max idea solves problem ( a ) by employing a weaker metric : deviation from extrema instead of the mean . It can also be said that the Min/Max metric considers a uniform probability density between the extrema . Problem ( b ) , which exists even for this newer metric , is solved by the normalization scheme described in the main paper for computing the sum total deviation . We refer the reader to Appendix C for a comprehensive summary of challenges with the Mean/Var metric , and also to the computed means and variances ( in the last row of Table 4 ) ."}, "1": {"review_id": "r1g6MCEtwr-1", "review_text": "This paper uses Gram matrices for OOD detection. This enables the reliable detection of far-from-distribution examples, which is a long-standing and surprisingly difficult problem in OOD detection. This is the best paper in my batch due to the strength of the results. However, this paper should more accurately reflect the contribution: this helps with far-from-distribution examples, not near-distribution yet OOD examples. For instance, I used their code and found that their technique leads to an AUROC of 79.01% when using CIFAR-10 as the in-distribution and CIFAR-100 as the OOD set. Likewise, with their code I found having CIFAR-100 as in and CIFAR-10 as out gives an AUROC of 67.95%. This is much worse than currently existing techniques. Hence the paper should re-frame or qualify their results as helping with far-from-distribution detection or the detection of obvious anomalies. This paper makes a solid stride in improving the detection of garbage inputs, but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks. Small comments: > Lee et al. (2018b)\u2014to the best of our knowledge, the current SOTA technique by a significant margin This should be again qualified and expanded. If you assume access to knowledge of the test distribution, then Mahalanobis is easily the best. If not, the Outlier Exposure is best. If you assume access to no extra data during training, then the maximum softmax probability + rotation prediction is best [1]. > However, while the OE method is able to generalize across different non-training distributions, it does not achieve the SOTA rates of Lee et al. (2018b) on most cases. There are different senses of state-of-the-art and these should be qualified. Does this technique do better if you do 5th and 9th percentile instead of min and max? Is it important to do the min and max with training examples instead of validation examples? (Not a pressing question.) > Can work without access to OOD validation examples? Table 1 is deceptive. OE does not need the \"validation\" examples. I suggest two columns instead of one. \"Can this work without knowledge of OOD test examples? Does this use OOD training examples?\" Show AUROC and AUPR. Detection accuracy is an unusual metric relative to AUPR (OOD as positive). Show CIFAR-10 vs CIFAR-100 in the tables or I'll downgrade my rating, since otherwise the paper is not leaving an accurate impression. Since OE is complementary, perhaps this technique can be combined to tackle these near-distribution cases? The title is confusing. \"Zero-Shot\" could be applied to various techniques in this space. Perhaps emphasize Gram matrices? In their code: validation_indices = random.sample(range(len(all_test_deviations)),int(0.1*len(all_test_deviations))) test_indices = sorted(list(set(range(len(all_test_deviations)))-set(validation_indices))) These indices change with every power, which is unrealistic. Please fix the sets beforehand. Since neural style transfer, which uses Gram matrices, works much better with VGG architectures than ResNets, does this technique work better with VGG architectures? [1] Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. Hendrycks et al. NeurIPS 2019.", "rating": "8: Accept", "reply_text": "We are grateful to R3 for their incredibly detailed review , and we are delighted that they took the time and effort to run our code . This is remarkable , and greatly appreciated . Thank you ! \u201c This paper uses Gram matrices for OOD detection . This enables the reliable detection of far-from-distribution examples , which is a long-standing and surprisingly difficult problem in OOD detection . This is the best paper in my batch due to the strength of the results . However , this paper should more accurately reflect the contribution : this helps with far-from-distribution examples , not near-distribution yet OOD examples. \u201d We thank R3 for recognizing the strength of our results , and also for pointing out this important distinction regarding the contribution . We have taken initial steps to reflect this distinction , as described below , and our future revisions will do so further . \u201c For instance , I used their code and found that their technique leads to an AUROC of 79.01 % when using CIFAR-10 as the in-distribution and CIFAR-100 as the OOD set . Likewise , with their code I found having CIFAR-100 as in and CIFAR-10 as out gives an AUROC of 67.95 % . This is much worse than currently existing techniques . Hence the paper should re-frame or qualify their results as helping with far-from-distribution detection or the detection of obvious anomalies . This paper makes a solid stride in improving the detection of garbage inputs , but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks. \u201d Thank you for bringing this result to our attention . In all new ablation experiments and graphs , we have reported the results for these pairs . As of now , we have included a paragraph in the results section that notes the weakness of the metric , and places it within the context of other current results . We will obtain the results for other baseline methods and add the table as part of the main paper . Small comments : ========================= \u201c Lee et al . ( 2018b ) [ ... ] There are different senses of state-of-the-art and these should be qualified. \u201d We agree that it is important to qualify these , and we will do so in our next revision . ========================= \u201c Does this technique do better if you do 5th and 9th percentile instead of min and max ? Is it important to do the min and max with training examples instead of validation examples ? ( Not a pressing question . ) \u201d Sometimes , there is no change , but sometimes the result is adversely affected . If the validation examples are used to compute the min and max , there is a good chance that the test examples themselves will get a higher deviation value . In general , it is good to compute the min and max on a more representative distribution like the training examples . ========================= `` Can work without access to OOD validation examples ? Table 1 is deceptive . OE does not need the `` validation '' examples . I suggest two columns instead of one . `` Can this work without knowledge of OOD test examples ? Does this use OOD training examples ? '' We have fixed this ! ========================= \u201c Show AUROC and AUPR . Detection accuracy is an unusual metric relative to AUPR ( OOD as positive ) . \u201d We will add AUPR results for our metric and the baselines in a later revision . ========================= Show CIFAR-10 vs CIFAR-100 in the tables or I 'll downgrade my rating , since otherwise the paper is not leaving an accurate impression . As mentioned above , we have incorporated this into all new tables and graphs . Once we obtain results for the baselines , we will add it to Table 2 . ========================= Since OE is complementary , perhaps this technique can be combined to tackle these near-distribution cases ? This seems to be a promising future research direction , that we would be very interested in exploring more systematically ! ========================= \u201c In their code : validation_indices = random.sample ( range ( len ( all_test_deviations ) ) , int ( 0.1 * len ( all_test_deviations ) ) ) test_indices = sorted ( list ( set ( range ( len ( all_test_deviations ) ) ) -set ( validation_indices ) ) ) These indices change with every power , which is unrealistic . Please fix the sets beforehand. \u201d We apologize if the code was confusing ; the loop above this extracts the detection performance using 10 different random samples of the validation data . The loop over powers is done in the main ResNet/DenseNet class . We will comment the code . ========================= `` Since neural style transfer , which uses Gram matrices , works much better with VGG architectures than ResNets , does this technique work better with VGG architectures ? '' This is another very good question ; we will extract the results on VGG architectures . ========================= If there are any outstanding concerns that you would like us to address , we would be very happy to do so ."}, "2": {"review_id": "r1g6MCEtwr-2", "review_text": "The paper proposes a strategy for detecting out-of-distribution samples based on feature representations obtained via neural networks. Given a test sample, the proposed strategy checks whether the correlation values between the features of the test sample obtained at different channels of the same layer are coherent with those of the training samples known to belong to the estimated class of the test sample. The proposed strategy can be applied to pretrained networks, as it only requires the channel activation values to determine whether a sample is out of distribution or not. The studied problem is an important problem and the experimental results show that the proposed strategy leads to some performance gains in comparison to reference methods. However, in my view the main drawback of the study is that it is based on an ad-hoc methodology whose theoretical foundation is not quite clear. In particular, it would be good to provide some more explanations on the following issues: - What exactly motivates the assumption that the correlation values between different channels of the same layer provides a discriminative characteristics of the classes from each other? It would be natural to assume that different classes will have different activation levels at a certain channel of a certain layer. However, the idea of the paper is to look at how different channels correlate with each other. I cannot entirely grasp the motivation for this, as looking at the feature correlations is a bit more indirect than looking at the features themselves. It would be good to provide the justification of this choice. - What is the theoretical motivation behind using the p-th order Gram matrix, instead of using the original Gram matrix (e.g. p=1)? Some experimental justification is given, but it is also important to provide some theoretical insight if possible.", "rating": "3: Weak Reject", "reply_text": "We thank R1 for their careful reading of the paper and for their thoughtful and helpful questions . ==================================== 1 . `` What exactly motivates the assumption that the correlation values between different channels of the same layer provides a discriminative characteristics of the classes from each other ? It would be natural to assume that different classes will have different activation levels at a certain channel of a certain layer . However , the idea of the paper is to look at how different channels correlate with each other . I can not entirely grasp the motivation for this , as looking at the feature correlations is a bit more indirect than looking at the features themselves . It would be good to provide the justification of this choice . '' We believe that our response to R2 \u2019 s Q1 , together with new ablation tests and graphs in Appendix C in the revised submission , provide justification for our method . We might also add that our work is motivated by the Mahalanobis algorithm : apart from the means , the core constituent of the Mahalanobis distance is the covariance between channels of a layer ; while the Mahalanobis algorithm used a single covariance matrix independent of class and represented each channel by its mean , we decided to investigate if computing the gram matrix using all features of a channel would be useful . Roughly , the idea was to build a class-conditional distribution of gram matrices , which could later be used to compute the probability of an unseen gram-matrix at test time ; also , the distribution over gram matrices would implicitly consider the distribution over the mean activations of each channel ( which is essentially the Mahalanobis algorithm ) . ==================================== 2 . `` What is the theoretical motivation behind using the p-th order Gram matrix , instead of using the original Gram matrix ( e.g.p=1 ) ? Some experimental justification is given , but it is also important to provide some theoretical insight if possible . '' As this is exactly the same question as asked by R2 Q3 , please refer to our reply to that question . ==================================== We believe that the additional material prompted by these questions has strengthened the overall paper . We believe we have addressed all of your concerns , but if there are any outstanding ones , we would love to have the opportunity to address them ."}}