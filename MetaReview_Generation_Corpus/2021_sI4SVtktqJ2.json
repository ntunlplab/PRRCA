{"year": "2021", "forum": "sI4SVtktqJ2", "title": "Efficient randomized smoothing by denoising with learned score function", "decision": "Reject", "meta_review": "The paper proposes an improved method for randomized smoothing, reducing computationally complexity compared with some previous works. The authors propose to learn score functions to denoise the randomized image prior to feeding it to a trained classification model. More specifically,  two image denoising algorithms based on score estimation are proposed to be applied regardless of noise level/type.\n\n\nStrengths:\n- The paper shows strong quantitative results. The gap with white box smoothing is small on cifar, outperforming Salman et al. However according to the authors, the performance advantage could be mainly attributed to (1)  the use of better network architecture and (2) the multi-scale training, not the major contribution of a score-based denoiser. \n- The denoiser doesn't require access to the pre-trained classifiers.\n- The proposed method only requires training of one score network to handle various types of noise type/level, although reviewers have raised concerns about motivation to having a method that only needs one denoiser for multiple noise levels -  the computational bottleneck of randomized smoothing is the prediction time rather than training time and  using the same score function for multiple noise levels could be suboptimal.\n\nWeaknesses:\n- There are some concerns about the significance of the contribution as well as novelty of the work, as the denoising + pre-trained classifier architecture is already proposed. Specifically, the work can be seen as incremental to [1], although the work uses a score-based image denoiser whereas [1] uses a CNN based image denoiser and this work is more efficient as it requires only one score network, while [1] trained multiple denoisers with respect to each noise levels. \n- Reviewers have expressed concerns on the prediction efficiency of score-function based generative / denoising models.  The proposed method might exacerbate the weakness of randomized smoothing (i.e., slow prediction), especially in high-dimensions. \n-The reviewers are curious to see the benefit of the proposed denoiser over the state-of-the-art Gaussian denoisers (as used in [1]) under Gaussian noise setting.\n-Method seems to be effective for low-resolution images only. The gap with white box increases on Imagenet.\n\n[1]. Salman, Hadi, et al. \"Denoised Smoothing: A Provable Defense for Pretrained Classifiers.\" Advances in Neural Information Processing Systems 33 (2020).\n", "reviews": [{"review_id": "sI4SVtktqJ2-0", "review_text": "Update : I have read the author 's response and decided to keep my review , confidence , and score . Summary : randomized smoothing is a method to construct provably robust classifiers via additive Gaussian noises on the input . The authors propose to learn score functions as a means to denoise the randomized image prior to a trained classification model . As the denoising + pre-trained classifier architecture is already proposed , the contribution is only limited to the choice of using a score function . The justification and realization of the method is limited for two main reasons . See below . 1.Efficiency : one of the most critical bottleneck of randomized smoothing methods is the slow prediction time . The score-function based generative / denoising models are known for their slow sampling time , so the proposed method undermines randomized smoothing in efficiency . 2.Many design choices in this paper is not well justified . 2-1 ) How good does the RHS of Eq . ( 12 ) approximates the gradient descent procedure ? 2-2 ) Even if the true gradient descent can be executed , the bound in Eq . ( 13 ) seems very bad in high dimension , thus the smoothed classifier will not be accurate unless the pre-trained classifier is already robust in the local region . 2-3 ) Clearly using the same score function for multiple $ \\sigma $ is suboptimal . Although the authors mentioned this part as an advantage , but it is not clearly compared to existing methods . Would existing methods fail if they use the same denoising function for multiple $ \\sigma $ ?", "rating": "3: Clear rejection", "reply_text": "We sincerely appreciate your instructive review . We list the response for your questions below : 1 - We realize that the slow prediction time is bottleneck for randomized smoothing , and it is n't efficient . However , the efficiency that we deal in this paper is about generation of smoothed classifier . We did n't change any algorithm on original randomized smoothing , therefore is independent to our work . On the other hand , it is true that putting denoiser in front of the classifier slows down the prediction time . We empirically found out that it tooks 2-3 time ( which is similar to that of [ 1 ] ) for one-step denoiser and 4-10 time ( depends on the $ \\sigma $ ) compare to original randomized smoothing ( a.k.a white-box smoothing ) which seems reasonable for prediction time . As ImageNet is of high-dimension the multiplicity makes prediction more slower , we leave accelerating the prediction time for future work . 2-1.For our multi-step approach , there are two approximations involved . First is approximation of data distribution $ p $ with $ p_\\sigma $ , and second one is approximation of $ -\\log p_\\sigma ( x ) $ with score function $ s_ { \\theta , \\sigma } ( x ) $ . Note that the first approximation error is bounded by $ \\sigma\\sqrt { d } $ ( which is shown in Appendix A ) and second approximation error is generalization error which can be expressed by Radamacher complexity of used neural network for score network . We refer [ 2 ] for theoretic overview on learning score-based model ( or equivalently DAE ) . 2-2 As we demonstrated in our theorem , the bound is loose when image is of high-dimension . However , as our denoising algorithm anneals $ \\sigma\\rightarrow 0 $ , theoretically we can achieve small error bounds using extremely small $ \\sigma_L $ . Furthermore , it is true that robust pre-trained classifier helps the performance . In fact , we experimented with classifier trained with very small Gaussian noise $ \\sigma = 0.01 $ , but it did n't help the performance . Instead , as illustrated in Appendix C.2 the classifiers with better test accuracy results in better performance . 2-3 We did n't really understand your question . But based on our interpretation , even though we use only one score function for multiple $ \\sigma $ the score function is conditioned by predetermined sequence of $ \\sigma $ . Can you explain more about 'existing methods ' ? We hope your answer and willing to response . Overall , we sincerely thanks for kind and detailed review which helped us a lot . [ 1 ] Salman , Hadi , et al . `` Denoised Smoothing : A Provable Defense for Pretrained Classifiers . '' Advances in Neural Information Processing Systems 33 ( 2020 ) . [ 2 ] Block , Adam , Youssef Mroueh , and Alexander Rakhlin . `` Generative Modeling with Denoising Auto-Encoders and Langevin Sampling . '' arXiv preprint arXiv:2002.00107 ( 2020 ) ."}, {"review_id": "sI4SVtktqJ2-1", "review_text": "This paper proposes a method based on denoising to protect classifiers from adversarial attacks . Unlike existing methods based on randomized smoothing with various noise distributions to retrain several classifiers , the proposed one uses denoising as the preprocess of the classifier . The experimental results demonstrate the proposed method has good performance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your instructive review . Our work proposes score-based model for image denoising that can efficiently generate smoothed classifier without retraining classifiers . If there is any further question , let us know ."}, {"review_id": "sI4SVtktqJ2-2", "review_text": "This paper presents a denoising-based method for randomized smoothing that converts a base classifier into a smoothed one with p-robustness to adversarial examples . It considers a practical setting where the retraining/finetuning of the base classifier is largely inapplicable ( e.g.the commercial classification service with only API provided to users ) . To do this , it adopts a recently proposed methodology termed denoised smoothing [ 1 ] by prepending a custom-trained denoiser to the pretrained classifier . The major novelty of this work lies at the proposed denoising method using learned score function . The new denoising method only requires training one score network and is readily applicable to defend various $ l_p $ adversaries , which is a key feature not available in [ 1 ] . The experiments show the proposed method outperforms the previous denoising-based approach , and is sometimes on par with the white-box approach [ 2 ] that manipulates the classifier . Basically , this is an incremental work over [ 1 ] but the contributions claimed are perceived myself ( though I have to admit I 'm an expert on image denoising , instead of adversarial defense ) However , I do have some concerns about the method and the experiments , listed as follows : - The major advantage of the score-function-based denoiser is the flexibility to handle various noise types and levels . I do n't expect it can beat the specialized Gaussian denoiser [ 1 ] under Gaussian perturbation setting . As it is the case on Table 1/2 , I 'm wondering what 's the benefit of the proposed denoiser over the state-of-the-art Gaussian denoisers ( as used in [ 1 ] ) under Gaussian noise setting ? - The flexibility to tackle various $ l_p $ adversary , the key feature of the proposed method is not thoroughly evaluated . In Table 2 , I suggest the authors to add comparisons to [ 1 ] with denoiser trained on Gaussian noise setting , as well as ones trained with noise type aligned with the test setting . [ 1 ] Denoised Smoothing : A Provable Defense for Pretrained Classifiers , Arxiv 2020 [ 2 ] Certified Adversarial Robustness via Randomized Smoothing , ICML 2019", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your instructive review . We list the response for your questions below : For clarification , [ 1 ] trained denoising autoencoder ( DAE ) with MSE and classification loss , and our one-step denoiser is equivalent to DAE trained with MSE loss . The equivalence of DAE and denoising score matching ( DSM ) is studied through various literatures [ ] as we demonstrated in the paper . Therefore the perforemance gap between [ 1 ] and our one-step denoiser is due to architecture change and multi-scale training . We used U-net based architecture as in [ 2 ] , and we 've shown that the multi-scale training benefits us learning the score function easier in Appendix C. Note that we trained only one score network which learns score of perturbed data distribution with Gaussian noise . For the best of our knowledge , [ 1 ] or any other works experimented with denoiser trained with other noise types such as Laplace noise or uniform noise . For your information , soon We will add comparison for Gaussian noise settings . Overall , we sincerely thanks for kind and detailed review which helped us a lot . [ 1 ] Block , Adam , Youssef Mroueh , and Alexander Rakhlin . `` Generative Modeling with Denoising Auto-Encoders and Langevin Sampling . '' arXiv preprint arXiv:2002.00107 ( 2020 ) . [ 2 ] Vincent , Pascal . `` A connection between score matching and denoising autoencoders . '' Neural computation 23.7 ( 2011 ) : 1661-1674 ."}, {"review_id": "sI4SVtktqJ2-3", "review_text": "Summary & contribution : in this paper the authors propose an improved method for randomized smoothing ( for provable defense ) which performs better and is less computationally expensive than previous work . More specifically the authors propose two image denoising algorithms ( based on score estimation ) that can be applied regardless of noise level/type . In fact , the paper is mainly an improvement of Salman et al . ( 2020 ) with a blind denoiser . Therefore , it is not needed to train different models for different kind of attacks . Another advantage of the method is that one does not need to retrain the classifier and does not even need information from the pretrained classifier while the method from Salman et al.requires access to the classifier . Strengths : -strong quantitative results . Experimental section is promising . The gap with white box smoothing is small on cifar . The method outperforms Salman et al.-Only need to train one score network to handle various types of noise type/level -Denoiser does n't need access to the pretrained classifier . Weaknesses : -In my opinion writing can be improved . I am not very familiar with the literature and it took me some time to understand section 3 . More specifically I did not understand the motivation for using score based denoisers rather than a more `` standard '' algorithm for blind denoising . -Method seems to be effective for low-resolution images only . The gap with white box increases on Imagenet . Questions for the authors : -If I understand well , one-step denoiser can only handle gaussian noise whereas multi-step can be applied to any log concave distribution ? What is the advantage of one-step denoiser over multistep , does it perform better for gaussian noise ? -It is not clear to me why the method does not make use of the pretrained classifier but still outperforms Salman et al.which can acess the classifier ? Is it only due to the denoiser 's performance ? -I do not understand well the motivation for using score-based methods . Why scored based denoising in particular and not other blind denoising models ? The current method is impacted by the size of the images , while most of the existing blind denoising algorithms are not affected by the size of the image . Actually , why even focus on learning base methods ? Can not simpler methods with handcrafted priors do the job ? I might also be wrong . Could you please elaborate ? -Qualitatively speaking it seems to me that the visual performances are not very good when compared to existing denoising algorithms ( maybe a quantitative comparison in term of PSNR with other algorithms would be relevant , rather than only showing qualitative results ) . Also the multi step denoiser seems to give a lot of artefacts in figure 1 . -Are n't there missing reference in the related work regarding blind denoising ? -p5 `` the noise makes the support of the score function to be whole space , [ ... ] non-Gaussian or off-the-manifold samples '' . I do n't understand that part . At this stage I give a weak accept , but I would consider raising my score if authors answer my concerns . Typos : p8 `` without any re-trianing . '' p7 `` smoothingon '' p4 `` matching obejctiv ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your instructive review . We list the response for your questions below : Q1 : If I understand well , one-step denoiser can only handle gaussian noise whereas multi-step can be applied to any log concave distribution ? What is the advantage of one-step denoiser over multistep , does it perform better for gaussian noise ? A1 : Yes.The one-step denoiser is equivalent to a trained denoising autoencoder ( DAE ) which were used in [ 1 ] . DnCNN and MemNet are exemplar for such approach . On the other hand , the multi-step denoiser is a iterative algorithm that optimizes maximum a posteriori ( MAP ) loss with gradient approximated by score function , which allows various log-concave noise distributions . The one-step denoiser is much faster than multi-step denoiser as it only requires one forward pass to achieve a denoised image . Moreover , the empirical results show that one-step denoiser performs better than multi-step denoiser for Gaussian test setting . On the other hand , multi-step denoiser is 'efficient ' that can deal with various noise types . Q2 : It is not clear to me why the method does not make use of the pretrained classifier but still outperforms Salman et al.which can acess the classifier ? Is it only due to the denoiser 's performance ? A2 : The performance gap between [ 1 ] and our can be explained with two aspects : the use of better network architecture and the multi-scale training . We used the U-net based architecture with improved convolution layer as illustrated in [ 2 ] which is developed for score-based modeling . Besides the impact of architecture , we empirically found out that multi-scale training slightly boosts the performance ( Appendix C ) . Moreover , even though we did n't include in the paper as we think is irrelevant to our interest , we trained score network with classification regularization as done in [ 1 ] . We trained the loss function as following : $ $ L_ { \\text { score } } + \\gamma D_ { KL } ( f ( x ) \\| f ( \\hat { x } ) ) $ $ , where $ \\hat { x } = x -\\sigma^2s_ { \\theta , \\sigma } ( \\tilde { x } ) $ is denoised image from our one-step denoiser . We figure out that the classifier loss marginally improves the performance when $ \\sigma $ is large , but overall there were no difference ."}], "0": {"review_id": "sI4SVtktqJ2-0", "review_text": "Update : I have read the author 's response and decided to keep my review , confidence , and score . Summary : randomized smoothing is a method to construct provably robust classifiers via additive Gaussian noises on the input . The authors propose to learn score functions as a means to denoise the randomized image prior to a trained classification model . As the denoising + pre-trained classifier architecture is already proposed , the contribution is only limited to the choice of using a score function . The justification and realization of the method is limited for two main reasons . See below . 1.Efficiency : one of the most critical bottleneck of randomized smoothing methods is the slow prediction time . The score-function based generative / denoising models are known for their slow sampling time , so the proposed method undermines randomized smoothing in efficiency . 2.Many design choices in this paper is not well justified . 2-1 ) How good does the RHS of Eq . ( 12 ) approximates the gradient descent procedure ? 2-2 ) Even if the true gradient descent can be executed , the bound in Eq . ( 13 ) seems very bad in high dimension , thus the smoothed classifier will not be accurate unless the pre-trained classifier is already robust in the local region . 2-3 ) Clearly using the same score function for multiple $ \\sigma $ is suboptimal . Although the authors mentioned this part as an advantage , but it is not clearly compared to existing methods . Would existing methods fail if they use the same denoising function for multiple $ \\sigma $ ?", "rating": "3: Clear rejection", "reply_text": "We sincerely appreciate your instructive review . We list the response for your questions below : 1 - We realize that the slow prediction time is bottleneck for randomized smoothing , and it is n't efficient . However , the efficiency that we deal in this paper is about generation of smoothed classifier . We did n't change any algorithm on original randomized smoothing , therefore is independent to our work . On the other hand , it is true that putting denoiser in front of the classifier slows down the prediction time . We empirically found out that it tooks 2-3 time ( which is similar to that of [ 1 ] ) for one-step denoiser and 4-10 time ( depends on the $ \\sigma $ ) compare to original randomized smoothing ( a.k.a white-box smoothing ) which seems reasonable for prediction time . As ImageNet is of high-dimension the multiplicity makes prediction more slower , we leave accelerating the prediction time for future work . 2-1.For our multi-step approach , there are two approximations involved . First is approximation of data distribution $ p $ with $ p_\\sigma $ , and second one is approximation of $ -\\log p_\\sigma ( x ) $ with score function $ s_ { \\theta , \\sigma } ( x ) $ . Note that the first approximation error is bounded by $ \\sigma\\sqrt { d } $ ( which is shown in Appendix A ) and second approximation error is generalization error which can be expressed by Radamacher complexity of used neural network for score network . We refer [ 2 ] for theoretic overview on learning score-based model ( or equivalently DAE ) . 2-2 As we demonstrated in our theorem , the bound is loose when image is of high-dimension . However , as our denoising algorithm anneals $ \\sigma\\rightarrow 0 $ , theoretically we can achieve small error bounds using extremely small $ \\sigma_L $ . Furthermore , it is true that robust pre-trained classifier helps the performance . In fact , we experimented with classifier trained with very small Gaussian noise $ \\sigma = 0.01 $ , but it did n't help the performance . Instead , as illustrated in Appendix C.2 the classifiers with better test accuracy results in better performance . 2-3 We did n't really understand your question . But based on our interpretation , even though we use only one score function for multiple $ \\sigma $ the score function is conditioned by predetermined sequence of $ \\sigma $ . Can you explain more about 'existing methods ' ? We hope your answer and willing to response . Overall , we sincerely thanks for kind and detailed review which helped us a lot . [ 1 ] Salman , Hadi , et al . `` Denoised Smoothing : A Provable Defense for Pretrained Classifiers . '' Advances in Neural Information Processing Systems 33 ( 2020 ) . [ 2 ] Block , Adam , Youssef Mroueh , and Alexander Rakhlin . `` Generative Modeling with Denoising Auto-Encoders and Langevin Sampling . '' arXiv preprint arXiv:2002.00107 ( 2020 ) ."}, "1": {"review_id": "sI4SVtktqJ2-1", "review_text": "This paper proposes a method based on denoising to protect classifiers from adversarial attacks . Unlike existing methods based on randomized smoothing with various noise distributions to retrain several classifiers , the proposed one uses denoising as the preprocess of the classifier . The experimental results demonstrate the proposed method has good performance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your instructive review . Our work proposes score-based model for image denoising that can efficiently generate smoothed classifier without retraining classifiers . If there is any further question , let us know ."}, "2": {"review_id": "sI4SVtktqJ2-2", "review_text": "This paper presents a denoising-based method for randomized smoothing that converts a base classifier into a smoothed one with p-robustness to adversarial examples . It considers a practical setting where the retraining/finetuning of the base classifier is largely inapplicable ( e.g.the commercial classification service with only API provided to users ) . To do this , it adopts a recently proposed methodology termed denoised smoothing [ 1 ] by prepending a custom-trained denoiser to the pretrained classifier . The major novelty of this work lies at the proposed denoising method using learned score function . The new denoising method only requires training one score network and is readily applicable to defend various $ l_p $ adversaries , which is a key feature not available in [ 1 ] . The experiments show the proposed method outperforms the previous denoising-based approach , and is sometimes on par with the white-box approach [ 2 ] that manipulates the classifier . Basically , this is an incremental work over [ 1 ] but the contributions claimed are perceived myself ( though I have to admit I 'm an expert on image denoising , instead of adversarial defense ) However , I do have some concerns about the method and the experiments , listed as follows : - The major advantage of the score-function-based denoiser is the flexibility to handle various noise types and levels . I do n't expect it can beat the specialized Gaussian denoiser [ 1 ] under Gaussian perturbation setting . As it is the case on Table 1/2 , I 'm wondering what 's the benefit of the proposed denoiser over the state-of-the-art Gaussian denoisers ( as used in [ 1 ] ) under Gaussian noise setting ? - The flexibility to tackle various $ l_p $ adversary , the key feature of the proposed method is not thoroughly evaluated . In Table 2 , I suggest the authors to add comparisons to [ 1 ] with denoiser trained on Gaussian noise setting , as well as ones trained with noise type aligned with the test setting . [ 1 ] Denoised Smoothing : A Provable Defense for Pretrained Classifiers , Arxiv 2020 [ 2 ] Certified Adversarial Robustness via Randomized Smoothing , ICML 2019", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your instructive review . We list the response for your questions below : For clarification , [ 1 ] trained denoising autoencoder ( DAE ) with MSE and classification loss , and our one-step denoiser is equivalent to DAE trained with MSE loss . The equivalence of DAE and denoising score matching ( DSM ) is studied through various literatures [ ] as we demonstrated in the paper . Therefore the perforemance gap between [ 1 ] and our one-step denoiser is due to architecture change and multi-scale training . We used U-net based architecture as in [ 2 ] , and we 've shown that the multi-scale training benefits us learning the score function easier in Appendix C. Note that we trained only one score network which learns score of perturbed data distribution with Gaussian noise . For the best of our knowledge , [ 1 ] or any other works experimented with denoiser trained with other noise types such as Laplace noise or uniform noise . For your information , soon We will add comparison for Gaussian noise settings . Overall , we sincerely thanks for kind and detailed review which helped us a lot . [ 1 ] Block , Adam , Youssef Mroueh , and Alexander Rakhlin . `` Generative Modeling with Denoising Auto-Encoders and Langevin Sampling . '' arXiv preprint arXiv:2002.00107 ( 2020 ) . [ 2 ] Vincent , Pascal . `` A connection between score matching and denoising autoencoders . '' Neural computation 23.7 ( 2011 ) : 1661-1674 ."}, "3": {"review_id": "sI4SVtktqJ2-3", "review_text": "Summary & contribution : in this paper the authors propose an improved method for randomized smoothing ( for provable defense ) which performs better and is less computationally expensive than previous work . More specifically the authors propose two image denoising algorithms ( based on score estimation ) that can be applied regardless of noise level/type . In fact , the paper is mainly an improvement of Salman et al . ( 2020 ) with a blind denoiser . Therefore , it is not needed to train different models for different kind of attacks . Another advantage of the method is that one does not need to retrain the classifier and does not even need information from the pretrained classifier while the method from Salman et al.requires access to the classifier . Strengths : -strong quantitative results . Experimental section is promising . The gap with white box smoothing is small on cifar . The method outperforms Salman et al.-Only need to train one score network to handle various types of noise type/level -Denoiser does n't need access to the pretrained classifier . Weaknesses : -In my opinion writing can be improved . I am not very familiar with the literature and it took me some time to understand section 3 . More specifically I did not understand the motivation for using score based denoisers rather than a more `` standard '' algorithm for blind denoising . -Method seems to be effective for low-resolution images only . The gap with white box increases on Imagenet . Questions for the authors : -If I understand well , one-step denoiser can only handle gaussian noise whereas multi-step can be applied to any log concave distribution ? What is the advantage of one-step denoiser over multistep , does it perform better for gaussian noise ? -It is not clear to me why the method does not make use of the pretrained classifier but still outperforms Salman et al.which can acess the classifier ? Is it only due to the denoiser 's performance ? -I do not understand well the motivation for using score-based methods . Why scored based denoising in particular and not other blind denoising models ? The current method is impacted by the size of the images , while most of the existing blind denoising algorithms are not affected by the size of the image . Actually , why even focus on learning base methods ? Can not simpler methods with handcrafted priors do the job ? I might also be wrong . Could you please elaborate ? -Qualitatively speaking it seems to me that the visual performances are not very good when compared to existing denoising algorithms ( maybe a quantitative comparison in term of PSNR with other algorithms would be relevant , rather than only showing qualitative results ) . Also the multi step denoiser seems to give a lot of artefacts in figure 1 . -Are n't there missing reference in the related work regarding blind denoising ? -p5 `` the noise makes the support of the score function to be whole space , [ ... ] non-Gaussian or off-the-manifold samples '' . I do n't understand that part . At this stage I give a weak accept , but I would consider raising my score if authors answer my concerns . Typos : p8 `` without any re-trianing . '' p7 `` smoothingon '' p4 `` matching obejctiv ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your instructive review . We list the response for your questions below : Q1 : If I understand well , one-step denoiser can only handle gaussian noise whereas multi-step can be applied to any log concave distribution ? What is the advantage of one-step denoiser over multistep , does it perform better for gaussian noise ? A1 : Yes.The one-step denoiser is equivalent to a trained denoising autoencoder ( DAE ) which were used in [ 1 ] . DnCNN and MemNet are exemplar for such approach . On the other hand , the multi-step denoiser is a iterative algorithm that optimizes maximum a posteriori ( MAP ) loss with gradient approximated by score function , which allows various log-concave noise distributions . The one-step denoiser is much faster than multi-step denoiser as it only requires one forward pass to achieve a denoised image . Moreover , the empirical results show that one-step denoiser performs better than multi-step denoiser for Gaussian test setting . On the other hand , multi-step denoiser is 'efficient ' that can deal with various noise types . Q2 : It is not clear to me why the method does not make use of the pretrained classifier but still outperforms Salman et al.which can acess the classifier ? Is it only due to the denoiser 's performance ? A2 : The performance gap between [ 1 ] and our can be explained with two aspects : the use of better network architecture and the multi-scale training . We used the U-net based architecture with improved convolution layer as illustrated in [ 2 ] which is developed for score-based modeling . Besides the impact of architecture , we empirically found out that multi-scale training slightly boosts the performance ( Appendix C ) . Moreover , even though we did n't include in the paper as we think is irrelevant to our interest , we trained score network with classification regularization as done in [ 1 ] . We trained the loss function as following : $ $ L_ { \\text { score } } + \\gamma D_ { KL } ( f ( x ) \\| f ( \\hat { x } ) ) $ $ , where $ \\hat { x } = x -\\sigma^2s_ { \\theta , \\sigma } ( \\tilde { x } ) $ is denoised image from our one-step denoiser . We figure out that the classifier loss marginally improves the performance when $ \\sigma $ is large , but overall there were no difference ."}}