{"year": "2017", "forum": "Bk0FWVcgx", "title": "Topology and Geometry of Half-Rectified Network Optimization", "decision": "Accept (Poster)", "meta_review": "The paper presents an analysis of deep ReLU networks, and contrasts it with linear networks. It makes good progress towards providing a theoretical explanation of the difficult problem of characterizing the critical points of this highly nonconvex function. \n\nI agree with the authors that their approach is superior to earlier works based on spin-glass models or other approximations that make highly unrealistic assumptions. The ReLU structure allows them to provide concrete analysis, without making such approximations, as opposed to more general activation units.\n\nThe relationship between smoothness of data distribution and the level of model overparamterization is used to characterize the ability to reach the global optimum. The intuition that having no approximation error (due to overparameterization) leads to more tractable optimization problem is intuitive. Such findings have been seen before (e.g. in tensor decomposition, the global optima can be reached when the amount of noise is small). I recommend the authors to connect it to such findings for other nonconvex problems.  The paper does not address overfitting, as they themselves point out in the conclusion. However, I do expect it to be a very challenging problem.", "reviews": [{"review_id": "Bk0FWVcgx-0", "review_text": "This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.", "rating": "2: Strong rejection", "reply_text": "We appreciate the review , and regret that the reviewer found the work unconvincing . We have several clarifying questions for the reviewer so as to improve the quality of our manuscript : `` This is an incremental result ( several related results that the authors of the paper mentioned here were already published ) . '' Which results in particular is the reviewer referring to ? The connectivity of linear networks is , of course , not new , as we make clear in the manuscript , but we are not aware of any proof of linear network connectivity that leverages the topology of level sets of the loss function as the primary proof technique . If we have overlooked some of the literature in our discussion in the Introduction , we would of course be more than happy to add the relevant citations to our manuscript . Our proof of the connectivity of linear networks is meant -- in part -- to help motivate the primary innovation of the paper : that thinking about how easy or difficult it is to traverse the level sets of the loss function can provide intuition about the topology of the loss function as well as the prevalence of local minima . Thus , while the result of the linear case proof is not novel , we find that our particular proof is still useful for intuition-building . `` The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical . '' Could the reviewer be more specific about how our results are `` weaker '' ? We admit that the degree to which our results are `` technical '' may be a matter of philosophy , but did the reviewer have a particular alternative proof in mind which uses fewer technical assumptions ? We would be happy to soften our language , as we may not be aware of the `` state of the art '' so to speak . `` The main theoretical result - Theorem 2.4 is not convincing at all . '' Again , we regret that the reviewer found the result unconvincing . Could the reviewer clarify what they mean by this ? We will expand the discussion around Theorem 2.4 to make the practical relevance of the bound slightly more clear in the next iteration of the manuscript . `` Furthermore , the paper is badly written . No theoretical intuition is given , the experimental section is weak and in some places the formatting is wrong . '' The weakness of the experimental section notwithstanding , the primary purpose of the section was to be exploratory . We aimed to provide evidence that a wide variety of models that are significantly more complicated than single ReLU networks * also * appear to have the same sort of connectedness leveraged by our proof technique . It is also meant to provide intuition for the proof technique -- that these paths through the space of weights which keep test loss low are actually tractable to calculate , and not abstract mathematical objects . That said , could the reviewer be more specific about what they find `` weak '' ? We would be more than happy to expand parts of the experimental section , especially if the intent was not clear . We are aware of the slight page-break in eq . ( 10 ) , and the broken links in Appendix E. If the reviewer spotted any more formatting errors , we will gladly fix them !"}, {"review_id": "Bk0FWVcgx-1", "review_text": "This paper studies the energy landscape of the loss function in neural networks. It is generally clearly written and nicely provides intuitions for the results. One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized. It also quantifies, in a way, the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path. It would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent. The paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path. Using this they show that the loss seems to become more nonconvex when the loss is smaller. This is also quite interesting. The work does have some significant limitations, which is not surprising given the difficulty of fully analyzing the network loss function. However, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss. I would have also appreciated a little more practical discussion of the bound in Theorem 2.4. It is hard to tell whether this bound is tight enough to be practically relevant. ", "rating": "7: Good paper, accept", "reply_text": "We appreciate the feedback from the reviewer ! We have a couple of comments below : `` The work does have some significant limitations , which is not surprising given the difficulty of fully analyzing the network loss function . However , the authors are quite clear about these limitations , which especially include not yet analyzing deep networks and analyzing only the oracle loss , and not the empirical loss . I would have also appreciated a little more practical discussion of the bound in Theorem 2.4 . It is hard to tell whether this bound is tight enough to be practically relevant . '' Indeed , we are also quite interested in further studying the tradeoff between model overparameterization and ease-of-training . Saying too much more than this is difficult because the bound used in Theorem 2.4 is strongly problem dependent . Using our normalized geodesic measure , it should be straightforward to probe this tradeoff on individual problems by simply varying network size . And , encouragingly , it seems that the amount of overparameterization necessary to provide connections , in practice , is small . If we had to speculate , the ease with which we can connect networks might be because state-of-the-art networks that are used on modern datasets are actually already quite over-parameterized . See for example Zhang et al . 's `` Understanding deep learning requires rethinking generalization '' , where the authors demonstrate that neural networks can easily completely memorize entire training sets . Thus , modern networks might be considerably larger than is necessary to be connected ."}, {"review_id": "Bk0FWVcgx-2", "review_text": "This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area. Pros: 1. Providing new theory about existence of \"poor\" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer. 2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. Cons: The results are very specific in both topology and geometry analysis. 1. The analysis is performed only over a \"single\" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. 2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm. With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate the extensive feedback from the reviewer ! We have a couple comments on the reviewer 's feedback . `` The results are very specific in both topology and geometry analysis . 1.The analysis is performed only over a `` single '' hidden layer ReLU network . Given the importance of depth in deep architectures , this result can not really explain the kinds of architectures we are interested in practically . '' Indeed , one long term goal would be an extension of our results to deeper architectures . We find the numerical experiments encouraging in that connectivity appears to be easy to achieve in deep convnets , which seems to suggest that a more general strategy exists for connecting models to one another ( the paths used in our proof construction are quite specific to the architecture , as the reviewer points out , and were chosen so as to be analytically tractable ) . The reviewer might also be interested in our our response to Reviewer 1 discussing the generalization error results of Zhang et al . `` 2.The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points . For example , there might exist a straight line between the two ( which is considered as easy by the geodesic criterion ) , but this line might be going through a very narrow valley , challenging gradient based optimization algorithms ( and thus extremely difficult to navigate in practice ) . '' We definitely agree that there could be easy paths missed by our algorithm because they might be in narrow , hard-to-find valleys . And indeed , in practice , the paths that are generated by our algorithm are sampled from those regions of weight space that are tractable to explore via gradient descent . While it would be great to be able to find the truly minimal geodesic path connecting two randomly trained networks , we suspect that this may become intractable once networks are trained sufficiently close to their global minimum . Philosophically , it might even be undesirable to identify two networks as `` connected '' if it takes an intractable-to-find path in weight space to connect them . That is , there are two separate questions that are independently interesting : 1 . Can networks be connected to one another in principle ? and 2.Can networks be connected to one another in practice ? We hope that our work can help illuminate both of these questions . `` In addition , the proposed algorithm for computing the normalized geodesic is a greedy heuristic , which as far as I can tell , makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm . '' We agree that the greedy heuristic makes it difficult to fully trust the measured normalized geodesics . We would like to emphasize , though , that the greedy algorithm , in those cases where it succeeds in finding a connecting path , does provide an upper bound to the true normalized geodesic length . We 've considered several alternative algorithms , and hope to pursue the alternative algorithm discussed in Appendix A in the future ."}], "0": {"review_id": "Bk0FWVcgx-0", "review_text": "This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.", "rating": "2: Strong rejection", "reply_text": "We appreciate the review , and regret that the reviewer found the work unconvincing . We have several clarifying questions for the reviewer so as to improve the quality of our manuscript : `` This is an incremental result ( several related results that the authors of the paper mentioned here were already published ) . '' Which results in particular is the reviewer referring to ? The connectivity of linear networks is , of course , not new , as we make clear in the manuscript , but we are not aware of any proof of linear network connectivity that leverages the topology of level sets of the loss function as the primary proof technique . If we have overlooked some of the literature in our discussion in the Introduction , we would of course be more than happy to add the relevant citations to our manuscript . Our proof of the connectivity of linear networks is meant -- in part -- to help motivate the primary innovation of the paper : that thinking about how easy or difficult it is to traverse the level sets of the loss function can provide intuition about the topology of the loss function as well as the prevalence of local minima . Thus , while the result of the linear case proof is not novel , we find that our particular proof is still useful for intuition-building . `` The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical . '' Could the reviewer be more specific about how our results are `` weaker '' ? We admit that the degree to which our results are `` technical '' may be a matter of philosophy , but did the reviewer have a particular alternative proof in mind which uses fewer technical assumptions ? We would be happy to soften our language , as we may not be aware of the `` state of the art '' so to speak . `` The main theoretical result - Theorem 2.4 is not convincing at all . '' Again , we regret that the reviewer found the result unconvincing . Could the reviewer clarify what they mean by this ? We will expand the discussion around Theorem 2.4 to make the practical relevance of the bound slightly more clear in the next iteration of the manuscript . `` Furthermore , the paper is badly written . No theoretical intuition is given , the experimental section is weak and in some places the formatting is wrong . '' The weakness of the experimental section notwithstanding , the primary purpose of the section was to be exploratory . We aimed to provide evidence that a wide variety of models that are significantly more complicated than single ReLU networks * also * appear to have the same sort of connectedness leveraged by our proof technique . It is also meant to provide intuition for the proof technique -- that these paths through the space of weights which keep test loss low are actually tractable to calculate , and not abstract mathematical objects . That said , could the reviewer be more specific about what they find `` weak '' ? We would be more than happy to expand parts of the experimental section , especially if the intent was not clear . We are aware of the slight page-break in eq . ( 10 ) , and the broken links in Appendix E. If the reviewer spotted any more formatting errors , we will gladly fix them !"}, "1": {"review_id": "Bk0FWVcgx-1", "review_text": "This paper studies the energy landscape of the loss function in neural networks. It is generally clearly written and nicely provides intuitions for the results. One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized. It also quantifies, in a way, the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path. It would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent. The paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path. Using this they show that the loss seems to become more nonconvex when the loss is smaller. This is also quite interesting. The work does have some significant limitations, which is not surprising given the difficulty of fully analyzing the network loss function. However, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss. I would have also appreciated a little more practical discussion of the bound in Theorem 2.4. It is hard to tell whether this bound is tight enough to be practically relevant. ", "rating": "7: Good paper, accept", "reply_text": "We appreciate the feedback from the reviewer ! We have a couple of comments below : `` The work does have some significant limitations , which is not surprising given the difficulty of fully analyzing the network loss function . However , the authors are quite clear about these limitations , which especially include not yet analyzing deep networks and analyzing only the oracle loss , and not the empirical loss . I would have also appreciated a little more practical discussion of the bound in Theorem 2.4 . It is hard to tell whether this bound is tight enough to be practically relevant . '' Indeed , we are also quite interested in further studying the tradeoff between model overparameterization and ease-of-training . Saying too much more than this is difficult because the bound used in Theorem 2.4 is strongly problem dependent . Using our normalized geodesic measure , it should be straightforward to probe this tradeoff on individual problems by simply varying network size . And , encouragingly , it seems that the amount of overparameterization necessary to provide connections , in practice , is small . If we had to speculate , the ease with which we can connect networks might be because state-of-the-art networks that are used on modern datasets are actually already quite over-parameterized . See for example Zhang et al . 's `` Understanding deep learning requires rethinking generalization '' , where the authors demonstrate that neural networks can easily completely memorize entire training sets . Thus , modern networks might be considerably larger than is necessary to be connected ."}, "2": {"review_id": "Bk0FWVcgx-2", "review_text": "This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area. Pros: 1. Providing new theory about existence of \"poor\" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer. 2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. Cons: The results are very specific in both topology and geometry analysis. 1. The analysis is performed only over a \"single\" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. 2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm. With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate the extensive feedback from the reviewer ! We have a couple comments on the reviewer 's feedback . `` The results are very specific in both topology and geometry analysis . 1.The analysis is performed only over a `` single '' hidden layer ReLU network . Given the importance of depth in deep architectures , this result can not really explain the kinds of architectures we are interested in practically . '' Indeed , one long term goal would be an extension of our results to deeper architectures . We find the numerical experiments encouraging in that connectivity appears to be easy to achieve in deep convnets , which seems to suggest that a more general strategy exists for connecting models to one another ( the paths used in our proof construction are quite specific to the architecture , as the reviewer points out , and were chosen so as to be analytically tractable ) . The reviewer might also be interested in our our response to Reviewer 1 discussing the generalization error results of Zhang et al . `` 2.The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points . For example , there might exist a straight line between the two ( which is considered as easy by the geodesic criterion ) , but this line might be going through a very narrow valley , challenging gradient based optimization algorithms ( and thus extremely difficult to navigate in practice ) . '' We definitely agree that there could be easy paths missed by our algorithm because they might be in narrow , hard-to-find valleys . And indeed , in practice , the paths that are generated by our algorithm are sampled from those regions of weight space that are tractable to explore via gradient descent . While it would be great to be able to find the truly minimal geodesic path connecting two randomly trained networks , we suspect that this may become intractable once networks are trained sufficiently close to their global minimum . Philosophically , it might even be undesirable to identify two networks as `` connected '' if it takes an intractable-to-find path in weight space to connect them . That is , there are two separate questions that are independently interesting : 1 . Can networks be connected to one another in principle ? and 2.Can networks be connected to one another in practice ? We hope that our work can help illuminate both of these questions . `` In addition , the proposed algorithm for computing the normalized geodesic is a greedy heuristic , which as far as I can tell , makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm . '' We agree that the greedy heuristic makes it difficult to fully trust the measured normalized geodesics . We would like to emphasize , though , that the greedy algorithm , in those cases where it succeeds in finding a connecting path , does provide an upper bound to the true normalized geodesic length . We 've considered several alternative algorithms , and hope to pursue the alternative algorithm discussed in Appendix A in the future ."}}