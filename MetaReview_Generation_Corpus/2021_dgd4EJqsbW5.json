{"year": "2021", "forum": "dgd4EJqsbW5", "title": "Control-Aware Representations for Model-based Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper addresses the question of RL in high-dimensional spaces by learning lower-dimensional representations for control purposes. The work contains both theoretical and empirical results that shows the promise of the proposed approach.\n\nWhile the reviewers had initial concerns, including with a problem in a proof and questions around the contributions, after robust responses and discussions this paper is now in good shape.", "reviews": [{"review_id": "dgd4EJqsbW5-0", "review_text": "This paper aims to address an important question in reinforcement learning : policy learning from high-dimensional sensory observations . The authors propose an algorithm for Learning Controllable Embedding ( LCE ) based on policy iteration in the latent space . The authors provide a theorem to show how the policy performance in latent-space policy improvement depends on the learned representation and develop three algorithmic variations that attempt to maximize the theoretical lower bounds . In the experiments , the proposed algorithm CARL shows improved performance when compared with other LCE baseline algorithms . While I 'm not particularly familiar with the field of LCE , I think the idea of learning a representation that is suitable for policy improvement is an interesting idea . The readability of this paper is also pretty good , which can be difficult to get right because the of the correspondence between the original space and the latent space . Overall the paper is easy to follow . While I do think Algorithm 1 is reasonable , I found its theoretical foundation , namely Theorem 1 , is incorrect . In the proof of Theorem 7 on p15 in the appendix , I do not think the implication T^2 VE ( x ) < T VE ( x ) + \\gamma Delta ( x ) for all x , would hold . Because Bellman operator contracts in the L-inf norm , a basic inequality would rather take a form of T^2 VE ( x ) < T VE ( x ) + \\gamma sup_y Delta ( y ) . In addition to this , another minor error happens in the first equation on pg 16 , where I believe the correct right hand side would be 1/ ( 1-gamma ) sup_y Delta ( y ) , without the gamma dependency . However , a bound that depends on L-inf norm would be quite bad for Theorem 1 , and current data collection process in Alg 1 is not sufficient for minimizing it . I think it might be possible not using an L-inf bound but using an expected error based on the policy 's rollout distribution . However , this change would largely change the theoretical results , and perhaps the motivation or details of the algorithm design . Therefore , I do not think the paper is ready for acceptance at the current stage without a large revision . If the authors can address this question properly , I would raise my score . Beyond the flaw in the theory , there are some parts which can benefit from some clarification : 1 . In the offline CARL , how does the algorithm address the issue of out of distribution error due to using a batch dataset ? 2.The authors argue that the loss here is different from PCC many times in the paper , but they never explain whether the choice here is better ( or in which way ) . 3.In line 4 of Alg 1 , how do we ensure such pi would exist ? 4.What is the definition of `` compatible reward function '' in the last paragraph on p4 ? 5.For completeness of presentation , please include the definition of curvature loss .", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c Proof of Theorem 7 \u201d We thank the reviewer for bringing into our attention this major typo in the proof of Theorem 7 , and as a result in the statement of Theorem 1 ( Eq.4 ) .You are right , when we apply the Bellman operator to TV - V < \\Delta , we do not obtain T^2V ( x ) - TV ( x ) < \\gamma \\Delta ( x ) , for any x . However , we can show that for any x , we have T^2V ( x ) - TV ( x ) < \\gamma E_ { x\\sim P_ { \\pi o E } } ] [ \\Delta ( x ) ] ) . This will result in a change in the statement of Theorem 7 and all the subsequent results , and finally in the statement of Theorem 1 , that x should come from the \\gamma-occupancy measure of the current policy ( \\pi o E ) . So , we do not obtain a result in L-inf norm that as correctly mentioned by the reviewer is not desirable . We revised the appendix and the statement of Theorem 1 ( Eq.4 ) in the paper to reflect this change . However , this change has no effect on the CARL algorithms as the samples are collected by following the current policy ( \\pi o E ) . Response to other questions 1 ) In Offline CARL , similar to other LCE methods , such as E2C and PCC , we assume that the data is collected by an exploratory policy that provides a good coverage of the parts of the state-action space that are relevant to the task at hand ( see the discussions on Page 5 ) . Violation of this assumption would add error to the process , similar to all offline RL settings , and require certain corrections to alleviate its effects . Studying this issue is outside the scope of this work and is an interesting future direction . 2 ) As mentioned in the paper , the CARL \u2019 s loss function has close connections to that in PCC ( see Page 5 ) , although they have been derived from completely different perspectives . While the loss function in CARL has been derived such that the learned representation is suitable for a policy iteration style algorithm , the one in PCC has been derived to learn a latent space that is amenable to locally linear control algorithms . As discussed in Section 3 , since PCC has been designed for an offline setting ( i.e. , one-shot representation learning and control ) , its prediction and consistency terms are independent of a particular policy and are defined for state-action pairs . While CARL has been designed for an online setting ( i.e. , interleaving representation learning and control ) , and thus , all its loss terms depend on the current policy . Despite the similarities in loss functions , as we show in our experiments , offline CARL ( which is a member of the CARL family ) outperforms PCC . Moreover , the other members of the CARL family are more superior to PCC and offline CARL , as shown in our experiments , mainly because they better address the data collection issue discussed in Question 1 , by interleaving representation learning and control . 3 ) As discussed in the description of online CARL in Section 4 , we approximate the operation in Line 4 of Algorithm 1 by a process we refer to as policy distillation . To compute \\pi , we project the observation policy \\mu onto the latent space by minimizing D_KL ( \\mu || \\pi \\circ E ) . There might be other ways to implement this line of the algorithm ( including removing it ) , which requires more investigation . We showed in our experiments ( see Appendix F1 ) that the results when we remove this line ( no distillation ) are worse than those with distillation . We revised the paper to better explain this step of the algorithm . 4 ) Similar to other LCE methods , such as E2C and PCC , we define the reward function in the latent space . We refer to a reward function as compatible , if when it is optimized ( in the latent space ) , the resulting policy ( projected back to the observation space ) solves the problem . For example , in a goal-based task , a reward function that measures the negative distance from each latent state to the image of the goal ( in the latent space ) is compatible . For clarification , we added the above description to the paper ( see Footnote 4 ) . 5 ) We added the definition of the curvature loss , which is identical to that in the PCC paper , at the end of Section 3 ( see Page 5 ) ."}, {"review_id": "dgd4EJqsbW5-1", "review_text": "This paper proposes a new representation learning + RL algorithm called CARL , with a specific objective for learning a latent representation and dynamics model coupled with SAC policy learning in the latent space . Experiments on a few domains show CARL outperforming previous algorithms such as DREAMER and PCC . Pros : + The key points of the paper are relatively well organized and motivated properly . + The experimental results succinctly demonstrate the promise of the proposed approach . Cons : - It is difficult to follow important details about the operation of this relatively complicated method . - The experimental results are not sufficient for this largely empirical work . With these pros and cons in mind , I am recommending a weak reject . See below for additional detailed comments . EDIT : After discussion , I have increased my score and am recommending weak accept . See the discussion with the authors for details . Quality The paper studies an important problem , proposes a novel solution , and has promising experimental results . However , the main drawback in terms of the quality of the work is that the results are not complete enough . For work that is empirically driven , I do not view the current results as sufficient for publication . In particular , DREAMER appears to be competitive with CARL on a few domains . But DREAMER was also evaluated much more broadly across many tasks from the DeepMind control suite , indicating a level of robustness and performance that is , at best , hinted at in this work for CARL . A wider suite of experiments , for example using the same tasks as DREAMER , would go a long way in better shaping the reader 's understanding of the proposed method . Clarity As mentioned , the main points of the paper are presented well . The problem is properly motivated , and a central theorem gives rise to the proposed representation learning method . I did not check the proof for this theorem , but it appears sensible . However , the finer points in the paper , which are also very important , are difficult to follow . For example , what is `` model-based SAC '' ? There does not appear to be a proper explanation or citation for this . Is the learned dynamics model F used in some way to learn the Q-function ? Is this novel , or is it from prior work ? Considering the proposition that replacing other control algorithms in the latent space with model-based SAC is important for the overall performance improvement , a description of model-based SAC is important . Furthermore , an ablation study would be helpful in terms of understanding the relative importance of this change vs the proposed representation learning approach , which seems to be the novel part . Some other minor concerns about the methodological sections : there are many hyperparameters and not much guidance as to how to pick these ; more discussion of why there are different versions of CARL and what are their respective strengths and weaknesses would be useful , especially for VCARL ; I personally found the last paragraph of the VCARL description almost impossible to follow . Originality To the best of my knowledge , the representation learning algorithm itself is novel . Perhaps a related work that is overlooked is https : //arxiv.org/abs/1907.00953 , which apparently has been accepted to NeurIPS 2020 but has been out for some time . At a high level , this work also incorporates representation learning into SAC , though the underlying details are different . Still , this approach seems actually more closely related than some of the current citations and comparisons , e.g. , SOLAR and DREAMER . At least a citation seems to be in order , and preferably a comparison . Indeed , this prior work also carries out a more comprehensive evaluation on more tasks than the current work . Significance This work has the potential to be significant , as many researchers and practitioners are currently interested in how to make deep RL more efficient and performative , in particular in visual settings . However , without a more comprehensive evaluation , it is difficult to judge for sure .", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c more experiments \u201d It is important to note that in addition to proposing algorithms to tackle the important problem of control from high-dimensional observations , we consider deriving a representation learning loss function from the control and dynamic programming principles as another contribution of our work . Returning to the sufficiency of our experiment , as explained in response to Reviewer 1 , we study control problems in which the observations x have been selected such that the system is Markovian in the observation space X ( see the beginning of Section 2 ) . This is the same class of problems studied in E2C , RCE , SOLAR , and PCC , and this is why we selected them as our baselines , used the problems in their experiments , and conducted a comprehensive evaluation of these methods . We also experimented with Dreamer but as discussed in the paper , we did not expect it to perform well in our problems , because it has been designed for more general class of control problems ( than X-Markovian ) , those that can be modeled as a POMDP . The goal of this paper is not to derive an algorithm that outperforms Dreamer ( or similar algorithms ) in problems that belong to the DeepMind suit . Our goal is to derive a representation learning loss function that is suitable for an important class of controllers ( approximate policy iteration ) and devise algorithms that properly interleave this representation learning and control . As explained in response to Reviewer 1 , extending CARL to handle more involved problems requires using more powerful encoders ( e.g. , RNNs ) and learning the latent reward function as a part of the representation learning process , which we believe both are doable . We left this extension and experimenting with more involved environments as a part of our future work . \u201c model-based SAC \u201d Model-based SAC is simply SAC when the data is generated from the model , instead of from the agent \u2019 s interaction with the environment . We thought that the meaning is clear , but to further clarify , we added a footnote ( Footnote 6 ) to the updated version of the paper . However , we do not believe that this is enough reason for the reviewer to question the clarity of the paper and to state that important details are difficult to follow . \u201c ablation study \u201d We have done several ablation studies in the paper . Comparing offline CARL with PCC , because of the close connections between their loss functions , shows the importance of SAC as the control algorithm in place of iLQR in PCC . Comparing online CARL with offline CARL and PCC shows the importance of interleaving representation learning and control . Comparing online CARL with SOLAR shows the advantage of using the CARL loss function . Comparing CARL with and without policy distillation shows the effect of this process in the performance of the algorithm . Although there are other combinations that can be investigated , we believe we have already done a fair amount of ablation studies in the paper . If the reviewer has a particular ablation study in mind , it would be good to clearly state it that we see if we can provide its results by the end of the rebuttal phase . \u201c hyper-parameters \u201d Please see our response to Reviewer 1 . To summarize , the theory ( Theorem 1 ) provides a high-level guideline for selecting the hyper-parameters of CARL \u2019 s loss function . However , in practice , to further optimize the performance of the CARL algorithms , we set ( a subset of ) these hyper-parameters via grid search . To address the reviewer \u2019 s comment , we clarified this point by adding a discussion in Footnote 5 . \u201c why there are different versions of CARL \u201d Offline CARL is for problems in which a large batch of exploratory data is available in advance , and thus , interleaving representation learning and control can not add much value to the method . Moreover , it is used for an ablation study to compare CARL with PCC and see the effect of using SAC instead of iLQR . The main goal of V-CARL , as explained in the paper , is to establish a closer connection between representation learning and control by weighing the loss function using the TD-error of the current policy . \u201c a related work \u201d We thank the reviewer for bringing the SLAC work into our attention . It definitely has some connections to our work . We added a reference to it in the updated version of the paper ."}, {"review_id": "dgd4EJqsbW5-2", "review_text": "This paper examines the problem of learning controllable embedding ( LCE ) , with the goal of learning good representations ( usually achieved using variational inference algorithms ) such that the maximum cumulative reward can be achieved . The main difference lies in the simultaneous learning of both the low-dimensional latent space as well as the action policy . One of the main strengths of this paper is found in Theorem 1 . The authors devise a simple policy iteration approach in the low dimensional learned space . Then , using mostly qualitative analysis , a bound on the policy improvement error is formulated . This error combines several intuitive and straightforward factors , which are then extracted to form more involved offline and online reinforcement learning algorithms . I have read through the proofs , and they seems correct . While I appreciate the quality of the theoretical work , the paper had some drawbacks that brought me to my current score : 1 . The loss function consists of many hyper-parameters . The authors should provide some guidelines for choosing these hyper-parameters due to the large number of possible combinations , and clearly state how the scalings affect performance . 2.Experimentation is lacking . While the authors conducted experiments mostly on toy problems , I expect them to compare against more involved environments which are harder to model . Their comparison with state of the art algorithms ( e.g.Dreamer ) which were also tested on such environments is thus not fair . 3.Minor comment : There is a newer version of Dreamer that the authors can compared against : https : //arxiv.org/pdf/2010.02193.pdf 4 . Minor comment : How would CARL compare against model-free offline RL methods , or generally to algorithms that are not SAC ? Question to authors : Would there be a benefit in removing F altogether and learning a mapping X - > Z - > X \u2019 without transitioning in the latent space ? ( i.e. , errors III and IV in Theorem 1 ) Finally , it would be beneficial if the authors could include code for their work . If the authors ca n't supply the complete code base , even code snippets with clarifying explanations to demonstrate their main ideas would be beneficial . This would greatly improve the quality and credibility of their work as well as the reviews . To conclude , the paper provides strong theoretical intuition , which is a significant value-add of the paper . Nevertheless , its lack of experimentation and large number of hyper-parameters limit its overall quality . If the authors provide substantial improvement in the experimentation I will increase my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Response to main questions 1 ) Theorem 1 provides a high-level guideline for selecting the hyper-parameters of the loss function : \\lambda_ { ed } = 2R_\\max / ( 1-\\gamma ) ^2 , \\lambda_c = \\lambda_p = \\sqrt { 2 } \\gamma R_\\max / ( 1-\\gamma ) ^2 , and \\lambda_ { reg } = \\sqrt { 2 } \\gamma R_\\max / ( 1-\\gamma ) . However , in practice , to further optimize the performance of the CARL algorithms , we set ( a subset of ) these hyper-parameters via grid search . To address the reviewer \u2019 s comment , we clarified this point by adding a discussion in Footnote 5 . 2 ) It is important to note that we study the class of control problems in which the observations x have been selected such that the system is Markovian in the observation space X ( see the beginning of Section 2 ) . This is the same class of problems studied in E2C , RCE , SOLAR , and PCC , and this is why we selected them as our baselines . We did not include the results of E2C and RCE , because PCC has previously shown to be superior to them ( see Levine et al.2019 ) .We compared our algorithms with Dreamer in X-Markovian problems considered in the paper , and Dreamer did not perform as well as CARL ( with the same number of samples ) . This was expected as Dreamer has been designed for more general class of control problems ( than X-Markovian ) , those that can be modeled as a POMDP . Extension of CARL to properly handle more involved environments ( e.g. , POMDP problems ) requires using more powerful encoders ( e.g. , RNNs ) and learning the latent reward function as a part of the representation learning process . As shown in Section 3 , learning the reward function is in fact a part of the CARL \u2019 s loss function and can be easily included in the algorithm . Using other encoders requires a bit more work but we believe it should be doable . We left this extension and experimenting with more involved environments as a part of our future work . 3 ) We were not aware of this very new version of Dreamer and thank the reviewer for providing a reference to it . We added a citation to this work in the updated version of the paper . 4 ) As described in the paper , CARL is a model-based RL ( MBRL ) algorithm that works in the latent space . Similar to most comparisons between MBRL and model-free RL algorithms ( e.g. , see the MBPO paper ) , when the dynamics is learned reasonably accurately , CARL can be much more data-efficient than any model-free algorithm . However , we can definitely find problems in which a model-free algorithm outperforms CARL . \u201c removing F \u201d Removing F and learning the mapping X - > Z - > X \u2019 is definitely a viable approach that definitely has the potential to be investigated as a future work . However , as described both in the paper and in Comment 4 about a model-free approach , these are all reasonable approaches that can work ( or do not work ) for some problems . However , the main idea of LCE ( and as a result CARL ) is to avoid direct prediction of the next observation , which could be challenging when the observation is high dimensional . Instead LCE suggests to learn a latent space and a latent dynamics F , and to control the systems there . \u201c Code \u201d The code is in PyTorch and we plan to open-source it with the final version of the paper ."}], "0": {"review_id": "dgd4EJqsbW5-0", "review_text": "This paper aims to address an important question in reinforcement learning : policy learning from high-dimensional sensory observations . The authors propose an algorithm for Learning Controllable Embedding ( LCE ) based on policy iteration in the latent space . The authors provide a theorem to show how the policy performance in latent-space policy improvement depends on the learned representation and develop three algorithmic variations that attempt to maximize the theoretical lower bounds . In the experiments , the proposed algorithm CARL shows improved performance when compared with other LCE baseline algorithms . While I 'm not particularly familiar with the field of LCE , I think the idea of learning a representation that is suitable for policy improvement is an interesting idea . The readability of this paper is also pretty good , which can be difficult to get right because the of the correspondence between the original space and the latent space . Overall the paper is easy to follow . While I do think Algorithm 1 is reasonable , I found its theoretical foundation , namely Theorem 1 , is incorrect . In the proof of Theorem 7 on p15 in the appendix , I do not think the implication T^2 VE ( x ) < T VE ( x ) + \\gamma Delta ( x ) for all x , would hold . Because Bellman operator contracts in the L-inf norm , a basic inequality would rather take a form of T^2 VE ( x ) < T VE ( x ) + \\gamma sup_y Delta ( y ) . In addition to this , another minor error happens in the first equation on pg 16 , where I believe the correct right hand side would be 1/ ( 1-gamma ) sup_y Delta ( y ) , without the gamma dependency . However , a bound that depends on L-inf norm would be quite bad for Theorem 1 , and current data collection process in Alg 1 is not sufficient for minimizing it . I think it might be possible not using an L-inf bound but using an expected error based on the policy 's rollout distribution . However , this change would largely change the theoretical results , and perhaps the motivation or details of the algorithm design . Therefore , I do not think the paper is ready for acceptance at the current stage without a large revision . If the authors can address this question properly , I would raise my score . Beyond the flaw in the theory , there are some parts which can benefit from some clarification : 1 . In the offline CARL , how does the algorithm address the issue of out of distribution error due to using a batch dataset ? 2.The authors argue that the loss here is different from PCC many times in the paper , but they never explain whether the choice here is better ( or in which way ) . 3.In line 4 of Alg 1 , how do we ensure such pi would exist ? 4.What is the definition of `` compatible reward function '' in the last paragraph on p4 ? 5.For completeness of presentation , please include the definition of curvature loss .", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c Proof of Theorem 7 \u201d We thank the reviewer for bringing into our attention this major typo in the proof of Theorem 7 , and as a result in the statement of Theorem 1 ( Eq.4 ) .You are right , when we apply the Bellman operator to TV - V < \\Delta , we do not obtain T^2V ( x ) - TV ( x ) < \\gamma \\Delta ( x ) , for any x . However , we can show that for any x , we have T^2V ( x ) - TV ( x ) < \\gamma E_ { x\\sim P_ { \\pi o E } } ] [ \\Delta ( x ) ] ) . This will result in a change in the statement of Theorem 7 and all the subsequent results , and finally in the statement of Theorem 1 , that x should come from the \\gamma-occupancy measure of the current policy ( \\pi o E ) . So , we do not obtain a result in L-inf norm that as correctly mentioned by the reviewer is not desirable . We revised the appendix and the statement of Theorem 1 ( Eq.4 ) in the paper to reflect this change . However , this change has no effect on the CARL algorithms as the samples are collected by following the current policy ( \\pi o E ) . Response to other questions 1 ) In Offline CARL , similar to other LCE methods , such as E2C and PCC , we assume that the data is collected by an exploratory policy that provides a good coverage of the parts of the state-action space that are relevant to the task at hand ( see the discussions on Page 5 ) . Violation of this assumption would add error to the process , similar to all offline RL settings , and require certain corrections to alleviate its effects . Studying this issue is outside the scope of this work and is an interesting future direction . 2 ) As mentioned in the paper , the CARL \u2019 s loss function has close connections to that in PCC ( see Page 5 ) , although they have been derived from completely different perspectives . While the loss function in CARL has been derived such that the learned representation is suitable for a policy iteration style algorithm , the one in PCC has been derived to learn a latent space that is amenable to locally linear control algorithms . As discussed in Section 3 , since PCC has been designed for an offline setting ( i.e. , one-shot representation learning and control ) , its prediction and consistency terms are independent of a particular policy and are defined for state-action pairs . While CARL has been designed for an online setting ( i.e. , interleaving representation learning and control ) , and thus , all its loss terms depend on the current policy . Despite the similarities in loss functions , as we show in our experiments , offline CARL ( which is a member of the CARL family ) outperforms PCC . Moreover , the other members of the CARL family are more superior to PCC and offline CARL , as shown in our experiments , mainly because they better address the data collection issue discussed in Question 1 , by interleaving representation learning and control . 3 ) As discussed in the description of online CARL in Section 4 , we approximate the operation in Line 4 of Algorithm 1 by a process we refer to as policy distillation . To compute \\pi , we project the observation policy \\mu onto the latent space by minimizing D_KL ( \\mu || \\pi \\circ E ) . There might be other ways to implement this line of the algorithm ( including removing it ) , which requires more investigation . We showed in our experiments ( see Appendix F1 ) that the results when we remove this line ( no distillation ) are worse than those with distillation . We revised the paper to better explain this step of the algorithm . 4 ) Similar to other LCE methods , such as E2C and PCC , we define the reward function in the latent space . We refer to a reward function as compatible , if when it is optimized ( in the latent space ) , the resulting policy ( projected back to the observation space ) solves the problem . For example , in a goal-based task , a reward function that measures the negative distance from each latent state to the image of the goal ( in the latent space ) is compatible . For clarification , we added the above description to the paper ( see Footnote 4 ) . 5 ) We added the definition of the curvature loss , which is identical to that in the PCC paper , at the end of Section 3 ( see Page 5 ) ."}, "1": {"review_id": "dgd4EJqsbW5-1", "review_text": "This paper proposes a new representation learning + RL algorithm called CARL , with a specific objective for learning a latent representation and dynamics model coupled with SAC policy learning in the latent space . Experiments on a few domains show CARL outperforming previous algorithms such as DREAMER and PCC . Pros : + The key points of the paper are relatively well organized and motivated properly . + The experimental results succinctly demonstrate the promise of the proposed approach . Cons : - It is difficult to follow important details about the operation of this relatively complicated method . - The experimental results are not sufficient for this largely empirical work . With these pros and cons in mind , I am recommending a weak reject . See below for additional detailed comments . EDIT : After discussion , I have increased my score and am recommending weak accept . See the discussion with the authors for details . Quality The paper studies an important problem , proposes a novel solution , and has promising experimental results . However , the main drawback in terms of the quality of the work is that the results are not complete enough . For work that is empirically driven , I do not view the current results as sufficient for publication . In particular , DREAMER appears to be competitive with CARL on a few domains . But DREAMER was also evaluated much more broadly across many tasks from the DeepMind control suite , indicating a level of robustness and performance that is , at best , hinted at in this work for CARL . A wider suite of experiments , for example using the same tasks as DREAMER , would go a long way in better shaping the reader 's understanding of the proposed method . Clarity As mentioned , the main points of the paper are presented well . The problem is properly motivated , and a central theorem gives rise to the proposed representation learning method . I did not check the proof for this theorem , but it appears sensible . However , the finer points in the paper , which are also very important , are difficult to follow . For example , what is `` model-based SAC '' ? There does not appear to be a proper explanation or citation for this . Is the learned dynamics model F used in some way to learn the Q-function ? Is this novel , or is it from prior work ? Considering the proposition that replacing other control algorithms in the latent space with model-based SAC is important for the overall performance improvement , a description of model-based SAC is important . Furthermore , an ablation study would be helpful in terms of understanding the relative importance of this change vs the proposed representation learning approach , which seems to be the novel part . Some other minor concerns about the methodological sections : there are many hyperparameters and not much guidance as to how to pick these ; more discussion of why there are different versions of CARL and what are their respective strengths and weaknesses would be useful , especially for VCARL ; I personally found the last paragraph of the VCARL description almost impossible to follow . Originality To the best of my knowledge , the representation learning algorithm itself is novel . Perhaps a related work that is overlooked is https : //arxiv.org/abs/1907.00953 , which apparently has been accepted to NeurIPS 2020 but has been out for some time . At a high level , this work also incorporates representation learning into SAC , though the underlying details are different . Still , this approach seems actually more closely related than some of the current citations and comparisons , e.g. , SOLAR and DREAMER . At least a citation seems to be in order , and preferably a comparison . Indeed , this prior work also carries out a more comprehensive evaluation on more tasks than the current work . Significance This work has the potential to be significant , as many researchers and practitioners are currently interested in how to make deep RL more efficient and performative , in particular in visual settings . However , without a more comprehensive evaluation , it is difficult to judge for sure .", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c more experiments \u201d It is important to note that in addition to proposing algorithms to tackle the important problem of control from high-dimensional observations , we consider deriving a representation learning loss function from the control and dynamic programming principles as another contribution of our work . Returning to the sufficiency of our experiment , as explained in response to Reviewer 1 , we study control problems in which the observations x have been selected such that the system is Markovian in the observation space X ( see the beginning of Section 2 ) . This is the same class of problems studied in E2C , RCE , SOLAR , and PCC , and this is why we selected them as our baselines , used the problems in their experiments , and conducted a comprehensive evaluation of these methods . We also experimented with Dreamer but as discussed in the paper , we did not expect it to perform well in our problems , because it has been designed for more general class of control problems ( than X-Markovian ) , those that can be modeled as a POMDP . The goal of this paper is not to derive an algorithm that outperforms Dreamer ( or similar algorithms ) in problems that belong to the DeepMind suit . Our goal is to derive a representation learning loss function that is suitable for an important class of controllers ( approximate policy iteration ) and devise algorithms that properly interleave this representation learning and control . As explained in response to Reviewer 1 , extending CARL to handle more involved problems requires using more powerful encoders ( e.g. , RNNs ) and learning the latent reward function as a part of the representation learning process , which we believe both are doable . We left this extension and experimenting with more involved environments as a part of our future work . \u201c model-based SAC \u201d Model-based SAC is simply SAC when the data is generated from the model , instead of from the agent \u2019 s interaction with the environment . We thought that the meaning is clear , but to further clarify , we added a footnote ( Footnote 6 ) to the updated version of the paper . However , we do not believe that this is enough reason for the reviewer to question the clarity of the paper and to state that important details are difficult to follow . \u201c ablation study \u201d We have done several ablation studies in the paper . Comparing offline CARL with PCC , because of the close connections between their loss functions , shows the importance of SAC as the control algorithm in place of iLQR in PCC . Comparing online CARL with offline CARL and PCC shows the importance of interleaving representation learning and control . Comparing online CARL with SOLAR shows the advantage of using the CARL loss function . Comparing CARL with and without policy distillation shows the effect of this process in the performance of the algorithm . Although there are other combinations that can be investigated , we believe we have already done a fair amount of ablation studies in the paper . If the reviewer has a particular ablation study in mind , it would be good to clearly state it that we see if we can provide its results by the end of the rebuttal phase . \u201c hyper-parameters \u201d Please see our response to Reviewer 1 . To summarize , the theory ( Theorem 1 ) provides a high-level guideline for selecting the hyper-parameters of CARL \u2019 s loss function . However , in practice , to further optimize the performance of the CARL algorithms , we set ( a subset of ) these hyper-parameters via grid search . To address the reviewer \u2019 s comment , we clarified this point by adding a discussion in Footnote 5 . \u201c why there are different versions of CARL \u201d Offline CARL is for problems in which a large batch of exploratory data is available in advance , and thus , interleaving representation learning and control can not add much value to the method . Moreover , it is used for an ablation study to compare CARL with PCC and see the effect of using SAC instead of iLQR . The main goal of V-CARL , as explained in the paper , is to establish a closer connection between representation learning and control by weighing the loss function using the TD-error of the current policy . \u201c a related work \u201d We thank the reviewer for bringing the SLAC work into our attention . It definitely has some connections to our work . We added a reference to it in the updated version of the paper ."}, "2": {"review_id": "dgd4EJqsbW5-2", "review_text": "This paper examines the problem of learning controllable embedding ( LCE ) , with the goal of learning good representations ( usually achieved using variational inference algorithms ) such that the maximum cumulative reward can be achieved . The main difference lies in the simultaneous learning of both the low-dimensional latent space as well as the action policy . One of the main strengths of this paper is found in Theorem 1 . The authors devise a simple policy iteration approach in the low dimensional learned space . Then , using mostly qualitative analysis , a bound on the policy improvement error is formulated . This error combines several intuitive and straightforward factors , which are then extracted to form more involved offline and online reinforcement learning algorithms . I have read through the proofs , and they seems correct . While I appreciate the quality of the theoretical work , the paper had some drawbacks that brought me to my current score : 1 . The loss function consists of many hyper-parameters . The authors should provide some guidelines for choosing these hyper-parameters due to the large number of possible combinations , and clearly state how the scalings affect performance . 2.Experimentation is lacking . While the authors conducted experiments mostly on toy problems , I expect them to compare against more involved environments which are harder to model . Their comparison with state of the art algorithms ( e.g.Dreamer ) which were also tested on such environments is thus not fair . 3.Minor comment : There is a newer version of Dreamer that the authors can compared against : https : //arxiv.org/pdf/2010.02193.pdf 4 . Minor comment : How would CARL compare against model-free offline RL methods , or generally to algorithms that are not SAC ? Question to authors : Would there be a benefit in removing F altogether and learning a mapping X - > Z - > X \u2019 without transitioning in the latent space ? ( i.e. , errors III and IV in Theorem 1 ) Finally , it would be beneficial if the authors could include code for their work . If the authors ca n't supply the complete code base , even code snippets with clarifying explanations to demonstrate their main ideas would be beneficial . This would greatly improve the quality and credibility of their work as well as the reviews . To conclude , the paper provides strong theoretical intuition , which is a significant value-add of the paper . Nevertheless , its lack of experimentation and large number of hyper-parameters limit its overall quality . If the authors provide substantial improvement in the experimentation I will increase my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Response to main questions 1 ) Theorem 1 provides a high-level guideline for selecting the hyper-parameters of the loss function : \\lambda_ { ed } = 2R_\\max / ( 1-\\gamma ) ^2 , \\lambda_c = \\lambda_p = \\sqrt { 2 } \\gamma R_\\max / ( 1-\\gamma ) ^2 , and \\lambda_ { reg } = \\sqrt { 2 } \\gamma R_\\max / ( 1-\\gamma ) . However , in practice , to further optimize the performance of the CARL algorithms , we set ( a subset of ) these hyper-parameters via grid search . To address the reviewer \u2019 s comment , we clarified this point by adding a discussion in Footnote 5 . 2 ) It is important to note that we study the class of control problems in which the observations x have been selected such that the system is Markovian in the observation space X ( see the beginning of Section 2 ) . This is the same class of problems studied in E2C , RCE , SOLAR , and PCC , and this is why we selected them as our baselines . We did not include the results of E2C and RCE , because PCC has previously shown to be superior to them ( see Levine et al.2019 ) .We compared our algorithms with Dreamer in X-Markovian problems considered in the paper , and Dreamer did not perform as well as CARL ( with the same number of samples ) . This was expected as Dreamer has been designed for more general class of control problems ( than X-Markovian ) , those that can be modeled as a POMDP . Extension of CARL to properly handle more involved environments ( e.g. , POMDP problems ) requires using more powerful encoders ( e.g. , RNNs ) and learning the latent reward function as a part of the representation learning process . As shown in Section 3 , learning the reward function is in fact a part of the CARL \u2019 s loss function and can be easily included in the algorithm . Using other encoders requires a bit more work but we believe it should be doable . We left this extension and experimenting with more involved environments as a part of our future work . 3 ) We were not aware of this very new version of Dreamer and thank the reviewer for providing a reference to it . We added a citation to this work in the updated version of the paper . 4 ) As described in the paper , CARL is a model-based RL ( MBRL ) algorithm that works in the latent space . Similar to most comparisons between MBRL and model-free RL algorithms ( e.g. , see the MBPO paper ) , when the dynamics is learned reasonably accurately , CARL can be much more data-efficient than any model-free algorithm . However , we can definitely find problems in which a model-free algorithm outperforms CARL . \u201c removing F \u201d Removing F and learning the mapping X - > Z - > X \u2019 is definitely a viable approach that definitely has the potential to be investigated as a future work . However , as described both in the paper and in Comment 4 about a model-free approach , these are all reasonable approaches that can work ( or do not work ) for some problems . However , the main idea of LCE ( and as a result CARL ) is to avoid direct prediction of the next observation , which could be challenging when the observation is high dimensional . Instead LCE suggests to learn a latent space and a latent dynamics F , and to control the systems there . \u201c Code \u201d The code is in PyTorch and we plan to open-source it with the final version of the paper ."}}