{"year": "2018", "forum": "HJOQ7MgAW", "title": "Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum", "decision": "Reject", "meta_review": "The paper performs an ablation analysis on LSTM, showing that the gating component is the most important. There is little novelty in the analysis, and in its current form, its impact is rather limited.", "reviews": [{"review_id": "HJOQ7MgAW-0", "review_text": "This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate. It shows comparable results with standard LSTM. I believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results. However, the formulation is very similar to \"[1] Semi-supervised Question Retrieval with Gated Convolutions\" 2016 by Lei, and \"Deriving Neural Architectures from Sequence and Graph Kernels\" which give theoretical view from string kernel about why this type of networks works. Both of the two paper don't have output gate and non-linearity of \"Wx_t\" and results on PTB also stronger than this paper. It also have some visualization about how the model decay the weights. Other AnonReviewer also point out some similar work. I won't repeat it here. In the paper, the author argued \"we propose and evaluate the minimal changes...\" but I think the these type of analysis also been covered by [1], Figure 5. On the experimental side, to draw the conclusion, \"weighted sum\" is enough for LSTM. I think at least Machine Translation and other classification results should be added. I'm not very familiar with SQuAD dataset, but the results seems worse than \"Reading Wikipedia to answer open-domain questions\" Table 4 which seems use a vanilla LSTM setup. Update: the revised version of the paper addresses all my concerns about experiments. So I increased my score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We would like to draw the reviewer 's attention to two major points : =Paper Contribution= The goal of this paper is not to propose a model that outperforms LSTMs , but to understand how LSTMs work . The aptly-named `` LSTM - RNN '' is the minimal change from the original LSTM that allows us to test whether the weighted summing mechanism ( as computed by LSTMs ) is sufficiently powerful to facilitate NLP models . The main point of this paper is that the gating mechanism ( which is able to compute element-wise weighted sums ) is a powerful model , and that the embedded vanilla RNN is redundant - at least on the variety of tasks on which we evaluated it on . =Controlled Experiments= By controlling every variable ( model , data , hyperparameters , etc ) and running head-to-head comparisons between LSTM and LSTM - RNN , we deduce whether the recurrence in the content layer is redundant . These experiments are vastly different from the ones in `` Deriving Neural Architectures from Sequence and Graph Kernels '' , since comparing the PTB results of Lei et al with ours does not control for a wide variety of changes in the experiment . The results of Lei et al are significantly stronger due to a deviation in both model and hyperparameters from the original setting of Zaremba et al ( 2014 ) , which we followed . Specifically , these deviations include : 1 ) highway connections 2 ) tying input and output embeddings 3 ) variational dropout 4 ) different dropout rates 5 ) different number of layers 6 ) different number of dimensions 7 ) different initialization scheme All of these factors have a massive effect on performance , vastly outweighing the performance difference between one recurrent architecture and another on benchmarks such as PTB . In particular , the combination of highway connections and additional layers accounts for about 20 perplexity points . The reason we are so familiar with these details is because we reproduced Lei et al 's result , and tried to decouple the core model from these other variables . However , when we ran their model in the original Zaremba et al setting , their model did exhibit a drop in performance with respect to LSTMs . We are happy to add an additional section to the paper that demonstrates how tampering with the gating mechanism can actually reduce performance in some cases . This will complement the current experiments , which deal with simplifying the content layer . =Other Comments= - You compare our results on SQuAD , which are based on the BiDAF model ( Seo et al , 2017 ) , to the results of DrQA ( Chen et al , 2017 ) . These are two different QA models , and the comparison is flawed in the same way that the comparison of our PTB result to Lei et al 's is invalid . - We are happy to run on additional benchmarks if there is a good reason to suspect that the current 4 benchmarks do not provide sufficient coverage of interesting linguistic phenomena . Specifically , the classification tasks are a poor test of RNN 's modeling power since in many cases the best methods are simple bag-of-words classifiers ."}, {"review_id": "HJOQ7MgAW-1", "review_text": "This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the \u201ccontent update\u201d values computed at each time step. The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition. Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context. This reduced form of the LSTM is shown to perform comparably to \u201cfull\u201d LSTMs. The decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs. The proposed model variations (which replaces the \u201ccontent update\u201d that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary. First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update. You get rid of one, not the other. You can make an argument for why the one that was ablated was \u201cmore interesting\u201d, but really this is an obvious empirical question that should be addressed. The second problem of what tasks to evaluate on is a general problem with comparing RNNs. One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible. Although I don\u2019t think this is the responsibility of this paper (although something that should be considered). Finally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs. When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those. Notes on clarity: Before Eq 1 it\u2019s hard to know what the antecedent of \u201cwhich\u201d is without reading ahead. For componentwise multiplication, you have been using \\circ, but then for the iterated component wise product, \\prod is used. To be consistent, notation like \\odot and \\bigodot might be a bit clearer. The discussion of dynamic programming: the dynamic program is also only available because the attention pattern is limited in a way that self attention is not. This might be worth mentioning. When presenting Eq 11, the definition of w_j^t elides a lot of complexity. Indeed, w_j^t is only ever implicitly defined in Eq 8, whereas things like the input and forget gates are defined multiple times in the text. Since w_j^t can be defined iteratively and recursively (as a dynamic program), it\u2019s probably worth writing both out, for expository clarity. Eq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content. Table 4 is unclear. In a language model, the figure looks like it is attending to the word that is being generated, which is clearly not what you want to convey since language models don\u2019t condition on the word they are predicting. Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word? In any case, this figure should be corrected to reflect this. This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments . We will amend the paper to address all the clarity issues you pointed out . Indeed , the gates can also be seen as vanilla RNNs , but there is also evidence that removing the recurrent nature of the gates still does not cripple an LSTM ( see , for example , QRNNs https : //arxiv.org/pdf/1611.01576.pdf and SRUs https : //arxiv.org/pdf/1709.02755.pdf ) . However , both QRNNs and SRUs add alternative mechanisms that are not present in the original LSTMs ; QRNNs add multi-token convolutions , while SRUs add highway connections . The experiments in our paper are the first to explicitly isolate the gating mechanism from the embedded vanilla RNN in the content layer . The model we describe as `` LSTM - RNN '' is the minimal change that allows us to test whether the weighted summing mechanism ( as computed by LSTMs ) is sufficiently powerful to facilitate NLP models . This ablation is very different from the ones in ( Greff et al , 2015 ) and GRUs , since those models retain the recurrent content layer . We also ran similar experiments based on these models ( e.g . `` GRU - RNN '' ) , but removed them from the paper to keep the discussion focused and succinct . We are happy to expand the paper accordingly if necessary . ( Also , note that both GRUs and gate-coupled LSTMs are computing weighted averages rather than weighted sums , which appears to yield slightly lower performance in some tasks from preliminary experiments . ) Regarding evaluation on a non-NLP task : our goal was to get a better understanding of why LSTMs are so useful for NLP . We will make sure that our claims are hedged accordingly . We are also happy to run experiments on an additional non-language task if this is a major concern ."}, {"review_id": "HJOQ7MgAW-2", "review_text": "Summary: the paper proposes a new insight to LSTM in which the core is an element-wise weighted sum. The paper then argues that LSTM is redundant by keeping only input and forget gates to compute the weights. Experimental results show that the simplified versions work as well as the full LSTM. Comment: I kinda like the idea and welcome this line of research. The paper is very well written and has nice visualisation of demonstrating weights. I have only one question: in the simplified versions, content(x_t) = Wx_t , which works very well (outperforming full LSTM). I was wondering if the problem is from the tanh activation function (eq 2). What if content(x_t) = W_1 . h_{t-1} + W_2 . x_t? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . Regarding the question about tanh , we did run preliminary experiments with the suggested setting , in which the model is identical to an LSTM save for the absence of a tanh in the content layer . Performance was very similar to the original LSTM . We also experimented with a content layer of tanh ( Wx ) , and found the results to be very similar to those of `` LSTM - RNN '' that were presented in the paper ."}], "0": {"review_id": "HJOQ7MgAW-0", "review_text": "This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate. It shows comparable results with standard LSTM. I believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results. However, the formulation is very similar to \"[1] Semi-supervised Question Retrieval with Gated Convolutions\" 2016 by Lei, and \"Deriving Neural Architectures from Sequence and Graph Kernels\" which give theoretical view from string kernel about why this type of networks works. Both of the two paper don't have output gate and non-linearity of \"Wx_t\" and results on PTB also stronger than this paper. It also have some visualization about how the model decay the weights. Other AnonReviewer also point out some similar work. I won't repeat it here. In the paper, the author argued \"we propose and evaluate the minimal changes...\" but I think the these type of analysis also been covered by [1], Figure 5. On the experimental side, to draw the conclusion, \"weighted sum\" is enough for LSTM. I think at least Machine Translation and other classification results should be added. I'm not very familiar with SQuAD dataset, but the results seems worse than \"Reading Wikipedia to answer open-domain questions\" Table 4 which seems use a vanilla LSTM setup. Update: the revised version of the paper addresses all my concerns about experiments. So I increased my score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We would like to draw the reviewer 's attention to two major points : =Paper Contribution= The goal of this paper is not to propose a model that outperforms LSTMs , but to understand how LSTMs work . The aptly-named `` LSTM - RNN '' is the minimal change from the original LSTM that allows us to test whether the weighted summing mechanism ( as computed by LSTMs ) is sufficiently powerful to facilitate NLP models . The main point of this paper is that the gating mechanism ( which is able to compute element-wise weighted sums ) is a powerful model , and that the embedded vanilla RNN is redundant - at least on the variety of tasks on which we evaluated it on . =Controlled Experiments= By controlling every variable ( model , data , hyperparameters , etc ) and running head-to-head comparisons between LSTM and LSTM - RNN , we deduce whether the recurrence in the content layer is redundant . These experiments are vastly different from the ones in `` Deriving Neural Architectures from Sequence and Graph Kernels '' , since comparing the PTB results of Lei et al with ours does not control for a wide variety of changes in the experiment . The results of Lei et al are significantly stronger due to a deviation in both model and hyperparameters from the original setting of Zaremba et al ( 2014 ) , which we followed . Specifically , these deviations include : 1 ) highway connections 2 ) tying input and output embeddings 3 ) variational dropout 4 ) different dropout rates 5 ) different number of layers 6 ) different number of dimensions 7 ) different initialization scheme All of these factors have a massive effect on performance , vastly outweighing the performance difference between one recurrent architecture and another on benchmarks such as PTB . In particular , the combination of highway connections and additional layers accounts for about 20 perplexity points . The reason we are so familiar with these details is because we reproduced Lei et al 's result , and tried to decouple the core model from these other variables . However , when we ran their model in the original Zaremba et al setting , their model did exhibit a drop in performance with respect to LSTMs . We are happy to add an additional section to the paper that demonstrates how tampering with the gating mechanism can actually reduce performance in some cases . This will complement the current experiments , which deal with simplifying the content layer . =Other Comments= - You compare our results on SQuAD , which are based on the BiDAF model ( Seo et al , 2017 ) , to the results of DrQA ( Chen et al , 2017 ) . These are two different QA models , and the comparison is flawed in the same way that the comparison of our PTB result to Lei et al 's is invalid . - We are happy to run on additional benchmarks if there is a good reason to suspect that the current 4 benchmarks do not provide sufficient coverage of interesting linguistic phenomena . Specifically , the classification tasks are a poor test of RNN 's modeling power since in many cases the best methods are simple bag-of-words classifiers ."}, "1": {"review_id": "HJOQ7MgAW-1", "review_text": "This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the \u201ccontent update\u201d values computed at each time step. The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition. Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context. This reduced form of the LSTM is shown to perform comparably to \u201cfull\u201d LSTMs. The decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs. The proposed model variations (which replaces the \u201ccontent update\u201d that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary. First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update. You get rid of one, not the other. You can make an argument for why the one that was ablated was \u201cmore interesting\u201d, but really this is an obvious empirical question that should be addressed. The second problem of what tasks to evaluate on is a general problem with comparing RNNs. One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible. Although I don\u2019t think this is the responsibility of this paper (although something that should be considered). Finally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs. When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those. Notes on clarity: Before Eq 1 it\u2019s hard to know what the antecedent of \u201cwhich\u201d is without reading ahead. For componentwise multiplication, you have been using \\circ, but then for the iterated component wise product, \\prod is used. To be consistent, notation like \\odot and \\bigodot might be a bit clearer. The discussion of dynamic programming: the dynamic program is also only available because the attention pattern is limited in a way that self attention is not. This might be worth mentioning. When presenting Eq 11, the definition of w_j^t elides a lot of complexity. Indeed, w_j^t is only ever implicitly defined in Eq 8, whereas things like the input and forget gates are defined multiple times in the text. Since w_j^t can be defined iteratively and recursively (as a dynamic program), it\u2019s probably worth writing both out, for expository clarity. Eq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content. Table 4 is unclear. In a language model, the figure looks like it is attending to the word that is being generated, which is clearly not what you want to convey since language models don\u2019t condition on the word they are predicting. Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word? In any case, this figure should be corrected to reflect this. This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments . We will amend the paper to address all the clarity issues you pointed out . Indeed , the gates can also be seen as vanilla RNNs , but there is also evidence that removing the recurrent nature of the gates still does not cripple an LSTM ( see , for example , QRNNs https : //arxiv.org/pdf/1611.01576.pdf and SRUs https : //arxiv.org/pdf/1709.02755.pdf ) . However , both QRNNs and SRUs add alternative mechanisms that are not present in the original LSTMs ; QRNNs add multi-token convolutions , while SRUs add highway connections . The experiments in our paper are the first to explicitly isolate the gating mechanism from the embedded vanilla RNN in the content layer . The model we describe as `` LSTM - RNN '' is the minimal change that allows us to test whether the weighted summing mechanism ( as computed by LSTMs ) is sufficiently powerful to facilitate NLP models . This ablation is very different from the ones in ( Greff et al , 2015 ) and GRUs , since those models retain the recurrent content layer . We also ran similar experiments based on these models ( e.g . `` GRU - RNN '' ) , but removed them from the paper to keep the discussion focused and succinct . We are happy to expand the paper accordingly if necessary . ( Also , note that both GRUs and gate-coupled LSTMs are computing weighted averages rather than weighted sums , which appears to yield slightly lower performance in some tasks from preliminary experiments . ) Regarding evaluation on a non-NLP task : our goal was to get a better understanding of why LSTMs are so useful for NLP . We will make sure that our claims are hedged accordingly . We are also happy to run experiments on an additional non-language task if this is a major concern ."}, "2": {"review_id": "HJOQ7MgAW-2", "review_text": "Summary: the paper proposes a new insight to LSTM in which the core is an element-wise weighted sum. The paper then argues that LSTM is redundant by keeping only input and forget gates to compute the weights. Experimental results show that the simplified versions work as well as the full LSTM. Comment: I kinda like the idea and welcome this line of research. The paper is very well written and has nice visualisation of demonstrating weights. I have only one question: in the simplified versions, content(x_t) = Wx_t , which works very well (outperforming full LSTM). I was wondering if the problem is from the tanh activation function (eq 2). What if content(x_t) = W_1 . h_{t-1} + W_2 . x_t? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . Regarding the question about tanh , we did run preliminary experiments with the suggested setting , in which the model is identical to an LSTM save for the absence of a tanh in the content layer . Performance was very similar to the original LSTM . We also experimented with a content layer of tanh ( Wx ) , and found the results to be very similar to those of `` LSTM - RNN '' that were presented in the paper ."}}