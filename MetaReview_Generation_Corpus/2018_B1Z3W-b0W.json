{"year": "2018", "forum": "B1Z3W-b0W", "title": "Learning to Infer", "decision": "Invite to Workshop Track", "meta_review": "This paper is intersting but has a few flaws that still need to be addressed. As one reviewer noted, \"the authors seems to have simply applied the method of Andrychowicz et al. If they added some discussion and experiments clearly showing why this is a better way to improve the existing inference methods, the paper might have more impact.\".\nOverall, this work builds on existing work, but does not really dig deep enough for answers to these questions raised by the reviewers. The committee still feels this paper will be of great value at ICLR and recommends it for a workshop paper.\n", "reviews": [{"review_id": "B1Z3W-b0W-0", "review_text": "This paper proposes an iterative inference scheme for latent variable models that use inference networks. Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al. The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network). My main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach. It appears that the iterative method should result in \"direct improvement with additional samples and inference iterations\". I am supposing this is at the test time. It is not clear exactly when this will be useful. I believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al. 2014 (they used this method to perform data imputation). The paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE. In summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . We hope to clarify points that were unclear through this reply as well as revisions to the paper . Regarding the utility of our method : `` It appears that the iterative method should result in `` direct improvement with additional samples and inference iterations '' ... It is not clear exactly when this will be useful\u2026the paper needs to do a better job in justifying the advantages obtained by the proposed method . '' Additional samples and inference iterations help at both training and test time . We presented these experiments to show two aspects of iterative inference models that are distinct from standard inference models , helping readers to distinguish between these models . The main advantage of iterative inference models is that they outperform similar standard inference models in terms of log likelihood , i.e.iterative inference models are better able to capture the data distribution . Increasing the number of samples or inference iterations provides two additional knobs with which to widen this performance gap . We will attempt to make this clearer in the revised paper . Regarding iterative approaches with VAEs : `` I believe an iterative approach is also possible to perform with the standard VAE , e.g. , by bootstrapping over the input data and then using the iterative scheme of Rezende et . al.2014 ( they used this method to perform data imputation ) . '' Such an approach would be qualitatively different than the approach presented here . The data imputation scheme from in ( Rezende et.al.2014 ) involves iteratively encoding partial observations or reconstructions . If we understand your comment , at best , that approach could only perform as well as a VAE with full observations . Encoding reconstructions would likely introduce further errors . Regarding training difficulty : `` The paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE . '' We found training iterative inference models to be relatively straightforward and easy to implement . There were no tricks necessary to train these models , and we found that iterative inference models start learning to improve their inference estimates almost immediately . We will include further discussion of this point in Appendix B to assure readers . We will also release code upon publication ."}, {"review_id": "B1Z3W-b0W-1", "review_text": "This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution. The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm. This approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family). This source of error is often ignored in the literature, although there are some exceptions that may be worth mentioning: * Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class). * The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. * The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network. * A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs. They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables. I think this is good and potentially important work, although I do have some questions/concerns about the results in Table 1 (see below). Some more specific comments: Figure 2: I think this might be clearer if you unrolled a couple of iterations in (a) and (c). (Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset. A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm. Section 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014). First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance. Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1. An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter. Why not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}? Table 1: These results are strange in a few ways: * The gap between the standard and iterative inference network seems very small (0.3 nats at most). This is much smaller than the gap in Figure 5(a). * The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture. I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . We hope to clarify points that were unclear through this reply as well as revisions to the paper . `` A very recent paper by Krishnan et al . ( https : //arxiv.org/pdf/1710.06085.pdf , posted to arXiv days before the ICLR deadline ) is probably closest ; it also examines using iterative optimization ( but no learning-to-learn ) to improve training of VAEs . They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse , high-dimensional data like text and recommendations ; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables . '' We became aware of the work by Krishnan et al.after the deadline , and we will cite them as concurrent work . We find it interesting that they did not see a larger improvement on binarized MNIST , as this may point to qualitative differences between their approach and learned optimization . We plan to include additional experiments in the appendix applying iterative inference models to sparse data . `` Figure 2 : I think this might be clearer if you unrolled a couple of iterations in ( a ) and ( c ) . '' Thank you for the suggestion . We plan to include an additional figure in the appendix showing these iterative approaches unrolled in time . `` ( Dempster et al.1977 ) is not the best reference for this section ; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset . A more relevant reference would be Stochastic Variational Inference by Hoffman et al. ( 2013 ) ... '' We will cite Hoffman et al . ( 2013 ) .We were initially hesitant to cite this reference as they make use of natural gradients , which are absent in this work . `` The statement p ( z ) =N ( z ; mu_p , Sigma_p ) doesn \u2019 t quite match the formulation of Rezende & Mohamed ( 2014\u2026in the case where there are two or more layers , the joint distribution of all z need not be Gaussian ( or even unimodal ) \u2026 '' We chose this formulation in the derivation because it provides a more general treatment . As pointed out , it is unnecessary in the case of a one-level model . However , this formulation is applicable in the hierarchical case , where the prior is typically some arbitrary factorized Gaussian density . The discussion in Section 4 applies to one-level models , which are most commonly used in practice . You are correct that a hierarchical prior need not take the form of a Gaussian , and we discuss this model form in further detail in Appendix A.6 . We will attempt to make this point clearer . `` Why not have mu_ { q , t+1 } depend on sigma_ { q , t } as well as mu_ { q , t } ? '' This is , in fact , what we do in practice . VAEs have typically been presented as having separate functions for each approximate posterior term , which then share parameters to simplify the model and make learning more efficient . We followed this convention . `` The gap between the standard and iterative inference network seems very small ( 0.3 nats at most ) . This is much smaller than the gap in Figure 5 ( a ) . '' Table 1 presents negative log-likelihood estimates using 5,000 importance weighted samples , whereas all other figures show lower bound estimates using a single sample . The gap between negative log-likelihood estimates and lower bound estimates need not be the same , as they depend on the tightness of the bounds . We will make this distinction clearer in the paper . `` The MNIST results are suspiciously good overall ... I don \u2019 t think I \u2019 ve ever seen a number better than ~87 nats . '' Our results agree with ( S\u00f8nderby et al. , 2016 ) , who report a NLL of ~85 nats for a nearly identical model architecture ( compare with our ~84 nats for a standard inference model ) . As in their experiments , we use the dynamically binarized version of MNIST , which results in higher log-likelihoods as compared with statically binarized MNIST . The additional ~1 nat gap is likely due to different activation functions and encoding architecture . We used exponential linear units ( ELU ) , which we have always found to yield superior performance over leaky ReLUs used in ( S\u00f8nderby et al. , 2016 ) . We also used residual encoding networks , which tend to perform better ."}, {"review_id": "B1Z3W-b0W-2", "review_text": "Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two. In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients. The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate. Although probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context. From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016). Indeed the paper perhaps anticipates this perspective and preemptively offers that \"variational inference is a qualitatively different optimization problem\" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work. But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient. That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. Beyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE. While these results are enlightening, most of the conclusions are not entirely unexpected. For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result. It would certainly seem strange if this were not the case. And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance. Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016). In terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison? For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs? Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration. In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained. Other minor comment: * In Fig. 5(a), it seems like the performance of the standard inference model is still improving but the iterative inference model has mostly saturated. * A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . We hope to clarify points that were unclear through this reply as well as revisions to the paper . Regarding novelty : `` \u2026the paper\u2026represents a fairly straightforward application of ( Andrychowicz et al. , 2016 ) . \u2026learning-to-learn is a quite general concept already , and the exact model structure is not the key novel ingredient . '' While our work is related to that of ( Andrychowicz et al. , 2016 ) , there are several novel distinctions : 1. we apply learned optimization models to variational inference , 2. we empirically demonstrate that feedforward networks can perform optimization , whereas previous works required recurrent networks , 3. we develop a novel encoding form that approximates derivatives . To the best of our knowledge , these findings are not fully discussed or demonstrated in the literature . Unlike learning , variational inference optimization operates over fewer steps and is performed separately for each example , rather than across different tasks . Furthermore , our experiments with hierarchical latent variable models demonstrate a qualitatively different form of optimization model , split across separate networks on multiple levels of optimized variables . The optimization model architecture is an important contribution , as all previous works have only used recurrent neural networks , implicitly assuming that learned optimization requires coordination over multiple steps . We have shown that feedforward networks can learn to perform optimization , outperforming optimizers like ADAM and RMSProp that capture additional curvature information from decaying moments . The reviewer states , \u201c the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. \u201d We have shown that computing approximate posterior gradients is unnecessary ; a model can learn to optimize using locally computed errors . To the best of our knowledge , this is the first time this observation has been explicitly identified in the literature , providing a novel form of learned optimization models . Regarding seemingly unsurprising results : `` ... most of the conclusions are not entirely unexpected . '' `` ... these results are entirely expected as this phenomena has already been well-documented in ( Andrychowicz et al. , 2016 ) . '' The results on inference optimization capabilities ( Section 5.1 and Figure 6 ) are interesting for the reason that they are what we would expect . It \u2019 s un-intuitive and surprising that an iterative inference model can learn to optimize a generative model , and our results verify that this is done in a reasonable manner . Few works in the VAE literature have discussed optimization performance , so it is instructive to visualize and quantify how various methods compare . Regarding experimental comparisons : `` ... is this really an apples-to-apples comparison ? \u2026might eq . ( 4 ) involve fewer parameters than eq . ( 5 ) since there are fewer inputs ? ... if one plays around with the standard inference architecture a bit , perhaps similar results could be obtained . '' The gradient encoding iterative inference model ( eq.5 ) in Figure 5b has fewer input parameters ( 256 vs. 784 ) , yet outperforms the standard inference model . We will clarify this point . As the reviewer points out , a perfect comparison of models is difficult to perform : varying numbers of inputs result in varying numbers of input parameters . Yet , the number of parameters processing information from the data is constant across both models , showing that gradients and errors contain additional information . Regarding our results , we found that iterative inference models outperformed standard models across a variety of architectures ( varying network/latent width , residual/dense connections , batch norm , etc . ) on the benchmark data sets . The experiments are representative of this finding , which we hope to clarify in the revised paper . Miscellaneous : `` A downside\u2026is that it requires computing gradients of the objective even at test time ... '' Iterative inference models that encode gradients require these gradients at test time , which we will state more clearly . However , the error encoding models that we introduce do not require these gradients , one of their benefits that we highlight . `` \u2026there is no demonstration of reconstruction quality relative to existing models , which could be helpful for evaluating relative performance . '' The purpose of Figure 4 is to provide a qualitative verification of our inference optimization , not to demonstrate superior reconstruction quality . It would also be difficult for humans to visually inspect these differences , as they likely involve slight differences in pixel intensities . `` In Fig.5 ( a ) , \u2026the standard inference model is still improving but the iterative inference model has mostly saturated . '' We agree , but this does not impact the main empirical findings from section 5.2 : iterative inference models improve significantly with more approximate posterior samples ."}], "0": {"review_id": "B1Z3W-b0W-0", "review_text": "This paper proposes an iterative inference scheme for latent variable models that use inference networks. Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al. The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network). My main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach. It appears that the iterative method should result in \"direct improvement with additional samples and inference iterations\". I am supposing this is at the test time. It is not clear exactly when this will be useful. I believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al. 2014 (they used this method to perform data imputation). The paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE. In summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . We hope to clarify points that were unclear through this reply as well as revisions to the paper . Regarding the utility of our method : `` It appears that the iterative method should result in `` direct improvement with additional samples and inference iterations '' ... It is not clear exactly when this will be useful\u2026the paper needs to do a better job in justifying the advantages obtained by the proposed method . '' Additional samples and inference iterations help at both training and test time . We presented these experiments to show two aspects of iterative inference models that are distinct from standard inference models , helping readers to distinguish between these models . The main advantage of iterative inference models is that they outperform similar standard inference models in terms of log likelihood , i.e.iterative inference models are better able to capture the data distribution . Increasing the number of samples or inference iterations provides two additional knobs with which to widen this performance gap . We will attempt to make this clearer in the revised paper . Regarding iterative approaches with VAEs : `` I believe an iterative approach is also possible to perform with the standard VAE , e.g. , by bootstrapping over the input data and then using the iterative scheme of Rezende et . al.2014 ( they used this method to perform data imputation ) . '' Such an approach would be qualitatively different than the approach presented here . The data imputation scheme from in ( Rezende et.al.2014 ) involves iteratively encoding partial observations or reconstructions . If we understand your comment , at best , that approach could only perform as well as a VAE with full observations . Encoding reconstructions would likely introduce further errors . Regarding training difficulty : `` The paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE . '' We found training iterative inference models to be relatively straightforward and easy to implement . There were no tricks necessary to train these models , and we found that iterative inference models start learning to improve their inference estimates almost immediately . We will include further discussion of this point in Appendix B to assure readers . We will also release code upon publication ."}, "1": {"review_id": "B1Z3W-b0W-1", "review_text": "This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution. The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm. This approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family). This source of error is often ignored in the literature, although there are some exceptions that may be worth mentioning: * Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class). * The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. * The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network. * A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs. They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables. I think this is good and potentially important work, although I do have some questions/concerns about the results in Table 1 (see below). Some more specific comments: Figure 2: I think this might be clearer if you unrolled a couple of iterations in (a) and (c). (Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset. A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm. Section 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014). First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance. Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1. An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter. Why not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}? Table 1: These results are strange in a few ways: * The gap between the standard and iterative inference network seems very small (0.3 nats at most). This is much smaller than the gap in Figure 5(a). * The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture. I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . We hope to clarify points that were unclear through this reply as well as revisions to the paper . `` A very recent paper by Krishnan et al . ( https : //arxiv.org/pdf/1710.06085.pdf , posted to arXiv days before the ICLR deadline ) is probably closest ; it also examines using iterative optimization ( but no learning-to-learn ) to improve training of VAEs . They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse , high-dimensional data like text and recommendations ; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables . '' We became aware of the work by Krishnan et al.after the deadline , and we will cite them as concurrent work . We find it interesting that they did not see a larger improvement on binarized MNIST , as this may point to qualitative differences between their approach and learned optimization . We plan to include additional experiments in the appendix applying iterative inference models to sparse data . `` Figure 2 : I think this might be clearer if you unrolled a couple of iterations in ( a ) and ( c ) . '' Thank you for the suggestion . We plan to include an additional figure in the appendix showing these iterative approaches unrolled in time . `` ( Dempster et al.1977 ) is not the best reference for this section ; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset . A more relevant reference would be Stochastic Variational Inference by Hoffman et al. ( 2013 ) ... '' We will cite Hoffman et al . ( 2013 ) .We were initially hesitant to cite this reference as they make use of natural gradients , which are absent in this work . `` The statement p ( z ) =N ( z ; mu_p , Sigma_p ) doesn \u2019 t quite match the formulation of Rezende & Mohamed ( 2014\u2026in the case where there are two or more layers , the joint distribution of all z need not be Gaussian ( or even unimodal ) \u2026 '' We chose this formulation in the derivation because it provides a more general treatment . As pointed out , it is unnecessary in the case of a one-level model . However , this formulation is applicable in the hierarchical case , where the prior is typically some arbitrary factorized Gaussian density . The discussion in Section 4 applies to one-level models , which are most commonly used in practice . You are correct that a hierarchical prior need not take the form of a Gaussian , and we discuss this model form in further detail in Appendix A.6 . We will attempt to make this point clearer . `` Why not have mu_ { q , t+1 } depend on sigma_ { q , t } as well as mu_ { q , t } ? '' This is , in fact , what we do in practice . VAEs have typically been presented as having separate functions for each approximate posterior term , which then share parameters to simplify the model and make learning more efficient . We followed this convention . `` The gap between the standard and iterative inference network seems very small ( 0.3 nats at most ) . This is much smaller than the gap in Figure 5 ( a ) . '' Table 1 presents negative log-likelihood estimates using 5,000 importance weighted samples , whereas all other figures show lower bound estimates using a single sample . The gap between negative log-likelihood estimates and lower bound estimates need not be the same , as they depend on the tightness of the bounds . We will make this distinction clearer in the paper . `` The MNIST results are suspiciously good overall ... I don \u2019 t think I \u2019 ve ever seen a number better than ~87 nats . '' Our results agree with ( S\u00f8nderby et al. , 2016 ) , who report a NLL of ~85 nats for a nearly identical model architecture ( compare with our ~84 nats for a standard inference model ) . As in their experiments , we use the dynamically binarized version of MNIST , which results in higher log-likelihoods as compared with statically binarized MNIST . The additional ~1 nat gap is likely due to different activation functions and encoding architecture . We used exponential linear units ( ELU ) , which we have always found to yield superior performance over leaky ReLUs used in ( S\u00f8nderby et al. , 2016 ) . We also used residual encoding networks , which tend to perform better ."}, "2": {"review_id": "B1Z3W-b0W-2", "review_text": "Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two. In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients. The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate. Although probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context. From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016). Indeed the paper perhaps anticipates this perspective and preemptively offers that \"variational inference is a qualitatively different optimization problem\" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work. But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient. That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. Beyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE. While these results are enlightening, most of the conclusions are not entirely unexpected. For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result. It would certainly seem strange if this were not the case. And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance. Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016). In terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison? For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs? Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration. In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained. Other minor comment: * In Fig. 5(a), it seems like the performance of the standard inference model is still improving but the iterative inference model has mostly saturated. * A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . We hope to clarify points that were unclear through this reply as well as revisions to the paper . Regarding novelty : `` \u2026the paper\u2026represents a fairly straightforward application of ( Andrychowicz et al. , 2016 ) . \u2026learning-to-learn is a quite general concept already , and the exact model structure is not the key novel ingredient . '' While our work is related to that of ( Andrychowicz et al. , 2016 ) , there are several novel distinctions : 1. we apply learned optimization models to variational inference , 2. we empirically demonstrate that feedforward networks can perform optimization , whereas previous works required recurrent networks , 3. we develop a novel encoding form that approximates derivatives . To the best of our knowledge , these findings are not fully discussed or demonstrated in the literature . Unlike learning , variational inference optimization operates over fewer steps and is performed separately for each example , rather than across different tasks . Furthermore , our experiments with hierarchical latent variable models demonstrate a qualitatively different form of optimization model , split across separate networks on multiple levels of optimized variables . The optimization model architecture is an important contribution , as all previous works have only used recurrent neural networks , implicitly assuming that learned optimization requires coordination over multiple steps . We have shown that feedforward networks can learn to perform optimization , outperforming optimizers like ADAM and RMSProp that capture additional curvature information from decaying moments . The reviewer states , \u201c the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. \u201d We have shown that computing approximate posterior gradients is unnecessary ; a model can learn to optimize using locally computed errors . To the best of our knowledge , this is the first time this observation has been explicitly identified in the literature , providing a novel form of learned optimization models . Regarding seemingly unsurprising results : `` ... most of the conclusions are not entirely unexpected . '' `` ... these results are entirely expected as this phenomena has already been well-documented in ( Andrychowicz et al. , 2016 ) . '' The results on inference optimization capabilities ( Section 5.1 and Figure 6 ) are interesting for the reason that they are what we would expect . It \u2019 s un-intuitive and surprising that an iterative inference model can learn to optimize a generative model , and our results verify that this is done in a reasonable manner . Few works in the VAE literature have discussed optimization performance , so it is instructive to visualize and quantify how various methods compare . Regarding experimental comparisons : `` ... is this really an apples-to-apples comparison ? \u2026might eq . ( 4 ) involve fewer parameters than eq . ( 5 ) since there are fewer inputs ? ... if one plays around with the standard inference architecture a bit , perhaps similar results could be obtained . '' The gradient encoding iterative inference model ( eq.5 ) in Figure 5b has fewer input parameters ( 256 vs. 784 ) , yet outperforms the standard inference model . We will clarify this point . As the reviewer points out , a perfect comparison of models is difficult to perform : varying numbers of inputs result in varying numbers of input parameters . Yet , the number of parameters processing information from the data is constant across both models , showing that gradients and errors contain additional information . Regarding our results , we found that iterative inference models outperformed standard models across a variety of architectures ( varying network/latent width , residual/dense connections , batch norm , etc . ) on the benchmark data sets . The experiments are representative of this finding , which we hope to clarify in the revised paper . Miscellaneous : `` A downside\u2026is that it requires computing gradients of the objective even at test time ... '' Iterative inference models that encode gradients require these gradients at test time , which we will state more clearly . However , the error encoding models that we introduce do not require these gradients , one of their benefits that we highlight . `` \u2026there is no demonstration of reconstruction quality relative to existing models , which could be helpful for evaluating relative performance . '' The purpose of Figure 4 is to provide a qualitative verification of our inference optimization , not to demonstrate superior reconstruction quality . It would also be difficult for humans to visually inspect these differences , as they likely involve slight differences in pixel intensities . `` In Fig.5 ( a ) , \u2026the standard inference model is still improving but the iterative inference model has mostly saturated . '' We agree , but this does not impact the main empirical findings from section 5.2 : iterative inference models improve significantly with more approximate posterior samples ."}}