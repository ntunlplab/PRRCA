{"year": "2019", "forum": "BkMn9jAcYQ", "title": "Countering Language Drift via Grounding", "decision": "Reject", "meta_review": "This paper proposes a method to resolve \"language drift,\" where a pre-trained X->language model trained in an X->language->Y pipeline drifts away from being natural language. In particular, it proposes to add an auxiliary training objective that performs grounding with multimodal input to fix this problem. Results are good on a task where translation is done between two languages.\n\nThe main concern that was raised with this paper by most of the reviewers is the validity of the proposed task itself. Even after extensive discussion with the authors, it is not clear that there is a very convincing scenario where we both have a pre-trained X->language, care about the intermediate results, and have some sort of grounded input to fix this drift. While I do understand the MT task is supposed to be a testbed for the true objective, it feel it is necessary to additionally have one convincing use case where this is a real problem and not just the artificially contrived. This use case could either be of practical use (e.g. potentially useful in an application), or of interest from the point of view of cognitive plausibility (e.g. similar to how children actually learn, and inspired by cognitive science literature).\n\nA concern that offshoots from this is that because the underlying idea is compelling (some sort of grounding to inform language learning), a paper at a high-profile conference such as ICLR may help re-popularize this line of research, which has been a niche for a while. Normally I would say this is definitely a good thing; I think considering grounding in language learning is definitely an important research direction, and have been a fan of this line of work since reading Roy's seminal work on it from 15 years ago. However, if the task used in this paper, which is of questionable value and reality, becomes the benchmark for this line of work I think this might lead other follow-up work in the wrong direction.  I feel that this is a critical issue, and the paper will be much stronger after a more realistic task setting is added.\n\nThus, I am not recommending acceptance at this time, but would definitely like the authors to think hard and carefully about a good and realistic benchmark for the task, and follow up with a revised version of the paper in the future.", "reviews": [{"review_id": "BkMn9jAcYQ-0", "review_text": "Summary: This paper tries to verify a hypothesis that language grounding DO help to overcome language drift when two agents creating their own protocol in order to communicate with each other. There are several constraints to enforce: 1) naturalness, say \"Englishness\", 2) grounded in visual semantics. The experiments prove that both constraints help the most (say, BLUE score). 1) w/o 2) restricts the vocabulary into a small set with the most frequent words, while 1) with 2) can resemble the original distribution. Strength: - How to make the protocol automatically created by two agents much explainable/meaningful is a very interesting topic. This paper explores plausible constraints to reach this goal. Weakness: - Visual grounding task brings more data there. To fairly compare, I hope to add one more baseline PG+LM+G_text, where G_text simply means to use text data (captions) alone, i.e., without visual signals. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer2 , Thank you for your constructive feedback ! We agree that making multi agent communication more interpretable is both interesting and important . - Grounding \u201c brings more data \u201d : thanks for pointing this out . We added this stronger baseline where the LM was trained on both WikiText103 and the captions from Flickr30k and MS COCO datasets ( see Table 4 , LM=All ) , and found that introducing visual grounding still outperforms this baseline ."}, {"review_id": "BkMn9jAcYQ-1", "review_text": "This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this \u201clanguage drift\u201d problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. They demonstrate that the jointly optimized agents perform best when regularized in this manner to prevent language drift. 1. Framing: I\u2019m uncertain about the framing of this paper. The authors pose the problem of \u201clanguage drift,\u201d which is indeed a frequent problem in multi-agent communication tasks where the principle supervision involves non-linguistic inputs and outputs. They then design a three-language MT task as a test case, where the inputs and outputs are both linguistic. Why attack this particular task and grounding solution? I can imagine some potential goals of the paper, but also see more direct ways to address each of the potential goals than what the authors have chosen: 1a. Study how to counter language drift in general \u2014 why not choose a more intuitive two-agent communication task, e.g. navigation, game playing, etc.? 1b. Study how to counter language drift in the MT task \u2014 aren\u2019t there simpler solutions to prevent language drift in this particular task? e.g. require \u201ccycle-consistency\u201d \u2013 that it be possible to reconstruct the French input using the French output? Why pick multimodal grounding, given that it imposes substantial additional data requirements? 1c. Build a better/more data-efficient machine translation system \u2014 this could be an interesting goal and suitable for the paper, but this doesn\u2019t seem to be the framing that the authors intend. 2. Interpretation of first results: 2a. Thanks for including standard deviation estimates! I think it\u2019s also important that you do some sort of significance testing on the comparison between PG+LM+G and PG+LM performance for Fr->En->De \u2014 these numbers look pretty close to me. You could run e.g. a simple sign test on examples within each corpus between the two conditions. 2b. It would also be good to know how robust your results are to hyperparameter settings (especially the entropy regularization hyperparameter). 3. Token frequency results: These are intriguing but quite confusing to me! 3a. How sensitive are these results to your entropy regularization setup? How does PG behave without entropy regularization? 3b. Table 6 shows that the PG model has very different drift for different POS categories. Does this explain away the change in the token frequency distribution? What do the token frequency effects look like for PG within the open-class / content word categories (i.e., controlling for the huge difference in closed-class behavior)? 4. Minor comments: 4a. There\u2019s a related problem in unsupervised representation learning for language. Work on VAEs for language, for example, has shown that the encoder often collapses meaning differences in the latent representation, and leans on an overly powerful decoder in order to pick up all of the lost information. It would be good to reference this work in your framing (see e.g. Bowman et al. (2015)). 4b. In sec. 3.1 you overload notation for R. Can you subscript these so that it\u2019s especially clear in your results which systems are following which reward function? 4c. Great to show some qualitative examples in Table 7 \u2014 can you explicitly state where these are from (dev set vs. test set?) and whether they are randomly sampled? References: Bowman et al. (2015). Generating sentences from a continuous space. https://arxiv.org/abs/1511.06349 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 and AC , Thank you both for your constructive feedback ! We think there is a slight misunderstanding here : the reason we have chosen this setup is exactly because this particular task and setup directly addresses the problem of language drift , in a way where the semantics stays identical while the communication channel gets only extrinsic reward ( i.e. , the meaning is exactly the same for all languages and modalities ) . In addition , every single utterance has very clear and very well-known metrics , in the shape of BLEU and NLL/perplexity , allowing us to measure performance at every single step . We would very much welcome any suggestions for other tasks where this setup would be possible , and where data is available , but we think that AnonReviewer1 's suggestions ( while of course very welcome ) do not satisfy these criteria : other two-agent communication tasks such as navigation or game-playing have neither clearly defined metrics nor easily available NL data . Hence , we do not think our setup is artificial , but ideal for understanding the problem as best as we possibly can . As for auxiliary objectives or other solutions for preventing drift : - If cycle-consistency means a French-French auto-encoder with English intermediary , that is a much weaker setting than French-German with English intermediary , because there is no way for the agents to focus on superficial information and really only the meaning of the sentence is at stake . Our setting is much more difficult than this sort of cycle-consistency , as the focus is on the semantics , which is why we want to avoid language drift in the first place . In other words , cycle-consistency is a special case of what we 've done , we do not expect any different result from it . - Occasionally sampling batches of French-English data with the original MLE objective does not prevent drift , unless we do that so often that we undo the advantages of fine-tuning . Note that performance is much better than training simply with an MLE objective , due to fine-tuning . Occasionally training with MLE will avoid some drift but at the expense of performance improvements . Our work clearly shows that the alternative solution of adding language model constraints is insufficient . If you like , we would be happy to train French-French and German-German autoencoders , with English intermediates , and show that the same results hold as what we report for the harder French-German case . Similarly , we would be happy to add experiments showing that occasionally training on batches with MLE does not work as well as our proposed solution . With regard to cognitive plausibility of the three-way translation task , we disagree : In 2009 , the United States Census Bureau reported that about 20 percent of Americans speak a language other than English at home . Therefore we would like to point out that this three-way translation task , using English as an intermediate language , is not only cognitively plausible but a reality for one fifth of Americans . A useful interpretation of our setup is that Agent A is communicating their ( French ) thoughts in English so that Agent B understands the intended message in their ( German ) thoughts . In a sense our setup is more plausible than the cycle-consistent autoencoder , because no two people speak exactly the same language or have exactly the same thoughts ."}, {"review_id": "BkMn9jAcYQ-2", "review_text": "The paper presents an approach to refining a translation system with grounding (in addition to LM scores) in the loop to manage linguistic drift. The intuition is straightforward and results are clearly presented, but the gains are unfortunately much weaker than I would have hoped for. The results for both Fr-En and Fr-En-De only show very small gains for adding grounding, often with PG+LM results being within 1 std-dev of the PG+LM+G results. Otherwise, the results are quite nice with interesting increases in linguistic diversity. This leads me to wonder if this approach would show more gains with a human evaluation rather than BLEU score. What is the performance of PG+G without the +LM? Minor -- In Fig 2, should the green line (PG+LM) have continued climbing to >21 BLEU?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you for your helpful comments ! Please see our response below . - Is BLEU the right metric ? : Given the availability of ground truth English references , we believe BLEU is the best metric we have , as it is clearly interpretable and well-understood . BLEU of course also has weaknesses , and we agree that a human evaluation would be very interesting . In this setting , however , we believe BLEU is sufficient for making our argument . - Gains for adding grounding are small ( within 1 std dev ) : We agree that the results for PG+LM and PG+LM+G are often close in BLEU score . Our qualitative analyses , on the other hand , strongly indicate that PG and PG+LM models learn a biased token distribution : namely , PG finetuning ignores the content words , while the PG+LM finetuning excessively encourages them . We find that visual grounding is key to retaining the original token distribution . - The results of PG+G without +LM is shown in Table 4 , with PG+LM+G with \u201c LM=None \u201d . In hindsight we see that we should have made this clearer : we updated the table to make this stand out more . The PG+G model , although outperformed by PG+LM and PG+LM+G , still produces less drift than the vanilla PG finetuning alone . - Fig.2 , would PG+LM continue to over 21 ? : We used the same early stopping criteria based on communication performance ( Fr- > En- > De BLEU ) for all our models . Hence this model was early stopped at that point ."}], "0": {"review_id": "BkMn9jAcYQ-0", "review_text": "Summary: This paper tries to verify a hypothesis that language grounding DO help to overcome language drift when two agents creating their own protocol in order to communicate with each other. There are several constraints to enforce: 1) naturalness, say \"Englishness\", 2) grounded in visual semantics. The experiments prove that both constraints help the most (say, BLUE score). 1) w/o 2) restricts the vocabulary into a small set with the most frequent words, while 1) with 2) can resemble the original distribution. Strength: - How to make the protocol automatically created by two agents much explainable/meaningful is a very interesting topic. This paper explores plausible constraints to reach this goal. Weakness: - Visual grounding task brings more data there. To fairly compare, I hope to add one more baseline PG+LM+G_text, where G_text simply means to use text data (captions) alone, i.e., without visual signals. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer2 , Thank you for your constructive feedback ! We agree that making multi agent communication more interpretable is both interesting and important . - Grounding \u201c brings more data \u201d : thanks for pointing this out . We added this stronger baseline where the LM was trained on both WikiText103 and the captions from Flickr30k and MS COCO datasets ( see Table 4 , LM=All ) , and found that introducing visual grounding still outperforms this baseline ."}, "1": {"review_id": "BkMn9jAcYQ-1", "review_text": "This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this \u201clanguage drift\u201d problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. They demonstrate that the jointly optimized agents perform best when regularized in this manner to prevent language drift. 1. Framing: I\u2019m uncertain about the framing of this paper. The authors pose the problem of \u201clanguage drift,\u201d which is indeed a frequent problem in multi-agent communication tasks where the principle supervision involves non-linguistic inputs and outputs. They then design a three-language MT task as a test case, where the inputs and outputs are both linguistic. Why attack this particular task and grounding solution? I can imagine some potential goals of the paper, but also see more direct ways to address each of the potential goals than what the authors have chosen: 1a. Study how to counter language drift in general \u2014 why not choose a more intuitive two-agent communication task, e.g. navigation, game playing, etc.? 1b. Study how to counter language drift in the MT task \u2014 aren\u2019t there simpler solutions to prevent language drift in this particular task? e.g. require \u201ccycle-consistency\u201d \u2013 that it be possible to reconstruct the French input using the French output? Why pick multimodal grounding, given that it imposes substantial additional data requirements? 1c. Build a better/more data-efficient machine translation system \u2014 this could be an interesting goal and suitable for the paper, but this doesn\u2019t seem to be the framing that the authors intend. 2. Interpretation of first results: 2a. Thanks for including standard deviation estimates! I think it\u2019s also important that you do some sort of significance testing on the comparison between PG+LM+G and PG+LM performance for Fr->En->De \u2014 these numbers look pretty close to me. You could run e.g. a simple sign test on examples within each corpus between the two conditions. 2b. It would also be good to know how robust your results are to hyperparameter settings (especially the entropy regularization hyperparameter). 3. Token frequency results: These are intriguing but quite confusing to me! 3a. How sensitive are these results to your entropy regularization setup? How does PG behave without entropy regularization? 3b. Table 6 shows that the PG model has very different drift for different POS categories. Does this explain away the change in the token frequency distribution? What do the token frequency effects look like for PG within the open-class / content word categories (i.e., controlling for the huge difference in closed-class behavior)? 4. Minor comments: 4a. There\u2019s a related problem in unsupervised representation learning for language. Work on VAEs for language, for example, has shown that the encoder often collapses meaning differences in the latent representation, and leans on an overly powerful decoder in order to pick up all of the lost information. It would be good to reference this work in your framing (see e.g. Bowman et al. (2015)). 4b. In sec. 3.1 you overload notation for R. Can you subscript these so that it\u2019s especially clear in your results which systems are following which reward function? 4c. Great to show some qualitative examples in Table 7 \u2014 can you explicitly state where these are from (dev set vs. test set?) and whether they are randomly sampled? References: Bowman et al. (2015). Generating sentences from a continuous space. https://arxiv.org/abs/1511.06349 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 and AC , Thank you both for your constructive feedback ! We think there is a slight misunderstanding here : the reason we have chosen this setup is exactly because this particular task and setup directly addresses the problem of language drift , in a way where the semantics stays identical while the communication channel gets only extrinsic reward ( i.e. , the meaning is exactly the same for all languages and modalities ) . In addition , every single utterance has very clear and very well-known metrics , in the shape of BLEU and NLL/perplexity , allowing us to measure performance at every single step . We would very much welcome any suggestions for other tasks where this setup would be possible , and where data is available , but we think that AnonReviewer1 's suggestions ( while of course very welcome ) do not satisfy these criteria : other two-agent communication tasks such as navigation or game-playing have neither clearly defined metrics nor easily available NL data . Hence , we do not think our setup is artificial , but ideal for understanding the problem as best as we possibly can . As for auxiliary objectives or other solutions for preventing drift : - If cycle-consistency means a French-French auto-encoder with English intermediary , that is a much weaker setting than French-German with English intermediary , because there is no way for the agents to focus on superficial information and really only the meaning of the sentence is at stake . Our setting is much more difficult than this sort of cycle-consistency , as the focus is on the semantics , which is why we want to avoid language drift in the first place . In other words , cycle-consistency is a special case of what we 've done , we do not expect any different result from it . - Occasionally sampling batches of French-English data with the original MLE objective does not prevent drift , unless we do that so often that we undo the advantages of fine-tuning . Note that performance is much better than training simply with an MLE objective , due to fine-tuning . Occasionally training with MLE will avoid some drift but at the expense of performance improvements . Our work clearly shows that the alternative solution of adding language model constraints is insufficient . If you like , we would be happy to train French-French and German-German autoencoders , with English intermediates , and show that the same results hold as what we report for the harder French-German case . Similarly , we would be happy to add experiments showing that occasionally training on batches with MLE does not work as well as our proposed solution . With regard to cognitive plausibility of the three-way translation task , we disagree : In 2009 , the United States Census Bureau reported that about 20 percent of Americans speak a language other than English at home . Therefore we would like to point out that this three-way translation task , using English as an intermediate language , is not only cognitively plausible but a reality for one fifth of Americans . A useful interpretation of our setup is that Agent A is communicating their ( French ) thoughts in English so that Agent B understands the intended message in their ( German ) thoughts . In a sense our setup is more plausible than the cycle-consistent autoencoder , because no two people speak exactly the same language or have exactly the same thoughts ."}, "2": {"review_id": "BkMn9jAcYQ-2", "review_text": "The paper presents an approach to refining a translation system with grounding (in addition to LM scores) in the loop to manage linguistic drift. The intuition is straightforward and results are clearly presented, but the gains are unfortunately much weaker than I would have hoped for. The results for both Fr-En and Fr-En-De only show very small gains for adding grounding, often with PG+LM results being within 1 std-dev of the PG+LM+G results. Otherwise, the results are quite nice with interesting increases in linguistic diversity. This leads me to wonder if this approach would show more gains with a human evaluation rather than BLEU score. What is the performance of PG+G without the +LM? Minor -- In Fig 2, should the green line (PG+LM) have continued climbing to >21 BLEU?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you for your helpful comments ! Please see our response below . - Is BLEU the right metric ? : Given the availability of ground truth English references , we believe BLEU is the best metric we have , as it is clearly interpretable and well-understood . BLEU of course also has weaknesses , and we agree that a human evaluation would be very interesting . In this setting , however , we believe BLEU is sufficient for making our argument . - Gains for adding grounding are small ( within 1 std dev ) : We agree that the results for PG+LM and PG+LM+G are often close in BLEU score . Our qualitative analyses , on the other hand , strongly indicate that PG and PG+LM models learn a biased token distribution : namely , PG finetuning ignores the content words , while the PG+LM finetuning excessively encourages them . We find that visual grounding is key to retaining the original token distribution . - The results of PG+G without +LM is shown in Table 4 , with PG+LM+G with \u201c LM=None \u201d . In hindsight we see that we should have made this clearer : we updated the table to make this stand out more . The PG+G model , although outperformed by PG+LM and PG+LM+G , still produces less drift than the vanilla PG finetuning alone . - Fig.2 , would PG+LM continue to over 21 ? : We used the same early stopping criteria based on communication performance ( Fr- > En- > De BLEU ) for all our models . Hence this model was early stopped at that point ."}}