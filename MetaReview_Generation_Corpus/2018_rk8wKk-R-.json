{"year": "2018", "forum": "rk8wKk-R-", "title": "Convolutional Sequence Modeling Revisited", "decision": "Invite to Workshop Track", "meta_review": "meta score: 5\n\nThis paper gives a thorough experimental comparison of convolutional vs recurrent networks for a variety of sequence modelling tasks.  The experimentation is thorough, but the main point of the paper,  that convolutional networks are unjustly ignored for sequence modelling, is overstated as there are several areas where convolutional networks are well explored.\nPros:\n clear and well-written\n thorough set of experiments\nCons\n original contribution is not strong\n it is not as radical to consider convolutional networks for sequence modeling as the authors seem to suggest\n", "reviews": [{"review_id": "rk8wKk-R--0", "review_text": "In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. They start by proposing a generic temporal convolution sequence model which leverages recent advances in the field, discuss the respective advantages of convolutional and recurrent networks, and benchmark their architecture on a number of different tasks. The paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works. The only part which is slightly less convincing is the section about effective memory size. While it is true that learning longer term dependencies can be difficult in standard RNN architectures, it is interesting to notice that the SoTA results presented in appendix B.3 for language modeling on larger data sets are architectures which focus on remedying this difficulty (cache model and hierarchical LSTM). It would also be interesting to see how TCN works on word prediction tasks which are devised explicitly to test for longer memory, such as Lambada (1) or Children Books Test (2). As a minor point, adding a measure of complexity in terms of number of operations could be a useful hardware-independent indication of the computational cost of the architecture. Pros: - Clearly written, well executed paper - Makes a strong point for the use of convolutional architecture for sequences - Provides useful benchmarks for the community Cons: - The claims on effective memory size need more context and justification 1: The LAMBADA dataset: Word prediction requiring a broad discourse context, Paperno et al. 2016 2: The Goldilocks principle: reading children's books with explicit memory representation, Hill et al. 2016", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the review , we agree with virtually all your points . As per your suggestion , we are currently integrating experiments on the LAMBADA dataset into the paper , and will post a revision with these results shortly ."}, {"review_id": "rk8wKk-R--1", "review_text": "The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The paper describes an architecture similar to wavenet with residual connections. Empirical results are presented on a large number of tasks where the convolutional network often outperforms modern recurrent baselines or reaches similar performance. The biggest strength of the paper is the large number of tasks on which the models are evaluated. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. That said, I don\u2019t think that all the tasks are very relevant for comparing convolutional and recurrent architectures. While the time windows that RNNs can deal with are infinite in principle, it is common knowledge that the effective length of the dependencies RNNs can model is quite limited in practise. Many of the artificial task like the adding problem and sequential MNIST have been designed to highlight this weakness of RNNs. I don\u2019t find it very surprising that these tasks are easy to solve with a feedforward architecture with a large enough context window. The more impressive results are in my opinion those on the language modelling tasks where one would indeed expect RNNs to be more suitable for capturing dependencies that require stack-like memory functionality. While the related work is quite comprehensive, it downplays the popularity of convolutional architectures throughout history a bit. Especially in speech recognition, RNNs have only recently started to gain popularity while deep feedforward networks applied to overlapping time windows (i.e., 1D convolutions) have been the state-of-the-art for years. Of course the recent successes of dilated convolutions are likely to change the landscape in this application domain yet again. The paper is well-structured and written. If anything, it is perhaps a little bit wordy at times but I prefer that over obscurity due to brevity. The ideas in the paper are not novel and neither do the authors claim that they are. Unfortunately, I also think that the impact of the work is also somewhat limited due to the enormous success of the wavenet architecture. I do think that the results on the real-world tasks are valuable and worthy of publication. However, I feel that the authors exaggerate the extent to which researchers in this field still consider RNNs superior models for sequences. + Many experiments and tasks. + Well-written and clear. + Good results - Somewhat exaggerated claims about the extent to which RNNs are still being considered more suitable sequence models than dilated convolutions. Especially in light of the success of Wavenet. - Not much novelty/originality. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for this review . We agree on most points , except in the ultimate conclusions and assessment of the current `` default '' mindset of temporal modeling in RNNs . First , we agree that speech data in particular ( or perhaps audio data more broadly ) , is indeed one instance where CNNs do appear to have a historical edge over recurrent models , and we can emphasize this in the background section . Indeed , as you mention , the success of WaveNet has certainly made clear the power of CNNs in this application domain . The question , then , is to what extent the community already feels that the success of WaveNet in the speech setting is sufficient to `` standardize '' the use of CNNs across all sequence prediction tasks . And our genuine impression here is that these ideas have yet to permeate the mindset of the community for generic sequence prediction . Numerous resources ( e.g. , Goodfellow et al . 's deep learning book , with its chapter `` Sequence Modeling : Recurrent and Recursive Nets '' , plus virtually all current papers on recurrent networks ) , still highlight LSTMs and other similar architectures as the `` standard '' for sequence modeling . The precise goal of our work is to highlight the fact that WaveNet-like architectures ( though substantially simplified too , as we describe below ) can indeed work well across the many other settings we consider . And we feel that this is an important point to make empirically , even if the results or conclusion may seem `` unsurprising '' to people who are very familiar with CNN architectures . The second point , also , is that the architecture we consider is indeed simpler than WaveNet in many respects : e.g.no gated activation but just ReLUs ( which , as we highlighted in our response to a previous reviewer , we will include more experimentation on in a forthcoming update ) , no context stacks , etc ; and residual units and dilation structure that more directly mirror the corresponding `` standard '' architectures in convolutional image networks . Thus , a practitioner wishing to apply WaveNet-style architectures to some new sequence prediction task may be unclear about which elements of the architecture are really necessary , and we attempt to distill this as much as possible in our current paper . Overall , therefore , we agree that the significance of our current work is largely making the empirical point that TCN architectures are not just for audio , but really for any sequence modeling problem . But we do feel that this is an important point to make and thoroughly substantiate , even given the success of WaveNet ."}, {"review_id": "rk8wKk-R--2", "review_text": "This paper argues that convolutional networks should be the default approach for sequence modeling. The paper is nicely done and rather easy to understand. Nevertheless, I find it difficult to assess its significance. In order to support the original hypothesis, I think that a much larger and more diverse set of experiments should have been considered. As pointed out by another reviewer please add https://arxiv.org/abs/1703.04691 to your references.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your note , though we honestly found it a bit surprising . The entire point of our paper _is_ to evaluate the improved TCN performance over a large and diverse set of experiments , and on this point it is by far the single _most diverse_ study of CNN vs. RNN performance that we are aware of . And while many of the particular benchmarks are indeed `` small-sized '' in and of themselves , they are standard benchmarks for evaluating the performance of recurrent networks ( see appendix A for some references to papers that used these benchmark tests ) ; and we include experiments on domains such as Wikitext-103 , which is certainly not a small dataset . Regarding arXiv:1703.04691 , see our comments in the response to the discussant who originally brought this up ."}], "0": {"review_id": "rk8wKk-R--0", "review_text": "In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. They start by proposing a generic temporal convolution sequence model which leverages recent advances in the field, discuss the respective advantages of convolutional and recurrent networks, and benchmark their architecture on a number of different tasks. The paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works. The only part which is slightly less convincing is the section about effective memory size. While it is true that learning longer term dependencies can be difficult in standard RNN architectures, it is interesting to notice that the SoTA results presented in appendix B.3 for language modeling on larger data sets are architectures which focus on remedying this difficulty (cache model and hierarchical LSTM). It would also be interesting to see how TCN works on word prediction tasks which are devised explicitly to test for longer memory, such as Lambada (1) or Children Books Test (2). As a minor point, adding a measure of complexity in terms of number of operations could be a useful hardware-independent indication of the computational cost of the architecture. Pros: - Clearly written, well executed paper - Makes a strong point for the use of convolutional architecture for sequences - Provides useful benchmarks for the community Cons: - The claims on effective memory size need more context and justification 1: The LAMBADA dataset: Word prediction requiring a broad discourse context, Paperno et al. 2016 2: The Goldilocks principle: reading children's books with explicit memory representation, Hill et al. 2016", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the review , we agree with virtually all your points . As per your suggestion , we are currently integrating experiments on the LAMBADA dataset into the paper , and will post a revision with these results shortly ."}, "1": {"review_id": "rk8wKk-R--1", "review_text": "The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The paper describes an architecture similar to wavenet with residual connections. Empirical results are presented on a large number of tasks where the convolutional network often outperforms modern recurrent baselines or reaches similar performance. The biggest strength of the paper is the large number of tasks on which the models are evaluated. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. That said, I don\u2019t think that all the tasks are very relevant for comparing convolutional and recurrent architectures. While the time windows that RNNs can deal with are infinite in principle, it is common knowledge that the effective length of the dependencies RNNs can model is quite limited in practise. Many of the artificial task like the adding problem and sequential MNIST have been designed to highlight this weakness of RNNs. I don\u2019t find it very surprising that these tasks are easy to solve with a feedforward architecture with a large enough context window. The more impressive results are in my opinion those on the language modelling tasks where one would indeed expect RNNs to be more suitable for capturing dependencies that require stack-like memory functionality. While the related work is quite comprehensive, it downplays the popularity of convolutional architectures throughout history a bit. Especially in speech recognition, RNNs have only recently started to gain popularity while deep feedforward networks applied to overlapping time windows (i.e., 1D convolutions) have been the state-of-the-art for years. Of course the recent successes of dilated convolutions are likely to change the landscape in this application domain yet again. The paper is well-structured and written. If anything, it is perhaps a little bit wordy at times but I prefer that over obscurity due to brevity. The ideas in the paper are not novel and neither do the authors claim that they are. Unfortunately, I also think that the impact of the work is also somewhat limited due to the enormous success of the wavenet architecture. I do think that the results on the real-world tasks are valuable and worthy of publication. However, I feel that the authors exaggerate the extent to which researchers in this field still consider RNNs superior models for sequences. + Many experiments and tasks. + Well-written and clear. + Good results - Somewhat exaggerated claims about the extent to which RNNs are still being considered more suitable sequence models than dilated convolutions. Especially in light of the success of Wavenet. - Not much novelty/originality. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for this review . We agree on most points , except in the ultimate conclusions and assessment of the current `` default '' mindset of temporal modeling in RNNs . First , we agree that speech data in particular ( or perhaps audio data more broadly ) , is indeed one instance where CNNs do appear to have a historical edge over recurrent models , and we can emphasize this in the background section . Indeed , as you mention , the success of WaveNet has certainly made clear the power of CNNs in this application domain . The question , then , is to what extent the community already feels that the success of WaveNet in the speech setting is sufficient to `` standardize '' the use of CNNs across all sequence prediction tasks . And our genuine impression here is that these ideas have yet to permeate the mindset of the community for generic sequence prediction . Numerous resources ( e.g. , Goodfellow et al . 's deep learning book , with its chapter `` Sequence Modeling : Recurrent and Recursive Nets '' , plus virtually all current papers on recurrent networks ) , still highlight LSTMs and other similar architectures as the `` standard '' for sequence modeling . The precise goal of our work is to highlight the fact that WaveNet-like architectures ( though substantially simplified too , as we describe below ) can indeed work well across the many other settings we consider . And we feel that this is an important point to make empirically , even if the results or conclusion may seem `` unsurprising '' to people who are very familiar with CNN architectures . The second point , also , is that the architecture we consider is indeed simpler than WaveNet in many respects : e.g.no gated activation but just ReLUs ( which , as we highlighted in our response to a previous reviewer , we will include more experimentation on in a forthcoming update ) , no context stacks , etc ; and residual units and dilation structure that more directly mirror the corresponding `` standard '' architectures in convolutional image networks . Thus , a practitioner wishing to apply WaveNet-style architectures to some new sequence prediction task may be unclear about which elements of the architecture are really necessary , and we attempt to distill this as much as possible in our current paper . Overall , therefore , we agree that the significance of our current work is largely making the empirical point that TCN architectures are not just for audio , but really for any sequence modeling problem . But we do feel that this is an important point to make and thoroughly substantiate , even given the success of WaveNet ."}, "2": {"review_id": "rk8wKk-R--2", "review_text": "This paper argues that convolutional networks should be the default approach for sequence modeling. The paper is nicely done and rather easy to understand. Nevertheless, I find it difficult to assess its significance. In order to support the original hypothesis, I think that a much larger and more diverse set of experiments should have been considered. As pointed out by another reviewer please add https://arxiv.org/abs/1703.04691 to your references.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your note , though we honestly found it a bit surprising . The entire point of our paper _is_ to evaluate the improved TCN performance over a large and diverse set of experiments , and on this point it is by far the single _most diverse_ study of CNN vs. RNN performance that we are aware of . And while many of the particular benchmarks are indeed `` small-sized '' in and of themselves , they are standard benchmarks for evaluating the performance of recurrent networks ( see appendix A for some references to papers that used these benchmark tests ) ; and we include experiments on domains such as Wikitext-103 , which is certainly not a small dataset . Regarding arXiv:1703.04691 , see our comments in the response to the discussant who originally brought this up ."}}