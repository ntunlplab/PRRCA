{"year": "2021", "forum": "PpshD0AXfA", "title": "Generative Time-series Modeling with Fourier Flows", "decision": "Accept (Poster)", "meta_review": "Nice ideas with practical advantages.  ", "reviews": [{"review_id": "PpshD0AXfA-0", "review_text": "# # # Summary The paper presents Fourier Flows ( FF ) , which is a time series generative model in the frequency domain . It shows that the Jacobian of the DFT is equal to 1 , which means that DFT does not add too much overhead . The results on the real-world datasets are encouraging and expected because the predictive results mainly rely on the overall trend of the time series . By analysis in the frequency domain , we usually can capture the main trend accurately . The main concerns for the paper are the computational overhead on the proposed algorithm on non-periodic , long , and variable-length time series . # # # Feedback * The strongest point of the paper is when the authors show that the Jacobian of the DFT matrix is 1 . Thus , taking DFT does not make the generative model much more complex . * The other advantage of FF can be in handling missing values . Unfortunately , the authors do not expand on this aspect . * The main issue with the FF is its computational complexity . The authors write : `` To guarantee a lossless recovery of the time-series x via inverse DFT , we ensure that $ N\\geq T $ . '' This means FF is impractical for long time series and quite inefficient in handling variable-length time series . * Moreover , Fourier transforms are mainly intended for periodic signals and do not provide concise representations of non-stationary signals . This inefficiency is why Wavelets and DWT have been invented . The synthetic data generation model gives an unfair advantage to FF because it uses periodic signals . * The authors ' description for Eq . ( 8 ) is misleading . $ \\mathbf { H } $ and $ \\mathbf { h } $ are not fixed parameters , they are functions of the input . Thus , the model is a form of self-attention . I hope that the authors have done the training correctly . * The authors criticize GANs for memorization but never show that FFs do not memorize data points . * The choice of generating H using a BiRNN on only the $ Im ( X ) $ is puzzling . Why did n't the authors use complex-valued RNNs to operate on both real and imaginary parts and not lose the phase information ? * On page 2 , the authors discuss a parametric model for T , but they never elaborate further on it . * `` where $ x_- $ signifies the reversed version of $ x $ '' : the authors should clarify that this is the reflection with respect to $ x=0 $ axis . Otherwise , we should add a time-shift operation too . - # # # Post-Response Update I do n't think the authors have answered my second set of questions . While there are some doubts remaining in the paper , the idea looks fine . Although , I think a new paper with DWT will outperform this approach soon . I do not change my vote at this time .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your insightful comments and valuable suggestions . In what follows , we provide a point-by-point response to your comments , and explain the changes that will take place in the final version of the paper in order to reflect your suggestions . * * * Clarifications * * * First off , we would like to address some of your concerns by clarifying some points that may have not been clear in the original version of this submission . 1 ) * Regarding equation ( 8 ) and the notation for $ H $ and $ h $ * Thank you for pointing out to this typo . As you mention , $ H $ and $ h $ are not fixed parameters , but they should rather be denoted as $ H ( x ) $ and $ h ( x ) $ to avoid confusion . Of course , we have done the training using the spectral filtering architecture in Sec 4.2 and Fig 1 . 2 ) * Do FFs memorize training data ? * Please note that our criticism of generative models that can potentially memorize data points was directed at the * * implicit density modeling approach * * in general , and not just GANs . This is one of the key motivations for developing an * * explicit density model * * instead . Fourier flows , unlike GANs , are explicit density models that specify a class of probability distributions as its hypothesis class , and then selects the best distribution within this class to maximize a tractable and exact likelihoods . Because of its reliance on a maximum likelihood approach instead of a discriminator-generator architecture , normalizing flows has been shown to be less prone to data memorization [ R1 ] , and some previous works have even combined Flows and GANs to help `` regularize '' GANs ' training and suppress its potential for data memorization [ R2 ] . While the theoretical generalization properties of GANs have been previously investigated ( Nagarajan et al . ( 2018 ) ) , less work has been done in studying the theoretical generalization performance of flows . Proving that normalizing flows do not memorize data is beyond the scope of this paper as it would amount for a broader and more general result that does not just apply for Fourier flows or the time series setup . Please note though that we have shown ( empirically ) that FFs produce smoother distributions that do not concentrate probability mass on training points compared to TimeGANs ( see Figure 2 ( a ) ) . In the final version of this paper , we will make it clear that rigorous comparisons of data memorization potential in both classes of models is an important topic for future work , and we will be careful with phrasing the claims related to data memorization in the introductory section . 3 ) * Why BiRNN operates on $ Im ( X ) $ only ? * We can think of the complex number $ Re ( X ) + j Im ( X ) $ ( for a $ D $ -dimensional $ X $ ) as a new feature with $ 2D $ dimensions . The spectral filtering layer in our model uses the same architecture of the affine coupling layer in RealNVP and GLOW : it uses half of the dimensions ( which in this case is $ Im ( X ) $ ) to generate the data-dependent filter $ H $ , and then applies this filter to the remaining dimensions ( which in this case is $ Re ( X ) $ ) . The advantage of this scheme is that inversion is easy as the Jacobean of this transformation is a triangular matrix . * * Please note that this transformation does not throw away the phase information . * * In fact , the phase information is conveyed in $ angle ( X ) $ , which is split between both the real and imaginary values of $ X $ . Moreover , both $ Re ( X ) $ and $ Im ( X ) $ are present in the computation of $ Y_1 $ as highlighted in Table 1 . Finally , and most importantly , when we cascade multiple flows , we alternate between generating the filter using $ Re ( X ) $ and $ Im ( X ) $ ( and applying it to $ Im ( X ) $ and $ Re ( X ) $ , respectively ) , so the spectral filter is not generated exculsively using $ Im ( X ) $ in all of the cascaded flows . This point was not mentioned in the original submission but we will highlight it in the final version of the paper . 4 ) * Other issues * - The sequence length $ T $ is modeled through a simple binomial distribution . This was mentioned in page 2 but we will highlight it more prominently in the final version of the paper . - We will clarify that $ x_ { - } $ is a reflection with respect to the $ x=0 $ axis . Thank you for pointing out to this issue . - It is true that another advantage of FF is that it can be in handling missing values . Because the competing baselines do not have any inherent mechanisms for handling missing data , we did not consider missing data in our experiments to enable a fair comparison . However , we will add a discussion on the advantages of FFs when dealing with missing variables or irregulary sampled time series ."}, {"review_id": "PpshD0AXfA-1", "review_text": "The authors propose a flow generative model for time-series in the Fourier domain . The time-series data are first converted to the Fourier domain . Instead of the affine coupling layer previously presented in literature , the authors have designed a frequency domain version of the same . Pros : 1.The paper is well-written and is largely easy to follow . 2.Operating in the Fourier domain as well the affine coupling layer proposed in the paper are interesting novel contributions in the context of designing flow models for time-series . 3.Based on the experiments presented , Fourier Flow is surprisingly effective when compared to RealNVP , and also slightly outperforms other methods like TimeGAN . Cons : 1.The authors should try to explain what it is about operating in the Fourier domain causes the improvements in prediction error . As the Fourier transform is invertible , I would imagine that the novel affine coupling layer has a larger role to play in this . This is also what the authors seem to suggest in Section 4.3 . It would be great if the authors could add a baseline experiment operating in the time domain with the proposed affine layer ( Equation ( 8 ) ) . 2.In terms of the explanations in the paper , a lot of space has been devoted to explain the Fourier transform , its properties and how to compute it efficiently . While it is done well , I feel these are very well known facts . For the determinant of the Fourier matrix , we can just use the fact that it is unitary and we immediately have |det ( W ) | = 1 . Instead , additional experiments analysing the experiments could be more useful , such as understanding the role of network depth etc . for this particular method , or additional t-SNE plots for the remaining datasets . 3.As in all papers such as this one , it is not easy to see how the baseline approaches have been trained and how well the hyperparameters have been tuned . Based on the author response , I am willing to increase my rating for this paper . UPDATE AFTER AUTHOR RESPONSE : The authors have addressed all my queries and made the changes that I requested . I am increasing my rating reflecting this . The paper has novelty , good experiments and improved performance . I would like to see the paper accepted .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your insightful review ! In what follows , we address all your comments , and highlight the changes that will be applied to the final version of the paper in order to incorporate your suggestions . * * * Explaining the source of improvements in predictive accuracy * * * As you pointed out , and as evident in both Figure 2 ( b ) and Table 2 , Fourier Flows improved the predictive accuracy of models trained on sinusoidal synthetic data ; this improvement is more notable in the synthetic data experiment ( Figure 2 ( b ) ) than in the real data experiments ( Table 2 ) . Please note that the key idea behind the Fourier flow is to use a unitary transformation ( i.e.one with a low-complexity/trivial Jacobean ) that can then be followed by other transformations that are not bound to the strict structural assumptions adopted in the literature . The combination of DFT + a subsequent transformation ( even as simple as an affine transform ) creates an overall complex transformation . * * We are highlighting this point to stress that Fourier transofrm is not a redundant step , and that the spectral filtering layer by itself would reduce to a standard RealNVP model ( with an RNN instead of an NN ) without the DFT layer . * * As for the improvement in predictive accuracy , we believe that this comes from three sources , which we explain as follows : 1 ) * DFT layer compresses temporal information into a low-dimensional spectral representation * The first modeling advantage in a Fourier Flow is that the DFT layer compresses temporal patterns into lower-dimensional spectral patterns , enabling a more efficient distribution learning . Consider for instance the sinusoidal example in Sec 5.1 . In this example the data $ x $ is drawn from the following stochastic process : $ x \\sim \\sin ( 2\\pi\\ , f\\ , t ) , $ where $ f $ is a random frequency drawn from a predefined distribution . To model length- $ T $ time series drawn from such process using conventional methods , we would need to model a $ T $ -dimensional random variable . However , using the DFT transform , we would be modeling the spectral representation of $ x $ given by : $ X = \\frac { j } { 2 } [ \\delta ( v + f ) - \\delta ( v - f ) ] , $ where $ \\delta $ is the Dirac-delta function . Thus , the spectral representation $ X $ can be fully described with one piece of information , which is the location of the frequency component $ f $ . This means that modeling the distribution of $ X $ simply reduces to modeling a 1-dimensional random variable , which is much more efficient than modeling a $ T $ -dimensional variable . This is the reason why our model significantly outeprforms RealNVP in the task of modeling random sine waves in Section 5.1 . While the sinusoidal example ( or periodic signals in general ) is an extreme case wherein a single variable describes the entire time series , a spectral representation of a time series will in general be a low-dimensional compression of repetitive temporal patterns and trends . 2 ) * Fourier flows enable constructing complex transformations at no extra cost for Jacobean computation * As explained above , even without the novel spectral layer , a DFT layer followed by a simple transform ( such as an affine transform ) would amount for a complex overall transform without any extra complications associated with the Jacobean transformation . Also , because the DFT layer parameters are not learnable , the Fourier flow does not involve more parameters than an equivalent time-domain flow that uses the same filtering layer $ h $ . 3 ) * The novel spectral filtering layer * As you rightly point out , the novel spectral filtering layer used after the DFT step is another source of modeling improvement . We will highlight the three points above in the final version of the paper ."}, {"review_id": "PpshD0AXfA-2", "review_text": "Summary : The paper introduces a new convolutional flow architecture that uses the DFT to convert the generated time series to the frequency domain . Convolutions are performed by multiplication in the frequency domain through a spectral affine layer that transform the even or odd part of the signal using a data dependent filter . The resulting time-domain convolution has input dependent weights , an interesting and original approach clearly different from other convolutional flow such as [ 1 ] . Relevance : Time series generative modeling has a wide range of crucial applications in fields ranging from medicine to finance . The new method shows very promising performance and it has the potential to become a state-of-the-art method for time series generation . Originality : The use of the DFT and spectral affine layers is original in the context of normalizing flows . Importantly , the DFT is very suitable for flows since it is an isometry and has therefore a trivial Jacobin . The use of input dependent convolutions is very interesting even in the context of regular ConvNet architectures . Scientific quality : The method is clearly presented and well motivated . The experiment section cover a decently large set of experiments and performance are compared with a very large number of state-of-the-art baselines . Pros : - Original new architecture well motivated for time series applications - Rigorous experiments with multiple relevant baselines - Very promising experimental results Cons : - It would have been useful to see a wider range of real world applications Minor points : - I would like to have a figure with the time domain sampled generated by the FF and the other baselines . It is strange to see a generative modeling paper without a figure showing the generated samples . References : [ 1 ] Karami , Mahdi , et al . `` Invertible convolutional flow . '' Advances in Neural Information Processing Systems . 2019 .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your valuable comments . Below is a description of the changes that we will apply to the final version of the paper in order to incorporate all your suggestions . * * * Additional experimental results * * * We have added two data sets to the experimental setup in Section 5.2 : ( a ) MIMIC-III , which is data set for intensive care unit ( ICU ) time-series data [ R1 ] , and ( b ) an electrocardiogram ( ECG ) data set from Kaggle [ R2 ] . These two data sets cover two medical applications with different classes of time series data : ICU data comprises trends of biomarkers for severly ill patients over time , and ECG data comprises quasi-periodic heart 's rhythm data . We hope that these additional data sets cover a wider range of applications as requested . We compared Fourier Flows with RealNVP and the most competitive baseline ( TimeGAN ) results are provided below . In the final paper , we will add the performance results for all of the remaining baselines as well . _______________________________________________________ * * Predictive scores ( 95 % confidence intervals ) * * _______________________________________________________ * * Model * * - * * MIMIC * * - * * ECG * * _______________________________________________________ TimeGAN-0.1501 $ \\pm $ 0.04 - 0.1621 $ \\pm $ 0.03 RealNVP -- 0.1521 $ \\pm $ 0.05 - 0.1602 $ \\pm $ 0.03 FF -- 0.1517 $ \\pm $ 0.04 - 0.1567 $ \\pm $ 0.02 _______________________________________________________ As can be seen in the Table above , FF significantly outperforms the other models on the ECG data setthis is likely because of the quasi-period nature of this data , which makes Fourier transform a natural representation for samples in ECG . FF also performs competitvely on MIMIC-III . * * * Visualizing synthetic data samples * * * Thank you for this important suggestion . Samples from the sinusodial data set ( Sec 5.1 ) , the stock data and the ECG data ( Sec 5.2 ) can be viewed through this anonymized link : https : //ibb.co/wMkM2GX . We will add these visualizations to the supplementary material of the final version of the paper . * * * References * * * [ R1 ] Johnson , Alistair EW , et al . `` MIMIC-III , a freely accessible critical care database . '' Scientific data 3.1 ( 2016 ) : 1-9 . [ R2 ] https : //www.kaggle.com/shayanfazeli/heartbeat/tasks"}, {"review_id": "PpshD0AXfA-3", "review_text": "Pros : + This paper proposed a novel generative model named Fourier flow for modeling time series data . The model incorporates Fourier transformation into normalizing flows and considers the time series on the frequency domain , rather than the time domain . Such a combination looks interesting and novel . I think this paper will have some impact on the field of normalizing flows and time series analysis . + The writing of this paper looks good , which makes it easy to follow . Cons : - The empirical study is somewhat weak . The experimental results are not very impressive in the paper . The improvement of the performance seems to be marginal . Only one real-world dataset is considered . Please give a short description of the dataset . As the proposed method is for the general time series , the authors are suggested to evaluate their method on more datasets , especially from various fields . Besides , please give a definition of the metric predictive score . - The paper mentions that the computational cost is no larger than some SOTAmethods . It would be better to give a discussion on the complexities of your method and the SOTAs . It would also be more convincing if the authors can show the running time in the experiment .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your insightful review ! Below is a detailed response to your comments and a description of how we will incorporate all your suggestions in the final version of the paper . * * * Improvements in predictive performance * * * Please note that the main goal of the paper was to develop a first explicit likelihood model for time series data that is trained via exact likelihood maximization . Thus , while it is true that the performance improvements ( with respect to the predictive power of an RNN model trained on synthetic data ) on most data sets were marginal , the key message of our paper is that it is possible to use explicit likelihood generative models that * * perform competitively * * with SOTA implicit likelihood models such as GANs . We believe that this is a significant result for three reasons : - To the best of our knowledge , this is the first generative model for time series data that can be optimized using the * * exact * * likelihood . This is an advancement on existing explicit likelihood models , such as HMMs ( Beal et al . ( 2002 ) ) or Deep state space models ( Krishnan et al . ( 2017 ) ) , which can only be optimized using variational lower bounds . - Unlike SOTA implicit likelihood models , such as GANs , our model 's accuracy ( goodness-of-fit ) can be evaluated using the exact model likelihood . - Unlike SOTA implicit likelihood models , such as GANs , explicit likelihood models such as ours are less prone to data memorization , which is suitable for applications with strict privacy requirements . * * * Expanding the experimental results * * * First off , we would like to highlight that in our original submission , we * * have considered three real-world data sets ( Energy , Stocks and Lung cancer ) * * and not just one as indicated in your review . In addition to these 3 data sets , we have also added two extra data sets to the experimental setup in Section 5.2 : ( a ) MIMIC-III , which is data set for intensive care unit ( ICU ) time-series data [ R1 ] , and ( b ) an electrocardiogram ( ECG ) data set from Kaggle [ R2 ] . These extra data sets cover two medical applications with different classes of time series data : ICU data comprises trends of biomarkers for severly ill patients over time , and ECG data comprises quasi-periodic heart 's rhythm data . We hope that these extra data sets address your request for a strengthened empirical study and widened application fields . Performance comparison with the most competitive baseline ( TimeGAN ) is provided below . In the final paper , we will add the performance results for all of the remaining baselines as well . _______________________________________________________ * * Predictive scores ( 95 % confidence intervals ) * * _______________________________________________________ * * Model * * - * * MIMIC * * - * * ECG * * _______________________________________________________ TimeGAN-0.1501 $ \\pm $ 0.04 - 0.1621 $ \\pm $ 0.03 RealNVP -- 0.1521 $ \\pm $ 0.05 - 0.1602 $ \\pm $ 0.03 FF -- 0.1517 $ \\pm $ 0.04 - 0.1567 $ \\pm $ 0.02 _______________________________________________________ * * * Description of the data set and evaluation metrics * * * Please note that , as mentioned in the paper , we have replicated the experimental setup in ( Yoon et al . ( 2019 ) ) the detailed description of the data sets and evaluation metrics were provided in this paper . However , to address this comment , we will add the following descriptions to the supplementary material in the final version of our paper . _______________________________________________________ Dataset -- Number of Sequences -- Dimensions -- Avg . Sequence Length _______________________________________________________ Stocks -- 3,773 -- 6 -- 24 Energy - 19,711 -- 29 -- 24 Lung cancer -- 149,967 -- 54 -- 58 _______________________________________________________ The evaluation metric was the defined as the mean absolute error ( MAE ) of the predictions made by a model trained on the synthetic data and tested on the real data . The predictive model for all baseline was 2-layer LSTM model for sequence prediction , trained to predict the next observation at each time step as highlighted in ( Yoon et al . ( 2019 ) ) ."}], "0": {"review_id": "PpshD0AXfA-0", "review_text": "# # # Summary The paper presents Fourier Flows ( FF ) , which is a time series generative model in the frequency domain . It shows that the Jacobian of the DFT is equal to 1 , which means that DFT does not add too much overhead . The results on the real-world datasets are encouraging and expected because the predictive results mainly rely on the overall trend of the time series . By analysis in the frequency domain , we usually can capture the main trend accurately . The main concerns for the paper are the computational overhead on the proposed algorithm on non-periodic , long , and variable-length time series . # # # Feedback * The strongest point of the paper is when the authors show that the Jacobian of the DFT matrix is 1 . Thus , taking DFT does not make the generative model much more complex . * The other advantage of FF can be in handling missing values . Unfortunately , the authors do not expand on this aspect . * The main issue with the FF is its computational complexity . The authors write : `` To guarantee a lossless recovery of the time-series x via inverse DFT , we ensure that $ N\\geq T $ . '' This means FF is impractical for long time series and quite inefficient in handling variable-length time series . * Moreover , Fourier transforms are mainly intended for periodic signals and do not provide concise representations of non-stationary signals . This inefficiency is why Wavelets and DWT have been invented . The synthetic data generation model gives an unfair advantage to FF because it uses periodic signals . * The authors ' description for Eq . ( 8 ) is misleading . $ \\mathbf { H } $ and $ \\mathbf { h } $ are not fixed parameters , they are functions of the input . Thus , the model is a form of self-attention . I hope that the authors have done the training correctly . * The authors criticize GANs for memorization but never show that FFs do not memorize data points . * The choice of generating H using a BiRNN on only the $ Im ( X ) $ is puzzling . Why did n't the authors use complex-valued RNNs to operate on both real and imaginary parts and not lose the phase information ? * On page 2 , the authors discuss a parametric model for T , but they never elaborate further on it . * `` where $ x_- $ signifies the reversed version of $ x $ '' : the authors should clarify that this is the reflection with respect to $ x=0 $ axis . Otherwise , we should add a time-shift operation too . - # # # Post-Response Update I do n't think the authors have answered my second set of questions . While there are some doubts remaining in the paper , the idea looks fine . Although , I think a new paper with DWT will outperform this approach soon . I do not change my vote at this time .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your insightful comments and valuable suggestions . In what follows , we provide a point-by-point response to your comments , and explain the changes that will take place in the final version of the paper in order to reflect your suggestions . * * * Clarifications * * * First off , we would like to address some of your concerns by clarifying some points that may have not been clear in the original version of this submission . 1 ) * Regarding equation ( 8 ) and the notation for $ H $ and $ h $ * Thank you for pointing out to this typo . As you mention , $ H $ and $ h $ are not fixed parameters , but they should rather be denoted as $ H ( x ) $ and $ h ( x ) $ to avoid confusion . Of course , we have done the training using the spectral filtering architecture in Sec 4.2 and Fig 1 . 2 ) * Do FFs memorize training data ? * Please note that our criticism of generative models that can potentially memorize data points was directed at the * * implicit density modeling approach * * in general , and not just GANs . This is one of the key motivations for developing an * * explicit density model * * instead . Fourier flows , unlike GANs , are explicit density models that specify a class of probability distributions as its hypothesis class , and then selects the best distribution within this class to maximize a tractable and exact likelihoods . Because of its reliance on a maximum likelihood approach instead of a discriminator-generator architecture , normalizing flows has been shown to be less prone to data memorization [ R1 ] , and some previous works have even combined Flows and GANs to help `` regularize '' GANs ' training and suppress its potential for data memorization [ R2 ] . While the theoretical generalization properties of GANs have been previously investigated ( Nagarajan et al . ( 2018 ) ) , less work has been done in studying the theoretical generalization performance of flows . Proving that normalizing flows do not memorize data is beyond the scope of this paper as it would amount for a broader and more general result that does not just apply for Fourier flows or the time series setup . Please note though that we have shown ( empirically ) that FFs produce smoother distributions that do not concentrate probability mass on training points compared to TimeGANs ( see Figure 2 ( a ) ) . In the final version of this paper , we will make it clear that rigorous comparisons of data memorization potential in both classes of models is an important topic for future work , and we will be careful with phrasing the claims related to data memorization in the introductory section . 3 ) * Why BiRNN operates on $ Im ( X ) $ only ? * We can think of the complex number $ Re ( X ) + j Im ( X ) $ ( for a $ D $ -dimensional $ X $ ) as a new feature with $ 2D $ dimensions . The spectral filtering layer in our model uses the same architecture of the affine coupling layer in RealNVP and GLOW : it uses half of the dimensions ( which in this case is $ Im ( X ) $ ) to generate the data-dependent filter $ H $ , and then applies this filter to the remaining dimensions ( which in this case is $ Re ( X ) $ ) . The advantage of this scheme is that inversion is easy as the Jacobean of this transformation is a triangular matrix . * * Please note that this transformation does not throw away the phase information . * * In fact , the phase information is conveyed in $ angle ( X ) $ , which is split between both the real and imaginary values of $ X $ . Moreover , both $ Re ( X ) $ and $ Im ( X ) $ are present in the computation of $ Y_1 $ as highlighted in Table 1 . Finally , and most importantly , when we cascade multiple flows , we alternate between generating the filter using $ Re ( X ) $ and $ Im ( X ) $ ( and applying it to $ Im ( X ) $ and $ Re ( X ) $ , respectively ) , so the spectral filter is not generated exculsively using $ Im ( X ) $ in all of the cascaded flows . This point was not mentioned in the original submission but we will highlight it in the final version of the paper . 4 ) * Other issues * - The sequence length $ T $ is modeled through a simple binomial distribution . This was mentioned in page 2 but we will highlight it more prominently in the final version of the paper . - We will clarify that $ x_ { - } $ is a reflection with respect to the $ x=0 $ axis . Thank you for pointing out to this issue . - It is true that another advantage of FF is that it can be in handling missing values . Because the competing baselines do not have any inherent mechanisms for handling missing data , we did not consider missing data in our experiments to enable a fair comparison . However , we will add a discussion on the advantages of FFs when dealing with missing variables or irregulary sampled time series ."}, "1": {"review_id": "PpshD0AXfA-1", "review_text": "The authors propose a flow generative model for time-series in the Fourier domain . The time-series data are first converted to the Fourier domain . Instead of the affine coupling layer previously presented in literature , the authors have designed a frequency domain version of the same . Pros : 1.The paper is well-written and is largely easy to follow . 2.Operating in the Fourier domain as well the affine coupling layer proposed in the paper are interesting novel contributions in the context of designing flow models for time-series . 3.Based on the experiments presented , Fourier Flow is surprisingly effective when compared to RealNVP , and also slightly outperforms other methods like TimeGAN . Cons : 1.The authors should try to explain what it is about operating in the Fourier domain causes the improvements in prediction error . As the Fourier transform is invertible , I would imagine that the novel affine coupling layer has a larger role to play in this . This is also what the authors seem to suggest in Section 4.3 . It would be great if the authors could add a baseline experiment operating in the time domain with the proposed affine layer ( Equation ( 8 ) ) . 2.In terms of the explanations in the paper , a lot of space has been devoted to explain the Fourier transform , its properties and how to compute it efficiently . While it is done well , I feel these are very well known facts . For the determinant of the Fourier matrix , we can just use the fact that it is unitary and we immediately have |det ( W ) | = 1 . Instead , additional experiments analysing the experiments could be more useful , such as understanding the role of network depth etc . for this particular method , or additional t-SNE plots for the remaining datasets . 3.As in all papers such as this one , it is not easy to see how the baseline approaches have been trained and how well the hyperparameters have been tuned . Based on the author response , I am willing to increase my rating for this paper . UPDATE AFTER AUTHOR RESPONSE : The authors have addressed all my queries and made the changes that I requested . I am increasing my rating reflecting this . The paper has novelty , good experiments and improved performance . I would like to see the paper accepted .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your insightful review ! In what follows , we address all your comments , and highlight the changes that will be applied to the final version of the paper in order to incorporate your suggestions . * * * Explaining the source of improvements in predictive accuracy * * * As you pointed out , and as evident in both Figure 2 ( b ) and Table 2 , Fourier Flows improved the predictive accuracy of models trained on sinusoidal synthetic data ; this improvement is more notable in the synthetic data experiment ( Figure 2 ( b ) ) than in the real data experiments ( Table 2 ) . Please note that the key idea behind the Fourier flow is to use a unitary transformation ( i.e.one with a low-complexity/trivial Jacobean ) that can then be followed by other transformations that are not bound to the strict structural assumptions adopted in the literature . The combination of DFT + a subsequent transformation ( even as simple as an affine transform ) creates an overall complex transformation . * * We are highlighting this point to stress that Fourier transofrm is not a redundant step , and that the spectral filtering layer by itself would reduce to a standard RealNVP model ( with an RNN instead of an NN ) without the DFT layer . * * As for the improvement in predictive accuracy , we believe that this comes from three sources , which we explain as follows : 1 ) * DFT layer compresses temporal information into a low-dimensional spectral representation * The first modeling advantage in a Fourier Flow is that the DFT layer compresses temporal patterns into lower-dimensional spectral patterns , enabling a more efficient distribution learning . Consider for instance the sinusoidal example in Sec 5.1 . In this example the data $ x $ is drawn from the following stochastic process : $ x \\sim \\sin ( 2\\pi\\ , f\\ , t ) , $ where $ f $ is a random frequency drawn from a predefined distribution . To model length- $ T $ time series drawn from such process using conventional methods , we would need to model a $ T $ -dimensional random variable . However , using the DFT transform , we would be modeling the spectral representation of $ x $ given by : $ X = \\frac { j } { 2 } [ \\delta ( v + f ) - \\delta ( v - f ) ] , $ where $ \\delta $ is the Dirac-delta function . Thus , the spectral representation $ X $ can be fully described with one piece of information , which is the location of the frequency component $ f $ . This means that modeling the distribution of $ X $ simply reduces to modeling a 1-dimensional random variable , which is much more efficient than modeling a $ T $ -dimensional variable . This is the reason why our model significantly outeprforms RealNVP in the task of modeling random sine waves in Section 5.1 . While the sinusoidal example ( or periodic signals in general ) is an extreme case wherein a single variable describes the entire time series , a spectral representation of a time series will in general be a low-dimensional compression of repetitive temporal patterns and trends . 2 ) * Fourier flows enable constructing complex transformations at no extra cost for Jacobean computation * As explained above , even without the novel spectral layer , a DFT layer followed by a simple transform ( such as an affine transform ) would amount for a complex overall transform without any extra complications associated with the Jacobean transformation . Also , because the DFT layer parameters are not learnable , the Fourier flow does not involve more parameters than an equivalent time-domain flow that uses the same filtering layer $ h $ . 3 ) * The novel spectral filtering layer * As you rightly point out , the novel spectral filtering layer used after the DFT step is another source of modeling improvement . We will highlight the three points above in the final version of the paper ."}, "2": {"review_id": "PpshD0AXfA-2", "review_text": "Summary : The paper introduces a new convolutional flow architecture that uses the DFT to convert the generated time series to the frequency domain . Convolutions are performed by multiplication in the frequency domain through a spectral affine layer that transform the even or odd part of the signal using a data dependent filter . The resulting time-domain convolution has input dependent weights , an interesting and original approach clearly different from other convolutional flow such as [ 1 ] . Relevance : Time series generative modeling has a wide range of crucial applications in fields ranging from medicine to finance . The new method shows very promising performance and it has the potential to become a state-of-the-art method for time series generation . Originality : The use of the DFT and spectral affine layers is original in the context of normalizing flows . Importantly , the DFT is very suitable for flows since it is an isometry and has therefore a trivial Jacobin . The use of input dependent convolutions is very interesting even in the context of regular ConvNet architectures . Scientific quality : The method is clearly presented and well motivated . The experiment section cover a decently large set of experiments and performance are compared with a very large number of state-of-the-art baselines . Pros : - Original new architecture well motivated for time series applications - Rigorous experiments with multiple relevant baselines - Very promising experimental results Cons : - It would have been useful to see a wider range of real world applications Minor points : - I would like to have a figure with the time domain sampled generated by the FF and the other baselines . It is strange to see a generative modeling paper without a figure showing the generated samples . References : [ 1 ] Karami , Mahdi , et al . `` Invertible convolutional flow . '' Advances in Neural Information Processing Systems . 2019 .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your valuable comments . Below is a description of the changes that we will apply to the final version of the paper in order to incorporate all your suggestions . * * * Additional experimental results * * * We have added two data sets to the experimental setup in Section 5.2 : ( a ) MIMIC-III , which is data set for intensive care unit ( ICU ) time-series data [ R1 ] , and ( b ) an electrocardiogram ( ECG ) data set from Kaggle [ R2 ] . These two data sets cover two medical applications with different classes of time series data : ICU data comprises trends of biomarkers for severly ill patients over time , and ECG data comprises quasi-periodic heart 's rhythm data . We hope that these additional data sets cover a wider range of applications as requested . We compared Fourier Flows with RealNVP and the most competitive baseline ( TimeGAN ) results are provided below . In the final paper , we will add the performance results for all of the remaining baselines as well . _______________________________________________________ * * Predictive scores ( 95 % confidence intervals ) * * _______________________________________________________ * * Model * * - * * MIMIC * * - * * ECG * * _______________________________________________________ TimeGAN-0.1501 $ \\pm $ 0.04 - 0.1621 $ \\pm $ 0.03 RealNVP -- 0.1521 $ \\pm $ 0.05 - 0.1602 $ \\pm $ 0.03 FF -- 0.1517 $ \\pm $ 0.04 - 0.1567 $ \\pm $ 0.02 _______________________________________________________ As can be seen in the Table above , FF significantly outperforms the other models on the ECG data setthis is likely because of the quasi-period nature of this data , which makes Fourier transform a natural representation for samples in ECG . FF also performs competitvely on MIMIC-III . * * * Visualizing synthetic data samples * * * Thank you for this important suggestion . Samples from the sinusodial data set ( Sec 5.1 ) , the stock data and the ECG data ( Sec 5.2 ) can be viewed through this anonymized link : https : //ibb.co/wMkM2GX . We will add these visualizations to the supplementary material of the final version of the paper . * * * References * * * [ R1 ] Johnson , Alistair EW , et al . `` MIMIC-III , a freely accessible critical care database . '' Scientific data 3.1 ( 2016 ) : 1-9 . [ R2 ] https : //www.kaggle.com/shayanfazeli/heartbeat/tasks"}, "3": {"review_id": "PpshD0AXfA-3", "review_text": "Pros : + This paper proposed a novel generative model named Fourier flow for modeling time series data . The model incorporates Fourier transformation into normalizing flows and considers the time series on the frequency domain , rather than the time domain . Such a combination looks interesting and novel . I think this paper will have some impact on the field of normalizing flows and time series analysis . + The writing of this paper looks good , which makes it easy to follow . Cons : - The empirical study is somewhat weak . The experimental results are not very impressive in the paper . The improvement of the performance seems to be marginal . Only one real-world dataset is considered . Please give a short description of the dataset . As the proposed method is for the general time series , the authors are suggested to evaluate their method on more datasets , especially from various fields . Besides , please give a definition of the metric predictive score . - The paper mentions that the computational cost is no larger than some SOTAmethods . It would be better to give a discussion on the complexities of your method and the SOTAs . It would also be more convincing if the authors can show the running time in the experiment .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your insightful review ! Below is a detailed response to your comments and a description of how we will incorporate all your suggestions in the final version of the paper . * * * Improvements in predictive performance * * * Please note that the main goal of the paper was to develop a first explicit likelihood model for time series data that is trained via exact likelihood maximization . Thus , while it is true that the performance improvements ( with respect to the predictive power of an RNN model trained on synthetic data ) on most data sets were marginal , the key message of our paper is that it is possible to use explicit likelihood generative models that * * perform competitively * * with SOTA implicit likelihood models such as GANs . We believe that this is a significant result for three reasons : - To the best of our knowledge , this is the first generative model for time series data that can be optimized using the * * exact * * likelihood . This is an advancement on existing explicit likelihood models , such as HMMs ( Beal et al . ( 2002 ) ) or Deep state space models ( Krishnan et al . ( 2017 ) ) , which can only be optimized using variational lower bounds . - Unlike SOTA implicit likelihood models , such as GANs , our model 's accuracy ( goodness-of-fit ) can be evaluated using the exact model likelihood . - Unlike SOTA implicit likelihood models , such as GANs , explicit likelihood models such as ours are less prone to data memorization , which is suitable for applications with strict privacy requirements . * * * Expanding the experimental results * * * First off , we would like to highlight that in our original submission , we * * have considered three real-world data sets ( Energy , Stocks and Lung cancer ) * * and not just one as indicated in your review . In addition to these 3 data sets , we have also added two extra data sets to the experimental setup in Section 5.2 : ( a ) MIMIC-III , which is data set for intensive care unit ( ICU ) time-series data [ R1 ] , and ( b ) an electrocardiogram ( ECG ) data set from Kaggle [ R2 ] . These extra data sets cover two medical applications with different classes of time series data : ICU data comprises trends of biomarkers for severly ill patients over time , and ECG data comprises quasi-periodic heart 's rhythm data . We hope that these extra data sets address your request for a strengthened empirical study and widened application fields . Performance comparison with the most competitive baseline ( TimeGAN ) is provided below . In the final paper , we will add the performance results for all of the remaining baselines as well . _______________________________________________________ * * Predictive scores ( 95 % confidence intervals ) * * _______________________________________________________ * * Model * * - * * MIMIC * * - * * ECG * * _______________________________________________________ TimeGAN-0.1501 $ \\pm $ 0.04 - 0.1621 $ \\pm $ 0.03 RealNVP -- 0.1521 $ \\pm $ 0.05 - 0.1602 $ \\pm $ 0.03 FF -- 0.1517 $ \\pm $ 0.04 - 0.1567 $ \\pm $ 0.02 _______________________________________________________ * * * Description of the data set and evaluation metrics * * * Please note that , as mentioned in the paper , we have replicated the experimental setup in ( Yoon et al . ( 2019 ) ) the detailed description of the data sets and evaluation metrics were provided in this paper . However , to address this comment , we will add the following descriptions to the supplementary material in the final version of our paper . _______________________________________________________ Dataset -- Number of Sequences -- Dimensions -- Avg . Sequence Length _______________________________________________________ Stocks -- 3,773 -- 6 -- 24 Energy - 19,711 -- 29 -- 24 Lung cancer -- 149,967 -- 54 -- 58 _______________________________________________________ The evaluation metric was the defined as the mean absolute error ( MAE ) of the predictions made by a model trained on the synthetic data and tested on the real data . The predictive model for all baseline was 2-layer LSTM model for sequence prediction , trained to predict the next observation at each time step as highlighted in ( Yoon et al . ( 2019 ) ) ."}}