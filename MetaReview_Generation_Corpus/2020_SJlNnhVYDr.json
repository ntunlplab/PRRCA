{"year": "2020", "forum": "SJlNnhVYDr", "title": "Soft Token Matching for Interpretable Low-Resource Classification", "decision": "Reject", "meta_review": "The authors focus on low-resource text classifications tasks augmented with \"rationales\". They propose a new technique that improves performance over existing approaches and that allows human inspection of the learned weights.\n\nAlthough the reviewers did not find any major faults with the paper, they were in consensus that the paper should be rejected at this time. Generally, the reviewers' reservations were in terms of novelty and extent of technical contribution.\n\nGiven the large number of submissions this year, I am recommending rejection for this paper.\n", "reviews": [{"review_id": "SJlNnhVYDr-0", "review_text": "After responses: I read the authors response and decided to stick to my original score mostly because: 1 - I understand that interpretability is hard to define. I also agree with the authors response. However, this is still not reflected in the paper in any way. I expect a discussion on what is the relevant definition used in the paper and how does it fit to that definition. Currently, it is very confusing to the reader. 2 - I understand the authors' response that few-shot learning is a different empirical setting. However, authors also agree that settings are some-what relevant. I really do not see any gain by NOT discussing the few-shot learning literature. At the end, a reader is interested in this work if they have limited data. Moreover, other ways to address limited data issue should be discussed. ----- The manuscript is proposing a few-shot classification setting in which training set includes only few examples. The main contribution is using prototype embeddings and representing each word as cosine distances to these prototype embeddings. Moreover, the final classification is weighted summation of the per-token decisions followed by a soft-max. Per-token classifiers are obtained with an MLP using the cosine distances as features. When the relevance labels are available, they are used in training to boost gradients. PRO(s) The proposed method is interesting and addressing an important problem. There are many few-shot scenarios and finding good models for them is impactful. The results are promising and the proposed method is more interpretable than the existing NLP classifiers. I disagree with the claim that the model is interpretable. However, I appreciate the effort to interpret the model. CON(s) The model is not interpretable because 1) it starts with embeddings and they are not interpretable, 2) model is full of non-linearities and decision boundaries are not possible to find. In other words, it is not possible to answer \"what would make this model predict some other classifier\". The authors should discuss the existing few-shot learning mechanisms. Especially, \"Prototypical Networks for Few-shot Learning\" is very relevant. I also think it can be included as a baseline with very minimal modifications. The writing is not complete. The authors do not even discuss how the prototypes are learned. I am assuming it is done using full gradient-descent over all parameters. However, this is not clearly discussed. Implementation details should be discussed more clearly. SUMMARY I believe the manuscript is definitely interesting and has a potential. In the mean time, It is not ready for publication. It needs a through review of few-shot learning. Authors should also discuss can any of the few-shot learning methods be included in the experimental study. If the answer is yes, it should be. If the answer is no, it should be explained clearly. Although my recommendation is weak-reject, I am happy to bump it up if these issues are addressed.", "rating": "3: Weak Reject", "reply_text": "We would like to thank the reviewer for his/her constructive comments . We provide an answer to each point raised below . * * QUESTION * * The model is not interpretable because 1 ) it starts with embeddings and they are not interpretable , 2 ) model is full of non-linearities and decision boundaries are not possible to find . In other words , it is not possible to answer `` what would make this model predict some other classifier '' . * * ANSWER * * We thank the reviewer for letting us clarify this point . It is common knowledge that it is very tricky to define what it means for a model to be interpretable [ 1 ] . We do agree with the reviewer that we can not interpret the given embedding space and that it is difficult to derive decision boundaries for the model ; however , when using the term \u201c interpretable \u201d , we followed the definition of Interpretability given in [ 2 ] , i.e. , \u201c to which extent the model and/or its predictions are human understandable \u201d . We argue to have provided a model that is interpretable according to this definition . When a sentence is classified , we are able to inspect the individual contribution of each token to the final classification score , and by design the model is forced to assign high scores to relevant tokens , as shown in Figure 2 , page 8 . Moreover , we can understand which prototypes/logical features led to such an important individual score . In turn , this allows us to understand which prototypes were most relevant , and we are able to match them with semantic concepts associated with pre-trained embeddings , using the mechanism discussed in Section 4.3 . In summary , we are able to give a human-readable suggestion on what semantic concepts are most important in the sentence and how they combine to give the final answer . This allows us to \u201c bypass \u201d the barrier imposed by the presence of non-linearities in the architecture . Nonetheless , we kindly ask the reviewer for further suggestions on how he/she thinks we should rephrase this concept in the paper if needed . For the time being , we have added a reference to [ 2 ] in our paper to clarify what we mean with the term \u201c interpretable \u201d . * * QUESTION * * The authors should discuss the existing few-shot learning mechanisms . Especially , `` Prototypical Networks for Few-shot Learning '' is very relevant . I also think it can be included as a baseline with very minimal modifications . * * ANSWER * * We tend to disagree with the reviewer on this matter . Despite similar , we think our experimental setting significantly differs from the few-shot scenario , in which ( as also written in [ 3 ] and [ 4 ] ) a classifier must generalize to new classes not seen in the training set . In our scenario , we do not have unseen classes . Moreover , in order to exploit rationales , it is necessary to see the corresponding class . These are the main reasons why we decided not to include few-shot learning mechanisms in the related works in the first place . We believe these two application scenarios have to be clearly separated from each other . However , we thank the reviewer for her/his comment as it might have shed light on a future work in which we modify PARCUS to fit the few-shot or zero-shot setting , and we hope our explanation satisfies the reviewer \u2019 s concern . * * QUESTION * * The writing is not complete . The authors do not even discuss how the prototypes are learned . I am assuming it is done using full gradient-descent over all parameters . However , this is not clearly discussed . Implementation details should be discussed more clearly . * * ANSWER * * We thank the reviewer for highlighting this point . We addressed this point in Section 3 ."}, {"review_id": "SJlNnhVYDr-1", "review_text": "The authors propose PARCUS (\"Pattern Representations on Continuous Spaces\"), a model which computes a soft-matching probability for all words in an input sequence with so-called prototypes in order to predict a label for the input. Furthermore, for training, PARCUS makes use of rationales. Those are indicators of input importance, and help to boost the loss for relevant tokens. The main motivation to use PARCUS is that it works better in a low-resource setting than recent state-of-the-art models for the high-resource case. This is due to it having relatively few parameters and to it having a strong inductive bias. However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising. Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW. Another selling point of PARCUS is that it's interpretable. While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have. Overall, the paper seems solid. ========== Update: After reading the other reviews and the responses by the authors, I lowered my score from 6 to 3.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for mentioning the benefits of the proposed model . We now clarify some of the reviewer \u2019 s doubts : * * QUESTION * * BERT-based models in the low-resource case is not very surprising * * ANSWER * * While this result may not look surprising , to the best of our knowledge it was not addressed before for the specific case of BERT . In [ 1 ] , the authors claim that a large pre-trained model can be very helpful in transfer learning scenarios , and they also suggest how to best fine-tune BERT . We followed their guidelines and included the results to provide convincing evidence that , in this extreme scenario , our model can perform better ( even without the use of rationales ) . * * QUESTION * * Looking at the experiments , the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW . * * ANSWER * * The reviewer is correct . Apart from the reasons mentioned in the paper , it is possible to observe ( by manual inspection ) that the tweets of the HATESPEECH dataset are very noisy , short and often similar in meaning . This clearly helps models based on n-gram features , as we have argued in our work . SPOUSE and MOVIEREVIEW are different in this sense . SPOUSE , which is where PARCUS performs very well , contains sentences of very different nature and context , which makes it very important to focus on specific concepts ( hence the use of prototypes seems appropriate ) . MOVIEREVIEW , on the other hand , contains very long reviews that need \u201c filtering \u201d to highlight the important concepts . This is another context in which PARCUS can be successfully applied , as training a complex model on few data points that contain \u201c lengthy \u201d sentences can be a hard task to solve ."}, {"review_id": "SJlNnhVYDr-2", "review_text": "This paper considers the problem of text classification, especially the settings in which the number of labeled sentences is very small. However, authors assume, annotations of rationales behind the label, i.e. highlighting tokens in a sentence which are important in deciding its label. As per my understanding, this is a big limitation. Second, the proposed model makes inference of class labels just based upon occurrence of words in a sentence, rather than making more sophisticated inferences relying upon sub-sequence patterns at least. The idea proposed in the paper is to learn prototype vectors which have high similarity w.r.t. tokens in sentences, especially the highlighted one. I didn't understand the justification for learning such prototypes in the first place. This works build upon a workshop paper. The idea proposed in the paper, even in the specific problem context considered, are incremental. I don't think that this kind of work aligns with the theme of learning representations. This paper may be suitable for publication in an NLP workshop as the baseline model. ", "rating": "1: Reject", "reply_text": "We thank the reviewer for writing this review . Although we appreciated the interesting insight that has been expressed in the reviewer \u2019 s comment ( about the extension of our model to sub-sequence patterns ) , we do respectfully disagree with many of the points that have been raised . In particular , we found that some of the comments are quite difficult to address given that their nature is not particularly constructive . Some of the questions posed by the reviewer are difficult to contextualize ; in particular , it looks like the reviewer might have misunderstood the assumptions and the advantages of the proposed work , despite the reviewer confidence about this review . The reviewer has never mentioned that this paper lacks clarity of presentation , but all the answers to his/her questions have already been clearly stated and contextualized in the paper . Moreover , some of the reviewer \u2019 s claims about the value of this work are not backed up by evidence or detailed motivations . As we also state below , we think this work should be judged in terms of how it exploits the additional rationales , rather than criticizing their presence in the first place . Nonetheless , in Section 4.2 we also show ( via ablation studies ) how our model performs well even without the use of rationales . In light of this premise and of the following comments ( which are meant to further clarify our main research questions ) we kindly ask the reviewer to reconsider his/her judgment ."}], "0": {"review_id": "SJlNnhVYDr-0", "review_text": "After responses: I read the authors response and decided to stick to my original score mostly because: 1 - I understand that interpretability is hard to define. I also agree with the authors response. However, this is still not reflected in the paper in any way. I expect a discussion on what is the relevant definition used in the paper and how does it fit to that definition. Currently, it is very confusing to the reader. 2 - I understand the authors' response that few-shot learning is a different empirical setting. However, authors also agree that settings are some-what relevant. I really do not see any gain by NOT discussing the few-shot learning literature. At the end, a reader is interested in this work if they have limited data. Moreover, other ways to address limited data issue should be discussed. ----- The manuscript is proposing a few-shot classification setting in which training set includes only few examples. The main contribution is using prototype embeddings and representing each word as cosine distances to these prototype embeddings. Moreover, the final classification is weighted summation of the per-token decisions followed by a soft-max. Per-token classifiers are obtained with an MLP using the cosine distances as features. When the relevance labels are available, they are used in training to boost gradients. PRO(s) The proposed method is interesting and addressing an important problem. There are many few-shot scenarios and finding good models for them is impactful. The results are promising and the proposed method is more interpretable than the existing NLP classifiers. I disagree with the claim that the model is interpretable. However, I appreciate the effort to interpret the model. CON(s) The model is not interpretable because 1) it starts with embeddings and they are not interpretable, 2) model is full of non-linearities and decision boundaries are not possible to find. In other words, it is not possible to answer \"what would make this model predict some other classifier\". The authors should discuss the existing few-shot learning mechanisms. Especially, \"Prototypical Networks for Few-shot Learning\" is very relevant. I also think it can be included as a baseline with very minimal modifications. The writing is not complete. The authors do not even discuss how the prototypes are learned. I am assuming it is done using full gradient-descent over all parameters. However, this is not clearly discussed. Implementation details should be discussed more clearly. SUMMARY I believe the manuscript is definitely interesting and has a potential. In the mean time, It is not ready for publication. It needs a through review of few-shot learning. Authors should also discuss can any of the few-shot learning methods be included in the experimental study. If the answer is yes, it should be. If the answer is no, it should be explained clearly. Although my recommendation is weak-reject, I am happy to bump it up if these issues are addressed.", "rating": "3: Weak Reject", "reply_text": "We would like to thank the reviewer for his/her constructive comments . We provide an answer to each point raised below . * * QUESTION * * The model is not interpretable because 1 ) it starts with embeddings and they are not interpretable , 2 ) model is full of non-linearities and decision boundaries are not possible to find . In other words , it is not possible to answer `` what would make this model predict some other classifier '' . * * ANSWER * * We thank the reviewer for letting us clarify this point . It is common knowledge that it is very tricky to define what it means for a model to be interpretable [ 1 ] . We do agree with the reviewer that we can not interpret the given embedding space and that it is difficult to derive decision boundaries for the model ; however , when using the term \u201c interpretable \u201d , we followed the definition of Interpretability given in [ 2 ] , i.e. , \u201c to which extent the model and/or its predictions are human understandable \u201d . We argue to have provided a model that is interpretable according to this definition . When a sentence is classified , we are able to inspect the individual contribution of each token to the final classification score , and by design the model is forced to assign high scores to relevant tokens , as shown in Figure 2 , page 8 . Moreover , we can understand which prototypes/logical features led to such an important individual score . In turn , this allows us to understand which prototypes were most relevant , and we are able to match them with semantic concepts associated with pre-trained embeddings , using the mechanism discussed in Section 4.3 . In summary , we are able to give a human-readable suggestion on what semantic concepts are most important in the sentence and how they combine to give the final answer . This allows us to \u201c bypass \u201d the barrier imposed by the presence of non-linearities in the architecture . Nonetheless , we kindly ask the reviewer for further suggestions on how he/she thinks we should rephrase this concept in the paper if needed . For the time being , we have added a reference to [ 2 ] in our paper to clarify what we mean with the term \u201c interpretable \u201d . * * QUESTION * * The authors should discuss the existing few-shot learning mechanisms . Especially , `` Prototypical Networks for Few-shot Learning '' is very relevant . I also think it can be included as a baseline with very minimal modifications . * * ANSWER * * We tend to disagree with the reviewer on this matter . Despite similar , we think our experimental setting significantly differs from the few-shot scenario , in which ( as also written in [ 3 ] and [ 4 ] ) a classifier must generalize to new classes not seen in the training set . In our scenario , we do not have unseen classes . Moreover , in order to exploit rationales , it is necessary to see the corresponding class . These are the main reasons why we decided not to include few-shot learning mechanisms in the related works in the first place . We believe these two application scenarios have to be clearly separated from each other . However , we thank the reviewer for her/his comment as it might have shed light on a future work in which we modify PARCUS to fit the few-shot or zero-shot setting , and we hope our explanation satisfies the reviewer \u2019 s concern . * * QUESTION * * The writing is not complete . The authors do not even discuss how the prototypes are learned . I am assuming it is done using full gradient-descent over all parameters . However , this is not clearly discussed . Implementation details should be discussed more clearly . * * ANSWER * * We thank the reviewer for highlighting this point . We addressed this point in Section 3 ."}, "1": {"review_id": "SJlNnhVYDr-1", "review_text": "The authors propose PARCUS (\"Pattern Representations on Continuous Spaces\"), a model which computes a soft-matching probability for all words in an input sequence with so-called prototypes in order to predict a label for the input. Furthermore, for training, PARCUS makes use of rationales. Those are indicators of input importance, and help to boost the loss for relevant tokens. The main motivation to use PARCUS is that it works better in a low-resource setting than recent state-of-the-art models for the high-resource case. This is due to it having relatively few parameters and to it having a strong inductive bias. However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising. Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW. Another selling point of PARCUS is that it's interpretable. While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have. Overall, the paper seems solid. ========== Update: After reading the other reviews and the responses by the authors, I lowered my score from 6 to 3.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for mentioning the benefits of the proposed model . We now clarify some of the reviewer \u2019 s doubts : * * QUESTION * * BERT-based models in the low-resource case is not very surprising * * ANSWER * * While this result may not look surprising , to the best of our knowledge it was not addressed before for the specific case of BERT . In [ 1 ] , the authors claim that a large pre-trained model can be very helpful in transfer learning scenarios , and they also suggest how to best fine-tune BERT . We followed their guidelines and included the results to provide convincing evidence that , in this extreme scenario , our model can perform better ( even without the use of rationales ) . * * QUESTION * * Looking at the experiments , the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW . * * ANSWER * * The reviewer is correct . Apart from the reasons mentioned in the paper , it is possible to observe ( by manual inspection ) that the tweets of the HATESPEECH dataset are very noisy , short and often similar in meaning . This clearly helps models based on n-gram features , as we have argued in our work . SPOUSE and MOVIEREVIEW are different in this sense . SPOUSE , which is where PARCUS performs very well , contains sentences of very different nature and context , which makes it very important to focus on specific concepts ( hence the use of prototypes seems appropriate ) . MOVIEREVIEW , on the other hand , contains very long reviews that need \u201c filtering \u201d to highlight the important concepts . This is another context in which PARCUS can be successfully applied , as training a complex model on few data points that contain \u201c lengthy \u201d sentences can be a hard task to solve ."}, "2": {"review_id": "SJlNnhVYDr-2", "review_text": "This paper considers the problem of text classification, especially the settings in which the number of labeled sentences is very small. However, authors assume, annotations of rationales behind the label, i.e. highlighting tokens in a sentence which are important in deciding its label. As per my understanding, this is a big limitation. Second, the proposed model makes inference of class labels just based upon occurrence of words in a sentence, rather than making more sophisticated inferences relying upon sub-sequence patterns at least. The idea proposed in the paper is to learn prototype vectors which have high similarity w.r.t. tokens in sentences, especially the highlighted one. I didn't understand the justification for learning such prototypes in the first place. This works build upon a workshop paper. The idea proposed in the paper, even in the specific problem context considered, are incremental. I don't think that this kind of work aligns with the theme of learning representations. This paper may be suitable for publication in an NLP workshop as the baseline model. ", "rating": "1: Reject", "reply_text": "We thank the reviewer for writing this review . Although we appreciated the interesting insight that has been expressed in the reviewer \u2019 s comment ( about the extension of our model to sub-sequence patterns ) , we do respectfully disagree with many of the points that have been raised . In particular , we found that some of the comments are quite difficult to address given that their nature is not particularly constructive . Some of the questions posed by the reviewer are difficult to contextualize ; in particular , it looks like the reviewer might have misunderstood the assumptions and the advantages of the proposed work , despite the reviewer confidence about this review . The reviewer has never mentioned that this paper lacks clarity of presentation , but all the answers to his/her questions have already been clearly stated and contextualized in the paper . Moreover , some of the reviewer \u2019 s claims about the value of this work are not backed up by evidence or detailed motivations . As we also state below , we think this work should be judged in terms of how it exploits the additional rationales , rather than criticizing their presence in the first place . Nonetheless , in Section 4.2 we also show ( via ablation studies ) how our model performs well even without the use of rationales . In light of this premise and of the following comments ( which are meant to further clarify our main research questions ) we kindly ask the reviewer to reconsider his/her judgment ."}}