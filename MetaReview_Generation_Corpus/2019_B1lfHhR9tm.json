{"year": "2019", "forum": "B1lfHhR9tm", "title": "The Natural Language Decathlon: Multitask Learning as Question Answering", "decision": "Reject", "meta_review": "This paper presents a new multi-task training and evaluation set up called the Natural Language Decathlon, and evaluates models on it. While this AC is sympathetic to any work which introduces new datasets and evaluation tasks, the reviewers agreed amongst themselves that the paper is not quite ready for publication. The main concern is that multi-task learning should show benefits of transferring representations or other model components between tasks, demonstrating better generalisation and less task-specific overfitting, but that the results in the paper do not properly show this effect. A more thorough study of which tasks \"interact constructively\" and what model changes can properly exploit this needs to be done. With this further work, the AC has no doubt that this dataset and task suite, and associated models, will be very valuable to the NLP community.\n\nI should note that there were some issues during the review period which lead to AC-confidential communication between AC and authors, and AC and reviewers, to be leaked to the reviewers. It was due to an OpenReview bug, and no party is at fault. Through private discussion with the interested parties, we were able to resolve this matter, and through careful examination of the discussion, I am satisfied that the reviews and final recommendations of the reviewers were properly argued for and presented in good faith.", "reviews": [{"review_id": "B1lfHhR9tm-0", "review_text": "Update: I've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental set-up and multi-task/single-task differences. Original Review: This paper provides a new framework for multitask learning in nlp by taking advantage of the similarities in 10 common NLP tasks. The modeling is building on pre-existing qa models but has some original aspects that were augmented to accommodate the various tasks. The decaNLP framework could be a useful benchmark for other nlp researchers. Experiments indicate that the multi-task set-up does worse on average than the single-task set-up. I wish there was more analysis on why multi-task setups are helpful in some tasks and not others. With a bit more fine-grained analysis, the experiments and framework in this paper could be very beneficial towards other researchers who want to experiment with multi-task learning or who want to use the decaNLP framework as a benchmark. I also found the adaptation to new tasks and zero-shot experiments very interesting but the set-up was not described very concretely: -in the transfer learning section, I hope the writers will elaborate on whether the performance gain is coming from the model being pretrained on a multi-task objective or if there would still be performance gain by pretraining a model on only one of those tasks. For example, would a model pre-trained solely on IWSLT see the same performance gain when transferred to English->Czech as in Figure 4? Or is it actually the multi-task training that is causing the improvement in transfer learning? -Can you please add more detail about the setup for replacing +/- with happy/angry or supportive/unsupportive? What were the (empirical) results of that experiment? I think the paper doesn\u2019t quite stand on its own without the appendix, which is a major weakness in terms of clarity. The related work, for example, should really be included in the main body of the paper. I also recommend that more of the original insights (such as the experimentation with curriculum learning) should be included in the body of the text to count towards original contributions. As a suggestion, the authors may be able to condense the discussion of the 10 tasks in order to make more room in the main text for a related work section plus more of their motivations and experimental results. If necessary, the main paper *can* exceed 8 pages and still fit ICLR guidelines. Very minor detail: I noticed some inconsistency in the bibliography regarding full names vs. first initials only.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding your point about the gap between single- and multi-task performance , I 'll point you to our response to R3 so that you do n't have redundant reading . Regarding the transfer learning experiments . The performance gain does not come from the multi-task objective , as single-task models would exhibit similar behavior for their respective task . What we find interesting here is that the multi-task model retains transferability to all of the tasks it has been trained on . In this sense , these experiments verify that the representations of the multi-task model are somehow compressing the transferable utility of ten single task models into a single model ( 10x smaller ! ) . For the label replacement on the SST dataset , the empirical results show a minor degradation in performance ( ~1 % , so ~86 vs ~87 according to Table 2 and subsection 4.3 ) . This was a naive replacement mapping all answers that were 'positive ' to 'happy ' and all answers that were 'negative ' to 'angry ' . This shows how the model is learning to capitalize on the common output space ( all of English in GloVe ) to adapt to new labels without any additional training . This is advantageous over models that do not actually generate answer sequences because it allows them to be more robust in intuitive ways . You 're certainly right that the appendix carries a lot of useful information and some of the details about contributions . We had moved the related works to the appendix because that was the only way we found we could sufficiently do justice to the long line of literature in multi-task learning as well as all of the literature for each task , but it does seem we will need to include at least a part of our full related works in the main body . There is quite a bit of material overall , and we thank you for your suggestions about where to cut/condense and how to prioritize information . Thank you again for your questions and your feedback about organization ."}, {"review_id": "B1lfHhR9tm-1", "review_text": "The paper formulates several different NLP problems as Q&A problem and proposed a general deep learning architecture. All these tasks are trained together. If the goal is to achieve general AI, the paper gives a good starting point. One technical novelty is the deep learning architecture for this general Q&A problem including the multi-pointer-generator. The paper presents an example of how to do a multi-task learning for 10 different tasks. It raises a very challenging problem or in some way release a new dataset. If our goal is to optimize a single task, the usefulness of the method proposed by the paper is questionable. As we know, multi-task learning works well if some important knowledge shared by different tasks can be learned and leveraged. From table 2, we see for many problems, the results of the single task training are better than the multi-task training, meaning that other tasks can't really help at least under this framework. This makes me doubt if this multi-task learning is useful if our goal is to optimize the performance of a single task. This general model also sacrifices some important prior knowledge of an individual task. For example, for the Squad, the prior that the answer is a continuous span. Ideally, the prior knowledge should be leveraged. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , thank you for your review . You touch upon a crucial point that does require clarification : the gap between the single- and multi-task performance . As you mentioned , the multi-task learning literature has taught us at least one thing : related tasks tend to help each other , and unrelated tasks tend to interfere with each other . The latter is an interesting phenomenon , and it is what we see as the primary multi-task learning problem of concern in this paper , and we are proposing decaNLP as a benchmark for measuring progress on this problem . There are two ways in which unrelated tasks tend to interfere . The first is during the modeling phase where some tasks prevent us from using priors ( like span prediction for QA or a German-only output vocabulary ) that would be useful for some tasks . The second is during the training phase where some tasks tend to interfere with representation learning . These two kinds of interference lead to two kinds of gaps that we measure with this benchmark . The first is the gap between the current best decaNLP model ( in the single- and multi-task settings ) and a combination of state-of-the-art models for each task . The second is the gap between the best decaNLP model in the multi-task setting and a combination of ten of those best decaNLP models each trained for a single task . The concrete contributions of this paper are 1 ) the preparation of benchmark along with reasonable sequence-to-sequence baselines , 2 ) progress on the first kind of gap by switching from seq2seq to multi-sequence-to-sequence with MQAN ( by transforming problems into QA triplets ) , and 3 ) progress on the second kind of gap by demonstrating the superiority of anti-curriculum learning ( or pre-training on harder tasks ) over the baseline fully joint training . 3 ) actually ties multi-task learning back to transfer learning as an effective means of representation learning . But yes , we have not yet entirely closed these gaps ; as you mentioned , that is a key part of the challenge to the community . We have chosen to introduce this challenge now because we believe solutions to this problems are within reach in the near future if the community focuses on them . And yes , though this approach will likely be successful whenever tasks are related ( just based on what we know from the rest of the multi-task learning literature ) , it is sometimes not yet the best way to optimize for single-task performance . Keep in mind though that it did lead to new state-of-the-art results on WikiSQL despite no direct modeling or tuning for that task . Thanks again for your time ."}, {"review_id": "B1lfHhR9tm-2", "review_text": "I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it. I have three major complaints with this paper: 1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering. Question answering is not a unified phenomenon. There is no such thing as \"general question answering\", not even for humans. Consider \"What is 2 + 3?\", \"What's the terminal velocity of a rain drop?\", and \"What is the meaning of life?\" All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems. Question answering is a _format_ for studying particular phenomena. Sometimes it is useful to pose a task as QA, and sometimes it is not. QA is not a useful format for studying problems when you only have a single question (like \"what is the sentiment?\" or \"what is the translation?\"), and there is no hope of transfer from a related task. Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems. We have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks). I don't see any compelling justification for setting things up this way. 2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering. There is nothing new in the transfer results that were presented here, however. For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first). For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly). For the Czech task, fine tuning a pre-trained model has already been shown to help. Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before. The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label. It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case. 3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice. Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious. Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior. The three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results. Any of these three could have been a single conference paper, had it been done well. As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point. Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?). For MQAN, there's more than a page of the core new architecture that's pushed into the appendix. And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere.", "rating": "3: Clear rejection", "reply_text": "Yes , I understand that your intent was probably not rudeness . I did n't think it was my place to publicly comment on your writing compared to say , R1 , who makes nearly all the same criticisms and gives an equally low score without using terms that are condescending like 'misguided ' . I did not post this publicly because I am both an author and a reviewer , and I understand that I am biased towards reading this review as more negative than it should be read . That is why I posted this to ACs and Higher so that they could evaluate . For some reason , the system must have some unintuitive behavior ( too me at least ) that sends you an email for comments on your posts regardless of the chosen visibility . Not sure what happened since the original post is still visible to me and was not deleted . Now that you 've posted it to Everyone , I might as well clarify . As a reviewer , my criticism of this review has nothing to do with QA or the paper itself . Title and 1 ) seem to be written too combatively ( perhaps to use this platform to balance out `` very prominent , public voice [ s ] advocating for it '' ? ) . I do n't think this is the place for that . On my view , authors submit for review to get valuable criticism . The reviewer 's ultimate goal should be to tell authors how to improve their research ; it should not be to combat the research agenda . The paper has problems , especially in total content and organization . As mentioned in the post to ACs and Higher , you raise important criticisms in points 2 ) and 3 ) . But , I think Title and 1 ) deviated from what I see as a reviewer 's goal too much . I just think you could have done without 1 ) , and you should also avoid using words like 'misguided ' unless you intend to up the rudeness factor by a few notches . We might just have to agree to disagree here about tone and word choice , maybe even about the goal of reviewing . Now switching back into author mode . 2 ) You 're right that the transfer learning experiments for any one task are not new results . What we find interesting here is that the multi-task model retains transferability to all of the tasks it has been trained on . In this sense , these experiments verify that the representations of the multi-task model are somehow compressing the transferable utility of ten single-task models into a single model ( 10x smaller ) . Regarding your point about the gap between single- and multi-task performance , I 'll point you to our response to R3 so that you do n't have to do redundant reading . Regarding switching classification labels . Yes , this is a rough approximation for something we were trying to study -- whether the model could adapt to new , but related kinds of questions and adapt its output space . Certainly this experimental design has some problems , but we do think it demonstrates the more general capacity of the model to switch output spaces based on the question because the model must realize that even though the context is the same , it must use different output labels based on different questions . 3 ) No objections here . Organizing all this information into a reasonable order is tough , and clearly one big take-away from this reviewing process is to break things down into more conference-sized chunks rather than cram everything into appendices . Definitely do n't put related works in an appendix -- it is disrespectful even if the intentions were good ( more space to expand on it all ) . Final paragraph ) A lot of additional valuable feedback here . This gives a good sense of how we might restructure and support claims with new experiments . Very much appreciated . Overall , thanks for the discussion . Even though I disagreed with your reviewing style for 1 ) , I think you make really good points in the remainder of your review . Thanks for offering so much of your time ."}], "0": {"review_id": "B1lfHhR9tm-0", "review_text": "Update: I've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental set-up and multi-task/single-task differences. Original Review: This paper provides a new framework for multitask learning in nlp by taking advantage of the similarities in 10 common NLP tasks. The modeling is building on pre-existing qa models but has some original aspects that were augmented to accommodate the various tasks. The decaNLP framework could be a useful benchmark for other nlp researchers. Experiments indicate that the multi-task set-up does worse on average than the single-task set-up. I wish there was more analysis on why multi-task setups are helpful in some tasks and not others. With a bit more fine-grained analysis, the experiments and framework in this paper could be very beneficial towards other researchers who want to experiment with multi-task learning or who want to use the decaNLP framework as a benchmark. I also found the adaptation to new tasks and zero-shot experiments very interesting but the set-up was not described very concretely: -in the transfer learning section, I hope the writers will elaborate on whether the performance gain is coming from the model being pretrained on a multi-task objective or if there would still be performance gain by pretraining a model on only one of those tasks. For example, would a model pre-trained solely on IWSLT see the same performance gain when transferred to English->Czech as in Figure 4? Or is it actually the multi-task training that is causing the improvement in transfer learning? -Can you please add more detail about the setup for replacing +/- with happy/angry or supportive/unsupportive? What were the (empirical) results of that experiment? I think the paper doesn\u2019t quite stand on its own without the appendix, which is a major weakness in terms of clarity. The related work, for example, should really be included in the main body of the paper. I also recommend that more of the original insights (such as the experimentation with curriculum learning) should be included in the body of the text to count towards original contributions. As a suggestion, the authors may be able to condense the discussion of the 10 tasks in order to make more room in the main text for a related work section plus more of their motivations and experimental results. If necessary, the main paper *can* exceed 8 pages and still fit ICLR guidelines. Very minor detail: I noticed some inconsistency in the bibliography regarding full names vs. first initials only.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding your point about the gap between single- and multi-task performance , I 'll point you to our response to R3 so that you do n't have redundant reading . Regarding the transfer learning experiments . The performance gain does not come from the multi-task objective , as single-task models would exhibit similar behavior for their respective task . What we find interesting here is that the multi-task model retains transferability to all of the tasks it has been trained on . In this sense , these experiments verify that the representations of the multi-task model are somehow compressing the transferable utility of ten single task models into a single model ( 10x smaller ! ) . For the label replacement on the SST dataset , the empirical results show a minor degradation in performance ( ~1 % , so ~86 vs ~87 according to Table 2 and subsection 4.3 ) . This was a naive replacement mapping all answers that were 'positive ' to 'happy ' and all answers that were 'negative ' to 'angry ' . This shows how the model is learning to capitalize on the common output space ( all of English in GloVe ) to adapt to new labels without any additional training . This is advantageous over models that do not actually generate answer sequences because it allows them to be more robust in intuitive ways . You 're certainly right that the appendix carries a lot of useful information and some of the details about contributions . We had moved the related works to the appendix because that was the only way we found we could sufficiently do justice to the long line of literature in multi-task learning as well as all of the literature for each task , but it does seem we will need to include at least a part of our full related works in the main body . There is quite a bit of material overall , and we thank you for your suggestions about where to cut/condense and how to prioritize information . Thank you again for your questions and your feedback about organization ."}, "1": {"review_id": "B1lfHhR9tm-1", "review_text": "The paper formulates several different NLP problems as Q&A problem and proposed a general deep learning architecture. All these tasks are trained together. If the goal is to achieve general AI, the paper gives a good starting point. One technical novelty is the deep learning architecture for this general Q&A problem including the multi-pointer-generator. The paper presents an example of how to do a multi-task learning for 10 different tasks. It raises a very challenging problem or in some way release a new dataset. If our goal is to optimize a single task, the usefulness of the method proposed by the paper is questionable. As we know, multi-task learning works well if some important knowledge shared by different tasks can be learned and leveraged. From table 2, we see for many problems, the results of the single task training are better than the multi-task training, meaning that other tasks can't really help at least under this framework. This makes me doubt if this multi-task learning is useful if our goal is to optimize the performance of a single task. This general model also sacrifices some important prior knowledge of an individual task. For example, for the Squad, the prior that the answer is a continuous span. Ideally, the prior knowledge should be leveraged. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , thank you for your review . You touch upon a crucial point that does require clarification : the gap between the single- and multi-task performance . As you mentioned , the multi-task learning literature has taught us at least one thing : related tasks tend to help each other , and unrelated tasks tend to interfere with each other . The latter is an interesting phenomenon , and it is what we see as the primary multi-task learning problem of concern in this paper , and we are proposing decaNLP as a benchmark for measuring progress on this problem . There are two ways in which unrelated tasks tend to interfere . The first is during the modeling phase where some tasks prevent us from using priors ( like span prediction for QA or a German-only output vocabulary ) that would be useful for some tasks . The second is during the training phase where some tasks tend to interfere with representation learning . These two kinds of interference lead to two kinds of gaps that we measure with this benchmark . The first is the gap between the current best decaNLP model ( in the single- and multi-task settings ) and a combination of state-of-the-art models for each task . The second is the gap between the best decaNLP model in the multi-task setting and a combination of ten of those best decaNLP models each trained for a single task . The concrete contributions of this paper are 1 ) the preparation of benchmark along with reasonable sequence-to-sequence baselines , 2 ) progress on the first kind of gap by switching from seq2seq to multi-sequence-to-sequence with MQAN ( by transforming problems into QA triplets ) , and 3 ) progress on the second kind of gap by demonstrating the superiority of anti-curriculum learning ( or pre-training on harder tasks ) over the baseline fully joint training . 3 ) actually ties multi-task learning back to transfer learning as an effective means of representation learning . But yes , we have not yet entirely closed these gaps ; as you mentioned , that is a key part of the challenge to the community . We have chosen to introduce this challenge now because we believe solutions to this problems are within reach in the near future if the community focuses on them . And yes , though this approach will likely be successful whenever tasks are related ( just based on what we know from the rest of the multi-task learning literature ) , it is sometimes not yet the best way to optimize for single-task performance . Keep in mind though that it did lead to new state-of-the-art results on WikiSQL despite no direct modeling or tuning for that task . Thanks again for your time ."}, "2": {"review_id": "B1lfHhR9tm-2", "review_text": "I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it. I have three major complaints with this paper: 1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering. Question answering is not a unified phenomenon. There is no such thing as \"general question answering\", not even for humans. Consider \"What is 2 + 3?\", \"What's the terminal velocity of a rain drop?\", and \"What is the meaning of life?\" All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems. Question answering is a _format_ for studying particular phenomena. Sometimes it is useful to pose a task as QA, and sometimes it is not. QA is not a useful format for studying problems when you only have a single question (like \"what is the sentiment?\" or \"what is the translation?\"), and there is no hope of transfer from a related task. Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems. We have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks). I don't see any compelling justification for setting things up this way. 2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering. There is nothing new in the transfer results that were presented here, however. For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first). For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly). For the Czech task, fine tuning a pre-trained model has already been shown to help. Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before. The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label. It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case. 3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice. Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious. Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior. The three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results. Any of these three could have been a single conference paper, had it been done well. As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point. Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?). For MQAN, there's more than a page of the core new architecture that's pushed into the appendix. And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere.", "rating": "3: Clear rejection", "reply_text": "Yes , I understand that your intent was probably not rudeness . I did n't think it was my place to publicly comment on your writing compared to say , R1 , who makes nearly all the same criticisms and gives an equally low score without using terms that are condescending like 'misguided ' . I did not post this publicly because I am both an author and a reviewer , and I understand that I am biased towards reading this review as more negative than it should be read . That is why I posted this to ACs and Higher so that they could evaluate . For some reason , the system must have some unintuitive behavior ( too me at least ) that sends you an email for comments on your posts regardless of the chosen visibility . Not sure what happened since the original post is still visible to me and was not deleted . Now that you 've posted it to Everyone , I might as well clarify . As a reviewer , my criticism of this review has nothing to do with QA or the paper itself . Title and 1 ) seem to be written too combatively ( perhaps to use this platform to balance out `` very prominent , public voice [ s ] advocating for it '' ? ) . I do n't think this is the place for that . On my view , authors submit for review to get valuable criticism . The reviewer 's ultimate goal should be to tell authors how to improve their research ; it should not be to combat the research agenda . The paper has problems , especially in total content and organization . As mentioned in the post to ACs and Higher , you raise important criticisms in points 2 ) and 3 ) . But , I think Title and 1 ) deviated from what I see as a reviewer 's goal too much . I just think you could have done without 1 ) , and you should also avoid using words like 'misguided ' unless you intend to up the rudeness factor by a few notches . We might just have to agree to disagree here about tone and word choice , maybe even about the goal of reviewing . Now switching back into author mode . 2 ) You 're right that the transfer learning experiments for any one task are not new results . What we find interesting here is that the multi-task model retains transferability to all of the tasks it has been trained on . In this sense , these experiments verify that the representations of the multi-task model are somehow compressing the transferable utility of ten single-task models into a single model ( 10x smaller ) . Regarding your point about the gap between single- and multi-task performance , I 'll point you to our response to R3 so that you do n't have to do redundant reading . Regarding switching classification labels . Yes , this is a rough approximation for something we were trying to study -- whether the model could adapt to new , but related kinds of questions and adapt its output space . Certainly this experimental design has some problems , but we do think it demonstrates the more general capacity of the model to switch output spaces based on the question because the model must realize that even though the context is the same , it must use different output labels based on different questions . 3 ) No objections here . Organizing all this information into a reasonable order is tough , and clearly one big take-away from this reviewing process is to break things down into more conference-sized chunks rather than cram everything into appendices . Definitely do n't put related works in an appendix -- it is disrespectful even if the intentions were good ( more space to expand on it all ) . Final paragraph ) A lot of additional valuable feedback here . This gives a good sense of how we might restructure and support claims with new experiments . Very much appreciated . Overall , thanks for the discussion . Even though I disagreed with your reviewing style for 1 ) , I think you make really good points in the remainder of your review . Thanks for offering so much of your time ."}}