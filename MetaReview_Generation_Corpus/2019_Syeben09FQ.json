{"year": "2019", "forum": "Syeben09FQ", "title": "Evaluating GANs via Duality", "decision": "Reject", "meta_review": "All reviewers still argue for rejection for the submitted paper. The AC thinks that this paper should be published at some point, but for now it is a \"revise and resubmit\".", "reviews": [{"review_id": "Syeben09FQ-0", "review_text": "In this paper, the authors proposed the duality gap as the criterion for evaluating the training of GAN. To justify the proposed criterion, the authors designed empirical experiments on both synthetic and real-world datasets to demonstrate the ability of the duality gap for detecting divergence, mode collapse, sample equality, as well as the generalization to other application domains besides image generation. Comparing with the existing criteria, e.g., FID and INC, the duality gap shows better ability and computational efficiency. However, the paper ignores rich literatures in optimization that uses the duality gap as the criterion for characterizing the convergence of algorithms for min-max saddle point problem, e.g., [1]. In fact, in optimization community, using duality gap to screening the convergence on saddle point problem is a common knowledge. [1] even provides the finite-step convergence rate when the saddle point problem is convex-concave. This paper is only introducing that into machine learning community. Therefore, the novelty of the paper seems not enough. Secondly, the duality gap is only able to screen the optimization convergence and the solution quality w.r.t. **the same objective**. It is not valid to compare different GANs with different losses function using the duality gap. Theoretically, for any loss function derived from some divergences, e.g. [2], the global optimal solution can always achieve zero duality gap. In other words, for different GANs, with different objectives, the duality gap cannot distinguish which one is better. In such sense, the title is very misleading. Thirdly, how the evaluate such criterion in practice in GAN scenario is not clearly explained. Considering the neural network parametrization of both the generator and discriminator, the argmax_v M(u, v) and argmin_u M(u, v) is not tractable. Without the optimal solution, what is the meaning of the \"duality gap\" should be explained. What will happen if we only obtain the suboptimal solutions which themselves are model collapsed? Without such discussion in both theoretical and/or empirical aspects, I am not very convincing about the conclusion. Finally, if one follows the Fenchel dual view of GAN in [2, 3], the min-max is the variational form of some divergences, which the GANs are directly optimizing. It is straightforwardly to see the better min-max value is, the smaller divergence between generated samples and ground-truth is, and thus, the better quality of the generator is. The fact that min-max objective is indeed able to characterize the quality of generator is obvious and well-known. Otherwise, there is not need to use such objective in the optimization to train the model. [1] Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. SIAM J. on Optimization, 19(4):1574\u20131609. [2] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014. [3] S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. arXiv:1606.00709, 2016", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful comments . In the following we address their concerns and questions : 1 . \u201c Please justify the novelty and validity \u201d First , we would like to emphasize that the lack of a convergence metric for GANs is an open issue in the community . As discussed in the introduction , the need for such a metric is crucial and affects several important aspects such as : - * Convergence analysis * . Over the past years , GANs have been the subject of intense research in the community , giving rise to a plethora of GAN models as well as training methods . In practice , it has been observed that GANs might not converge under certain settings . While this non-convergence behavior can in practice be visually recognized for some low-dimensional examples ( such as a 2D mixture of Gaussian ) , this is in general more difficult in high-dimensional spaces due to the lack of a convergence metric . This problem is actually often discussed in the litterature , see e.g.Mescheder et al . [ ICML 2018 ] : \u201c Measuring convergence for GANs is hard for high dimensional problems , because we lack a metric that can reliably detect non-convergent behavior . We therefore first examine the behavior [ ... ] on simple 2D examples where we can assess convergence using an estimate of the Wasserstein-1-distance. \u201d [ 1 ] - * Stopping criteria and meaningful curve * . GANs are known to be hard to train in practice [ 2 ] . One of the common challenges practitioners are facing is when to stop training . See for example [ 3 ] : \u201c GAN foundations : cons : Unclear stopping criteria \u201d . In particular , it is well known that the curves of the discriminator and generator losses oscillate and are non-informative as to whether the model is improving or not ( See Fig.12 and 13 ) . This is especially troublesome when a GAN is trained on non-image data in which case one might not be able to use visual inspection or FID/Inception score as a proxy . - * Domain-independent evaluation metric * . Commonly used evaluation metrics such as FID and Inception Score are mainly suitable for natural images as they rely on a pretrained Imagenet classifier . This is also a problem that is commonly discussed in the literature , see e.g . [ 4 ] : \u201c Generative adversarial networks are a promising [ ... ] that has so far been held back by unstable training and by the lack of a proper evaluation metric. \u201d . Instead the metric suggested in the paper is does not require any specific type of data and was for example shown empirically to generalize to cosmological data . Hence , the metric we propose is a more generic tool that can serve as a ) a monitoring tool to help practitioners throughout training , b ) a domain-independent metric that can help spread the use of GANs to non-image domains . The duality gap ( DG ) and the minimax value are natural metrics for this , as they are well known to capture exactly that . As rightfully pointed out by the reviewer , the duality gap is a well-known notion in optimization and our contribution is its introduction as a metric for GANs . An important aspect we discuss in the paper is with regard to an efficient way to estimate the duality gap without slowing down training . Note that although the two metrics may seem \u201c too natural \u201d from an optimization point of view , they are simply * * not * * used in the community , despite the need for them as we discussed earlier . See for example Salimans et al : \u201c Generative adversarial networks lack an objective function , which makes it difficult to compare performance of different models. \u201d [ 4 ] and \u201c GAN optimization challenges : No robust stopping criteria in practice ( unlike likelihood based learning ) \u201d [ 5 ] . In this work , we argue that such a metric does exist and it indeed comes naturally from the objective function . This is also what our experiments demonstrate . 2. \u201c The paper ignores rich literatures in optimization ... \u201d Yes , we do agree , but note that ( to the best of our knowledge ) almost all the existing literature focuses on solving minimax problems with convex-concave objectives and therefore existing proof guarantees do not apply to GANs . Our contribution does not relate to optimising GANs , but instead in showing that the duality gap can be empirically computed and yields good estimates of the convergence of a GAN . We revised the text to clearly emphasize this and also included the suggested reference ."}, {"review_id": "Syeben09FQ-1", "review_text": "This work proposes to use duality gap and minimax loss as measures for monitoring the progress of training GANs. The authors first showed a relationship between duality gap(DG) and Jensen-Shannon divergence and non-negativeness on DG. Then, a comprehensive discussion was presented on how to estimate and efficiently compute DG. A series of experiments were designed on synthetic data and real-world image data to show 1) how duality gap is sensitive to capture non-convergence during training and 2) how minimax loss efficiently reflects the sample quality from generator. I was not very familiar with GANs, thus I'm not sure on the significance of paper and would like to see opinions from other reviews on this. For reviewing this paper, I also read the cited works such as Salimans (2016), Heusel (2017). Compared with them, the theoretical contribution of this work seems less significant. Also, I'm not quite impressed by the advantages of proposed metrics. However, this work is nicely written, the ideas are delivered clearly, experiments are nicely designed. I kind of enjoying reading this paper due to its clarity. Other concerns: There are two D_1 in Equation Mixed Nash equilibrium. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . We appreciate the comments on the nice flow of the paper and the carefully designed experimental section . Your two concerns are ( 1 ) \u201c I was not very familiar with GANs , thus I 'm not sure on the significance of paper \u201d and ( 2 ) \u201c I 'm not quite impressed by the advantages of proposed metrics \u201d , which we address below : 1 . Significance GANs are a 2-player minimax game , which makes their objective function different than the more commonly encountered likelihood optimization problems , thus yielding new challenges in terms of optimization and evaluation . For evaluating likelihood-based models , a common metric to use is the test loss , whereas for minimax problems it is not clear what the equivalent would be [ 1 ] . In particular , with the absence of such a metric , practitioners are facing several problems , such as ( i ) determining whether the model has converged , ( ii ) determining when to stop training ( see Fig.12 and 13 ) , ( iii ) having a meaningful curve throughout training as the discriminator and generator losses are not intuitive , ( iv ) comparing different runs and ( v ) debugging the model in the sense of ( un ) stable mode collapse , non-convergence etc . See for example [ 1 ] : \u201c Generative adversarial networks are not born with a good objection function that can inform us about the training progress . Without a good evaluation metric , it is like working in the dark . No good sign to tell when to stop ; No good indicator to compare the performance of multiple models. \u201d . Current methods for stopping criteria rely on visual inspection and/or using some sample-quality metric as a proxy . However this is not principled and it is unclear how to compute the metrics in non-image domains . Thus another open challenge with GANs is ( vi ) having a useful metric that is domain independent . In this work , we argue that such a natural metric exists , namely the duality gap and its minimax part . We show how to compute it in practice in an efficient way and demonstrate its desirable properties across different GAN pitfalls , domains and GAN objectives . Thus , in our opinion , this work is of significance for the GAN community , not only for practical purposes as it gives a solution to the previously mentioned open problems ( i-vi ) both in theory and practice , but also from research perspective as it gives a reliable non-convergence metric to help analyse which methods actually converge , which is one of the central issues of GANs . Note that current practical analyses mainly focus on 2-dimensional problems where the solution can be visually inspected due to the lack of such a metric [ 2 ] . 2.Advantages of the proposed metrics From a theoretical perspective , the DG is very natural for the detection of non-convergent behaviors , it is always non-negative and is zero if and only if the model has reached a ( Nash ) equilibrium . The experimental results presented in the paper provide a thorough evaluation of the metric introduced in our submission . We included various tests that focus on common pitfalls encountered with GANs and demonstrated that the proposed metric can detect these corner cases . In particular : In experiment 5.1 we demonstrate that the * DG yields a meaningful curve * throughout training and detects convergent and non-convergent behaviours . Please note that the commonly used metrics such as FID and Inception score can not be applied to these datasets . In experiment 5.2 we show that the * DG detects stable mode collapse * and can distinguish between stable and unstable collapses . In experiment 5.3 we empirically demonstrate that the * minimax metric detects visual sample quality ( adding noise , Gaussian swirl and blur ) and is very sensitive to change of modes * ( mode dropping , mode invention and intra-mode collapse ) . It works better than Inception score , and as well as FID . However , both the Inception score and FID rely on a pre-trained Imagenet classifier , whereas our metrics need no labeled data or a pre-trained classifier . Finally , in experiment 5.4 we show the * DG metric can be applied on another GAN minimax formulation ( WGAN ) and on another domain that is not natural images ( cosmology data ) * . We find that the metric is highly correlated with a domain specific measure of performance used in cosmology . Note that the domain-specific metric requires expert knowledge and its computation is very slow , unlike the DG . Furthermore , the Inception score and FID can not be applied on this data as they require an imagenet classifier ( i.e.trained with labeled natural images ) ."}, {"review_id": "Syeben09FQ-2", "review_text": "The focus of the submission is GANs (generative adversarial network), a recent and popular min-max generative modelling approach. Training GANs is considered to be a challenging problem due to the min-max nature of the task. The authors propose two duality-inspired stopping criteria to monitor the efficiency and convergence of GAN learning. Though training GAN can have some useful applications, the contribution of the submission is pretty moderate. i) Duality-inspired approaches, embedded also in optimization have already been proposed: see for example 'Xu Chen, Jiang Wang, Hao Ge. Training Generative Adversarial Networks via Primal-Dual Subgradient Methods: A Lagrangian Perspective on GAN. ICLR-2018.'. ii) The notion of generator and discriminator networks with unbounded capacity (which is an assumption in 'Proposition 1') lacks formal definition. I looked up the cited Goodfellow et al. (2014) work; it similarly does not define the concept. Based on the informal definition it is not clear whether they exist or are computationally tractable. Minor comments: -MMD is a specific instance of integral probability metrics when in the latter the function space is chosen to be the unit ball of a reproducing kernel Hilbert space; they are not synonyms. -mixed Nash equilibrium: E_{v\\sim D_1} should be E_{v\\sim D_2}. -It might be better to call Table 1 as Figure 1. -References: abbreviations and names should be capitalized (e.g., gan, mnist, wasserstein, nash, cifar). Lucic et al. (2017) has been accepted to NIPS-2018.", "rating": "3: Clear rejection", "reply_text": "Thank you for reviewing our paper . Please find our replies inline : 1 . \u201c Though training GAN can have some useful applications , the contribution of the submission is pretty moderate. \u201d First , we would like to stress that the contribution of our paper is not to train GANs . Instead , our contribution is to propose a reliable convergence metric for GANs that can be computed efficiently . The need for such a convergence metric has been pointed out for example in the context of analysing convergence of GANs [ 1 ] and understanding when to stop the GAN training [ 2 , 3 ] ( see also Fig.12 and 13 ) . Indeed , most empirical convergence analyses are for 2-dimensional problems due to the lack of such metric ( See for example Mescheder et al . [ ICML 2018 ] : \u201c Measuring convergence for GANs is hard for high dimensional problems , because we lack a metric that can reliably detect non-convergent behavior . We therefore first examine the behavior [ ... ] on simple 2D examples where we can assess convergence using an estimate of the Wasserstein-1-distance. \u201d [ 1 ] ) . Furthermore , since GANs are framed as a 2-player minimax game the stopping criteria is unclear in comparison to the more traditional likelihood training [ 2 ] . In this work we argue that there is a convergence metric suitable for the general GAN game . In particular , our main contributions are : - Proposing the duality gap as a natural convergence metric and the minimax metric as a performance metric in GANs - Show how an unbiased estimate of the metrics can be efficiently computed in practice without slowing down training - Design experiments that target all of the common pitfalls of GANs ( stable and unstable mode dropping/invention , intra-mode collapse , non-image domain , distortion of visual quality etc . ) and demonstrate empirically that the metrics are able to capture and detect all of those Thus the two metrics show very desirable properties both in theory and practice . We believe that the DG metric is very helpful as a monitoring tool for any practitioner training a GAN . The benefits are : ( i ) knowing whether the model has converged ; ( ii ) knowing when to stop training ; ( iii ) have a meaningful curve throughout training that reflects the performance of the model ( i.e.whether it \u2019 s improving or not ) ; ( iv ) comparison of different runs and hyperparameter searches and ( v ) debugging . As computing the metric is very efficient in practice this comes at no significant computational cost , and unlike other metrics requires no labels or a pre-trained classifier and can be applied to any minimax GAN formulation and any domain as demonstrated empirically . Further , it allows for pushing the research of the non-convergence issue on GANs on problems that are beyond 2-dimensional where they can visually be analysed . We have updated the write-up to make our contribution clearer , both with respect to existing work , as well as the importance of a convergence metric for the community . 2. \u201c Duality-inspired approaches , embedded also in optimization have already been proposed \u201d Although the reference cited by the reviewer does discuss duality for GANs , it does so in a very different context since it discusses a Lagrangian view to train GANs while we are interested in using the duality GAP as a convergence measure ( and not as a training criterion ) . One problem we focus on in our submission is to demonstrate how to efficiently compute such measure during training , we therefore do not modify the training objective . We added a brief discussion in the related work section . 3. \u201c The notion of generator and discriminator networks with unbounded capacity ( which is an assumption in 'Proposition 1 ' ) lacks formal definition \u201d As noted by the reviewer , we re-used the notion originally introduced in the GAN paper . Informally , we consider the capacity as the flexibility of a model to learn a variety of functions . More formally , we regard the capacity as the size of the space that can be approximated with the generator and discriminator . In most cases , neural networks are universal approximators and can therefore approximate any function ( i.e.they are dense in the target space ) , thus leading us to assume they have \u201c unbounded capacity \u201d . We hope that we have cleared out any confusion and are looking forward to the reviewer \u2019 s reply . References : [ 1 ] Mescheder et al.Which Training Methods for GANs fo actually converge ? [ ICML 2018 ] arXiv:1801.04406 [ 2 ] Chiu et al.GAN Foundations , [ CSC254 , University of Toronto ] , < https : //www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/gan-foundations.pdf # page=9 > [ 3 ] Ermon et al.Generative Adversarial Networks , [ cs236 , Stanford ] , < http : //cs236.stanford.edu/assets/slides/cs236_lecture9.pdf # page=19 >"}], "0": {"review_id": "Syeben09FQ-0", "review_text": "In this paper, the authors proposed the duality gap as the criterion for evaluating the training of GAN. To justify the proposed criterion, the authors designed empirical experiments on both synthetic and real-world datasets to demonstrate the ability of the duality gap for detecting divergence, mode collapse, sample equality, as well as the generalization to other application domains besides image generation. Comparing with the existing criteria, e.g., FID and INC, the duality gap shows better ability and computational efficiency. However, the paper ignores rich literatures in optimization that uses the duality gap as the criterion for characterizing the convergence of algorithms for min-max saddle point problem, e.g., [1]. In fact, in optimization community, using duality gap to screening the convergence on saddle point problem is a common knowledge. [1] even provides the finite-step convergence rate when the saddle point problem is convex-concave. This paper is only introducing that into machine learning community. Therefore, the novelty of the paper seems not enough. Secondly, the duality gap is only able to screen the optimization convergence and the solution quality w.r.t. **the same objective**. It is not valid to compare different GANs with different losses function using the duality gap. Theoretically, for any loss function derived from some divergences, e.g. [2], the global optimal solution can always achieve zero duality gap. In other words, for different GANs, with different objectives, the duality gap cannot distinguish which one is better. In such sense, the title is very misleading. Thirdly, how the evaluate such criterion in practice in GAN scenario is not clearly explained. Considering the neural network parametrization of both the generator and discriminator, the argmax_v M(u, v) and argmin_u M(u, v) is not tractable. Without the optimal solution, what is the meaning of the \"duality gap\" should be explained. What will happen if we only obtain the suboptimal solutions which themselves are model collapsed? Without such discussion in both theoretical and/or empirical aspects, I am not very convincing about the conclusion. Finally, if one follows the Fenchel dual view of GAN in [2, 3], the min-max is the variational form of some divergences, which the GANs are directly optimizing. It is straightforwardly to see the better min-max value is, the smaller divergence between generated samples and ground-truth is, and thus, the better quality of the generator is. The fact that min-max objective is indeed able to characterize the quality of generator is obvious and well-known. Otherwise, there is not need to use such objective in the optimization to train the model. [1] Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. SIAM J. on Optimization, 19(4):1574\u20131609. [2] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014. [3] S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. arXiv:1606.00709, 2016", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful comments . In the following we address their concerns and questions : 1 . \u201c Please justify the novelty and validity \u201d First , we would like to emphasize that the lack of a convergence metric for GANs is an open issue in the community . As discussed in the introduction , the need for such a metric is crucial and affects several important aspects such as : - * Convergence analysis * . Over the past years , GANs have been the subject of intense research in the community , giving rise to a plethora of GAN models as well as training methods . In practice , it has been observed that GANs might not converge under certain settings . While this non-convergence behavior can in practice be visually recognized for some low-dimensional examples ( such as a 2D mixture of Gaussian ) , this is in general more difficult in high-dimensional spaces due to the lack of a convergence metric . This problem is actually often discussed in the litterature , see e.g.Mescheder et al . [ ICML 2018 ] : \u201c Measuring convergence for GANs is hard for high dimensional problems , because we lack a metric that can reliably detect non-convergent behavior . We therefore first examine the behavior [ ... ] on simple 2D examples where we can assess convergence using an estimate of the Wasserstein-1-distance. \u201d [ 1 ] - * Stopping criteria and meaningful curve * . GANs are known to be hard to train in practice [ 2 ] . One of the common challenges practitioners are facing is when to stop training . See for example [ 3 ] : \u201c GAN foundations : cons : Unclear stopping criteria \u201d . In particular , it is well known that the curves of the discriminator and generator losses oscillate and are non-informative as to whether the model is improving or not ( See Fig.12 and 13 ) . This is especially troublesome when a GAN is trained on non-image data in which case one might not be able to use visual inspection or FID/Inception score as a proxy . - * Domain-independent evaluation metric * . Commonly used evaluation metrics such as FID and Inception Score are mainly suitable for natural images as they rely on a pretrained Imagenet classifier . This is also a problem that is commonly discussed in the literature , see e.g . [ 4 ] : \u201c Generative adversarial networks are a promising [ ... ] that has so far been held back by unstable training and by the lack of a proper evaluation metric. \u201d . Instead the metric suggested in the paper is does not require any specific type of data and was for example shown empirically to generalize to cosmological data . Hence , the metric we propose is a more generic tool that can serve as a ) a monitoring tool to help practitioners throughout training , b ) a domain-independent metric that can help spread the use of GANs to non-image domains . The duality gap ( DG ) and the minimax value are natural metrics for this , as they are well known to capture exactly that . As rightfully pointed out by the reviewer , the duality gap is a well-known notion in optimization and our contribution is its introduction as a metric for GANs . An important aspect we discuss in the paper is with regard to an efficient way to estimate the duality gap without slowing down training . Note that although the two metrics may seem \u201c too natural \u201d from an optimization point of view , they are simply * * not * * used in the community , despite the need for them as we discussed earlier . See for example Salimans et al : \u201c Generative adversarial networks lack an objective function , which makes it difficult to compare performance of different models. \u201d [ 4 ] and \u201c GAN optimization challenges : No robust stopping criteria in practice ( unlike likelihood based learning ) \u201d [ 5 ] . In this work , we argue that such a metric does exist and it indeed comes naturally from the objective function . This is also what our experiments demonstrate . 2. \u201c The paper ignores rich literatures in optimization ... \u201d Yes , we do agree , but note that ( to the best of our knowledge ) almost all the existing literature focuses on solving minimax problems with convex-concave objectives and therefore existing proof guarantees do not apply to GANs . Our contribution does not relate to optimising GANs , but instead in showing that the duality gap can be empirically computed and yields good estimates of the convergence of a GAN . We revised the text to clearly emphasize this and also included the suggested reference ."}, "1": {"review_id": "Syeben09FQ-1", "review_text": "This work proposes to use duality gap and minimax loss as measures for monitoring the progress of training GANs. The authors first showed a relationship between duality gap(DG) and Jensen-Shannon divergence and non-negativeness on DG. Then, a comprehensive discussion was presented on how to estimate and efficiently compute DG. A series of experiments were designed on synthetic data and real-world image data to show 1) how duality gap is sensitive to capture non-convergence during training and 2) how minimax loss efficiently reflects the sample quality from generator. I was not very familiar with GANs, thus I'm not sure on the significance of paper and would like to see opinions from other reviews on this. For reviewing this paper, I also read the cited works such as Salimans (2016), Heusel (2017). Compared with them, the theoretical contribution of this work seems less significant. Also, I'm not quite impressed by the advantages of proposed metrics. However, this work is nicely written, the ideas are delivered clearly, experiments are nicely designed. I kind of enjoying reading this paper due to its clarity. Other concerns: There are two D_1 in Equation Mixed Nash equilibrium. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . We appreciate the comments on the nice flow of the paper and the carefully designed experimental section . Your two concerns are ( 1 ) \u201c I was not very familiar with GANs , thus I 'm not sure on the significance of paper \u201d and ( 2 ) \u201c I 'm not quite impressed by the advantages of proposed metrics \u201d , which we address below : 1 . Significance GANs are a 2-player minimax game , which makes their objective function different than the more commonly encountered likelihood optimization problems , thus yielding new challenges in terms of optimization and evaluation . For evaluating likelihood-based models , a common metric to use is the test loss , whereas for minimax problems it is not clear what the equivalent would be [ 1 ] . In particular , with the absence of such a metric , practitioners are facing several problems , such as ( i ) determining whether the model has converged , ( ii ) determining when to stop training ( see Fig.12 and 13 ) , ( iii ) having a meaningful curve throughout training as the discriminator and generator losses are not intuitive , ( iv ) comparing different runs and ( v ) debugging the model in the sense of ( un ) stable mode collapse , non-convergence etc . See for example [ 1 ] : \u201c Generative adversarial networks are not born with a good objection function that can inform us about the training progress . Without a good evaluation metric , it is like working in the dark . No good sign to tell when to stop ; No good indicator to compare the performance of multiple models. \u201d . Current methods for stopping criteria rely on visual inspection and/or using some sample-quality metric as a proxy . However this is not principled and it is unclear how to compute the metrics in non-image domains . Thus another open challenge with GANs is ( vi ) having a useful metric that is domain independent . In this work , we argue that such a natural metric exists , namely the duality gap and its minimax part . We show how to compute it in practice in an efficient way and demonstrate its desirable properties across different GAN pitfalls , domains and GAN objectives . Thus , in our opinion , this work is of significance for the GAN community , not only for practical purposes as it gives a solution to the previously mentioned open problems ( i-vi ) both in theory and practice , but also from research perspective as it gives a reliable non-convergence metric to help analyse which methods actually converge , which is one of the central issues of GANs . Note that current practical analyses mainly focus on 2-dimensional problems where the solution can be visually inspected due to the lack of such a metric [ 2 ] . 2.Advantages of the proposed metrics From a theoretical perspective , the DG is very natural for the detection of non-convergent behaviors , it is always non-negative and is zero if and only if the model has reached a ( Nash ) equilibrium . The experimental results presented in the paper provide a thorough evaluation of the metric introduced in our submission . We included various tests that focus on common pitfalls encountered with GANs and demonstrated that the proposed metric can detect these corner cases . In particular : In experiment 5.1 we demonstrate that the * DG yields a meaningful curve * throughout training and detects convergent and non-convergent behaviours . Please note that the commonly used metrics such as FID and Inception score can not be applied to these datasets . In experiment 5.2 we show that the * DG detects stable mode collapse * and can distinguish between stable and unstable collapses . In experiment 5.3 we empirically demonstrate that the * minimax metric detects visual sample quality ( adding noise , Gaussian swirl and blur ) and is very sensitive to change of modes * ( mode dropping , mode invention and intra-mode collapse ) . It works better than Inception score , and as well as FID . However , both the Inception score and FID rely on a pre-trained Imagenet classifier , whereas our metrics need no labeled data or a pre-trained classifier . Finally , in experiment 5.4 we show the * DG metric can be applied on another GAN minimax formulation ( WGAN ) and on another domain that is not natural images ( cosmology data ) * . We find that the metric is highly correlated with a domain specific measure of performance used in cosmology . Note that the domain-specific metric requires expert knowledge and its computation is very slow , unlike the DG . Furthermore , the Inception score and FID can not be applied on this data as they require an imagenet classifier ( i.e.trained with labeled natural images ) ."}, "2": {"review_id": "Syeben09FQ-2", "review_text": "The focus of the submission is GANs (generative adversarial network), a recent and popular min-max generative modelling approach. Training GANs is considered to be a challenging problem due to the min-max nature of the task. The authors propose two duality-inspired stopping criteria to monitor the efficiency and convergence of GAN learning. Though training GAN can have some useful applications, the contribution of the submission is pretty moderate. i) Duality-inspired approaches, embedded also in optimization have already been proposed: see for example 'Xu Chen, Jiang Wang, Hao Ge. Training Generative Adversarial Networks via Primal-Dual Subgradient Methods: A Lagrangian Perspective on GAN. ICLR-2018.'. ii) The notion of generator and discriminator networks with unbounded capacity (which is an assumption in 'Proposition 1') lacks formal definition. I looked up the cited Goodfellow et al. (2014) work; it similarly does not define the concept. Based on the informal definition it is not clear whether they exist or are computationally tractable. Minor comments: -MMD is a specific instance of integral probability metrics when in the latter the function space is chosen to be the unit ball of a reproducing kernel Hilbert space; they are not synonyms. -mixed Nash equilibrium: E_{v\\sim D_1} should be E_{v\\sim D_2}. -It might be better to call Table 1 as Figure 1. -References: abbreviations and names should be capitalized (e.g., gan, mnist, wasserstein, nash, cifar). Lucic et al. (2017) has been accepted to NIPS-2018.", "rating": "3: Clear rejection", "reply_text": "Thank you for reviewing our paper . Please find our replies inline : 1 . \u201c Though training GAN can have some useful applications , the contribution of the submission is pretty moderate. \u201d First , we would like to stress that the contribution of our paper is not to train GANs . Instead , our contribution is to propose a reliable convergence metric for GANs that can be computed efficiently . The need for such a convergence metric has been pointed out for example in the context of analysing convergence of GANs [ 1 ] and understanding when to stop the GAN training [ 2 , 3 ] ( see also Fig.12 and 13 ) . Indeed , most empirical convergence analyses are for 2-dimensional problems due to the lack of such metric ( See for example Mescheder et al . [ ICML 2018 ] : \u201c Measuring convergence for GANs is hard for high dimensional problems , because we lack a metric that can reliably detect non-convergent behavior . We therefore first examine the behavior [ ... ] on simple 2D examples where we can assess convergence using an estimate of the Wasserstein-1-distance. \u201d [ 1 ] ) . Furthermore , since GANs are framed as a 2-player minimax game the stopping criteria is unclear in comparison to the more traditional likelihood training [ 2 ] . In this work we argue that there is a convergence metric suitable for the general GAN game . In particular , our main contributions are : - Proposing the duality gap as a natural convergence metric and the minimax metric as a performance metric in GANs - Show how an unbiased estimate of the metrics can be efficiently computed in practice without slowing down training - Design experiments that target all of the common pitfalls of GANs ( stable and unstable mode dropping/invention , intra-mode collapse , non-image domain , distortion of visual quality etc . ) and demonstrate empirically that the metrics are able to capture and detect all of those Thus the two metrics show very desirable properties both in theory and practice . We believe that the DG metric is very helpful as a monitoring tool for any practitioner training a GAN . The benefits are : ( i ) knowing whether the model has converged ; ( ii ) knowing when to stop training ; ( iii ) have a meaningful curve throughout training that reflects the performance of the model ( i.e.whether it \u2019 s improving or not ) ; ( iv ) comparison of different runs and hyperparameter searches and ( v ) debugging . As computing the metric is very efficient in practice this comes at no significant computational cost , and unlike other metrics requires no labels or a pre-trained classifier and can be applied to any minimax GAN formulation and any domain as demonstrated empirically . Further , it allows for pushing the research of the non-convergence issue on GANs on problems that are beyond 2-dimensional where they can visually be analysed . We have updated the write-up to make our contribution clearer , both with respect to existing work , as well as the importance of a convergence metric for the community . 2. \u201c Duality-inspired approaches , embedded also in optimization have already been proposed \u201d Although the reference cited by the reviewer does discuss duality for GANs , it does so in a very different context since it discusses a Lagrangian view to train GANs while we are interested in using the duality GAP as a convergence measure ( and not as a training criterion ) . One problem we focus on in our submission is to demonstrate how to efficiently compute such measure during training , we therefore do not modify the training objective . We added a brief discussion in the related work section . 3. \u201c The notion of generator and discriminator networks with unbounded capacity ( which is an assumption in 'Proposition 1 ' ) lacks formal definition \u201d As noted by the reviewer , we re-used the notion originally introduced in the GAN paper . Informally , we consider the capacity as the flexibility of a model to learn a variety of functions . More formally , we regard the capacity as the size of the space that can be approximated with the generator and discriminator . In most cases , neural networks are universal approximators and can therefore approximate any function ( i.e.they are dense in the target space ) , thus leading us to assume they have \u201c unbounded capacity \u201d . We hope that we have cleared out any confusion and are looking forward to the reviewer \u2019 s reply . References : [ 1 ] Mescheder et al.Which Training Methods for GANs fo actually converge ? [ ICML 2018 ] arXiv:1801.04406 [ 2 ] Chiu et al.GAN Foundations , [ CSC254 , University of Toronto ] , < https : //www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/gan-foundations.pdf # page=9 > [ 3 ] Ermon et al.Generative Adversarial Networks , [ cs236 , Stanford ] , < http : //cs236.stanford.edu/assets/slides/cs236_lecture9.pdf # page=19 >"}}