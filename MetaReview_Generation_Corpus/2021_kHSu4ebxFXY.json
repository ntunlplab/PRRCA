{"year": "2021", "forum": "kHSu4ebxFXY", "title": "MARS: Markov Molecular Sampling for Multi-objective Drug Discovery", "decision": "Accept (Spotlight)", "meta_review": "This work proposes a method for generating candidate molecules using a novel fragment-based MCMC proposal mechanism.\n\nPros:\n* Well-written paper\n* Novel idea for an important application\n* Very good empirical performance compared to the state-of-the-art in multi-objective molecule generation\n* Careful ablation studies\n\nCons:\n* Some details were missing (runtime, experimental details) and have been added to the revised version.\n\nThe authors engaged in an extensive discussion with the reviewers and modified their paper to address the reviewer concerns.\n\nAfter discussions three reviewers recommend accepting the work and consider it a novel and useful contribution to the field.\n\nOne reviewer (Reviewer 3) is not satisfied by the authors comments and has concerns about the work regarding: asymptotic correctness of the sampling; fairness of the experimental comparison; and computational complexity.  The authors provide detailed justifications for their choices.  After looking at the discussion there are two factors:\n1. technical arguments regarding the correctness of the sampling method; the authors justify the correctness by known results for adaptive MCMC methods, and the argument is sound, and the area chair fully accepts the authors' arguments as correct and applicable.\n2. extend of the experimental evaluation and suitable baseline methods; this is partially subjective.  The authors provide extensive experiments in their work and justify exclusion of certain methods in that they do not easily apply to the multi-objective setting.  In addition, Reviewer 3 demands a comparison of generated molecules per time, which is plausibly useful, however, none of the prior works have used such a metric in a consistent manner and it is clearly challenging to do so fairly as such metric would depend on specifics of the implementation and computer.  The authors have updated their paper and added runtime information for their method.  The area chair fully accepts the authors' arguments and justification for the current experimental scope.\n\nIn summary the area chair considers the remaining concerns by Reviewer 3 as invalid; in particular, the authors have made extensive efforts to engage and educate the reviewer.", "reviews": [{"review_id": "kHSu4ebxFXY-0", "review_text": "Summary : The paper proposes a sampling-based approach for multiple property optimization in molecule generation . Strength : the sampling approach is an interesting new direction for molecule generation . Also multi-objective molecule generation is an important task . Weakness : I have several concerns regarding this paper . ( 1 ) Section 3.2 is unclear , with lots of details missing . For example , what is the input for MPNN ? i.e. , x in M_\\theta ( x ) . How to determine the supervision signal p_add , p_delete for M_\\theta ( x ) ? When add/deleted fragments , how many fragments are added/deleted in a single transition kernel ? If multiple fragments are added/deleted , are they added/deleted sequentially or following some particular rule ? The MPNN description ( Eq.5 -- 9 ) is borrowed from [ 1 ] , please cite the paper . The paper trains the MPNN during sampling procedure , which seems very challenging , in the first several steps the model is from scratch , how to guarantee the sampled molecule is reasonable ? If initially , the sampled molecules can not provide informative supervision , it will be hard to train a strong MPNN model . Also , the number of training samples also looks very limited . In [ 1 ] , it requires a large amount of existing drug molecules to pretrain a good MPNN model . ( 2 ) Unfair comparison between the proposed sampling model and non-sampling baselines . For example , when sample size N=5000 , the proposed model will query oracle to select a subset of generated molecules . However , baselines such as JTVAE directly output the generated molecules , all of which are kept . Therefore , in performance comparison if you compare the best molecules in 5000 molecules with a single molecule generated by JTVAE , this is unfair . Please clarify . ( 3 ) complexity issue : How many times do the authors need to query the oracles when performing optimization using the proposed sampling algorithm ? How does this compare to the baselines ? Each time you sample a new molecule , you need to evaluate the acceptance rate in Eq.2 , while such an evaluation requires evaluation of target distribution in Eq.1 as well as need to call the oracle ( e.g. , evaluating the property ) , which is very expensive . ( 4 ) Lack of theoretical guarantee . The authors need to show the sampled molecules follow target distribution . The authors also argue they adaptively train the MPNN , so the transition kernel will change during sampling . It is non-trivial to guarantee the convergence to target distribution . ( 5 ) There are also issues with experimental setting and results . For example Important baselines are missing . For example , Graph2Graph [ 2 ] and hierarchical generation [ 3 ] . One baseline RationaleRL outperform the proposed MARS in terms of success rate in many settings . The fairness in model comparison issue I mentioned in ( 2 ) How to use t-SNE to visualize molecule distribution ? It needs more details . [ 1 ] Hu et al , ICLR 2020 , Strategies For Pre-training Graph Neural Networks [ 2 ] Jin et al , ICLR 2019 , Learning Multimodal Graph-to-Graph Translation for Molecule Optimization . [ 3 ] Jin et al , ICML 2020 , Hierarchical Generation of Molecular Graphs using Structural Motifs .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks so much for your comments and constructive suggestions ! We will respond to your concerns on the issues of theoretical guarantees , the comparison fairness , the computation complexity , and some method or experiment details respectively in the following paragraphs . Hope these replies could resolve your concerns , and any further comments are welcome ! -- * * Q : Lack of theoretical guarantee about the training and convergence : 1 ) The paper trains the MPNN during sampling procedure , which seems very challenging ; 2 ) need to show the sampled molecules follow target distribution ; 3 ) It is non-trivial to guarantee the convergence to target distribution . * * A : Thanks very much for your kind reminder ! We have provided some further discussions on the training and convergence issues in Section 3.2 ( Discussion on training and convergence ) . In brief , the sampling process will converge if the adaptive proposal itself converges . And this can be satisfied by employing an adaptive optimizer ( e.g. , Adam optimizer ) to update the model parameters . Also , the stationary distribution of annealed MCMC will converge to the set of global optima if we set the temperature cooling schedule appropriately . > * * Discussion on training and convergence . * * On the one hand , for a MCMC sampling process based on an adaptive proposal to converge , it is required that the proposal $ q_ { \\theta } $ will converge as the sampling time step goes to infinity ( Rosenthal , 2011 ) . We can easily satisfy this requirement by choosing an appropriate training strategy ( e.g. , use an adaptive optimizer like Adam ( Paszke et al. , 2017 ) to update model parameters ) . On the other hand , most convergence results for simulated annealing typically state that , if for a give a temperature $ T_i $ , the homogeneous Markov transition kernel mixes quickly enough , then convergence to the set of global maxima of $ \\pi ( x ) $ is ensured for a temperature sequence $ T_i = ( C \\ln ( i+ T_0 ) ) ^ { -1 } $ , where $ C $ and $ T_0 $ are problem-dependent ( Andrieu et al. , 2003 ) . Therefore , we can elaborately design the cooling strategy of annealing to make a better approximation to the global optima . * * Q : Unfair comparison between the proposed sampling model and non-sampling baselines . * * A : Thanks for raising your concerns , but we think the comparison is fair for the following reasons : 1 . For the comparison fairness in the evaluation , all the baselines have generated 5000 molecules , and the 5000 generated/sampled molecules are estimated as a whole set . We are not comparing the single best molecule . 2.For fairness in querying the oracles , though we will utilize the oracle during sampling , other baselines will also have access to the oracles ( e.g. , labels for training a gaussian process predictor ) . Baselines like JTVAE need to do optimization in the latent space . GCPN needs to query the oracles for the reward . * * Q : complexity issue : How many times do the authors need to query the oracles ? Calling oracles is expensive . * * A : In the sampling process , the system will query the oracle for $ NT $ times , where $ N $ is the number of sampling paths ( $ N=5000 $ for our experiment ) , and $ T $ is the total number of sampling steps ( $ T=500 $ roughly for the average overall score getting converged ) . However , querying oracles multiple times is not as expensive as it seems : 1 . We can use ML models or expert-designed rules to estimate the molecules in a short time . For example , in our experiment ( GSK3b+JNK3+QED+SA ) , the time we spent on querying the oracles is within 2 hours , and this is acceptable comparing to the time spent in the conventional drug design and discovery process , which usually takes months to years . 2.We can reduce the time in querying the oracles by a parallel implementation , as the property scores will not intervene with each other . Moreover , other baseline methods also need to query oracle many times . For example : 1 . When training GCPN and RationaleRL with reinforcement learning , the rewards are computed with oracles . 2.In translation-based or latent space-based approaches , a large amount of high-quality data is needed , and collecting them requires to query the oracles more times and even take expensive experiments ."}, {"review_id": "kHSu4ebxFXY-1", "review_text": "Summary : The authors propose a novel way to generate molecules with specified objectives , named MArkov moleculaR Sampling ( MARS ) . The idea of MARS is based on generating the chemical candidates by iterative editing fragments of molecular graphs . To transform a molecule x into another molecule x\u2032 , the authors considers two sets of graph editing actions fragment adding and fragment deleting , where fragments are connected components in molecules separated by single bonds . To generate the molecules with desired objectives , MARS is using Markov chain Monte Carlo sampling with specified annealing scheme , together with graph convolutional neural network . The results reported in the following paper are very promising and show that this could be a good direction in the area of multi-objective molecules optimization . == Pros : 1 . The idea proposed by authors seems novel . It is the combination of MCMC based on molecules , together with message passing neural networks . 2.The proposed model is a new state of the art in the area of multi-objective molecules optimization . In the experimental section the authors shows that molecules generated by MARS have the highest desired objectives in 5 out of 6 proposed tasks . Simultaneously the generated set of molecules seems novel and diverse . 3.The molecules generated by MARS are evenly distributed in the space with a range of novel regions covered , as showed in Figure 3 . This is a desirable behavior , better than in the other generative models , where we can see clusters of generated molecules . == Cons : 1 . The proposed strategy seems constrained by the selection of the fragment dataset . 2.The instruction of calculating probability densities over fragments in the vocabulary is not clear . More precisely , the authors states that hidden state for fragment graphs is given by h^ { graph } = MaxPooling ( { h^ { node } _ { u } } ) , however they do not state whether the nodes taken for aggregation are from the fragment itself or from combined fragment and molecule x or whether from the molecule x only . == Questions during rebuttal period : 1 . It is not stated in the text what is the initial molecule . Do you have some starting set of molecules or always start from the same ? 2.How fast is the convergence of the proposed method ? It would be nice to see some plot with time steps on X axis and score on Y axis . 3.The authors did not state what was the number of train steps used to converge MARS in their experiments . What number of time steps did you use ? How long does the MARS training takes , compared to other methods ? == == Reasons for score : Overall , I vote for accepting this paper . The problem of multi-objective molecules optimization is hard and really important in cheminformatics . The idea proposed by the authors is novel and confirmed experimentally . Hopefully the authors can address my concern in the rebuttal period .", "rating": "7: Good paper, accept", "reply_text": "Thanks so much for your comments and constructive suggestions ! We will respond to your concerns on the selection of fragment vocabulary , the convergence speed , and some method details respectively in the following paragraphs . Hope these replies could resolve your concerns , and any further comments are welcome ! * * Q : The proposed strategy seems constrained by the selection of the fragment dataset . * * A : Thanks for your kind notes . Yes , the fragment vocabulary is curial for our proposed method , and an insufficient vocabulary could lead to a constrained strategy ( e.g. , some molecules can not be obtained by adding/removing fragment actions ) . However , this issue is addressed in our approach using the following strategies : 1 . We have employed a massive molecular database ( i.e. , ChEMBL ) to construct our fragment vocabulary , guaranteeing a broad coverage of the essential elements in chemistry ( e.g. , -H , -C , -F , -O , =O , and aromatic rings ) . 2.Our approach adopts a sequence of flexible molecular editing actions ( adding and deleting ) which can generate complex fragments and eventually complex drug molecules using the relative simple fragments in our vocabulary as building blocks . It is possible that a tailored vocabulary can help MARS to produce better performances on specific tasks ( e.g. , selecting fragments based on the prior knowledge of the drug target ) . However , in this paper , we mainly focus on designing a general and flexible molecular sampling framework . For specific applications , we also provide a means for the users to define their own molecular databases and vocabulary construction strategies . * * Q : How fast is the convergence of the proposed method ? What was the number of train steps used to converge MARS ? * * A : In our GSK3b+JNK3+QED+SA experiment where $ N=5000 $ molecule sampling paths are simultaneously considered , MARS takes roughly 500 sampling steps ( i.e. , $ T=500 $ ) and 16 hours in total to converge . We have posted some plots with time steps on the X-axis , as you suggested in Section 4.2 . As for other baselines , RationaleRL takes 5.7 hours to fine-tune the model , and GA+D takes 278 steps and 2.2h to achieve its best performance . However , one thing should be noted , both RationaleRL and GA+D are greatly accelerated by adopting parallel computation , which is not implemented in MARS yet . We believe MARS will have comparable computation efficiency if the conversions of molecules to graphs and molecular evaluations are computed parallelly . In addition , comparing to the conventional drug discovery process , which usually takes months to years , the time we spent on molecular generation models is ignorable ( at most several hours ) . And in molecular generation , it is more important to well explore the chemical space -- find novel and diverse active molecules [ 1 ] . * * Q : What nodes are taken for aggregation in MPNNs ? * * A : Thanks for pointing out this ! All the atom nodes in the molecule x are taken for aggregation in MPNNs ( therefore , the input is from the molecule x only ) , and we have clarified this in Section 3.2 ( Parameterizing with MPNNs ) . We do this mainly for computation efficiency consideration . More computational time is needed if both fragments and molecule $ x $ are considered since the number of combinations is proportional to the fragment vocabulary size , which is usually large . * * Q : It is not stated in the text what is the initial molecule . * * A : All sampling paths are started with the identical initial molecule `` C-C '' , which is also adopted by previous graph generation methods for organic molecules ( e.g . [ 2 ] ) .We have clarified this in our revision ( Section 4.1 , Implementation details ) . [ 1 ] Huggins et al. , Rational Methods for the Selection of Diverse Screening Compounds , ACS Chem . Biol.2011 [ 2 ] You et al.Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation . NeurIPS 2018 ."}, {"review_id": "kHSu4ebxFXY-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposes a method to generate molecular graphs with multiple optimized properties . Molecular graphs are constructed/edited by the iterative addition and removal of molecular fragments . A MCMC search procedure , guided by a learned graph neural network that proposes good graph edit actions , is used to sample molecules with optimized properties . The proposed model is compared with some baselines on a few multi-objective optimization tasks and shows good performance . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I vote for acceptance . The paper presents an interesting method for multi-objective molecule optimization that shows good performance in the evaluation . However , I have some concerns about the benchmark tasks and baselines that hopefully could be addressed # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strengths : * Paper is written in a clear way , and is well structured * Proposed model shows very good performance in the evaluation , compared to other baselines * Ablation studies that provide useful insight about the model Weaknesses : * Some concerns about the benchmark tasks and baselines ( see below ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions and other comments : * I think that the summary product score ( success rate x novelty x diversity ) could be a bit misleading for the casual reader . Looking at Table 2 , it is clear to me that different models have different performance characteristics in relation to the success rate , novelty and diversity model performance metrics . In a practical use case , one may not equally weight the importance of these performance metrics , and instead decide on a trade off depending on the specific problem . Eg based on Table 2 , we would pick the MARS model for high success rate , GA+D for high novelty , RationaleRL for high diversity . However , the simple summary product score ( especially with the highlighted column in Table 1 and 2 ) make this nuance harder to see . * The abstract claim : `` [ The method ] outperforms the best prior methods by 100 % in terms of success rate , novelty , and diversity of generated molecules . '' is a bit ambiguous , since it seems to only happen with the product score in 1 of the 6 tasks * Do you have any data on the ability of the proposed model to optimize raw property scores ? Although it is useful to see the proportion of molecules generated containing properties above a predefined threshold , sometimes we may be interested getting the handful of best molecules . Eg the most potent inhibitor , or to a lesser extent the molecules with the highest QED and SA scores . * For the success rate metric , what was the reasoning for the thresholds ? Eg GSK3B/JNK3 > 0.5 , QED > 0.6 , SA > 0.67 * How did you implement the weighting for the multiple objectives ( GSK3B/JNK3 , QED , SA ) in GCPN and JT-VAE ? It seems that the weighting of the different objectives is another set of model hyperparameters that requires appropriate tuning . * Any thoughts about the applicability of some public molecule generation/optimization benchmarks for this work , eg https : //github.com/BenevolentAI/guacamol ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks so much for your comments and constructive suggestions ! We will address your concerns regarding the evaluation metrics , the baselines , and the benchmark respectively in the following paragraphs . Hope these replies could resolve your concerns , and any further comments are welcome ! * * Q : The summary product score ( success rate x novelty x diversity ) could be a bit misleading for the casual reader . * * A : Thanks for your kind reminder ! These metrics ( i.e. , success rate , novelty , and diversity ) indeed capture different performance characteristics of models and should be considered thoroughly according to specific applications . Along with these metrics , we propose the so-called summary product score mainly to provide an overall comparison of different methods . Generally , for drug discovery , it is the best case if a molecular generation system can simultaneously achieve high scores on all three metrics . This summary product score is meaningful by intuitively presenting the percentage of generated molecules being simultaneously bio-active , novel , and diverse , which are essential criteria to be considered in building a suitable drug candidate library in early-stage drug discovery [ 1 ] . This issue is also discussed in our revised submission ( Section 4.1 Evaluation Metrics ) following your suggestions . * * Q : For the success rate metric , what was the reasoning for the thresholds ? * * A : We set these thresholds by following previous work [ 2 ] . Intuitively , the reasoning for these thresholds is that , we require the generated molecules to be bio-active ( GSK3 $ \\beta $ /JNK3 > 0.5 [ 2,3 ] ) , drug-like ( QED > 0.6 ) and easy to synthesize ( SA > 0.67 ) . * * Q : How did you implement the weighting for the multiple objectives ( GSK3B/JNK3 , QED , SA ) in GCPN and JT-VAE ? * * A : We equally weight all the objectives in all the settings for both our method and other baselines . Moreover , we do this for two reasons : 1 . For comparison fairness , so that MARS and all the baselines will have a consistent goal ; 2 . We have also tried to tune the weights to improve GCPN and JT-VAE , but the results remained roughly the same . As for RationaleRL , we employed their implementation where the environment will give the agent a discrete reward only when all objective thresholds are satisfied . Based on the above-stated reasons , we reported the results obtained using equally weighted objectives in the paper . However , in different scenarios , we can practically tune these hyperparameters according to the various requirements . * * Q : Do you have any data on the ability of the proposed model to optimize raw property scores ? We may be interested getting the handful of best molecules . * * A : Thanks for your interest in the molecules generated by our model ! We have provided some examples of generated molecules in our revised paper . Please refer to Appendix A for more details . * * Q : Any thoughts about the applicability of some public molecule generation/optimization benchmarks for this work ? * * A : Thanks for your kind suggestion on the GuacaMol benchmark , which could be a suitable platform to evaluate the proposed method as it includes multi-objective molecular generation tasks . However , we can hardly provide a comprehensive and reliable evaluation of all models on the GuacaMol benchmark in this paper , considering the tight rebuttal schedule . We are currently conducting the experiments and will release the results in our future publications . * * Q : The abstract claim is a bit ambiguous , `` [ The method ] outperforms the best prior methods by 100 % in terms of success rate , novelty , and diversity of generated molecules . `` * * A : Thanks for your kind notes ! We have rephrased this sentence as `` in the most challenging setting where four objectives -- bio-activities to two different targets , drug-likeness and synthesizability -- are simultaneously considered , our method outperforms the state-of-the-art significantly in a comprehensive evaluation . '' in our revised version . [ 1 ] Huggins et al. , Rational Methods for the Selection of Diverse Screening Compounds , ACS Chem . Biol.2011 [ 2 ] Jin et al. , Multi-Objective Molecule Generation using Interpretable Substructures , ICML 2020 [ 3 ] Li et al. , Multi-objective de novo drug design with conditional graph generative model , J.Chem.Info 2018"}, {"review_id": "kHSu4ebxFXY-3", "review_text": "SUMMARY : The authors present an elegant Markov-Chain Monte Carlo ( MCMC ) method to carry out the task of generating molecular structures that satisfy several objectives . PROS : - The work is well written , concise and easy to follow - The methodology is competitive with other approaches that are state-of-the-art in the optimization of single properties ( such as GA-D ) and show that they outperform them in most cases . - The references that it cites are balanced . - The multiobjective optimization is based on biological objectives CONS : - I see no major cons with this work .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks so much for your positive and insightful comments ! We highly appreciate your efforts and time in evaluating our paper . In the future , we are interested in applying our proposed MARS in practical drug discovery scenarios and further improving it by developing more adaptive training strategies and more advanced parameterization approaches for the molecular editing proposal ."}], "0": {"review_id": "kHSu4ebxFXY-0", "review_text": "Summary : The paper proposes a sampling-based approach for multiple property optimization in molecule generation . Strength : the sampling approach is an interesting new direction for molecule generation . Also multi-objective molecule generation is an important task . Weakness : I have several concerns regarding this paper . ( 1 ) Section 3.2 is unclear , with lots of details missing . For example , what is the input for MPNN ? i.e. , x in M_\\theta ( x ) . How to determine the supervision signal p_add , p_delete for M_\\theta ( x ) ? When add/deleted fragments , how many fragments are added/deleted in a single transition kernel ? If multiple fragments are added/deleted , are they added/deleted sequentially or following some particular rule ? The MPNN description ( Eq.5 -- 9 ) is borrowed from [ 1 ] , please cite the paper . The paper trains the MPNN during sampling procedure , which seems very challenging , in the first several steps the model is from scratch , how to guarantee the sampled molecule is reasonable ? If initially , the sampled molecules can not provide informative supervision , it will be hard to train a strong MPNN model . Also , the number of training samples also looks very limited . In [ 1 ] , it requires a large amount of existing drug molecules to pretrain a good MPNN model . ( 2 ) Unfair comparison between the proposed sampling model and non-sampling baselines . For example , when sample size N=5000 , the proposed model will query oracle to select a subset of generated molecules . However , baselines such as JTVAE directly output the generated molecules , all of which are kept . Therefore , in performance comparison if you compare the best molecules in 5000 molecules with a single molecule generated by JTVAE , this is unfair . Please clarify . ( 3 ) complexity issue : How many times do the authors need to query the oracles when performing optimization using the proposed sampling algorithm ? How does this compare to the baselines ? Each time you sample a new molecule , you need to evaluate the acceptance rate in Eq.2 , while such an evaluation requires evaluation of target distribution in Eq.1 as well as need to call the oracle ( e.g. , evaluating the property ) , which is very expensive . ( 4 ) Lack of theoretical guarantee . The authors need to show the sampled molecules follow target distribution . The authors also argue they adaptively train the MPNN , so the transition kernel will change during sampling . It is non-trivial to guarantee the convergence to target distribution . ( 5 ) There are also issues with experimental setting and results . For example Important baselines are missing . For example , Graph2Graph [ 2 ] and hierarchical generation [ 3 ] . One baseline RationaleRL outperform the proposed MARS in terms of success rate in many settings . The fairness in model comparison issue I mentioned in ( 2 ) How to use t-SNE to visualize molecule distribution ? It needs more details . [ 1 ] Hu et al , ICLR 2020 , Strategies For Pre-training Graph Neural Networks [ 2 ] Jin et al , ICLR 2019 , Learning Multimodal Graph-to-Graph Translation for Molecule Optimization . [ 3 ] Jin et al , ICML 2020 , Hierarchical Generation of Molecular Graphs using Structural Motifs .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks so much for your comments and constructive suggestions ! We will respond to your concerns on the issues of theoretical guarantees , the comparison fairness , the computation complexity , and some method or experiment details respectively in the following paragraphs . Hope these replies could resolve your concerns , and any further comments are welcome ! -- * * Q : Lack of theoretical guarantee about the training and convergence : 1 ) The paper trains the MPNN during sampling procedure , which seems very challenging ; 2 ) need to show the sampled molecules follow target distribution ; 3 ) It is non-trivial to guarantee the convergence to target distribution . * * A : Thanks very much for your kind reminder ! We have provided some further discussions on the training and convergence issues in Section 3.2 ( Discussion on training and convergence ) . In brief , the sampling process will converge if the adaptive proposal itself converges . And this can be satisfied by employing an adaptive optimizer ( e.g. , Adam optimizer ) to update the model parameters . Also , the stationary distribution of annealed MCMC will converge to the set of global optima if we set the temperature cooling schedule appropriately . > * * Discussion on training and convergence . * * On the one hand , for a MCMC sampling process based on an adaptive proposal to converge , it is required that the proposal $ q_ { \\theta } $ will converge as the sampling time step goes to infinity ( Rosenthal , 2011 ) . We can easily satisfy this requirement by choosing an appropriate training strategy ( e.g. , use an adaptive optimizer like Adam ( Paszke et al. , 2017 ) to update model parameters ) . On the other hand , most convergence results for simulated annealing typically state that , if for a give a temperature $ T_i $ , the homogeneous Markov transition kernel mixes quickly enough , then convergence to the set of global maxima of $ \\pi ( x ) $ is ensured for a temperature sequence $ T_i = ( C \\ln ( i+ T_0 ) ) ^ { -1 } $ , where $ C $ and $ T_0 $ are problem-dependent ( Andrieu et al. , 2003 ) . Therefore , we can elaborately design the cooling strategy of annealing to make a better approximation to the global optima . * * Q : Unfair comparison between the proposed sampling model and non-sampling baselines . * * A : Thanks for raising your concerns , but we think the comparison is fair for the following reasons : 1 . For the comparison fairness in the evaluation , all the baselines have generated 5000 molecules , and the 5000 generated/sampled molecules are estimated as a whole set . We are not comparing the single best molecule . 2.For fairness in querying the oracles , though we will utilize the oracle during sampling , other baselines will also have access to the oracles ( e.g. , labels for training a gaussian process predictor ) . Baselines like JTVAE need to do optimization in the latent space . GCPN needs to query the oracles for the reward . * * Q : complexity issue : How many times do the authors need to query the oracles ? Calling oracles is expensive . * * A : In the sampling process , the system will query the oracle for $ NT $ times , where $ N $ is the number of sampling paths ( $ N=5000 $ for our experiment ) , and $ T $ is the total number of sampling steps ( $ T=500 $ roughly for the average overall score getting converged ) . However , querying oracles multiple times is not as expensive as it seems : 1 . We can use ML models or expert-designed rules to estimate the molecules in a short time . For example , in our experiment ( GSK3b+JNK3+QED+SA ) , the time we spent on querying the oracles is within 2 hours , and this is acceptable comparing to the time spent in the conventional drug design and discovery process , which usually takes months to years . 2.We can reduce the time in querying the oracles by a parallel implementation , as the property scores will not intervene with each other . Moreover , other baseline methods also need to query oracle many times . For example : 1 . When training GCPN and RationaleRL with reinforcement learning , the rewards are computed with oracles . 2.In translation-based or latent space-based approaches , a large amount of high-quality data is needed , and collecting them requires to query the oracles more times and even take expensive experiments ."}, "1": {"review_id": "kHSu4ebxFXY-1", "review_text": "Summary : The authors propose a novel way to generate molecules with specified objectives , named MArkov moleculaR Sampling ( MARS ) . The idea of MARS is based on generating the chemical candidates by iterative editing fragments of molecular graphs . To transform a molecule x into another molecule x\u2032 , the authors considers two sets of graph editing actions fragment adding and fragment deleting , where fragments are connected components in molecules separated by single bonds . To generate the molecules with desired objectives , MARS is using Markov chain Monte Carlo sampling with specified annealing scheme , together with graph convolutional neural network . The results reported in the following paper are very promising and show that this could be a good direction in the area of multi-objective molecules optimization . == Pros : 1 . The idea proposed by authors seems novel . It is the combination of MCMC based on molecules , together with message passing neural networks . 2.The proposed model is a new state of the art in the area of multi-objective molecules optimization . In the experimental section the authors shows that molecules generated by MARS have the highest desired objectives in 5 out of 6 proposed tasks . Simultaneously the generated set of molecules seems novel and diverse . 3.The molecules generated by MARS are evenly distributed in the space with a range of novel regions covered , as showed in Figure 3 . This is a desirable behavior , better than in the other generative models , where we can see clusters of generated molecules . == Cons : 1 . The proposed strategy seems constrained by the selection of the fragment dataset . 2.The instruction of calculating probability densities over fragments in the vocabulary is not clear . More precisely , the authors states that hidden state for fragment graphs is given by h^ { graph } = MaxPooling ( { h^ { node } _ { u } } ) , however they do not state whether the nodes taken for aggregation are from the fragment itself or from combined fragment and molecule x or whether from the molecule x only . == Questions during rebuttal period : 1 . It is not stated in the text what is the initial molecule . Do you have some starting set of molecules or always start from the same ? 2.How fast is the convergence of the proposed method ? It would be nice to see some plot with time steps on X axis and score on Y axis . 3.The authors did not state what was the number of train steps used to converge MARS in their experiments . What number of time steps did you use ? How long does the MARS training takes , compared to other methods ? == == Reasons for score : Overall , I vote for accepting this paper . The problem of multi-objective molecules optimization is hard and really important in cheminformatics . The idea proposed by the authors is novel and confirmed experimentally . Hopefully the authors can address my concern in the rebuttal period .", "rating": "7: Good paper, accept", "reply_text": "Thanks so much for your comments and constructive suggestions ! We will respond to your concerns on the selection of fragment vocabulary , the convergence speed , and some method details respectively in the following paragraphs . Hope these replies could resolve your concerns , and any further comments are welcome ! * * Q : The proposed strategy seems constrained by the selection of the fragment dataset . * * A : Thanks for your kind notes . Yes , the fragment vocabulary is curial for our proposed method , and an insufficient vocabulary could lead to a constrained strategy ( e.g. , some molecules can not be obtained by adding/removing fragment actions ) . However , this issue is addressed in our approach using the following strategies : 1 . We have employed a massive molecular database ( i.e. , ChEMBL ) to construct our fragment vocabulary , guaranteeing a broad coverage of the essential elements in chemistry ( e.g. , -H , -C , -F , -O , =O , and aromatic rings ) . 2.Our approach adopts a sequence of flexible molecular editing actions ( adding and deleting ) which can generate complex fragments and eventually complex drug molecules using the relative simple fragments in our vocabulary as building blocks . It is possible that a tailored vocabulary can help MARS to produce better performances on specific tasks ( e.g. , selecting fragments based on the prior knowledge of the drug target ) . However , in this paper , we mainly focus on designing a general and flexible molecular sampling framework . For specific applications , we also provide a means for the users to define their own molecular databases and vocabulary construction strategies . * * Q : How fast is the convergence of the proposed method ? What was the number of train steps used to converge MARS ? * * A : In our GSK3b+JNK3+QED+SA experiment where $ N=5000 $ molecule sampling paths are simultaneously considered , MARS takes roughly 500 sampling steps ( i.e. , $ T=500 $ ) and 16 hours in total to converge . We have posted some plots with time steps on the X-axis , as you suggested in Section 4.2 . As for other baselines , RationaleRL takes 5.7 hours to fine-tune the model , and GA+D takes 278 steps and 2.2h to achieve its best performance . However , one thing should be noted , both RationaleRL and GA+D are greatly accelerated by adopting parallel computation , which is not implemented in MARS yet . We believe MARS will have comparable computation efficiency if the conversions of molecules to graphs and molecular evaluations are computed parallelly . In addition , comparing to the conventional drug discovery process , which usually takes months to years , the time we spent on molecular generation models is ignorable ( at most several hours ) . And in molecular generation , it is more important to well explore the chemical space -- find novel and diverse active molecules [ 1 ] . * * Q : What nodes are taken for aggregation in MPNNs ? * * A : Thanks for pointing out this ! All the atom nodes in the molecule x are taken for aggregation in MPNNs ( therefore , the input is from the molecule x only ) , and we have clarified this in Section 3.2 ( Parameterizing with MPNNs ) . We do this mainly for computation efficiency consideration . More computational time is needed if both fragments and molecule $ x $ are considered since the number of combinations is proportional to the fragment vocabulary size , which is usually large . * * Q : It is not stated in the text what is the initial molecule . * * A : All sampling paths are started with the identical initial molecule `` C-C '' , which is also adopted by previous graph generation methods for organic molecules ( e.g . [ 2 ] ) .We have clarified this in our revision ( Section 4.1 , Implementation details ) . [ 1 ] Huggins et al. , Rational Methods for the Selection of Diverse Screening Compounds , ACS Chem . Biol.2011 [ 2 ] You et al.Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation . NeurIPS 2018 ."}, "2": {"review_id": "kHSu4ebxFXY-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposes a method to generate molecular graphs with multiple optimized properties . Molecular graphs are constructed/edited by the iterative addition and removal of molecular fragments . A MCMC search procedure , guided by a learned graph neural network that proposes good graph edit actions , is used to sample molecules with optimized properties . The proposed model is compared with some baselines on a few multi-objective optimization tasks and shows good performance . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I vote for acceptance . The paper presents an interesting method for multi-objective molecule optimization that shows good performance in the evaluation . However , I have some concerns about the benchmark tasks and baselines that hopefully could be addressed # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strengths : * Paper is written in a clear way , and is well structured * Proposed model shows very good performance in the evaluation , compared to other baselines * Ablation studies that provide useful insight about the model Weaknesses : * Some concerns about the benchmark tasks and baselines ( see below ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions and other comments : * I think that the summary product score ( success rate x novelty x diversity ) could be a bit misleading for the casual reader . Looking at Table 2 , it is clear to me that different models have different performance characteristics in relation to the success rate , novelty and diversity model performance metrics . In a practical use case , one may not equally weight the importance of these performance metrics , and instead decide on a trade off depending on the specific problem . Eg based on Table 2 , we would pick the MARS model for high success rate , GA+D for high novelty , RationaleRL for high diversity . However , the simple summary product score ( especially with the highlighted column in Table 1 and 2 ) make this nuance harder to see . * The abstract claim : `` [ The method ] outperforms the best prior methods by 100 % in terms of success rate , novelty , and diversity of generated molecules . '' is a bit ambiguous , since it seems to only happen with the product score in 1 of the 6 tasks * Do you have any data on the ability of the proposed model to optimize raw property scores ? Although it is useful to see the proportion of molecules generated containing properties above a predefined threshold , sometimes we may be interested getting the handful of best molecules . Eg the most potent inhibitor , or to a lesser extent the molecules with the highest QED and SA scores . * For the success rate metric , what was the reasoning for the thresholds ? Eg GSK3B/JNK3 > 0.5 , QED > 0.6 , SA > 0.67 * How did you implement the weighting for the multiple objectives ( GSK3B/JNK3 , QED , SA ) in GCPN and JT-VAE ? It seems that the weighting of the different objectives is another set of model hyperparameters that requires appropriate tuning . * Any thoughts about the applicability of some public molecule generation/optimization benchmarks for this work , eg https : //github.com/BenevolentAI/guacamol ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks so much for your comments and constructive suggestions ! We will address your concerns regarding the evaluation metrics , the baselines , and the benchmark respectively in the following paragraphs . Hope these replies could resolve your concerns , and any further comments are welcome ! * * Q : The summary product score ( success rate x novelty x diversity ) could be a bit misleading for the casual reader . * * A : Thanks for your kind reminder ! These metrics ( i.e. , success rate , novelty , and diversity ) indeed capture different performance characteristics of models and should be considered thoroughly according to specific applications . Along with these metrics , we propose the so-called summary product score mainly to provide an overall comparison of different methods . Generally , for drug discovery , it is the best case if a molecular generation system can simultaneously achieve high scores on all three metrics . This summary product score is meaningful by intuitively presenting the percentage of generated molecules being simultaneously bio-active , novel , and diverse , which are essential criteria to be considered in building a suitable drug candidate library in early-stage drug discovery [ 1 ] . This issue is also discussed in our revised submission ( Section 4.1 Evaluation Metrics ) following your suggestions . * * Q : For the success rate metric , what was the reasoning for the thresholds ? * * A : We set these thresholds by following previous work [ 2 ] . Intuitively , the reasoning for these thresholds is that , we require the generated molecules to be bio-active ( GSK3 $ \\beta $ /JNK3 > 0.5 [ 2,3 ] ) , drug-like ( QED > 0.6 ) and easy to synthesize ( SA > 0.67 ) . * * Q : How did you implement the weighting for the multiple objectives ( GSK3B/JNK3 , QED , SA ) in GCPN and JT-VAE ? * * A : We equally weight all the objectives in all the settings for both our method and other baselines . Moreover , we do this for two reasons : 1 . For comparison fairness , so that MARS and all the baselines will have a consistent goal ; 2 . We have also tried to tune the weights to improve GCPN and JT-VAE , but the results remained roughly the same . As for RationaleRL , we employed their implementation where the environment will give the agent a discrete reward only when all objective thresholds are satisfied . Based on the above-stated reasons , we reported the results obtained using equally weighted objectives in the paper . However , in different scenarios , we can practically tune these hyperparameters according to the various requirements . * * Q : Do you have any data on the ability of the proposed model to optimize raw property scores ? We may be interested getting the handful of best molecules . * * A : Thanks for your interest in the molecules generated by our model ! We have provided some examples of generated molecules in our revised paper . Please refer to Appendix A for more details . * * Q : Any thoughts about the applicability of some public molecule generation/optimization benchmarks for this work ? * * A : Thanks for your kind suggestion on the GuacaMol benchmark , which could be a suitable platform to evaluate the proposed method as it includes multi-objective molecular generation tasks . However , we can hardly provide a comprehensive and reliable evaluation of all models on the GuacaMol benchmark in this paper , considering the tight rebuttal schedule . We are currently conducting the experiments and will release the results in our future publications . * * Q : The abstract claim is a bit ambiguous , `` [ The method ] outperforms the best prior methods by 100 % in terms of success rate , novelty , and diversity of generated molecules . `` * * A : Thanks for your kind notes ! We have rephrased this sentence as `` in the most challenging setting where four objectives -- bio-activities to two different targets , drug-likeness and synthesizability -- are simultaneously considered , our method outperforms the state-of-the-art significantly in a comprehensive evaluation . '' in our revised version . [ 1 ] Huggins et al. , Rational Methods for the Selection of Diverse Screening Compounds , ACS Chem . Biol.2011 [ 2 ] Jin et al. , Multi-Objective Molecule Generation using Interpretable Substructures , ICML 2020 [ 3 ] Li et al. , Multi-objective de novo drug design with conditional graph generative model , J.Chem.Info 2018"}, "3": {"review_id": "kHSu4ebxFXY-3", "review_text": "SUMMARY : The authors present an elegant Markov-Chain Monte Carlo ( MCMC ) method to carry out the task of generating molecular structures that satisfy several objectives . PROS : - The work is well written , concise and easy to follow - The methodology is competitive with other approaches that are state-of-the-art in the optimization of single properties ( such as GA-D ) and show that they outperform them in most cases . - The references that it cites are balanced . - The multiobjective optimization is based on biological objectives CONS : - I see no major cons with this work .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks so much for your positive and insightful comments ! We highly appreciate your efforts and time in evaluating our paper . In the future , we are interested in applying our proposed MARS in practical drug discovery scenarios and further improving it by developing more adaptive training strategies and more advanced parameterization approaches for the molecular editing proposal ."}}