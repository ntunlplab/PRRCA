{"year": "2021", "forum": "PsdsEbzxZWr", "title": "Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection", "decision": "Reject", "meta_review": "This paper conducts a theoretical and empirical analysis of the Generative Adversarial Training method (GAT). Although many comments have been addressed in the rebuttal, the reviewers still have few (but important) concerns, including the memorization effects and the lack of comparisons. ", "reviews": [{"review_id": "PsdsEbzxZWr-0", "review_text": "In this paper the authors provide a theoretical and empirical analysis of the Generative Adversarial Training method ( GAT ) which is used to train models for OOD and adversarial example detection . The GAT method is analyzed from a game theoretical prespective , focusing on the differences with GAN training , which are sometime conflated with it in the literature . The authors show that GAT and GAN have different training problems ( maximin vs minimax ) which have different optimal solutions . The authors also propose a variant of GAT called Unconstraned GAT , which replaces the PGD attack in the inner optimization loop with an unconstrained steepest ascent update . They propose this training algorithm for both OOD detection , adversarial OOD detection and generative sampling . They discuss the sensitivity of the proposed algorithm on the step size , which seems to critically depend on the model architecture and dataset . Experiments are performed on standard image datasets , using ImageNet as a source of known OOD examples . Overall the work is interesting , however I find some issues : - The key point of the theoretical anaysis comparing GAT with GAN is only made at the end . It would be better to mention in the introduction at a high level why GAT is preferable to GAN . - Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with . They should be defined , possibly first in an intuitive way and then formally . - The proposed algorithm seem quite sensitive on the step size . This might limit its practical applicability ( minor issue : the step size is denoted as lambda in the text and gamma in the algorithm box ) . - The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample . You would need to use an SGLD-like algorithm in order get unbiased samples . EDIT : The revision addressed my concerns , I 'm raising my evaluation to 7 .", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer : Thank you very much for your time and insightful comments ! We have revised the paper based on your feedback . Below we response to your questions . * * The key point of the theoretical analysis comparing GAT with GAN is only made at the end . It would be better to mention in the introduction at a high level why GAT is preferable to GAN . * * Thanks for the suggestion . We have included in the introduction a brief discussion about the relative merit of the proposed approach . * * Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with . They should be defined , possibly first in an intuitive way and then formally . * * Thanks for the suggestion . In Appendix B we have included a discussion about these two problems in the game theory framework . To make the materials more accessible , in Appendix C we also included a demonstration of the general strategy for solving a maximin problem . * * The proposed algorithm seem quite sensitive on the step size . This might limit its practical applicability ( minor issue : the step size is denoted as lambda in the text and gamma in the algorithm box ) . * * We agree that the step size could be a potential problem . However , just as our failure mode diagnosis indicates , the problem of step size being too large can be detected in an early stage of training . We also observed that our algorithm works as long as the step size is below a certain threshold . Given these information , we think a working step size can be quickly found using binary search . Besides , in the paper we have provided the step size values for future work \u2019 s reference . Thanks , we have fixed the notation error . * * The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample . You would need to use an SGLD-like algorithm in order get unbiased samples . * * Thanks for the suggestion . This is a known limitation of our method and we have included a discussion about this issue on Page 9 . We think this issue might be mitigated by constraining the number of steps and step sizes , as the gradient-based generation process is governed by these two parameters . The suggested algorithm looks new and interesting to us , and we will definitely look into it ."}, {"review_id": "PsdsEbzxZWr-1", "review_text": "* * 1.Summary and contributions : Briefly summarize the paper and its contributions * * This work analyzed the optimal solutions for the Generative adversarial training ( GAT ) and the convergence property of the training algorithm . This work also compared the minimax and maximin games , both theoretically , and empirically , with the help of a nice 2D toy example . This work also developed an unconstrained version of GAT , and evaluated it on image generation and out of distribution detection tasks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 2 . Strengths : Describe the strengths of the work . Typical criteria include : soundness of the claims ( theoretical grounding , empirical evaluation ) , significance and novelty of the contribution , and relevance to the community . * * I found the theoretical analysis to be interesting , and I especially like the 2D toy experiment in Figure 2 , it strongly and clearly justified the importance of using a uniformly distributed p_ ( -k ) distribution . I also liked the thorough discussion of the distinction between the minimax and maximin games . The paper has interesting ideas and a lot of content , it also introduced adversarial OOD samples , which are all very interesting to me . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 3 . Weaknesses : Explain the limitations of this work along the same axes as above . * * Memorization : I think optimizing the D approach is problematic in terms of memorization , there \u2019 s nothing stopping the model to memorize the data , especially in the high dimensional space which makes the mass distribution to be very sparse . In the Celeb A results in the middle of Figure 3 , notice how the images ( 4,1 ) , ( 4,3 ) look almost exactly the same , and they are also very similar to ( 1,3 ) , ( 3,1 ) and ( 3,4 ) . In figure 2b and 2c , notice how the distribution all collapses to 1 point , I wonder whether this is one perspective or intuition on this problem . Unverified claim : At the bottom of page 7 , \u201c These results suggest that with a high capacity model and proper training , a robust OOD detection system is within reach. \u201d where these results referred to reducing the data complexity . I think the results in this paper are not enough to make this claim . The authors \u2019 logic here is that , if their model performs better on a simple dataset , then it implies the problem is insufficient model capacity . The assumption here is that the model can scale , which is completely unverified . Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet . An unverified claim like this is always a warning sign as it may mislead the readers and community . Lack of ablation study : I really liked the toy experiments in Figure 2 , which justified the use of uniform distribution in the data space . However this only provides intuition for the higher dimensional cases , it is still necessary to conduct an ablation study to verify this is indeed the case for higher-dimensional cases , for scientific rigor . Unfair comparison : For image generation results , the authors \u2019 method was compared with GANs , and the motivation is that both of the methods are trained adversarially . However , the generators of GANs never got to see the real data during train time . Here authors are optimizing D , which is trained on real data , thus I don \u2019 t think it is fair to compare with GANs . Weak baseline for OOD ( outlier exposure ) and insufficient comparison : Outlier exposure is no longer the state of the art OOD detection methods and there are so many other methods that perform better than outlier exposure on CIFAR10 , for example , see Detecting Out-of-Distribution Examples with Gram Matrices : https : //arxiv.org/abs/1912.12510 Lack of Related Work section : This paper does not have a related work section . The related works are very briefly discussed in the introduction , but I think that is far from enough . I understand there is a page limit , but even putting a related work section in the appendix would be very helpful for readers to get a more complete understanding of where this work stands . Unclear writings : See section 4 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 4 . Clarity : Is the paper well written ? * * Typos : At the top of page 3 : as a results - > as a result Bottom of page 8 : a OOD - > an OOD Ambiguity : \u201c elementary mass \u201d was not defined before being used , it \u2019 d be helpful to the readers to briefly define what \u201c elementary mass \u201d means in this specific context . \u201c In order to cause more local maxima to be eliminated \u201d could be worded better . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 5 . Reasons for score * * In conclusion , the ideas are very interesting but I think there is a lot more work to be done before this paper could be accepted .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear reviewer , Thank you very much for your time and constructive feedback ! We have conducted more experiments and revised the paper according to your suggestions . Below , we respond to each of your comments and look forward to your further feedback . * * Memorization : I think optimizing the D approach is problematic in terms of memorization , there \u2019 s nothing stopping the model to memorize the data , especially in the high dimensional space which makes the mass distribution to be very sparse . * * We completely agree that our approach has the potential problem of overfitting . Our analysis says that in Figure 1 scenario 1 an ideal $ D $ solution has no local maxima and global maxima at support of $ p_ { k } $ , which means overfitting when $ p_ { k } $ is an empirical distribution . However , the overfitting issue is not unique to our approach . Generative models that work via the maximizing likelihood principle all have this potential problem . This is because maximizing likelihood is equivalent to minimizing the KL divergence , and a KL divergence of 0 indicates that the generated distribution and target distribution are identical \u2013 for empirical distributions , identical means the supports of the two distributions form a one-to-one match , which is exactly the definition of \u201c memorization \u201d . GANs is little bit different since it works by minimizing the JS divergence , but for empirical distributions a JS divergence of 0 also means that samples of two distributions form a one-to-one match . When that happens , the generator can only generate what are already in the target training data . Second , our analysis of Figure 1 scenario 1 uses two assumptions , one is that the perturbation space covers the support of $ p_ { k } $ , and the other is that $ D $ has enough capacity ( which is not always true in practice especially for high dimensional complex data ) . Regarding the first assumption , our training algorithm ( Algorithm 3 ) uses $ K $ to control the perturbation size - this actually provides a mechanism for constraining the perturbation space and mitigating the overfitting problem . In the appendix we also have results demonstrating this effect : models trained with different $ K $ tend to generate samples of different levels of fidelity . * * In the Celeb A results in the middle of Figure 3 , notice how the images ( 4,1 ) , ( 4,3 ) look almost exactly the same , and they are also very similar to ( 1,3 ) , ( 3,1 ) and ( 3,4 ) . * * * * In figure 2b and 2c , notice how the distribution all collapses to 1 point , I wonder whether this is one perspective or intuition on this problem . * * This is indeed a limitation of our approach . For the 2D case , it seems that the D model we obtained via the algorithm has a single global maximum ( Figure 2b ) or a few global maxima ( Figure 2c ) . In these cases , performing gradient ascent on $ D $ causes the $ p_ { -k } $ data to be concentrated on these maxima points . For the face experiment , it seems that the samples are stuck at local maxima points , as the resulting images do not resemble real face images ; several images look the same because their seed images are trapped at the same local maximum point . This issue tends to happen when the source images are not diverse enough . For instance , in Figure 17 we can see when source images are Gaussian noise or uniform noise the resulting images looks quite similar . We think at least for the case where $ D $ has no local maxima , this issue could be mitigated by properly controlling number of steps and step size when performing gradient ascent on $ D $ . We are also looking into the SGLD algorithm suggested by reviewer 1 . We have included a discussion about this limitation on Page 9 . * * Unverified claim : At the bottom of page 7 , \u201c These results suggest that with a high capacity model and proper training , a robust OOD detection system is within reach. \u201d where these results referred to reducing the data complexity . I think the results in this paper are not enough to make this claim . The authors \u2019 logic here is that , if their model performs better on a simple dataset , then it implies the problem is insufficient model capacity . The assumption here is that the model can scale , which is completely unverified . Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet . An unverified claim like this is always a warning sign as it may mislead the readers and community . * * We agree that the CIFAR10-classes 0 experiment is insufficient , as it only focuses on the \u201c data complexity \u201d side . We conducted an additional experiment where we stick to the same CIFAR10 data but used ResNet18 as the $ D $ model . ResNet18 is much larger than the default model of ResNet-CIFAR in terms of disk space ( 43MB vs. 4.6MB ) , but the performance increase is only marginal ( Appendix H2 ) . Given these results we are unable to conclude that it is \u201c simply a capacity and data complexity \u201d issue . We have revised this paragraph and removed the unwarranted claim ( Page 7 ) ."}, {"review_id": "PsdsEbzxZWr-2", "review_text": "The authors investigated generative adversarial training ( GAT ) and made two main contributions : 1 ) theoretically analyzed its maxmin objective and compared with the minmax formulation used by GANs , and 2 ) applied GAT to OOD detection and generative models . Extensive experiments were performed for evaluating the proposed algorithm . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strong points : . The authors analyzed the maxmin formulation of GAT theoretically by deriving the optimal solutions under different scenarios , and also showed the conditions under which the algorithm converges to the optimal solution . .The authors pointed out the difference between the maxmin formulation and the minmax formulation used by GANs , and extended GAT for two applications : OOD detection and generative models . .Extensive experiments were performed for evaluating the proposed algorithm . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Weak points : . In Section 3.1 , for the convenience of analysis , the authors transformed equation ( 4 ) to ( 5 ) . Are these two optimization problems equivalent ? Seems not . Please add brief explanation . .Algorithm 1 is claimed to solve equation ( 5 ) . However , in Step 2 the maximization is still over B ( x , \\epsilon ) . It seems that Algorithm 1 is to solve equation ( 4 ) . Please explain . .Section 3.2 : a practical consideration is that Step 2 can not be perfectly solved and the authors thus proposed to use a p_ { -k } that is uniformly distributed in the data space . The authors claimed that such condition always results in no maxima and global maxima at Supp ( p_k ) . Can this conclusion be theoretically proved ? One potential limitation is that using a p_ { -k } that is uniformly distributed could lead to a long training time for achieving a satisfactory performance especially in high dimensional . .OOD detection is an active area and there are a lot of papers regarding this . In experiment , the authors mainly focused on evaluating the proposed algorithm , and did not compare it with any other OOD detection methods , e.g. , log-likelihood based and likelihood ratio based . Although K = 0 can be thought of as exposing to outliers , the resulting algorithm is still under GAT framework . As OOD detection is suggested as a main application of GAT , it is important to compare it with state-of-the-art . .It is hard to compare the quality of generation by only observing the generated images . It 's better to show some quantitative comparison by using , e.g , FID score , which is commonly adopted for evaluating GANs . # # # # # # # # # # # # # # # # # # # # Some typos : . In equation ( 3 ) , should be \\lambda .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , Thank you very much for your time and constructive feedback ! We have conducted more experiments and revised the paper to address you concerns . Below , we respond to each of your comments and look forward to your further feedback . * * In Section 3.1 , for the convenience of analysis , the authors transformed equation ( 4 ) to ( 5 ) . Are these two optimization problems equivalent ? Seems not . Please add brief explanation . * * * * Algorithm 1 is claimed to solve equation ( 5 ) . However , in Step 2 the maximization is still over B ( x , \\epsilon ) . It seems that Algorithm 1 is to solve equation ( 4 ) . Please explain . * * Thanks for pointing this out . Problem ( 4 ) to ( 5 ) are equivalent when $ \\mathbb { B } ( x , \\epsilon ) =\\mathcal { S } $ , which we have assumed ( \u201c instead of using $ \\epsilon $ -balls imposed on individual data samples , we use the notion of a common perturbation space\u2026 \u201d ) but didn \u2019 t explicitly stated . Similarly , we should have mentioned that Algorithm 1 solves problem ( 4 ) only when $ \\mathbb { B } ( x , \\epsilon ) =\\mathcal { S } $ . We have made this more explicit in the updated manuscript . * * OOD detection is an active area and there are a lot of papers regarding this . In experiment , the authors mainly focused on evaluating the proposed algorithm , and did not compare it with any other OOD detection methods , e.g. , log-likelihood based and likelihood ratio based . Although K = 0 can be thought of as exposing to outliers , the resulting algorithm is still under GAT framework . As OOD detection is suggested as a main application of GAT , it is important to compare it with state-of-the-art . * * We completely agree . We have included a review of related work on OOD detection in appendix A . In the results section we also added a comparison with several baselines and state-of-the-art methods , for both the standard ( Table 3 ) and adversarial ( Table 4 ) OOD detection task . * * It is hard to compare the quality of generation by only observing the generated images . It 's better to show some quantitative comparison by using , e.g , FID score , which is commonly adopted for evaluating GANs . * * We agree . We have included the FID scores in table 8 . * * Some typos : In equation ( 3 ) , should be \\lambda . * * Thanks , we have fixed the notation error ."}], "0": {"review_id": "PsdsEbzxZWr-0", "review_text": "In this paper the authors provide a theoretical and empirical analysis of the Generative Adversarial Training method ( GAT ) which is used to train models for OOD and adversarial example detection . The GAT method is analyzed from a game theoretical prespective , focusing on the differences with GAN training , which are sometime conflated with it in the literature . The authors show that GAT and GAN have different training problems ( maximin vs minimax ) which have different optimal solutions . The authors also propose a variant of GAT called Unconstraned GAT , which replaces the PGD attack in the inner optimization loop with an unconstrained steepest ascent update . They propose this training algorithm for both OOD detection , adversarial OOD detection and generative sampling . They discuss the sensitivity of the proposed algorithm on the step size , which seems to critically depend on the model architecture and dataset . Experiments are performed on standard image datasets , using ImageNet as a source of known OOD examples . Overall the work is interesting , however I find some issues : - The key point of the theoretical anaysis comparing GAT with GAN is only made at the end . It would be better to mention in the introduction at a high level why GAT is preferable to GAN . - Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with . They should be defined , possibly first in an intuitive way and then formally . - The proposed algorithm seem quite sensitive on the step size . This might limit its practical applicability ( minor issue : the step size is denoted as lambda in the text and gamma in the algorithm box ) . - The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample . You would need to use an SGLD-like algorithm in order get unbiased samples . EDIT : The revision addressed my concerns , I 'm raising my evaluation to 7 .", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer : Thank you very much for your time and insightful comments ! We have revised the paper based on your feedback . Below we response to your questions . * * The key point of the theoretical analysis comparing GAT with GAN is only made at the end . It would be better to mention in the introduction at a high level why GAT is preferable to GAN . * * Thanks for the suggestion . We have included in the introduction a brief discussion about the relative merit of the proposed approach . * * Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with . They should be defined , possibly first in an intuitive way and then formally . * * Thanks for the suggestion . In Appendix B we have included a discussion about these two problems in the game theory framework . To make the materials more accessible , in Appendix C we also included a demonstration of the general strategy for solving a maximin problem . * * The proposed algorithm seem quite sensitive on the step size . This might limit its practical applicability ( minor issue : the step size is denoted as lambda in the text and gamma in the algorithm box ) . * * We agree that the step size could be a potential problem . However , just as our failure mode diagnosis indicates , the problem of step size being too large can be detected in an early stage of training . We also observed that our algorithm works as long as the step size is below a certain threshold . Given these information , we think a working step size can be quickly found using binary search . Besides , in the paper we have provided the step size values for future work \u2019 s reference . Thanks , we have fixed the notation error . * * The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample . You would need to use an SGLD-like algorithm in order get unbiased samples . * * Thanks for the suggestion . This is a known limitation of our method and we have included a discussion about this issue on Page 9 . We think this issue might be mitigated by constraining the number of steps and step sizes , as the gradient-based generation process is governed by these two parameters . The suggested algorithm looks new and interesting to us , and we will definitely look into it ."}, "1": {"review_id": "PsdsEbzxZWr-1", "review_text": "* * 1.Summary and contributions : Briefly summarize the paper and its contributions * * This work analyzed the optimal solutions for the Generative adversarial training ( GAT ) and the convergence property of the training algorithm . This work also compared the minimax and maximin games , both theoretically , and empirically , with the help of a nice 2D toy example . This work also developed an unconstrained version of GAT , and evaluated it on image generation and out of distribution detection tasks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 2 . Strengths : Describe the strengths of the work . Typical criteria include : soundness of the claims ( theoretical grounding , empirical evaluation ) , significance and novelty of the contribution , and relevance to the community . * * I found the theoretical analysis to be interesting , and I especially like the 2D toy experiment in Figure 2 , it strongly and clearly justified the importance of using a uniformly distributed p_ ( -k ) distribution . I also liked the thorough discussion of the distinction between the minimax and maximin games . The paper has interesting ideas and a lot of content , it also introduced adversarial OOD samples , which are all very interesting to me . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 3 . Weaknesses : Explain the limitations of this work along the same axes as above . * * Memorization : I think optimizing the D approach is problematic in terms of memorization , there \u2019 s nothing stopping the model to memorize the data , especially in the high dimensional space which makes the mass distribution to be very sparse . In the Celeb A results in the middle of Figure 3 , notice how the images ( 4,1 ) , ( 4,3 ) look almost exactly the same , and they are also very similar to ( 1,3 ) , ( 3,1 ) and ( 3,4 ) . In figure 2b and 2c , notice how the distribution all collapses to 1 point , I wonder whether this is one perspective or intuition on this problem . Unverified claim : At the bottom of page 7 , \u201c These results suggest that with a high capacity model and proper training , a robust OOD detection system is within reach. \u201d where these results referred to reducing the data complexity . I think the results in this paper are not enough to make this claim . The authors \u2019 logic here is that , if their model performs better on a simple dataset , then it implies the problem is insufficient model capacity . The assumption here is that the model can scale , which is completely unverified . Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet . An unverified claim like this is always a warning sign as it may mislead the readers and community . Lack of ablation study : I really liked the toy experiments in Figure 2 , which justified the use of uniform distribution in the data space . However this only provides intuition for the higher dimensional cases , it is still necessary to conduct an ablation study to verify this is indeed the case for higher-dimensional cases , for scientific rigor . Unfair comparison : For image generation results , the authors \u2019 method was compared with GANs , and the motivation is that both of the methods are trained adversarially . However , the generators of GANs never got to see the real data during train time . Here authors are optimizing D , which is trained on real data , thus I don \u2019 t think it is fair to compare with GANs . Weak baseline for OOD ( outlier exposure ) and insufficient comparison : Outlier exposure is no longer the state of the art OOD detection methods and there are so many other methods that perform better than outlier exposure on CIFAR10 , for example , see Detecting Out-of-Distribution Examples with Gram Matrices : https : //arxiv.org/abs/1912.12510 Lack of Related Work section : This paper does not have a related work section . The related works are very briefly discussed in the introduction , but I think that is far from enough . I understand there is a page limit , but even putting a related work section in the appendix would be very helpful for readers to get a more complete understanding of where this work stands . Unclear writings : See section 4 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 4 . Clarity : Is the paper well written ? * * Typos : At the top of page 3 : as a results - > as a result Bottom of page 8 : a OOD - > an OOD Ambiguity : \u201c elementary mass \u201d was not defined before being used , it \u2019 d be helpful to the readers to briefly define what \u201c elementary mass \u201d means in this specific context . \u201c In order to cause more local maxima to be eliminated \u201d could be worded better . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * 5 . Reasons for score * * In conclusion , the ideas are very interesting but I think there is a lot more work to be done before this paper could be accepted .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear reviewer , Thank you very much for your time and constructive feedback ! We have conducted more experiments and revised the paper according to your suggestions . Below , we respond to each of your comments and look forward to your further feedback . * * Memorization : I think optimizing the D approach is problematic in terms of memorization , there \u2019 s nothing stopping the model to memorize the data , especially in the high dimensional space which makes the mass distribution to be very sparse . * * We completely agree that our approach has the potential problem of overfitting . Our analysis says that in Figure 1 scenario 1 an ideal $ D $ solution has no local maxima and global maxima at support of $ p_ { k } $ , which means overfitting when $ p_ { k } $ is an empirical distribution . However , the overfitting issue is not unique to our approach . Generative models that work via the maximizing likelihood principle all have this potential problem . This is because maximizing likelihood is equivalent to minimizing the KL divergence , and a KL divergence of 0 indicates that the generated distribution and target distribution are identical \u2013 for empirical distributions , identical means the supports of the two distributions form a one-to-one match , which is exactly the definition of \u201c memorization \u201d . GANs is little bit different since it works by minimizing the JS divergence , but for empirical distributions a JS divergence of 0 also means that samples of two distributions form a one-to-one match . When that happens , the generator can only generate what are already in the target training data . Second , our analysis of Figure 1 scenario 1 uses two assumptions , one is that the perturbation space covers the support of $ p_ { k } $ , and the other is that $ D $ has enough capacity ( which is not always true in practice especially for high dimensional complex data ) . Regarding the first assumption , our training algorithm ( Algorithm 3 ) uses $ K $ to control the perturbation size - this actually provides a mechanism for constraining the perturbation space and mitigating the overfitting problem . In the appendix we also have results demonstrating this effect : models trained with different $ K $ tend to generate samples of different levels of fidelity . * * In the Celeb A results in the middle of Figure 3 , notice how the images ( 4,1 ) , ( 4,3 ) look almost exactly the same , and they are also very similar to ( 1,3 ) , ( 3,1 ) and ( 3,4 ) . * * * * In figure 2b and 2c , notice how the distribution all collapses to 1 point , I wonder whether this is one perspective or intuition on this problem . * * This is indeed a limitation of our approach . For the 2D case , it seems that the D model we obtained via the algorithm has a single global maximum ( Figure 2b ) or a few global maxima ( Figure 2c ) . In these cases , performing gradient ascent on $ D $ causes the $ p_ { -k } $ data to be concentrated on these maxima points . For the face experiment , it seems that the samples are stuck at local maxima points , as the resulting images do not resemble real face images ; several images look the same because their seed images are trapped at the same local maximum point . This issue tends to happen when the source images are not diverse enough . For instance , in Figure 17 we can see when source images are Gaussian noise or uniform noise the resulting images looks quite similar . We think at least for the case where $ D $ has no local maxima , this issue could be mitigated by properly controlling number of steps and step size when performing gradient ascent on $ D $ . We are also looking into the SGLD algorithm suggested by reviewer 1 . We have included a discussion about this limitation on Page 9 . * * Unverified claim : At the bottom of page 7 , \u201c These results suggest that with a high capacity model and proper training , a robust OOD detection system is within reach. \u201d where these results referred to reducing the data complexity . I think the results in this paper are not enough to make this claim . The authors \u2019 logic here is that , if their model performs better on a simple dataset , then it implies the problem is insufficient model capacity . The assumption here is that the model can scale , which is completely unverified . Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet . An unverified claim like this is always a warning sign as it may mislead the readers and community . * * We agree that the CIFAR10-classes 0 experiment is insufficient , as it only focuses on the \u201c data complexity \u201d side . We conducted an additional experiment where we stick to the same CIFAR10 data but used ResNet18 as the $ D $ model . ResNet18 is much larger than the default model of ResNet-CIFAR in terms of disk space ( 43MB vs. 4.6MB ) , but the performance increase is only marginal ( Appendix H2 ) . Given these results we are unable to conclude that it is \u201c simply a capacity and data complexity \u201d issue . We have revised this paragraph and removed the unwarranted claim ( Page 7 ) ."}, "2": {"review_id": "PsdsEbzxZWr-2", "review_text": "The authors investigated generative adversarial training ( GAT ) and made two main contributions : 1 ) theoretically analyzed its maxmin objective and compared with the minmax formulation used by GANs , and 2 ) applied GAT to OOD detection and generative models . Extensive experiments were performed for evaluating the proposed algorithm . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strong points : . The authors analyzed the maxmin formulation of GAT theoretically by deriving the optimal solutions under different scenarios , and also showed the conditions under which the algorithm converges to the optimal solution . .The authors pointed out the difference between the maxmin formulation and the minmax formulation used by GANs , and extended GAT for two applications : OOD detection and generative models . .Extensive experiments were performed for evaluating the proposed algorithm . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Weak points : . In Section 3.1 , for the convenience of analysis , the authors transformed equation ( 4 ) to ( 5 ) . Are these two optimization problems equivalent ? Seems not . Please add brief explanation . .Algorithm 1 is claimed to solve equation ( 5 ) . However , in Step 2 the maximization is still over B ( x , \\epsilon ) . It seems that Algorithm 1 is to solve equation ( 4 ) . Please explain . .Section 3.2 : a practical consideration is that Step 2 can not be perfectly solved and the authors thus proposed to use a p_ { -k } that is uniformly distributed in the data space . The authors claimed that such condition always results in no maxima and global maxima at Supp ( p_k ) . Can this conclusion be theoretically proved ? One potential limitation is that using a p_ { -k } that is uniformly distributed could lead to a long training time for achieving a satisfactory performance especially in high dimensional . .OOD detection is an active area and there are a lot of papers regarding this . In experiment , the authors mainly focused on evaluating the proposed algorithm , and did not compare it with any other OOD detection methods , e.g. , log-likelihood based and likelihood ratio based . Although K = 0 can be thought of as exposing to outliers , the resulting algorithm is still under GAT framework . As OOD detection is suggested as a main application of GAT , it is important to compare it with state-of-the-art . .It is hard to compare the quality of generation by only observing the generated images . It 's better to show some quantitative comparison by using , e.g , FID score , which is commonly adopted for evaluating GANs . # # # # # # # # # # # # # # # # # # # # Some typos : . In equation ( 3 ) , should be \\lambda .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , Thank you very much for your time and constructive feedback ! We have conducted more experiments and revised the paper to address you concerns . Below , we respond to each of your comments and look forward to your further feedback . * * In Section 3.1 , for the convenience of analysis , the authors transformed equation ( 4 ) to ( 5 ) . Are these two optimization problems equivalent ? Seems not . Please add brief explanation . * * * * Algorithm 1 is claimed to solve equation ( 5 ) . However , in Step 2 the maximization is still over B ( x , \\epsilon ) . It seems that Algorithm 1 is to solve equation ( 4 ) . Please explain . * * Thanks for pointing this out . Problem ( 4 ) to ( 5 ) are equivalent when $ \\mathbb { B } ( x , \\epsilon ) =\\mathcal { S } $ , which we have assumed ( \u201c instead of using $ \\epsilon $ -balls imposed on individual data samples , we use the notion of a common perturbation space\u2026 \u201d ) but didn \u2019 t explicitly stated . Similarly , we should have mentioned that Algorithm 1 solves problem ( 4 ) only when $ \\mathbb { B } ( x , \\epsilon ) =\\mathcal { S } $ . We have made this more explicit in the updated manuscript . * * OOD detection is an active area and there are a lot of papers regarding this . In experiment , the authors mainly focused on evaluating the proposed algorithm , and did not compare it with any other OOD detection methods , e.g. , log-likelihood based and likelihood ratio based . Although K = 0 can be thought of as exposing to outliers , the resulting algorithm is still under GAT framework . As OOD detection is suggested as a main application of GAT , it is important to compare it with state-of-the-art . * * We completely agree . We have included a review of related work on OOD detection in appendix A . In the results section we also added a comparison with several baselines and state-of-the-art methods , for both the standard ( Table 3 ) and adversarial ( Table 4 ) OOD detection task . * * It is hard to compare the quality of generation by only observing the generated images . It 's better to show some quantitative comparison by using , e.g , FID score , which is commonly adopted for evaluating GANs . * * We agree . We have included the FID scores in table 8 . * * Some typos : In equation ( 3 ) , should be \\lambda . * * Thanks , we have fixed the notation error ."}}