{"year": "2019", "forum": "ryeYHi0ctQ", "title": "DPSNet: End-to-end Deep Plane Sweep Stereo", "decision": "Accept (Poster)", "meta_review": "A deep neural network pipeline for multiview stereo is presented. After rebuttal and discussion, all reviewers learn toward accepting the paper. Reviewer3 points to good results, but is concerned that the technical aspects are somewhat straightforward, and thus the contribution in this area is limited. The AC concurs with the reviewers.", "reviews": [{"review_id": "ryeYHi0ctQ-0", "review_text": "Summary This paper proposes an end-to-end learnable multiview stereo depth estimation network, which is basically very similar to the GCNet (Kendall et.al 2017) or PSMNet (Chang et.al 2018) for stereo estimation. The differences are using SPN to warp feature w.r.t RT, adding a multi view averaging cost and a cost aggregation component for final depth regression, which transform the original network to support multi-view stereo, yielding performance boost over other baselines. Technically, I believe it is sound because cost volume from stereo matching has already been demonstrate very effective in boosting performance because it use underlining geometry constraint. My major concern lies in three aspects. 1) Another most recent SOTA algorithm is MVSNet (Yao et.al ECCV 2018), the paper should be considered for comparison. In addition, the structure is even more similar with the proposed network architecture. 2) The evaluation metrics are mostly use for single view depths, it is not consistent with paper of DeepMVS (Tab. 1) or that from MVSNet. Therefore, it might be hard to actual understand whether the numbers are exactly comparable. 3) Since the method largely improved over their baseline algorithms, and the number between different papers are hard to compare. In my opinion, to better show the results, I suggest submitting results to an online benchmark with test data for verifying the results. such as ETH3D multi view benchmark, where everything is standardized. I hope the author can make strong feedback for validating the results. ####### . After rebuttal The author makes more clear indication of the performance contribution of the completeness of recovery.", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 ) Another most recent SOTA algorithm is MVSNet ( Yao et.al ECCV 2018 ) , the paper should be considered for comparison . In addition , the structure is even more similar with the proposed network architecture . Thanks for your suggestion to improve our experiments . Following your comment , we have added a description of MVSNet at the end of Sec.2 and mentioned the differences from our DPSNet . Traditional stereo matching and multiview stereo consist of four steps : initial matching , cost aggregation , multi-label optimization and refinement . In our opinion , MVSNet mainly focuses on computing initial matching and refinement . Our contributions are mainly on initial matching and cost aggregation . Even though the main scheme for warping is similar , the cost volume generation and aggregation approaches are different . MVSNet was developed concurrently to our work , and there are differences in contributions . In particular , ( 1 ) our DPSNet concatenates the reference image features and warped pair image features , then builds a cost volume . If multiple views are available ( more than 3 views ) , we iteratively compute the cost volume and average them . This cost volume generation strategy can produce a more confident cost volume as more views are matched . Moreover , we show that our DPSNet can be generalized to binocular stereo , while MVSNet builds a cost volume based on the variance of features , which fails to estimate accurate depth values when only using two views . ( 2 ) Our novel cost aggregation network shows significant performance improvements on textureless regions over MVSNet \u2019 s refinement with a reference image feature . ( 3 ) Both the minimum and maximum depth range are user-defined parameters and scene-dependent . As an alternative , our DPSNet uniformly samples matching planes in inverse-depth scale , which is beneficial for alleviating depth quantization error and reconstructing scenes with large depth ranges . We can not say with certainty which method performs better . What we can say clearly is that both methods have shown great performance improvements by applying traditional techniques used in multiview stereo into deep learning architectures . In other words , both studies have independent academic contributions . For a comparison to MVSNet , please refer to the next response ."}, {"review_id": "ryeYHi0ctQ-1", "review_text": "The paper describes a method for learning a deep neural network for multi-view stereo. The overall network includes feature-extraction layers applied to all images, followed by a spatial-transformer network (which is differentiable, but with no learnable parameters) that is applied to warp these features from every matching image to the reference image's co-ordinate frame for a series of candidate depth planes, followed by concatenation of the reference and match image features and 3D convolution layers to form a cost volume. The cost volumes of different pairs are averaged, and additional layers are used to refine this cost volume while relying on the reference image's RGB features, followed by soft-max and an expectation over depth values to output the final depth at each pixel. The entire network is trained end-to-end and experiments show that it outperforms state-of-the-art methods for MVS by a significant margin on a number of datasets. Overall, I have a positive view of the paper and believe it should be accepted to ICLR. However, I would like the authors to address the following issues: - While the proposed network is complex, I do believe the description of the architecture could be a little better. It would be good to clarify that i indexes view (and N is the total number of views), and provide a few more definitions for the terms in equation (2): namely, are R and t the extrinsics of the reference camera or the i^th camera, etc. The overall approach is clear (for each plane, the method maps features from the paired camera to the reference camera assuming all points in the the world lie on that plane), but it would be good to clarify the specifics. It might also be useful to emphasize that the cost-volume generation is per-pair (perhaps change the title of Sec 3.2) and that these volumes are averaged for all pairs. - It might also be useful to apply the algorithm to the rectified binocular stereo case (where the warping and definition of planes by disparity are much simpler), and show comparisons to the many stereo algorithms on datasets like KITTI. At some level, the proposed algorithm can be thought of taking approaches proved to be successful for rectified binocular stereo and generalizing them (by generic warping + plane sweep) to the multi-view case. Hence, such comparisons could be illuminating. (Note: the method doesn't need to outperform the state-of-the-art there, but the results would be informative). - I do believe the paper would significantly benefit from more discussion of DeepMVS since it's clearly the closest to this method (also solves MVS by deep networks + plane sweep). DeepMVS also learns the matching cost for cost volume generation, and the major difference seems to be that this method is learned end-to-end. It would be better to have a more detailed discussion of the differences (the current discussion at the end of Sec 2 is a little short on details)---architectures, super-vision at the end of the cost-volume vs end-to-end, etc. ", "rating": "7: Good paper, accept", "reply_text": "1 ) While the proposed network is complex , I do believe the description of the architecture could be a little better . It would be good to clarify that i indexes view ( and N is the total number of views ) , and provide a few more definitions for the terms in equation ( 2 ) : namely , are R and t the extrinsics of the reference camera or the i^th camera , etc . The overall approach is clear ( for each plane , the method maps features from the paired camera to the reference camera assuming all points in the the world lie on that plane ) , but it would be good to clarify the specifics . It might also be useful to emphasize that the cost-volume generation is per-pair ( perhaps change the title of Sec 3.2 ) and that these volumes are averaged for all pairs . Thanks for your suggestions to improve our manuscript . Following your comments , we have clarified these parts by adding descriptions in Sec 3.2 . In the first paragraph of Sec.3.2 , we explain that our cost volume generation is based on unstructured two-view images and can be extended to multiview matching by averaging other cost volumes to reduce a negative effect of image noise . In particular , we have changed the title of Sec 3.2 from \u201c Cost Volume Generation \u201d to \u201c Cost Volume Generation using Unstructured Two-View Images \u201d to highlight that the cost volume is computed from an image pair . We also added explanations for the notations i , R and t in the third paragraph of Sec.3.2 ."}, {"review_id": "ryeYHi0ctQ-2", "review_text": "This paper proposes a method for stereo reconstruction using Deep Learning. Like some previous methods, a 'cost volume' is first computed by plane sweeping, in other words the cost volume is indexed by the 2D locations in the image plane, and the disparities for 3D planes parallel to the image plane. A network then predicts the disparities for each image location from this cost volume. The contributions with respect to the state-of-the-art are: - the cost volume is computed using differential warps, thus the network can be trained end-to-end; - a better cost volume is computed from the original cost volume and the reference image. The results look good, both quantitatively and qualitatively. The paper reads well, and related work is correctly referenced. There is nothing wrong with the proposed method, it makes sense and I am convinced it works well. However, I found the contributions quite straightforward, and it is difficult to get excited about the paper. More details would be welcome for Section 3.2", "rating": "6: Marginally above acceptance threshold", "reply_text": "More details would be welcome for Section 3.2 Thanks for your positive comments on our paper . Since your comment is the same as that of Reviewer 1 , please refer to the response 1 to Reviewer 1 \u2019 s question and check our revised manuscript ."}], "0": {"review_id": "ryeYHi0ctQ-0", "review_text": "Summary This paper proposes an end-to-end learnable multiview stereo depth estimation network, which is basically very similar to the GCNet (Kendall et.al 2017) or PSMNet (Chang et.al 2018) for stereo estimation. The differences are using SPN to warp feature w.r.t RT, adding a multi view averaging cost and a cost aggregation component for final depth regression, which transform the original network to support multi-view stereo, yielding performance boost over other baselines. Technically, I believe it is sound because cost volume from stereo matching has already been demonstrate very effective in boosting performance because it use underlining geometry constraint. My major concern lies in three aspects. 1) Another most recent SOTA algorithm is MVSNet (Yao et.al ECCV 2018), the paper should be considered for comparison. In addition, the structure is even more similar with the proposed network architecture. 2) The evaluation metrics are mostly use for single view depths, it is not consistent with paper of DeepMVS (Tab. 1) or that from MVSNet. Therefore, it might be hard to actual understand whether the numbers are exactly comparable. 3) Since the method largely improved over their baseline algorithms, and the number between different papers are hard to compare. In my opinion, to better show the results, I suggest submitting results to an online benchmark with test data for verifying the results. such as ETH3D multi view benchmark, where everything is standardized. I hope the author can make strong feedback for validating the results. ####### . After rebuttal The author makes more clear indication of the performance contribution of the completeness of recovery.", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 ) Another most recent SOTA algorithm is MVSNet ( Yao et.al ECCV 2018 ) , the paper should be considered for comparison . In addition , the structure is even more similar with the proposed network architecture . Thanks for your suggestion to improve our experiments . Following your comment , we have added a description of MVSNet at the end of Sec.2 and mentioned the differences from our DPSNet . Traditional stereo matching and multiview stereo consist of four steps : initial matching , cost aggregation , multi-label optimization and refinement . In our opinion , MVSNet mainly focuses on computing initial matching and refinement . Our contributions are mainly on initial matching and cost aggregation . Even though the main scheme for warping is similar , the cost volume generation and aggregation approaches are different . MVSNet was developed concurrently to our work , and there are differences in contributions . In particular , ( 1 ) our DPSNet concatenates the reference image features and warped pair image features , then builds a cost volume . If multiple views are available ( more than 3 views ) , we iteratively compute the cost volume and average them . This cost volume generation strategy can produce a more confident cost volume as more views are matched . Moreover , we show that our DPSNet can be generalized to binocular stereo , while MVSNet builds a cost volume based on the variance of features , which fails to estimate accurate depth values when only using two views . ( 2 ) Our novel cost aggregation network shows significant performance improvements on textureless regions over MVSNet \u2019 s refinement with a reference image feature . ( 3 ) Both the minimum and maximum depth range are user-defined parameters and scene-dependent . As an alternative , our DPSNet uniformly samples matching planes in inverse-depth scale , which is beneficial for alleviating depth quantization error and reconstructing scenes with large depth ranges . We can not say with certainty which method performs better . What we can say clearly is that both methods have shown great performance improvements by applying traditional techniques used in multiview stereo into deep learning architectures . In other words , both studies have independent academic contributions . For a comparison to MVSNet , please refer to the next response ."}, "1": {"review_id": "ryeYHi0ctQ-1", "review_text": "The paper describes a method for learning a deep neural network for multi-view stereo. The overall network includes feature-extraction layers applied to all images, followed by a spatial-transformer network (which is differentiable, but with no learnable parameters) that is applied to warp these features from every matching image to the reference image's co-ordinate frame for a series of candidate depth planes, followed by concatenation of the reference and match image features and 3D convolution layers to form a cost volume. The cost volumes of different pairs are averaged, and additional layers are used to refine this cost volume while relying on the reference image's RGB features, followed by soft-max and an expectation over depth values to output the final depth at each pixel. The entire network is trained end-to-end and experiments show that it outperforms state-of-the-art methods for MVS by a significant margin on a number of datasets. Overall, I have a positive view of the paper and believe it should be accepted to ICLR. However, I would like the authors to address the following issues: - While the proposed network is complex, I do believe the description of the architecture could be a little better. It would be good to clarify that i indexes view (and N is the total number of views), and provide a few more definitions for the terms in equation (2): namely, are R and t the extrinsics of the reference camera or the i^th camera, etc. The overall approach is clear (for each plane, the method maps features from the paired camera to the reference camera assuming all points in the the world lie on that plane), but it would be good to clarify the specifics. It might also be useful to emphasize that the cost-volume generation is per-pair (perhaps change the title of Sec 3.2) and that these volumes are averaged for all pairs. - It might also be useful to apply the algorithm to the rectified binocular stereo case (where the warping and definition of planes by disparity are much simpler), and show comparisons to the many stereo algorithms on datasets like KITTI. At some level, the proposed algorithm can be thought of taking approaches proved to be successful for rectified binocular stereo and generalizing them (by generic warping + plane sweep) to the multi-view case. Hence, such comparisons could be illuminating. (Note: the method doesn't need to outperform the state-of-the-art there, but the results would be informative). - I do believe the paper would significantly benefit from more discussion of DeepMVS since it's clearly the closest to this method (also solves MVS by deep networks + plane sweep). DeepMVS also learns the matching cost for cost volume generation, and the major difference seems to be that this method is learned end-to-end. It would be better to have a more detailed discussion of the differences (the current discussion at the end of Sec 2 is a little short on details)---architectures, super-vision at the end of the cost-volume vs end-to-end, etc. ", "rating": "7: Good paper, accept", "reply_text": "1 ) While the proposed network is complex , I do believe the description of the architecture could be a little better . It would be good to clarify that i indexes view ( and N is the total number of views ) , and provide a few more definitions for the terms in equation ( 2 ) : namely , are R and t the extrinsics of the reference camera or the i^th camera , etc . The overall approach is clear ( for each plane , the method maps features from the paired camera to the reference camera assuming all points in the the world lie on that plane ) , but it would be good to clarify the specifics . It might also be useful to emphasize that the cost-volume generation is per-pair ( perhaps change the title of Sec 3.2 ) and that these volumes are averaged for all pairs . Thanks for your suggestions to improve our manuscript . Following your comments , we have clarified these parts by adding descriptions in Sec 3.2 . In the first paragraph of Sec.3.2 , we explain that our cost volume generation is based on unstructured two-view images and can be extended to multiview matching by averaging other cost volumes to reduce a negative effect of image noise . In particular , we have changed the title of Sec 3.2 from \u201c Cost Volume Generation \u201d to \u201c Cost Volume Generation using Unstructured Two-View Images \u201d to highlight that the cost volume is computed from an image pair . We also added explanations for the notations i , R and t in the third paragraph of Sec.3.2 ."}, "2": {"review_id": "ryeYHi0ctQ-2", "review_text": "This paper proposes a method for stereo reconstruction using Deep Learning. Like some previous methods, a 'cost volume' is first computed by plane sweeping, in other words the cost volume is indexed by the 2D locations in the image plane, and the disparities for 3D planes parallel to the image plane. A network then predicts the disparities for each image location from this cost volume. The contributions with respect to the state-of-the-art are: - the cost volume is computed using differential warps, thus the network can be trained end-to-end; - a better cost volume is computed from the original cost volume and the reference image. The results look good, both quantitatively and qualitatively. The paper reads well, and related work is correctly referenced. There is nothing wrong with the proposed method, it makes sense and I am convinced it works well. However, I found the contributions quite straightforward, and it is difficult to get excited about the paper. More details would be welcome for Section 3.2", "rating": "6: Marginally above acceptance threshold", "reply_text": "More details would be welcome for Section 3.2 Thanks for your positive comments on our paper . Since your comment is the same as that of Reviewer 1 , please refer to the response 1 to Reviewer 1 \u2019 s question and check our revised manuscript ."}}