{"year": "2019", "forum": "rylV6i09tX", "title": "Interpreting Adversarial Robustness: A View from Decision Surface in Input Space", "decision": "Reject", "meta_review": "This paper studies the relationship between flatness in parameter space and generalization. They show through visualization experiments on MNIST and CIFAR-10 that there is no obvious relationship between the two. However, the reviewers found the motivation for the visualization approach unconvincing and further found significant overlap between the proposed method and that of Ross & Doshi. Thus the paper should improve its framing, experimental insights and relation to prior work before being ready for publication.", "reviews": [{"review_id": "rylV6i09tX-0", "review_text": "This paper argues that analyzing loss surfaces in parameter space for the purposes of evaluating adversarial robustness and generalization is ineffective, while measuring input loss surfaces is more accurate. By converting loss surfaces to decision surfaces (which denote the difference between the max and second highest logit), the authors show that all adversarial attack methods appear similar wrt the decision surface. This result is then related to the statistics of the input Jacobian and Hessian, which are shown to differ across adversarially sensitive and robust models. Finally, a regularization method based on regularizing the input Jacobian is proposed and evaluated. All of these results are shown through experiments on MNIST and CIFAR-10. In general, the paper is clear, though there are a number of typos. With respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references). Finally, regarding significance, many of the insights provided this paper are true by definition, and are therefore unlikely to have a significant impact. While I strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend. Please see the comments below for more detail. Major comments: 1) In the beginning of the paper, adversarial robustness and generalization are equated. However, adversarial robustness and generalization are not necessarily equivalent, and in fact, several papers have provided evidence against this notion, showing that adversarial inputs are likely to be present even for very good models [5, 6] and that adversarially sensitive models can often generalize quite well [2]. Moreover, all the experiments within the paper only address adversarial robustness rather than generalization to unperturbed samples. 2) One of the main results of this paper is that the loss surface wrt input space is more sensitive to adversarial perturbations than the loss surface wrt parameter space. Because adversarial inputs are defined in input space, by definition, the loss surface wrt to the input must be sensitive to adversarial examples. This result therefore appears true by definition. Moreover, [3] related the input Jacobian to generalization, finding a similar result, but is not discussed or cited. 3) The main result of Section 3 is that all adversarial attacks \u201cutilize the decision surface geometry properties to cross the decision boundary within least distance.\u201d While to my knowledge the decision surface visualization is novel and might have important uses, this statement is again true by definition, given that adversarial attack methods try to find the smallest perturbation which changes the network decision. As a result, all methods must find directions which are short paths in the decision surface. It is therefore unclear what additional insight this analysis presents. 4) How does measuring the loss landscape as an indicator for adversarial robustness differ from simply trying to find adversarial examples as is common? If anything, it seems it should be more computationally expensive as points are sampled in a grid search vs optimized for. 5) The proposed regularizer for adversarial robustness, based on regularizing the input Jacobian, is very similar to what was proposed in [1], yet [1] is not discussed or cited. Minor comments: 1) The paper\u2019s first sentence states that \u201cIt is commonly believed that a neural network\u2019s generalization is correlated to ...the flatness of the local minima in parameter space.\u201d However, [4] showed several years ago that the local minima flatness can be arbitrarily rescaled and has been fairly influential. While [4] is cited in the paper, it is only cited in the related work section as support for the statement that local minima flatness is related to generalization when this is precisely opposite the point this paper makes. [4] should be discussed in more detail, both in the introduction and the related work section. 2) The paper is quite lengthy, going right up against the hard 10 page limit. While this may be acceptable for papers with large figures or which require the extra space, this paper does not currently meet that threshold. 3) Throughout the figures, axes should be labeled. 4) In section 2.2, it is stated that both networks achieve optimal accuracy of ~90% on CIFAR-10. This is not optimal accuracy and hasn\u2019t been for several years [7]. 5) Why is equation 2 calculated with respect to the logit layer vs the normalized softmax layer? Using the unnormalized logits may introduce noise due to scaling. 6) In Figure 8, the scales of the Hessian are extremely different. Does this impact the measurement of sparseness? Typos: 1) Introduction, second paragraph: \u201cFor example, ResNet model usually converges to\u2026\u201d should be \u201cFor example, ResNet models usually converge to\u2026\u201d 2) Introduction, second paragraph: \u201c...defected by the adversarial noises...\u201d should be \u201c...defected by adversarial noise\u2026\u201d 3) Introduction, third paragraph: \u201c...introduced by adversarial noises...\u201d should be \u201c...introduced by adversarial noise\u2026\u201d 4) Section 3.1, first paragraph: \u201ccross entropy based loss surface is\u2026\u201d should be \u201ccross entropy based loss surfaces is\u2026\u201d [1] Jakubovitz, Daniel, and Raja Giryes. \"Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization.\" arXiv preprint arXiv:1803.08680 (2018). ECCV 2018. [2] Zahavy, Tom, et al. \"Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms.\" arXiv preprint arXiv:1602.02389 (2016). ICLR Workshop 2018 [3] Novak, Roman, et al. \"Sensitivity and generalization in neural networks: an empirical study.\" arXiv preprint arXiv:1802.08760 (2018). ICLR 2018. [4] Dinh, Laurent, et al. \"Sharp Minima Can Generalize For Deep Nets.\" International Conference on Machine Learning. 2017. [5] Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. \"Adversarial vulnerability for any classifier.\" arXiv preprint arXiv:1802.08686 (2018). NIPS 2018. [6] Gilmer, Justin, et al. \"Adversarial spheres.\" arXiv preprint arXiv:1801.02774 (2018). ICLR Workshop 2018. [7] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html ", "rating": "3: Clear rejection", "reply_text": "Thanks for the reviewer \u2019 s precious comments ! We do believe that there is a lot to be improved in this paper ! 1 ) About the first concern of the reviewer , the reason of \u201c equated generalization \u201d is that previously upon the finish of this paper , there is no clear definition of the generalization in the adversarial settings . And our statement is actually saying that \u201c adversarial robust generalization \u201d doesn \u2019 t equal to \u201c standard generalization \u201d , as in the NeurIPS \u2019 18 tutorial slides , https : //media.neurips.cc/Conferences/NIPS2018/Slides/adversarial_ml_slides_parts_1_4.pdf , page 29-30 . We can now give the formal definition maybe in the future paper , thanks for the advice . 2 ) For the second concern , by briefly comparing weight/input space visualization , the main conclusion we want to draw is that , past generalization analysis can not be well adopted in the adversarial settings . I think this should make more sense . 3 ) About the sec.3 , we don \u2019 t agree with the reviewer that the visualization insights are trivial because the visualization results are one of the key intuition , which is \u201c The geometry slopes ( gradients ) matter a lot in the model \u2019 s robustness \u201d and this is the motivation of why regulating jacobian could improve the robustness in the whole paper . 4 ) About the robustness indicator , we have shown a case study in the Sec4.3 ROBUSTNESS INDICATOR EVALUATION . We could compare two model \u2019 s Jacobian & Hessian to distinguish two model \u2019 s robustness easily , as shown in Fig.8 . This is not done by grid search , but by backpropagation to compute Jacobian and Hessian , therefore not computationally expensive . 5 ) About the Jacobian Regulation novelty , we do agree that the robust training part is not a significant contribution , as also mentioned by reviewer-2 . Our reply about the difference is here : https : //openreview.net/forum ? id=rylV6i09tX & noteId=HJgm8ciA3X under the reviewer-2 \u2019 s comments . 6 ) About the related work and other minor comments , thanks again for the precious comments !"}, {"review_id": "rylV6i09tX-1", "review_text": "The authors demonstrated that the loss surface visualized in parameter space does not reflect its robustness to adversarial examples. By analyzing the geometric properties of the loss surface in both parameter space and input space, they find input space is more appropriate in evaluating the generalization and adversarial robustness of a neural network. Therefore, they extend the loss surface to decision surface. They further visualized the adversarial attack trajectory on decision surfaces in input space, and formalized the adversarial robustness indicator. Finally, a robust training method guided by the indicator is proposed to smooth the decision surface. This paper is interesting and well organized. The idea of plotting loss surface in input space seems to be a natural extension of the loss surface w.r.t to weight change. The loss surface in input space measures the network\u2019s robustness to the perturbation of inputs, which naturally shows the influence of adversarial examples and is suitable for studying the robustness to adversarial examples. Note that loss surface in parameter space measures the network\u2019s robustness to the perturbation of the weights with given inputs, which implicitly assumes the data distribution is not significantly changed so that the loss surface may have similar geometry on the unseen data. The claim of \u201cthese significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space\u201d are not well supported as the comparison between Figure 2(a) and Figure 3 seems to be unfair and misleading. Fig 2 are plotted based on input data without any adversarial examples. So it is expected to see that Fig 2(a) and Fig 2(b) have similar contours. However, the loss surface in weight space may still be able to show their difference if the they are both plotted with the adversarial inputs. I believe that models trained by Min-Max robust training will be more stable in comparison with the normally trained models. It would be great if the author provide such plots. I would expect the normal model to have a high and flat surface while the robust model shows reasonable loss with small changes in weight space. How to choose \\alpha and \\beta for loss surface of input space for Fig 3 and Fig 4 (first row)? How are \\alpha and \\beta normalized for loss surface visualization in weight space as in Eq 1? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We have updated our submitted paper to address your concerns in Sec.2.2 and in Appendix 8.5 . Thanks a lot for the reviewer 's suggestions ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- We thank the reviewers for liking the visualization idea ! 1.About your first concern that \u201c The claim of \u2018 these significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space \u2019 are not well supported \u201d , please let us provide some clarification . We have provided the natural model and robust MinMax model \u2019 s contour maps on natural input and adversarial inputs and their Visualizations in the Appendix ( the updated version ) 8.5 , Fig.16 . As expected , in parameter space , natural model 's loss surface on adversarial inputs has a larger base height than the robust model , i.e.the average loss values are higher than robust models . But such gap is only obvious on weak attacks , like FGSM . When we use stronger attacks like C\\ & W attack , the loss surface of the natural model and robust model become similar again : Both models ' surfaces demonstrate high cross-entropy loss with no obvious distinction . Therefore , as mentioned in the review , we can indeed use the loss surface in weight space to show their robustness difference if they are both plotted with weak adversarial inputs . But when we are facing stronger iterative attacks , the loss surface in weight space can no longer show any difference , thus can not indicate the model robustness . By contrast , our input space loss surfaces can explicitly show the model robustness difference with no such restrictions , and the robustness difference can also be more obviously demonstrated , as shown in main paper Fig.3 . Therefore , we think this is the advantage of using input space loss surface to indicate the model robustness . We will absolutely update the statement in the main paper . 2.In Fig.3 , both x-axes ( alpha ) in ( a ) and ( b ) are chosen as random directions with normalization , and both y-axes ( beta ) are chosen as FGSM attack direction [ 1 ] . In Fig.4 , all x-axes ( alpha ) in ( a ) - ( d ) are chosen as random directions with normalization , and y-axes are as following ( formulas in Eq.3 ) : ( a ) random direction , ( b ) FGSM attack direction [ 1 ] , ( c ) Least-likely class attack direction [ 1 ] , ( d ) C & W attack direction [ 2 ] , all with normalization . 3.The gradients and random noises are normalized as in Fast gradient sign method [ 1 ] , which is the pixel-wise sign , i.e.beta = sign ( beta ) . We thank the reviewer for the constructive comments on the first point , and we will absolutely update the statement to be more accurate . [ 1 ] Adversarial examples in the physical world , Alexey Kurakin et al , 2016 . [ 2 ] Towards evaluating the robustness of neural networks . Carlini , Nicholas , and David Wagner . IEEE ( SP ) , 2017 ."}, {"review_id": "rylV6i09tX-2", "review_text": "This paper uses visualization methods to study how adversarial training methods impact the decision surface of neural networks. The authors also propose a gradient-based regularizer to improve robustness during training. Some things I liked about this paper: The authors are the first to visualize the \"decision boundary loss\". I also find this to be a better and more thorough study of loss functions than I have seen in other papers. The quality of the visualizations is notably higher than I've seen elsewhere on this subject. I have a few criticisms of this paper that I list below: 1) I'm not convinced that the decision surface is more informative than the loss surface. There is indeed a big \"hole\" in the middle of the plots in Figure 4, but it seems like that is only because the first contour is drawn at too high a level to see what is going on below. More contours are needed to see what is going on in that central region. 2) The proposed regularizer is very similar to the method of Ross & Doshi. It would be good if this similarity was addressed more directly in the paper. It feels like it's been brushed under the rug. 3) In the MNIST results in Table 1: These results are much less extensive than the results for CIFAR. It would especially be nice to see the MinMax results since those of commonly considered to be the state of the art. The fact that they are omitted makes it feel like something is being hidden from the reader. 4) The results of the proposed regularization method aren't super strong. For CIFAR, the proposed method combined with adversarial training beats MinMax only for small perturbations of size 3, and does worse for larger perturbations. The original MinMax model is optimized for a perturbation of size 8. I wonder if a MinMax result with smaller epsilon would be dominant in the regime of small perturbations. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "5 ) And we still need to claim two points about why our method is still meaningful here : a ) The training cost of MinMax makes it hard to scale . MinMax is commonly known that this method can not generalize to large-scale datasets [ 2 ] , e.g.ImageNet , since every training step in MinMax needs to generate PGD adversarial examples through 10-30 backpropagations . This makes training large-scale MinMax robust models impractical . But our method has better scaling ability than MinMax , the time consumption by double-backpropagation per training step is about 2.1 times than normal training , which is thus 5-15 times less than MinMax . b ) Meanwhile , on CIFAR10 , the gap between MinMax and our method is not that large . Especially , the robustness gap under eps=3 attacks ( FGSM , BIM , C & W ) is negligible as shown in Table.2 . And about the robustness degradation under larger step attack , our reason analysis is stated in Sec 5.2 , that because Taylor Approximation performs well in a small neighborhood but has limitations against larger step attack , which is a limitation of our method which we also talked in the paper . 6 ) Lastly , as our paper named `` Interpreting Adversarial Robustness : A View from Decision Surface in Input Space '' , we sincerely hope that reviewer could also take our paper \u2019 s other contributions into consideration , like revealing the nature of adversarial examples and robustness are actually solving NN \u2019 s neighborhood underfitting issue , the shared mechanism of various adversarial attacks by decision loss surface visualization and interpretation , proof of the relationship between loss geometry and adversarial robustness by Jacobian and Hessian \u2019 s geometry properties , etc. , and we believe our paper is a thorough analysis and interpretation work in current interpreting adversarial attack and robustness research . Again , we thank the reviewer for the detailed reviews ! [ 1 ] Andrew Slavin Ross and Finale Doshi-Velez . Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients . In AAAI , 2018 . [ 2 ] Kannan , Harini , Alexey Kurakin , and Ian Goodfellow . `` Adversarial Logit Pairing . '' arXiv preprint arXiv:1803.06373 ( 2018 ) ."}], "0": {"review_id": "rylV6i09tX-0", "review_text": "This paper argues that analyzing loss surfaces in parameter space for the purposes of evaluating adversarial robustness and generalization is ineffective, while measuring input loss surfaces is more accurate. By converting loss surfaces to decision surfaces (which denote the difference between the max and second highest logit), the authors show that all adversarial attack methods appear similar wrt the decision surface. This result is then related to the statistics of the input Jacobian and Hessian, which are shown to differ across adversarially sensitive and robust models. Finally, a regularization method based on regularizing the input Jacobian is proposed and evaluated. All of these results are shown through experiments on MNIST and CIFAR-10. In general, the paper is clear, though there are a number of typos. With respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references). Finally, regarding significance, many of the insights provided this paper are true by definition, and are therefore unlikely to have a significant impact. While I strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend. Please see the comments below for more detail. Major comments: 1) In the beginning of the paper, adversarial robustness and generalization are equated. However, adversarial robustness and generalization are not necessarily equivalent, and in fact, several papers have provided evidence against this notion, showing that adversarial inputs are likely to be present even for very good models [5, 6] and that adversarially sensitive models can often generalize quite well [2]. Moreover, all the experiments within the paper only address adversarial robustness rather than generalization to unperturbed samples. 2) One of the main results of this paper is that the loss surface wrt input space is more sensitive to adversarial perturbations than the loss surface wrt parameter space. Because adversarial inputs are defined in input space, by definition, the loss surface wrt to the input must be sensitive to adversarial examples. This result therefore appears true by definition. Moreover, [3] related the input Jacobian to generalization, finding a similar result, but is not discussed or cited. 3) The main result of Section 3 is that all adversarial attacks \u201cutilize the decision surface geometry properties to cross the decision boundary within least distance.\u201d While to my knowledge the decision surface visualization is novel and might have important uses, this statement is again true by definition, given that adversarial attack methods try to find the smallest perturbation which changes the network decision. As a result, all methods must find directions which are short paths in the decision surface. It is therefore unclear what additional insight this analysis presents. 4) How does measuring the loss landscape as an indicator for adversarial robustness differ from simply trying to find adversarial examples as is common? If anything, it seems it should be more computationally expensive as points are sampled in a grid search vs optimized for. 5) The proposed regularizer for adversarial robustness, based on regularizing the input Jacobian, is very similar to what was proposed in [1], yet [1] is not discussed or cited. Minor comments: 1) The paper\u2019s first sentence states that \u201cIt is commonly believed that a neural network\u2019s generalization is correlated to ...the flatness of the local minima in parameter space.\u201d However, [4] showed several years ago that the local minima flatness can be arbitrarily rescaled and has been fairly influential. While [4] is cited in the paper, it is only cited in the related work section as support for the statement that local minima flatness is related to generalization when this is precisely opposite the point this paper makes. [4] should be discussed in more detail, both in the introduction and the related work section. 2) The paper is quite lengthy, going right up against the hard 10 page limit. While this may be acceptable for papers with large figures or which require the extra space, this paper does not currently meet that threshold. 3) Throughout the figures, axes should be labeled. 4) In section 2.2, it is stated that both networks achieve optimal accuracy of ~90% on CIFAR-10. This is not optimal accuracy and hasn\u2019t been for several years [7]. 5) Why is equation 2 calculated with respect to the logit layer vs the normalized softmax layer? Using the unnormalized logits may introduce noise due to scaling. 6) In Figure 8, the scales of the Hessian are extremely different. Does this impact the measurement of sparseness? Typos: 1) Introduction, second paragraph: \u201cFor example, ResNet model usually converges to\u2026\u201d should be \u201cFor example, ResNet models usually converge to\u2026\u201d 2) Introduction, second paragraph: \u201c...defected by the adversarial noises...\u201d should be \u201c...defected by adversarial noise\u2026\u201d 3) Introduction, third paragraph: \u201c...introduced by adversarial noises...\u201d should be \u201c...introduced by adversarial noise\u2026\u201d 4) Section 3.1, first paragraph: \u201ccross entropy based loss surface is\u2026\u201d should be \u201ccross entropy based loss surfaces is\u2026\u201d [1] Jakubovitz, Daniel, and Raja Giryes. \"Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization.\" arXiv preprint arXiv:1803.08680 (2018). ECCV 2018. [2] Zahavy, Tom, et al. \"Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms.\" arXiv preprint arXiv:1602.02389 (2016). ICLR Workshop 2018 [3] Novak, Roman, et al. \"Sensitivity and generalization in neural networks: an empirical study.\" arXiv preprint arXiv:1802.08760 (2018). ICLR 2018. [4] Dinh, Laurent, et al. \"Sharp Minima Can Generalize For Deep Nets.\" International Conference on Machine Learning. 2017. [5] Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. \"Adversarial vulnerability for any classifier.\" arXiv preprint arXiv:1802.08686 (2018). NIPS 2018. [6] Gilmer, Justin, et al. \"Adversarial spheres.\" arXiv preprint arXiv:1801.02774 (2018). ICLR Workshop 2018. [7] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html ", "rating": "3: Clear rejection", "reply_text": "Thanks for the reviewer \u2019 s precious comments ! We do believe that there is a lot to be improved in this paper ! 1 ) About the first concern of the reviewer , the reason of \u201c equated generalization \u201d is that previously upon the finish of this paper , there is no clear definition of the generalization in the adversarial settings . And our statement is actually saying that \u201c adversarial robust generalization \u201d doesn \u2019 t equal to \u201c standard generalization \u201d , as in the NeurIPS \u2019 18 tutorial slides , https : //media.neurips.cc/Conferences/NIPS2018/Slides/adversarial_ml_slides_parts_1_4.pdf , page 29-30 . We can now give the formal definition maybe in the future paper , thanks for the advice . 2 ) For the second concern , by briefly comparing weight/input space visualization , the main conclusion we want to draw is that , past generalization analysis can not be well adopted in the adversarial settings . I think this should make more sense . 3 ) About the sec.3 , we don \u2019 t agree with the reviewer that the visualization insights are trivial because the visualization results are one of the key intuition , which is \u201c The geometry slopes ( gradients ) matter a lot in the model \u2019 s robustness \u201d and this is the motivation of why regulating jacobian could improve the robustness in the whole paper . 4 ) About the robustness indicator , we have shown a case study in the Sec4.3 ROBUSTNESS INDICATOR EVALUATION . We could compare two model \u2019 s Jacobian & Hessian to distinguish two model \u2019 s robustness easily , as shown in Fig.8 . This is not done by grid search , but by backpropagation to compute Jacobian and Hessian , therefore not computationally expensive . 5 ) About the Jacobian Regulation novelty , we do agree that the robust training part is not a significant contribution , as also mentioned by reviewer-2 . Our reply about the difference is here : https : //openreview.net/forum ? id=rylV6i09tX & noteId=HJgm8ciA3X under the reviewer-2 \u2019 s comments . 6 ) About the related work and other minor comments , thanks again for the precious comments !"}, "1": {"review_id": "rylV6i09tX-1", "review_text": "The authors demonstrated that the loss surface visualized in parameter space does not reflect its robustness to adversarial examples. By analyzing the geometric properties of the loss surface in both parameter space and input space, they find input space is more appropriate in evaluating the generalization and adversarial robustness of a neural network. Therefore, they extend the loss surface to decision surface. They further visualized the adversarial attack trajectory on decision surfaces in input space, and formalized the adversarial robustness indicator. Finally, a robust training method guided by the indicator is proposed to smooth the decision surface. This paper is interesting and well organized. The idea of plotting loss surface in input space seems to be a natural extension of the loss surface w.r.t to weight change. The loss surface in input space measures the network\u2019s robustness to the perturbation of inputs, which naturally shows the influence of adversarial examples and is suitable for studying the robustness to adversarial examples. Note that loss surface in parameter space measures the network\u2019s robustness to the perturbation of the weights with given inputs, which implicitly assumes the data distribution is not significantly changed so that the loss surface may have similar geometry on the unseen data. The claim of \u201cthese significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space\u201d are not well supported as the comparison between Figure 2(a) and Figure 3 seems to be unfair and misleading. Fig 2 are plotted based on input data without any adversarial examples. So it is expected to see that Fig 2(a) and Fig 2(b) have similar contours. However, the loss surface in weight space may still be able to show their difference if the they are both plotted with the adversarial inputs. I believe that models trained by Min-Max robust training will be more stable in comparison with the normally trained models. It would be great if the author provide such plots. I would expect the normal model to have a high and flat surface while the robust model shows reasonable loss with small changes in weight space. How to choose \\alpha and \\beta for loss surface of input space for Fig 3 and Fig 4 (first row)? How are \\alpha and \\beta normalized for loss surface visualization in weight space as in Eq 1? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We have updated our submitted paper to address your concerns in Sec.2.2 and in Appendix 8.5 . Thanks a lot for the reviewer 's suggestions ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- We thank the reviewers for liking the visualization idea ! 1.About your first concern that \u201c The claim of \u2018 these significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space \u2019 are not well supported \u201d , please let us provide some clarification . We have provided the natural model and robust MinMax model \u2019 s contour maps on natural input and adversarial inputs and their Visualizations in the Appendix ( the updated version ) 8.5 , Fig.16 . As expected , in parameter space , natural model 's loss surface on adversarial inputs has a larger base height than the robust model , i.e.the average loss values are higher than robust models . But such gap is only obvious on weak attacks , like FGSM . When we use stronger attacks like C\\ & W attack , the loss surface of the natural model and robust model become similar again : Both models ' surfaces demonstrate high cross-entropy loss with no obvious distinction . Therefore , as mentioned in the review , we can indeed use the loss surface in weight space to show their robustness difference if they are both plotted with weak adversarial inputs . But when we are facing stronger iterative attacks , the loss surface in weight space can no longer show any difference , thus can not indicate the model robustness . By contrast , our input space loss surfaces can explicitly show the model robustness difference with no such restrictions , and the robustness difference can also be more obviously demonstrated , as shown in main paper Fig.3 . Therefore , we think this is the advantage of using input space loss surface to indicate the model robustness . We will absolutely update the statement in the main paper . 2.In Fig.3 , both x-axes ( alpha ) in ( a ) and ( b ) are chosen as random directions with normalization , and both y-axes ( beta ) are chosen as FGSM attack direction [ 1 ] . In Fig.4 , all x-axes ( alpha ) in ( a ) - ( d ) are chosen as random directions with normalization , and y-axes are as following ( formulas in Eq.3 ) : ( a ) random direction , ( b ) FGSM attack direction [ 1 ] , ( c ) Least-likely class attack direction [ 1 ] , ( d ) C & W attack direction [ 2 ] , all with normalization . 3.The gradients and random noises are normalized as in Fast gradient sign method [ 1 ] , which is the pixel-wise sign , i.e.beta = sign ( beta ) . We thank the reviewer for the constructive comments on the first point , and we will absolutely update the statement to be more accurate . [ 1 ] Adversarial examples in the physical world , Alexey Kurakin et al , 2016 . [ 2 ] Towards evaluating the robustness of neural networks . Carlini , Nicholas , and David Wagner . IEEE ( SP ) , 2017 ."}, "2": {"review_id": "rylV6i09tX-2", "review_text": "This paper uses visualization methods to study how adversarial training methods impact the decision surface of neural networks. The authors also propose a gradient-based regularizer to improve robustness during training. Some things I liked about this paper: The authors are the first to visualize the \"decision boundary loss\". I also find this to be a better and more thorough study of loss functions than I have seen in other papers. The quality of the visualizations is notably higher than I've seen elsewhere on this subject. I have a few criticisms of this paper that I list below: 1) I'm not convinced that the decision surface is more informative than the loss surface. There is indeed a big \"hole\" in the middle of the plots in Figure 4, but it seems like that is only because the first contour is drawn at too high a level to see what is going on below. More contours are needed to see what is going on in that central region. 2) The proposed regularizer is very similar to the method of Ross & Doshi. It would be good if this similarity was addressed more directly in the paper. It feels like it's been brushed under the rug. 3) In the MNIST results in Table 1: These results are much less extensive than the results for CIFAR. It would especially be nice to see the MinMax results since those of commonly considered to be the state of the art. The fact that they are omitted makes it feel like something is being hidden from the reader. 4) The results of the proposed regularization method aren't super strong. For CIFAR, the proposed method combined with adversarial training beats MinMax only for small perturbations of size 3, and does worse for larger perturbations. The original MinMax model is optimized for a perturbation of size 8. I wonder if a MinMax result with smaller epsilon would be dominant in the regime of small perturbations. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "5 ) And we still need to claim two points about why our method is still meaningful here : a ) The training cost of MinMax makes it hard to scale . MinMax is commonly known that this method can not generalize to large-scale datasets [ 2 ] , e.g.ImageNet , since every training step in MinMax needs to generate PGD adversarial examples through 10-30 backpropagations . This makes training large-scale MinMax robust models impractical . But our method has better scaling ability than MinMax , the time consumption by double-backpropagation per training step is about 2.1 times than normal training , which is thus 5-15 times less than MinMax . b ) Meanwhile , on CIFAR10 , the gap between MinMax and our method is not that large . Especially , the robustness gap under eps=3 attacks ( FGSM , BIM , C & W ) is negligible as shown in Table.2 . And about the robustness degradation under larger step attack , our reason analysis is stated in Sec 5.2 , that because Taylor Approximation performs well in a small neighborhood but has limitations against larger step attack , which is a limitation of our method which we also talked in the paper . 6 ) Lastly , as our paper named `` Interpreting Adversarial Robustness : A View from Decision Surface in Input Space '' , we sincerely hope that reviewer could also take our paper \u2019 s other contributions into consideration , like revealing the nature of adversarial examples and robustness are actually solving NN \u2019 s neighborhood underfitting issue , the shared mechanism of various adversarial attacks by decision loss surface visualization and interpretation , proof of the relationship between loss geometry and adversarial robustness by Jacobian and Hessian \u2019 s geometry properties , etc. , and we believe our paper is a thorough analysis and interpretation work in current interpreting adversarial attack and robustness research . Again , we thank the reviewer for the detailed reviews ! [ 1 ] Andrew Slavin Ross and Finale Doshi-Velez . Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients . In AAAI , 2018 . [ 2 ] Kannan , Harini , Alexey Kurakin , and Ian Goodfellow . `` Adversarial Logit Pairing . '' arXiv preprint arXiv:1803.06373 ( 2018 ) ."}}