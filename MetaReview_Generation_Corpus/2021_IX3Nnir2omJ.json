{"year": "2021", "forum": "IX3Nnir2omJ", "title": "Characterizing signal propagation to close the performance gap in unnormalized ResNets", "decision": "Accept (Poster)", "meta_review": "This paper analyses the signal propagation through residual architectures; then suggests a scaling method which, together with weight standardization, allows to train such networks to high accuracy with batch-norm; it demonstrates that the method performs better than previous methods (Fixup, SkipInit), and can be used on more advanced architectures. \n\nThe reviewers initially had several concerns, but after the author's revision, these concerns were addressed and most reviewers recommended acceptance. One reviewer did not respond, but I think these concerns were addressed. I think it will help to further convince the readers on the usefulness of the method readers if the authors would check the sensitivity to the learning rate with the current method and compare with other methods (SkipInit, Fixup, BN). The reason I'm suggesting this is that I think one of the main reasons BN is still in popular use is that it commonly tends to make training more robust to changes in hyper-parameters, such as the learning rate (while other methods, like SkipInit and Fixup, require more hyper-parameter tuning).\n\nOverall the analysis and the suggested method seem useful, especially at a small batch size and the writing is mostly clear, so I recommend acceptance. \n\n", "reviews": [{"review_id": "IX3Nnir2omJ-0", "review_text": "The paper proposes a novel , and mathematically well motivated initialization scheme for deep ResNet like models . This is used to train batchnorm free Resnets that compare to their baseline . They also train RegNet like models that are fairly close to EfficientNets in terms of performance . Comments : You list problems with batchnorm : 1 ) 'it breaks independence between training examples ' Why is this a problem ? You do n't include any experiments showing your method works well in the small batch regime where batchnorm is problematic . 2 ) 'it is suprisingly expensive to compute ' Batchnorm was widely adopted as it made networks train much faster . And there is no computational overhead at test time . Does your method allow models to be trained more quickly , i.e.how does accuracy compare after $ N $ minutes of training time ? 3 ) 'it often results in unexpected bugs ' I do n't understand this point . Batchnorm has lead to many state-of-the-art results . In contrast you have only managed to get good results by using large amounts of prior knowledge that was obtained from experiments on the same validation set and using batchnorm . Section 5.1 Why is the training collapse of the 288 layer models not reflected in Table 1 ? Regarding Figure 3 : Why compare your method * * with * * data augmentation and EfficientNets * * without * * ? That seems very misleading to me . Getting close to proper EfficientNet performance without batchnorm is a decent achievement . Why include an inferior , incomparable baseline ? -- update I am upgrading to 7 : Good paper accept ; as my concerns have been addressed by the additional experiments . The proposed method is not yet a drop in replacement for BatchNorm in general , but it can be useful in specific circumstances , i.e.small batch-size training .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive feedback . We have included a top-level response that addresses some of the reviewer \u2019 s comments as well as describes new experimental results we have now added to an updated version of the paper . We address additional specific comments below . > Why is breaking independence between training examples a problem ? Small batch results ? We have added results with small batch training to our draft , as described in our top level comment . There are both theoretical and practical limitations of breaking the independence between training examples . Most theoretical models of optimization on neural networks require the assumption that all training examples are independent , and analyzing optimization behaviour without this assumption becomes significantly more challenging for non-trivial neural network models . From a practical standpoint , introducing dependencies between training examples leads to a number of additional design considerations ( or additional communication ) when implementing BatchNorm on distributed systems ( e.g. , does one use per-device BatchNorm or accept the cost of cross-replica BatchNorm ? ) . This issue is also specifically noted in setups like contrastive learning ( MoCo and SimCLR ) where it requires specific care in order to prevent breakdown of the contrastive objective . > \u201c Surprisingly expensive to compute \u201d , but no experiments . Do NF-Nets train faster than BatchNorm ? The primary cases where BatchNorm \u2019 s compute cost is \u201c surprising \u201d is due to the memory overhead it can incur or in distributed training . For example , for training semantic segmentation models with large image sizes this cost can be substantial , and has driven the development of techniques like In-Place Activated BatchNorm [ 1 ] , where the authors note that addressing this issue with BatchNorm can reduce memory consumption by up to 50 % for some models . In the case where one requires cross-replica BatchNorm ( as is used in EfficientNet training ) one also incurs inter-device communication costs which will vary widely with the hardware . We have added training speed numbers to appendix A.2 comparing NF-ResNets against BN-ResNets on a single GPU ; our models are mildly faster in this simple test-case , which does not result in a big difference in accuracy after N minutes of training . Our NFNets models are substantially faster than EfficientNets , but this is more due to depthwise convs in EfficientNets being very slow in practice . Given this , we have revised the statement in the abstract to remove \u201c surprisingly \u201d and instead use the more factual statement that BatchNorm \u201c can incur compute and memory overhead. \u201d > Unexpected bugs ? NF-Nets build on large amount of prior knowledge . We thank the reviewer for noting our lack of citation on bugs stemming from BatchNorm . In our top-level comment titled \u201c The trouble with BatchNorm \u201d , we list a number of known examples , and have added a citation to prior academic work specifically noting the prevalence of bugs related to BatchNorm . We agree that BatchNorm has been an integral part of state-of-the-art models in a number of tasks over the past few years , and we certainly build on a lot of prior knowledge from these highly tuned normalized models . However , BatchNorm \u2019 s importance on this front does not mean that it comes without disadvantages ( as evidenced by the large body of work attempting to develop alternative normalizers or remove normalization ) , and we would argue that its prevalence is largely driven by ( a ) alternative normalizers typically incurring costs at inference or performing worse and ( b ) previous methods that remove normalization like FixUp or SkipInit resulting in degraded generalization performance . Our motivation is therefore to build on the strong prior work ( largely derived using batch-normalized models ) to develop models that are sufficiently robust and performant to be of interest to the community while not having the disadvantages of BatchNorm . > Training collapse of 288 nets . This was an accidental omission ; we have updated Table 1 to indicate that the values reported for these runs are computed over the random seeds which do not collapse . > Comparing method with augs to EffNets without augs ? Why include EffNets without augs as a baseline ? We have updated the pareto front figure to only include EffNets with Augmentations . Please note that our results table ( table 3 ) contains both standard preprocessing and training with augmentations for both EfficientNets and NFNets across all FLOP targets for completeness . [ 1 ] Samuel Rota Bul\u00f2 , Lorenzo Porzi , Peter Kontschieder . In-Place Activated BatchNorm for Memory-Optimized Training of DNNs . CVPR 2018 [ 2 ] ICLR anonymous submission : Deconstructing the Regularization of BatchNorm . Link : https : //openreview.net/pdf ? id=d-XzF81Wg1 ."}, {"review_id": "IX3Nnir2omJ-1", "review_text": "Summary : The authors propose a set of visualization tools named SPPs for better monitoring the key indicators , including Average Channel Squared Mean , Average Channel Variance , etc. , of hidden activations . With SPPs in hand , they find two key failure modes at initialization in deep ResNets and then develop Normalizer-Free ResNets using Scaled Weight Standardization , achieving competitive performance on ResNet-288 and Efficient-Net . Strength : -- The overall idea makes sense and the proposed method of removing batch normalization can reduce computational resources and speed up computing greatly . -- The visualization of the key indicators may be helpful for understanding how batch normalization works and how to remove batch normalization . -- The experiment results seem that the proposed method achieves good performance in large-scale ResNets . Weakness : -- There is a lack of accuracy comparison with other removing batch normalization works . -- It will be better to prove the effectiveness of the proposed method in more tasks , not only the classification task . Comments : ( 1 ) The batch size you used for training is 1024 . As you know , decreasing batch size is good for online training on portable devices . Can the proposed method decrease the batch size to a small value like 2/4/8 after removing batch normalization ? And if it is difficult , can you give an explanation on how batch size affects the performance after removing batch normalization with the proposed method . ( 2 ) You have visualized the key indicators of the proposed initialization method . Can you give the visualization of the same indicators of other initialization methods like Fixup initialization and make some comparison ? I think it \u2019 s within your ability . ( 3 ) It \u2019 s better to add some accuracy comparisons with other removing batch normalization works . UPDATE : The author has addressed most of my concerns , but regarding the motivations and the benefits for the community , I still keep my score .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their positive feedback . We have incorporated all of this feedback into our draft ( the new experimental results in our paper are described in detail in our top-level comment on OpenReview ) and here respond to specific points . > Comparison to other alternatives to normalization We have now included comparisons with Fixup initialization and SkipInit initialization on ResNets of various depths in table 1 . In all cases , NF-ResNets outperform both Fixup and SkipInit . > Small batch experiments We have updated the draft to include additional experiments on ResNets for small batch sizes ( 8 and 4 ) in table 2 , and find that NF-ResNets behave much better than BN-ResNets in this case . In particular , when going from batch size 8 to batch size 4 , NF-ResNets do not lose any performance , but BN-ResNets suffer severe performance drops . > SPPs with Fixup initialization We have added SPPs for models with FixUp and for ResNet-V1 ( post-activation ) models in appendix F. > Additional downstream tasks We have now added experiments on transfer from BN-ResNets and NF-ResNets to the downstream tasks of Pascal VOC semantic segmentation and NYUv2 depth estimation , and find that NF-ResNets perform comparably to their batch-normalized counterparts on both tasks ."}, {"review_id": "IX3Nnir2omJ-2", "review_text": "This paper proposes the signal propagation plot ( spp ) which is a tool for analyzing residual networks and analyzes ResNet with/without BN . Based on the investigation , the authors first provide ResNet results without normalization with the proposed scaled weight standardization . Furthermore , the authors provide a bunch of models that are competitive to EfficientNets based on RegNetY-400MF , which seem to be highly tuned in terms of architecture design . Pros ) 1.This paper is well written and easy to follow . 2.The proposed SPP seems to be a good tool for analyzing a model . Cons ) 1.The authors seem to have failed to provide any reasons for needing normalization free ResNet over the original BN-ResNet . 2.It is not clear that the trained model without NF with SWS can be used as a backbone that can be directly applied to downstream tasks ( e.g. , object detection ) . 3.I do n't think it was necessary to show competitive results with EfficientNets using the other baseline - RegNet . Especially , the proposed models ( which are compared with EfficientNets ) are highly-tuned trying to surpass EfficientNets ' accuracy . This type of paper would be better to be focused on investigating the characteristics of a network . Comments ) 1 . The major problem of this paper is none of the advantages of NF with SWS are highlighted over BN , so it is hard to find any reasons for replacing BN with NF-SWS . I mean non of the disadvantages of BN are addressed by the proposed method . 2.SPP could enlighten that an unusual ReLU-BN-Conv ordering would have some benefits but why should a network without normalization mimick the SPP trend of ResNet ? It is unnatural that a network without normalization should follow the ResNet 's behavior . 3.From the equation x_ { l+1 } =x_l+ a * f ( x_l/b_l ) at p.4 , the proposed approach eventually normalizing the feature even if a or b_l is fixed . Moreover , gamma in eq . ( 3 ) additionally scaling up or down the weight which ultimately gives an effect on the computed feature . 4.How did the authors compute gamma in eq . ( 3 ) in a training phase ? Why the provided code contains a learnable gamma ? 5.Why `` zero padding '' at p.5 affects the variance decay in the rightmost graph in Figure 2 ? 6.Please clarify why RegNetY-400MF chose it as the baseline . It is not clear that the authors pick RegNet and tune it highly and compared with EfficientNets . If the authors decided to use RegNets , then it is natural to use NF-SWS-RegNets are compared with the original RegNets without any big modifications as shown in the model details in the Appendix . 7.Did the authors use trained ResNetV2-600 or randomly-initialized model ? 8.How does SPP goes on with the post-activation network which uses the original bottleneck block consists of Conv-BN-ReLU ? 9.How can the resolution downsampling block in a ResNet affect averaged channel mean and variance ? 10.Why the ResNet experiments are done with weight decay of 5e-5 ? The common knowledge of training ResNet is with weighing decay of 1e-4 , so one may unconvinced the result because of the tuning . 11.Comparing EfficientNets that are trained without CutMix and Mixup ( but used randaug or autoaug ) with the proposed models with cutmix and mixup seems to be not fair .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their valuable suggestions and detailed comments . We have included a top-level response that addresses some of the reviewer \u2019 s comments as well as describes additional experimental results we have now added to an updated version of the paper . Here we present additional responses for R1 . > \u201c None of the disadvantages of BN are addressed by the proposed method \u201d The proposed method overcomes the following disadvantages : BatchNorm introduces a dependence between elements in a batch . Normalizer-Free Networks with Scaled weight Standardization do not have this property . Please see our top-level response titled \u201c The trouble with BatchNorm \u201d for a discussion on how this batch-dependence can be problematic in models . This batch-dependence aspect of BatchNorm also means that it is not always clear how to implement it in distributed training . When one may have a low per-device batch size , BatchNorm can behave poorly or require the additional communication overhead of synced cross-replica BatchNorm . In contrast , our proposed method is completely invariant to the number of devices ( its implementation does not change ) and does not require cross-device communication in the forward pass . As requested by the reviewers , we have now added ResNet experiments in the small batch setting to the paper , where networks with BatchNorm perform very poorly , while NF-ResNets perform well . BatchNorm has different behavior at training and test time . Norm-Free Nets and Scaled Weight Standardization behave the same at training and testing time . Scaled WS also does not require the maintenance of running averages . Other alternatives that remove the train-test discrepancy ( GroupNorm or LayerNorm ) incur compute costs at inference . Scaled WS does not have this property : it can be completely \u201c constant-folded \u201d at test time , and is accordingly free at inference ( as with BatchNorm ) . BatchNorm can incur substantial memory overhead and can be expensive to compute when using large image sizes . This has previously motivated such works as \u201c In-Place Activated BatchNorm \u201d , for tasks such as semantic segmentation where this is an issue . Scaled WS does not have this disadvantage , as it operates on the weights , which for ConvNets tend to be small relative to the size of the activations . We have added new experimental results to our paper ( mentioned in our top-level response ) comparing training speed to demonstrate the computational advantage of Scaled WS over BN . > \u201c why should a network without normalization mimick the SPP trend of ResNet \u201d The choice of how signals should propagate in unnormalized networks is largely one of design . In Appendix G.2 , we note that we initially explored designing networks to have constant variance , which without prior knowledge one might assume to be a superior choice . We found that such networks were not as performant , and reasoned that mimicking a signal propagation template which we know to work well was a good design choice , as is supported by our experiments . > \u201c How did the authors compute gamma in eq . ( 3 ) in a training phase ? Why the provided code contains a learnable gamma ? \u201d Gamma is a nonlinearity-specific scalar constant . It is not learnable , but is a fixed scalar derived similar to the constants from He Initialization , in order to make a nonlinearity approximately variance-preserving . See section 4.3 in the paper for more discussion on this . As mentioned in the text , gamma values can be computed analytically if the form of the variance is known when assuming the distribution of the input , or approximated empirically using the code provided in Appendix D.1 . The provided code does contain a zero-initialized scalar gain at the end of a ResNet block , in order to allow one to implement SkipInit . As mentioned in the paper , this is one of several relaxations we employ which we find to work well empirically . > \u201c Did the authors use trained ResNetV2-600 or randomly-initialized model ? \u201d The SPPs are presented for models at initialization . We have clarified this in the paper . > \u201c How does SPP goes on with the post-activation network which uses the original bottleneck block consists of Conv-BN-ReLU ? \u201d We have now added the SPP for post-activation ResNets and ResNets with FixUp in the appendix section F ( figures 10 and 11 ) ."}], "0": {"review_id": "IX3Nnir2omJ-0", "review_text": "The paper proposes a novel , and mathematically well motivated initialization scheme for deep ResNet like models . This is used to train batchnorm free Resnets that compare to their baseline . They also train RegNet like models that are fairly close to EfficientNets in terms of performance . Comments : You list problems with batchnorm : 1 ) 'it breaks independence between training examples ' Why is this a problem ? You do n't include any experiments showing your method works well in the small batch regime where batchnorm is problematic . 2 ) 'it is suprisingly expensive to compute ' Batchnorm was widely adopted as it made networks train much faster . And there is no computational overhead at test time . Does your method allow models to be trained more quickly , i.e.how does accuracy compare after $ N $ minutes of training time ? 3 ) 'it often results in unexpected bugs ' I do n't understand this point . Batchnorm has lead to many state-of-the-art results . In contrast you have only managed to get good results by using large amounts of prior knowledge that was obtained from experiments on the same validation set and using batchnorm . Section 5.1 Why is the training collapse of the 288 layer models not reflected in Table 1 ? Regarding Figure 3 : Why compare your method * * with * * data augmentation and EfficientNets * * without * * ? That seems very misleading to me . Getting close to proper EfficientNet performance without batchnorm is a decent achievement . Why include an inferior , incomparable baseline ? -- update I am upgrading to 7 : Good paper accept ; as my concerns have been addressed by the additional experiments . The proposed method is not yet a drop in replacement for BatchNorm in general , but it can be useful in specific circumstances , i.e.small batch-size training .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive feedback . We have included a top-level response that addresses some of the reviewer \u2019 s comments as well as describes new experimental results we have now added to an updated version of the paper . We address additional specific comments below . > Why is breaking independence between training examples a problem ? Small batch results ? We have added results with small batch training to our draft , as described in our top level comment . There are both theoretical and practical limitations of breaking the independence between training examples . Most theoretical models of optimization on neural networks require the assumption that all training examples are independent , and analyzing optimization behaviour without this assumption becomes significantly more challenging for non-trivial neural network models . From a practical standpoint , introducing dependencies between training examples leads to a number of additional design considerations ( or additional communication ) when implementing BatchNorm on distributed systems ( e.g. , does one use per-device BatchNorm or accept the cost of cross-replica BatchNorm ? ) . This issue is also specifically noted in setups like contrastive learning ( MoCo and SimCLR ) where it requires specific care in order to prevent breakdown of the contrastive objective . > \u201c Surprisingly expensive to compute \u201d , but no experiments . Do NF-Nets train faster than BatchNorm ? The primary cases where BatchNorm \u2019 s compute cost is \u201c surprising \u201d is due to the memory overhead it can incur or in distributed training . For example , for training semantic segmentation models with large image sizes this cost can be substantial , and has driven the development of techniques like In-Place Activated BatchNorm [ 1 ] , where the authors note that addressing this issue with BatchNorm can reduce memory consumption by up to 50 % for some models . In the case where one requires cross-replica BatchNorm ( as is used in EfficientNet training ) one also incurs inter-device communication costs which will vary widely with the hardware . We have added training speed numbers to appendix A.2 comparing NF-ResNets against BN-ResNets on a single GPU ; our models are mildly faster in this simple test-case , which does not result in a big difference in accuracy after N minutes of training . Our NFNets models are substantially faster than EfficientNets , but this is more due to depthwise convs in EfficientNets being very slow in practice . Given this , we have revised the statement in the abstract to remove \u201c surprisingly \u201d and instead use the more factual statement that BatchNorm \u201c can incur compute and memory overhead. \u201d > Unexpected bugs ? NF-Nets build on large amount of prior knowledge . We thank the reviewer for noting our lack of citation on bugs stemming from BatchNorm . In our top-level comment titled \u201c The trouble with BatchNorm \u201d , we list a number of known examples , and have added a citation to prior academic work specifically noting the prevalence of bugs related to BatchNorm . We agree that BatchNorm has been an integral part of state-of-the-art models in a number of tasks over the past few years , and we certainly build on a lot of prior knowledge from these highly tuned normalized models . However , BatchNorm \u2019 s importance on this front does not mean that it comes without disadvantages ( as evidenced by the large body of work attempting to develop alternative normalizers or remove normalization ) , and we would argue that its prevalence is largely driven by ( a ) alternative normalizers typically incurring costs at inference or performing worse and ( b ) previous methods that remove normalization like FixUp or SkipInit resulting in degraded generalization performance . Our motivation is therefore to build on the strong prior work ( largely derived using batch-normalized models ) to develop models that are sufficiently robust and performant to be of interest to the community while not having the disadvantages of BatchNorm . > Training collapse of 288 nets . This was an accidental omission ; we have updated Table 1 to indicate that the values reported for these runs are computed over the random seeds which do not collapse . > Comparing method with augs to EffNets without augs ? Why include EffNets without augs as a baseline ? We have updated the pareto front figure to only include EffNets with Augmentations . Please note that our results table ( table 3 ) contains both standard preprocessing and training with augmentations for both EfficientNets and NFNets across all FLOP targets for completeness . [ 1 ] Samuel Rota Bul\u00f2 , Lorenzo Porzi , Peter Kontschieder . In-Place Activated BatchNorm for Memory-Optimized Training of DNNs . CVPR 2018 [ 2 ] ICLR anonymous submission : Deconstructing the Regularization of BatchNorm . Link : https : //openreview.net/pdf ? id=d-XzF81Wg1 ."}, "1": {"review_id": "IX3Nnir2omJ-1", "review_text": "Summary : The authors propose a set of visualization tools named SPPs for better monitoring the key indicators , including Average Channel Squared Mean , Average Channel Variance , etc. , of hidden activations . With SPPs in hand , they find two key failure modes at initialization in deep ResNets and then develop Normalizer-Free ResNets using Scaled Weight Standardization , achieving competitive performance on ResNet-288 and Efficient-Net . Strength : -- The overall idea makes sense and the proposed method of removing batch normalization can reduce computational resources and speed up computing greatly . -- The visualization of the key indicators may be helpful for understanding how batch normalization works and how to remove batch normalization . -- The experiment results seem that the proposed method achieves good performance in large-scale ResNets . Weakness : -- There is a lack of accuracy comparison with other removing batch normalization works . -- It will be better to prove the effectiveness of the proposed method in more tasks , not only the classification task . Comments : ( 1 ) The batch size you used for training is 1024 . As you know , decreasing batch size is good for online training on portable devices . Can the proposed method decrease the batch size to a small value like 2/4/8 after removing batch normalization ? And if it is difficult , can you give an explanation on how batch size affects the performance after removing batch normalization with the proposed method . ( 2 ) You have visualized the key indicators of the proposed initialization method . Can you give the visualization of the same indicators of other initialization methods like Fixup initialization and make some comparison ? I think it \u2019 s within your ability . ( 3 ) It \u2019 s better to add some accuracy comparisons with other removing batch normalization works . UPDATE : The author has addressed most of my concerns , but regarding the motivations and the benefits for the community , I still keep my score .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their positive feedback . We have incorporated all of this feedback into our draft ( the new experimental results in our paper are described in detail in our top-level comment on OpenReview ) and here respond to specific points . > Comparison to other alternatives to normalization We have now included comparisons with Fixup initialization and SkipInit initialization on ResNets of various depths in table 1 . In all cases , NF-ResNets outperform both Fixup and SkipInit . > Small batch experiments We have updated the draft to include additional experiments on ResNets for small batch sizes ( 8 and 4 ) in table 2 , and find that NF-ResNets behave much better than BN-ResNets in this case . In particular , when going from batch size 8 to batch size 4 , NF-ResNets do not lose any performance , but BN-ResNets suffer severe performance drops . > SPPs with Fixup initialization We have added SPPs for models with FixUp and for ResNet-V1 ( post-activation ) models in appendix F. > Additional downstream tasks We have now added experiments on transfer from BN-ResNets and NF-ResNets to the downstream tasks of Pascal VOC semantic segmentation and NYUv2 depth estimation , and find that NF-ResNets perform comparably to their batch-normalized counterparts on both tasks ."}, "2": {"review_id": "IX3Nnir2omJ-2", "review_text": "This paper proposes the signal propagation plot ( spp ) which is a tool for analyzing residual networks and analyzes ResNet with/without BN . Based on the investigation , the authors first provide ResNet results without normalization with the proposed scaled weight standardization . Furthermore , the authors provide a bunch of models that are competitive to EfficientNets based on RegNetY-400MF , which seem to be highly tuned in terms of architecture design . Pros ) 1.This paper is well written and easy to follow . 2.The proposed SPP seems to be a good tool for analyzing a model . Cons ) 1.The authors seem to have failed to provide any reasons for needing normalization free ResNet over the original BN-ResNet . 2.It is not clear that the trained model without NF with SWS can be used as a backbone that can be directly applied to downstream tasks ( e.g. , object detection ) . 3.I do n't think it was necessary to show competitive results with EfficientNets using the other baseline - RegNet . Especially , the proposed models ( which are compared with EfficientNets ) are highly-tuned trying to surpass EfficientNets ' accuracy . This type of paper would be better to be focused on investigating the characteristics of a network . Comments ) 1 . The major problem of this paper is none of the advantages of NF with SWS are highlighted over BN , so it is hard to find any reasons for replacing BN with NF-SWS . I mean non of the disadvantages of BN are addressed by the proposed method . 2.SPP could enlighten that an unusual ReLU-BN-Conv ordering would have some benefits but why should a network without normalization mimick the SPP trend of ResNet ? It is unnatural that a network without normalization should follow the ResNet 's behavior . 3.From the equation x_ { l+1 } =x_l+ a * f ( x_l/b_l ) at p.4 , the proposed approach eventually normalizing the feature even if a or b_l is fixed . Moreover , gamma in eq . ( 3 ) additionally scaling up or down the weight which ultimately gives an effect on the computed feature . 4.How did the authors compute gamma in eq . ( 3 ) in a training phase ? Why the provided code contains a learnable gamma ? 5.Why `` zero padding '' at p.5 affects the variance decay in the rightmost graph in Figure 2 ? 6.Please clarify why RegNetY-400MF chose it as the baseline . It is not clear that the authors pick RegNet and tune it highly and compared with EfficientNets . If the authors decided to use RegNets , then it is natural to use NF-SWS-RegNets are compared with the original RegNets without any big modifications as shown in the model details in the Appendix . 7.Did the authors use trained ResNetV2-600 or randomly-initialized model ? 8.How does SPP goes on with the post-activation network which uses the original bottleneck block consists of Conv-BN-ReLU ? 9.How can the resolution downsampling block in a ResNet affect averaged channel mean and variance ? 10.Why the ResNet experiments are done with weight decay of 5e-5 ? The common knowledge of training ResNet is with weighing decay of 1e-4 , so one may unconvinced the result because of the tuning . 11.Comparing EfficientNets that are trained without CutMix and Mixup ( but used randaug or autoaug ) with the proposed models with cutmix and mixup seems to be not fair .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their valuable suggestions and detailed comments . We have included a top-level response that addresses some of the reviewer \u2019 s comments as well as describes additional experimental results we have now added to an updated version of the paper . Here we present additional responses for R1 . > \u201c None of the disadvantages of BN are addressed by the proposed method \u201d The proposed method overcomes the following disadvantages : BatchNorm introduces a dependence between elements in a batch . Normalizer-Free Networks with Scaled weight Standardization do not have this property . Please see our top-level response titled \u201c The trouble with BatchNorm \u201d for a discussion on how this batch-dependence can be problematic in models . This batch-dependence aspect of BatchNorm also means that it is not always clear how to implement it in distributed training . When one may have a low per-device batch size , BatchNorm can behave poorly or require the additional communication overhead of synced cross-replica BatchNorm . In contrast , our proposed method is completely invariant to the number of devices ( its implementation does not change ) and does not require cross-device communication in the forward pass . As requested by the reviewers , we have now added ResNet experiments in the small batch setting to the paper , where networks with BatchNorm perform very poorly , while NF-ResNets perform well . BatchNorm has different behavior at training and test time . Norm-Free Nets and Scaled Weight Standardization behave the same at training and testing time . Scaled WS also does not require the maintenance of running averages . Other alternatives that remove the train-test discrepancy ( GroupNorm or LayerNorm ) incur compute costs at inference . Scaled WS does not have this property : it can be completely \u201c constant-folded \u201d at test time , and is accordingly free at inference ( as with BatchNorm ) . BatchNorm can incur substantial memory overhead and can be expensive to compute when using large image sizes . This has previously motivated such works as \u201c In-Place Activated BatchNorm \u201d , for tasks such as semantic segmentation where this is an issue . Scaled WS does not have this disadvantage , as it operates on the weights , which for ConvNets tend to be small relative to the size of the activations . We have added new experimental results to our paper ( mentioned in our top-level response ) comparing training speed to demonstrate the computational advantage of Scaled WS over BN . > \u201c why should a network without normalization mimick the SPP trend of ResNet \u201d The choice of how signals should propagate in unnormalized networks is largely one of design . In Appendix G.2 , we note that we initially explored designing networks to have constant variance , which without prior knowledge one might assume to be a superior choice . We found that such networks were not as performant , and reasoned that mimicking a signal propagation template which we know to work well was a good design choice , as is supported by our experiments . > \u201c How did the authors compute gamma in eq . ( 3 ) in a training phase ? Why the provided code contains a learnable gamma ? \u201d Gamma is a nonlinearity-specific scalar constant . It is not learnable , but is a fixed scalar derived similar to the constants from He Initialization , in order to make a nonlinearity approximately variance-preserving . See section 4.3 in the paper for more discussion on this . As mentioned in the text , gamma values can be computed analytically if the form of the variance is known when assuming the distribution of the input , or approximated empirically using the code provided in Appendix D.1 . The provided code does contain a zero-initialized scalar gain at the end of a ResNet block , in order to allow one to implement SkipInit . As mentioned in the paper , this is one of several relaxations we employ which we find to work well empirically . > \u201c Did the authors use trained ResNetV2-600 or randomly-initialized model ? \u201d The SPPs are presented for models at initialization . We have clarified this in the paper . > \u201c How does SPP goes on with the post-activation network which uses the original bottleneck block consists of Conv-BN-ReLU ? \u201d We have now added the SPP for post-activation ResNets and ResNets with FixUp in the appendix section F ( figures 10 and 11 ) ."}}