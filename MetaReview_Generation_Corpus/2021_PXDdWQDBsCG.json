{"year": "2021", "forum": "PXDdWQDBsCG", "title": "Shape Defense", "decision": "Reject", "meta_review": "The paper proposed a method for adversarial robustness by considering information from the edge map of the images. Two reviewers point out the similarities of the paper with previous work ([1]) and it is unclear whether the benefits come from binarization of the input or from shape information. As such, the paper is not suggested for publication at this time", "reviews": [{"review_id": "PXDdWQDBsCG-0", "review_text": "Summary : This paper aims to improve adversarial robustness considering the information about the object shape details with the means of edge maps . Two different strategies are proposed to increase model robustness using the edge maps : i ) conduct the adversarial training on the input images , which are concatenated with its corresponding edge map as an additional input channel to the image . Here , the edge maps are recomputed and concatenated to the adversarial inputs after their generation during adversarial training . ii ) Utilize a conditional GAN to generate the images from clean data distribution that is conditioned on the edge maps and later the classifier performance is evaluated on the generated images . Here , the authors claim that these two strategies improves the classifier robustness after conducting experiments across 10 different datasets . They also studied the effectiveness of their strategy when combined with background subtraction , the defense against poisoning attacks and robustness against natural image corruptions . Strengths : + Motivation is clear . + The proposed strategies are interesting , explained clearly , easy to follow and are different from existing works . + Rigorous experiments across 10 different datasets are carried out to demonstrate the effectiveness of the proposed strategies . + Results demonstrate that the model robustness can be significantly improved using proposed strategies utilizing the edge maps . The improvement not only limited to adversarial robustness but also extends to backdoor attacks and natural image corruptions . Weaknesses : - The major concern lies in the evaluation of the proposed strategies . Here , the authors considers that their method purify the input image before passing it to the model and an adaptive attack against their edge map based defense strategies will likely results in structural damage to the edge map . However , it is crucial to evaluate the proposed defense against an adversarial attack which craft the adversarial examples to produce minimal structural alterations to the edge map but mislead the model predictions . An adversary could potentially optimize the perturbation in such manner and may remain successful in attacking the model . - Results on CIFAR-10 and Icons-50 of GAN bases shape defense depicted in Figure 4 do not provide solid evidence on the model robustness . Here , the performance on clean inputs degraded significantly and the improvement seen in perturbed samples might be the result of the trade-off between model robustness and generalization as noted in the literature . - The claims on robustness against natural image corruptions using the edge maps seems to be valid only on GTSRB dataset and do not hold true for TinyImageNet and Icons-50 as seen in Figure 6 . The robustness of the model with edge maps is similar or on par with the model without edge maps on these two datasets . These results suggest that additional usage of edge maps do not improve model robustness and the improved performance seen on GTSRB could be attributed to the simple nature of the objects in the dataset . Final thoughts : The proposed method is clearly motivated . Although the performance gains on adversarial robustness is significant , there are critical points yet to be addressed . Therefore , I marginally accept this paper . Post rebuttal : The authors have devised an adaptive attack to craft the adversarial examples against edge maps and shown that the proposed technique is still remain robust . However , the essence of robustness in this work lies in the BINARIZATION of the input ( i.e. , binarized edge maps ) which is shown in the previous work [ 1 ] and need not necessarily attribute to the shape information obtained through edge maps . I recently came across state-of-the-art deep edge detector [ 2 ] that produces non-binarized edge maps , which could be interesting for authors to validate their approach using such non-binary inputs . Hence , I maintain my initial rating and marginally accept this paper . [ 1 ] ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS , ICLR 2019 [ 2 ] Richer Convolutional Features for Edge Detection , CVPR 2017", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for finding our work interesting and for your review . Regarding evaluation of the proposed strategies : Indeed , we believe that any perceptible adaptive attack against our edge map based defense strategies will likely result in perceptible structural damage to the edge map ( and hence easily detectable by the eye ) . Designing attacks to add minimal ( and imperceptible ) structural alterations to the edge map is non-trivial and perhaps not easy to come up with . However , one possible approach that we can think of is to use a deep edge detector in the pipeline and attack the entire pipeline with an adversarial attack . We will add the results here soon . GSD results on CIFAR-10 and Icons-50 : We did not really try to optimize the GSD method to achieve the best image generation here . It is more like a proof of concept on natural scenes . We used the very basic image translation method ( pix2pix ) without perceptual loss . Performance of the GSD approach depends on the dataset as well . For example , constructing faces from face edge map is much easier since faces are usually registered at eyes . For natural scenes , it is harder but some recent methods are very promising ( BIG GAN and StyleGAN ) . Nevertheless , as we show even without perfect image generation , models are more robust compared to the classifier trained on the original images . The trade-off between accuracy and robustness is mainly because of the imperfect conditional image generation . In other words , generating better images is going to improve both accuracy and robustness ( no trade-off ) . Robustness against natural image corruptions : Perhaps not very pronounced , but across the three datasets there is at least one case where incorporating edges outperforms the original image-only classifier . Notice that we are using the Canny edge detector , and this is why results are better over GTSRB . With better edge detectors ( e.g. , deep ones ) , it is expected to see better results . Overall , our results are in alignment with Geirhos et al . ( 2018 ) where they showed ResNet-50 trained on the Stylized-ImageNet dataset performs better than the vanilla ResNet-50 on both clean and distorted images . As you mentioned , our approach is orthogonal to the existing works in adversarial defense and introduces a new idea which is using shape information . Shape and edge are invariant to adversarial perturbations ( and also common image corruptions ) as shown in Fig.1 and are important features based on which humans classify objects and process scenes . Conversely , CNNs rely more on texture . We believe this is perhaps the main reason why CNNs are so brittle . Moving forward in object recognition , it seems inevitable to use shape information to achieve robustness ."}, {"review_id": "PXDdWQDBsCG-1", "review_text": "Summary : - In this paper , the authors try to learn robust models for visual recognition and propose two defense methods , Edge-guided Adversarial Training and GAN-based Shape Defense ( GSD ) , to use shape bias and background subtraction to strengthen the model robustness . However , I have still some concerns below : - In summary , the paper is hard to follow and the writting is not clear , such as the detailed motivation of the proposed methods and the structure of this paper . - For the experiments , a big dataset is needed , such as CIFAR-100 . In addition , the results are not convincing , i.e. , the evaluation on FGSM and PGD attack is not enough , some gradient-free attacks are needed .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review ! Interestingly , the other reviewers find the writing and structure of the paper very clear . We motivate the idea clearly in Fig.1.We will expand some parts of the paper ( since an additional page will be allowed ) and will move Fig.7 in the supplement to the main text . Regarding need for big datasets : We already have 10 datasets ( with different image resolutions ) and some of them are bigger than CIFAR-100 ! Regarding gradient-free attacks : As mentioned in the paper , our main goal was to show that incorporating edges helps adversarial robustness over a range of attacks including white-box , black-box , and backdoor as well as common image corruptions . We did not aim to optimize the approach to beat all other defenses . Gradient free methods are slow and hard to conduct . Further , attacks such as the one-pixel attack , will not change the edge map . Therefore , it is very likely that they wont be effective against our proposed approaches . For instance , a classifier trained on the edge map will not be affected by only changing one pixel in the image . In other words , the edge map will be the same . Our defenses offer new ideas and are different from the existing approaches ."}, {"review_id": "PXDdWQDBsCG-2", "review_text": "This paper investigates incorporating shape information in deep neural networks to improve their adversarial robustness . It proposes two methods : the first one is to augment the input with the corresponding edge and then adversarially train a CNN on the augmented input . The second idea is to train a conditional GAN to reconstruct images from edge maps and use the reconstructed image as input to a standard classifier . 1.The description of the proposed defense in section 3 seems to be limited . It is not clear why the author applied a conditional GAN to reconstruct clean images from edge maps . In other words , what is the motivation for designing GSD on top of EAT ? 2.The authors use Canny edge detector to extract edges . Why not use neural network based edge extractors [ 2 ] as they give better edges ? What is the motivation here ? 3.Considering the possible obfuscated gradient issues of white-box attacks [ 3 ] , the authors should explicitly describe their efforts to evaluate against strong custom adaptive attacks . 4.In terms of the experiments , the authors claim that they investigated adaptive attack but I did not see any quantitative experiment results . They also claim that any adaptive attack would cause perceptible changes to the edges . This is not an excuse for not doing quantitative study ; the authors already considered adversarial perturbations with magnitude as large as 64 . Such magnitude can also cause perceptible changes to images as Figure 8 shows . 5.For EAT , what is the performance if the model is not adversarially trained ? Why use adversarial training in EAT but not in GSD ? I believe these analyses are required for an in-depth understanding of how the proposed defense works . 6.Last but not least , the algorithms proposed in this paper looks similar ( almost the same ) to this paper [ 1 ] from previous year : ( a ) The edge-guided adversarial training ( EST ) is basically applying adversarial training on EdgeNetRob in [ 1 ] ; ( b ) The GAN-based shape defense ( GSD ) is exactly the same as EdgeGANRob in [ 1 ] ; ( c ) Both of them use canny edge detector to extract edges . Can the authors highlight the differences ? If this is a separate paper , given the previous work [ 1 ] that already proposed this idea , the contribution of this work seems to be limited . [ 1 ] Shape Features Improve General Model Robustness . https : //openreview.net/forum ? id=SJlPZlStwS , 2019 . [ 2 ] Richer Convolutional Features for Edge Detection . Liu , et al TPAMI , 2019 . [ 3 ] Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples . Anish et al , ICML 2018 .", "rating": "3: Clear rejection", "reply_text": "Thanks for your review and your detailed comments . 1.GSD is not on top of EAT and does not extend it . As is mentioned , our goal is to purify the input using GSD . We were inspired by the pix2pix paper where edges were converted to the rgb images . So if the edge map stays the same for an imperceptible perturbation it should result it in a good generated image and hence accurate classification by a classifier trained on generated images and/or original images . We will expand this section . 2.Proposed methods can be used with any edge detection model . The reason we choose Canny is because it is easy to use and it is hard to attack it since it has a complicated algorithm . Nonetheless , we are going to use deep learning edge detectors and attack the entire pipeline . We will also include results against other strong attacks such as the boundary attack . 3 & 4.As an additional analysis against adaptive attacks , we will use deep edge detectors and perform attacks against the entire pipeline . We will also consider other attacks such as the boundary attacks and also gradient free attacks . See rev # 3 's comments . 5.Results of EAT over original images ( without adversarial training ) is shown on the left columns for all datasets . We do perform adversarial training over GSD . Please see Fig.4.Stars ( over MNIST and FashionMNIST ) show performance when a classifier is trained over images generated from edge maps of original and perturbed images . As it can be seen adversarial training of GSD improves the robustness . 6.Thanks for the pointer . Indeed this work is similar to ours but we have invented the proposed algorithms independently and were not aware of this work at the time of the submission . We went through this paper . First , it has been withdrawn from the last year ICLR and seems not to be published elsewhere . Therefore , it is not possible to assess their methods . Nonetheless , there are significant differences between our methods and theirs as explained below : - We concat the edge map in depth to the rgb image whereas they train a classifier on the edge map . Their approach is just one of our baselines . - Our approach ( EAT ) allows adversarial training whereas EdgeNetRob is just a classifier on top of the edge map . - They propose a customized canny edge detector to make it robust , whereas we use the basic Canny or Sobel edge detectors . - We evaluate our approach against common natural perturbations - We motivate the observation that edge map is invariant to adversarial perturbations - We extend other defenses such as FAST adversarial training and Free adversarial training - They present results over Fashion MNIST and CIFAR10 whereas here we present results over 10 datasets ( including natural scenes ) - We propose foreground detection to defend against backdoor attacks in conjunction with edge detection - Compared to their GAN based method , we use natural scenes whereas they use Fashion MNIST and Celeb dataset which are much easier than CIFAR and GTSRB - We evaluate models over a wide range of parameters and attacks We will certainly also discuss the mentioned paper in the discussion section and compare our work with theirs ."}, {"review_id": "PXDdWQDBsCG-3", "review_text": "This paper studies how to incorporate shape ( particularly depth map ) into CNN for more robust models . The study focuses on image classification . Specifically , this paper proposes two depth-map-based defense : 1 ) Edge-guided Adversarial Training ( EAT ) , which use depth map as an additional input 2 ) GAN-based Shape Defense ( GSD ) , which learns a generator from depth map to reconstructed images , which is then used as net input . Experiments on 10 datasets shows the effectiveness of the proposed two defenses against white-box attacks including FGSM and PGD40 . To further demonstrate the effectiveness , the authors also conduct some other experiments : 1 ) the proposed EAT goes well with two fast AT algorithms ; 2 ) the proposed algorithm can also be used to defend backdoor attack ; 3 ) edge makes CNN more robust to common image corruptions . I think the topic that tries to explore and understand the connection between shape and CNN is very important and somewhat under-studied . This paper provides some interesting empirical results and insights to the community . 1.The assumption of this paper is that edge map does not change much under adversarial attacks . I think this relies on two things : A ) we use tradition non-deep-net based edge detector like canny edge ; B ) the adversarial perturbation is pixel-based perturbation . I am curious to see how the proposed algorithm work with B ) , but for now , I will focus on A ) below . For A ) , for harder and more realistic datasets , canny may not work well and we may resort to deep-net-based edge detector ( like HED ) . I think the author also mentions this for cifar10 and tinyimagenet , which is only 32x32 and 64x64 . This problem likely becomes more severe when we deal with larger real images . But , if we use deep-net-based edge detector , we now break the assumption that edge map does not change much under adversarial attacks . Since it is deep net and so it is fragile . So I am not sure how the proposed method work with deep-net-based edge detectors , on a somewhat more realistic image datasets . 2.For GSD , as mentioned by the authors , it is similar to the two GAN/VAE-based baselines . I am curious to see how it compares with them ? 3.Also for GSD , considering scaling up to more realistic images with more visual patterns , it would be super hard to learn a mapping from pure depth to rgb , since it is ill-posed and under-determined . The two baseline methods do not have this under-determination because their input is rgb images . Also , the learned mapper would be very correlated with and overfitting to the training data , which hinders generalization . 4.How does CW attack perform against the proposed defense ? 5.A minor thing about reference . For the fast AT algorithms , to my best knowledge , I think there is a third one `` Bilateral Adversarial Training : Towards Fast Training of More Robust Models Against Adversarial Attacks '' published in ICCV 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and finding our work interesting and important . 1.Regarding B ) a pixel-based perturbation ( or modifying a few pixels for that matter ) , has to be sharp enough to change the edge map and consequently the classifier built on the edge map . Such a sharp modification to the pixel value may make the perturbation perceptible ( assuming that edge detector matches with human perception of edges and shape ) . Regarding A ) Great point ! Indeed , using a deep edge detector which tend to capture more salient and perceptually appealing edges , may improve the results , and as you mentioned breaks the system since the entire pipeline is differentiable now . The key here is that we use the Canny method which generates binary edge maps . If a deep edge detector also generates a binary edge map and is used inside the image \u2014 > edge detector \u2014 > classifier pipeline , the attacker then can only back propagate to the image if he uses a differentiable function ( eg sigmoid ) at the end of the edge detector . Crafting an adversarial image for the new system , will likely perturb the pixels slightly ( it won \u2019 t change the main edges to keep the perturbation imperceptible ) . Submitting the adversarial image to the original pipeline that uses the binarized version of the deep edge detector then will not fool the classifier . We have conducted an experiment that used HED edge detector to build adversarial images . Adversarial images crafted for this pipeline do not fool the original classifier build on Canny edge maps . Please see Appendix L. 2 . Compare with DefenseGAN paper , our results are slightly lower than theirs but it is mainly because the classifiers that we use have lower standard accuracy ( eg. , over MNIST the standard accuracy of their model is 99.0+ whereas ours is around 96.0+ , over MNIST their classifier has around 92 % accuracy where as we get around 88 % ) . This explains why their robust model perform a bit better than us . They do not however report results on CIFAR and other natural scenes datasets , whereas we do . It might not be easy to find the correct z corresponding to the clean image closest to the perturbed image using DefenseGAN over natural scenes . In our method , we explicitly use shape information which makes it easier to explain and aligns with other works highlighting the importance of shape in object recognition . Finally , as is also mentioned in the DefenseGAN paper , \u201c The success of Defense-GAN relies on the expressiveness and generative power of the GAN . However , training GANs is still a challenging task and an active area of research , and if the GAN is not properly trained and tuned , the performance of Defense-GAN will suffer on both original and adversarial examples \u201d . 3.Do you mean \u2018 edge map \u2019 by \u2018 depth \u2019 ? The quality of generated data depends of GAN performance . Even without conditioning on edge map some recent GANs ( eg. , BigGAN ) are able to generate high quality cluttered natural scenes . Conditioning on an edge map ( even imperfect ) makes the job easier . As we have discussed using better GANs will improve the presented results in Fig.4.The two baselines use the original image but they are not robust ! Also , regarding your comment \u201c the learned mapper would be very correlated with and overfitting to the training data , which hinders generalization. \u201d , why would be the case if generated images have high quality ( for example over MNIST and FashionMNIST ) . Finally , the idea of purification GAN has of course limitations . Its accuracy will depend on the type of data . If a GAN works well on a certain dataset the GSD would be a good defense ! So , future progress on GAN will result in better GSD . Low performance on GSD on some problems ( again we have not optimized the method by exploring all GANs ) does not mean this method is not worth further exploration . 4.We performed an additional experiment to test the robustness of the proposed method against CW . Results shown in Appx . J show that shape defense is robust to CW attack . 5.Thanks for mentioning this paper . We will cite it in the version of the paper ."}], "0": {"review_id": "PXDdWQDBsCG-0", "review_text": "Summary : This paper aims to improve adversarial robustness considering the information about the object shape details with the means of edge maps . Two different strategies are proposed to increase model robustness using the edge maps : i ) conduct the adversarial training on the input images , which are concatenated with its corresponding edge map as an additional input channel to the image . Here , the edge maps are recomputed and concatenated to the adversarial inputs after their generation during adversarial training . ii ) Utilize a conditional GAN to generate the images from clean data distribution that is conditioned on the edge maps and later the classifier performance is evaluated on the generated images . Here , the authors claim that these two strategies improves the classifier robustness after conducting experiments across 10 different datasets . They also studied the effectiveness of their strategy when combined with background subtraction , the defense against poisoning attacks and robustness against natural image corruptions . Strengths : + Motivation is clear . + The proposed strategies are interesting , explained clearly , easy to follow and are different from existing works . + Rigorous experiments across 10 different datasets are carried out to demonstrate the effectiveness of the proposed strategies . + Results demonstrate that the model robustness can be significantly improved using proposed strategies utilizing the edge maps . The improvement not only limited to adversarial robustness but also extends to backdoor attacks and natural image corruptions . Weaknesses : - The major concern lies in the evaluation of the proposed strategies . Here , the authors considers that their method purify the input image before passing it to the model and an adaptive attack against their edge map based defense strategies will likely results in structural damage to the edge map . However , it is crucial to evaluate the proposed defense against an adversarial attack which craft the adversarial examples to produce minimal structural alterations to the edge map but mislead the model predictions . An adversary could potentially optimize the perturbation in such manner and may remain successful in attacking the model . - Results on CIFAR-10 and Icons-50 of GAN bases shape defense depicted in Figure 4 do not provide solid evidence on the model robustness . Here , the performance on clean inputs degraded significantly and the improvement seen in perturbed samples might be the result of the trade-off between model robustness and generalization as noted in the literature . - The claims on robustness against natural image corruptions using the edge maps seems to be valid only on GTSRB dataset and do not hold true for TinyImageNet and Icons-50 as seen in Figure 6 . The robustness of the model with edge maps is similar or on par with the model without edge maps on these two datasets . These results suggest that additional usage of edge maps do not improve model robustness and the improved performance seen on GTSRB could be attributed to the simple nature of the objects in the dataset . Final thoughts : The proposed method is clearly motivated . Although the performance gains on adversarial robustness is significant , there are critical points yet to be addressed . Therefore , I marginally accept this paper . Post rebuttal : The authors have devised an adaptive attack to craft the adversarial examples against edge maps and shown that the proposed technique is still remain robust . However , the essence of robustness in this work lies in the BINARIZATION of the input ( i.e. , binarized edge maps ) which is shown in the previous work [ 1 ] and need not necessarily attribute to the shape information obtained through edge maps . I recently came across state-of-the-art deep edge detector [ 2 ] that produces non-binarized edge maps , which could be interesting for authors to validate their approach using such non-binary inputs . Hence , I maintain my initial rating and marginally accept this paper . [ 1 ] ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS , ICLR 2019 [ 2 ] Richer Convolutional Features for Edge Detection , CVPR 2017", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for finding our work interesting and for your review . Regarding evaluation of the proposed strategies : Indeed , we believe that any perceptible adaptive attack against our edge map based defense strategies will likely result in perceptible structural damage to the edge map ( and hence easily detectable by the eye ) . Designing attacks to add minimal ( and imperceptible ) structural alterations to the edge map is non-trivial and perhaps not easy to come up with . However , one possible approach that we can think of is to use a deep edge detector in the pipeline and attack the entire pipeline with an adversarial attack . We will add the results here soon . GSD results on CIFAR-10 and Icons-50 : We did not really try to optimize the GSD method to achieve the best image generation here . It is more like a proof of concept on natural scenes . We used the very basic image translation method ( pix2pix ) without perceptual loss . Performance of the GSD approach depends on the dataset as well . For example , constructing faces from face edge map is much easier since faces are usually registered at eyes . For natural scenes , it is harder but some recent methods are very promising ( BIG GAN and StyleGAN ) . Nevertheless , as we show even without perfect image generation , models are more robust compared to the classifier trained on the original images . The trade-off between accuracy and robustness is mainly because of the imperfect conditional image generation . In other words , generating better images is going to improve both accuracy and robustness ( no trade-off ) . Robustness against natural image corruptions : Perhaps not very pronounced , but across the three datasets there is at least one case where incorporating edges outperforms the original image-only classifier . Notice that we are using the Canny edge detector , and this is why results are better over GTSRB . With better edge detectors ( e.g. , deep ones ) , it is expected to see better results . Overall , our results are in alignment with Geirhos et al . ( 2018 ) where they showed ResNet-50 trained on the Stylized-ImageNet dataset performs better than the vanilla ResNet-50 on both clean and distorted images . As you mentioned , our approach is orthogonal to the existing works in adversarial defense and introduces a new idea which is using shape information . Shape and edge are invariant to adversarial perturbations ( and also common image corruptions ) as shown in Fig.1 and are important features based on which humans classify objects and process scenes . Conversely , CNNs rely more on texture . We believe this is perhaps the main reason why CNNs are so brittle . Moving forward in object recognition , it seems inevitable to use shape information to achieve robustness ."}, "1": {"review_id": "PXDdWQDBsCG-1", "review_text": "Summary : - In this paper , the authors try to learn robust models for visual recognition and propose two defense methods , Edge-guided Adversarial Training and GAN-based Shape Defense ( GSD ) , to use shape bias and background subtraction to strengthen the model robustness . However , I have still some concerns below : - In summary , the paper is hard to follow and the writting is not clear , such as the detailed motivation of the proposed methods and the structure of this paper . - For the experiments , a big dataset is needed , such as CIFAR-100 . In addition , the results are not convincing , i.e. , the evaluation on FGSM and PGD attack is not enough , some gradient-free attacks are needed .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review ! Interestingly , the other reviewers find the writing and structure of the paper very clear . We motivate the idea clearly in Fig.1.We will expand some parts of the paper ( since an additional page will be allowed ) and will move Fig.7 in the supplement to the main text . Regarding need for big datasets : We already have 10 datasets ( with different image resolutions ) and some of them are bigger than CIFAR-100 ! Regarding gradient-free attacks : As mentioned in the paper , our main goal was to show that incorporating edges helps adversarial robustness over a range of attacks including white-box , black-box , and backdoor as well as common image corruptions . We did not aim to optimize the approach to beat all other defenses . Gradient free methods are slow and hard to conduct . Further , attacks such as the one-pixel attack , will not change the edge map . Therefore , it is very likely that they wont be effective against our proposed approaches . For instance , a classifier trained on the edge map will not be affected by only changing one pixel in the image . In other words , the edge map will be the same . Our defenses offer new ideas and are different from the existing approaches ."}, "2": {"review_id": "PXDdWQDBsCG-2", "review_text": "This paper investigates incorporating shape information in deep neural networks to improve their adversarial robustness . It proposes two methods : the first one is to augment the input with the corresponding edge and then adversarially train a CNN on the augmented input . The second idea is to train a conditional GAN to reconstruct images from edge maps and use the reconstructed image as input to a standard classifier . 1.The description of the proposed defense in section 3 seems to be limited . It is not clear why the author applied a conditional GAN to reconstruct clean images from edge maps . In other words , what is the motivation for designing GSD on top of EAT ? 2.The authors use Canny edge detector to extract edges . Why not use neural network based edge extractors [ 2 ] as they give better edges ? What is the motivation here ? 3.Considering the possible obfuscated gradient issues of white-box attacks [ 3 ] , the authors should explicitly describe their efforts to evaluate against strong custom adaptive attacks . 4.In terms of the experiments , the authors claim that they investigated adaptive attack but I did not see any quantitative experiment results . They also claim that any adaptive attack would cause perceptible changes to the edges . This is not an excuse for not doing quantitative study ; the authors already considered adversarial perturbations with magnitude as large as 64 . Such magnitude can also cause perceptible changes to images as Figure 8 shows . 5.For EAT , what is the performance if the model is not adversarially trained ? Why use adversarial training in EAT but not in GSD ? I believe these analyses are required for an in-depth understanding of how the proposed defense works . 6.Last but not least , the algorithms proposed in this paper looks similar ( almost the same ) to this paper [ 1 ] from previous year : ( a ) The edge-guided adversarial training ( EST ) is basically applying adversarial training on EdgeNetRob in [ 1 ] ; ( b ) The GAN-based shape defense ( GSD ) is exactly the same as EdgeGANRob in [ 1 ] ; ( c ) Both of them use canny edge detector to extract edges . Can the authors highlight the differences ? If this is a separate paper , given the previous work [ 1 ] that already proposed this idea , the contribution of this work seems to be limited . [ 1 ] Shape Features Improve General Model Robustness . https : //openreview.net/forum ? id=SJlPZlStwS , 2019 . [ 2 ] Richer Convolutional Features for Edge Detection . Liu , et al TPAMI , 2019 . [ 3 ] Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples . Anish et al , ICML 2018 .", "rating": "3: Clear rejection", "reply_text": "Thanks for your review and your detailed comments . 1.GSD is not on top of EAT and does not extend it . As is mentioned , our goal is to purify the input using GSD . We were inspired by the pix2pix paper where edges were converted to the rgb images . So if the edge map stays the same for an imperceptible perturbation it should result it in a good generated image and hence accurate classification by a classifier trained on generated images and/or original images . We will expand this section . 2.Proposed methods can be used with any edge detection model . The reason we choose Canny is because it is easy to use and it is hard to attack it since it has a complicated algorithm . Nonetheless , we are going to use deep learning edge detectors and attack the entire pipeline . We will also include results against other strong attacks such as the boundary attack . 3 & 4.As an additional analysis against adaptive attacks , we will use deep edge detectors and perform attacks against the entire pipeline . We will also consider other attacks such as the boundary attacks and also gradient free attacks . See rev # 3 's comments . 5.Results of EAT over original images ( without adversarial training ) is shown on the left columns for all datasets . We do perform adversarial training over GSD . Please see Fig.4.Stars ( over MNIST and FashionMNIST ) show performance when a classifier is trained over images generated from edge maps of original and perturbed images . As it can be seen adversarial training of GSD improves the robustness . 6.Thanks for the pointer . Indeed this work is similar to ours but we have invented the proposed algorithms independently and were not aware of this work at the time of the submission . We went through this paper . First , it has been withdrawn from the last year ICLR and seems not to be published elsewhere . Therefore , it is not possible to assess their methods . Nonetheless , there are significant differences between our methods and theirs as explained below : - We concat the edge map in depth to the rgb image whereas they train a classifier on the edge map . Their approach is just one of our baselines . - Our approach ( EAT ) allows adversarial training whereas EdgeNetRob is just a classifier on top of the edge map . - They propose a customized canny edge detector to make it robust , whereas we use the basic Canny or Sobel edge detectors . - We evaluate our approach against common natural perturbations - We motivate the observation that edge map is invariant to adversarial perturbations - We extend other defenses such as FAST adversarial training and Free adversarial training - They present results over Fashion MNIST and CIFAR10 whereas here we present results over 10 datasets ( including natural scenes ) - We propose foreground detection to defend against backdoor attacks in conjunction with edge detection - Compared to their GAN based method , we use natural scenes whereas they use Fashion MNIST and Celeb dataset which are much easier than CIFAR and GTSRB - We evaluate models over a wide range of parameters and attacks We will certainly also discuss the mentioned paper in the discussion section and compare our work with theirs ."}, "3": {"review_id": "PXDdWQDBsCG-3", "review_text": "This paper studies how to incorporate shape ( particularly depth map ) into CNN for more robust models . The study focuses on image classification . Specifically , this paper proposes two depth-map-based defense : 1 ) Edge-guided Adversarial Training ( EAT ) , which use depth map as an additional input 2 ) GAN-based Shape Defense ( GSD ) , which learns a generator from depth map to reconstructed images , which is then used as net input . Experiments on 10 datasets shows the effectiveness of the proposed two defenses against white-box attacks including FGSM and PGD40 . To further demonstrate the effectiveness , the authors also conduct some other experiments : 1 ) the proposed EAT goes well with two fast AT algorithms ; 2 ) the proposed algorithm can also be used to defend backdoor attack ; 3 ) edge makes CNN more robust to common image corruptions . I think the topic that tries to explore and understand the connection between shape and CNN is very important and somewhat under-studied . This paper provides some interesting empirical results and insights to the community . 1.The assumption of this paper is that edge map does not change much under adversarial attacks . I think this relies on two things : A ) we use tradition non-deep-net based edge detector like canny edge ; B ) the adversarial perturbation is pixel-based perturbation . I am curious to see how the proposed algorithm work with B ) , but for now , I will focus on A ) below . For A ) , for harder and more realistic datasets , canny may not work well and we may resort to deep-net-based edge detector ( like HED ) . I think the author also mentions this for cifar10 and tinyimagenet , which is only 32x32 and 64x64 . This problem likely becomes more severe when we deal with larger real images . But , if we use deep-net-based edge detector , we now break the assumption that edge map does not change much under adversarial attacks . Since it is deep net and so it is fragile . So I am not sure how the proposed method work with deep-net-based edge detectors , on a somewhat more realistic image datasets . 2.For GSD , as mentioned by the authors , it is similar to the two GAN/VAE-based baselines . I am curious to see how it compares with them ? 3.Also for GSD , considering scaling up to more realistic images with more visual patterns , it would be super hard to learn a mapping from pure depth to rgb , since it is ill-posed and under-determined . The two baseline methods do not have this under-determination because their input is rgb images . Also , the learned mapper would be very correlated with and overfitting to the training data , which hinders generalization . 4.How does CW attack perform against the proposed defense ? 5.A minor thing about reference . For the fast AT algorithms , to my best knowledge , I think there is a third one `` Bilateral Adversarial Training : Towards Fast Training of More Robust Models Against Adversarial Attacks '' published in ICCV 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and finding our work interesting and important . 1.Regarding B ) a pixel-based perturbation ( or modifying a few pixels for that matter ) , has to be sharp enough to change the edge map and consequently the classifier built on the edge map . Such a sharp modification to the pixel value may make the perturbation perceptible ( assuming that edge detector matches with human perception of edges and shape ) . Regarding A ) Great point ! Indeed , using a deep edge detector which tend to capture more salient and perceptually appealing edges , may improve the results , and as you mentioned breaks the system since the entire pipeline is differentiable now . The key here is that we use the Canny method which generates binary edge maps . If a deep edge detector also generates a binary edge map and is used inside the image \u2014 > edge detector \u2014 > classifier pipeline , the attacker then can only back propagate to the image if he uses a differentiable function ( eg sigmoid ) at the end of the edge detector . Crafting an adversarial image for the new system , will likely perturb the pixels slightly ( it won \u2019 t change the main edges to keep the perturbation imperceptible ) . Submitting the adversarial image to the original pipeline that uses the binarized version of the deep edge detector then will not fool the classifier . We have conducted an experiment that used HED edge detector to build adversarial images . Adversarial images crafted for this pipeline do not fool the original classifier build on Canny edge maps . Please see Appendix L. 2 . Compare with DefenseGAN paper , our results are slightly lower than theirs but it is mainly because the classifiers that we use have lower standard accuracy ( eg. , over MNIST the standard accuracy of their model is 99.0+ whereas ours is around 96.0+ , over MNIST their classifier has around 92 % accuracy where as we get around 88 % ) . This explains why their robust model perform a bit better than us . They do not however report results on CIFAR and other natural scenes datasets , whereas we do . It might not be easy to find the correct z corresponding to the clean image closest to the perturbed image using DefenseGAN over natural scenes . In our method , we explicitly use shape information which makes it easier to explain and aligns with other works highlighting the importance of shape in object recognition . Finally , as is also mentioned in the DefenseGAN paper , \u201c The success of Defense-GAN relies on the expressiveness and generative power of the GAN . However , training GANs is still a challenging task and an active area of research , and if the GAN is not properly trained and tuned , the performance of Defense-GAN will suffer on both original and adversarial examples \u201d . 3.Do you mean \u2018 edge map \u2019 by \u2018 depth \u2019 ? The quality of generated data depends of GAN performance . Even without conditioning on edge map some recent GANs ( eg. , BigGAN ) are able to generate high quality cluttered natural scenes . Conditioning on an edge map ( even imperfect ) makes the job easier . As we have discussed using better GANs will improve the presented results in Fig.4.The two baselines use the original image but they are not robust ! Also , regarding your comment \u201c the learned mapper would be very correlated with and overfitting to the training data , which hinders generalization. \u201d , why would be the case if generated images have high quality ( for example over MNIST and FashionMNIST ) . Finally , the idea of purification GAN has of course limitations . Its accuracy will depend on the type of data . If a GAN works well on a certain dataset the GSD would be a good defense ! So , future progress on GAN will result in better GSD . Low performance on GSD on some problems ( again we have not optimized the method by exploring all GANs ) does not mean this method is not worth further exploration . 4.We performed an additional experiment to test the robustness of the proposed method against CW . Results shown in Appx . J show that shape defense is robust to CW attack . 5.Thanks for mentioning this paper . We will cite it in the version of the paper ."}}