{"year": "2018", "forum": "Sy5OAyZC-", "title": "On the Use of Word Embeddings Alone to Represent Natural Language Sequences", "decision": "Reject", "meta_review": "This work presents a strong baseline model for several NLP-ish tasks such as document classification, sentence classification, representation learning based NLI, and text matching. In terms of originality, reviewers found that \"there is not much contribution in terms of technical novelty\" but that \"one might also conclude that we need more challenging dataset\". There was significant discussion about whether it \"sheds new lights on limitations of existing methods\" or whether the results were \"marginally surprising\". In terms of quality, reviewers found it to be an \"insightful analysis\" and noted that these \"SWEMs should be considered a strong baseline in future work\".\n\nThere was significant discussion with the AC about the signficance of the work. In the opinion of the AC reviewers did were too quick to accept the authors novelty claims, and did not push them enough to include other baselines in their tables that were not overly deep model. In particular the AC felt that important numbers were left out of the experiment tables, for document classification that muddied the results. The response of the authors was:\n\n\"Moreover, fasttext and our SWEM variants all belong to the category of simpler methods (with parameter-free compositional functions). Since our motivation is to explore the necessity of employing complicated compositional functions for various NLP tasks, we do not think it is necessary for us to make any comparisons between fasttext and SWEM.\"\n\nIn addition when a reviewer pointed out the lack of inclusion of FOFE embeddings, the authors noted something similar\n\n\"Besides, we totally agree that developing sentence embeddings that are both simple and efficient is a very promising research direction (FOFE is a great work along this line).\"\n\nThe reviewer correctly pointed out related work that shows a model very similar to what the author's propose. In general this seems like evidence that the techniques are known, not that they are significant and novel. ", "reviews": [{"review_id": "Sy5OAyZC--0", "review_text": "This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word-embedding based models. Average and/or max pooling over word embeddings (which are initialized from pretrained embeddings) is used to obtain a fixed-length representation for natural language sequences, which is then fed through a single layer MLP classifier. In many of the 9 evaluation tasks, this approach is found to match or outperform single-layer CNNs or RNNs. The varied findings are very clearly presented and helpfully summarized, and for each task setting the authors perform an insightful analysis. My only criticism would be the fact that the study is limited to English, even though the conclusions are explicitly scoped in light of this. Moreover, I wonder how well the findings would hold in a setting with a more severe OOV problem than is perhaps present in the studied datasets. Besides concluding from the presented results that these SWEMs should be considered a strong baseline in future work, one might also conclude that we need more challenging datasets! Minor things: - It wasn't entirely clear how the text matching tasks are encoded. Are the two sequences combined into a single sequence before applying the model, or something else? I might have missed this detail. - Given the two ways of using the Glove embeddings for initialization (direct update vs mapping them with an MLP into the task space), it would be helpful to know which one ended up being used (i.e. optimal) in each setting. - Something went wrong with the font size for the remainder of the text near Figure 1. ** Update ** Thanks for addressing my questions in the author response. After following the other discussion thread about the novelty claims, I believe I didn't weigh that aspect strongly enough in my original rating, so I'm revising it. I remain of the opinion that this paper offers a useful systematic comparison that goes sufficiently beyond the focus of the two related papers mentioned in that thread (fasttext and Parikh's). ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive review ! - For the text matching tasks , we first use a certain ( single ) compositional function ( mean/max pooling , LSTM or CNN ) to encode both sequences into two fixed-length vectors . Then we compare the two vectors by taking their concatenation , element-wise subtraction and element-wise product . These three features are concatenated together and further sent to an MLP classifier for prediction . - We found that the following tasks performed stronger empirically by mapping with an MLP , while keeping the Glove embeddings fixed : SNLI , MultiNLI , MR , SST-1 , SST-2 and TREC . This will be included in future edition . - We agree that extending our investigation to other language would be an interesting future direction to pursue . Besides , we definitely need more challenging datasets ( where higher-level semantic features can be leveraged ) for a deeper understanding of natural language ! - OOV problem : empirically , we found that the performance of SWEMs is not sensitive to the choice of vocabulary size , in other words , the number of OOV words . As discussed in the Supplementary , the key words used for predictions are typically of a frequency of around 200 to 300 in the training set . Therefore , we conjecture that treating those relatively rarely words ( e.g.appear less than 50 times ) as OOV would not have a big impact on the final results . - Thanks for pointing out . We will fix the font size issue in the revised version ."}, {"review_id": "Sy5OAyZC--1", "review_text": "This paper empirically investigates the differences realized by using compositional functions over word embeddings as compared to directly operating the word embeddings. That is, the authors seek to explore the advantages afforded by RNN/CNN based models that induce intermediate semantic representations of texts, as opposed to simpler (parameter-free) approaches to composing these, like addition. In sum, I think this is exploration is interesting, and suggests that we should perhaps experiment more regularly with simple aggregation methods like SWEM. On the other hand, the differences across the models is relatively modest, and the data resists clear conclusions, so I'm not sure that the work will be very impactful. In my view, then, this work does constitute a contribution, albeit a modest one. I do think the general notion of attempting to simplify models until performance begins to degrade is a fruitful path to explore, as models continue to increase in complexity despite compelling evidence that this is always needed. Strengths --- + This paper does highlight a gap in existing work, as far as I am aware: namely, I am not sure that there are generally known trade-offs associated with different compositional models over token embeddings for NLP. However, it is not clear that we should expect there to be a consistent result to this question across all NLP tasks. + The results are marginally surprising, insofar as I would have expected the CNN/RNN (particularly the former) to dominate the simpler aggregation approaches, and this does not seem borne out by the data. Although this trend is seemingly reversed on the short text data, muddying the story. Weaknesses --- - There are a number of important limitations here, many of which the authors themselves note, which mitigate the implications of the reported results. First, this is a small set of tasks, and results may not hold more generally. It would have been nice to see some work on Seq2Seq tasks, or sequence tagging tasks at least. - I was surprised to see no mention of the \"Fixed-Size Ordinally-Forgetting Encoding Method\" (FOFE) proposed by Zhang et al. in 2015, which would seem to be a natural point of comparison here, given that it sits in a sweet spot of being simple and efficient while still expressive enough to preserve word-order information. This actually seems like a pretty glaring omission given that it meets many of the desiderata the authors put forward. - The interpretability angle discussed seems underdeveloped. I'm not sure that being able to identify individual words (as the authors have listed) meaningfully constitutes \"interpretability\" -- standard CNNs, e.g., lend themselves to this as well by tracing back through the filter activations. - Some of the questions addressed seem tangential to the main question of the paper -- e.g., word vector dimensionality seems an orthogonal issue to the composition function, and would influence performance for the more complex architectures as well. Smaller comments --- - On page 1, the authors write \"By representing each word as a fixed-length vector, these embeddings can group semantically similar words, while explicitly encoding rich linguistic regularities and patterns\", but actually I would say that these *implicitly* encode such regularities, rather than explicitly. - \"architecture in Kim 2014; Collobert et al. 2011; Gan et al. 2017\" -- citation formatting a bit weird here. *** Update based on author response *** I have read the authors response and thank them for the additional details. Regarding the limited set of problems: of course any given work can only explore so many tasks, but for this to have general implications in NLP I would maintain that a standard (structured) sequence tagging task/dataset should have been considered. This is not about the number of datasets, but rather than diversity of the output spaces therein. I appreciated the additional details regarding FOFE, which as the authors themselves note in their response is essentially a generalization of SWEM. Overall, the response has not changed my opinion on this paper: I think this (exploring simple representations and baselines) is an important direction in NLP, but feel that the paper would greatly benefit from additional work. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive feedback ! - Although our paper has discussed a limited set of problems ( which is true for almost any research ) , we argue that we have explored 15 different NLP datasets ( detailed information in Supplementary ) , which should have covered a wide range of real-world application scenarios . More importantly , our work also sheds lights on how SWEM model works and what types of information are needed for distinct tasks . Therefore , we suppose that our conclusions here should be helpful and general in many cases of interest . For example , if we are solving a text sequence matching problem where word-order information does not contribute a lot ( including textual entailment , paraphrase identification , question answering ) , according to our research , we would know that employing complicated compositions , such as LSTM or CNN , may not be necessary . In this regard , our work reveals several general rules ( along with careful analysis ) on rationally selecting model for various NLP tasks , which should be useful for future research . - \u201c Interpretability \u201d definition : we think that there are some misunderstandings here . The key of our \u201c interpretability \u201d here is that we can endow each dimension of word embeddings , learned by SWEM-max , with a topic-specific meaning . That is , embeddings for individual words with a shared semantic topic typically have their largest values in a shared dimension . We are aware that word embeddings , such as Word2vec , can also be interpreted with some simple vector arithmetics ( e.g.element-wise addition ) , but we suppose that the property of word vectors mentioned above could be an even more straightforward interpretation regarding how information has been encoded in word vectors . This type of \u201c interpretability \u201d has been previously discussed in [ 1 , 2 ] . - FOFE model : Thanks for pointing out this inspiring reference . The idea of employing a constant forgetting factor to model word-order information is very interesting . In this regard , we implemented the FOFE model and tested it on both Yahoo and Yelp Polarity datasets . We experimented with different choice of the constant forgetting factor ( \\alpha ) : \\alpha 0.9 0.99 0.999 0.9999 1.0 SWEM-aver SWEM-concat Yelp P. 84.58 93.01 93.81 93.79 93.48 93.59 93.76 Yahoo ! Ans 72.66 72.72 73.03 72.82 72.97 73.14 73.53 It is worth noting that when \\alpha = 1 , FOFE is very similar to SWEM-aver model , except the fact that FOFE takes the sum over all words , rather than average . As shown above , with a careful selection of \\alpha , FOFE can get slightly better performance on Yelp dataset ( with \\alpha = 0.999 ) , compared to SWEM-concat . While on Yahoo dataset , we do not observe significant performance gains with the FOFE model . These results are in consistent with our observations that word-order features are necessary for sentiment analysis , but not for topic prediction . We will include this reference and the additional results in the revised version . Besides , we totally agree that developing sentence embeddings that are both simple and efficient is a very promising research direction ( FOFE is a great work along this line ) . - Thanks for pointing out the wording and format issue . We will fix them accordingly in the revision . Hopefully our clarifications could address the concerns and questions raised in your review . Thanks ! [ 1 ] Lipton , Zachary C. `` The mythos of model interpretability . '' arXiv preprint arXiv:1606.03490 ( 2016 ) . [ 2 ] Subramanian , Anant , et al . `` SPINE : SParse Interpretable Neural Embeddings . '' arXiv preprint arXiv:1711.08792 ( 2017 ) ."}, {"review_id": "Sy5OAyZC--2", "review_text": "This paper extensively compares simple word embedding based models (SWEMs) to RNN/CNN based-models on a suite of NLP tasks. Experiments on document classification, sentence classification, and natural language sequence matching show that SWEMs perform competitively or even better in the majority of cases. The authors also propose to use max pooling to complement average pooling for combining information from word embeddings in a SWEM model to improve interpretability. While there is not much contribution in terms of technical novelty, I think this is an interesting paper that sheds new lights on limitations of existing methods for learning sentence and document representations. The paper is well written and the experiments are quite convincing. - An interesting finding is that word embeddings are better for longer documents, whereas RNN/CNN models are better for shorter text. Do the authors have any sense on whether this is because of the difficulty in training an RNN/CNN model for long documents or whether compositions are not necessary since there are multiple predictive independent cues in a long text? - It would be useful to include a linear classification model that takes the word embeddings as an input in the comparison (SWEM-learned). - How crucial is it to retrain the word embeddings on the task of interest (from GloVe initialization) to obtain good performance?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your positive feedback ! - According to our experiments , we tend to think that compositions are not as necessary for longer documents as for short sentences , which is the main reason that SWEM performs comparable or even better than RNN or CNN . The evidence here is two-fold : first , for Yelp review datasets , the text sequences considered are also long documents . However , since word-order features are necessary for sentiment analysis tasks ( as demonstrated from multiple perspectives in the paper ) , CNN or LSTM has shown better results than SWEM . This indicates that even in the case of modeling longer text , LSTM and CNN could potentially take advantage of compositional ( word-order ) features if necessary . Second , we did observe that there are typically multiple key words ( i.e.predictive independent cues ) in a longer text for prediction , especially in the case of topic predictions ( where the key words could be very topic-specific ) . This may intuitively explain why compositions are not necessary for document categorization . - Thanks for suggesting this ! We agree that including a linear classifier comparison would be useful . In this regard , we trained and tested our SWEM-concat model along with a linear classifier ( denoted as SWEM-linear ) , the results are shown as below ( word embeddings are initialized from GloVe and directly updated during training ) : Model Yahoo ! Ans.Yelp P. SWEM-concat 73.53 93.76 SWEM-linear 73.18 93.66 As shown above , employing a linear classifier only leads to a very small performance drop for both Yahoo and Yelp datasets . This observation highlights that SWEM model is able to extract robust and informative sentence representations . - It is quite necessary to fine-tune the GloVe embeddings . As discussed in the paper , an intrinsic difference between GloVe and SWEM-learned embeddings is that the latter are very sparse . This is closely related to the fact that SWEM is utilizing the key words to make predictions . As a result , we would need to update GloVe embeddings or transform them to another space to boost the model performance . At the same time , we also found that for large-scale datasets ( such as Yahoo or Yelp dataset ) , initializing with GloVe does not contribute a lot to the final results ( i.e.randomly initializing the word embeddings leads to similar performance ) ."}], "0": {"review_id": "Sy5OAyZC--0", "review_text": "This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word-embedding based models. Average and/or max pooling over word embeddings (which are initialized from pretrained embeddings) is used to obtain a fixed-length representation for natural language sequences, which is then fed through a single layer MLP classifier. In many of the 9 evaluation tasks, this approach is found to match or outperform single-layer CNNs or RNNs. The varied findings are very clearly presented and helpfully summarized, and for each task setting the authors perform an insightful analysis. My only criticism would be the fact that the study is limited to English, even though the conclusions are explicitly scoped in light of this. Moreover, I wonder how well the findings would hold in a setting with a more severe OOV problem than is perhaps present in the studied datasets. Besides concluding from the presented results that these SWEMs should be considered a strong baseline in future work, one might also conclude that we need more challenging datasets! Minor things: - It wasn't entirely clear how the text matching tasks are encoded. Are the two sequences combined into a single sequence before applying the model, or something else? I might have missed this detail. - Given the two ways of using the Glove embeddings for initialization (direct update vs mapping them with an MLP into the task space), it would be helpful to know which one ended up being used (i.e. optimal) in each setting. - Something went wrong with the font size for the remainder of the text near Figure 1. ** Update ** Thanks for addressing my questions in the author response. After following the other discussion thread about the novelty claims, I believe I didn't weigh that aspect strongly enough in my original rating, so I'm revising it. I remain of the opinion that this paper offers a useful systematic comparison that goes sufficiently beyond the focus of the two related papers mentioned in that thread (fasttext and Parikh's). ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive review ! - For the text matching tasks , we first use a certain ( single ) compositional function ( mean/max pooling , LSTM or CNN ) to encode both sequences into two fixed-length vectors . Then we compare the two vectors by taking their concatenation , element-wise subtraction and element-wise product . These three features are concatenated together and further sent to an MLP classifier for prediction . - We found that the following tasks performed stronger empirically by mapping with an MLP , while keeping the Glove embeddings fixed : SNLI , MultiNLI , MR , SST-1 , SST-2 and TREC . This will be included in future edition . - We agree that extending our investigation to other language would be an interesting future direction to pursue . Besides , we definitely need more challenging datasets ( where higher-level semantic features can be leveraged ) for a deeper understanding of natural language ! - OOV problem : empirically , we found that the performance of SWEMs is not sensitive to the choice of vocabulary size , in other words , the number of OOV words . As discussed in the Supplementary , the key words used for predictions are typically of a frequency of around 200 to 300 in the training set . Therefore , we conjecture that treating those relatively rarely words ( e.g.appear less than 50 times ) as OOV would not have a big impact on the final results . - Thanks for pointing out . We will fix the font size issue in the revised version ."}, "1": {"review_id": "Sy5OAyZC--1", "review_text": "This paper empirically investigates the differences realized by using compositional functions over word embeddings as compared to directly operating the word embeddings. That is, the authors seek to explore the advantages afforded by RNN/CNN based models that induce intermediate semantic representations of texts, as opposed to simpler (parameter-free) approaches to composing these, like addition. In sum, I think this is exploration is interesting, and suggests that we should perhaps experiment more regularly with simple aggregation methods like SWEM. On the other hand, the differences across the models is relatively modest, and the data resists clear conclusions, so I'm not sure that the work will be very impactful. In my view, then, this work does constitute a contribution, albeit a modest one. I do think the general notion of attempting to simplify models until performance begins to degrade is a fruitful path to explore, as models continue to increase in complexity despite compelling evidence that this is always needed. Strengths --- + This paper does highlight a gap in existing work, as far as I am aware: namely, I am not sure that there are generally known trade-offs associated with different compositional models over token embeddings for NLP. However, it is not clear that we should expect there to be a consistent result to this question across all NLP tasks. + The results are marginally surprising, insofar as I would have expected the CNN/RNN (particularly the former) to dominate the simpler aggregation approaches, and this does not seem borne out by the data. Although this trend is seemingly reversed on the short text data, muddying the story. Weaknesses --- - There are a number of important limitations here, many of which the authors themselves note, which mitigate the implications of the reported results. First, this is a small set of tasks, and results may not hold more generally. It would have been nice to see some work on Seq2Seq tasks, or sequence tagging tasks at least. - I was surprised to see no mention of the \"Fixed-Size Ordinally-Forgetting Encoding Method\" (FOFE) proposed by Zhang et al. in 2015, which would seem to be a natural point of comparison here, given that it sits in a sweet spot of being simple and efficient while still expressive enough to preserve word-order information. This actually seems like a pretty glaring omission given that it meets many of the desiderata the authors put forward. - The interpretability angle discussed seems underdeveloped. I'm not sure that being able to identify individual words (as the authors have listed) meaningfully constitutes \"interpretability\" -- standard CNNs, e.g., lend themselves to this as well by tracing back through the filter activations. - Some of the questions addressed seem tangential to the main question of the paper -- e.g., word vector dimensionality seems an orthogonal issue to the composition function, and would influence performance for the more complex architectures as well. Smaller comments --- - On page 1, the authors write \"By representing each word as a fixed-length vector, these embeddings can group semantically similar words, while explicitly encoding rich linguistic regularities and patterns\", but actually I would say that these *implicitly* encode such regularities, rather than explicitly. - \"architecture in Kim 2014; Collobert et al. 2011; Gan et al. 2017\" -- citation formatting a bit weird here. *** Update based on author response *** I have read the authors response and thank them for the additional details. Regarding the limited set of problems: of course any given work can only explore so many tasks, but for this to have general implications in NLP I would maintain that a standard (structured) sequence tagging task/dataset should have been considered. This is not about the number of datasets, but rather than diversity of the output spaces therein. I appreciated the additional details regarding FOFE, which as the authors themselves note in their response is essentially a generalization of SWEM. Overall, the response has not changed my opinion on this paper: I think this (exploring simple representations and baselines) is an important direction in NLP, but feel that the paper would greatly benefit from additional work. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive feedback ! - Although our paper has discussed a limited set of problems ( which is true for almost any research ) , we argue that we have explored 15 different NLP datasets ( detailed information in Supplementary ) , which should have covered a wide range of real-world application scenarios . More importantly , our work also sheds lights on how SWEM model works and what types of information are needed for distinct tasks . Therefore , we suppose that our conclusions here should be helpful and general in many cases of interest . For example , if we are solving a text sequence matching problem where word-order information does not contribute a lot ( including textual entailment , paraphrase identification , question answering ) , according to our research , we would know that employing complicated compositions , such as LSTM or CNN , may not be necessary . In this regard , our work reveals several general rules ( along with careful analysis ) on rationally selecting model for various NLP tasks , which should be useful for future research . - \u201c Interpretability \u201d definition : we think that there are some misunderstandings here . The key of our \u201c interpretability \u201d here is that we can endow each dimension of word embeddings , learned by SWEM-max , with a topic-specific meaning . That is , embeddings for individual words with a shared semantic topic typically have their largest values in a shared dimension . We are aware that word embeddings , such as Word2vec , can also be interpreted with some simple vector arithmetics ( e.g.element-wise addition ) , but we suppose that the property of word vectors mentioned above could be an even more straightforward interpretation regarding how information has been encoded in word vectors . This type of \u201c interpretability \u201d has been previously discussed in [ 1 , 2 ] . - FOFE model : Thanks for pointing out this inspiring reference . The idea of employing a constant forgetting factor to model word-order information is very interesting . In this regard , we implemented the FOFE model and tested it on both Yahoo and Yelp Polarity datasets . We experimented with different choice of the constant forgetting factor ( \\alpha ) : \\alpha 0.9 0.99 0.999 0.9999 1.0 SWEM-aver SWEM-concat Yelp P. 84.58 93.01 93.81 93.79 93.48 93.59 93.76 Yahoo ! Ans 72.66 72.72 73.03 72.82 72.97 73.14 73.53 It is worth noting that when \\alpha = 1 , FOFE is very similar to SWEM-aver model , except the fact that FOFE takes the sum over all words , rather than average . As shown above , with a careful selection of \\alpha , FOFE can get slightly better performance on Yelp dataset ( with \\alpha = 0.999 ) , compared to SWEM-concat . While on Yahoo dataset , we do not observe significant performance gains with the FOFE model . These results are in consistent with our observations that word-order features are necessary for sentiment analysis , but not for topic prediction . We will include this reference and the additional results in the revised version . Besides , we totally agree that developing sentence embeddings that are both simple and efficient is a very promising research direction ( FOFE is a great work along this line ) . - Thanks for pointing out the wording and format issue . We will fix them accordingly in the revision . Hopefully our clarifications could address the concerns and questions raised in your review . Thanks ! [ 1 ] Lipton , Zachary C. `` The mythos of model interpretability . '' arXiv preprint arXiv:1606.03490 ( 2016 ) . [ 2 ] Subramanian , Anant , et al . `` SPINE : SParse Interpretable Neural Embeddings . '' arXiv preprint arXiv:1711.08792 ( 2017 ) ."}, "2": {"review_id": "Sy5OAyZC--2", "review_text": "This paper extensively compares simple word embedding based models (SWEMs) to RNN/CNN based-models on a suite of NLP tasks. Experiments on document classification, sentence classification, and natural language sequence matching show that SWEMs perform competitively or even better in the majority of cases. The authors also propose to use max pooling to complement average pooling for combining information from word embeddings in a SWEM model to improve interpretability. While there is not much contribution in terms of technical novelty, I think this is an interesting paper that sheds new lights on limitations of existing methods for learning sentence and document representations. The paper is well written and the experiments are quite convincing. - An interesting finding is that word embeddings are better for longer documents, whereas RNN/CNN models are better for shorter text. Do the authors have any sense on whether this is because of the difficulty in training an RNN/CNN model for long documents or whether compositions are not necessary since there are multiple predictive independent cues in a long text? - It would be useful to include a linear classification model that takes the word embeddings as an input in the comparison (SWEM-learned). - How crucial is it to retrain the word embeddings on the task of interest (from GloVe initialization) to obtain good performance?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your positive feedback ! - According to our experiments , we tend to think that compositions are not as necessary for longer documents as for short sentences , which is the main reason that SWEM performs comparable or even better than RNN or CNN . The evidence here is two-fold : first , for Yelp review datasets , the text sequences considered are also long documents . However , since word-order features are necessary for sentiment analysis tasks ( as demonstrated from multiple perspectives in the paper ) , CNN or LSTM has shown better results than SWEM . This indicates that even in the case of modeling longer text , LSTM and CNN could potentially take advantage of compositional ( word-order ) features if necessary . Second , we did observe that there are typically multiple key words ( i.e.predictive independent cues ) in a longer text for prediction , especially in the case of topic predictions ( where the key words could be very topic-specific ) . This may intuitively explain why compositions are not necessary for document categorization . - Thanks for suggesting this ! We agree that including a linear classifier comparison would be useful . In this regard , we trained and tested our SWEM-concat model along with a linear classifier ( denoted as SWEM-linear ) , the results are shown as below ( word embeddings are initialized from GloVe and directly updated during training ) : Model Yahoo ! Ans.Yelp P. SWEM-concat 73.53 93.76 SWEM-linear 73.18 93.66 As shown above , employing a linear classifier only leads to a very small performance drop for both Yahoo and Yelp datasets . This observation highlights that SWEM model is able to extract robust and informative sentence representations . - It is quite necessary to fine-tune the GloVe embeddings . As discussed in the paper , an intrinsic difference between GloVe and SWEM-learned embeddings is that the latter are very sparse . This is closely related to the fact that SWEM is utilizing the key words to make predictions . As a result , we would need to update GloVe embeddings or transform them to another space to boost the model performance . At the same time , we also found that for large-scale datasets ( such as Yahoo or Yelp dataset ) , initializing with GloVe does not contribute a lot to the final results ( i.e.randomly initializing the word embeddings leads to similar performance ) ."}}