{"year": "2019", "forum": "ByeMB3Act7", "title": "Learning to Screen for Fast Softmax  Inference on Large Vocabulary Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive soft-max inference scales linearly with the vocabulary size. The proposed approach is based on a clustering step combined with per-cluster, smaller soft-maxes. It retains differentiability with the Gumbel softmax trick. The experimental results are impressive. There are some minor flaws, however there's consensus among the reviewers the paper should be published.\n", "reviews": [{"review_id": "ByeMB3Act7-0", "review_text": "This paper proposes a novel method to speedup softmax computation at test time. Their approach is to partition the large vocabulary set into several discrete clusters, select the cluster first, and then do a small scale exact softmax in the selected cluster. Training is done by utilizing the Gumbel softmax trick. Pros: 1. The method provides another way that allows the model to learn an adaptive clustering of vocabulary. And the whole model is made differentiable by the Gumbel softmax trick. 2. The experimental results, in terms of precision, is quite strong. The proposed method is significantly better than baseline methods, which is a really exciting thing to see. 3. The paper is written clearly and the method is simple and easily understandable. Cons: 1. I\u2019d be really expecting to see how the model will perform if it is trained from scratch in NMT tasks. And I have reasons for this. Since the model is proposed for large vocabularies, the vocabulary of PTB (10K) is by no terms large. However, the vocabulary size in NMT could easily reach 30K, which would be a more suitable testbed for showing the advantage of the proposed method. 2. Apart from the nice precision results, the performance margin in terms of perplexity seems not as big as that of precision. And according to earlier discussions in the thread, the author confirmed that they are comparing the precision w.r.t. original softmax, not the true next words. This could raise a possible assumption that the model doesn\u2019t really get the probabilities correct, but somehow only fits on the rank of the words that was predicted by the original softmax. Maybe that is related to the loss? However, I believe sorting this problem out is kind of beyond the scope of this paper. 3. In another scenario, I think adding some qualitative analysis could better present the work. For example, visualize the words that got clustered into the same cluster, etc. In general, I am satisfied with the content and enjoys reading the paper. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments and that you enjoyed reading the paper ! Responses to questions : -- about larger vocabulary experiment : We have added an experiment with a much larger dataset -- - Wikitext103 with vocabulary size to be 80k . The result of prediction time speedup versus accuracy is shown in Figure 9 in the new version . As you can see from the figure , we can achieve more than 15x speedup with accuracy of 99.8 % . In addition , in Table 3 , we show the result on DE-EN , an NMT task with vocabulary size around 25k . We summarize the vocabulary size of all the datasets in Table 1 . -- about perplexity and probability estimation This is a great point . We agree that our method tends to generate better approximation of ranking of the words instead of probability of that word . The main reason for the reduced gain for PPL is that to compute PPL , after performing our method ( L2S ) , we need an additional step to assign a probability to words that are not located in the predicted cluster , although this is a rare case ( less than 5 % chance ) . There are several potential ways to model this rare case and we chose to use SVD to approximate probability ( same as svd softmax [ Kyuhong Shim et.al in NIPS 2017 ] ) ; however , SVD itself has lots of computational overhead . Therefore prediction time speedup is less pronounced for PPL than for the accuracy results . On the other hand , we get reasonable probability estimation when the word is within the predicted cluster ( usually they are top-k predicted words ) . Therefore we still achieve very good ( > 10x ) speed up in NMT tasks with beam search ( see Table 3 ) . -- about qualitative analysis We have added two qualitative analyses in the new version . Firstly , we show the words from different clusters learned from our method in Table 7 , and observe some interesting structures -- some words with similar meanings are in the same cluster . Secondly , examples of translation pairs by our method compared with original softmax results are shown in Table 8 ."}, {"review_id": "ByeMB3Act7-1", "review_text": "This paper presents an approximation to the softmax function to reduce the computational cost at inference time and the proposed approach is evaluated on language modeling and machine translation tasks. The main idea of the proposed approach is to pick a subset of the most probable outputs on which exact softmax is performed to sample top-k targets. The proposed method, namely Learning to Screen (L2S), learns jointly context vector clustering and candidate subsets in an end-to-end fashion, so that it enables to achieve competitive performance. The authors carried out NMT experiments over the vocabulary size of 25K. It would be interesting if the authors provide a result on speed-up of L2S over full softmax with respect to the vocabulary size. Also, the performance of L2S on larger vocabularies such as 80K or 100K needs to be discussed. Any quantitative examples regarding the clustering parameters and label sets would be helpful. L2S is designed to learn to screen a few words, but no example of the screening part is provided in the paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We want to thank the reviewer for the useful suggestions ! ! -- about larger vocabulary experiment : We have added an experiment with a much larger dataset -- - Wikitext103 with vocabulary size of 80k . The result of prediction time speedup versus accuracy is shown in Figure 9 in the new version . As you can see from the figure , we can achieve more than 15x speedup with accuracy of 99.8 % . In addition , in Table 3 , we show the result on DE-EN , an NMT task with vocabulary size around 25k . We summarize the vocabulary size of all the datasets in Table 1 . -- about result on speed-up of L2S over full softmax with respect to the vocabulary size We have included an experiment of prediction time speed-up versus vocabulary size on PTB dataset . Results are summarized in Figure 8 . In this figure , we could observe that our method can achieve higher speed-up with larger vocabulary size . -- about clustering parameters and label sets We have added Table 7 to show the label sets learned from our method . We observe some interesting clusters -- -some words with similar meanings are in the same cluster ."}, {"review_id": "ByeMB3Act7-2", "review_text": "The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. * pros: - the paper is well written. - the idea is simple but BRILLIANT. - the used techniques are good (especially to learn word clusters). - the experimental results (speed up softmax at test time) are impressive. * cons: - the model is not end-to-end because word clusters are not continuous. But it not an important factor. - it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time. - it would be better if the authors show some clusters for both input examples and corresponding word clusters. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We are thankful for the constructive comments ! ! -- about word clusters are not continuous and training end to end There are several ways to make word clusters continuous such as using soft clustering , however , these strategies on the other hand will increase the prediction time . Even though word clusters representation is not continuous in L2S , our model can still train end-to-end in the sense that the clustering stage and the label selection are trained jointly with the gumbel technique . Our algorithm back-propagates the gradient to the clustering weights to update both clustering partition and label sets simultaneously . -- about speeding up training time We focus on speeding up prediction in this work . We could potentially use the same idea -- clustering+learning candidate words , to speed up training as well since we could narrow down the update on a few candidate words instead of the entire vocabulary when updating softmax \u2019 s weight matrix . This is certainly an interesting future direction to work on . -- qualitative examples We have added two qualitative analyses in the new version . Firstly , we show the words from different clusters learned from our method in Table 7 , and observe some interesting structures -- -some words with similar meanings are in the same cluster . Secondly , examples of translation pairs by our method compared with full softmax results are shown in Table 8 ."}], "0": {"review_id": "ByeMB3Act7-0", "review_text": "This paper proposes a novel method to speedup softmax computation at test time. Their approach is to partition the large vocabulary set into several discrete clusters, select the cluster first, and then do a small scale exact softmax in the selected cluster. Training is done by utilizing the Gumbel softmax trick. Pros: 1. The method provides another way that allows the model to learn an adaptive clustering of vocabulary. And the whole model is made differentiable by the Gumbel softmax trick. 2. The experimental results, in terms of precision, is quite strong. The proposed method is significantly better than baseline methods, which is a really exciting thing to see. 3. The paper is written clearly and the method is simple and easily understandable. Cons: 1. I\u2019d be really expecting to see how the model will perform if it is trained from scratch in NMT tasks. And I have reasons for this. Since the model is proposed for large vocabularies, the vocabulary of PTB (10K) is by no terms large. However, the vocabulary size in NMT could easily reach 30K, which would be a more suitable testbed for showing the advantage of the proposed method. 2. Apart from the nice precision results, the performance margin in terms of perplexity seems not as big as that of precision. And according to earlier discussions in the thread, the author confirmed that they are comparing the precision w.r.t. original softmax, not the true next words. This could raise a possible assumption that the model doesn\u2019t really get the probabilities correct, but somehow only fits on the rank of the words that was predicted by the original softmax. Maybe that is related to the loss? However, I believe sorting this problem out is kind of beyond the scope of this paper. 3. In another scenario, I think adding some qualitative analysis could better present the work. For example, visualize the words that got clustered into the same cluster, etc. In general, I am satisfied with the content and enjoys reading the paper. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments and that you enjoyed reading the paper ! Responses to questions : -- about larger vocabulary experiment : We have added an experiment with a much larger dataset -- - Wikitext103 with vocabulary size to be 80k . The result of prediction time speedup versus accuracy is shown in Figure 9 in the new version . As you can see from the figure , we can achieve more than 15x speedup with accuracy of 99.8 % . In addition , in Table 3 , we show the result on DE-EN , an NMT task with vocabulary size around 25k . We summarize the vocabulary size of all the datasets in Table 1 . -- about perplexity and probability estimation This is a great point . We agree that our method tends to generate better approximation of ranking of the words instead of probability of that word . The main reason for the reduced gain for PPL is that to compute PPL , after performing our method ( L2S ) , we need an additional step to assign a probability to words that are not located in the predicted cluster , although this is a rare case ( less than 5 % chance ) . There are several potential ways to model this rare case and we chose to use SVD to approximate probability ( same as svd softmax [ Kyuhong Shim et.al in NIPS 2017 ] ) ; however , SVD itself has lots of computational overhead . Therefore prediction time speedup is less pronounced for PPL than for the accuracy results . On the other hand , we get reasonable probability estimation when the word is within the predicted cluster ( usually they are top-k predicted words ) . Therefore we still achieve very good ( > 10x ) speed up in NMT tasks with beam search ( see Table 3 ) . -- about qualitative analysis We have added two qualitative analyses in the new version . Firstly , we show the words from different clusters learned from our method in Table 7 , and observe some interesting structures -- some words with similar meanings are in the same cluster . Secondly , examples of translation pairs by our method compared with original softmax results are shown in Table 8 ."}, "1": {"review_id": "ByeMB3Act7-1", "review_text": "This paper presents an approximation to the softmax function to reduce the computational cost at inference time and the proposed approach is evaluated on language modeling and machine translation tasks. The main idea of the proposed approach is to pick a subset of the most probable outputs on which exact softmax is performed to sample top-k targets. The proposed method, namely Learning to Screen (L2S), learns jointly context vector clustering and candidate subsets in an end-to-end fashion, so that it enables to achieve competitive performance. The authors carried out NMT experiments over the vocabulary size of 25K. It would be interesting if the authors provide a result on speed-up of L2S over full softmax with respect to the vocabulary size. Also, the performance of L2S on larger vocabularies such as 80K or 100K needs to be discussed. Any quantitative examples regarding the clustering parameters and label sets would be helpful. L2S is designed to learn to screen a few words, but no example of the screening part is provided in the paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We want to thank the reviewer for the useful suggestions ! ! -- about larger vocabulary experiment : We have added an experiment with a much larger dataset -- - Wikitext103 with vocabulary size of 80k . The result of prediction time speedup versus accuracy is shown in Figure 9 in the new version . As you can see from the figure , we can achieve more than 15x speedup with accuracy of 99.8 % . In addition , in Table 3 , we show the result on DE-EN , an NMT task with vocabulary size around 25k . We summarize the vocabulary size of all the datasets in Table 1 . -- about result on speed-up of L2S over full softmax with respect to the vocabulary size We have included an experiment of prediction time speed-up versus vocabulary size on PTB dataset . Results are summarized in Figure 8 . In this figure , we could observe that our method can achieve higher speed-up with larger vocabulary size . -- about clustering parameters and label sets We have added Table 7 to show the label sets learned from our method . We observe some interesting clusters -- -some words with similar meanings are in the same cluster ."}, "2": {"review_id": "ByeMB3Act7-2", "review_text": "The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. * pros: - the paper is well written. - the idea is simple but BRILLIANT. - the used techniques are good (especially to learn word clusters). - the experimental results (speed up softmax at test time) are impressive. * cons: - the model is not end-to-end because word clusters are not continuous. But it not an important factor. - it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time. - it would be better if the authors show some clusters for both input examples and corresponding word clusters. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We are thankful for the constructive comments ! ! -- about word clusters are not continuous and training end to end There are several ways to make word clusters continuous such as using soft clustering , however , these strategies on the other hand will increase the prediction time . Even though word clusters representation is not continuous in L2S , our model can still train end-to-end in the sense that the clustering stage and the label selection are trained jointly with the gumbel technique . Our algorithm back-propagates the gradient to the clustering weights to update both clustering partition and label sets simultaneously . -- about speeding up training time We focus on speeding up prediction in this work . We could potentially use the same idea -- clustering+learning candidate words , to speed up training as well since we could narrow down the update on a few candidate words instead of the entire vocabulary when updating softmax \u2019 s weight matrix . This is certainly an interesting future direction to work on . -- qualitative examples We have added two qualitative analyses in the new version . Firstly , we show the words from different clusters learned from our method in Table 7 , and observe some interesting structures -- -some words with similar meanings are in the same cluster . Secondly , examples of translation pairs by our method compared with full softmax results are shown in Table 8 ."}}