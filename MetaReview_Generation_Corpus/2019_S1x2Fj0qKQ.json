{"year": "2019", "forum": "S1x2Fj0qKQ", "title": "Whitening and Coloring Batch Transform for GANs", "decision": "Accept (Poster)", "meta_review": "The paper addresses normalisation and conditioning of GANs. The authors propose to replace class-conditional batch norm with whitening and class-conditional coloring. Evaluation demonstrates that the method performs very well, and the ablation studies confirm the design choices. After extensive discussion, all reviewers agreed that this is a solid contribution, and the paper should be accepted. ", "reviews": [{"review_id": "S1x2Fj0qKQ-0", "review_text": "This paper proposed Whitening and Coloring (WC) transform to replace batch normalization (BN) in generators for GAN. WC generalize BN by normalizing features with decorrelating (whitening) matrix, and then denormalizing (coloring) features by learnable weights. The main advantage of WC is that it exploits the full correlation matrix of features, while BN only considers the diagonal. WC is differentiable and is only 1.32x slower than BN. The authors also apply conditional WC, which learn the parameters of coloring conditioned on labels, to conditional image generation. Experimental results show WC achieves better inception score and FI distance comparing to BN on CIFAR-10, CIFAR-100, STL-10 and Tiny Imagenet. Furthermore, the conditional image generation results by WC are better than all previous methods. I have some detailed comments below. + The paper is well written, and I generally enjoyed reading the paper. + The experimental results look sufficient, and I appreciate the ablation study sections. + The score on supervised CIFAR-10 is better than previous methods. - The main text is longer than expectation. I would suggest shorten section 3.1 Cholesky decomposition, section 4 conditional color transformation and the text in section 5 experiments. - The proposed WC transform is general. It is a bit unclear why it is particularly effective for generator in GAN. Exploiting the full correlation matrix sounds reasonable, but it may also introduce unstability. It would help if the authors have an intuitive way to show that whitening is better than normalization. - It is unclear why conditional WC can be used for generation conditioned on class labels. In Dumoulin 2016, conditional instance normalization is used for generating images conditioned on styles. As image styles are described by Gram matrix (correlation) of features, changing first order and second order statistics of features is reasonable for image generation conditioned on styles. I cannot understand why conditional WC can be used for generation conditioned on class labels. I would like the authors to carefully explain the motivation, and also provide visual results like using the same random noise as input, but only changing the class conditions. - It is unclear to me why the proposed whitening based on Cholesky decomposition is better than ZCA-based in Huang 2018. Specifically, could the authors explain why WC is better than W_{aca}C in Table 3? - The authors claim progressive GAN used a larger generator to achieve a better performance than WC. The WC layer is generally larger than BN layer and has more learnable parameters. Could the authors compare the number of parameter of generator in BN-ResNet, WC-ResNet, and progressive GAN? - In Table 3, std-C is better than WC-diag, which indicates coloring is more important. In Table 6, cWC-diag is better than c-std-C, which indicates whitening is more important. Why? - What is the batch size used for training? For conditional WC, do the samples in each minibatch have same label? - Having ImageNet results will be a big support for the paper. =========== comments after reading rebuttal =========== I appreciate the authors' feedback. I raised my score for Fig 7 showing the conditional images, and for experiments on ImageNet. I think WC is a reasonable extension to BN, and I generally like the extensive experiments. However, the paper is still borderline to me for the following concerns. - I strongly encourage the authors to shorten the paper to the recommended 8-page. - The motivation of WC for GAN is still unclear. WC is general extension of BN, and a simplified version has been shown to be effective for discrimination in Huang 2018. I understand the empirically good performance for GAN. But I am not convinced why WC is particularly effective for GAN, comparing to discrimination. The smoothness explanation of BN applies to both GAN and discrimination. I actually think it may be nontrivial to extend the smoothness argument from BN to WC. - The motivation of cWC is still unclear. I did not find the details of cBN for class-label conditions, and how they motivated it in (Gulrajani et al. (2017) and (Miyato et al. 2018). Even if it has been used before, I would encourage the authors to restate the motivation in the paper. Saying it has been used before is an unsatisfactory answer for an unintuitive setting. - Another less important comment is that it is still hard to say how much benefits we get from the more learnable parameters in WC than BN. It is probably not so important because it can be a good trade-off for state-of-the-art results. In table 3 for unconditioned generation, it looks like the benefits come a lot from the larger parameter space. For conditioned generation in table 6, I am not sure if whitening is conditioned or not, which makes it less reliable to me. If whitening is conditioned, then the samples in each minibatches may not be enough to get a stable whitening. If whitening is unconditioned, then there seems to be a mismatch between whitening and coloring. ====== second round after rebuttal ============= I raise the score again for the commitment of shortening the paper and the detailed response from the authors. That being said, I am not fully convinced about motivations for WC and cWC. - GAN training is more difficult and unstable, but that does not explain why WC is particularly effective for GAN training. - I have never seen papers saying cBN/cWC is better than other conditional generator conditioned on class labels. I think the capacity argument is interesting, but I am not sure if it applies to convolutional net (where the mean and variance of a channel is used), or how well it can explain the performance because neural nets are overparameterized in general. I would encourage authors to include these discussions in the paper. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your detailed review . Below our answers . Q : The main text is longer than expectation . I would suggest shorten section 3.1 Cholesky decomposition , section 4 conditional color transformation and the text in section 5 experiments . A : Thank you for your suggestion . It is not clear to us how much the new version can be different from the submitted one , however , in the final version we will shorten the paper and possibly move Sec.3.1 and 3.2 to the Appendix , if you think they are less important . -- -- - Q : The proposed WC transform is general . It is a bit unclear why it is particularly effective for generator in GAN . Exploiting the full correlation matrix sounds reasonable , but it may also introduce unstability . It would help if the authors have an intuitive way to show that whitening is better than normalization . A : Please , see our answer to Reviewer # 1 concerning the motivation behind the choice of a GAN scenario and our first answer to Reviewer # 3 concerning a more intuitive explanation of why whitening is better than standardization . -- -- - Q : It is unclear why conditional WC can be used for generation conditioned on class labels . In Dumoulin 2016 , conditional instance normalization is used for generating images conditioned on styles . As image styles are described by Gram matrix ( correlation ) of features , changing first order and second order statistics of features is reasonable for image generation conditioned on styles . I can not understand why conditional WC can be used for generation conditioned on class labels . I would like the authors to carefully explain the motivation , and also provide visual results like using the same random noise as input , but only changing the class conditions . A : Since the introduction of the cBN in ( Dumoulin et al . ( 2016b ) ) , many works showed that this is a very powerful method for representing object-class information in conditional GANs ( e.g. , ( Gulrajani et al . ( 2017 ) ) ; ( Miyato et al . ( 2018 ) ) ) .We basically extend this idea using ( class-specific ) convolutional filters instead of ( class-specific ) scaling parameters ( Sec.1 , 2 ) .Note that G needs information about y . Our conditional coloring filters implicitly represent y by projecting the features into class-specific distributions . In other words , all the layers of our generator share the weights across all the classes , except for the convolutional filters in the conditional-coloring layers . This is enough for the network to influence the image generation process with respect to a specific categorical variable ( y ) . In Sec.2 we briefly describe other methods used in the literature to condition the generation process on a specific class label . Following your suggestion , in the new version of the paper we added the new Fig.7 in Appendix F in which we keep fixed the value of z ( the noise vector ) and we vary y . -- -- - Q : It is unclear to me why the proposed whitening based on Cholesky decomposition is better than ZCA-based in Huang 2018 . Specifically , could the authors explain why WC is better than W_ { aca } C in Table 3 ? A : Please , see our second answer to Reviewer # 3 -- -- - Q : The authors claim progressive GAN used a larger generator to achieve a better performance than WC . The WC layer is generally larger than BN layer and has more learnable parameters . Could the authors compare the number of parameter of generator in BN-ResNet , WC-ResNet , and progressive GAN ? A : You are right : both WC and cWC have more learnable weights , due to the coloring step . However , note that our performance boost does not depend only on the increase of the convolutional-filter number . For instance , if you compare c-std-C and c-std-C_ { sa } with the corresponding whitening-based versions cWC and cWC_ { sa } , having exactly the same number of parameters , you see that the latters drastically outperform the former ones ( Tab.6 ) .We emphasized this in the new paper at the end of Sec.5.2.1.Please , see also below our answer about the ImageNet experiment . The number of overall parameters used in the architectures you mentioned are : BN-ResNet ( called SN + ResNet + Proj.Discr.in the paper ) . G : 4.3M ; D : 1M . WC-ResNet ( called WC + SN + ResNet + Proj.Discr.in the paper ) . G : 4.7M ; D : 1M . Progressive GAN . G : 18.9M ; D : 18.9M . where G indicates the Generator and D the Discriminator ."}, {"review_id": "S1x2Fj0qKQ-1", "review_text": "This paper proposes to generalize both BN and cBN using Whitening and Coloring based batch normalization. Whitening is an enhanced version of mean subtraction and normalization by standard deviation. Coloring is an enhanced version of per-dimension scaling and shifting. Evaluation experiments are conducted on different datasets and using different GAN networks and training protocols. Empirical results show improvements over BN and cBN. The proposed method WC is interesting, but there are some unclear issues. 1. Two motivations for this paper: BN improves the conditioning of the Jacobian, stability of GAN training is related to the conditioning of the Jocobian. These motivate the paper to develop enhanced versions of BN/cBN, as said in the introduction. More discussions why WC can further improve the conditioning over ordinary BN would be better. 2. It is not clear why WC performs better than W_zca C (Table 3), though the improvement is moderate. The difference is that WC uses Cholesky decomposition and ZCA uses eigenvalue decomposition. Compared to W_zca C, WC seems to be an incremental contribution. 3. It is not clear why the proposed method is much faster than ZCA-based whitening. =========== comments after reading response =========== The authors make a good response, which clarifies the unclear issues from my first review. I remove the mention of the concurrent submission. Specially, the new Appendix D with the new Fig. 4 clearly explains and shows the benefit of WC over W_zca.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . Below our answers . Q : Two motivations for this paper : BN improves the conditioning of the Jacobian , stability of GAN training is related to the conditioning of the Jacobian . These motivate the paper to develop enhanced versions of BN/cBN , as said in the introduction . More discussions why WC can further improve the conditioning over ordinary BN would be better . A : Besides the works mentioned in Sec.1 , different other works showed the relation between the smoothness of the landscape of the loss function and the input-feature normalization and this is the key motivation behind batch-based normalization techniques , including our WC . For instance , ( ( Huang et al . ( 2018 ) ) , [ A ] , [ B ] ) show that better conditioning of the covariance matrix of the input features leads to better conditioning of the Hessian of the loss function , making the gradient descent weight updates closer to Newton updates . However , BN only performs standardization ( of each layer 's input ) . As noticed in ( Huang et al . ( 2018 ) ) , when the features are correlated , `` standardization barely improves the conditioning of the covariance matrix , whereas whitening remains effective . '' For example , in 2D , perfectly correlated features `` means all points lie close to the line y = x and BN does not change the shape of the distribution '' ( Huang et al . ( 2018 ) ) .Conversely , full-feature whitening completely decorrelates the batch samples , thus potentially improving the smoothness of the loss function . Our empirical results show that this is ( significantly ) true , at least in a GAN scenario . In the new version of the paper we have emphasized the relation between the smoothness of the loss function and the input-feature normalization and the consequent expected advantage in using full-feature whitening in the new Sec.6 ( ex-appendix D , now with a new discussion on this topic at the end of the section ) . Finally , note that a second , fundamental motivation of our work is specific to the conditional GAN setting , where the proposed conditional Coloring can represent richer class-dependent information ( see Abstract and Sec.1 , 4 , 6 ) . [ A ] Y. LeCun et al. , `` Efficient backprop '' . In : `` Neural Networks : Tricks of the Trade '' , 1998 [ B ] S. Wiesler and H. Ney . `` A convergence analysis of log-linear training '' , NIPS , 2011 -- -- -- Q : It is not clear why WC performs better than W_zca C ( Table 3 ) , though the improvement is moderate . The difference is that WC uses Cholesky decomposition and ZCA uses eigenvalue decomposition . Compared to W_zca C , WC seems to be an incremental contribution . A : Note that W_ { zca } C is not the Decorrelated Batch Normalization ( DBN ) proposed in ( Huang et al . ( 2018 ) ) because , for instance , no coloring is used in DBN . Hence W_ { zca } C is a variant of our WC in which the whitening phase is performed using ZCA . This is explained in the last lines of Sec.5.1.1.Concerning the reason why WC performs better than W_ { zca } C , we believe this is due to the higher stability of the Cholesky decomposition with respect to the Singular Value Decomposition ( SVD ) used in the ZCA-whitening . Specifically , in the following we refer to Appendix A.2 of ( Huang et al . ( 2018 ) ) , where the backpropagation formulas used for the proposed ZCA-based whitening are presented ( we used the same backpropagation in our W_ { zca } C experiments ) . The gradient of the loss with respect to the covariance matrix depends on a matrix called K in Eq . ( A.11 ) of that Appendix . The ( i , j ) -th element of K is ( by definition ) inversely proportional to the difference of the ( i , j ) -th singular values ( 1/ ( sigma_i - sigma_j ) ) of the covariance matrix . As a consequence , if some of the singular values are identical or very close to each other , then computing K_i , j is ill-conditioned . What we empirically observed is that W_ { zca } C may be highly unstable and training may start to drastically deteriorate after some iterations . Indeed , the results reported in Tab . 3 refer to the * best * IS-FID values observed during training . After about 40k iterations , W_ { zca } C suddenly degenerated , collapsing to a model that always produces a constant , uniform grey image . To show this phenomenon , we repeated training of both W_ { zca } C and WC and we added a new figure ( Fig.4 ) in the new paper ( please , see the new Appendix D , in which we discuss about W_ { zca } C instability issues ) . The new Fig.4 shows different IS/training-iteration curves corresponding to both WC and W_ { zca } C. As you can see , the W_ { zca } C training behaviour may drastically degenerate at some point . Conversely , our WC ( and cWC ) has never showed these drastic instability phenomena ."}, {"review_id": "S1x2Fj0qKQ-2", "review_text": "This paper tends to address the instability problem in GAN training by replacing batch normalization(BN) with whitening and coloring transform(WC) to provide a full-feature decorrelation. This paper consider both uncondition and condition cases. In general, the idea of replacing BN with WC is interesting and well motivated. The proposed method looks novel to me. Compared with ZCA whitening in Huang et al. 2018, the Cholesky decomposition is much faster and performs better. The experiments show the promising results and demonstrate the proposed method is easily to integrate with other advanced technic. The experimental results also illustrate the role of each components and well supports the motivation of proposed method. My only concern is that the proposed WC algorithm seems to have capability of applying to many tasks including discriminative scenario. This paper seems to have potential to be a more general paper about the WC method. Why just consider GAN? What is the performance of WC compared with BN/ZCA whiten in other tasks. It would be better if the authors can elaborate the motivation of choosing GAN as the application. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . Below our answer . Q : My only concern is that the proposed WC algorithm seems to have capability of applying to many tasks including discriminative scenario . This paper seems to have potential to be a more general paper about the WC method . Why just consider GAN ? What is the performance of WC compared with BN/ZCA whiten in other tasks . It would be better if the authors can elaborate the motivation of choosing GAN as the application . A : Please , note that in the ex-Appendix D ( which is now Sec.6 ) we actually compare WC with both BN and DBN ( proposed in ( Huang et al . ( 2018 ) ) and based on the ZCA whitening ) in a discriminative scenario using the protocol suggested in ( Huang et al . ( 2018 ) ) .The results reported in the ex-Tab . 10 ( now Tab.7 ) show that both WC and DBN achieve lower errors than BN . Moreover , WC is only slightly worse than DBN ( e.g. , using a ResNet-32 , WC has a 0.0006 error rate higher than DBN ) . However , note also that the maximum error rate difference using the protocol suggested in ( Huang et al . ( 2018 ) ) over all the tested normalization techniques , is lower than 0.01 . This probably shows that in a discriminative scenario , replacing standardization ( BN ) with full-feature whitening ( e.g. , using our WC or DBN ) is much less useful than in a GAN scenario , in which we got results drastically different when using WC/cWC with respect to BN/cBN . From this empirical analysis we conclude that full-feature whitening shows its major application potential in a GAN setting . The reason behind this different behaviour ( marginal accuracy boost in a discriminative scenario vs. large boost in a GAN setting ) is probably due to the higher instability of GAN training with respect to discriminative networks . Indeed , as mentioned in Sec.1 , recent papers ( Santurkar et al . ( 2018 ) ; Kohler et al . ( 2018 ) ) show that the main reason of the success of BN relies on an improved training stability . As a consequence , our extension of feature standardization to feature whitening goes in the direction of further improving this stability , which is much more important for GANs than for discriminative networks ( please , see also our first answer to Reviewer # 3 ) . In the new version of the paper we have moved Appendix D to Sec.6 and we have added a discussion at the end of that section which summarizes the above analysis . Finally , note that a second , important motivation of our work , for conditional GANs , is that our cWC can represent class-specific information using more informative filters ( Sec.1 ) , and this second aspect is naturally related to a GAN-based application ."}], "0": {"review_id": "S1x2Fj0qKQ-0", "review_text": "This paper proposed Whitening and Coloring (WC) transform to replace batch normalization (BN) in generators for GAN. WC generalize BN by normalizing features with decorrelating (whitening) matrix, and then denormalizing (coloring) features by learnable weights. The main advantage of WC is that it exploits the full correlation matrix of features, while BN only considers the diagonal. WC is differentiable and is only 1.32x slower than BN. The authors also apply conditional WC, which learn the parameters of coloring conditioned on labels, to conditional image generation. Experimental results show WC achieves better inception score and FI distance comparing to BN on CIFAR-10, CIFAR-100, STL-10 and Tiny Imagenet. Furthermore, the conditional image generation results by WC are better than all previous methods. I have some detailed comments below. + The paper is well written, and I generally enjoyed reading the paper. + The experimental results look sufficient, and I appreciate the ablation study sections. + The score on supervised CIFAR-10 is better than previous methods. - The main text is longer than expectation. I would suggest shorten section 3.1 Cholesky decomposition, section 4 conditional color transformation and the text in section 5 experiments. - The proposed WC transform is general. It is a bit unclear why it is particularly effective for generator in GAN. Exploiting the full correlation matrix sounds reasonable, but it may also introduce unstability. It would help if the authors have an intuitive way to show that whitening is better than normalization. - It is unclear why conditional WC can be used for generation conditioned on class labels. In Dumoulin 2016, conditional instance normalization is used for generating images conditioned on styles. As image styles are described by Gram matrix (correlation) of features, changing first order and second order statistics of features is reasonable for image generation conditioned on styles. I cannot understand why conditional WC can be used for generation conditioned on class labels. I would like the authors to carefully explain the motivation, and also provide visual results like using the same random noise as input, but only changing the class conditions. - It is unclear to me why the proposed whitening based on Cholesky decomposition is better than ZCA-based in Huang 2018. Specifically, could the authors explain why WC is better than W_{aca}C in Table 3? - The authors claim progressive GAN used a larger generator to achieve a better performance than WC. The WC layer is generally larger than BN layer and has more learnable parameters. Could the authors compare the number of parameter of generator in BN-ResNet, WC-ResNet, and progressive GAN? - In Table 3, std-C is better than WC-diag, which indicates coloring is more important. In Table 6, cWC-diag is better than c-std-C, which indicates whitening is more important. Why? - What is the batch size used for training? For conditional WC, do the samples in each minibatch have same label? - Having ImageNet results will be a big support for the paper. =========== comments after reading rebuttal =========== I appreciate the authors' feedback. I raised my score for Fig 7 showing the conditional images, and for experiments on ImageNet. I think WC is a reasonable extension to BN, and I generally like the extensive experiments. However, the paper is still borderline to me for the following concerns. - I strongly encourage the authors to shorten the paper to the recommended 8-page. - The motivation of WC for GAN is still unclear. WC is general extension of BN, and a simplified version has been shown to be effective for discrimination in Huang 2018. I understand the empirically good performance for GAN. But I am not convinced why WC is particularly effective for GAN, comparing to discrimination. The smoothness explanation of BN applies to both GAN and discrimination. I actually think it may be nontrivial to extend the smoothness argument from BN to WC. - The motivation of cWC is still unclear. I did not find the details of cBN for class-label conditions, and how they motivated it in (Gulrajani et al. (2017) and (Miyato et al. 2018). Even if it has been used before, I would encourage the authors to restate the motivation in the paper. Saying it has been used before is an unsatisfactory answer for an unintuitive setting. - Another less important comment is that it is still hard to say how much benefits we get from the more learnable parameters in WC than BN. It is probably not so important because it can be a good trade-off for state-of-the-art results. In table 3 for unconditioned generation, it looks like the benefits come a lot from the larger parameter space. For conditioned generation in table 6, I am not sure if whitening is conditioned or not, which makes it less reliable to me. If whitening is conditioned, then the samples in each minibatches may not be enough to get a stable whitening. If whitening is unconditioned, then there seems to be a mismatch between whitening and coloring. ====== second round after rebuttal ============= I raise the score again for the commitment of shortening the paper and the detailed response from the authors. That being said, I am not fully convinced about motivations for WC and cWC. - GAN training is more difficult and unstable, but that does not explain why WC is particularly effective for GAN training. - I have never seen papers saying cBN/cWC is better than other conditional generator conditioned on class labels. I think the capacity argument is interesting, but I am not sure if it applies to convolutional net (where the mean and variance of a channel is used), or how well it can explain the performance because neural nets are overparameterized in general. I would encourage authors to include these discussions in the paper. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your detailed review . Below our answers . Q : The main text is longer than expectation . I would suggest shorten section 3.1 Cholesky decomposition , section 4 conditional color transformation and the text in section 5 experiments . A : Thank you for your suggestion . It is not clear to us how much the new version can be different from the submitted one , however , in the final version we will shorten the paper and possibly move Sec.3.1 and 3.2 to the Appendix , if you think they are less important . -- -- - Q : The proposed WC transform is general . It is a bit unclear why it is particularly effective for generator in GAN . Exploiting the full correlation matrix sounds reasonable , but it may also introduce unstability . It would help if the authors have an intuitive way to show that whitening is better than normalization . A : Please , see our answer to Reviewer # 1 concerning the motivation behind the choice of a GAN scenario and our first answer to Reviewer # 3 concerning a more intuitive explanation of why whitening is better than standardization . -- -- - Q : It is unclear why conditional WC can be used for generation conditioned on class labels . In Dumoulin 2016 , conditional instance normalization is used for generating images conditioned on styles . As image styles are described by Gram matrix ( correlation ) of features , changing first order and second order statistics of features is reasonable for image generation conditioned on styles . I can not understand why conditional WC can be used for generation conditioned on class labels . I would like the authors to carefully explain the motivation , and also provide visual results like using the same random noise as input , but only changing the class conditions . A : Since the introduction of the cBN in ( Dumoulin et al . ( 2016b ) ) , many works showed that this is a very powerful method for representing object-class information in conditional GANs ( e.g. , ( Gulrajani et al . ( 2017 ) ) ; ( Miyato et al . ( 2018 ) ) ) .We basically extend this idea using ( class-specific ) convolutional filters instead of ( class-specific ) scaling parameters ( Sec.1 , 2 ) .Note that G needs information about y . Our conditional coloring filters implicitly represent y by projecting the features into class-specific distributions . In other words , all the layers of our generator share the weights across all the classes , except for the convolutional filters in the conditional-coloring layers . This is enough for the network to influence the image generation process with respect to a specific categorical variable ( y ) . In Sec.2 we briefly describe other methods used in the literature to condition the generation process on a specific class label . Following your suggestion , in the new version of the paper we added the new Fig.7 in Appendix F in which we keep fixed the value of z ( the noise vector ) and we vary y . -- -- - Q : It is unclear to me why the proposed whitening based on Cholesky decomposition is better than ZCA-based in Huang 2018 . Specifically , could the authors explain why WC is better than W_ { aca } C in Table 3 ? A : Please , see our second answer to Reviewer # 3 -- -- - Q : The authors claim progressive GAN used a larger generator to achieve a better performance than WC . The WC layer is generally larger than BN layer and has more learnable parameters . Could the authors compare the number of parameter of generator in BN-ResNet , WC-ResNet , and progressive GAN ? A : You are right : both WC and cWC have more learnable weights , due to the coloring step . However , note that our performance boost does not depend only on the increase of the convolutional-filter number . For instance , if you compare c-std-C and c-std-C_ { sa } with the corresponding whitening-based versions cWC and cWC_ { sa } , having exactly the same number of parameters , you see that the latters drastically outperform the former ones ( Tab.6 ) .We emphasized this in the new paper at the end of Sec.5.2.1.Please , see also below our answer about the ImageNet experiment . The number of overall parameters used in the architectures you mentioned are : BN-ResNet ( called SN + ResNet + Proj.Discr.in the paper ) . G : 4.3M ; D : 1M . WC-ResNet ( called WC + SN + ResNet + Proj.Discr.in the paper ) . G : 4.7M ; D : 1M . Progressive GAN . G : 18.9M ; D : 18.9M . where G indicates the Generator and D the Discriminator ."}, "1": {"review_id": "S1x2Fj0qKQ-1", "review_text": "This paper proposes to generalize both BN and cBN using Whitening and Coloring based batch normalization. Whitening is an enhanced version of mean subtraction and normalization by standard deviation. Coloring is an enhanced version of per-dimension scaling and shifting. Evaluation experiments are conducted on different datasets and using different GAN networks and training protocols. Empirical results show improvements over BN and cBN. The proposed method WC is interesting, but there are some unclear issues. 1. Two motivations for this paper: BN improves the conditioning of the Jacobian, stability of GAN training is related to the conditioning of the Jocobian. These motivate the paper to develop enhanced versions of BN/cBN, as said in the introduction. More discussions why WC can further improve the conditioning over ordinary BN would be better. 2. It is not clear why WC performs better than W_zca C (Table 3), though the improvement is moderate. The difference is that WC uses Cholesky decomposition and ZCA uses eigenvalue decomposition. Compared to W_zca C, WC seems to be an incremental contribution. 3. It is not clear why the proposed method is much faster than ZCA-based whitening. =========== comments after reading response =========== The authors make a good response, which clarifies the unclear issues from my first review. I remove the mention of the concurrent submission. Specially, the new Appendix D with the new Fig. 4 clearly explains and shows the benefit of WC over W_zca.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . Below our answers . Q : Two motivations for this paper : BN improves the conditioning of the Jacobian , stability of GAN training is related to the conditioning of the Jacobian . These motivate the paper to develop enhanced versions of BN/cBN , as said in the introduction . More discussions why WC can further improve the conditioning over ordinary BN would be better . A : Besides the works mentioned in Sec.1 , different other works showed the relation between the smoothness of the landscape of the loss function and the input-feature normalization and this is the key motivation behind batch-based normalization techniques , including our WC . For instance , ( ( Huang et al . ( 2018 ) ) , [ A ] , [ B ] ) show that better conditioning of the covariance matrix of the input features leads to better conditioning of the Hessian of the loss function , making the gradient descent weight updates closer to Newton updates . However , BN only performs standardization ( of each layer 's input ) . As noticed in ( Huang et al . ( 2018 ) ) , when the features are correlated , `` standardization barely improves the conditioning of the covariance matrix , whereas whitening remains effective . '' For example , in 2D , perfectly correlated features `` means all points lie close to the line y = x and BN does not change the shape of the distribution '' ( Huang et al . ( 2018 ) ) .Conversely , full-feature whitening completely decorrelates the batch samples , thus potentially improving the smoothness of the loss function . Our empirical results show that this is ( significantly ) true , at least in a GAN scenario . In the new version of the paper we have emphasized the relation between the smoothness of the loss function and the input-feature normalization and the consequent expected advantage in using full-feature whitening in the new Sec.6 ( ex-appendix D , now with a new discussion on this topic at the end of the section ) . Finally , note that a second , fundamental motivation of our work is specific to the conditional GAN setting , where the proposed conditional Coloring can represent richer class-dependent information ( see Abstract and Sec.1 , 4 , 6 ) . [ A ] Y. LeCun et al. , `` Efficient backprop '' . In : `` Neural Networks : Tricks of the Trade '' , 1998 [ B ] S. Wiesler and H. Ney . `` A convergence analysis of log-linear training '' , NIPS , 2011 -- -- -- Q : It is not clear why WC performs better than W_zca C ( Table 3 ) , though the improvement is moderate . The difference is that WC uses Cholesky decomposition and ZCA uses eigenvalue decomposition . Compared to W_zca C , WC seems to be an incremental contribution . A : Note that W_ { zca } C is not the Decorrelated Batch Normalization ( DBN ) proposed in ( Huang et al . ( 2018 ) ) because , for instance , no coloring is used in DBN . Hence W_ { zca } C is a variant of our WC in which the whitening phase is performed using ZCA . This is explained in the last lines of Sec.5.1.1.Concerning the reason why WC performs better than W_ { zca } C , we believe this is due to the higher stability of the Cholesky decomposition with respect to the Singular Value Decomposition ( SVD ) used in the ZCA-whitening . Specifically , in the following we refer to Appendix A.2 of ( Huang et al . ( 2018 ) ) , where the backpropagation formulas used for the proposed ZCA-based whitening are presented ( we used the same backpropagation in our W_ { zca } C experiments ) . The gradient of the loss with respect to the covariance matrix depends on a matrix called K in Eq . ( A.11 ) of that Appendix . The ( i , j ) -th element of K is ( by definition ) inversely proportional to the difference of the ( i , j ) -th singular values ( 1/ ( sigma_i - sigma_j ) ) of the covariance matrix . As a consequence , if some of the singular values are identical or very close to each other , then computing K_i , j is ill-conditioned . What we empirically observed is that W_ { zca } C may be highly unstable and training may start to drastically deteriorate after some iterations . Indeed , the results reported in Tab . 3 refer to the * best * IS-FID values observed during training . After about 40k iterations , W_ { zca } C suddenly degenerated , collapsing to a model that always produces a constant , uniform grey image . To show this phenomenon , we repeated training of both W_ { zca } C and WC and we added a new figure ( Fig.4 ) in the new paper ( please , see the new Appendix D , in which we discuss about W_ { zca } C instability issues ) . The new Fig.4 shows different IS/training-iteration curves corresponding to both WC and W_ { zca } C. As you can see , the W_ { zca } C training behaviour may drastically degenerate at some point . Conversely , our WC ( and cWC ) has never showed these drastic instability phenomena ."}, "2": {"review_id": "S1x2Fj0qKQ-2", "review_text": "This paper tends to address the instability problem in GAN training by replacing batch normalization(BN) with whitening and coloring transform(WC) to provide a full-feature decorrelation. This paper consider both uncondition and condition cases. In general, the idea of replacing BN with WC is interesting and well motivated. The proposed method looks novel to me. Compared with ZCA whitening in Huang et al. 2018, the Cholesky decomposition is much faster and performs better. The experiments show the promising results and demonstrate the proposed method is easily to integrate with other advanced technic. The experimental results also illustrate the role of each components and well supports the motivation of proposed method. My only concern is that the proposed WC algorithm seems to have capability of applying to many tasks including discriminative scenario. This paper seems to have potential to be a more general paper about the WC method. Why just consider GAN? What is the performance of WC compared with BN/ZCA whiten in other tasks. It would be better if the authors can elaborate the motivation of choosing GAN as the application. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . Below our answer . Q : My only concern is that the proposed WC algorithm seems to have capability of applying to many tasks including discriminative scenario . This paper seems to have potential to be a more general paper about the WC method . Why just consider GAN ? What is the performance of WC compared with BN/ZCA whiten in other tasks . It would be better if the authors can elaborate the motivation of choosing GAN as the application . A : Please , note that in the ex-Appendix D ( which is now Sec.6 ) we actually compare WC with both BN and DBN ( proposed in ( Huang et al . ( 2018 ) ) and based on the ZCA whitening ) in a discriminative scenario using the protocol suggested in ( Huang et al . ( 2018 ) ) .The results reported in the ex-Tab . 10 ( now Tab.7 ) show that both WC and DBN achieve lower errors than BN . Moreover , WC is only slightly worse than DBN ( e.g. , using a ResNet-32 , WC has a 0.0006 error rate higher than DBN ) . However , note also that the maximum error rate difference using the protocol suggested in ( Huang et al . ( 2018 ) ) over all the tested normalization techniques , is lower than 0.01 . This probably shows that in a discriminative scenario , replacing standardization ( BN ) with full-feature whitening ( e.g. , using our WC or DBN ) is much less useful than in a GAN scenario , in which we got results drastically different when using WC/cWC with respect to BN/cBN . From this empirical analysis we conclude that full-feature whitening shows its major application potential in a GAN setting . The reason behind this different behaviour ( marginal accuracy boost in a discriminative scenario vs. large boost in a GAN setting ) is probably due to the higher instability of GAN training with respect to discriminative networks . Indeed , as mentioned in Sec.1 , recent papers ( Santurkar et al . ( 2018 ) ; Kohler et al . ( 2018 ) ) show that the main reason of the success of BN relies on an improved training stability . As a consequence , our extension of feature standardization to feature whitening goes in the direction of further improving this stability , which is much more important for GANs than for discriminative networks ( please , see also our first answer to Reviewer # 3 ) . In the new version of the paper we have moved Appendix D to Sec.6 and we have added a discussion at the end of that section which summarizes the above analysis . Finally , note that a second , important motivation of our work , for conditional GANs , is that our cWC can represent class-specific information using more informative filters ( Sec.1 ) , and this second aspect is naturally related to a GAN-based application ."}}