{"year": "2019", "forum": "r1gGpjActQ", "title": "Hint-based Training for Non-Autoregressive Translation", "decision": "Reject", "meta_review": "\n\n+ sufficiently strong results\n\n+ a fast / parallelizable model\n\n\n- Novelty with respect to previous work is not as great (see AnonReviewer1 and AnonReviewer2's comments)\n\n- The same reviewers raised concerns about the discussion of related work (e.g., positioning with respect to work on knowledge distillation). I agree that the very related work of Roy et al should be mentioned, even though it has not been published it has been on arxiv since May.\n\n- Ablation studies are only on smaller IWSLT datasets, confirming that the hints from an auto-regressive model are beneficial (whereas the main results are on WMT)\n\n-  I agree with R1 that the important modeling details (e.g., describing how the latent structure is generated) should not be described only in the appendix, esp given non-standard modeling choices.  R1 is concerned that a model which does not have any autoregressive components (i.e. not even for the latent state) may have trouble representing multiple modes.  I do find it surprising that the model with non-autoregressive latent state works well however I do not find this a sufficient ground for rejection on its own. However, emphasizing this point and discussing the implication in the paper makes a lot of sense, and should have been done.  As of now, it is downplayed. R1 is concerned that such model may be gaming BLEU: as BLEU is less sensitive to long-distance dependencies, they may get damaged for the model which does not have any autoregressive components.  Again, given the standards in the field, I do not think it is fair to require human evaluation, but I agree that including it would strengthen the paper and the arguments.\n\n\nOverall, I do believe that the paper is sufficiently interesting and should get published but I also believe that it needs further revisions / further experiments.\n\n\n", "reviews": [{"review_id": "r1gGpjActQ-0", "review_text": "In this paper, the authors propose an extension to the Non Autoregressive Translation model by Gu et. al, to improve the accuracy of Non autoregressive models as compared to the autoregressive translation models. The authors propose using hints which can occur as 1. Hidden output matching by incurring a penalty if the cosine distance between the representation differ according to a threshold. The authors state that this reduces same output word repetition which is common for NART models 2. Reducing the KL divergence between the attention distribution of the teacher and the student model in the encoder-decoder attention part of the model. We see experimental evidence from 3 tasks showing the effectiveness of this technique. The strengths of this paper are the speedup improvements of using these techniques on the student model while also improving BLEU scores. The paper is easy to read and the visualisations are useful. The main issue with this paper is the delta contribution as compared to the NART model is Gu et. al. The 2 techniques, although simple, don't make up for technical novelty. It would also be good to see more analysis on how much the word repetition reduces using these techniques quantitatively, and performance especially on longer length sequences. Another issue is the comparison of latency measurements for decoding. The authors state that the hardware and the setting under which the latency measurements are done might be different as compared to previous numbers. Though still impressive speedup improvements, it somehow becomes fuzzy to understand the actual gains.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review ! The main contribution of the paper is to show that by our proposed hint-based training algorithm , a simple non-autoregressive model without a complex submodule can reach competitive performance near an autoregressive model , while still being orders of magnitude faster . We think the results and findings are significant and will be helpful to future works in this direction . We conduct the following two experiments according to the suggestions : - According to our study , the proposed algorithm reduces the percentage of repetitive words by more than 20 % in IWSLT De-En task . - We filter out all the sentences whose lengths are at least 40 in the test set of IWSLT De-En , and test the baseline model and the model trained with hints on the subsampled set . It turns out that our model outperforms the baseline model by more than 3 points in term of BLEU ( 20.63 v.s.17.48 ) .Note that the incoherent patterns like repetitive words are a common phenomenon among sentences of all lengths , rather than a special problem for long sentences . As stated in the paper , it is quite difficult to find a uniform fair measure for comparing the speed of non-autoregressive models . Since the speedup of non-autoregressive models comes from their fine-grained parallelism , traditional metrics like FLOPs do not fit . Absolute metrics like latency highly depends on underlying hardware and code implementations . Therefore , we use speedup in our paper as a better relative measure for a fair comparison ."}, {"review_id": "r1gGpjActQ-1", "review_text": "This paper proposes to distill knowledge from intermediary hidden states and attention weights to improve non-autoregressive neural machine translation. Strengths: Results are sufficiently strong. Inference is much faster than for auto-regressive models, while BLEU scores are reasonably close. The approach is simple, only necessitating two auxiliary loss functions during training, and rescoring for inference. Weaknesses: The discussion of related work is deficient. Learning from hints is a variant of knowledge distillation (KD). Another form of KD, using the auto-regressive model output instead of the reference, was shown to be useful for non-autoregressive neural machine translation (Gu et al., 2017, already cited). The authors mention using that technique in section 4.1, but don't discuss how it relates to their work. [1] should also probably be cited. Hu et al. [2] apply a slightly different form of attention weight distillation. However, the preprint of that paper was available just over one month before the ICLR submission deadline. Questions and other remarks: Do the baselines use greedy or beam search? Why batch size 1 for decoding? With larger batch sizes, the speed-up may be limited by how many candidates fit in memory for rescoring. Please fix \"are not commonly appeared\" on page 4, section 3.1. [1] Kim, Yoon and Alexander M. Rush. \"Sequence-Level Knowledge Distillation\" EMNLP. 2016. [2] Hu, Minghao et al. \"Attention-Guided Answer Distillation for Machine Reading Comprehension\" EMNLP. 2018", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review ! We have added discussions on related references and KD to the paper . In machine translation , the state-of-the-art model uses beam search and thus we follow to use it in the baseline and make comparisons . Batch size 1 for decoding is a common practice when comparing the speedups of non-autoregressive translation models [ 1 , 2 , 3 ] . We follow the practice of previous works to make a fair comparison . Setting batch size 1 and studying the efficiency is also reasonable . Just consider the applications where the translation computation is done on a portable device ( e.g.offline translation app on a smartphone ) . In such a scenario , the user inputs one sentence and expects the translation result . [ 1 ] Gu , Jiatao , et al . `` Non-autoregressive neural machine translation . '' ICLR 2018 [ 2 ] Lee , Jason , Elman Mansimov , and Kyunghyun Cho . `` Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement . '' EMNLP 2018 [ 3 ] Kaiser , \u0141ukasz , et al . `` Fast Decoding in Sequence Models Using Discrete Latent Variables . '' ICML 2018 ."}, {"review_id": "r1gGpjActQ-2", "review_text": "This work proposes a non-autoregressive Neural Machine Translation model which the authors call NART, as opposed to an autoregressive model which is referred to as an ART model. The main idea behind this work is to leverage a well trained ART model to inform the hidden states and the word alignment of NART models. The joint distribution of the targets y given the inputs x, is factorized into two components as in previous works on non-autoregressive MT: an intermediate z which is first predicted from x, which captures the autoregressive part, while the prediction of y given z is non-autoregressive. This is the approach taken e.g., in Gu et al, Kaiser et al, Roy et al., and this also seems to be the approach of this work. The authors argue that improving the expressiveness of z (as was done in Kaiser et al, Roy et al), is expensive and so the authors propose a simple formulation for z. In particular, z is a sequence of the same length as the targets, where the j^{th} entry z_j is a weighted sum of the embedding of the inputs x (the weights depend in a deterministic fashion on j) . Given this z, the model predicts the targets completely non-autoregressively. However, this by itself is not entirely sufficient, and so the authors also utilize \"hints\": 1) If the pairwise cosine similarity between two successive hidden states in the student NART model is above a certain threshold, while the similarity is lower than another threshold in the ART model, then the NART model incurs a cost proportional to this similarity 2) A KL term is used to encourage the distribution of attention weights of the student ART model to match that of the teacher NART model. These two loss terms are used in different proportions (using additional hyperparameters) together with maximizing the likelihood term. Quality: The paper is not very well written and is often hard to follow in parts. Here are some examples of the writing that feel awkward: -- Consequently, people start to develop Non-AutoRegressive neural machine Translation (NART) models to speed up the inference process (Gu et al., 2017; Kaiser et al., 2018; Lee et al., 2018). -- In order to speed up to the inference process, a line of works begin to develop non-autoregressive translation models. Originality: The idea of using an autoregressive teacher model to improve a non-autoregressive translation model has been used in Gu et al., Roy et al., where knowledge distillation is used. So knowledge distillation paper from Hinton et al., should be cited. Moreover, the authors have missed comparing their work to that of Roy et al. (https://arxiv.org/abs/1805.11063), which greatly improves on the work of Kaiser et al., and almost closes the gap between a non-autoregressive model and an autoregressive model (26.7 BLEU vs 27 BLEU on En-De) while being orders of magnitude faster. So it is not true that: -- \"While the NART models achieve significant speedup during inference (Gu et al., 2017), their accuracy is considerably lower than their ART counterpart.\" -- \"Non-autoregressive translation (NART) models have suffered from low-quality translation results\" Significance: The work introduces the idea of using hints for non-autoregressive machine translation. However, I have a technical concern: It seems that the authors complain that previous works like Kaiser et al, Roy et al, use sophisticated submodules to help the expressiveness of z and this was the cause for slowness. However, the way the authors define z seems to have some problems: - z_j does not depend on z_1, ..., z_{j-1}, so where is the autoregressive dependencies being captured? - z_1, z_2, ..., z_{T_y} depends only on the length of y, and does not depend on y in any other way. Given x, predicting z is trivial and I don't see why that should help the model f(y | z, x) help at all? - Given such a trivial z, one can just assume that your model is completely factorial i.e. P(y|x) = \\prod_{i} P(y_i|x) since the intermediate z has no information on the y's except it's length. This is quite suspicious to me, and it seems that if this works, then a completely factorial model should work as well if we only use the \"hints\" from the ART teacher model. This is a red flag to me, and I am finding this hard to believe.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review ! We believe there are some misunderstandings here . We respond to the concerns as below . We will also post our source codes and trained models for verification after the double-blind review period . 1.Regarding the design choice of z The reviewer considers that in a non-autoregressive translation model , \u201c z_j does not depend on z_1 , ... , z_ { j-1 } \u201d and \u201c z_1 , z_2 , ... , z_ { T_y } depends only on the length of y \u201d are unreasonable and red flags . However , before our paper , such setting has been shown to work in non-autoregressive translation . We believe the reviewer \u2019 s understanding to this might be incorrect . Gu et al . [ 1 ] and we choose to generate z using non-autoregressive ways and ours is a further simplification of [ 1 ] . In [ 1 ] , The hidden z_1 , \u2026 , z_ { T_y } ( the \u201c fertility \u201d module ) are also mutually independently generated , and have an only limited dependency on y . Please note that the simplicity of z does not mean that the model will definitely suffer from poor translation quality . Although the hidden z is simple , the model itself is a deep neural network , consisting of different components ( self attention layer , encoder to decoder attention layer , positional attention layer ) that enable the model to learn a complex mapping from x and z to y . We believe a simple design choice of z is enough and do not list it as a major contribution of our work . Our main technical contribution is to improve the model performance by a more well-designed training algorithm from teachers . Our experimental results show that by our carefully designed training algorithm , a non-autoregressive model with a simple z can achieve near autoregressive performance , while benefiting from the speedup brought by the little overhead of such a simple z . 2.Regarding `` orders of magnitude faster '' related works Thanks for pointing out the recent work from Roy et al . [ 3 ] , which is also an ICLR submission this year . By checking their paper , we can see that the speedup of Roy et al . [ 3 ] is not `` orders of magnitude faster '' . It is only 4.08x when reaching the highest performance , comparing to 17.8x in our work . It is not true that the model by Roy et al.is `` orders of magnitude faster '' . The main reason is that they choose to use a more complex z using an autoregressive module . Such overhead of z will greatly hurt their speedup , which also contradicts with the initial purpose of introducing non-autoregressive modeling . We believe the translation quality of our model ( 25.2 for WMT En-De , 29.52 for WMT De-En ) is significant given the large speedup of our model . [ 1 ] Gu , Jiatao , et al . `` Non-autoregressive neural machine translation . '' ICLR 2018 [ 2 ] Lee , Jason , Elman Mansimov , and Kyunghyun Cho . `` Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement . '' EMNLP 2018 [ 3 ] Roy , Aurko , et al . `` Theory and Experiments on Vector Quantized Autoencoders . '' arXiv 2018"}], "0": {"review_id": "r1gGpjActQ-0", "review_text": "In this paper, the authors propose an extension to the Non Autoregressive Translation model by Gu et. al, to improve the accuracy of Non autoregressive models as compared to the autoregressive translation models. The authors propose using hints which can occur as 1. Hidden output matching by incurring a penalty if the cosine distance between the representation differ according to a threshold. The authors state that this reduces same output word repetition which is common for NART models 2. Reducing the KL divergence between the attention distribution of the teacher and the student model in the encoder-decoder attention part of the model. We see experimental evidence from 3 tasks showing the effectiveness of this technique. The strengths of this paper are the speedup improvements of using these techniques on the student model while also improving BLEU scores. The paper is easy to read and the visualisations are useful. The main issue with this paper is the delta contribution as compared to the NART model is Gu et. al. The 2 techniques, although simple, don't make up for technical novelty. It would also be good to see more analysis on how much the word repetition reduces using these techniques quantitatively, and performance especially on longer length sequences. Another issue is the comparison of latency measurements for decoding. The authors state that the hardware and the setting under which the latency measurements are done might be different as compared to previous numbers. Though still impressive speedup improvements, it somehow becomes fuzzy to understand the actual gains.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review ! The main contribution of the paper is to show that by our proposed hint-based training algorithm , a simple non-autoregressive model without a complex submodule can reach competitive performance near an autoregressive model , while still being orders of magnitude faster . We think the results and findings are significant and will be helpful to future works in this direction . We conduct the following two experiments according to the suggestions : - According to our study , the proposed algorithm reduces the percentage of repetitive words by more than 20 % in IWSLT De-En task . - We filter out all the sentences whose lengths are at least 40 in the test set of IWSLT De-En , and test the baseline model and the model trained with hints on the subsampled set . It turns out that our model outperforms the baseline model by more than 3 points in term of BLEU ( 20.63 v.s.17.48 ) .Note that the incoherent patterns like repetitive words are a common phenomenon among sentences of all lengths , rather than a special problem for long sentences . As stated in the paper , it is quite difficult to find a uniform fair measure for comparing the speed of non-autoregressive models . Since the speedup of non-autoregressive models comes from their fine-grained parallelism , traditional metrics like FLOPs do not fit . Absolute metrics like latency highly depends on underlying hardware and code implementations . Therefore , we use speedup in our paper as a better relative measure for a fair comparison ."}, "1": {"review_id": "r1gGpjActQ-1", "review_text": "This paper proposes to distill knowledge from intermediary hidden states and attention weights to improve non-autoregressive neural machine translation. Strengths: Results are sufficiently strong. Inference is much faster than for auto-regressive models, while BLEU scores are reasonably close. The approach is simple, only necessitating two auxiliary loss functions during training, and rescoring for inference. Weaknesses: The discussion of related work is deficient. Learning from hints is a variant of knowledge distillation (KD). Another form of KD, using the auto-regressive model output instead of the reference, was shown to be useful for non-autoregressive neural machine translation (Gu et al., 2017, already cited). The authors mention using that technique in section 4.1, but don't discuss how it relates to their work. [1] should also probably be cited. Hu et al. [2] apply a slightly different form of attention weight distillation. However, the preprint of that paper was available just over one month before the ICLR submission deadline. Questions and other remarks: Do the baselines use greedy or beam search? Why batch size 1 for decoding? With larger batch sizes, the speed-up may be limited by how many candidates fit in memory for rescoring. Please fix \"are not commonly appeared\" on page 4, section 3.1. [1] Kim, Yoon and Alexander M. Rush. \"Sequence-Level Knowledge Distillation\" EMNLP. 2016. [2] Hu, Minghao et al. \"Attention-Guided Answer Distillation for Machine Reading Comprehension\" EMNLP. 2018", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review ! We have added discussions on related references and KD to the paper . In machine translation , the state-of-the-art model uses beam search and thus we follow to use it in the baseline and make comparisons . Batch size 1 for decoding is a common practice when comparing the speedups of non-autoregressive translation models [ 1 , 2 , 3 ] . We follow the practice of previous works to make a fair comparison . Setting batch size 1 and studying the efficiency is also reasonable . Just consider the applications where the translation computation is done on a portable device ( e.g.offline translation app on a smartphone ) . In such a scenario , the user inputs one sentence and expects the translation result . [ 1 ] Gu , Jiatao , et al . `` Non-autoregressive neural machine translation . '' ICLR 2018 [ 2 ] Lee , Jason , Elman Mansimov , and Kyunghyun Cho . `` Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement . '' EMNLP 2018 [ 3 ] Kaiser , \u0141ukasz , et al . `` Fast Decoding in Sequence Models Using Discrete Latent Variables . '' ICML 2018 ."}, "2": {"review_id": "r1gGpjActQ-2", "review_text": "This work proposes a non-autoregressive Neural Machine Translation model which the authors call NART, as opposed to an autoregressive model which is referred to as an ART model. The main idea behind this work is to leverage a well trained ART model to inform the hidden states and the word alignment of NART models. The joint distribution of the targets y given the inputs x, is factorized into two components as in previous works on non-autoregressive MT: an intermediate z which is first predicted from x, which captures the autoregressive part, while the prediction of y given z is non-autoregressive. This is the approach taken e.g., in Gu et al, Kaiser et al, Roy et al., and this also seems to be the approach of this work. The authors argue that improving the expressiveness of z (as was done in Kaiser et al, Roy et al), is expensive and so the authors propose a simple formulation for z. In particular, z is a sequence of the same length as the targets, where the j^{th} entry z_j is a weighted sum of the embedding of the inputs x (the weights depend in a deterministic fashion on j) . Given this z, the model predicts the targets completely non-autoregressively. However, this by itself is not entirely sufficient, and so the authors also utilize \"hints\": 1) If the pairwise cosine similarity between two successive hidden states in the student NART model is above a certain threshold, while the similarity is lower than another threshold in the ART model, then the NART model incurs a cost proportional to this similarity 2) A KL term is used to encourage the distribution of attention weights of the student ART model to match that of the teacher NART model. These two loss terms are used in different proportions (using additional hyperparameters) together with maximizing the likelihood term. Quality: The paper is not very well written and is often hard to follow in parts. Here are some examples of the writing that feel awkward: -- Consequently, people start to develop Non-AutoRegressive neural machine Translation (NART) models to speed up the inference process (Gu et al., 2017; Kaiser et al., 2018; Lee et al., 2018). -- In order to speed up to the inference process, a line of works begin to develop non-autoregressive translation models. Originality: The idea of using an autoregressive teacher model to improve a non-autoregressive translation model has been used in Gu et al., Roy et al., where knowledge distillation is used. So knowledge distillation paper from Hinton et al., should be cited. Moreover, the authors have missed comparing their work to that of Roy et al. (https://arxiv.org/abs/1805.11063), which greatly improves on the work of Kaiser et al., and almost closes the gap between a non-autoregressive model and an autoregressive model (26.7 BLEU vs 27 BLEU on En-De) while being orders of magnitude faster. So it is not true that: -- \"While the NART models achieve significant speedup during inference (Gu et al., 2017), their accuracy is considerably lower than their ART counterpart.\" -- \"Non-autoregressive translation (NART) models have suffered from low-quality translation results\" Significance: The work introduces the idea of using hints for non-autoregressive machine translation. However, I have a technical concern: It seems that the authors complain that previous works like Kaiser et al, Roy et al, use sophisticated submodules to help the expressiveness of z and this was the cause for slowness. However, the way the authors define z seems to have some problems: - z_j does not depend on z_1, ..., z_{j-1}, so where is the autoregressive dependencies being captured? - z_1, z_2, ..., z_{T_y} depends only on the length of y, and does not depend on y in any other way. Given x, predicting z is trivial and I don't see why that should help the model f(y | z, x) help at all? - Given such a trivial z, one can just assume that your model is completely factorial i.e. P(y|x) = \\prod_{i} P(y_i|x) since the intermediate z has no information on the y's except it's length. This is quite suspicious to me, and it seems that if this works, then a completely factorial model should work as well if we only use the \"hints\" from the ART teacher model. This is a red flag to me, and I am finding this hard to believe.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review ! We believe there are some misunderstandings here . We respond to the concerns as below . We will also post our source codes and trained models for verification after the double-blind review period . 1.Regarding the design choice of z The reviewer considers that in a non-autoregressive translation model , \u201c z_j does not depend on z_1 , ... , z_ { j-1 } \u201d and \u201c z_1 , z_2 , ... , z_ { T_y } depends only on the length of y \u201d are unreasonable and red flags . However , before our paper , such setting has been shown to work in non-autoregressive translation . We believe the reviewer \u2019 s understanding to this might be incorrect . Gu et al . [ 1 ] and we choose to generate z using non-autoregressive ways and ours is a further simplification of [ 1 ] . In [ 1 ] , The hidden z_1 , \u2026 , z_ { T_y } ( the \u201c fertility \u201d module ) are also mutually independently generated , and have an only limited dependency on y . Please note that the simplicity of z does not mean that the model will definitely suffer from poor translation quality . Although the hidden z is simple , the model itself is a deep neural network , consisting of different components ( self attention layer , encoder to decoder attention layer , positional attention layer ) that enable the model to learn a complex mapping from x and z to y . We believe a simple design choice of z is enough and do not list it as a major contribution of our work . Our main technical contribution is to improve the model performance by a more well-designed training algorithm from teachers . Our experimental results show that by our carefully designed training algorithm , a non-autoregressive model with a simple z can achieve near autoregressive performance , while benefiting from the speedup brought by the little overhead of such a simple z . 2.Regarding `` orders of magnitude faster '' related works Thanks for pointing out the recent work from Roy et al . [ 3 ] , which is also an ICLR submission this year . By checking their paper , we can see that the speedup of Roy et al . [ 3 ] is not `` orders of magnitude faster '' . It is only 4.08x when reaching the highest performance , comparing to 17.8x in our work . It is not true that the model by Roy et al.is `` orders of magnitude faster '' . The main reason is that they choose to use a more complex z using an autoregressive module . Such overhead of z will greatly hurt their speedup , which also contradicts with the initial purpose of introducing non-autoregressive modeling . We believe the translation quality of our model ( 25.2 for WMT En-De , 29.52 for WMT De-En ) is significant given the large speedup of our model . [ 1 ] Gu , Jiatao , et al . `` Non-autoregressive neural machine translation . '' ICLR 2018 [ 2 ] Lee , Jason , Elman Mansimov , and Kyunghyun Cho . `` Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement . '' EMNLP 2018 [ 3 ] Roy , Aurko , et al . `` Theory and Experiments on Vector Quantized Autoencoders . '' arXiv 2018"}}