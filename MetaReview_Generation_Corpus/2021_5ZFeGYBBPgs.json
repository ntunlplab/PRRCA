{"year": "2021", "forum": "5ZFeGYBBPgs", "title": "Second-Moment Loss: A Novel Regression Objective for Improved Uncertainties", "decision": "Reject", "meta_review": "Dear Authors,\n\nThank you very much for your very detailed feedback and also updating the manuscript in the rebuttal phase. Your effort has highly contributed to clarifying some of the concerns raised by the reviewers and improving our understanding of your work. \n\nOn the other hand, we still think that the current work has rather limited novelty, and motivation and theoretical justification need to be further enhanced to be accepted for ICLR.\n\nFor these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. The reviewers added further comments after receiving your feedback. I hope their comments are useful for improving your work for future publication.", "reviews": [{"review_id": "5ZFeGYBBPgs-0", "review_text": "The paper proposes an objective function i.e. , second-moment loss ( SML ) to better evaluate the uncertainty based on MC dropout . A full network is used to model the mean , while sub-networks are explicitly used to optimize the model variance . According to the claim , it seems the main novelty is introducing a new network for variance . First , it is not new that using one network for accuracy ( e.g. , mean ) and another for uncertainty ( e.g. , variance ) . There are existing models that should be compared and discussed . The authors claim that the model can be to adaptive to domain shift , but there is no explanation why the model can do this ? It is unclear or at least not well motivated why the authors proposed the second-term-moment and why it is advantageous than others . The authors should discuss more recent proposed methods in introduction to show the neccessarity or clear advantages for the proposed one . It is unclear what the gray lines ( sub-networks ) mean in Fig.1 ? And how to obtain the gray lines ? It is better to explain in the introduction or caption of fig . 1.The authors used \u201c prediction uncertainty \u201d and \u201c data-independent uncertainty \u201d . What is the relationship between predictive uncertainty and epistemic uncertainty ? What is data-inherent uncertainty ? what the relationships between it and aleatoric uncertainty ? Why the authors call the distance $ |f_\\tilde { \\theta } \u2013 f_ { \\theta } | $ aleatoric uncertainty ? Although the objective is simple , in the current state , the model is still not clear enough to follow . - Update after rebuttal - Thanks for the feedback from the authors . Unfortunately , although some parts are clarified , the main issues still exist . Overall , the novelty is limited and the motivation is still not clear enough .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the helpful feedback and provide clarifications in the following . > # # # # # # The paper proposes an objective function i.e. , second-moment loss ( SML ) to better evaluate the uncertainty based on MC dropout . A full network is used to model the mean , while sub-networks are explicitly used to optimize the model variance . > # # # # # # According to the claim , it seems the main novelty is introducing a new network for variance . First , it is not new that using one network for accuracy ( e.g. , mean ) and another for uncertainty ( e.g. , variance ) . There are existing models that should be compared and discussed . We do not make use of an additional network but use sub-networks to encode the variance , similar to MC Dropout . That is , we obtain all measurements from a single DNN via implicit dropout ensembles . Training sampled sub-networks with a separate objective function , that extends the regression one , is to the best of our knowledge a novel approach . > # # # # # # The authors claim that the model can be to adaptive to domain shit , but there is no explanation why the model can do this ? What we aimed to say is that the model is able to identify out-of-distribution data as it can result from a domain shift . It is typically assumed that dropout-based approaches have a certain \u201c sensitivity \u201d to out-of-distribution data due to a lacking alignment of the sub-networks given unknown input . Our approach models both aleatoric and epistemic uncertainty by means of sub-networks . It is this integration of uncertainty into the very structure of the network that enables the observed good performance under data shift . These rather heuristic arguments aside , we performed out-of-distribution tests on 13 different datasets with 4 different ways to split data ( 52 tests in total , each cross-validated ) . Overall , we obtained reliable and competitive results for our method . Moreover , we specifically looked at the safety relevant case of extremely unreliable uncertainties , and could ( empirically ) show that our method is , by a large margin , more stable than our major competitors ( parametric deep ensembles ) , meaning extremely unreliable uncertainties are more rare . > # # # # # # It is unclear or at least not well motivated why the authors proposed the second-term-moment and why it is advantageous than others . The authors should discuss more recent proposed methods in introduction to show the neccessarity or clear advantages for the proposed one . We will extend the motivation of the proposed loss in section 3 . For now , the first paragraphs of section 3 outline the ideas underlying the SML . The paragraphs after equation ( 1 ) detail on the SML components and discuss its structure . We would also like to refer to the extended abstract that we uploaded and that further elaborates on the motivation of our approach . The empirical evaluation shows advantages of SML over the most common existing approaches to predictive uncertainty like deep ensembles , MC dropout , and parametric uncertainty . We are happy to include more approaches in our analysis . Which would you consider as the most promising ? > # # # # # # It is unclear what the gray lines ( sub-networks ) mean in Fig.1 ? And how to obtain the gray lines ? It is better to explain in the introduction or caption of fig . 1.We will include a better explanation of Figure 1 in the revised version of the paper . In short , each grey line represents the outputs of one of 200 sub-networks that are obtained by applying dropout-based sampling to the trained full network . For details on the data sets , the neural architecture and the uncertainty methods please refer to section 4 and references therein ."}, {"review_id": "5ZFeGYBBPgs-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper proposes a new objective function to prevent underestimating uncertainties . The authors claimed that the new form leads to state-of-the-art performance in several numerical experiments . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - A new objective function is easy to understand . - The motivation for the work is sensible . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - Although the motivation of the SML loss in Section 3 is sensible , but the theoretical grounds for the SML loss look somewhat weak . MC dropout ( Gal & Ghahramani , 2016 ) is to maximize the evidence lower bound and learns Bayesian neural networks . What are the properties of the optimal solution of the proposed objective function ? - Following the question , is it restricted to the dropout networks ? How can this new loss function be applied other than the dropout networks ? - The authors use the ( gaussian ) negative log-likelihood with the mean and the variance of the sub-network outputs as one of the evaluation measures . In case of MC dropout , the authors use posterior mean estimates $ E ( E ( Y |\\theta , X ) ) $ for $ \\mu $ , but a conditional posterior variance estimates $ Var ( E ( Y | \\theta , X ) ) $ for $ \\sigma^2 $ . Thus , it makes sense the MC dropout might underestimate uncertainties because $ Var ( E ( Y | \\theta , X ) ) \\leq Var ( Y | X ) $ . However , Kendall & Gal ( 2017 ) , which the author cited in the manuscript , addressed this issue and provided a better uncertainty quantification method . But this method is not considered in the experiments . - The paper 's writing makes it very hard to understand the results . For instance , implementation details for Figure 1 or its pointer are not provided . Also , Figure 3 should be improved . The current presentation is confusing and it 's hard to recognize which is better . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I vote for rejection . I may well have missed some points in my reading , so clarification is welcome . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # After the author response As the authors address the reviewer 's concerns , I changed the rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the helpful feedback and provide clarifications in the following . > # # # # # # Cons : > # # # # # # Although the motivation of the SML loss in Section 3 is sensible , but the theoretical grounds for the SML loss looks somewhat weak . MC dropout ( Gal & Ghahramani , 2016 ) is to maximize the evidence lower bound and learns Bayesian neural networks . What are the properties of the optimal solution of the proposed objective function ? Studying the loss landscape induced by the second-moment loss is an important topic that we tried to cover in appendix A.1 . For 1D data , the bi-modality of this loss landscape is a key finding ( see Fig.4 and accompanying text for details ) . We will move parts of this appendix to the main text in the revised version of the paper . > # # # # # # Following the question , is it restricted to the dropout networks ? How can this new loss function be applied other than the dropout networks ? We expect the SML to be \u201c applicable to other models that allow to formulate sub-networks given some kind of mean model \u201c ( see end of section 3 ) . One such model class might be Bayesian approaches beyond MC dropout that learn individual non-binary distributions for each network weight . With slight adjustments our idea of direct variance training carries over to ensemble models without explicit mean model , as well . > # # # # # # The authors use the ( gaussian ) negative log-likelihood with the mean and the variance of the sub-network outputs as one of the evaluation measures . In case of MC dropout , the authors use posterior mean estimates $ E ( E ( Y|\\theta , X ) ) $ for $ \\mu $ , but a conditional posterior variance estimates for $ \\sigma^2 $ . Thus , it makes sense the MC dropout might underestimate uncertainties because $ Var ( E ( Y|\\theta , X ) ) \\leq Var ( Y|X ) $ . However , Kendall & Gal ( 2017 ) , which the author cited in the manuscript , addressed this issue and provided a better uncertainty quantification method . But this method is not considered in the experiments . It is a valid point that MC Dropout has a tendency to underestimate uncertainties . We believe your argument roughly coincides with our footnote 1 ( page 3 ) , which was one of the motivations to implement the SML in the first place . Regarding the mentioned method by Kendall & Gal ( 2017 ) , it is a combination of MC dropout and parametric uncertainty ( PU-MC ) . We originally decided to benchmark only against \u201c atomistic \u201d methods , as many combinations or variants exist in the literature , see , e.g. , [ 6 ] for a combination of MC with DE . However , we took up your suggestion and report our findings below ( follow link below : Tables 1-3 ) . While PU-MC has comparable performance to the other PU-based methods , and is therefore similar to our SML ( follow link below : Tables 1-2 ) , we still observe significant differences due to the different nature of how uncertainty is quantified . This concerns especially the difference for the ETL , where SML still has a significant advantage ( follow link below : Table 3 ) . Additional empirical results : https : //i.ibb.co/R2FWb82/Additional-empirical-results-for-the-Second-Moment-Loss.png [ 6 ] Filos , A. , Farquhar , S. , Gomez , A. N. , Rudner , T. G. , Kenton , Z. , Smith , L. , ... & Gal , Y . ( 2019 ) .A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks . arXiv preprint arXiv:1912.10481 . > # # # # # # The paper 's writing makes it very hard to understand the results . For instance , implementation details for Figure 1 or its pointer are not provided . Also , Figure 3 should be improved . The current presentation is confusing and it 's hard to recognize which is better . We apologize if the presentation of our work did not fully support an easy understanding of the outlined concepts . We will include a better explanation of Figure 1 in the revised version of the paper . In short , each grey line represents the outputs of one of 200 sub-networks that are obtained by applying dropout-based sampling to the trained full network . For details on the data sets ( toy_noise and toy_hf ) , the neural architecture and the uncertainty methods please refer to section 4 and references therein . Regarding Figure 3 , we would like to refer to the second paragraph in section 4.2 that explains the structure of the figure in detail . Moreover , appendix B.3 provides the numerical results underlying it . For the revised version of the paper we are working on a visualization which more strongly emphasizes the SML to allow easier comparison ."}, {"review_id": "5ZFeGYBBPgs-2", "review_text": "# # # # # Summary The paper proposes a variant of Monte-Carlo Dropout that aims to obtain better uncertainty estimates by a better adjustment of the output variance . In concrete , the paper proposes a novel objective function consisting of two terms . The first term is a simple regression loss between the model expected output and the data label . The second term , named `` second-moment loss '' penalizes the differences in the gap between the expected and sample output and the gap between the expected output and the data label . In other words , the output should match the data label in expectation , and the output variance produced by Dropout `` sub-networks '' should follow the variance of the prediction residuals , as a proxy for aleatoric uncertainty . # # # # # Pros - I think the idea overall makes sense , it is simple to implement and could be an interesting addition to the MC Dropout toolkit . - Experimental results on toy datasets show the models trained with the proposed loss are able to effectively capture the aleatoric uncertainty . - On real datasets , the proposed method is compared to many uncertainty estimation methods and shows competitive performance . # # # # # Cons - One potential pitfall could happen when the expected output is far from correct due to model limitations and the random `` sub-network '' predictions become heavily biased . I think further discussion on this issue would be welcome . - The paper is in general correctly structured but leaves some important details to the appendix . Maybe it is because my lack of familiarity with these evaluation metrics , but in my opinion it may be good to clarify them in the main paper . For example : - The `` normalized residuals '' $ r_i $ in page 6 are not properly introduced in the main text . - Is n't the ECE , as defined in the appendix , a measure of fit of the residuals to a uniform distribution ? If yes , why is this a reasonable assumption and why would this indicate a correctly characterized uncertainty ? - Are the $ \\mu_i $ and $ \\sigma_i $ in the evaluation metrics computed from the empirical predictions or from the data ? If computed from the predictions , then the proposed Wasserstein metric looks like a measure of Gaussianity . This would be in contradiction with the ECE metric , and the same questions as in the previous point would apply : why is Gaussianity a reasonable model for the residuals and a desiderata for the output distribution ? what is the intuition for why this test indicates good uncertainty estimates ? - For a fair comparison with ensemble methods , the total number of parameters and operations should be taken into account . * * * * * * * * * * * * * * After Rebuttal : I thank the authors for their extensive answers and clarifications . Overall , I maintain my positive outlook on this work . Although theoretical justification could be improved , I think the experiments do signal that there is something interesting and valuable in this simple approach for characterizing uncertainty .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the helpful feedback and provide clarifications in the following . > # # # # # # Cons > # # # # # # One potential pitfall could happen when the expected output is far from correct due to model limitations and the random `` sub-network '' predictions become heavily biased . I think further discussion on this issue would be welcome . We asked ourselves a similar question . That is why we considered the high-frequency toy experiment ( see section 4.1 ) where the dataset has a much higher complexity than the deliberately small networks . Our SML and PU are still capable of modelling the occurring network errors . We suspect that taking only the absolute error values gives PU and SML an advantage in modelling them , as those values more easily neglect the highly oscillatory structure of the original data . Thus , even small networks with low expressiveness can provide good uncertainties . We will stress this aspect in the updated version of our paper . > # # # # # # The paper is in general correctly structured but leaves some important details to the appendix . Maybe it is because my lack of familiarity with these evaluation metrics , but in my opinion it may be good to clarify them in the main paper . For example : > # # # # # # The `` normalized residuals '' in page 6 are not properly introduced in the main text . We apologize for this mistake in the organization of our paper . We will move the definition of the normalized prediction residuals ( $ r_i = ( \\mu_i - y_ { gt , i } ) / \\sigma_i $ ) from appendix B.1 to the main text . > # # # # # # Is n't the ECE , as defined in the appendix , a measure of fit of the residuals to a uniform distribution ? If yes , why is this a reasonable assumption and why would this indicate a correctly characterized uncertainty ? We will add additional clarification regarding this point to the main text . In short , we assume that prediction residuals are not uniformly but Gaussian distributed ( for a discussion of Gaussianity see the comment after the next one ) . ECE , however , is not directly based on the prediction residuals but on the quantiles of the assumed Gaussian residual distribution . Quantiles of any probability distribution are - by definition - uniformly distributed . Therefore deviations from the uniform distribution in quantiles are a measure of non-Gaussianity of the residuals . For a formal definition of ECE , see e.g. [ 1 ] . [ 1 ] Guo , C. , Pleiss , G. , Sun , Y. , & Weinberger , K. Q . ( 2017 ) .On calibration of modern neural networks . arXiv preprint arXiv:1706.04599 . > # # # # # # Are the $ \\mu_i $ and $ \\sigma_i $ in the evaluation metrics computed from the empirical predictions or from the data ? They are computed from the empirical predictions . For dropout-based methods , $ \\mu_i $ and $ \\sigma_i $ are estimated from a sample of empirical ( dropout-based ) predictions $ \\ { y_k\\ } , k=1 , .. , N $ with N being the number of ( dropout ) forward passes . For the other methods please see the explanations in appendix B.1 . We will add these explanations to the main text in the revised version of our paper ."}], "0": {"review_id": "5ZFeGYBBPgs-0", "review_text": "The paper proposes an objective function i.e. , second-moment loss ( SML ) to better evaluate the uncertainty based on MC dropout . A full network is used to model the mean , while sub-networks are explicitly used to optimize the model variance . According to the claim , it seems the main novelty is introducing a new network for variance . First , it is not new that using one network for accuracy ( e.g. , mean ) and another for uncertainty ( e.g. , variance ) . There are existing models that should be compared and discussed . The authors claim that the model can be to adaptive to domain shift , but there is no explanation why the model can do this ? It is unclear or at least not well motivated why the authors proposed the second-term-moment and why it is advantageous than others . The authors should discuss more recent proposed methods in introduction to show the neccessarity or clear advantages for the proposed one . It is unclear what the gray lines ( sub-networks ) mean in Fig.1 ? And how to obtain the gray lines ? It is better to explain in the introduction or caption of fig . 1.The authors used \u201c prediction uncertainty \u201d and \u201c data-independent uncertainty \u201d . What is the relationship between predictive uncertainty and epistemic uncertainty ? What is data-inherent uncertainty ? what the relationships between it and aleatoric uncertainty ? Why the authors call the distance $ |f_\\tilde { \\theta } \u2013 f_ { \\theta } | $ aleatoric uncertainty ? Although the objective is simple , in the current state , the model is still not clear enough to follow . - Update after rebuttal - Thanks for the feedback from the authors . Unfortunately , although some parts are clarified , the main issues still exist . Overall , the novelty is limited and the motivation is still not clear enough .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the helpful feedback and provide clarifications in the following . > # # # # # # The paper proposes an objective function i.e. , second-moment loss ( SML ) to better evaluate the uncertainty based on MC dropout . A full network is used to model the mean , while sub-networks are explicitly used to optimize the model variance . > # # # # # # According to the claim , it seems the main novelty is introducing a new network for variance . First , it is not new that using one network for accuracy ( e.g. , mean ) and another for uncertainty ( e.g. , variance ) . There are existing models that should be compared and discussed . We do not make use of an additional network but use sub-networks to encode the variance , similar to MC Dropout . That is , we obtain all measurements from a single DNN via implicit dropout ensembles . Training sampled sub-networks with a separate objective function , that extends the regression one , is to the best of our knowledge a novel approach . > # # # # # # The authors claim that the model can be to adaptive to domain shit , but there is no explanation why the model can do this ? What we aimed to say is that the model is able to identify out-of-distribution data as it can result from a domain shift . It is typically assumed that dropout-based approaches have a certain \u201c sensitivity \u201d to out-of-distribution data due to a lacking alignment of the sub-networks given unknown input . Our approach models both aleatoric and epistemic uncertainty by means of sub-networks . It is this integration of uncertainty into the very structure of the network that enables the observed good performance under data shift . These rather heuristic arguments aside , we performed out-of-distribution tests on 13 different datasets with 4 different ways to split data ( 52 tests in total , each cross-validated ) . Overall , we obtained reliable and competitive results for our method . Moreover , we specifically looked at the safety relevant case of extremely unreliable uncertainties , and could ( empirically ) show that our method is , by a large margin , more stable than our major competitors ( parametric deep ensembles ) , meaning extremely unreliable uncertainties are more rare . > # # # # # # It is unclear or at least not well motivated why the authors proposed the second-term-moment and why it is advantageous than others . The authors should discuss more recent proposed methods in introduction to show the neccessarity or clear advantages for the proposed one . We will extend the motivation of the proposed loss in section 3 . For now , the first paragraphs of section 3 outline the ideas underlying the SML . The paragraphs after equation ( 1 ) detail on the SML components and discuss its structure . We would also like to refer to the extended abstract that we uploaded and that further elaborates on the motivation of our approach . The empirical evaluation shows advantages of SML over the most common existing approaches to predictive uncertainty like deep ensembles , MC dropout , and parametric uncertainty . We are happy to include more approaches in our analysis . Which would you consider as the most promising ? > # # # # # # It is unclear what the gray lines ( sub-networks ) mean in Fig.1 ? And how to obtain the gray lines ? It is better to explain in the introduction or caption of fig . 1.We will include a better explanation of Figure 1 in the revised version of the paper . In short , each grey line represents the outputs of one of 200 sub-networks that are obtained by applying dropout-based sampling to the trained full network . For details on the data sets , the neural architecture and the uncertainty methods please refer to section 4 and references therein ."}, "1": {"review_id": "5ZFeGYBBPgs-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper proposes a new objective function to prevent underestimating uncertainties . The authors claimed that the new form leads to state-of-the-art performance in several numerical experiments . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - A new objective function is easy to understand . - The motivation for the work is sensible . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - Although the motivation of the SML loss in Section 3 is sensible , but the theoretical grounds for the SML loss look somewhat weak . MC dropout ( Gal & Ghahramani , 2016 ) is to maximize the evidence lower bound and learns Bayesian neural networks . What are the properties of the optimal solution of the proposed objective function ? - Following the question , is it restricted to the dropout networks ? How can this new loss function be applied other than the dropout networks ? - The authors use the ( gaussian ) negative log-likelihood with the mean and the variance of the sub-network outputs as one of the evaluation measures . In case of MC dropout , the authors use posterior mean estimates $ E ( E ( Y |\\theta , X ) ) $ for $ \\mu $ , but a conditional posterior variance estimates $ Var ( E ( Y | \\theta , X ) ) $ for $ \\sigma^2 $ . Thus , it makes sense the MC dropout might underestimate uncertainties because $ Var ( E ( Y | \\theta , X ) ) \\leq Var ( Y | X ) $ . However , Kendall & Gal ( 2017 ) , which the author cited in the manuscript , addressed this issue and provided a better uncertainty quantification method . But this method is not considered in the experiments . - The paper 's writing makes it very hard to understand the results . For instance , implementation details for Figure 1 or its pointer are not provided . Also , Figure 3 should be improved . The current presentation is confusing and it 's hard to recognize which is better . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I vote for rejection . I may well have missed some points in my reading , so clarification is welcome . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # After the author response As the authors address the reviewer 's concerns , I changed the rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the helpful feedback and provide clarifications in the following . > # # # # # # Cons : > # # # # # # Although the motivation of the SML loss in Section 3 is sensible , but the theoretical grounds for the SML loss looks somewhat weak . MC dropout ( Gal & Ghahramani , 2016 ) is to maximize the evidence lower bound and learns Bayesian neural networks . What are the properties of the optimal solution of the proposed objective function ? Studying the loss landscape induced by the second-moment loss is an important topic that we tried to cover in appendix A.1 . For 1D data , the bi-modality of this loss landscape is a key finding ( see Fig.4 and accompanying text for details ) . We will move parts of this appendix to the main text in the revised version of the paper . > # # # # # # Following the question , is it restricted to the dropout networks ? How can this new loss function be applied other than the dropout networks ? We expect the SML to be \u201c applicable to other models that allow to formulate sub-networks given some kind of mean model \u201c ( see end of section 3 ) . One such model class might be Bayesian approaches beyond MC dropout that learn individual non-binary distributions for each network weight . With slight adjustments our idea of direct variance training carries over to ensemble models without explicit mean model , as well . > # # # # # # The authors use the ( gaussian ) negative log-likelihood with the mean and the variance of the sub-network outputs as one of the evaluation measures . In case of MC dropout , the authors use posterior mean estimates $ E ( E ( Y|\\theta , X ) ) $ for $ \\mu $ , but a conditional posterior variance estimates for $ \\sigma^2 $ . Thus , it makes sense the MC dropout might underestimate uncertainties because $ Var ( E ( Y|\\theta , X ) ) \\leq Var ( Y|X ) $ . However , Kendall & Gal ( 2017 ) , which the author cited in the manuscript , addressed this issue and provided a better uncertainty quantification method . But this method is not considered in the experiments . It is a valid point that MC Dropout has a tendency to underestimate uncertainties . We believe your argument roughly coincides with our footnote 1 ( page 3 ) , which was one of the motivations to implement the SML in the first place . Regarding the mentioned method by Kendall & Gal ( 2017 ) , it is a combination of MC dropout and parametric uncertainty ( PU-MC ) . We originally decided to benchmark only against \u201c atomistic \u201d methods , as many combinations or variants exist in the literature , see , e.g. , [ 6 ] for a combination of MC with DE . However , we took up your suggestion and report our findings below ( follow link below : Tables 1-3 ) . While PU-MC has comparable performance to the other PU-based methods , and is therefore similar to our SML ( follow link below : Tables 1-2 ) , we still observe significant differences due to the different nature of how uncertainty is quantified . This concerns especially the difference for the ETL , where SML still has a significant advantage ( follow link below : Table 3 ) . Additional empirical results : https : //i.ibb.co/R2FWb82/Additional-empirical-results-for-the-Second-Moment-Loss.png [ 6 ] Filos , A. , Farquhar , S. , Gomez , A. N. , Rudner , T. G. , Kenton , Z. , Smith , L. , ... & Gal , Y . ( 2019 ) .A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks . arXiv preprint arXiv:1912.10481 . > # # # # # # The paper 's writing makes it very hard to understand the results . For instance , implementation details for Figure 1 or its pointer are not provided . Also , Figure 3 should be improved . The current presentation is confusing and it 's hard to recognize which is better . We apologize if the presentation of our work did not fully support an easy understanding of the outlined concepts . We will include a better explanation of Figure 1 in the revised version of the paper . In short , each grey line represents the outputs of one of 200 sub-networks that are obtained by applying dropout-based sampling to the trained full network . For details on the data sets ( toy_noise and toy_hf ) , the neural architecture and the uncertainty methods please refer to section 4 and references therein . Regarding Figure 3 , we would like to refer to the second paragraph in section 4.2 that explains the structure of the figure in detail . Moreover , appendix B.3 provides the numerical results underlying it . For the revised version of the paper we are working on a visualization which more strongly emphasizes the SML to allow easier comparison ."}, "2": {"review_id": "5ZFeGYBBPgs-2", "review_text": "# # # # # Summary The paper proposes a variant of Monte-Carlo Dropout that aims to obtain better uncertainty estimates by a better adjustment of the output variance . In concrete , the paper proposes a novel objective function consisting of two terms . The first term is a simple regression loss between the model expected output and the data label . The second term , named `` second-moment loss '' penalizes the differences in the gap between the expected and sample output and the gap between the expected output and the data label . In other words , the output should match the data label in expectation , and the output variance produced by Dropout `` sub-networks '' should follow the variance of the prediction residuals , as a proxy for aleatoric uncertainty . # # # # # Pros - I think the idea overall makes sense , it is simple to implement and could be an interesting addition to the MC Dropout toolkit . - Experimental results on toy datasets show the models trained with the proposed loss are able to effectively capture the aleatoric uncertainty . - On real datasets , the proposed method is compared to many uncertainty estimation methods and shows competitive performance . # # # # # Cons - One potential pitfall could happen when the expected output is far from correct due to model limitations and the random `` sub-network '' predictions become heavily biased . I think further discussion on this issue would be welcome . - The paper is in general correctly structured but leaves some important details to the appendix . Maybe it is because my lack of familiarity with these evaluation metrics , but in my opinion it may be good to clarify them in the main paper . For example : - The `` normalized residuals '' $ r_i $ in page 6 are not properly introduced in the main text . - Is n't the ECE , as defined in the appendix , a measure of fit of the residuals to a uniform distribution ? If yes , why is this a reasonable assumption and why would this indicate a correctly characterized uncertainty ? - Are the $ \\mu_i $ and $ \\sigma_i $ in the evaluation metrics computed from the empirical predictions or from the data ? If computed from the predictions , then the proposed Wasserstein metric looks like a measure of Gaussianity . This would be in contradiction with the ECE metric , and the same questions as in the previous point would apply : why is Gaussianity a reasonable model for the residuals and a desiderata for the output distribution ? what is the intuition for why this test indicates good uncertainty estimates ? - For a fair comparison with ensemble methods , the total number of parameters and operations should be taken into account . * * * * * * * * * * * * * * After Rebuttal : I thank the authors for their extensive answers and clarifications . Overall , I maintain my positive outlook on this work . Although theoretical justification could be improved , I think the experiments do signal that there is something interesting and valuable in this simple approach for characterizing uncertainty .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the helpful feedback and provide clarifications in the following . > # # # # # # Cons > # # # # # # One potential pitfall could happen when the expected output is far from correct due to model limitations and the random `` sub-network '' predictions become heavily biased . I think further discussion on this issue would be welcome . We asked ourselves a similar question . That is why we considered the high-frequency toy experiment ( see section 4.1 ) where the dataset has a much higher complexity than the deliberately small networks . Our SML and PU are still capable of modelling the occurring network errors . We suspect that taking only the absolute error values gives PU and SML an advantage in modelling them , as those values more easily neglect the highly oscillatory structure of the original data . Thus , even small networks with low expressiveness can provide good uncertainties . We will stress this aspect in the updated version of our paper . > # # # # # # The paper is in general correctly structured but leaves some important details to the appendix . Maybe it is because my lack of familiarity with these evaluation metrics , but in my opinion it may be good to clarify them in the main paper . For example : > # # # # # # The `` normalized residuals '' in page 6 are not properly introduced in the main text . We apologize for this mistake in the organization of our paper . We will move the definition of the normalized prediction residuals ( $ r_i = ( \\mu_i - y_ { gt , i } ) / \\sigma_i $ ) from appendix B.1 to the main text . > # # # # # # Is n't the ECE , as defined in the appendix , a measure of fit of the residuals to a uniform distribution ? If yes , why is this a reasonable assumption and why would this indicate a correctly characterized uncertainty ? We will add additional clarification regarding this point to the main text . In short , we assume that prediction residuals are not uniformly but Gaussian distributed ( for a discussion of Gaussianity see the comment after the next one ) . ECE , however , is not directly based on the prediction residuals but on the quantiles of the assumed Gaussian residual distribution . Quantiles of any probability distribution are - by definition - uniformly distributed . Therefore deviations from the uniform distribution in quantiles are a measure of non-Gaussianity of the residuals . For a formal definition of ECE , see e.g. [ 1 ] . [ 1 ] Guo , C. , Pleiss , G. , Sun , Y. , & Weinberger , K. Q . ( 2017 ) .On calibration of modern neural networks . arXiv preprint arXiv:1706.04599 . > # # # # # # Are the $ \\mu_i $ and $ \\sigma_i $ in the evaluation metrics computed from the empirical predictions or from the data ? They are computed from the empirical predictions . For dropout-based methods , $ \\mu_i $ and $ \\sigma_i $ are estimated from a sample of empirical ( dropout-based ) predictions $ \\ { y_k\\ } , k=1 , .. , N $ with N being the number of ( dropout ) forward passes . For the other methods please see the explanations in appendix B.1 . We will add these explanations to the main text in the revised version of our paper ."}}