{"year": "2020", "forum": "BJlahxHYDS", "title": "Conservative Uncertainty Estimation By Fitting  Prior Networks", "decision": "Accept (Poster)", "meta_review": "The paper provides theoretical justification for a previously proposed method for uncertainty estimation based on sampling from a prior distribution (Osband et al., Burda et al.).\n\nThe reviewers initially raised concerns about significance, clarity and experimental evaluation, but the author rebuttal addressed most of these concerns.\n\nIn the end, all the reviewers agreed that the paper deserves to be accepted.", "reviews": [{"review_id": "BJlahxHYDS-0", "review_text": "Overview: This paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. Essentially, instead of training a single predictor that outputs means and uncertainty estimates together, authors propose to have two separate models: one that outputs means, and one that outputs uncertainties. The later one consists of two networks: a randomly initialized \u201cprior\u201d which is fixed and is not trained, and a \u201cpredictor\u201d, which is then trained to predict the output of the randomly initialized \u201cprior\u201d applied to the training samples. Authors show that under some reasonable assumptions the resulting estimates are conservative and concentrated (i.e. bounded and converge to zero with more data). Writing quality: Overall, the paper is relatively well-written, although it might be at times hard to follow, especially for someone who is not familiar with the original work that used randomized prior functions (Burda\u201918, Osband \u201818, \u201819). Evaluation: The method is experimentally evaluated on a task of out-of-distribution detection on CIFAR+SVHN, and seems to perform on-par or better than the baselines (including \u201cstandard\u201d deep ensembles and dropout networks). In addition, there are experiments that demonstrate that the model is performing relatively well in terms of calibration (whether the model predictive behaviour makes sense as the model confidence changes). Decision: I find the core idea behind the paper quite interesting, however, as indicated by authors themselves, it has already been studied in a slightly different context (RL, works by Burda et. al, Osband et. al). That said, authors do provide additional insides for the supervised settings, and also analyse theoretically the behaviour of uncertainty estimates. Overall, I cannot say I am fully convinced that the paper should be accepted as is (also see questions below), but generally I am positive about this work, and hence the final score: \u201cweak accept\u201d. Additional comments / questions: (somewhat minor) p1: \u201cWhile deep ensembles \u2026, where the individual ensembles are trained on different data\u201c - here and related text, it should probably be \u201cindividual models\u201d / \u201cindividual networks\u201d. Generally, I am not convinced that these are strong arguments against deep ensembles. (minor) p2-p3: \u201c2. Preliminaries\u201d - I am not sure if this section adds much to the understanding, it would seem more natural to spend more time explaining the intuitions behind the net (kind of major) p3. \u201cprior\u201d - The explanation of why using a randomly initialized network makes sense is not very strict. I kind of get the general idea, but it is not clear to me why not use something less expensive, e.g. just random projections, and why do we actually need a full network. Intuitively it seems quite strange to waste a lot of capacity to fit to essentially fit a set of random weights: is it something that allows the network to avoid easily learning the \u201crandom prior\u201d? And, more generally, can this also be considered as a \u201ctrick\u201d to de-correlate individual predictors? I believe these points should be discussed in more detail. <update> I would like to thank authors for verbose response and the revised version: it is a bit more clear. I stand by my original rating. </update> ", "rating": "6: Weak Accept", "reply_text": "We address your points below . `` Overall , the paper is relatively well-written , although it might be at times hard to follow , especially for someone who is not familiar with the original work that used randomized prior functions ( Burda \u2019 18 , Osband \u2018 18 , \u2018 19 ) . '' Thanks for the kind words ! We will shortly upload a revised version of the paper with improved clarity . `` I find the core idea behind the paper quite interesting , however , as indicated by authors themselves , it has already been studied in a slightly different context ( RL , works by Burda et.al , Osband et . al ) .That said , authors do provide additional insides for the supervised settings , and also analyse theoretically the behaviour of uncertainty estimates . '' Thanks for pointing this out . The main contribution of the paper is a theoretical analysis of uncertainties obtained from fitting random priors for * arbitrary deep networks * . This is different from Osband 's work , which discusses Bayesian linear regression . We believe that the difference in setting is hugely significant , because non-linear function approximation is necessary for many applications of Machine Learning . The work by Burda et al.provides great empirical results in RL , but no theoretical justification for the uncertainties . `` ( somewhat minor ) p1 : 'While deep ensembles \u2026 , where the individual ensembles are trained on different data ' - here and related text , it should probably be 'individual models ' / 'individual networks ' . Generally , I am not convinced that these are strong arguments against deep ensembles . '' You are right about the wording - we meant 'individual network ' . In terms of comparing our work to deep ensembles , we wanted to make two points . First , our work can be understood as complementary , to be used in situations where the conservatism property is desired . Second , our method has some practical advantages over deep ensembles . In practice , deep ensembles can also become overconfident ( Figure 6 ) and poorly calibrated ( Figure 5 ) . Also , deep ensembles require more bootstraps ( we will describe this more explicitly in the updated paper ) . We will provide a more detailed comparison with deep ensembles in the revised version of the paper . Also , the main theoretical argument against deep ensembles is that the uncertainties obtained from them in practice ( when each ensemble was trained on the whole dataset ) do not have a connection to a Bayesian posterior . `` ( minor ) p2-p3 : \u201c 2 . Preliminaries \u201d - I am not sure if this section adds much to the understanding , it would seem more natural to spend more time explaining the intuitions behind the net '' In the revised submission , we will move some of the preliminary material to the appendix and use the space to provide more intuitions . `` The explanation of why using a randomly initialized network makes sense is not very strict . I kind of get the general idea , but it is not clear to me why not use something less expensive , e.g.just random projections , and why do we actually need a full network . Intuitively it seems quite strange to waste a lot of capacity to fit to essentially fit a set of random weights : is it something that allows the network to avoid easily learning the \u201c random prior \u201d ? And , more generally , can this also be considered as a \u201c trick \u201d to de-correlate individual predictors ? I believe these points should be discussed in more detail . '' Thanks for pointing this out . On the formal side , the justification for using random priors is that our uncertainty estimates overestimate the Gaussian posterior . Deep ensembles do not have a comparable guarantee . Intuitively , the role that the random networks play is to avoid spurious confidence . We would not say that random priors waste capacity since the predictor network still depends on the x 's ( training points ) and that is very useful for OOD detection . The fact that the predictors are trained independently means , as you say , that uncertainty estimates obtained from each predictor-prior pair are independent ( de-correlated ) . We will include these insights in the revised version of the paper . Overall , thanks for the detailed comments . If you have other suggestions for improving the paper , please share !"}, {"review_id": "BJlahxHYDS-1", "review_text": "The paper shows that the MSE of a deep network trained to match fixed random network is a conservative estimate of uncertainty in expectation over many such network pairs. An experiment compares this previously proposed method to other approaches for uncertainty estimation on CIFAR 10. Strengths: - Obtaining uncertainty estimates for predictions of deep neural networks is an important and open research question. - Proposition 1 is an interesting result, although the paper does not seem to discuss its significance and implications enough. Weaknesses: - Proposition 1 is described as \"our uncertainty estimates are never too small\". However, as the name of the proposition suggests, it seems to only hold in expectation over models trained, which is a quite different statement. - Proposition 2 seems to simply state that a small network can be distilled into a large network. Maybe I missed part of the reasoning here? Otherwise, it should probably not be highlighted as a major contribution. - The experimental evaluation is very limited, training only on CIFAR 10. While the experiments add little value to the community, this may be acceptable for a mostly theoretical paper. Clarity: - The paper was difficult to read and unclear in explanations. It could help to define notation upfront instead of introducing shorthands on the way. - In Figure 3, the Y axis limits should be fixed across seen and unseen histograms for the same method. The current presentation is a bit misleading here, as the presented method seems to have moved the most mass under this chart scaling. Comments: - As found in prior work cited in the submission, the method tends to perform well with just one network pair. This raises the question whether the contribution of the paper that holds in expectation over many pairs and the empirical success of the approach are connected. - The marginal posterior variance \\sigma^2(x_\\star) appears in various forms with hat, tilde, and different subscripts. It maybe worth assigning different letters to these to avoid confusion.", "rating": "6: Weak Accept", "reply_text": "Below , we address your points in detail . `` [ in the Strength Section ] Proposition 1 is an interesting result , although the paper does not seem to discuss its significance and implications enough '' . Overall , the main contribution of the paper is the we provide a sound theoretical framework for what was previously an ad-hoc method lacking principled justification . We will provide more background specifically about the usefulness Proposition 1 in a revised version of the paper . The main justification is that Proposition 1 guarantees that we wo n't become overconfident . We show empirically ( Figures 3 and 6 ) that this problem can happen when using deep ensembles . `` Proposition 1 is described as 'our uncertainty estimates are never too small ' . However , as the name of the proposition suggests , it seems to only hold in expectation over models trained , which is a quite different statement . '' You are right when you say that Proposition 1 on its own holds in expectation . However , we also provide a result for a finite number of trained models in Lemmas 2 and 3 in the `` Finite Bootstraps '' section of the paper . Combining these two Lemmas with Proposition 1 gives a conservatism guarantee that is valid even for just one trained model . We agree that it is not very clearly presented - we will will add an explicit corollary that provides a conservatism guarantee with a finite number of bootstraps / models . `` Proposition 2 seems to simply state that a small network can be distilled into a large network . Maybe I missed part of the reasoning here ? Otherwise , it should probably not be highlighted as a major contribution . '' Concentration is a crucial property for uncertainty estimates and we feel that a paper without this result would be incomplete . Proposition 2 shows that , given enough data , uncertainties become small on * unseen * points . This is not the same as simply distilling the smaller `` prior '' network on some dataset . In other words , in order for the concentration result to hold , the capacity of the larger ( predictor ) network has to be controlled ( which we do by the Lipschitz constant ) . While the Rademacher complexity tools we use to show this result are standard , to the best of our knowledge , they have not been used in a similar context . `` The experimental evaluation is very limited , training only on CIFAR 10 . While the experiments add little value to the community , this may be acceptable for a mostly theoretical paper . '' As you say , the experiments are meant to lend support to the theoretical part of the paper . The experimental evaluation has been carefully selected to focus on how conservative uncertainty estimates avoid overconfidence . For example , Figures 3 and 6 show that uncertainty estimates obtained from random priors show a better separation between seen and unseen points . They are also better calibrated - Figure 5 shows that the curves for random priors are closer to monotonic than for competing approaches . We believe that our experiments do provide a value for the community in the sense of validating theory . `` The paper was difficult to read and unclear in explanations . It could help to define notation upfront instead of introducing shorthands on the way . '' Thanks for pointing this out . We will upload a revised submission where the notation is clearer . `` In Figure 3 , the Y axis limits should be fixed across seen and unseen histograms for the same method . The current presentation is a bit misleading here , as the presented method seems to have moved the most mass under this chart scaling '' We will revise Figure 3 ( and also Figure 6 ) to allow a direct comparison . We agree that it will make the figures easier to interpret . `` [ In the comments ] As found in prior work cited in the submission , the method tends to perform well with just one network pair . This raises the question whether the contribution of the paper that holds in expectation over many pairs and the empirical success of the approach are connected . '' Thanks to Lemmas 2 and 3 , out theory carries over to a finite number of prior-predictor pairs ( including just one pair ) . We will add a corollary to make this explicit . `` The marginal posterior variance \\sigma^2 ( x_\\star ) appears in various forms with hat , tilde , and different subscripts . It maybe worth assigning different letters to these to avoid confusion . '' We will provide a revised version of the paper with an updated notation . Overall , thanks a lot for the feedback . Please let us know if you have other suggestions for improving the paper ."}, {"review_id": "BJlahxHYDS-2", "review_text": "This work introduces a simple technique to obtain uncertainty estimates for deep neural networks. This is achieved by having a set of random networks (i.e. neural networks where their parameters are randomly initialized) and then computing an uncertainty value based on the difference in the predictions between those random networks and networks that are trained to mimic them on a finite collection of points. The authors further show that this method results into uncertainties that are conservative, meaning that they are higher than the uncertainty of a hypothetical posterior, and concentrate, i.e. they converge towards zero when we get more and more data. The authors further draw connections to ensemble methods and discuss how such a method can be effectively realized in practice. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. This work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates. For this reason it can potentially serve as a strong baseline for this field. The theoretical considerations also help in providing some guarantees about such an approach. Having said that, in my opinion the writing could use some more work in order to make things more clear as some critical experimental details and baselines are missing and thus do not make the method as convincing. Furthermore, I also believe that some clarifications on the theoretical aspects of this work, will help in boosting its quality. More specifically: - How exactly do you apply your method on the classification scenario? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities? In appendix A you mention the architecture but not precisely how it is employed. I believe this can be an important piece of information in order to decipher the importance of e.g. the output dimensionality on the uncertainty quality, as higher dimensional outputs might be harder to approximate thus could induce a larger squared error and hence uncertainty. - What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task, i.e. how far away from concentration were the priors? - An effect that I found weird is the following: what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs? Wouldn\u2019t that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? One could imagine that for e.g. simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice. - For the conservatism you show that your uncertainty estimate is higher, on average, than the posterior variance when you sample points from the model itself. In a sense this guarantee translates to the actual data when the prior is \u201ccorrect\u201d. How do those conservatism guarantees translate to the case when there is model misspecification, i.e. when the prior is not correct? Perhaps a small toy example would be informative. - For the predictor networks as described in figure 2; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f? (This helps in understanding how easy / difficult is the task of the predictor network). - What is the accuracy on the actual in-distribution prediction task for the RP and baselines? What did \u201cB\u201d correspond to for the dropout networks? Was it the number of dropout samples you averaged over to get the final predictive? - How sensitive are the results on the actual initialization strategy of the prior network? It would be good to see e.g. some form of performance / init variance curve in order to decipher the sensitivity. Other comments - It is worth pointing out that [1] showed that Monte-Carlo dropout performs approximate MAP inference, which seems more plausible than the approximate Bayesian inference perspective of [2]. - In the introduction you argue that Bayesian neural networks rely on procedures different from standard supervised learning and thus most ML pipelines are not optimized for them in practice. Could you elaborate a bit about this statement? Variationally trained BNNs with e.g. the reparametrization trick [3, 4] are straightforward since you can just use backpropagation to update their (variational) parameters. - What is the x-axis for Figure 3 for the baselines? (I take it that for RP it is the \\hat{sigma}^2(x)). - I believe that a comparison against a simple variationally trained BNN would make the results more convincing. Misc - Second page, \u201cFigure 1, top two plota\u201d -> \u201cFigure 1, top two plots\u201d - Third page, \u201c[\u2026] introduced in equation 2 denotes the posterior covariance [\u2026.]\u201d -> \u201c[\u2026] introduced in equation 2 denotes the posterior variance [\u2026]\u201c - Fifth page, \u201c[\u2026] this makes it is reasonable for W large enough [\u2026]\u201d -> \u201c[\u2026] this makes it reasonable for W large enough [\u2026]\u201d - Sixth page, \u201cCorollary 1 and proposition 2\u201d; where is corollary 1? Do you mean Proposition 1? - Seventh page, \u201c[\u2026] inspired by, an builds on, [\u2026]\u201d -> \u201cinspired by, and builds on, [\u2026]\u201d - Ninth page \u201cmontonicity\u201d -> \u201cmonotonicity\u201d Overall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly. [1] Eric Nalisnick, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Padhraic Smyth, Dropout as a Structured Shrinkage Prior, 2019 [2] Yarin Gal, Zoubin Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, 2016 [3] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014 [4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014", "rating": "6: Weak Accept", "reply_text": "We answer your individual major points below . `` This work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates . For this reason it can potentially serve as a strong baseline for this field '' Thanks a lot . We appreciate . `` the writing could use some more work in order to make things more clear '' We will provide a revised version of the paper which we hope is clearer . `` How exactly do you apply your method on the classification scenario ? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities ? '' For classification , the architecture for the prior network was the same as the classification network , but using a final linear layer instead of the softmax layer . We used squared error on that last layer to get the uncertainties . We will clarify this in the revised paper . `` What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task , i.e.how far away from concentration were the priors ? '' The histogram of the training error is the same as the histogram of the uncertainties ( Figures 3 and 6 ) , first column . We will provide a summary of training / test errors in the revised version of the paper . `` An effect that I found weird is the following : what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs ? Wouldn \u2019 t that then imply that the uncertainty would be zero for any input ( even an out-of-distribution one ) , as the prior network and predictor network always agree ? One could imagine that for e.g.simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice . '' Our experiments show that exact network cloning does not happen in practice . On the theoretical side , a useful edge case to consider is if the priors are just linear functions and we are solving a one-dimensional regression problem . In this case , ( assuming no observation noise ) , evaluating the prior at two points is enough to fit the linear function and completely know the value of the prior everywhere . As you say , afterwards , the prior and predictor will always agree and uncertainty estimates will be zero . However , this is still consistent with our theory ! The reason for this is once we assume such a linear prior , we are comparing to a GP with a linear kernel . But a GP * with that kernel * will also have zero uncertainty after seeing two samples , hence the conservatism guarantee still holds . In practice , this means that we have to be careful when choosing the architecture of the prior networks However , this difficulty is no different from performing Bayesian inference with a prior inappropriate for the problem . Empirically , reasonable network architectures do not show such cloning behavior . `` For the conservatism you show that your uncertainty estimate is higher , on average , than the posterior variance when you sample points from the model itself . In a sense this guarantee translates to the actual data when the prior is \u201c correct \u201d . How do those conservatism guarantees translate to the case when there is model misspecification , i.e.when the prior is not correct ? Perhaps a small toy example would be informative . '' Our method is affected by model misspecification in the same way as Gaussian Processes are . In particular , our conservatism results guarantee that the uncertainty obtained from random priors are greater or equal than uncertainties obtained from a GP which uses the same prior . Also , since in practice the priors are neural networks , chosen to have large capacity , we are unlikely to end up in a situation where the prior has no support on the ground truth function . `` For the predictor networks as described in figure 2 ; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f ? ( This helps in understanding how easy / difficult is the task of the predictor network ) . '' In figure 2 , we train the entire predictor networks ( both parts ) . We will clarify the caption . `` What is the accuracy on the actual in-distribution prediction task for the RP and baselines ? '' In the paper , we reported the AUROC as it is a more robust . measure of classification quality . In the revised version , we will additionally provide accuracy numbers in the appendix . `` What did ' B ' correspond to for the dropout networks ? Was it the number of dropout samples you averaged over to get the final predictive ? '' As you say , B was the number of samples averaged over at test time . We will make this clearer in the revised submission . `` How sensitive are the results on the actual initialization strategy of the prior network ? It would be good to see e.g.some form of performance / init variance curve in order to decipher the sensitivity . '' We will try to do an ablation on the initialization scale ."}], "0": {"review_id": "BJlahxHYDS-0", "review_text": "Overview: This paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. Essentially, instead of training a single predictor that outputs means and uncertainty estimates together, authors propose to have two separate models: one that outputs means, and one that outputs uncertainties. The later one consists of two networks: a randomly initialized \u201cprior\u201d which is fixed and is not trained, and a \u201cpredictor\u201d, which is then trained to predict the output of the randomly initialized \u201cprior\u201d applied to the training samples. Authors show that under some reasonable assumptions the resulting estimates are conservative and concentrated (i.e. bounded and converge to zero with more data). Writing quality: Overall, the paper is relatively well-written, although it might be at times hard to follow, especially for someone who is not familiar with the original work that used randomized prior functions (Burda\u201918, Osband \u201818, \u201819). Evaluation: The method is experimentally evaluated on a task of out-of-distribution detection on CIFAR+SVHN, and seems to perform on-par or better than the baselines (including \u201cstandard\u201d deep ensembles and dropout networks). In addition, there are experiments that demonstrate that the model is performing relatively well in terms of calibration (whether the model predictive behaviour makes sense as the model confidence changes). Decision: I find the core idea behind the paper quite interesting, however, as indicated by authors themselves, it has already been studied in a slightly different context (RL, works by Burda et. al, Osband et. al). That said, authors do provide additional insides for the supervised settings, and also analyse theoretically the behaviour of uncertainty estimates. Overall, I cannot say I am fully convinced that the paper should be accepted as is (also see questions below), but generally I am positive about this work, and hence the final score: \u201cweak accept\u201d. Additional comments / questions: (somewhat minor) p1: \u201cWhile deep ensembles \u2026, where the individual ensembles are trained on different data\u201c - here and related text, it should probably be \u201cindividual models\u201d / \u201cindividual networks\u201d. Generally, I am not convinced that these are strong arguments against deep ensembles. (minor) p2-p3: \u201c2. Preliminaries\u201d - I am not sure if this section adds much to the understanding, it would seem more natural to spend more time explaining the intuitions behind the net (kind of major) p3. \u201cprior\u201d - The explanation of why using a randomly initialized network makes sense is not very strict. I kind of get the general idea, but it is not clear to me why not use something less expensive, e.g. just random projections, and why do we actually need a full network. Intuitively it seems quite strange to waste a lot of capacity to fit to essentially fit a set of random weights: is it something that allows the network to avoid easily learning the \u201crandom prior\u201d? And, more generally, can this also be considered as a \u201ctrick\u201d to de-correlate individual predictors? I believe these points should be discussed in more detail. <update> I would like to thank authors for verbose response and the revised version: it is a bit more clear. I stand by my original rating. </update> ", "rating": "6: Weak Accept", "reply_text": "We address your points below . `` Overall , the paper is relatively well-written , although it might be at times hard to follow , especially for someone who is not familiar with the original work that used randomized prior functions ( Burda \u2019 18 , Osband \u2018 18 , \u2018 19 ) . '' Thanks for the kind words ! We will shortly upload a revised version of the paper with improved clarity . `` I find the core idea behind the paper quite interesting , however , as indicated by authors themselves , it has already been studied in a slightly different context ( RL , works by Burda et.al , Osband et . al ) .That said , authors do provide additional insides for the supervised settings , and also analyse theoretically the behaviour of uncertainty estimates . '' Thanks for pointing this out . The main contribution of the paper is a theoretical analysis of uncertainties obtained from fitting random priors for * arbitrary deep networks * . This is different from Osband 's work , which discusses Bayesian linear regression . We believe that the difference in setting is hugely significant , because non-linear function approximation is necessary for many applications of Machine Learning . The work by Burda et al.provides great empirical results in RL , but no theoretical justification for the uncertainties . `` ( somewhat minor ) p1 : 'While deep ensembles \u2026 , where the individual ensembles are trained on different data ' - here and related text , it should probably be 'individual models ' / 'individual networks ' . Generally , I am not convinced that these are strong arguments against deep ensembles . '' You are right about the wording - we meant 'individual network ' . In terms of comparing our work to deep ensembles , we wanted to make two points . First , our work can be understood as complementary , to be used in situations where the conservatism property is desired . Second , our method has some practical advantages over deep ensembles . In practice , deep ensembles can also become overconfident ( Figure 6 ) and poorly calibrated ( Figure 5 ) . Also , deep ensembles require more bootstraps ( we will describe this more explicitly in the updated paper ) . We will provide a more detailed comparison with deep ensembles in the revised version of the paper . Also , the main theoretical argument against deep ensembles is that the uncertainties obtained from them in practice ( when each ensemble was trained on the whole dataset ) do not have a connection to a Bayesian posterior . `` ( minor ) p2-p3 : \u201c 2 . Preliminaries \u201d - I am not sure if this section adds much to the understanding , it would seem more natural to spend more time explaining the intuitions behind the net '' In the revised submission , we will move some of the preliminary material to the appendix and use the space to provide more intuitions . `` The explanation of why using a randomly initialized network makes sense is not very strict . I kind of get the general idea , but it is not clear to me why not use something less expensive , e.g.just random projections , and why do we actually need a full network . Intuitively it seems quite strange to waste a lot of capacity to fit to essentially fit a set of random weights : is it something that allows the network to avoid easily learning the \u201c random prior \u201d ? And , more generally , can this also be considered as a \u201c trick \u201d to de-correlate individual predictors ? I believe these points should be discussed in more detail . '' Thanks for pointing this out . On the formal side , the justification for using random priors is that our uncertainty estimates overestimate the Gaussian posterior . Deep ensembles do not have a comparable guarantee . Intuitively , the role that the random networks play is to avoid spurious confidence . We would not say that random priors waste capacity since the predictor network still depends on the x 's ( training points ) and that is very useful for OOD detection . The fact that the predictors are trained independently means , as you say , that uncertainty estimates obtained from each predictor-prior pair are independent ( de-correlated ) . We will include these insights in the revised version of the paper . Overall , thanks for the detailed comments . If you have other suggestions for improving the paper , please share !"}, "1": {"review_id": "BJlahxHYDS-1", "review_text": "The paper shows that the MSE of a deep network trained to match fixed random network is a conservative estimate of uncertainty in expectation over many such network pairs. An experiment compares this previously proposed method to other approaches for uncertainty estimation on CIFAR 10. Strengths: - Obtaining uncertainty estimates for predictions of deep neural networks is an important and open research question. - Proposition 1 is an interesting result, although the paper does not seem to discuss its significance and implications enough. Weaknesses: - Proposition 1 is described as \"our uncertainty estimates are never too small\". However, as the name of the proposition suggests, it seems to only hold in expectation over models trained, which is a quite different statement. - Proposition 2 seems to simply state that a small network can be distilled into a large network. Maybe I missed part of the reasoning here? Otherwise, it should probably not be highlighted as a major contribution. - The experimental evaluation is very limited, training only on CIFAR 10. While the experiments add little value to the community, this may be acceptable for a mostly theoretical paper. Clarity: - The paper was difficult to read and unclear in explanations. It could help to define notation upfront instead of introducing shorthands on the way. - In Figure 3, the Y axis limits should be fixed across seen and unseen histograms for the same method. The current presentation is a bit misleading here, as the presented method seems to have moved the most mass under this chart scaling. Comments: - As found in prior work cited in the submission, the method tends to perform well with just one network pair. This raises the question whether the contribution of the paper that holds in expectation over many pairs and the empirical success of the approach are connected. - The marginal posterior variance \\sigma^2(x_\\star) appears in various forms with hat, tilde, and different subscripts. It maybe worth assigning different letters to these to avoid confusion.", "rating": "6: Weak Accept", "reply_text": "Below , we address your points in detail . `` [ in the Strength Section ] Proposition 1 is an interesting result , although the paper does not seem to discuss its significance and implications enough '' . Overall , the main contribution of the paper is the we provide a sound theoretical framework for what was previously an ad-hoc method lacking principled justification . We will provide more background specifically about the usefulness Proposition 1 in a revised version of the paper . The main justification is that Proposition 1 guarantees that we wo n't become overconfident . We show empirically ( Figures 3 and 6 ) that this problem can happen when using deep ensembles . `` Proposition 1 is described as 'our uncertainty estimates are never too small ' . However , as the name of the proposition suggests , it seems to only hold in expectation over models trained , which is a quite different statement . '' You are right when you say that Proposition 1 on its own holds in expectation . However , we also provide a result for a finite number of trained models in Lemmas 2 and 3 in the `` Finite Bootstraps '' section of the paper . Combining these two Lemmas with Proposition 1 gives a conservatism guarantee that is valid even for just one trained model . We agree that it is not very clearly presented - we will will add an explicit corollary that provides a conservatism guarantee with a finite number of bootstraps / models . `` Proposition 2 seems to simply state that a small network can be distilled into a large network . Maybe I missed part of the reasoning here ? Otherwise , it should probably not be highlighted as a major contribution . '' Concentration is a crucial property for uncertainty estimates and we feel that a paper without this result would be incomplete . Proposition 2 shows that , given enough data , uncertainties become small on * unseen * points . This is not the same as simply distilling the smaller `` prior '' network on some dataset . In other words , in order for the concentration result to hold , the capacity of the larger ( predictor ) network has to be controlled ( which we do by the Lipschitz constant ) . While the Rademacher complexity tools we use to show this result are standard , to the best of our knowledge , they have not been used in a similar context . `` The experimental evaluation is very limited , training only on CIFAR 10 . While the experiments add little value to the community , this may be acceptable for a mostly theoretical paper . '' As you say , the experiments are meant to lend support to the theoretical part of the paper . The experimental evaluation has been carefully selected to focus on how conservative uncertainty estimates avoid overconfidence . For example , Figures 3 and 6 show that uncertainty estimates obtained from random priors show a better separation between seen and unseen points . They are also better calibrated - Figure 5 shows that the curves for random priors are closer to monotonic than for competing approaches . We believe that our experiments do provide a value for the community in the sense of validating theory . `` The paper was difficult to read and unclear in explanations . It could help to define notation upfront instead of introducing shorthands on the way . '' Thanks for pointing this out . We will upload a revised submission where the notation is clearer . `` In Figure 3 , the Y axis limits should be fixed across seen and unseen histograms for the same method . The current presentation is a bit misleading here , as the presented method seems to have moved the most mass under this chart scaling '' We will revise Figure 3 ( and also Figure 6 ) to allow a direct comparison . We agree that it will make the figures easier to interpret . `` [ In the comments ] As found in prior work cited in the submission , the method tends to perform well with just one network pair . This raises the question whether the contribution of the paper that holds in expectation over many pairs and the empirical success of the approach are connected . '' Thanks to Lemmas 2 and 3 , out theory carries over to a finite number of prior-predictor pairs ( including just one pair ) . We will add a corollary to make this explicit . `` The marginal posterior variance \\sigma^2 ( x_\\star ) appears in various forms with hat , tilde , and different subscripts . It maybe worth assigning different letters to these to avoid confusion . '' We will provide a revised version of the paper with an updated notation . Overall , thanks a lot for the feedback . Please let us know if you have other suggestions for improving the paper ."}, "2": {"review_id": "BJlahxHYDS-2", "review_text": "This work introduces a simple technique to obtain uncertainty estimates for deep neural networks. This is achieved by having a set of random networks (i.e. neural networks where their parameters are randomly initialized) and then computing an uncertainty value based on the difference in the predictions between those random networks and networks that are trained to mimic them on a finite collection of points. The authors further show that this method results into uncertainties that are conservative, meaning that they are higher than the uncertainty of a hypothetical posterior, and concentrate, i.e. they converge towards zero when we get more and more data. The authors further draw connections to ensemble methods and discuss how such a method can be effectively realized in practice. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. This work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates. For this reason it can potentially serve as a strong baseline for this field. The theoretical considerations also help in providing some guarantees about such an approach. Having said that, in my opinion the writing could use some more work in order to make things more clear as some critical experimental details and baselines are missing and thus do not make the method as convincing. Furthermore, I also believe that some clarifications on the theoretical aspects of this work, will help in boosting its quality. More specifically: - How exactly do you apply your method on the classification scenario? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities? In appendix A you mention the architecture but not precisely how it is employed. I believe this can be an important piece of information in order to decipher the importance of e.g. the output dimensionality on the uncertainty quality, as higher dimensional outputs might be harder to approximate thus could induce a larger squared error and hence uncertainty. - What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task, i.e. how far away from concentration were the priors? - An effect that I found weird is the following: what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs? Wouldn\u2019t that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? One could imagine that for e.g. simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice. - For the conservatism you show that your uncertainty estimate is higher, on average, than the posterior variance when you sample points from the model itself. In a sense this guarantee translates to the actual data when the prior is \u201ccorrect\u201d. How do those conservatism guarantees translate to the case when there is model misspecification, i.e. when the prior is not correct? Perhaps a small toy example would be informative. - For the predictor networks as described in figure 2; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f? (This helps in understanding how easy / difficult is the task of the predictor network). - What is the accuracy on the actual in-distribution prediction task for the RP and baselines? What did \u201cB\u201d correspond to for the dropout networks? Was it the number of dropout samples you averaged over to get the final predictive? - How sensitive are the results on the actual initialization strategy of the prior network? It would be good to see e.g. some form of performance / init variance curve in order to decipher the sensitivity. Other comments - It is worth pointing out that [1] showed that Monte-Carlo dropout performs approximate MAP inference, which seems more plausible than the approximate Bayesian inference perspective of [2]. - In the introduction you argue that Bayesian neural networks rely on procedures different from standard supervised learning and thus most ML pipelines are not optimized for them in practice. Could you elaborate a bit about this statement? Variationally trained BNNs with e.g. the reparametrization trick [3, 4] are straightforward since you can just use backpropagation to update their (variational) parameters. - What is the x-axis for Figure 3 for the baselines? (I take it that for RP it is the \\hat{sigma}^2(x)). - I believe that a comparison against a simple variationally trained BNN would make the results more convincing. Misc - Second page, \u201cFigure 1, top two plota\u201d -> \u201cFigure 1, top two plots\u201d - Third page, \u201c[\u2026] introduced in equation 2 denotes the posterior covariance [\u2026.]\u201d -> \u201c[\u2026] introduced in equation 2 denotes the posterior variance [\u2026]\u201c - Fifth page, \u201c[\u2026] this makes it is reasonable for W large enough [\u2026]\u201d -> \u201c[\u2026] this makes it reasonable for W large enough [\u2026]\u201d - Sixth page, \u201cCorollary 1 and proposition 2\u201d; where is corollary 1? Do you mean Proposition 1? - Seventh page, \u201c[\u2026] inspired by, an builds on, [\u2026]\u201d -> \u201cinspired by, and builds on, [\u2026]\u201d - Ninth page \u201cmontonicity\u201d -> \u201cmonotonicity\u201d Overall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly. [1] Eric Nalisnick, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Padhraic Smyth, Dropout as a Structured Shrinkage Prior, 2019 [2] Yarin Gal, Zoubin Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, 2016 [3] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014 [4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014", "rating": "6: Weak Accept", "reply_text": "We answer your individual major points below . `` This work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates . For this reason it can potentially serve as a strong baseline for this field '' Thanks a lot . We appreciate . `` the writing could use some more work in order to make things more clear '' We will provide a revised version of the paper which we hope is clearer . `` How exactly do you apply your method on the classification scenario ? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities ? '' For classification , the architecture for the prior network was the same as the classification network , but using a final linear layer instead of the softmax layer . We used squared error on that last layer to get the uncertainties . We will clarify this in the revised paper . `` What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task , i.e.how far away from concentration were the priors ? '' The histogram of the training error is the same as the histogram of the uncertainties ( Figures 3 and 6 ) , first column . We will provide a summary of training / test errors in the revised version of the paper . `` An effect that I found weird is the following : what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs ? Wouldn \u2019 t that then imply that the uncertainty would be zero for any input ( even an out-of-distribution one ) , as the prior network and predictor network always agree ? One could imagine that for e.g.simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice . '' Our experiments show that exact network cloning does not happen in practice . On the theoretical side , a useful edge case to consider is if the priors are just linear functions and we are solving a one-dimensional regression problem . In this case , ( assuming no observation noise ) , evaluating the prior at two points is enough to fit the linear function and completely know the value of the prior everywhere . As you say , afterwards , the prior and predictor will always agree and uncertainty estimates will be zero . However , this is still consistent with our theory ! The reason for this is once we assume such a linear prior , we are comparing to a GP with a linear kernel . But a GP * with that kernel * will also have zero uncertainty after seeing two samples , hence the conservatism guarantee still holds . In practice , this means that we have to be careful when choosing the architecture of the prior networks However , this difficulty is no different from performing Bayesian inference with a prior inappropriate for the problem . Empirically , reasonable network architectures do not show such cloning behavior . `` For the conservatism you show that your uncertainty estimate is higher , on average , than the posterior variance when you sample points from the model itself . In a sense this guarantee translates to the actual data when the prior is \u201c correct \u201d . How do those conservatism guarantees translate to the case when there is model misspecification , i.e.when the prior is not correct ? Perhaps a small toy example would be informative . '' Our method is affected by model misspecification in the same way as Gaussian Processes are . In particular , our conservatism results guarantee that the uncertainty obtained from random priors are greater or equal than uncertainties obtained from a GP which uses the same prior . Also , since in practice the priors are neural networks , chosen to have large capacity , we are unlikely to end up in a situation where the prior has no support on the ground truth function . `` For the predictor networks as described in figure 2 ; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f ? ( This helps in understanding how easy / difficult is the task of the predictor network ) . '' In figure 2 , we train the entire predictor networks ( both parts ) . We will clarify the caption . `` What is the accuracy on the actual in-distribution prediction task for the RP and baselines ? '' In the paper , we reported the AUROC as it is a more robust . measure of classification quality . In the revised version , we will additionally provide accuracy numbers in the appendix . `` What did ' B ' correspond to for the dropout networks ? Was it the number of dropout samples you averaged over to get the final predictive ? '' As you say , B was the number of samples averaged over at test time . We will make this clearer in the revised submission . `` How sensitive are the results on the actual initialization strategy of the prior network ? It would be good to see e.g.some form of performance / init variance curve in order to decipher the sensitivity . '' We will try to do an ablation on the initialization scale ."}}