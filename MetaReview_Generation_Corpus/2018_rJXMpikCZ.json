{"year": "2018", "forum": "rJXMpikCZ", "title": "Graph Attention Networks", "decision": "Accept (Poster)", "meta_review": "The authors appear to have largely addressed the concerns of the reviewers and commenters regarding related work and experiments. The results are strong, and this will likely be a useful contribution for the graph neural network literature.", "reviews": [{"review_id": "rJXMpikCZ-0", "review_text": "This paper has proposed a new method for classifying nodes of a graph. Their method can be used in both semi-supervised scenarios where the label of some of the nodes of the same graph as the graph in training is missing (Transductive) and in the scenario that the test is on a completely new graph (Inductive). Each layer of the network consists of feature representations for all of the nodes in the Graph. A linear transformation is applied to all the features in one layer and the output of the layer is the weighted sum of the transformed neighbours (including the node). The attention logit between node i and its neighbour k is calculated by a one layer fully connected network on top of the concatenation of the transformed representation of node i and transformed representation of the neighbour k. They also can incorporate the multi-head attention mechanism and average/concatenate the output of each head. Originality: Authors improve upon GraphSAGE by replacing the aggregate and sampling function at each layer with an attention mechanism. However, the significance of the attention mechanism has not been studied in the experiments. For example by reporting the results when attention is turned off (1/|N_i| for every node) and only a 0-1 mask for neighbours is used. They have compared with GraphSAGE only on PPI dataset. I would change my rating if they show that the 33% gain is mainly due to the attention in compare to other hyper-parameters. [The experiments are now more informative. Thanks] Also, in page 4 authors claim that GraphSAGE is limited because it samples a neighbourhood of each node and doesn't aggregate over all the neighbours in order to keep its computational footprint consistent. However, the current implementation of the proposed method is computationally equal to using all the vertices in GraphSAGE. Pros: - Interesting combination of attention and local graph representation learning. - Well written paper. It conveys the idea clearly. - State-of-the-art results on three datasets. Cons: - When comparing with spectral methods it would be better to mention that the depth of embedding propagation in this method is upper-bounded by the depth of the network. Therefore, limiting its adaptability to broader class of graph datasets. - Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank you for the comprehensive review ! Please refer to our global comment above for a list of all revisions we have applied to the paper -- -we are hopeful that they have addressed your comments appropriately . Primarily , thank you for suggesting the constant-attention experiment ( with 1/|Ni| coefficients ) ! This not only directly evaluates the significance of the attention mechanism on the inductive task , but allows for a comparison with a GCN-like inductive structure . We have successfully shown a benefit of using attention : The Const-GAT model achieved 0.934 +- 0.006 micro-F1 ; The GAT model achieved 0.973 +- 0.002 micro-F1 . Which demonstrates a clear positive effect of using an attention mechanism ( given that all other architectural and training properties are kept fixed across the two models ) . These results are clearly communicated in our revised paper now ( Section 3.3 introduces the experiment in the \u201c Inductive learning \u201d paragraph , while the results are outlined in Table 3 and discussed in Section 3.4 , paragraph 4 ) . Our intention was not to imply that our method is computationally more efficient than GraphSAGE -- -only that GraphSAGE \u2019 s design decisions ( sampling subsets of neighbourhoods ) have potentially limiting effects on its predictive power . We have rewrote bullet point 4 in Section 2.2 , to hopefully communicate this better . Lastly , we make explicit that the depth of our propagation is upper-bounded by network depth in Section 2.2 , paragraph 2 . We remark that GCN-like models suffer from the same issue , and that skip connections ( or similar constructs ) may be readily used to effectively increase the depth to desirable levels . The primary benefit of leveraging attention , as opposed to prior approaches to graph-structured feature aggregation , is being able to ( implicitly ) assign different importances to different neighbours , while simultaneously generalising to a wide range of degree distributions -- -these differences are stated in our paper in various locations ( e.g.Section 1 , paragraph 8 ; Section 2.2 , bullet point 2 ) . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}, {"review_id": "rJXMpikCZ-1", "review_text": "The paper introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks. Key components are an attention layer and the possibility to learn how to weight different nodes in the neighborhood without requiring spectral decompositions which are costly to be computed. I found the paper clearly written and very well presented. I want to thank the author for actively participating in the discussions and in clarifying already many of the details that I was missing. As also reported in the comments by T. Kipf I found the lack of comparison to previous works on attention and on constructions of NN for graph data are missing. In particular MoNet seems a more general framework, using features to compute node similarity is another way to specify the \"coordinate system\" for convolution. I would argue that in many cases the graph is given and that one would have to exploit its structure rather than the simple first order neighbors structure. I feel, in fact, that the paper deals mainly with \"localized metric-learning\" rather than using the information in the graph itself. There is no explicit usage of the graph beyond the selection of the local neighborhood. In many ways when I first read it I though it would be a modified version of memory networks (which have not been cited). Sec. 2.1 is basically describing a way to learn a matrix W so that the attention layer produces the weights to be used for convolution, or the relative coordinate system, which is to me a memory network like construction, where the memory is given by the neighborhood. I find the idea to use the multi-head attention very interesting, but one should consider the increase in number of parameters in the experimental section. I agree that the proposed method is computationally efficient but the authors should keep in mind that parallelizing across all edges involves lot of redundant copies (e.g. in a distributed system) as the neighborhoods highly overlap, at least for interesting graphs. The advantage with respect to methods that try to use LSTM in this domain in a naive manner is clear, however the similarity function (attention) in this work could be interpreted as the variable dictating the visit ordering. The authors seem to emphasize the use of GPU as the best way to scale their work but I tend to think that when nodes have varying degrees they would be highly unused. Main reason why they are widely used now is due to the structure in the representation of convolutional operations. Also in case of sparse data GPUs are not the best alternative. Experiments are very well described and performed, however as explained earlier some comparisons are needed. An interesting experiment could be to use the attention weights as adjacency matrix for GCN. Overall I liked the paper and the presentation, I think it is a simple yet effective way of dealing with graph structure data. However, I think that in many interesting cases the graph structure is relevant and cannot be used just to get the neighboring nodes (e.g. in social network analysis).", "rating": "7: Good paper, accept", "reply_text": "First of all , thank you very much for your thorough review , and for the variety of useful pointers within it ! Please refer to our global comment above for a list of all revisions we have applied to the paper -- -we are hopeful that they have addressed your comments appropriately . We have now added all the references to attention-like constructions ( such as MoNet and neighbourhood attention ) to our related work , as well as memory networks ( see Section 1 , paragraphs 6 and 9 ; also Section 2.2 , bullet point 5 ) . We fully agree with your comments about the increase in parameter count with multi-head attention , computational redundancy , and comparative advantages of GPUs in this domain , and have explicitly added them as remarks to our model \u2019 s analysis ( in Section 2.2 , bullet point 1 and paragraph 2 ) . While we agree that the graph structure is given in many interesting cases , in our approach we specifically sought to produce an operator explicitly capable of solving inductive problems ( which appear often , e.g. , in the biomedical domain , where the method needs to be able to generalise to new structures ) . A potential way of reconciling this when a graph structure is provided is to combine GAT-like and spectral layers in the same architecture . Further experiments ( as discussed by us in all previous comments ) have also been performed and are now explicitly listed in the paper \u2019 s Results section ( please see Tables 2 and 3 for a summary ) . We have also attempted to use the GAT coefficients as the aggregation matrix for GCNs ( both in an averaged and multi-head manner ) -- -but found that there were no clear performance changes compared to using the Laplacian . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}, {"review_id": "rJXMpikCZ-2", "review_text": "This is a paper about learning vector representations for the nodes of a graph. These embeddings can be used in downstream tasks the most common of which is node classification. Several existing approaches have been proposed in recent years. The authors provide a fair and almost comprehensive discussion of state of the art approaches. There are a couple of exception that have already been mentioned in a comment from Thomas Kipf and Michael Bronstein. A more precise discussion of the differences between existing approaches (especially MoNets) should be a crucial addition to the paper. You provide such a comparison in your answer to Michael's comment. To me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem. The proposed method introduces two forms of (simple) attention. Nothing groundbreaking here but still interesting enough and well explained. It might also be a good idea to compare your method to something like LLE (locally linear embedding). LLE also learns a weight for each of neighbors of a node and computes the embedding as a weighted average of the neighbor embeddings according to these weights. Your approach is different since it is learned end-to-end (not in two separate steps) and because it is applicable to arbitrary graphs (not just graphs where every node has exactly k neighbors as in LLE). Still, something to relate to. Please take a look at the comment by Fabian Jansen. I think he is on to something. It seems that the attention weight (from i to j) in the end is only a normalization operation that doesn't take the embedding of node i into account. There are two issues with the experiments. First, you don't report results on Pubmed because your method didn't scale. Considering that Pubmed has less than 20,000 nodes this shows a clear weakness of your approach. You write (in an answer to a comment) that it *should* be parallelizable but somehow you didn't make it work. We have to, however, evaluate the approach on what it is able to do at the moment. Having a complexity that is quadratic in the number of nodes is terrible and one of the major reasons learning with graphs has moved from kernels to neural approaches. While it is great that you acknowledge this openly as a weakness, it is currently not possible to claim that your method scales to even moderately sized graphs. Second, the experimental set-up on the Cora and Citeseer data sets should be properly randomized. As Thomas pointed out, for graph data the variance can be quite high. For some split the method might perform really well and less well for others. In your answer titled \"Requested clarifications\" to a different comment you provide numbers randomized over 10 runs. Did you randomize the parameter initialization only or also the the train/val/test splits? If you did the latter, this seems reasonable. In Kipf et al.'s GCN paper this is what was done (not over 100 splits as some other commenter claimed. The average over 100 runs pertained to the ICA method only.) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your detailed review ! Please refer to our global comment above for a list of all revisions we have applied to the paper -- -we are hopeful that they have addressed your comments appropriately . Fabian has indeed correctly identified that half of our attention weights were spurious . We have now rectified this by applying a simple nonlinearity ( the LeakyReLU ) prior to normalising , and anticipated that its application would provide better performance to the model on the PPI dataset ( which has a large number of training nodes ) . Indeed , we noticed no discernible change on Cora and Citeseer , but an increase in F1-score on PPI ( now at 0.973 +- 0.002 after 10 runs ; previously , as given in our reply to one of the comments below , it was 0.952 +- 0.006 ) . The new results may be found in Tables 2 and 3 . In the meantime , we have been successful at leveraging TensorFlow \u2019 s sparse_softmax operation , and produced a sparsified version of the GAT layer . We are happy to provide results on Pubmed , and they are now given in the revised version of the paper ( see Table 2 for a summary ) . We were able to match state-of-the-art level performance of MoNet and GCN ( at 79.0 +- 0.3 % after 100 runs ) . Similarly to the MoNet paper authors , we had to revise the GAT architecture slightly to accommodate Pubmed \u2019 s extremely small training set size ( of 60 examples ) , and this is clearly remarked in our experimental setup ( Section 3.3 ) . Finally , quoting directly from the work of Kipf and Welling : \u201c We trained and tested our model on the same dataset splits as in Yang et al . ( 2016 ) and report mean accuracy of 100 runs with random weight initializations. \u201d This implies that the splits were not randomised in the result reported by the GCN paper ( specifically , the one used to compare with other baseline approaches ) , but only the model initialisation -- -and this is exactly what we do as well . We , in fact , use exactly the code provided by Thomas Kipf at https : //github.com/tkipf/gcn/blob/master/gcn/utils.py # L24 to load the dataset splits . We have added all the required references to MoNet and LLE ( and many other pieces of related work ) in the revised version ( Section 1 , paragraphs 6 and 9 ; also Section 2.2 , bullet point 5 ) - thank you for pointing out LLE to us , which is an interesting and relevant piece of related work ! We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}], "0": {"review_id": "rJXMpikCZ-0", "review_text": "This paper has proposed a new method for classifying nodes of a graph. Their method can be used in both semi-supervised scenarios where the label of some of the nodes of the same graph as the graph in training is missing (Transductive) and in the scenario that the test is on a completely new graph (Inductive). Each layer of the network consists of feature representations for all of the nodes in the Graph. A linear transformation is applied to all the features in one layer and the output of the layer is the weighted sum of the transformed neighbours (including the node). The attention logit between node i and its neighbour k is calculated by a one layer fully connected network on top of the concatenation of the transformed representation of node i and transformed representation of the neighbour k. They also can incorporate the multi-head attention mechanism and average/concatenate the output of each head. Originality: Authors improve upon GraphSAGE by replacing the aggregate and sampling function at each layer with an attention mechanism. However, the significance of the attention mechanism has not been studied in the experiments. For example by reporting the results when attention is turned off (1/|N_i| for every node) and only a 0-1 mask for neighbours is used. They have compared with GraphSAGE only on PPI dataset. I would change my rating if they show that the 33% gain is mainly due to the attention in compare to other hyper-parameters. [The experiments are now more informative. Thanks] Also, in page 4 authors claim that GraphSAGE is limited because it samples a neighbourhood of each node and doesn't aggregate over all the neighbours in order to keep its computational footprint consistent. However, the current implementation of the proposed method is computationally equal to using all the vertices in GraphSAGE. Pros: - Interesting combination of attention and local graph representation learning. - Well written paper. It conveys the idea clearly. - State-of-the-art results on three datasets. Cons: - When comparing with spectral methods it would be better to mention that the depth of embedding propagation in this method is upper-bounded by the depth of the network. Therefore, limiting its adaptability to broader class of graph datasets. - Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank you for the comprehensive review ! Please refer to our global comment above for a list of all revisions we have applied to the paper -- -we are hopeful that they have addressed your comments appropriately . Primarily , thank you for suggesting the constant-attention experiment ( with 1/|Ni| coefficients ) ! This not only directly evaluates the significance of the attention mechanism on the inductive task , but allows for a comparison with a GCN-like inductive structure . We have successfully shown a benefit of using attention : The Const-GAT model achieved 0.934 +- 0.006 micro-F1 ; The GAT model achieved 0.973 +- 0.002 micro-F1 . Which demonstrates a clear positive effect of using an attention mechanism ( given that all other architectural and training properties are kept fixed across the two models ) . These results are clearly communicated in our revised paper now ( Section 3.3 introduces the experiment in the \u201c Inductive learning \u201d paragraph , while the results are outlined in Table 3 and discussed in Section 3.4 , paragraph 4 ) . Our intention was not to imply that our method is computationally more efficient than GraphSAGE -- -only that GraphSAGE \u2019 s design decisions ( sampling subsets of neighbourhoods ) have potentially limiting effects on its predictive power . We have rewrote bullet point 4 in Section 2.2 , to hopefully communicate this better . Lastly , we make explicit that the depth of our propagation is upper-bounded by network depth in Section 2.2 , paragraph 2 . We remark that GCN-like models suffer from the same issue , and that skip connections ( or similar constructs ) may be readily used to effectively increase the depth to desirable levels . The primary benefit of leveraging attention , as opposed to prior approaches to graph-structured feature aggregation , is being able to ( implicitly ) assign different importances to different neighbours , while simultaneously generalising to a wide range of degree distributions -- -these differences are stated in our paper in various locations ( e.g.Section 1 , paragraph 8 ; Section 2.2 , bullet point 2 ) . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}, "1": {"review_id": "rJXMpikCZ-1", "review_text": "The paper introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks. Key components are an attention layer and the possibility to learn how to weight different nodes in the neighborhood without requiring spectral decompositions which are costly to be computed. I found the paper clearly written and very well presented. I want to thank the author for actively participating in the discussions and in clarifying already many of the details that I was missing. As also reported in the comments by T. Kipf I found the lack of comparison to previous works on attention and on constructions of NN for graph data are missing. In particular MoNet seems a more general framework, using features to compute node similarity is another way to specify the \"coordinate system\" for convolution. I would argue that in many cases the graph is given and that one would have to exploit its structure rather than the simple first order neighbors structure. I feel, in fact, that the paper deals mainly with \"localized metric-learning\" rather than using the information in the graph itself. There is no explicit usage of the graph beyond the selection of the local neighborhood. In many ways when I first read it I though it would be a modified version of memory networks (which have not been cited). Sec. 2.1 is basically describing a way to learn a matrix W so that the attention layer produces the weights to be used for convolution, or the relative coordinate system, which is to me a memory network like construction, where the memory is given by the neighborhood. I find the idea to use the multi-head attention very interesting, but one should consider the increase in number of parameters in the experimental section. I agree that the proposed method is computationally efficient but the authors should keep in mind that parallelizing across all edges involves lot of redundant copies (e.g. in a distributed system) as the neighborhoods highly overlap, at least for interesting graphs. The advantage with respect to methods that try to use LSTM in this domain in a naive manner is clear, however the similarity function (attention) in this work could be interpreted as the variable dictating the visit ordering. The authors seem to emphasize the use of GPU as the best way to scale their work but I tend to think that when nodes have varying degrees they would be highly unused. Main reason why they are widely used now is due to the structure in the representation of convolutional operations. Also in case of sparse data GPUs are not the best alternative. Experiments are very well described and performed, however as explained earlier some comparisons are needed. An interesting experiment could be to use the attention weights as adjacency matrix for GCN. Overall I liked the paper and the presentation, I think it is a simple yet effective way of dealing with graph structure data. However, I think that in many interesting cases the graph structure is relevant and cannot be used just to get the neighboring nodes (e.g. in social network analysis).", "rating": "7: Good paper, accept", "reply_text": "First of all , thank you very much for your thorough review , and for the variety of useful pointers within it ! Please refer to our global comment above for a list of all revisions we have applied to the paper -- -we are hopeful that they have addressed your comments appropriately . We have now added all the references to attention-like constructions ( such as MoNet and neighbourhood attention ) to our related work , as well as memory networks ( see Section 1 , paragraphs 6 and 9 ; also Section 2.2 , bullet point 5 ) . We fully agree with your comments about the increase in parameter count with multi-head attention , computational redundancy , and comparative advantages of GPUs in this domain , and have explicitly added them as remarks to our model \u2019 s analysis ( in Section 2.2 , bullet point 1 and paragraph 2 ) . While we agree that the graph structure is given in many interesting cases , in our approach we specifically sought to produce an operator explicitly capable of solving inductive problems ( which appear often , e.g. , in the biomedical domain , where the method needs to be able to generalise to new structures ) . A potential way of reconciling this when a graph structure is provided is to combine GAT-like and spectral layers in the same architecture . Further experiments ( as discussed by us in all previous comments ) have also been performed and are now explicitly listed in the paper \u2019 s Results section ( please see Tables 2 and 3 for a summary ) . We have also attempted to use the GAT coefficients as the aggregation matrix for GCNs ( both in an averaged and multi-head manner ) -- -but found that there were no clear performance changes compared to using the Laplacian . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}, "2": {"review_id": "rJXMpikCZ-2", "review_text": "This is a paper about learning vector representations for the nodes of a graph. These embeddings can be used in downstream tasks the most common of which is node classification. Several existing approaches have been proposed in recent years. The authors provide a fair and almost comprehensive discussion of state of the art approaches. There are a couple of exception that have already been mentioned in a comment from Thomas Kipf and Michael Bronstein. A more precise discussion of the differences between existing approaches (especially MoNets) should be a crucial addition to the paper. You provide such a comparison in your answer to Michael's comment. To me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem. The proposed method introduces two forms of (simple) attention. Nothing groundbreaking here but still interesting enough and well explained. It might also be a good idea to compare your method to something like LLE (locally linear embedding). LLE also learns a weight for each of neighbors of a node and computes the embedding as a weighted average of the neighbor embeddings according to these weights. Your approach is different since it is learned end-to-end (not in two separate steps) and because it is applicable to arbitrary graphs (not just graphs where every node has exactly k neighbors as in LLE). Still, something to relate to. Please take a look at the comment by Fabian Jansen. I think he is on to something. It seems that the attention weight (from i to j) in the end is only a normalization operation that doesn't take the embedding of node i into account. There are two issues with the experiments. First, you don't report results on Pubmed because your method didn't scale. Considering that Pubmed has less than 20,000 nodes this shows a clear weakness of your approach. You write (in an answer to a comment) that it *should* be parallelizable but somehow you didn't make it work. We have to, however, evaluate the approach on what it is able to do at the moment. Having a complexity that is quadratic in the number of nodes is terrible and one of the major reasons learning with graphs has moved from kernels to neural approaches. While it is great that you acknowledge this openly as a weakness, it is currently not possible to claim that your method scales to even moderately sized graphs. Second, the experimental set-up on the Cora and Citeseer data sets should be properly randomized. As Thomas pointed out, for graph data the variance can be quite high. For some split the method might perform really well and less well for others. In your answer titled \"Requested clarifications\" to a different comment you provide numbers randomized over 10 runs. Did you randomize the parameter initialization only or also the the train/val/test splits? If you did the latter, this seems reasonable. In Kipf et al.'s GCN paper this is what was done (not over 100 splits as some other commenter claimed. The average over 100 runs pertained to the ICA method only.) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your detailed review ! Please refer to our global comment above for a list of all revisions we have applied to the paper -- -we are hopeful that they have addressed your comments appropriately . Fabian has indeed correctly identified that half of our attention weights were spurious . We have now rectified this by applying a simple nonlinearity ( the LeakyReLU ) prior to normalising , and anticipated that its application would provide better performance to the model on the PPI dataset ( which has a large number of training nodes ) . Indeed , we noticed no discernible change on Cora and Citeseer , but an increase in F1-score on PPI ( now at 0.973 +- 0.002 after 10 runs ; previously , as given in our reply to one of the comments below , it was 0.952 +- 0.006 ) . The new results may be found in Tables 2 and 3 . In the meantime , we have been successful at leveraging TensorFlow \u2019 s sparse_softmax operation , and produced a sparsified version of the GAT layer . We are happy to provide results on Pubmed , and they are now given in the revised version of the paper ( see Table 2 for a summary ) . We were able to match state-of-the-art level performance of MoNet and GCN ( at 79.0 +- 0.3 % after 100 runs ) . Similarly to the MoNet paper authors , we had to revise the GAT architecture slightly to accommodate Pubmed \u2019 s extremely small training set size ( of 60 examples ) , and this is clearly remarked in our experimental setup ( Section 3.3 ) . Finally , quoting directly from the work of Kipf and Welling : \u201c We trained and tested our model on the same dataset splits as in Yang et al . ( 2016 ) and report mean accuracy of 100 runs with random weight initializations. \u201d This implies that the splits were not randomised in the result reported by the GCN paper ( specifically , the one used to compare with other baseline approaches ) , but only the model initialisation -- -and this is exactly what we do as well . We , in fact , use exactly the code provided by Thomas Kipf at https : //github.com/tkipf/gcn/blob/master/gcn/utils.py # L24 to load the dataset splits . We have added all the required references to MoNet and LLE ( and many other pieces of related work ) in the revised version ( Section 1 , paragraphs 6 and 9 ; also Section 2.2 , bullet point 5 ) - thank you for pointing out LLE to us , which is an interesting and relevant piece of related work ! We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}}