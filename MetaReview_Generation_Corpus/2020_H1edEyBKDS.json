{"year": "2020", "forum": "H1edEyBKDS", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "decision": "Accept (Poster)", "meta_review": "This paper proposes a simple plug-and-play language model approach to the problem of controlled language generation. The problem is important and timely, and the approach is simple yet effective. Reviewers had some discussions whether  1) there is enough novelty, 2) evaluation task really shows effectiveness, and 3) this paper will inspire future research directions. \n\nAfter discussions of the above points, reviewers are leaning more positive, and I reflect their positive sentiment by recommending it to be accepted. I look forward to seeing this work presented at ICLR.", "reviews": [{"review_id": "H1edEyBKDS-0", "review_text": "The authors describe a method for training plug and play language models, a way to incorporate control elements into pre-trained LMs. In contrast to existing work, which often trains conditioned upon the control element, the authors emphasize that their method does not require re-training the initial LM. This is exciting and a great research direction. It is evaluated in a number of different settings. 1. The authors claim that this method is a baseline for controlled text generation (see e.g. the title). However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I don't see how this can be proposed as a baseline for controlled text generation is there is no comparison to other methods. I imagine the authors will emphasize that that's not fair - because their method doesn't require retraining the language model - but it is relevant to demonstrate if there is a gap in performance or not. As is, there is only one baseline- unconditional language model - and to me this is mostly a way to calibrate the evaluators and not a way to compare their model against other models. 2. Can the authors make a point or discuss the relationship of this work to neural style transfer? Compared to unsupervised style transfer approaches, which also use lists of words or attributes to learn to dis-entangle content and style, what are the benefits of the proposed approach and how would it compare? 3. Can the authors discuss the effectiveness of their control mechanism for less logical control settings? For example, what if there was \"religion\" for \"the potato\" prompt? Does the model still respect these settings, or no? 4. Can the authors add analysis on how much the model respects the control variables? This is quite common in existing controlled generation papers. If the model is updated to have the control variables and then is not provided with one at test time, what happens? Can you also control very easy to measure attributes, such as length? This question ties in with a general point I am ambivalent to in this paper- that it is very long, but there is very little analysis done on what makes the method work, why it is better than other control methods or control baselines, where the proposed control mechanism is not effective, how the model scales if there are large quantities of topics rather than just a few of them, if the BoW and discriminator attribute models work well together or if certain attributes are easier to learn than others, so the model focuses more on those when there are conflicts, etc 5. Missing citations: Previous work has investigated controlling various attributes of text generation. Several of these works have also controlled multiple attributes simultaneously. For example, here's a list of a few of the works that were missed: Kikuchi et al 2016 Ficler and Goldberg, 2017 Wang et al, 2017 Fan et al, 2018 Baheti et al, 2018 See et al, 2019 Martin et al, 2019 The related work section only focuses on very recent work, e.g. only one paper is discussed amongst a large body of existing work. I feel this is not an accurate reflection of how much previous work has investigated these techniques and analyzed how models deal with control variables. Please also cite: - which dataset was used for story generation, appears to be missing - top-k sampling I have read the author response. Thanks for the details and additional analysis in the paper. ", "rating": "6: Weak Accept", "reply_text": "> > In contrast to existing work , which often trains conditioned upon the control element , the authors emphasize that their method does not require re-training the initial LM . This is exciting and a great research direction . We are glad you like the research direction ! > > 1.The authors \u2026 . evaluation with any existing work that performs controlled text generation . Thank you for the suggestions . We have included the following baselines : i ) Weighted Decoding [ 1 ] , ii ) CTRL ( a conditional language model trained for controlled text generation ) , and iii ) a fine-tuned GPT-2 language model . Despite , CTRL being trained for the task ( and with over 4 times as many parameters ) and GPT-2 being fine-tuned for the task ( and with over twice as many parameters ) , we perform comparably with CTRL and outperform the fine-tuned GPT-2 based on human-evaluation/automated evaluation . We also clearly outperform the more direct approach of weighted decoding proposed [ 1 ] ( also , used in [ 3 ] ) . See Tables 4 , 5 for updated results and Section S7 for baseline details . > > 2.Can\u2026 discuss the relationship of this work to neural style transfer ? Compared to unsupervised style transfer approaches \u2026 what are the benefits of the proposed approach and how would it compare ? We have moved the discussion of neural style transfer from supplementary information section to Section 2 Related Work . Thanks for your suggestion . Benefits of PPLM over style transfer : -- Most style transfer approaches [ 2 ] require training a seq2seq model from scratch and it is also not possible to plug in new attributes that were not considered during training . -- Further , there are many domains outside style transfer where it is useful to control style -- for example , story writing [ 6 ] , dialogue systems [ 3 ] , where approaches from current work on style transfer are not directly applicable . We believe PPLM would be directly applicable to any transformer based generative model in all these domains . -- In contrast to current approaches for unsupervised neural style transfer [ 2 , 9 ] , our approach allows for fine-grained control ( e.g.How positive do we want our LM to be ? ) . -- We also note that controlled/stylized generation itself is a well studied problem [ 4 , 5 , 6 , 7 , 8 ] , and there are merits to generating text in a controlled manner outside of the style transfer setting . > > 3.Can the authors discuss the effectiveness of their control mechanism for less logical control settings ? .. `` religion '' for `` the potato '' prompt ? .. still respect these settings , or no ? This is a great idea ! We \u2019 ve added examples of how PPLM responds to the following odd or illogical topic-prefix combinations . The experiment is described in Section S9 and we list samples from various combinations in Tables S10-S16 . The conclusion of this experiment is that PPLM can handle those odd settings as well . For example , a sample from \u201c The potato \u201d + \u201c Religion \u201d is as follows : === Sample 1 ==== The potato , an ancient food , is considered a sacred plant by many Hindus . However , some Hindus believe that the potatoes are the seed of a demon . ... `` In India we have the Hindu god Vishnu , Vish , the God . He has come to the world , '' said a woman in Mumbai . `` He came to the world because of God . God came to the world to save people from the curse of the devil God . God came to save us from the curse of the devil , '' === end of Sample 1 ==== === Sample 2 ==== The potato salad that I have recently been making for our family is so good , I wanted to share it with you guys . This was my first attempt at a Potato Salad recipe , and I love it . It also reminds me why I love cooking . I love how good it tastes and how it reminds you why you love Cooking with God . I love how it is a great way to celebrate Thanksgiving and Christmas . It also reminds me why I am a Christian . I love how it reminds me why I love to === end of Sample 2 ===="}, {"review_id": "H1edEyBKDS-1", "review_text": "The paper proposes a Plug and Play LM model for controlled natural language generation. Similar to the idea of the Plug and Play Generative Networks for vision, the model plugs in a discriminator, which is either a bag-of-words model or a single layer classifier. The added simple discriminator is then coupled with a pre-trained generative language model such as GPT-2, to obtain a conditional probability for generating controllable text. The authors evaluate the proposed model using human evaluation studies and quantitative perplexity metrics, aiming at measuring the relevance and fluency of the generated text. Their experimental results show that the text generated is fluent and aligned with the desired attributes. The proposed method is simple and makes sense to me. The idea of how one can make good use of large, pre-trained generative language models is very neat here. However, I have two main concerns, as follows. 1. The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes. In this sense, how useful these kinds of text generation techniques are not clear to me. For example, the first two rows in Table 3 contain two paragraphs with very different main ideas to be conveyed. Similarly for sentences in Table 1. It seems that those sentences talk about very different topics/things to me, although they may reflect the desired control attributes. Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text? 2. The model is a straightforward adaption of the Plug and Play Generative Networks from the vision community. In short, the idea in the paper is simple and seems effective. On the other hand, the lack of a good evaluation metric makes me a bit uncertain about the contribution of the paper. I am willing to increase my evaluation score if I will be convinced by other reviews and comments. ", "rating": "3: Weak Reject", "reply_text": "> > The proposed method is simple and makes sense to me ... is very neat here . However , I have two main concerns , as follows . We thank you for your comments helping us improve the paper . We address your comments below . > > `` 1.The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes . In this sense , how useful these kinds of text generation techniques are not clear to me . .... Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text ? '' This is certainly the case . In our work , two samples from either an LM distribution p ( x ) or a controlled LM p ( x|a ) are independent . The task being studied is controlled generation as opposed to style transfer , the latter scenario in which one aims to retain content but adjust style . Our goal is not to control the language so that the idea being conveyed is retained . Although it would be great if our model could accomplish both feats , we would like to note that controlled generation on its own is an actively studied problem in the language community . Recently several approaches have been proposed towards solving the problem of open-ended controlled generation where the goal is to only generate language with specific attributes without controlling for context , for example , the following papers : [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] . Another paper , [ 6 ] , showed the benefits of language control ( without directly controlling the idea being conveyed ) on human judgement of the quality of engagement during interaction with a dialogue agent . We also note that the PPGN model in the paper inspiring this work does not control for deviation in context , but rather only controls for the generated image having the desired attribute ( i.e.PPGN and PPLM both perform \u201c controlled generation \u201d but not \u201c style transfer \u201d ) . For the open-ended controlled generation task ( such as studied in [ 1,2,3,4 ] ) , we consider several possible automatic and human evaluation metrics , including perplexity , dist scores , human fluency and attribute relevance scores . If you have any suggestions for other automatic evaluation metrics , we would be happy to consider including them . > > `` 2.The model is a straightforward adaptation of the Plug and Play Generative Networks from the vision community . '' We respectfully disagree that the adaptation was straightforward . While we would have been happy to apply the PPGN approach directly to the language domain , the adaptation actually required several modifications , summarized as follows : PPGN : -- A graphical model depiction of the network looks like this : h - > x - > y , where h is a latent code , x is an image , and y is a class or attribute . -- A single h generates an entire , single image x. h and x are both continuous , and the gradient w.r.t.y passes through x to h. -- The Markov chain is run in h space , with a separate p ( h ) model being trained and used to ensure h does not drift too far from high probability regions . -- Multiple steps are taken in h space , corresponding to multiple entire images . -- Noise is added in h space to obtain the correct diversity of images . PPLM : -- A graphical model depiction of the network looks like this : [ x1 - > ( h1 , x2 ) - > ( h2 , x3 ) , \u2026 ] - > y , h_t and x_t are the latents and byte-pairs at time t and y is an attribute . -- A single h generates a distribution over sentences x. h is continuous and x is discrete , and gradient w.r.t.y passes directly to h , with discrete x skipped , except in the distribution propagation approach in Sec 4.3 , which propagates through the single word x_t+1 ( \u201c Instead , as in -- Dai et al . ( 2019a ) , we use the distribution\u2026 \u201d ) . -- A complete Markov chain is not run , as this would require multiple full forward and backward passes through the transformer . Instead , we update only a sliding window of the recent past of h and sample only one word at a time . This is a compromise between speed and quality of the samples . The particular dependency structure of the transformer allows us to update only the past ( key , value ) pairs , which also allows for efficient sampling . -- Multiple steps are taken in h space as the sentence is constructed word by word . Multiple entire sentences are never produced . -- Noise is added via the sampling of each word in x space to obtain the correct diversity of sentences . [ 1 ] CTRL : A Conditional Transformer Language Model for Controllable Generation , Keskar et al. , https : //arxiv.org/abs/1909.05858 [ 2 ] Fine-Tuning Language Models from Human Preferences , Ziegler et al. , https : //arxiv.org/abs/1909.08593 [ 3 ] Towards controlled text generation , Hu et al. , https : //arxiv.org/abs/1703.00955 [ 4 ] Controlling Linguistic Style Aspects in Neural Language Generation , Ficler et al. , https : //arxiv.org/abs/1707.02633 [ 5 ] Towards Controllable Story Generation , Peng et al . [ 6 ] What makes a good conversation ? How controllable attributes affect human judgments , See et al. , NAACL \u2019 19"}, {"review_id": "H1edEyBKDS-2", "review_text": "The paper introduces an approach to the conditional generation of text, relying on pre-trained decoders, without fine-tuning and, in certain cases, without any training at all. The approach they introduce is following the framework known in NLP as noisy-channel modeling, previously standard in machine translation (in its SMT days), but undergoing certain revival recently (https://arxiv.org/abs/1611.02554, https://arxiv.org/abs/1910.00553,https://arxiv.org/abs/1908.05731,https://arxiv.org/abs/1907.06616). The authors do not mention this connection (they should!). Very differently from these previous approaches attempting to integrate the two factors in the search process (e.g., using reranking), the authors instead rely on gradient descent in the latent space of their model (Transformer), similarly to plug-n-play generative networks in image generation. I find this approach interesting and like the paper overall. However, I do not see why authors do not compare to more direct ways of integrating the conditional component into the model. This would have been tricky in the NMT papers mentioned above, as the entire source sentences need to be reconstructred, however, it should be quite straightforward in this work, with conditioning on single categorical control variables (or maybe a couple in the additional experiments in sect 4.4). Especially, given that the authors already make the predictions of the control variable independently per prediction (e.g., see eq. (5) in section 4.2) / greedily per prefix (bottom lines, page 7). I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. E.g., for the experiments defining topics as sets of seed words (section 4.2), when integrating factors directly (unlike the proposed approach, Table 3), there will be no increase in the probability of generating relevant words before the first seed word is generated. Another limitation is the lack of comparison to standard controlled generation work, i.e. those requiring training a model or/and fine-tuning pretrained decoder. I understand that the proposed approach falls in a different category and, of course, do not expect it to beat a fine-tuned model, but I'd like to get some feel for how much one loses by using this simpler method. There has been a lot of work on controlled generation in recent ~3 years, and they can also be combined with intializing and fine-tuning off-the-shelf pretrained decoders. There is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944 They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of 'steerability' rather than building a controlled-generation model. Given that style-controlled but otherwise unconditional generation may not have that many applications, I am curious how far you can push this approach. E.g., can you make it scale to more complicated data-to-text generation tasks (https://www.aclweb.org/anthology/D17-1239/)? Or, will the only application in this context be integrating new conditioning variables into pretrained conditional LMs? Minor: I am confused with the notation in \"Post-norm Geometric Mean Fusion\" section. It says that softmax is applied to the product of probabilities. Maybe to a linear interpolation of log-probs? Or maybe that's not softmax at all? Something seems off here. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments helping us improve the paper ! > > \u201c following the framework known in NLP as noisy-channel modeling \u2026 . this connection ( they should ! ) . \u201d Thanks for pointing out references to noisy-channel modeling . We have added a mention of the noisy channel modeling approach to our related work section and have discussed how that approach compares to PPLM ( Section 2 ) . > > \u201c I find this approach interesting and like the paper overall. \u201d Thanks ! > > \u201c However \u2026 do not compare to more direct ways of integrating the conditional ... expect the proposed approach to work better ( or at least differently ) but it would be interesting to see it confirmed . .... will be no increase in the probability of generating relevant words before the first seed word is generated. \u201d Thank you for these great suggestions . We \u2019 ve updated the paper to include this approach both for PPLM-BoW and PPLM-Discrim models : For PPLM-BoW : this corresponds to an existing approach referred to in literature as \u201c Weighted Decoding \u201d ( https : //www.aclweb.org/anthology/P17-4008/ , See et al. , NAACL \u2019 19 ) . For PPLM-Discrim : for each token in the vocabulary we compute p ( y=desired sentiment | x ) and sample from the distribution p ( x ) * p ( y=sentiment|x ) with top-k=5 . While the forward passes over the vocabulary are extremely expensive ( e.g.50000x ) , we get a sense of how well PPLM compares with a direct integration of the condition . We have included both sets of results in the paper ( Table 3 and Table 6 , row \u201c WD \u201d ) , where we find , as you presumed , that it does not work quite as well as PPLM . PPLM works better than directly integrating the conditioning into the decoding procedure . For the bag of words , we can further confirm the observation that the probability of generating relevant words before the first seed word from the bag does not increase . Another key difference is that the semantics of the bag of words are not captured , rather the decoder chooses to pick one of the words that fits context . For instance , a generated sample when conditioned on the prefix \u201c Once upon a time \u201d with the \u201c Space \u201d bag of words is \u201c I used to have a pretty good idea what a starfish was . I was a starfish biologist. \u201d . See Section 4.2 for details . For the sentiment control task , we find that this results in a lot of adversarial samples . Sequences often have a high attribute likelihood under the discriminator used during decoding but do not possess the attribute under human evaluation/external classifier evaluation . > > \u201c Another limitation is the lack of comparison to standard controlled generation .... fine-tuning off-the-shelf pretrained decoders. \u201d Thanks for the suggestion ! We have updated the paper to include comparisons with a recent conditional LM ( CTRL , https : //arxiv.org/abs/1909.05858 ) and a GPT-2 LM fine-tuned for positivity with RL and human preferences ( https : //arxiv.org/abs/1909.08593 ) . The details of the set-up can be found in the Section S7 , particuarly , S7.1 and S7.2 . We find PPLM performs comparably with CTRL on sentiment control ( Table 6 ) and ( perhaps surprisingly ) outperforms CTRL on topic control ( Table 3 ) . PPLM also significantly outperforms the fine-tuned GPT-2 model on the sentiment task . In all of the above cases , PPLM is at least as fluent or more fluent than the baselines ( CTRL & fine-tuned GPT-2 ) . This is impressive considering that the fine-tuned GPT-2 model has over twice as many parameters , and the CTRL conditional language model has over 4 times as many parameters and are specifically tuned/trained for controlled gen. > > \u201d There .. interesting relation to the NIPS 2019 paper \u2026 'steerability ' rather ... controlled-generation model. \u201d Thanks for the interesting connection ! We have included this ( Section 2 , Page 3 ) . > > \u201c Given that style-controlled \u2026 can push this approach ... pretrained conditional LMs ? \u201d We believe the PPLM approach should scale well to any method with a Transformer based decoder , including potentially the application you describe , or NMT or Dialogue systems , where See et al. \u2019 19 showed the utility of being able to control the response in dialogue systems . Just as PPLM allows one to combine a p ( x ) model and p ( a|x ) model to generate samples from p ( x|a ) , in the NMT scenario one could combine a base translation model p ( x_target | x_source ) with a p ( a | x_target , x_source ) to generate samples from p ( x_target | a , x_source ) . E.g.this could allow transforming a German to English translation model and a Twitter vs. Wikipedia classifier to translate German phrases into their English Twitter equivalents ! These are some of the immediate next steps we plan on exploring ! > > \u201d Minor : ... Something seems off here . Indeed this was an error -- we had posted a comment mentioning a correction on openreview . Thanks for pointing out ; we have fixed this in the revision now ."}], "0": {"review_id": "H1edEyBKDS-0", "review_text": "The authors describe a method for training plug and play language models, a way to incorporate control elements into pre-trained LMs. In contrast to existing work, which often trains conditioned upon the control element, the authors emphasize that their method does not require re-training the initial LM. This is exciting and a great research direction. It is evaluated in a number of different settings. 1. The authors claim that this method is a baseline for controlled text generation (see e.g. the title). However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I don't see how this can be proposed as a baseline for controlled text generation is there is no comparison to other methods. I imagine the authors will emphasize that that's not fair - because their method doesn't require retraining the language model - but it is relevant to demonstrate if there is a gap in performance or not. As is, there is only one baseline- unconditional language model - and to me this is mostly a way to calibrate the evaluators and not a way to compare their model against other models. 2. Can the authors make a point or discuss the relationship of this work to neural style transfer? Compared to unsupervised style transfer approaches, which also use lists of words or attributes to learn to dis-entangle content and style, what are the benefits of the proposed approach and how would it compare? 3. Can the authors discuss the effectiveness of their control mechanism for less logical control settings? For example, what if there was \"religion\" for \"the potato\" prompt? Does the model still respect these settings, or no? 4. Can the authors add analysis on how much the model respects the control variables? This is quite common in existing controlled generation papers. If the model is updated to have the control variables and then is not provided with one at test time, what happens? Can you also control very easy to measure attributes, such as length? This question ties in with a general point I am ambivalent to in this paper- that it is very long, but there is very little analysis done on what makes the method work, why it is better than other control methods or control baselines, where the proposed control mechanism is not effective, how the model scales if there are large quantities of topics rather than just a few of them, if the BoW and discriminator attribute models work well together or if certain attributes are easier to learn than others, so the model focuses more on those when there are conflicts, etc 5. Missing citations: Previous work has investigated controlling various attributes of text generation. Several of these works have also controlled multiple attributes simultaneously. For example, here's a list of a few of the works that were missed: Kikuchi et al 2016 Ficler and Goldberg, 2017 Wang et al, 2017 Fan et al, 2018 Baheti et al, 2018 See et al, 2019 Martin et al, 2019 The related work section only focuses on very recent work, e.g. only one paper is discussed amongst a large body of existing work. I feel this is not an accurate reflection of how much previous work has investigated these techniques and analyzed how models deal with control variables. Please also cite: - which dataset was used for story generation, appears to be missing - top-k sampling I have read the author response. Thanks for the details and additional analysis in the paper. ", "rating": "6: Weak Accept", "reply_text": "> > In contrast to existing work , which often trains conditioned upon the control element , the authors emphasize that their method does not require re-training the initial LM . This is exciting and a great research direction . We are glad you like the research direction ! > > 1.The authors \u2026 . evaluation with any existing work that performs controlled text generation . Thank you for the suggestions . We have included the following baselines : i ) Weighted Decoding [ 1 ] , ii ) CTRL ( a conditional language model trained for controlled text generation ) , and iii ) a fine-tuned GPT-2 language model . Despite , CTRL being trained for the task ( and with over 4 times as many parameters ) and GPT-2 being fine-tuned for the task ( and with over twice as many parameters ) , we perform comparably with CTRL and outperform the fine-tuned GPT-2 based on human-evaluation/automated evaluation . We also clearly outperform the more direct approach of weighted decoding proposed [ 1 ] ( also , used in [ 3 ] ) . See Tables 4 , 5 for updated results and Section S7 for baseline details . > > 2.Can\u2026 discuss the relationship of this work to neural style transfer ? Compared to unsupervised style transfer approaches \u2026 what are the benefits of the proposed approach and how would it compare ? We have moved the discussion of neural style transfer from supplementary information section to Section 2 Related Work . Thanks for your suggestion . Benefits of PPLM over style transfer : -- Most style transfer approaches [ 2 ] require training a seq2seq model from scratch and it is also not possible to plug in new attributes that were not considered during training . -- Further , there are many domains outside style transfer where it is useful to control style -- for example , story writing [ 6 ] , dialogue systems [ 3 ] , where approaches from current work on style transfer are not directly applicable . We believe PPLM would be directly applicable to any transformer based generative model in all these domains . -- In contrast to current approaches for unsupervised neural style transfer [ 2 , 9 ] , our approach allows for fine-grained control ( e.g.How positive do we want our LM to be ? ) . -- We also note that controlled/stylized generation itself is a well studied problem [ 4 , 5 , 6 , 7 , 8 ] , and there are merits to generating text in a controlled manner outside of the style transfer setting . > > 3.Can the authors discuss the effectiveness of their control mechanism for less logical control settings ? .. `` religion '' for `` the potato '' prompt ? .. still respect these settings , or no ? This is a great idea ! We \u2019 ve added examples of how PPLM responds to the following odd or illogical topic-prefix combinations . The experiment is described in Section S9 and we list samples from various combinations in Tables S10-S16 . The conclusion of this experiment is that PPLM can handle those odd settings as well . For example , a sample from \u201c The potato \u201d + \u201c Religion \u201d is as follows : === Sample 1 ==== The potato , an ancient food , is considered a sacred plant by many Hindus . However , some Hindus believe that the potatoes are the seed of a demon . ... `` In India we have the Hindu god Vishnu , Vish , the God . He has come to the world , '' said a woman in Mumbai . `` He came to the world because of God . God came to the world to save people from the curse of the devil God . God came to save us from the curse of the devil , '' === end of Sample 1 ==== === Sample 2 ==== The potato salad that I have recently been making for our family is so good , I wanted to share it with you guys . This was my first attempt at a Potato Salad recipe , and I love it . It also reminds me why I love cooking . I love how good it tastes and how it reminds you why you love Cooking with God . I love how it is a great way to celebrate Thanksgiving and Christmas . It also reminds me why I am a Christian . I love how it reminds me why I love to === end of Sample 2 ===="}, "1": {"review_id": "H1edEyBKDS-1", "review_text": "The paper proposes a Plug and Play LM model for controlled natural language generation. Similar to the idea of the Plug and Play Generative Networks for vision, the model plugs in a discriminator, which is either a bag-of-words model or a single layer classifier. The added simple discriminator is then coupled with a pre-trained generative language model such as GPT-2, to obtain a conditional probability for generating controllable text. The authors evaluate the proposed model using human evaluation studies and quantitative perplexity metrics, aiming at measuring the relevance and fluency of the generated text. Their experimental results show that the text generated is fluent and aligned with the desired attributes. The proposed method is simple and makes sense to me. The idea of how one can make good use of large, pre-trained generative language models is very neat here. However, I have two main concerns, as follows. 1. The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes. In this sense, how useful these kinds of text generation techniques are not clear to me. For example, the first two rows in Table 3 contain two paragraphs with very different main ideas to be conveyed. Similarly for sentences in Table 1. It seems that those sentences talk about very different topics/things to me, although they may reflect the desired control attributes. Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text? 2. The model is a straightforward adaption of the Plug and Play Generative Networks from the vision community. In short, the idea in the paper is simple and seems effective. On the other hand, the lack of a good evaluation metric makes me a bit uncertain about the contribution of the paper. I am willing to increase my evaluation score if I will be convinced by other reviews and comments. ", "rating": "3: Weak Reject", "reply_text": "> > The proposed method is simple and makes sense to me ... is very neat here . However , I have two main concerns , as follows . We thank you for your comments helping us improve the paper . We address your comments below . > > `` 1.The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes . In this sense , how useful these kinds of text generation techniques are not clear to me . .... Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text ? '' This is certainly the case . In our work , two samples from either an LM distribution p ( x ) or a controlled LM p ( x|a ) are independent . The task being studied is controlled generation as opposed to style transfer , the latter scenario in which one aims to retain content but adjust style . Our goal is not to control the language so that the idea being conveyed is retained . Although it would be great if our model could accomplish both feats , we would like to note that controlled generation on its own is an actively studied problem in the language community . Recently several approaches have been proposed towards solving the problem of open-ended controlled generation where the goal is to only generate language with specific attributes without controlling for context , for example , the following papers : [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] . Another paper , [ 6 ] , showed the benefits of language control ( without directly controlling the idea being conveyed ) on human judgement of the quality of engagement during interaction with a dialogue agent . We also note that the PPGN model in the paper inspiring this work does not control for deviation in context , but rather only controls for the generated image having the desired attribute ( i.e.PPGN and PPLM both perform \u201c controlled generation \u201d but not \u201c style transfer \u201d ) . For the open-ended controlled generation task ( such as studied in [ 1,2,3,4 ] ) , we consider several possible automatic and human evaluation metrics , including perplexity , dist scores , human fluency and attribute relevance scores . If you have any suggestions for other automatic evaluation metrics , we would be happy to consider including them . > > `` 2.The model is a straightforward adaptation of the Plug and Play Generative Networks from the vision community . '' We respectfully disagree that the adaptation was straightforward . While we would have been happy to apply the PPGN approach directly to the language domain , the adaptation actually required several modifications , summarized as follows : PPGN : -- A graphical model depiction of the network looks like this : h - > x - > y , where h is a latent code , x is an image , and y is a class or attribute . -- A single h generates an entire , single image x. h and x are both continuous , and the gradient w.r.t.y passes through x to h. -- The Markov chain is run in h space , with a separate p ( h ) model being trained and used to ensure h does not drift too far from high probability regions . -- Multiple steps are taken in h space , corresponding to multiple entire images . -- Noise is added in h space to obtain the correct diversity of images . PPLM : -- A graphical model depiction of the network looks like this : [ x1 - > ( h1 , x2 ) - > ( h2 , x3 ) , \u2026 ] - > y , h_t and x_t are the latents and byte-pairs at time t and y is an attribute . -- A single h generates a distribution over sentences x. h is continuous and x is discrete , and gradient w.r.t.y passes directly to h , with discrete x skipped , except in the distribution propagation approach in Sec 4.3 , which propagates through the single word x_t+1 ( \u201c Instead , as in -- Dai et al . ( 2019a ) , we use the distribution\u2026 \u201d ) . -- A complete Markov chain is not run , as this would require multiple full forward and backward passes through the transformer . Instead , we update only a sliding window of the recent past of h and sample only one word at a time . This is a compromise between speed and quality of the samples . The particular dependency structure of the transformer allows us to update only the past ( key , value ) pairs , which also allows for efficient sampling . -- Multiple steps are taken in h space as the sentence is constructed word by word . Multiple entire sentences are never produced . -- Noise is added via the sampling of each word in x space to obtain the correct diversity of sentences . [ 1 ] CTRL : A Conditional Transformer Language Model for Controllable Generation , Keskar et al. , https : //arxiv.org/abs/1909.05858 [ 2 ] Fine-Tuning Language Models from Human Preferences , Ziegler et al. , https : //arxiv.org/abs/1909.08593 [ 3 ] Towards controlled text generation , Hu et al. , https : //arxiv.org/abs/1703.00955 [ 4 ] Controlling Linguistic Style Aspects in Neural Language Generation , Ficler et al. , https : //arxiv.org/abs/1707.02633 [ 5 ] Towards Controllable Story Generation , Peng et al . [ 6 ] What makes a good conversation ? How controllable attributes affect human judgments , See et al. , NAACL \u2019 19"}, "2": {"review_id": "H1edEyBKDS-2", "review_text": "The paper introduces an approach to the conditional generation of text, relying on pre-trained decoders, without fine-tuning and, in certain cases, without any training at all. The approach they introduce is following the framework known in NLP as noisy-channel modeling, previously standard in machine translation (in its SMT days), but undergoing certain revival recently (https://arxiv.org/abs/1611.02554, https://arxiv.org/abs/1910.00553,https://arxiv.org/abs/1908.05731,https://arxiv.org/abs/1907.06616). The authors do not mention this connection (they should!). Very differently from these previous approaches attempting to integrate the two factors in the search process (e.g., using reranking), the authors instead rely on gradient descent in the latent space of their model (Transformer), similarly to plug-n-play generative networks in image generation. I find this approach interesting and like the paper overall. However, I do not see why authors do not compare to more direct ways of integrating the conditional component into the model. This would have been tricky in the NMT papers mentioned above, as the entire source sentences need to be reconstructred, however, it should be quite straightforward in this work, with conditioning on single categorical control variables (or maybe a couple in the additional experiments in sect 4.4). Especially, given that the authors already make the predictions of the control variable independently per prediction (e.g., see eq. (5) in section 4.2) / greedily per prefix (bottom lines, page 7). I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. E.g., for the experiments defining topics as sets of seed words (section 4.2), when integrating factors directly (unlike the proposed approach, Table 3), there will be no increase in the probability of generating relevant words before the first seed word is generated. Another limitation is the lack of comparison to standard controlled generation work, i.e. those requiring training a model or/and fine-tuning pretrained decoder. I understand that the proposed approach falls in a different category and, of course, do not expect it to beat a fine-tuned model, but I'd like to get some feel for how much one loses by using this simpler method. There has been a lot of work on controlled generation in recent ~3 years, and they can also be combined with intializing and fine-tuning off-the-shelf pretrained decoders. There is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944 They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of 'steerability' rather than building a controlled-generation model. Given that style-controlled but otherwise unconditional generation may not have that many applications, I am curious how far you can push this approach. E.g., can you make it scale to more complicated data-to-text generation tasks (https://www.aclweb.org/anthology/D17-1239/)? Or, will the only application in this context be integrating new conditioning variables into pretrained conditional LMs? Minor: I am confused with the notation in \"Post-norm Geometric Mean Fusion\" section. It says that softmax is applied to the product of probabilities. Maybe to a linear interpolation of log-probs? Or maybe that's not softmax at all? Something seems off here. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments helping us improve the paper ! > > \u201c following the framework known in NLP as noisy-channel modeling \u2026 . this connection ( they should ! ) . \u201d Thanks for pointing out references to noisy-channel modeling . We have added a mention of the noisy channel modeling approach to our related work section and have discussed how that approach compares to PPLM ( Section 2 ) . > > \u201c I find this approach interesting and like the paper overall. \u201d Thanks ! > > \u201c However \u2026 do not compare to more direct ways of integrating the conditional ... expect the proposed approach to work better ( or at least differently ) but it would be interesting to see it confirmed . .... will be no increase in the probability of generating relevant words before the first seed word is generated. \u201d Thank you for these great suggestions . We \u2019 ve updated the paper to include this approach both for PPLM-BoW and PPLM-Discrim models : For PPLM-BoW : this corresponds to an existing approach referred to in literature as \u201c Weighted Decoding \u201d ( https : //www.aclweb.org/anthology/P17-4008/ , See et al. , NAACL \u2019 19 ) . For PPLM-Discrim : for each token in the vocabulary we compute p ( y=desired sentiment | x ) and sample from the distribution p ( x ) * p ( y=sentiment|x ) with top-k=5 . While the forward passes over the vocabulary are extremely expensive ( e.g.50000x ) , we get a sense of how well PPLM compares with a direct integration of the condition . We have included both sets of results in the paper ( Table 3 and Table 6 , row \u201c WD \u201d ) , where we find , as you presumed , that it does not work quite as well as PPLM . PPLM works better than directly integrating the conditioning into the decoding procedure . For the bag of words , we can further confirm the observation that the probability of generating relevant words before the first seed word from the bag does not increase . Another key difference is that the semantics of the bag of words are not captured , rather the decoder chooses to pick one of the words that fits context . For instance , a generated sample when conditioned on the prefix \u201c Once upon a time \u201d with the \u201c Space \u201d bag of words is \u201c I used to have a pretty good idea what a starfish was . I was a starfish biologist. \u201d . See Section 4.2 for details . For the sentiment control task , we find that this results in a lot of adversarial samples . Sequences often have a high attribute likelihood under the discriminator used during decoding but do not possess the attribute under human evaluation/external classifier evaluation . > > \u201c Another limitation is the lack of comparison to standard controlled generation .... fine-tuning off-the-shelf pretrained decoders. \u201d Thanks for the suggestion ! We have updated the paper to include comparisons with a recent conditional LM ( CTRL , https : //arxiv.org/abs/1909.05858 ) and a GPT-2 LM fine-tuned for positivity with RL and human preferences ( https : //arxiv.org/abs/1909.08593 ) . The details of the set-up can be found in the Section S7 , particuarly , S7.1 and S7.2 . We find PPLM performs comparably with CTRL on sentiment control ( Table 6 ) and ( perhaps surprisingly ) outperforms CTRL on topic control ( Table 3 ) . PPLM also significantly outperforms the fine-tuned GPT-2 model on the sentiment task . In all of the above cases , PPLM is at least as fluent or more fluent than the baselines ( CTRL & fine-tuned GPT-2 ) . This is impressive considering that the fine-tuned GPT-2 model has over twice as many parameters , and the CTRL conditional language model has over 4 times as many parameters and are specifically tuned/trained for controlled gen. > > \u201d There .. interesting relation to the NIPS 2019 paper \u2026 'steerability ' rather ... controlled-generation model. \u201d Thanks for the interesting connection ! We have included this ( Section 2 , Page 3 ) . > > \u201c Given that style-controlled \u2026 can push this approach ... pretrained conditional LMs ? \u201d We believe the PPLM approach should scale well to any method with a Transformer based decoder , including potentially the application you describe , or NMT or Dialogue systems , where See et al. \u2019 19 showed the utility of being able to control the response in dialogue systems . Just as PPLM allows one to combine a p ( x ) model and p ( a|x ) model to generate samples from p ( x|a ) , in the NMT scenario one could combine a base translation model p ( x_target | x_source ) with a p ( a | x_target , x_source ) to generate samples from p ( x_target | a , x_source ) . E.g.this could allow transforming a German to English translation model and a Twitter vs. Wikipedia classifier to translate German phrases into their English Twitter equivalents ! These are some of the immediate next steps we plan on exploring ! > > \u201d Minor : ... Something seems off here . Indeed this was an error -- we had posted a comment mentioning a correction on openreview . Thanks for pointing out ; we have fixed this in the revision now ."}}