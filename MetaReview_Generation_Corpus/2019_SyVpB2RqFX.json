{"year": "2019", "forum": "SyVpB2RqFX", "title": "INFORMATION MAXIMIZATION AUTO-ENCODING", "decision": "Reject", "meta_review": "The paper proposes a principled modeling framework to train a stochastic auto-encoder that is regularized with mutual information maximization. For unsupervised learning, this auto-encoder produces a hybrid continuous-discrete latent representation. While the authors' response and revision have partially addressed some of the raised concerns on the technical analyses, the experimental evaluations presented in the paper do not appear adequate to justify the advantages of the proposed method over previously proposed ones, and the clarity (in particular, notation) needs further improvement. The proposed framework and techniques are potentially of interest to the machine learning community, but the paper of its current form fells below the acceptance bar. The authors are encouraged to improve the clarify of the paper and provide more convincing experiments (e.g., on high-dimensional datasets beyond MNIST).", "reviews": [{"review_id": "SyVpB2RqFX-0", "review_text": "Summary: the paper proposes a method for unsupervised disentangling of both discrete and continuous factors of variation in image data. It uses an autoencoder learned by optimising an additive loss composed of Mutual Information (MI) I(x;y,z) between the image x and the discrete+cts latents (y,z) and the reconstruction error. The mutual information is shown to decompose into I(x,y), I(x,z) and TC(y;z), and the I(x,z) is treated in a different manner to I(x,y). With Gaussian p(z|x), and it is shown that I(x,z_k) is maximal when p(z_k) is Gaussian. So KL(p(z_k)||N(0,1)) is optimised in lieu of optimising I(x,z), and I(x,y) (and TC(y;z)) is optimised by using mini-batch estimates of marginal distributions of y (and z). The paper claims improved disentangling of discrete and continuous latents compared to methods such as JointVAE and InfoVAE. Pros: - The derivation of the loss shows a nice link between Mutual information and total correlation in the latents. - It is a sensible idea to treat the MI terms of the discrete latents differently to the continuous latents - The mathematical and quantitative analysis of MI and its relation to decoder means and variances are informative. Cons: - There is not enough quantitative comparison of the quality of disentanglement across the different methods. The only values for this are the accuracy scores of the discrete factor, but for the continuous latents there are only qualitative latent traversals of single models, and I think these aren\u2019t enough for comparing different disentangling methods - this is too prone to cherry-picking. I think it\u2019s definitely necessary to report some metrics for disentangling that are averaged across multiple models trained with different random seeds. I understand that there are no ground truth cts factors for Mnist/FashionMnist, but this makes me think that a dataset such as dSprites (aka 2D Shapes) where the factors are known and has a mix of discrete and continuous factors would have been more suitable. Here you can use various metrics proposed in Eastwood et al, Kim et al, Chen et al for a quantitative comparison of the disentangled representations. - In figure 4, it says beta=lamda=5 for all models. Shouldn\u2019t you be doing a hyperparameter sweep for each model and choose the best value of hyperparameters for each? It could well be that beta=5 works best for IMAE but other values of beta/lambda can work better for the other models. - When comparing against JointVAE, the authors point out that the accuracy for JointVAE is worse than that of IMAE, a sign of overfitting. You also say that VAT helps maintain local smoothness so as to prevent overfitting. Then shouldn\u2019t you also be comparing against JointVAE + VAT? Looking at Appendix D, it seems like VAT makes a big difference in terms of I(y;y_true), so I\u2019m guessing it will also have a big impact on the accuracy. Thus JointVAE + VAT might beat IMAE in terms of accuracy as well, at which point it will be hard to argue that IMAE is superior in learning the discrete factor. - In the first paragraph of Section 4, the authors claim results on CelebA, but these are missing from the paper. Testing the approach on datasets more complex than (Fashion)Mnist would have been desirable. - There aren\u2019t any latent traversals for the discrete latents - this would be a useful visualisation to complement the accuracy plots in Figure 3. Qs and comments: - It\u2019s not clear why posterior approximation quality (used as a starting point for motivating the loss) is an important quantity for disentangling. - I see that the upper bound to I(x;z_k) in (4) and the objective in (6) have the same optimum at p(z_k) being Gaussian, but it\u2019s not clear that increasing one leads to increasing the other. Using (6) to replace (4) seems to require further justification, whether it be mathematical or empirical. - In proposition 2, I\u2019m sceptical as to how meaningful the derived bound is, especially when you set N to be the size of the minibatch (B) in practice. It also seems that for small delta (i.e. to ensure high probability on the bound) and large K_2 (less restrictive conditions on p(y) and \\hat{p}(y)), the bound can be quite big. - \\mathcal{L}_theta(y) in equation (10) hasn\u2019t been introduced yet. - The z dimension indices in the latent traversal plots of Figure 2 don\u2019t seem to match the x-axis of the left figure. It\u2019s not clear which are the estimates of I(x;z_k) for k=8,3,1 in the figure.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We sincerely thank the reviewer for the thoughtful comments and suggestions . In order to address the main concerns , we updated the numerical results by considering a more complex dataset and incorporating the suggested quantitative evaluations . Below , we start by addressing your concerns one by one . 1 ) * There is not enough quantitative comparison of the quality of disentanglement across the different methods . We fully agree that a quantitative comparison of the disentanglement quality regrading both continuous and discrete representations significantly improves the paper . We provide a quantitative comparison in terms of the disentanglement quality vs. reconstruction error trade-off on dSprites . The corresponding results are summarized in Figures 5 & 7 , where we train each method over a wide range of hyperparameter values , for each value we train over 8 random seeds . We found that , IMAE consistently performs better in terms of achieving better disentanglement quality vs. reconstruction error trade-off over a wide range of beta , gamma values . We attribute this to the effect of explicitly promoting statistical independent continuous latent factors in our objective . Compared to InfoVAE , by using comparatively large weight on the total correlation terms ( we set gamma=2 * beta in this paper ) , we are able to achieve better disentanglement quality without sacrificing the decoding quality too much . This allows us to obtain better disentanglement vs. reconstruction error trade-off , especially in the region where both the informativeness of latent representations and the decoding quality are fairly good . Although JointVAE attains better decoding quality as well as more informative ( overall ) representations with large beta values , the associated disentanglement quality is poor . We suspect that simply pushing the upper bound the mutual information towards a target value does not explicitly encourage disentanglement across the representation factors . 2 ) * Shouldn \u2019 t you be doing a hyperparameter sweep for each model and choose the best value of hyperparameters for each ? Thank you for the comments ! For the updated numerical results , we do sweep over a wide range of hyperparameter values and for each value we run every method over 10 random seeds ( 8 for dSprites due to the limited computational resource , will increases it to 10 or 15 later ) . 3 ) * Looking at Appendix D , it seems like VAT makes a big difference in terms of I ( y ; y_true ) , so I \u2019 m guessing it will also have a big impact on the accuracy . Thus JointVAE + VAT might beat IMAE in terms of accuracy as well , at which point it will be hard to argue that IMAE is superior in learning the discrete factor . In the initial version , we actually augmented all models with VAT in the numerical section , which can significantly improve all methods except betaVAE . The comparison between the results obtained by using ( solid ) and not using ( dashed ) VAT is provided in appendix . We do apologize if we did n't make it clear in the original submission . Same for the updated numerical results , we augment all models with VAT . We also provide one more result ( Figure 8 in Appendix F ) for JointVAE by running it with different target values C_y & C_z . Although JointVAE can achieve better reconstruction error by using larger target values C_y & C_z , the corresponding disentanglement / interpretability of representation factors can be very poor ( Figs 5 & 7 ) . 4 ) * In the first paragraph of Section 4 , the authors claim results on CelebA , but these are missing from the paper . Testing the approach on datasets more complex than ( Fashion ) Mnist would have been desirable . We were not able to do conduct experiments on celebA due to the time constraint and the limited computational resource . As for the updated version , we do quantitatively evaluate our approach on more challenging dataset ( dSprites ) against the other three approaches . However , we are not able to conduct comprehensive experiments on celebA for the same reason . We do apologize for that , we will incorporate the corresponding results ( hopefully also 3D chairs ) for the final version . 5 ) * There aren \u2019 t any latent traversals for the discrete latents - this would be a useful visualisation to complement the accuracy plots in Figure 3 . Thank you for the suggestion ! We incorporate the associated results in both Figure 2 and Figure 4 . We answered your questions in a separate note ."}, {"review_id": "SyVpB2RqFX-1", "review_text": "* This paper proposed a principled framework for auto-encoding through information maximization. A novel contribution of this paper is to introduce a hybrid continuous-discrete representation. The authors also related this approach with other related work such as \\beta-VAE and info-VAE, putting their work in context. Empirical results show that the learned representation has better trade-off among interpretability and decoding quality. * It seems a little strange to me to incorporate the VAT regularization to the IMAE framework in Section 4.2, as this is not included in the overall objective in Equation (10) and earlier analysis (Proposition 1 and 2). Will the conclusions in Proposition 1 and 2 change accordingly due to the inclusion of VAT regularization? * The paper states that IMAE has better trade-off among interpretability and decoding quality. But it is still unclear how a user can choose a good trade-off according to different applications. More discussion along this direction would be helpful. * I guess the L(y) term in Equation (10) is from Equation (9), but this is not stated explicitly in the paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank the reviewer for the positive feedback and the constructive comments/questions . In order to address the main concerns , we incorporate more quantitative comparisons and provide more comprehensive numerical results to evaluate IMAE , which we summarized above . Below are our answers for your questions . 1 ) * It seems a little strange to me to incorporate the VAT regularization to the IMAE framework in Section 4.2 , as this is not included in the overall objective in Equation ( 10 ) and earlier analysis ( Proposition 1 and 2 ) . Will the conclusions in Proposition 1 and 2 change accordingly due to the inclusion of VAT regularization ? VAT is proposed to resolve the inherent difficulty of learning interpretable discrete representations using neural network . As we mentioned at the beginning of section 4.2 , the high capacity of neural network makes it easy to learn a non-smooth function p ( y|x ) that can abruptly change its predictions without guaranteeing similar data samples will be mapped to similar y. VAT is proposed as a regularization to encourage local smoothness of the conditional distribution p ( y|x ) for discrete representations . In our experimental results , we found that using VAT are significantly helpful for learning interpretable discrete representations for all methods considered in this paper except betaVAE . More interpretable continuous representations can be obtained when the method is capable of learning discrete representations that match the true categorical information of data better , since less overlap between the manifolds of each category is induced . This in turn can better help the continuous representations to capture the variation ( feature ) information shared over different categories while simultaneously reducing the possibility for the continuous representations to encode the nuisance information between separated manifolds of each category . Propositions 1 & 2 are provided without considering VAT . As we discussed above , VAT is proposed as a regularization term . Based on our discussions above , we hypothesize that similar statements can still be true under mild assumption ( e.g. , the categorical data are comparatively separated and there does exist common feature information shared over different categories.Since VAT is incorporated to promote the local smoothness of p ( y|x ) , this should n't influence proposition 1 where the I ( x , y ) is defined w.r.t the global information between x & y . Proposition 2 is true in general regardless of y . ( Intuitively , including VAT can help continuous representations to better focus on learning feature information shared across categories , since VAT helps learn more interpretable y.2 ) * The paper states that IMAE has better trade-off among interpretability and decoding quality . But it is still unclear how a user can choose a good trade-off according to different applications . More discussion along this direction would be helpful . In the revision , we comprehensively evaluate IMAE against the other three methods on various datasets . For each dataset , we train each method with a wide range of hyperparameter values . The corresponding results are summarized in Figure 3 ( MNIST and Fashion MNIST ) and Figures 5 & 7 ( dSprites ) . As shown in Figures 3 , IMAE consistently outperforms the other three methods in terms of learning more interpretable ( accurate ) discrete representations over a wide range of hyperparameter values , while simultaneously achieving comparatively better decoding quality and more informative representations . This is further demonstrated in Figure 5 where we evaluate IMAE on a more challenging dataset ( dSprites ) and quantitatively evaluate the disentanglement vs. decoding quality trade-off . Still , IMAE performs better regarding the disentanglement score vs. reconstruction trade-off over a wide range of beta , gamma values . Moreover , as demonstrated in both figure 3 and figure 5 , in the region of interest where both the reconstruction error and the informativeness of representations are fairly good , IMAE achieves a much better reconstruction error vs. interpretability trade-off . We attribute this to the effects of 1 ) maximizing mutual information I ( X , Y ) is capable of learning more interpretable discrete representations that match the natural labels of data better ; 2 ) explicitly promoting statistically independent continuous latent factors by minimizing the total correlation term in our objective . By using comparatively large weight on the total correlation term ( we set gamma = 2 * beta in this paper . ) , we are able to achieve better disentanglement without sacrificing the decoding quality too much . 3 ) * I guess the L ( y ) term in Equation ( 10 ) is from Equation ( 9 ) , but this is not stated explicitly in the paper . Thank you for capturing the typo , which we corrected in the updated version ."}, {"review_id": "SyVpB2RqFX-2", "review_text": "This paper proposes an objective function for auto-encoding they call information maximizing auto encoding (IMAE). To set the stage for my review I will start with the following \"classical\" formulation of auto-encoding as the minimization of the following where we are training models for P(z|x) and P(x|z). beta H(z) + E_{x,z sim P(z|x)} -log P(x|z) (1) Here H(z) is defined by drawing x from the population and then drawing z from P(z|x). This is equivalent to classical rate-distortion coding when P(x|z) is an isotropic Gaussian in which case -log P(x|z) is just the L2 distortion between x and its reconstruction. The parameter beta controls the trade-off between the compression rate and the L2 distortion. This paper replaces minimizing (1) with maximizing beta I(x,z) + E_{x,z sim P(z|x)} log P(x|z) (2) This is equivalent to replacing H(z) in (1) by -I(x,z). But (2) admits a trivial solution of z=x. To prevent the trivial solution this paper proposes to regularize P(z) toward a desired distribution Q(z) and replacing I(x,z) with KL(P(z),Q(z)) by minimizing beta KL(P(z),Q(z)) + E_{x,z sim P(z|x)} - log P(x|z) (3) The paper contains an argument that this replacement is reasonable when Q(z) and P(z|x) are both Gaussian with diagonal covariances. I did not verify that argument but in any case it seems (3) is better than (2). For beta large (3) forces P(z) = Q(z) which fixes H(z) and the a-priori value H(Q). The regularization probably has other benefits. But these suggestions are fairly simple and any real assessment of their value must be done empirically. The papers experiments with MNIST seem insufficient for this. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We 'd like to thank the reviewer for the comments . Below we further clarify our objective and address your concern regrading the numerical results . 1 ) First of all , we want to point out the reviewer could misunderstand our objective . To be more specific , proposition 1 is regrading a single continuous representation factor z_k ( which is a scalar variable ) , therefore the trivial solution z = x suggested by the reviewer does not exists . In other words , the degenerate solution we seek to avoid is respect to each dimension of the continuous representation instead of z itself . Moreover , in this paper , we focus on learning low dimensional yet interpretable representations of the data . Using the setting you provided above , suppose z in R^K and the conditional distribution P ( z|x ) is factorial ( which is a typical assumption is the VAE literature ) , we first decompose I ( x , z ) as the following : I ( x , z ) = sum_k=1^K I ( x , z_k ) - KL ( P ( Z ) || product_k=1^K P ( Z_k ) ) where the first term of RHS quantifies the informativeness of each dimension z_k , and the second term is often referred as the `` total correlation '' of z which achieves the minimum ( zero ) if all dimensions of z are independent of each other . That is the mutual information I ( x , z ) inherently involves two keys terms that quantify the informativeness of each representation factor and the statistical dependence between these factors . We then propose to maximize informativeness of each ( scalar ) representation factor z_k while simultaneously encourage statistical independence across latent factors by minimizing the `` total correlation '' term . By doing so , we are expected to learn informative yet more disentangled representations ( see figure 5 ) . For each scalar representation factor z_k , we ( mathematically ) show in section 3.2 that the trivial solution of maximizing I ( x , z_k ) can be obtained by severely fragmenting the latent space . To be more specifically , the mutual information I ( x , z_k ) can be trivially maximized by mapping each data sample to a deterministic value of z_k , while dispersing the different z_k values associated with different data samples within a dramatically large space . This can results in discontinuity of the latent representations , which is not desired . A natural resolution for this problem is to restrict the variance the z_k to be a reasonable value , with which we propose to push the the marginal distribution of P ( z_k ) towards a gaussian distribution so as to achieve the upper bound of I ( x , z_k ) ( eq ( 4 ) ) in proposition 1 . 2 ) We also want to emphasize that , we propose a framework to learn a hybrid discrete-continuous representations of the data . We seek to learn semantically meaningful discrete representations while maintaining disentanglement of the continuous representations that capture the variations shared across categories . Unsupervised joint learning of disentangled continuous and discrete representations is a challenging problem due to the lack of prior for semantic awareness and other inherent difficulties that arise in learning discrete representations . To the best of our knowledge , our work is , apart from JointVAE , the only framework for jointly learning discrete and continuous representations in a completely unsupervised setting in the VAE literature . 3 ) We update the paper by considering more challenging dataset and incorporating more quantitative evaluations regarding the trade-off between interpretability of representations and decoding quality . Please refer to `` summarization of revision '' provided above ."}], "0": {"review_id": "SyVpB2RqFX-0", "review_text": "Summary: the paper proposes a method for unsupervised disentangling of both discrete and continuous factors of variation in image data. It uses an autoencoder learned by optimising an additive loss composed of Mutual Information (MI) I(x;y,z) between the image x and the discrete+cts latents (y,z) and the reconstruction error. The mutual information is shown to decompose into I(x,y), I(x,z) and TC(y;z), and the I(x,z) is treated in a different manner to I(x,y). With Gaussian p(z|x), and it is shown that I(x,z_k) is maximal when p(z_k) is Gaussian. So KL(p(z_k)||N(0,1)) is optimised in lieu of optimising I(x,z), and I(x,y) (and TC(y;z)) is optimised by using mini-batch estimates of marginal distributions of y (and z). The paper claims improved disentangling of discrete and continuous latents compared to methods such as JointVAE and InfoVAE. Pros: - The derivation of the loss shows a nice link between Mutual information and total correlation in the latents. - It is a sensible idea to treat the MI terms of the discrete latents differently to the continuous latents - The mathematical and quantitative analysis of MI and its relation to decoder means and variances are informative. Cons: - There is not enough quantitative comparison of the quality of disentanglement across the different methods. The only values for this are the accuracy scores of the discrete factor, but for the continuous latents there are only qualitative latent traversals of single models, and I think these aren\u2019t enough for comparing different disentangling methods - this is too prone to cherry-picking. I think it\u2019s definitely necessary to report some metrics for disentangling that are averaged across multiple models trained with different random seeds. I understand that there are no ground truth cts factors for Mnist/FashionMnist, but this makes me think that a dataset such as dSprites (aka 2D Shapes) where the factors are known and has a mix of discrete and continuous factors would have been more suitable. Here you can use various metrics proposed in Eastwood et al, Kim et al, Chen et al for a quantitative comparison of the disentangled representations. - In figure 4, it says beta=lamda=5 for all models. Shouldn\u2019t you be doing a hyperparameter sweep for each model and choose the best value of hyperparameters for each? It could well be that beta=5 works best for IMAE but other values of beta/lambda can work better for the other models. - When comparing against JointVAE, the authors point out that the accuracy for JointVAE is worse than that of IMAE, a sign of overfitting. You also say that VAT helps maintain local smoothness so as to prevent overfitting. Then shouldn\u2019t you also be comparing against JointVAE + VAT? Looking at Appendix D, it seems like VAT makes a big difference in terms of I(y;y_true), so I\u2019m guessing it will also have a big impact on the accuracy. Thus JointVAE + VAT might beat IMAE in terms of accuracy as well, at which point it will be hard to argue that IMAE is superior in learning the discrete factor. - In the first paragraph of Section 4, the authors claim results on CelebA, but these are missing from the paper. Testing the approach on datasets more complex than (Fashion)Mnist would have been desirable. - There aren\u2019t any latent traversals for the discrete latents - this would be a useful visualisation to complement the accuracy plots in Figure 3. Qs and comments: - It\u2019s not clear why posterior approximation quality (used as a starting point for motivating the loss) is an important quantity for disentangling. - I see that the upper bound to I(x;z_k) in (4) and the objective in (6) have the same optimum at p(z_k) being Gaussian, but it\u2019s not clear that increasing one leads to increasing the other. Using (6) to replace (4) seems to require further justification, whether it be mathematical or empirical. - In proposition 2, I\u2019m sceptical as to how meaningful the derived bound is, especially when you set N to be the size of the minibatch (B) in practice. It also seems that for small delta (i.e. to ensure high probability on the bound) and large K_2 (less restrictive conditions on p(y) and \\hat{p}(y)), the bound can be quite big. - \\mathcal{L}_theta(y) in equation (10) hasn\u2019t been introduced yet. - The z dimension indices in the latent traversal plots of Figure 2 don\u2019t seem to match the x-axis of the left figure. It\u2019s not clear which are the estimates of I(x;z_k) for k=8,3,1 in the figure.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We sincerely thank the reviewer for the thoughtful comments and suggestions . In order to address the main concerns , we updated the numerical results by considering a more complex dataset and incorporating the suggested quantitative evaluations . Below , we start by addressing your concerns one by one . 1 ) * There is not enough quantitative comparison of the quality of disentanglement across the different methods . We fully agree that a quantitative comparison of the disentanglement quality regrading both continuous and discrete representations significantly improves the paper . We provide a quantitative comparison in terms of the disentanglement quality vs. reconstruction error trade-off on dSprites . The corresponding results are summarized in Figures 5 & 7 , where we train each method over a wide range of hyperparameter values , for each value we train over 8 random seeds . We found that , IMAE consistently performs better in terms of achieving better disentanglement quality vs. reconstruction error trade-off over a wide range of beta , gamma values . We attribute this to the effect of explicitly promoting statistical independent continuous latent factors in our objective . Compared to InfoVAE , by using comparatively large weight on the total correlation terms ( we set gamma=2 * beta in this paper ) , we are able to achieve better disentanglement quality without sacrificing the decoding quality too much . This allows us to obtain better disentanglement vs. reconstruction error trade-off , especially in the region where both the informativeness of latent representations and the decoding quality are fairly good . Although JointVAE attains better decoding quality as well as more informative ( overall ) representations with large beta values , the associated disentanglement quality is poor . We suspect that simply pushing the upper bound the mutual information towards a target value does not explicitly encourage disentanglement across the representation factors . 2 ) * Shouldn \u2019 t you be doing a hyperparameter sweep for each model and choose the best value of hyperparameters for each ? Thank you for the comments ! For the updated numerical results , we do sweep over a wide range of hyperparameter values and for each value we run every method over 10 random seeds ( 8 for dSprites due to the limited computational resource , will increases it to 10 or 15 later ) . 3 ) * Looking at Appendix D , it seems like VAT makes a big difference in terms of I ( y ; y_true ) , so I \u2019 m guessing it will also have a big impact on the accuracy . Thus JointVAE + VAT might beat IMAE in terms of accuracy as well , at which point it will be hard to argue that IMAE is superior in learning the discrete factor . In the initial version , we actually augmented all models with VAT in the numerical section , which can significantly improve all methods except betaVAE . The comparison between the results obtained by using ( solid ) and not using ( dashed ) VAT is provided in appendix . We do apologize if we did n't make it clear in the original submission . Same for the updated numerical results , we augment all models with VAT . We also provide one more result ( Figure 8 in Appendix F ) for JointVAE by running it with different target values C_y & C_z . Although JointVAE can achieve better reconstruction error by using larger target values C_y & C_z , the corresponding disentanglement / interpretability of representation factors can be very poor ( Figs 5 & 7 ) . 4 ) * In the first paragraph of Section 4 , the authors claim results on CelebA , but these are missing from the paper . Testing the approach on datasets more complex than ( Fashion ) Mnist would have been desirable . We were not able to do conduct experiments on celebA due to the time constraint and the limited computational resource . As for the updated version , we do quantitatively evaluate our approach on more challenging dataset ( dSprites ) against the other three approaches . However , we are not able to conduct comprehensive experiments on celebA for the same reason . We do apologize for that , we will incorporate the corresponding results ( hopefully also 3D chairs ) for the final version . 5 ) * There aren \u2019 t any latent traversals for the discrete latents - this would be a useful visualisation to complement the accuracy plots in Figure 3 . Thank you for the suggestion ! We incorporate the associated results in both Figure 2 and Figure 4 . We answered your questions in a separate note ."}, "1": {"review_id": "SyVpB2RqFX-1", "review_text": "* This paper proposed a principled framework for auto-encoding through information maximization. A novel contribution of this paper is to introduce a hybrid continuous-discrete representation. The authors also related this approach with other related work such as \\beta-VAE and info-VAE, putting their work in context. Empirical results show that the learned representation has better trade-off among interpretability and decoding quality. * It seems a little strange to me to incorporate the VAT regularization to the IMAE framework in Section 4.2, as this is not included in the overall objective in Equation (10) and earlier analysis (Proposition 1 and 2). Will the conclusions in Proposition 1 and 2 change accordingly due to the inclusion of VAT regularization? * The paper states that IMAE has better trade-off among interpretability and decoding quality. But it is still unclear how a user can choose a good trade-off according to different applications. More discussion along this direction would be helpful. * I guess the L(y) term in Equation (10) is from Equation (9), but this is not stated explicitly in the paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank the reviewer for the positive feedback and the constructive comments/questions . In order to address the main concerns , we incorporate more quantitative comparisons and provide more comprehensive numerical results to evaluate IMAE , which we summarized above . Below are our answers for your questions . 1 ) * It seems a little strange to me to incorporate the VAT regularization to the IMAE framework in Section 4.2 , as this is not included in the overall objective in Equation ( 10 ) and earlier analysis ( Proposition 1 and 2 ) . Will the conclusions in Proposition 1 and 2 change accordingly due to the inclusion of VAT regularization ? VAT is proposed to resolve the inherent difficulty of learning interpretable discrete representations using neural network . As we mentioned at the beginning of section 4.2 , the high capacity of neural network makes it easy to learn a non-smooth function p ( y|x ) that can abruptly change its predictions without guaranteeing similar data samples will be mapped to similar y. VAT is proposed as a regularization to encourage local smoothness of the conditional distribution p ( y|x ) for discrete representations . In our experimental results , we found that using VAT are significantly helpful for learning interpretable discrete representations for all methods considered in this paper except betaVAE . More interpretable continuous representations can be obtained when the method is capable of learning discrete representations that match the true categorical information of data better , since less overlap between the manifolds of each category is induced . This in turn can better help the continuous representations to capture the variation ( feature ) information shared over different categories while simultaneously reducing the possibility for the continuous representations to encode the nuisance information between separated manifolds of each category . Propositions 1 & 2 are provided without considering VAT . As we discussed above , VAT is proposed as a regularization term . Based on our discussions above , we hypothesize that similar statements can still be true under mild assumption ( e.g. , the categorical data are comparatively separated and there does exist common feature information shared over different categories.Since VAT is incorporated to promote the local smoothness of p ( y|x ) , this should n't influence proposition 1 where the I ( x , y ) is defined w.r.t the global information between x & y . Proposition 2 is true in general regardless of y . ( Intuitively , including VAT can help continuous representations to better focus on learning feature information shared across categories , since VAT helps learn more interpretable y.2 ) * The paper states that IMAE has better trade-off among interpretability and decoding quality . But it is still unclear how a user can choose a good trade-off according to different applications . More discussion along this direction would be helpful . In the revision , we comprehensively evaluate IMAE against the other three methods on various datasets . For each dataset , we train each method with a wide range of hyperparameter values . The corresponding results are summarized in Figure 3 ( MNIST and Fashion MNIST ) and Figures 5 & 7 ( dSprites ) . As shown in Figures 3 , IMAE consistently outperforms the other three methods in terms of learning more interpretable ( accurate ) discrete representations over a wide range of hyperparameter values , while simultaneously achieving comparatively better decoding quality and more informative representations . This is further demonstrated in Figure 5 where we evaluate IMAE on a more challenging dataset ( dSprites ) and quantitatively evaluate the disentanglement vs. decoding quality trade-off . Still , IMAE performs better regarding the disentanglement score vs. reconstruction trade-off over a wide range of beta , gamma values . Moreover , as demonstrated in both figure 3 and figure 5 , in the region of interest where both the reconstruction error and the informativeness of representations are fairly good , IMAE achieves a much better reconstruction error vs. interpretability trade-off . We attribute this to the effects of 1 ) maximizing mutual information I ( X , Y ) is capable of learning more interpretable discrete representations that match the natural labels of data better ; 2 ) explicitly promoting statistically independent continuous latent factors by minimizing the total correlation term in our objective . By using comparatively large weight on the total correlation term ( we set gamma = 2 * beta in this paper . ) , we are able to achieve better disentanglement without sacrificing the decoding quality too much . 3 ) * I guess the L ( y ) term in Equation ( 10 ) is from Equation ( 9 ) , but this is not stated explicitly in the paper . Thank you for capturing the typo , which we corrected in the updated version ."}, "2": {"review_id": "SyVpB2RqFX-2", "review_text": "This paper proposes an objective function for auto-encoding they call information maximizing auto encoding (IMAE). To set the stage for my review I will start with the following \"classical\" formulation of auto-encoding as the minimization of the following where we are training models for P(z|x) and P(x|z). beta H(z) + E_{x,z sim P(z|x)} -log P(x|z) (1) Here H(z) is defined by drawing x from the population and then drawing z from P(z|x). This is equivalent to classical rate-distortion coding when P(x|z) is an isotropic Gaussian in which case -log P(x|z) is just the L2 distortion between x and its reconstruction. The parameter beta controls the trade-off between the compression rate and the L2 distortion. This paper replaces minimizing (1) with maximizing beta I(x,z) + E_{x,z sim P(z|x)} log P(x|z) (2) This is equivalent to replacing H(z) in (1) by -I(x,z). But (2) admits a trivial solution of z=x. To prevent the trivial solution this paper proposes to regularize P(z) toward a desired distribution Q(z) and replacing I(x,z) with KL(P(z),Q(z)) by minimizing beta KL(P(z),Q(z)) + E_{x,z sim P(z|x)} - log P(x|z) (3) The paper contains an argument that this replacement is reasonable when Q(z) and P(z|x) are both Gaussian with diagonal covariances. I did not verify that argument but in any case it seems (3) is better than (2). For beta large (3) forces P(z) = Q(z) which fixes H(z) and the a-priori value H(Q). The regularization probably has other benefits. But these suggestions are fairly simple and any real assessment of their value must be done empirically. The papers experiments with MNIST seem insufficient for this. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We 'd like to thank the reviewer for the comments . Below we further clarify our objective and address your concern regrading the numerical results . 1 ) First of all , we want to point out the reviewer could misunderstand our objective . To be more specific , proposition 1 is regrading a single continuous representation factor z_k ( which is a scalar variable ) , therefore the trivial solution z = x suggested by the reviewer does not exists . In other words , the degenerate solution we seek to avoid is respect to each dimension of the continuous representation instead of z itself . Moreover , in this paper , we focus on learning low dimensional yet interpretable representations of the data . Using the setting you provided above , suppose z in R^K and the conditional distribution P ( z|x ) is factorial ( which is a typical assumption is the VAE literature ) , we first decompose I ( x , z ) as the following : I ( x , z ) = sum_k=1^K I ( x , z_k ) - KL ( P ( Z ) || product_k=1^K P ( Z_k ) ) where the first term of RHS quantifies the informativeness of each dimension z_k , and the second term is often referred as the `` total correlation '' of z which achieves the minimum ( zero ) if all dimensions of z are independent of each other . That is the mutual information I ( x , z ) inherently involves two keys terms that quantify the informativeness of each representation factor and the statistical dependence between these factors . We then propose to maximize informativeness of each ( scalar ) representation factor z_k while simultaneously encourage statistical independence across latent factors by minimizing the `` total correlation '' term . By doing so , we are expected to learn informative yet more disentangled representations ( see figure 5 ) . For each scalar representation factor z_k , we ( mathematically ) show in section 3.2 that the trivial solution of maximizing I ( x , z_k ) can be obtained by severely fragmenting the latent space . To be more specifically , the mutual information I ( x , z_k ) can be trivially maximized by mapping each data sample to a deterministic value of z_k , while dispersing the different z_k values associated with different data samples within a dramatically large space . This can results in discontinuity of the latent representations , which is not desired . A natural resolution for this problem is to restrict the variance the z_k to be a reasonable value , with which we propose to push the the marginal distribution of P ( z_k ) towards a gaussian distribution so as to achieve the upper bound of I ( x , z_k ) ( eq ( 4 ) ) in proposition 1 . 2 ) We also want to emphasize that , we propose a framework to learn a hybrid discrete-continuous representations of the data . We seek to learn semantically meaningful discrete representations while maintaining disentanglement of the continuous representations that capture the variations shared across categories . Unsupervised joint learning of disentangled continuous and discrete representations is a challenging problem due to the lack of prior for semantic awareness and other inherent difficulties that arise in learning discrete representations . To the best of our knowledge , our work is , apart from JointVAE , the only framework for jointly learning discrete and continuous representations in a completely unsupervised setting in the VAE literature . 3 ) We update the paper by considering more challenging dataset and incorporating more quantitative evaluations regarding the trade-off between interpretability of representations and decoding quality . Please refer to `` summarization of revision '' provided above ."}}