{"year": "2021", "forum": "asLT0W1w7Li", "title": "Efficient Exploration for Model-based Reinforcement Learning with Continuous States and Actions", "decision": "Reject", "meta_review": "This paper presents a model-based posterior sampling algorithm in continuous state-action spaces theoretically and empirically. The work is interesting and the authors provide numerical evaluations of the proposed method. But the reviewers find the contribution of the work limited.\n", "reviews": [{"review_id": "asLT0W1w7Li-0", "review_text": "Review This paper proposes a new model-based reinforcement learning algorithm named MPC-PSRL . Theoretically , the authors provide regret analysis of the proposed algorithm . The authors also provide empirical results that MPC-PSRL outperforms other previous model-based RL algorithms , such as PETS or MBPO . However , it is not clear that the main contribution of this paper . Osband & Van Roy ( 2014 ) already provides the algorithm posterior sampling RL algorithm , named PSRL for continuous domain . If posterior distribution of MDP is modeled as GP and optimal policy is computed by MPC , then it is the same as the method in this paper . They also provide regret analysis for continuous domain with sub-Gaussian noise model . What are the major difficulties / differences compared to the previous work ? Questions : Is there any reason there are no empirical results on Pendulum and Cartpole with oracle rewards ? To emphasize the effect of posterior sampling ( maybe exploration ) , would you provide the results that using just mean instead of sampling ? It is not clear whether both GP modeling and exploration via posterior sampling have a significant impact on performance . Typos : Use \\sigma_R with \\sigma_r / \\sigma_P with \\sigma_f Theorem 1 in page 3 , d_\\phi should be d. In proof of theorem 1 , what is the definition of \\delta_k ( r ) and \\delta_k ( f ) ? score : 5 - > 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review and detailed comments ! We respectfully disagree that our theoretical novelty is unclear . * * Re 1 : Difference between regret in Osband & Van Roy ( 2014 ) and ours * * The main limitation of previous methods ( Osband & Van Roy ( 2014 ) , Chowdhury & Gopalan ( 2019 ) ) is that their bounds can be exponential in $ H $ . Please check our theoretical novelty compared to previous methods in the general comments above for details : * * https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK * * , where we show * * why the regret bound in Osband & Van Roy ( 2014 ) and Chowdhury & Gopalan ( 2019 ) can be exponential in $ H $ in continuous spaces in part 1.1 , and clarify our improvement with respect to $ H $ in part 1.2 * * . We also illustrate our improvement w.r.t.the dimension $ d $ in part 2 and highlight other differences between our proof and Chowdhury & Gopalan ( 2019 ) in part 3 . Our Bayesian regret is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ , which is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces , and it also matches the best-known frequentist regret ( Zanette et al . ( 2020 ) in our Related Work Section 2 ) . * * Re 2 : Why there is no result on Cartpole and Pendulum with oracle rewards * * As we mentioned in the fourth paragraph in Section 6 Experiments , trajectory optimization with oracle rewards in Cartpole and Pendulum is * * very easy * * , and there is almost no difference in the performance for all model-based algorithms we compare , so we omit showing these trivial learning curves . * * Re 3 : Performance of sampling verse mean * * Thanks for your suggestion ! We have added a comparison between posterior sampling and using the mean of the same Bayesian linear regression model , which shows that sampling further improves the performance than simply using the mean value . The results are shown below . | | cartpole | pendulum | pusher | reacher | pusher ( r ) | reacher ( r ) | |-| -- | -- | -- || -- || | mean | 8.6\u00b10.75 | 9.6\u00b10.49 | 103.4\u00b114.59 | 34.7\u00b12.31 | 118.2\u00b17.29 | 11.6\u00b11.36 | | sampling | 6.8\u00b11.36 | 5.0\u00b11.41 | 95.8\u00b125.29 | 23.2\u00b14.58 | 103.6\u00b111.11 | 9.4\u00b13.72 | Here we provide the episodes ( average of five trials with standard deviation ) used to solve different environments ( `` solve '' means the cumulative rewards exceed a certain threshold , which is 200 for Cartpole , -380 for Pendulum , -50 for Pusher , and -40 for Reacher ) . The environments with ( r ) indicate that the oracle reward function is provided . Some comments for this table : We use Bayesian linear regression ( BLR ) on the feature space in modeling ( which is Gaussian Process with linear kernels ) . We want to clarify that * * using the mean of BLR also provides some extent of exploration * * , because the * * covariance matrix * * , which provides epistemic uncertainty in posterior sampling , is also used if we use mean in prediction . The formula for mean corresponds to a ridge regression instead of simple linear regression . The above can be verified in the posterior distribution formula of $ w $ in our Section 5.1 : Bayesian update and posterior sampling . However , * * using mean does not explore as much as posterior sampling does * * , since the posterior sampling adds randomness , besides the information provided by mean . Besides , there is currently no theoretical guarantee for Bayesian regret using the mean value instead of sampling : the posterior sampling lemma wo n't work if the agent simply uses mean , and it is hard to derive bounds directly on $ \\Delta_k $ in the paper ( we only need to analyze $ \\tilde { \\Delta } _k $ if we can use the posterior sampling algorithm ) . ) Typos : we have fixed the typos and add some definitions . Thanks for pointing out ! Please let us know if you have further questions about our response . We are very happy to have further discussions !"}, {"review_id": "asLT0W1w7Li-1", "review_text": "The paper proposes a model-based posterior sampling algorithms with regret guarantees when the model is assumed to be drawn from a distribution randomly . The authors also provide numerical evaluations of the proposed method . - The contribution of this work as theoretical work is limited . There is no study on fundamental limit . In addition , the performance guarantee seems worse than existing ones , although a fair comparison might be unavailable due to different technical assumes . However , the authors do not provide numerical comparison to existing algorithms with performance guarantee . - It is not possible to assess the contribution from numerical comparison . There is no description on the hyperparameter selection of other algorithms ( PETS , MDPO , SAC , ... ) . Hence , it is not reproducible as well . - The definition of $ BayesRegret $ seems incorrect as it takes $ M^ * $ as input argument . The authors need to describe what they mean by the expectation in eq . ( 3 ) .In my understanding , $ BayesRegret $ should take hyper-parameter to generate $ M^ * $ as input , while $ Regret $ taking a random incidence of $ M^ * $ as input . - I do n't understand the meaning of regret bound $ \\tilde { O } ( H^ { 3/2 } d_\\phi T ) $ for non-linear case as regret of any algorithm is upper-bounded by $ R_ { max } T $ .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and detailed comments ! We respectfully disagree that our theoretical guarantee is worse than existing ones with no fundamental limit . We have responded to each point as below . Please let us know if you have questions or suggestions . * * Re 1 : Performance guarantee compared with previous works * * By `` worse than existing ones '' , are you referring to regret bounds in tabular RL , which is $ O ( H\\sqrt { SAT } ) $ ? ( The study of previous works on tabular spaces in both frequenist and Bayesian settings are also mentioned in the introduction and related work . ) Note that our analysis focuses on * * continuous state-action spaces * * , where $ S $ and $ A $ can go to infinity . The study of previous results on frequentist regrets is described in our Related Work ( Section 2 ) , where we show that the best result so far is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ . The study of previous works on Bayesian regrets is mentioned in our Introduction ( Section 1 ) , where we show that previous works fail to provide an exact dependency on episode length $ H $ . A detailed explanation for theoretical improvements can be found in our general comments on the theoretical novelty above ( * * https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK * * ) . Our Bayesian regret is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ , which is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces . our result also matches the best-known frequentist regret in continuous spaces ( Zanette et al . ( 2020 ) , described in related work ) , where they have also proved that it matches the lower bound for this setting . However , their algorithm is based on optimization over an upper confidence set , which is computationally prohibitive , while posterior sampling algorithm only require optimizing a single MDP . * * Re 2 : Hyperparameters and other experimental details for reproducibility * * Thanks for your suggestion ! We have added experimental details for PETS , MBPO , SAC , and DDPG in Appendix 2 . Hyperparameters for our experiments are also added for reproducibility . * * Re 3 : Definition for BayesRegret * * Thank you for pointing it out ! It is a typo : in $ BayesRegret $ it should be $ \\phi $ instead of $ M^ * $ , where $ \\phi $ is the prior distribution of $ M^ * $ . We follow the same definition of $ BayesRegret $ and $ Regret $ in Osband & Van Roy ( 2017 ) Section 2 . Here the expectation is taken with respect to the prior distribution of $ M $ . We have made the revision in the latest version . * * Re 4 : Regret bound typo in the abstract * * We sincerely apologize for the typo in the abstract ; it should be $ \\sqrt { T } $ as described in Section 4.2 . We have made the revision in the latest version . Please let us know if you have additional questions about our response . We are very happy to have further discussions ."}, {"review_id": "asLT0W1w7Li-2", "review_text": "Pros -- * The paper proposes a method to balance exploration and exploitation in reinforcement learning problems whose transitions and rewards are assumed to be sampled from Gaussian processes , and provide a Bayesian regret bound . Also , the paper shows how the proposed approach can be implemented in practice using model predictive control . Cons -- * It is not clear what is the novelty of the theoretical results in this paper when compared to the regret bounds by Chowdhury & Gopalan ( 2019 ) , who provide both frequentist and Bayesian bounds when the transitions and rewards are in an RKHS or sampled according to a GP . The bound of Chowdhury & Gopalan ( 2019 ) seem to be polynomial in the horizon H ( their paper mention a $ HSA\\sqrt { T } $ bound in the particular case of finite MDPs ) , whereas the current paper says that `` H is still unbounded '' in their result . Hence , further clarification is needed regarding this point . * The method is claimed to be computationally tractable : \u201c it can be easily implemented by only optimizing a single sampled MDP \u201d . However , the sampled MDP is continuous , and solving a continuous MDP is hard in general . Experimentally , the paper proposes the cross entropy method ( CEM ) for planning : in this case , planning is not exact and the regret bound does not hold anymore . I believe this issue should be made clear in the introduction/related work section . * Although my main concern is the theoretical novelty , the experimental section can be improved : it would be interesting to compare the proposed approach to other strategies for exploration in deep RL , for instance * Bellemare et al . ( 2016 ) , Unifying Count-Based Exploration and Intrinsic Motivation * Tang et al . ( 2017 ) , # Exploration : A Study of Count-Based Exploration for Deep Reinforcement Learning * Azizzadenesheli et al . ( 2018 ) , Efficient Exploration through Bayesian Deep Q-Networks ; and also perform experiments in small simple environments that satisfy the assumptions to check if the regret is sublinear ( e.g.a continuous \u201c grid world \u201d with noisy transitions ) . * In the nonlinear case ( Section 4.2 ) it is not clear how to follow the steps of Yang & Wang ( 2019 ) to derive a Bayesian regret bound with feature representation , since their assumption is related to low-rank MDPs instead of Gaussian processes . In addition , it would be interesting to discuss how the model would be sampled ( in Algorithm 1 ) in this case . Suggestions & remarks * Introduction : mention which of the cited papers proves the $ H \\sqrt { SAT } $ upper bound on the Bayesian regret , clarify whether $ T $ is the number of episodes , or $ H $ times the number of episodes . * Some definitions are missing : * The linear kernel should be defined before Theorem 1 * $ M^k $ is not defined before appearing in Eq.5 * Increase the font size of the text in Figure 1 . * In Algorithm 1 , include what are the input parameters ( e.g. $ \\sigma_r , \\sigma_f $ ) . * Some suggestions for the proof : * Write the relation between $ \\Delta_k $ and $ \\tilde { \\Delta } _k $ . * Include ( possibly in the appendix ) more details about the arguments in the paragraph below Eq.9.For instance , there is an argument about a bound on the information gain , which is not defined in the paper . Also , it might be useful ( for the reader ) to restate ( in the appendix ) the results by Williams & Vivarelli ( 2000 ) , Srinivas et al . ( 2012 ) and Russo & Van Roy ( 2014 ) required for the proof . Typos * Abstract : $ T $ instead of $ \\sqrt { T } $ in the regret bound with feature representation * Page 9 : Definition of MBPO , it should be \u201c Model-Based Policy ... \u201d * Typo in integration limits in Eq.12 ( the $ \\mu'-\\mu $ at the bottom should be $ \\mu-\\mu ' $ ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review , suggestions , and detailed comments ! We respectfully disagree that our theoretical novelty is unclear ( see Re1 ) . And the model-free baselines you proposed are already outperformed by existing model-based algorithms for continuous state-action spaces ( see Re3 ) . * * Re 1 : Theoretical novelty compared to the regret bounds by Chowdhury & Gopalan ( 2019 ) , and other papers * * We first clarify that the regret bound in Chowdhury & Gopalan ( 2019 ) can be exponential in $ H $ in continuous spaces , and the $ \\tilde { O } ( HSA\\sqrt { T } ) $ bound in Chowdhury & Gopalan ( 2019 ) is for tabular MDPs . In continuous spaces , S and A are infinite , and we focus on * * continuous state-action spaces * * in our paper . Please check the general comments here for more details : * * https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK * * , where we show * * why the regret bound in Chowdhury & Gopalan ( 2019 ) can be exponential in $ H $ in continuous spaces in part 1.1 , and clarify our improvement with respect to $ H $ in part 1.2 * * . We also illustrate our improvement w.r.t.the dimension $ d $ in part 2 and highlight other differences between our proof and Chowdhury & Gopalan ( 2019 ) in part 3 . Note that * * The best-known frequentist regret bound for continuous state-action space is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ * * in Zanette et al . ( 2020 ) Theorem 1 . Our Bayesian regret is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ , which is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces , and it also matches the best-known frequentist regret ( Zanette et al . ( 2020 ) in our Related Work Section 2 ) . * * Re 2 : Clarification on computational tractability * * By computationally tractable , we are mainly comparing to Zanette et al . ( 2020 ) , which achieve the same frequentist bound as ours . However , they use the UCB ( upper confidence bound ) algorithm and need to optimize over an upper confidence set of continuous MDPs , which is computationally prohibitive . Although solving continuous MDPs is generally hard , Chua et al . ( 2018 ) already show that MPC with CEM optimizer can solve Gym and Mujoco tasks within a handful of trials . Thus we conduct experiments in the * * environments that MPC planning itself is shown sufficient to solve the MDPs * * as in Chua et al . ( 2018 ) . Using the same planning method , our algorithm outperforms theirs , which indicates the effectiveness of efficient exploration . Thank you very much for pointing out that the planning method is an approximate solution , and we have added this clarification in the introduction ."}, {"review_id": "asLT0W1w7Li-3", "review_text": "This paper studies the model-based reinforcement learning . They propose a posterior sampling algorithm and provide a Bayesian regret guarantee . Under the assumption that the reward and transition functions are Gaussian processes with linear kernels , the regret bound is in the order of H^1.5 d sqrt { T } where H is the episode length . The dependence that regret is in polynomial of H is a nice property . However , this advantage seems to be obtained by the assumption of Gaussian processes with linear kernels . Besides , I have the following comments . 1 ) It seems that the result in Theorem 1 is quite straightforward from the results in Osband & Van Roy 2014 . Could you justify your contribution in this result . 2 ) What is MPC ? Suppose to be model predictive control ? It is not officially introduced in the context . 3 ) In terms of computational complexity , could you elaborate more on the computational complexity of each line ( component ) of the algorithm . For general cases , posterior sampling could also be expensive as there are no closed-form solutions , one may need to use MCMC method .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and detailed comments ! We respectfully disagree that our theoretical contribution is incremental . * * Re 1 : Our contribution in Theorem 1 compared to Osband & Van Roy ( 2014 ) * * Our bound enjoys lower order in terms of episode length $ H $ and dimensionality $ d $ compared to Osband & Van Roy ( 2014 ) . The improvement of $ H $ mainly comes from the use of our Lemma 1 ( which only make use of the noise assumption ) , instead of Gaussian processes with linear kernels . For a detailed analysis on our improvement of $ H $ and $ d $ , please refer to our general comments on theoretical novelty above ( https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK ) . Our result is NOT straightforward from results in previous works , since we provided an original Lemma 1 ( proved in appendix ) , to significant improve the order of $ H $ with the same assumption from the previous work , and the proof to use Lemma 1 to get Theorem 1 is also different from Osband & Van Roy ( 2014 ) . Our bound is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces , and it also matches the best-known frequentist regret ( Zanette et al . ( 2020 ) ) in continuous spaces as mentioned in our Related Work ( Section 2 ) . * * Re 2 : What is MPC * * MPC is model predictive control . Sorry for not citing the book in Section 5.2 ( it should be Camacho & Alba ( 2013 ) : `` Model predictive control '' . We have made a revision for this . Thank you for pointing out ! * * Re 3 : Computational complexity on each component of the algorithm * * The main cost of computation comes from MPC planning and posterior update of the Bayesian model . We use MPC action selection with a CEM optimizer as in Chua et al . ( 2018 ) , to solve a single sampled MDP . At each step in an episode , we need to calculate the predicted return from the current step up to $ \\tau $ steps ( which is the planning horizon , usually $ \\tau=30 $ ) . Using CEM , we generate $ M $ * $ \\tau $ actions to form $ M $ action sequences at each iteration , where $ M $ is the population size ( usually 500 ) , use the sampled MDP to generate random returns , and select several elite sequences to update the CEM distribution to use in next iteration ( the max iteration number is usually 5 ) . So in each episode , the complexity of computation from MPC is $ O ( \\tau MH ) $ . Overall , it enjoys a lower complexity than Chua et al . ( 2018 ) since we do not use an ensemble of different models . Posterior update : The matrix multiplication to calculate of matrix $ A $ in 5.1 is $ O ( d^2N ) $ , and the inverse of matrix $ A $ is $ O ( d^3 ) $ , where $ N $ is the number of data points , and $ d $ is the dimension of the last hidden layer of the neural network . In our experiments , we find forcing the last hidden layer dimension to be the same as the input dimension works well in our modeling , so the posterior update is not computationally expensive . Please let us know if you have additional questions . We are very happy to have further discussions ."}], "0": {"review_id": "asLT0W1w7Li-0", "review_text": "Review This paper proposes a new model-based reinforcement learning algorithm named MPC-PSRL . Theoretically , the authors provide regret analysis of the proposed algorithm . The authors also provide empirical results that MPC-PSRL outperforms other previous model-based RL algorithms , such as PETS or MBPO . However , it is not clear that the main contribution of this paper . Osband & Van Roy ( 2014 ) already provides the algorithm posterior sampling RL algorithm , named PSRL for continuous domain . If posterior distribution of MDP is modeled as GP and optimal policy is computed by MPC , then it is the same as the method in this paper . They also provide regret analysis for continuous domain with sub-Gaussian noise model . What are the major difficulties / differences compared to the previous work ? Questions : Is there any reason there are no empirical results on Pendulum and Cartpole with oracle rewards ? To emphasize the effect of posterior sampling ( maybe exploration ) , would you provide the results that using just mean instead of sampling ? It is not clear whether both GP modeling and exploration via posterior sampling have a significant impact on performance . Typos : Use \\sigma_R with \\sigma_r / \\sigma_P with \\sigma_f Theorem 1 in page 3 , d_\\phi should be d. In proof of theorem 1 , what is the definition of \\delta_k ( r ) and \\delta_k ( f ) ? score : 5 - > 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review and detailed comments ! We respectfully disagree that our theoretical novelty is unclear . * * Re 1 : Difference between regret in Osband & Van Roy ( 2014 ) and ours * * The main limitation of previous methods ( Osband & Van Roy ( 2014 ) , Chowdhury & Gopalan ( 2019 ) ) is that their bounds can be exponential in $ H $ . Please check our theoretical novelty compared to previous methods in the general comments above for details : * * https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK * * , where we show * * why the regret bound in Osband & Van Roy ( 2014 ) and Chowdhury & Gopalan ( 2019 ) can be exponential in $ H $ in continuous spaces in part 1.1 , and clarify our improvement with respect to $ H $ in part 1.2 * * . We also illustrate our improvement w.r.t.the dimension $ d $ in part 2 and highlight other differences between our proof and Chowdhury & Gopalan ( 2019 ) in part 3 . Our Bayesian regret is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ , which is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces , and it also matches the best-known frequentist regret ( Zanette et al . ( 2020 ) in our Related Work Section 2 ) . * * Re 2 : Why there is no result on Cartpole and Pendulum with oracle rewards * * As we mentioned in the fourth paragraph in Section 6 Experiments , trajectory optimization with oracle rewards in Cartpole and Pendulum is * * very easy * * , and there is almost no difference in the performance for all model-based algorithms we compare , so we omit showing these trivial learning curves . * * Re 3 : Performance of sampling verse mean * * Thanks for your suggestion ! We have added a comparison between posterior sampling and using the mean of the same Bayesian linear regression model , which shows that sampling further improves the performance than simply using the mean value . The results are shown below . | | cartpole | pendulum | pusher | reacher | pusher ( r ) | reacher ( r ) | |-| -- | -- | -- || -- || | mean | 8.6\u00b10.75 | 9.6\u00b10.49 | 103.4\u00b114.59 | 34.7\u00b12.31 | 118.2\u00b17.29 | 11.6\u00b11.36 | | sampling | 6.8\u00b11.36 | 5.0\u00b11.41 | 95.8\u00b125.29 | 23.2\u00b14.58 | 103.6\u00b111.11 | 9.4\u00b13.72 | Here we provide the episodes ( average of five trials with standard deviation ) used to solve different environments ( `` solve '' means the cumulative rewards exceed a certain threshold , which is 200 for Cartpole , -380 for Pendulum , -50 for Pusher , and -40 for Reacher ) . The environments with ( r ) indicate that the oracle reward function is provided . Some comments for this table : We use Bayesian linear regression ( BLR ) on the feature space in modeling ( which is Gaussian Process with linear kernels ) . We want to clarify that * * using the mean of BLR also provides some extent of exploration * * , because the * * covariance matrix * * , which provides epistemic uncertainty in posterior sampling , is also used if we use mean in prediction . The formula for mean corresponds to a ridge regression instead of simple linear regression . The above can be verified in the posterior distribution formula of $ w $ in our Section 5.1 : Bayesian update and posterior sampling . However , * * using mean does not explore as much as posterior sampling does * * , since the posterior sampling adds randomness , besides the information provided by mean . Besides , there is currently no theoretical guarantee for Bayesian regret using the mean value instead of sampling : the posterior sampling lemma wo n't work if the agent simply uses mean , and it is hard to derive bounds directly on $ \\Delta_k $ in the paper ( we only need to analyze $ \\tilde { \\Delta } _k $ if we can use the posterior sampling algorithm ) . ) Typos : we have fixed the typos and add some definitions . Thanks for pointing out ! Please let us know if you have further questions about our response . We are very happy to have further discussions !"}, "1": {"review_id": "asLT0W1w7Li-1", "review_text": "The paper proposes a model-based posterior sampling algorithms with regret guarantees when the model is assumed to be drawn from a distribution randomly . The authors also provide numerical evaluations of the proposed method . - The contribution of this work as theoretical work is limited . There is no study on fundamental limit . In addition , the performance guarantee seems worse than existing ones , although a fair comparison might be unavailable due to different technical assumes . However , the authors do not provide numerical comparison to existing algorithms with performance guarantee . - It is not possible to assess the contribution from numerical comparison . There is no description on the hyperparameter selection of other algorithms ( PETS , MDPO , SAC , ... ) . Hence , it is not reproducible as well . - The definition of $ BayesRegret $ seems incorrect as it takes $ M^ * $ as input argument . The authors need to describe what they mean by the expectation in eq . ( 3 ) .In my understanding , $ BayesRegret $ should take hyper-parameter to generate $ M^ * $ as input , while $ Regret $ taking a random incidence of $ M^ * $ as input . - I do n't understand the meaning of regret bound $ \\tilde { O } ( H^ { 3/2 } d_\\phi T ) $ for non-linear case as regret of any algorithm is upper-bounded by $ R_ { max } T $ .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and detailed comments ! We respectfully disagree that our theoretical guarantee is worse than existing ones with no fundamental limit . We have responded to each point as below . Please let us know if you have questions or suggestions . * * Re 1 : Performance guarantee compared with previous works * * By `` worse than existing ones '' , are you referring to regret bounds in tabular RL , which is $ O ( H\\sqrt { SAT } ) $ ? ( The study of previous works on tabular spaces in both frequenist and Bayesian settings are also mentioned in the introduction and related work . ) Note that our analysis focuses on * * continuous state-action spaces * * , where $ S $ and $ A $ can go to infinity . The study of previous results on frequentist regrets is described in our Related Work ( Section 2 ) , where we show that the best result so far is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ . The study of previous works on Bayesian regrets is mentioned in our Introduction ( Section 1 ) , where we show that previous works fail to provide an exact dependency on episode length $ H $ . A detailed explanation for theoretical improvements can be found in our general comments on the theoretical novelty above ( * * https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK * * ) . Our Bayesian regret is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ , which is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces . our result also matches the best-known frequentist regret in continuous spaces ( Zanette et al . ( 2020 ) , described in related work ) , where they have also proved that it matches the lower bound for this setting . However , their algorithm is based on optimization over an upper confidence set , which is computationally prohibitive , while posterior sampling algorithm only require optimizing a single MDP . * * Re 2 : Hyperparameters and other experimental details for reproducibility * * Thanks for your suggestion ! We have added experimental details for PETS , MBPO , SAC , and DDPG in Appendix 2 . Hyperparameters for our experiments are also added for reproducibility . * * Re 3 : Definition for BayesRegret * * Thank you for pointing it out ! It is a typo : in $ BayesRegret $ it should be $ \\phi $ instead of $ M^ * $ , where $ \\phi $ is the prior distribution of $ M^ * $ . We follow the same definition of $ BayesRegret $ and $ Regret $ in Osband & Van Roy ( 2017 ) Section 2 . Here the expectation is taken with respect to the prior distribution of $ M $ . We have made the revision in the latest version . * * Re 4 : Regret bound typo in the abstract * * We sincerely apologize for the typo in the abstract ; it should be $ \\sqrt { T } $ as described in Section 4.2 . We have made the revision in the latest version . Please let us know if you have additional questions about our response . We are very happy to have further discussions ."}, "2": {"review_id": "asLT0W1w7Li-2", "review_text": "Pros -- * The paper proposes a method to balance exploration and exploitation in reinforcement learning problems whose transitions and rewards are assumed to be sampled from Gaussian processes , and provide a Bayesian regret bound . Also , the paper shows how the proposed approach can be implemented in practice using model predictive control . Cons -- * It is not clear what is the novelty of the theoretical results in this paper when compared to the regret bounds by Chowdhury & Gopalan ( 2019 ) , who provide both frequentist and Bayesian bounds when the transitions and rewards are in an RKHS or sampled according to a GP . The bound of Chowdhury & Gopalan ( 2019 ) seem to be polynomial in the horizon H ( their paper mention a $ HSA\\sqrt { T } $ bound in the particular case of finite MDPs ) , whereas the current paper says that `` H is still unbounded '' in their result . Hence , further clarification is needed regarding this point . * The method is claimed to be computationally tractable : \u201c it can be easily implemented by only optimizing a single sampled MDP \u201d . However , the sampled MDP is continuous , and solving a continuous MDP is hard in general . Experimentally , the paper proposes the cross entropy method ( CEM ) for planning : in this case , planning is not exact and the regret bound does not hold anymore . I believe this issue should be made clear in the introduction/related work section . * Although my main concern is the theoretical novelty , the experimental section can be improved : it would be interesting to compare the proposed approach to other strategies for exploration in deep RL , for instance * Bellemare et al . ( 2016 ) , Unifying Count-Based Exploration and Intrinsic Motivation * Tang et al . ( 2017 ) , # Exploration : A Study of Count-Based Exploration for Deep Reinforcement Learning * Azizzadenesheli et al . ( 2018 ) , Efficient Exploration through Bayesian Deep Q-Networks ; and also perform experiments in small simple environments that satisfy the assumptions to check if the regret is sublinear ( e.g.a continuous \u201c grid world \u201d with noisy transitions ) . * In the nonlinear case ( Section 4.2 ) it is not clear how to follow the steps of Yang & Wang ( 2019 ) to derive a Bayesian regret bound with feature representation , since their assumption is related to low-rank MDPs instead of Gaussian processes . In addition , it would be interesting to discuss how the model would be sampled ( in Algorithm 1 ) in this case . Suggestions & remarks * Introduction : mention which of the cited papers proves the $ H \\sqrt { SAT } $ upper bound on the Bayesian regret , clarify whether $ T $ is the number of episodes , or $ H $ times the number of episodes . * Some definitions are missing : * The linear kernel should be defined before Theorem 1 * $ M^k $ is not defined before appearing in Eq.5 * Increase the font size of the text in Figure 1 . * In Algorithm 1 , include what are the input parameters ( e.g. $ \\sigma_r , \\sigma_f $ ) . * Some suggestions for the proof : * Write the relation between $ \\Delta_k $ and $ \\tilde { \\Delta } _k $ . * Include ( possibly in the appendix ) more details about the arguments in the paragraph below Eq.9.For instance , there is an argument about a bound on the information gain , which is not defined in the paper . Also , it might be useful ( for the reader ) to restate ( in the appendix ) the results by Williams & Vivarelli ( 2000 ) , Srinivas et al . ( 2012 ) and Russo & Van Roy ( 2014 ) required for the proof . Typos * Abstract : $ T $ instead of $ \\sqrt { T } $ in the regret bound with feature representation * Page 9 : Definition of MBPO , it should be \u201c Model-Based Policy ... \u201d * Typo in integration limits in Eq.12 ( the $ \\mu'-\\mu $ at the bottom should be $ \\mu-\\mu ' $ ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review , suggestions , and detailed comments ! We respectfully disagree that our theoretical novelty is unclear ( see Re1 ) . And the model-free baselines you proposed are already outperformed by existing model-based algorithms for continuous state-action spaces ( see Re3 ) . * * Re 1 : Theoretical novelty compared to the regret bounds by Chowdhury & Gopalan ( 2019 ) , and other papers * * We first clarify that the regret bound in Chowdhury & Gopalan ( 2019 ) can be exponential in $ H $ in continuous spaces , and the $ \\tilde { O } ( HSA\\sqrt { T } ) $ bound in Chowdhury & Gopalan ( 2019 ) is for tabular MDPs . In continuous spaces , S and A are infinite , and we focus on * * continuous state-action spaces * * in our paper . Please check the general comments here for more details : * * https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK * * , where we show * * why the regret bound in Chowdhury & Gopalan ( 2019 ) can be exponential in $ H $ in continuous spaces in part 1.1 , and clarify our improvement with respect to $ H $ in part 1.2 * * . We also illustrate our improvement w.r.t.the dimension $ d $ in part 2 and highlight other differences between our proof and Chowdhury & Gopalan ( 2019 ) in part 3 . Note that * * The best-known frequentist regret bound for continuous state-action space is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ * * in Zanette et al . ( 2020 ) Theorem 1 . Our Bayesian regret is $ \\tilde { O } ( H^ { 3/2 } d\\sqrt { T } ) $ , which is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces , and it also matches the best-known frequentist regret ( Zanette et al . ( 2020 ) in our Related Work Section 2 ) . * * Re 2 : Clarification on computational tractability * * By computationally tractable , we are mainly comparing to Zanette et al . ( 2020 ) , which achieve the same frequentist bound as ours . However , they use the UCB ( upper confidence bound ) algorithm and need to optimize over an upper confidence set of continuous MDPs , which is computationally prohibitive . Although solving continuous MDPs is generally hard , Chua et al . ( 2018 ) already show that MPC with CEM optimizer can solve Gym and Mujoco tasks within a handful of trials . Thus we conduct experiments in the * * environments that MPC planning itself is shown sufficient to solve the MDPs * * as in Chua et al . ( 2018 ) . Using the same planning method , our algorithm outperforms theirs , which indicates the effectiveness of efficient exploration . Thank you very much for pointing out that the planning method is an approximate solution , and we have added this clarification in the introduction ."}, "3": {"review_id": "asLT0W1w7Li-3", "review_text": "This paper studies the model-based reinforcement learning . They propose a posterior sampling algorithm and provide a Bayesian regret guarantee . Under the assumption that the reward and transition functions are Gaussian processes with linear kernels , the regret bound is in the order of H^1.5 d sqrt { T } where H is the episode length . The dependence that regret is in polynomial of H is a nice property . However , this advantage seems to be obtained by the assumption of Gaussian processes with linear kernels . Besides , I have the following comments . 1 ) It seems that the result in Theorem 1 is quite straightforward from the results in Osband & Van Roy 2014 . Could you justify your contribution in this result . 2 ) What is MPC ? Suppose to be model predictive control ? It is not officially introduced in the context . 3 ) In terms of computational complexity , could you elaborate more on the computational complexity of each line ( component ) of the algorithm . For general cases , posterior sampling could also be expensive as there are no closed-form solutions , one may need to use MCMC method .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and detailed comments ! We respectfully disagree that our theoretical contribution is incremental . * * Re 1 : Our contribution in Theorem 1 compared to Osband & Van Roy ( 2014 ) * * Our bound enjoys lower order in terms of episode length $ H $ and dimensionality $ d $ compared to Osband & Van Roy ( 2014 ) . The improvement of $ H $ mainly comes from the use of our Lemma 1 ( which only make use of the noise assumption ) , instead of Gaussian processes with linear kernels . For a detailed analysis on our improvement of $ H $ and $ d $ , please refer to our general comments on theoretical novelty above ( https : //openreview.net/forum ? id=asLT0W1w7Li & noteId=rJqD4DMG8jK ) . Our result is NOT straightforward from results in previous works , since we provided an original Lemma 1 ( proved in appendix ) , to significant improve the order of $ H $ with the same assumption from the previous work , and the proof to use Lemma 1 to get Theorem 1 is also different from Osband & Van Roy ( 2014 ) . Our bound is the best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces , and it also matches the best-known frequentist regret ( Zanette et al . ( 2020 ) ) in continuous spaces as mentioned in our Related Work ( Section 2 ) . * * Re 2 : What is MPC * * MPC is model predictive control . Sorry for not citing the book in Section 5.2 ( it should be Camacho & Alba ( 2013 ) : `` Model predictive control '' . We have made a revision for this . Thank you for pointing out ! * * Re 3 : Computational complexity on each component of the algorithm * * The main cost of computation comes from MPC planning and posterior update of the Bayesian model . We use MPC action selection with a CEM optimizer as in Chua et al . ( 2018 ) , to solve a single sampled MDP . At each step in an episode , we need to calculate the predicted return from the current step up to $ \\tau $ steps ( which is the planning horizon , usually $ \\tau=30 $ ) . Using CEM , we generate $ M $ * $ \\tau $ actions to form $ M $ action sequences at each iteration , where $ M $ is the population size ( usually 500 ) , use the sampled MDP to generate random returns , and select several elite sequences to update the CEM distribution to use in next iteration ( the max iteration number is usually 5 ) . So in each episode , the complexity of computation from MPC is $ O ( \\tau MH ) $ . Overall , it enjoys a lower complexity than Chua et al . ( 2018 ) since we do not use an ensemble of different models . Posterior update : The matrix multiplication to calculate of matrix $ A $ in 5.1 is $ O ( d^2N ) $ , and the inverse of matrix $ A $ is $ O ( d^3 ) $ , where $ N $ is the number of data points , and $ d $ is the dimension of the last hidden layer of the neural network . In our experiments , we find forcing the last hidden layer dimension to be the same as the input dimension works well in our modeling , so the posterior update is not computationally expensive . Please let us know if you have additional questions . We are very happy to have further discussions ."}}