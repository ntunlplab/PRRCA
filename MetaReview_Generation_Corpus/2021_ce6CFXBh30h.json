{"year": "2021", "forum": "ce6CFXBh30h", "title": "Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning", "decision": "Accept (Poster)", "meta_review": "This work proposes the Federated Matching algorithm as a novel method to tackle the problems in federated learning. The paper is well-written and original, and it contributes to the state-of-the-art. ", "reviews": [{"review_id": "ce6CFXBh30h-0", "review_text": "AFTER READING THE AUTHORS ' REPLY , I HAVE CHANGED MY RATING TO 6 . Even though authors introduce an interesting approach for a problem that has many potential practical applications , the paper suffers from two main weaknesses : 1 ) it is poorly organized , which makes it very hard to follow 2 ) it lacks a compelling , real-world dataset In terms of paper organization , in this reviewer 's opinion , it may be beneficial to decouple and present in a serial manner the solutions to the two main scenarios ( i.e. , labels at client vs server ) . The parallel presentation makes the story harder to follow , as the reader has to keep switching context from one scenario to the other . The very long captions of Figures 2 & 3 are not helpful , and Section 4 comes a bit out of nowhere , given that the pseudo-code for the two proposed algorithms only appears in the appendix . A better approach would be to use the body of the paper for an illustrative running example and the pseudo-code of the algorithms , while relegating the details to the appendix . In terms of the empirical validation , it is disappointing to see that you are using synthetic datasets . The paper would greatly benefit from having at least one real world application domain in which the proposed approaches `` move the needle . '' There are far too many papers in which a novel approach does great on synthetic data without impacting the state-of-the-art results on real-world domains . Other : - caption of Figure 1 : the data is `` available '' rather than `` given '' to the local client ( twice , under both `` a ) '' and `` b ) '' ) - page 2 : you make references to Figure 6 a & b , but you mean Figure 1 a & b - page 3 : in line 3 of the 1st paragraph of 3.1 , you refer to `` D '' without defining it - please spell-check & proof-read the paper : - `` Cleint '' -- > `` Client '' in the header of Table 1 ; - `` manly '' -- > `` mainly '' on page 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your constructive comments . Please see the response to individual comments below : * * ( 1 ) In terms of paper organization , in this reviewer 's opinion , it may be beneficial to decouple and present in a serial manner the solutions to the two main scenarios ( i.e. , labels at client vs server ) . The parallel presentation makes the story harder to follow , as the reader has to keep switching context from one scenario to the other . A better approach would be to use the body of the paper for an illustrative running example and the pseudo-code of the algorithms , while relegating the details to the appendix . * * - Thank you for your suggestions , and we have completely reorganized our paper , * * sequentially introducing the two main scenarios as suggested * * . We separated the descriptions of the two different scenarios into two subsequent sections ( Section 4 and Section 5 of the revision ) , to minimize context-switching between them . We have included illustrative running examples as well as the pseudo-codes of the algorithms for each section , in the revision . We believe that the paper has a largely improved organization after revising it according to your comment . * * ( 2 ) The very long captions of Figures 2 & 3 are not helpful . * * - We have * * split the Figure 3 * * into Figure 3 and Figure 4 in the revision , as introduce the two scenarios in two separate sections . Thus the length of the corresponding captions are also significantly reduced , which we believe is more readable than the ones in the previous version of the paper . * * ( 3 ) Section 4 comes a bit out of nowhere , given that the pseudo-code for the two proposed algorithms only appears in the appendix . * * - We have included the pseudo-codes of the algorithms in the Appendix due to page limit , but have included them back into the main paper ( Algorithm 1 in page 5 , Algorithm 2 in page 6 ) . * * ( 4 ) In terms of the empirical validation , it is disappointing to see that you are using synthetic datasets . The paper would greatly benefit from having at least one real world application domain in which the proposed approaches `` move the needle . '' There are far too many papers in which a novel approach does great on synthetic data without impacting the state-of-the-art results on real-world domains . * * - We agree with your point that validation on the real-world datasets is important . However , please understand that it is difficult for Academic researchers to secure publicly available real-world datasets . For this reason , the majority of federated learning algorithms validate on such synthetic datasets , and we believe that our evaluation is fair when compared with the existing works . Using publicly available data is also beneficial for reproducibility . - Nevertheless , we searched and found [ * * COVID-19 RADIOGRAPHY DATABASE * * ] ( https : //www.kaggle.com/tawsifurrahman/covid19-radiography-database ) which is a real-world dataset that consists of X-ray images from COVID and non-COVID patients . The COVID-19 dataset contains 219 X-ray images from the patients diagnosed of COVID-19 , 1341 images from normal ( healthy ) patients , and 1341 images from patients diagnosed of viral pneumonia . We use 10 clients with a fraction of 1.0 ( communication rate ) . We use 5 labeled examples per class for each client , leaving the rest of the image as unlabeled . We find this setting to be realistic as the datasets are , since we may not not have skilled radiologists that can fully label the X-ray images taken at the local hospitals . We compared our method against baselines which naively combine semi-supervised learning and federated learning models during training 100 rounds . The results are as follow : |COVID-19 Radiography Dataset ||| | : - : | : :| : - : | | - | Labels-at-Client | Labels-at-Server | | Methods | Acc . ( % ) | Acc . ( % ) | | FedProx-UDA | 74.24 \u00b1 0.25 | 80.11 \u00b1 0.18 | | FedProx-FixMatch | 70.02 \u00b1 0.28 | 72.15 \u00b1 0.14 | | * * FedMatch ( Ours ) * * | * * 78.67 \u00b1 0.23 * * | * * 84.32 \u00b1 0.11 * * | - As shown in the above table , our method consistently outperforms all base models with large margins ( around 4 % p-10 % p ) in both scenarios . We further provide test accuracy curves in Figure 8 of the Appendix . In the figure , our method trains faster than the base models and is more stable . We believe that these additional experimental results further strengthen our paper , and thank you for your suggestion . * * ( 5 ) Others ( typos ) * * - Thank you for the helpful suggestion . We have reflected all your comments regarding typos , incorrect references , caption descriptions , etc . ) in the revision ."}, {"review_id": "ce6CFXBh30h-1", "review_text": "In many real-world applications , local data are not always well labeled or fully labeled , and most of them are unlabeled . This paper introduced a novel learning paradigm , Federated Semi-Supervised Learning ( FSSL ) , to handle the federated learning from distributed and partially labeled data within the clients . Under this paradigm , the paper studies two different scenarios where partly labeled data appear on client nodes and merely appear on the sever node . In the introduction , the authors clearly presented the motivations of introducing FSSL and showed the differences between the proposed method and some existing ones . In the FSSL , the key technique is Federated Matching ( FedMath ) , which learns inter-client consistency between clients , and decomposes parameters to reduce both interference between supervised and unsupervised tasks , and communication cost . The Inter-Client Consistency Loss was most directly taken from some existing method . Thus , the critical innovation is the Decomposition of parameters , which has several benefits to the learned models . Basically , the proposed method is intuitively and technically sound . The authors conducted a batch of experiments to evaluate their proposed method on three different tasks under both labels-at-client and labels-at-server , compared with some baselines . The experimental results are positive . However , there are some minor issues in the experiment section . The authors mentioned that the proposed model can work under the scenario that some labels are not correct ( noisy labels ) . However , this point was not verified in the experiment section at different error labeling rates . It is shown that FedProx-UDA/FixMatch degrade when the labeled data increases from 5 to 10 in Figure 6 ( d ) . What causes this degradation and how the proposed method avoid it ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your time and effort in reviewing our paper . We respond to your comments below : ( 1 ) There are some minor issues in the experiment section . The authors mentioned that the proposed model can work under the scenario that some labels are not correct ( noisy labels ) . However , this point was not verified in the experiment section at different error labeling rates . - We double-checked our paper , but we were not able to find a scenario of noisy labels that you pointed out . Could you let us know what line numbers you refer to ? We only introduce two realistic scenarios , which are labels-at-client and labels-at-server scenarios . 1 . * * Labels-at-Client * * scenario : clients learn on both labeled and unlabeled data , while the global server only aggregates learned knowledge from the clients . 2 . * * Labels-at-Server * * scenario : clients learn on unlabeled data , while the global server learns on labeled data . - The proposed scenario with noisy labels seems highly practical , but seems like an orthogonal problem to the federated semi-supervised learning problem we tackle . Yet , we expect our method to be more robust than naive baselines since we have inter-client consistency which pseudo-labels by maximizing agreement . We will evaluate on this scenario if time allows . ( 2 ) It is shown that FedProx-UDA/FixMatch degrade when the labeled data increases from 5 to 10 in Figure 6 ( d ) . What causes this degradation and how the proposed method avoid it ? - This is an insightful comment . The base models suffer from catastrophic forgetting of the knowledge learned on labeled data , as they learn on unlabeled data with pseudo-labels . We conjecture that when the labeled data is small , unlabeled becomes relatively important as we can obtain limited information from labeled data , while it is less effective and even harmful when the amount of labeled data is sufficient . Our FedMatch on the other hand is not affected by the ratio of labeled and unlabeled samples , since it performs * * disjoint learning * * of parameters for labeled and unlabeled data , with * * parameter decomposition * * . - We provide further experimental evidence of our conjecture by additionally experimenting on data with different number of labels per class , using our method without the decomposition technique . Below are the new results : | Number of Labels per Class | 1 | 5 | 10 | 20 | | : :| : -- : | : -- : | : -- : | : -- : | | Methods | Acc . ( % ) | Acc . ( % ) | Acc . ( % ) | Acc . ( % ) | | FedProx-UDA | 31.95 | 47.45 | 41.4 | 47.15 | | FedProx-FixMatch | 30.01 | 47.2 | 34.25 | 44.5 | | FedMatch w/o Decomposition | * * 37.7 * * | 47.51 | 51.15 | 62.7 | | FedMatch | 37.65 | * * 54.5 * * | * * 60.65 * * | * * 66.1 * * | - As shown in the above table , FedMatch without decomposition underperforms FedMatch with decomposition when the number of labels per class increases from 5 to 10 ( around 3.x % p ) than from 1 to 5 ( around 9.x % p ) or from 10 to 20 ( 10.x % p ) , similarly to baselines . - Contrarily , our method with the decomposition technique shows consistent performance improvements with any number of labeled samples . These results can be clear evidence that our decomposition technique is effective in preserving reliable knowledge from labeled data . - We believe that this result will further strengthen our paper as it is another evidence that shows the effectiveness of disjoint learning , and have included it in the Section C.2 of the Appendix , in the revision . Thank you for the insightful comment ."}, {"review_id": "ce6CFXBh30h-2", "review_text": "This paper studies the problem of learning a joint model for different local data sets . Each model is trained individually based on each individual local data set , and the inference is performed by regularizing the results of different models . The main challenges come from the facts that each local data set belongs to different client hence the data is not shared and the labeled data in some local data set may be not sufficient . To utilize the unlabeled and labeled data , the paper proposes using unsupervised , semisupervised and supervised learning models . To incrementally training the model with different types of data , authors propose to decompose the model parameters as supervised and unsupervised model . The description of the problem is complicated and little confusing . The problem seems interesting , but one of my main main concerns is that I do not think authors solve the original challenge . The original challenge is that clients do not want to share data for model training . But the method proposed utilizes all the local data sets to learn the model . My another main concern is that the novel contribution of this work is not strong . My impression of the method proposed in this work is pretraining the model with labeled data and fine-tuning it with unlabeled data with constraints to make the fine-tuned model as close as the pretrained model .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We sincerely thank you for your time and effort in reviewing our paper . We believe that your main concerns are based on misunderstandings of our paper . Please see the response to individual comments below : The problem seems interesting , but one of my main main concerns is that I do not think authors solve the original challenge . The original challenge is that clients do not want to share data for model training . But the method proposed utilizes all the local data sets to learn the model . - This is a critical misunderstanding since in our framework , the clients * * do not share the local data * * with the others at all , and thus there is no * * compromise of data privacy * * at the local clients . Could you elaborate why you thought this would be the case ? What are shared instead in our framework , are the * * parameters * * learned on the local data , as with the conventional federated learning setting . Please see Figure 3 to see how the parameters are communicated across clients . My another main concern is that the novel contribution of this work is not strong . - We believe that you may have missed our contributions on the methodology part . Besides the unique setting we consider for federated semi-supervised learning ( `` Labels-at-Server '' setting ) , we propose both * * inter-client consistency loss * * which makes the prediction across models at multiple clients to be consistent , and the * * parameter decomposition * * for disjoint learning on labeled and unlabeled data . Both are our unique technical contributions that can effectively solve the federated semi-supervised learning problem in the two practical scenarios we described . Please see the second bullet point in the contributions we list in Page 2 , and Section 3.1 ( Inter-client Consistency Loss , Parameter Decomposition for Disjoint Learning ) for more detailed discussions of the contributions . My impression of the method proposed in this work is pretraining the model with labeled data and fine-tuning it with unlabeled data with constraints to make the fine-tuned model as close as the pretrained model . - This is another critical misunderstanding since our model * * does not fine-tune the model pretrained * * on the labeled data , on unlabeled data . The * * fine-tuning approach you describe is rather done by the baseline federated learning models * * ( FedAvg * SL , FedProx * UDA , FedProx * FixMatch ) , which result in severe performance degeneration since they do not prevent the model from forgetting what they have learned about the labeled data ( Please see Figure 5 ( b ) ) . On the other hand , our method does not result in any performance degeneration as it keeps the training of the parameter for labeled data to be * * completely disjoint * * from the unlabeled one with parameter decomposition ( Figure 5 ( b ) , FedMatch ( Ours ) ) , instead of finetuning the model learned on labeled data , with unlabeled data . - Thus there is * * no pretrained model or fine-tuning * * done with our model in neither of the two scenarios . In the Labels-at-Client scenario , both labeled and unlabeled data is generated locally , and the model learns on the two types of data concurrently . There is no fine-tuning happening in the Labels-at-Server scenario either , since the training for the labeled data and unlabeled data is also done at the same time Moreover , with our disjoint learning that allows to learn the parameters for the unlabeled data separately from the parameters for the labeled data , there is almost no interference across the learning for two different types of data ( Please see Figure 3 and Page 5 ) . We hope that these comments clarify your concerns regarding the data privacy preservation and the technical novelty of our work . Please let us know if there are any other concerns ."}, {"review_id": "ce6CFXBh30h-3", "review_text": "The authors propose a new semi-supervised federated learning algorithm ( FedMatch ) . Two scenarios are studies : 1 ) label-at-client ( labeled and unlabeled data are at client ) . 2 ) label-at-server ( labels are at server and unlabeled data are on client ) . The authors propose a disjoint learning which decomposes the model \\theta into supervised \\sigma and unsupervised $ \\psi $ such that $ \\theta = \\sigma + \\psi $ . the model is trained , in an alternating manner , to minimize the two loss functions for the supervised eq ( 4 ) and unsupervised model components eq ( 5 ) with inter-client consistency . The experimental results show an improvement w.r.t.to SOTA in terms of performance . The paper is sometime difficult and its clarity could be improved . I would consider the novelty of the method , from machine learning prospective , to be somewhat borderline as it uses known elements and adapt them to the introduced learning scenarios but overall the proposed solution is interesting as it seems to work well in practice . some detail comments : - $ \\pi $ in one of the terms in the norm needs to be removed in inter-class consistency loss ||p \u03b8 ( y|\u03c0 ( u ) ) \u2212 p \u03b8 ( y|\u03c0 ( u ) ) || - The disjoint learning scenario was not well introduced in paragraph `` Parameter Decomposition for Disjoint Learning '' - a missing section reference in ( see section ? ? ) . - Related to the reduction of communication costs : the bit-encoding of the model difference between the server and client is not clarified , i.e. , for example if the difference is coded using 64 bits and also the same encoding for the models there is no gain in communication . - In Table 1 , the bold should be indicative of the best method per column", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank you for your time and effort in reviewing our paper . We respond to individual comments below : ( 1 ) The paper is sometime difficult and its clarity could be improved - To further improve the clarity of our paper , we have completely reorganized our paper by sequentially introducing the two different scenarios ( `` Labels-at-Client '' and `` Labels-at-Server '' ) rather than elaborating on them simultaneously . - We have also corrected the typos and minor errors you pointed out in the revision ( missing reference , typos , some unclear descriptions ) . We will be grateful if you check our revision for the updates . ( 2 ) I would consider the novelty of the method , from machine learning prospective , to be somewhat borderline as it uses known elements and adapt them to the introduced learning scenarios but overall the proposed solution is interesting as it seems to work well in practice . '' * We appreciate that you find our work interesting . We would like to recap our contributions , as we believe that our work is highly novel in multiple aspects : - * * 1 ) Novel problem : * * We believe that our problem setting is highly novel , and is not a straightforward extension of semi-supervised learning in a federated learning setting , since `` Labels-at-Server '' scenario , where the labels are only available at the server , while clients only receive unlabeled data , is a unique but a realistic setting for federated learning . - * * 2 ) Inter-client consistency : * * The inter-client consistency term is novel since existing approaches enforce consistency across predictions on the perturbations of the same sample ( generated using data augmentation ) , while we enforce consistency across predictions from * * multiple models * * . Further , we do not straightforwardly enforce consistency across all models learned at all clients , since they may learn on heterogeneous data . Rather , we consider the * * task relevance * * to find the models that are helpful for a given task , by * * embedding the models based on their functional similarities * * and retrieving the nearest neighbors in the function space ( Please see the last paragraph of Inter-Client Consistency part in the methodology ) . - * * 3 ) Disjoint learning : * * To our knowledge , our is the first work that * * decomposes the parameters for labeled and unlabeled data * * for semi-supervised learning , and this has effectively * * prevented * * the unreliable knowledge from pseudo-labels from * * negatively affecting * * the reliable knowledge captured with labeled data . We believe that this is an important contribution , since catastrophic forgetting of the knowledge of labeled data is more severe in federated learning scenarios , where the models train on sequential data rather than on batch data . The proposed disjoint learning also provides a natural solution to `` Labels-at-Server '' scenario , where we need to combine knowledge of the labeled data learned at the server , with the knowledge of the unlabeled data learned at the clients . ( 3 ) Detailed Comments * We thank you so much for the detailed and helpful comments . We have reflected the corrections ( missing reference , typos , some unclear descriptions ) in the * * revision * * as follows : - We removed \u03c0 in one of the terms as you mentioned - We added detailed algorithms and corresponding illustrative running examples to complement the explanation of `` Parameter Decomposition for Disjoint Learning '' - We corrected a missing section reference that you mentioned . - Regarding the reduction of communication costs , it is important that we reduce the amount of information that needs to be sent between both server and client . The actual bit-level compression techniques are implementation problems , which are beyond our research topic . However , we added this assumption into our new revision . - We bold our methods , since the SL ( Supervised Learning ) models are not Federated Semi-Supervised Learning ( FSSL ) algorithms . We include them as our upper bound since they learn with full labels . We rather emphasize \u201c SL models learn with full labels \u201c in captions of both Table 1 and 2 ."}], "0": {"review_id": "ce6CFXBh30h-0", "review_text": "AFTER READING THE AUTHORS ' REPLY , I HAVE CHANGED MY RATING TO 6 . Even though authors introduce an interesting approach for a problem that has many potential practical applications , the paper suffers from two main weaknesses : 1 ) it is poorly organized , which makes it very hard to follow 2 ) it lacks a compelling , real-world dataset In terms of paper organization , in this reviewer 's opinion , it may be beneficial to decouple and present in a serial manner the solutions to the two main scenarios ( i.e. , labels at client vs server ) . The parallel presentation makes the story harder to follow , as the reader has to keep switching context from one scenario to the other . The very long captions of Figures 2 & 3 are not helpful , and Section 4 comes a bit out of nowhere , given that the pseudo-code for the two proposed algorithms only appears in the appendix . A better approach would be to use the body of the paper for an illustrative running example and the pseudo-code of the algorithms , while relegating the details to the appendix . In terms of the empirical validation , it is disappointing to see that you are using synthetic datasets . The paper would greatly benefit from having at least one real world application domain in which the proposed approaches `` move the needle . '' There are far too many papers in which a novel approach does great on synthetic data without impacting the state-of-the-art results on real-world domains . Other : - caption of Figure 1 : the data is `` available '' rather than `` given '' to the local client ( twice , under both `` a ) '' and `` b ) '' ) - page 2 : you make references to Figure 6 a & b , but you mean Figure 1 a & b - page 3 : in line 3 of the 1st paragraph of 3.1 , you refer to `` D '' without defining it - please spell-check & proof-read the paper : - `` Cleint '' -- > `` Client '' in the header of Table 1 ; - `` manly '' -- > `` mainly '' on page 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your constructive comments . Please see the response to individual comments below : * * ( 1 ) In terms of paper organization , in this reviewer 's opinion , it may be beneficial to decouple and present in a serial manner the solutions to the two main scenarios ( i.e. , labels at client vs server ) . The parallel presentation makes the story harder to follow , as the reader has to keep switching context from one scenario to the other . A better approach would be to use the body of the paper for an illustrative running example and the pseudo-code of the algorithms , while relegating the details to the appendix . * * - Thank you for your suggestions , and we have completely reorganized our paper , * * sequentially introducing the two main scenarios as suggested * * . We separated the descriptions of the two different scenarios into two subsequent sections ( Section 4 and Section 5 of the revision ) , to minimize context-switching between them . We have included illustrative running examples as well as the pseudo-codes of the algorithms for each section , in the revision . We believe that the paper has a largely improved organization after revising it according to your comment . * * ( 2 ) The very long captions of Figures 2 & 3 are not helpful . * * - We have * * split the Figure 3 * * into Figure 3 and Figure 4 in the revision , as introduce the two scenarios in two separate sections . Thus the length of the corresponding captions are also significantly reduced , which we believe is more readable than the ones in the previous version of the paper . * * ( 3 ) Section 4 comes a bit out of nowhere , given that the pseudo-code for the two proposed algorithms only appears in the appendix . * * - We have included the pseudo-codes of the algorithms in the Appendix due to page limit , but have included them back into the main paper ( Algorithm 1 in page 5 , Algorithm 2 in page 6 ) . * * ( 4 ) In terms of the empirical validation , it is disappointing to see that you are using synthetic datasets . The paper would greatly benefit from having at least one real world application domain in which the proposed approaches `` move the needle . '' There are far too many papers in which a novel approach does great on synthetic data without impacting the state-of-the-art results on real-world domains . * * - We agree with your point that validation on the real-world datasets is important . However , please understand that it is difficult for Academic researchers to secure publicly available real-world datasets . For this reason , the majority of federated learning algorithms validate on such synthetic datasets , and we believe that our evaluation is fair when compared with the existing works . Using publicly available data is also beneficial for reproducibility . - Nevertheless , we searched and found [ * * COVID-19 RADIOGRAPHY DATABASE * * ] ( https : //www.kaggle.com/tawsifurrahman/covid19-radiography-database ) which is a real-world dataset that consists of X-ray images from COVID and non-COVID patients . The COVID-19 dataset contains 219 X-ray images from the patients diagnosed of COVID-19 , 1341 images from normal ( healthy ) patients , and 1341 images from patients diagnosed of viral pneumonia . We use 10 clients with a fraction of 1.0 ( communication rate ) . We use 5 labeled examples per class for each client , leaving the rest of the image as unlabeled . We find this setting to be realistic as the datasets are , since we may not not have skilled radiologists that can fully label the X-ray images taken at the local hospitals . We compared our method against baselines which naively combine semi-supervised learning and federated learning models during training 100 rounds . The results are as follow : |COVID-19 Radiography Dataset ||| | : - : | : :| : - : | | - | Labels-at-Client | Labels-at-Server | | Methods | Acc . ( % ) | Acc . ( % ) | | FedProx-UDA | 74.24 \u00b1 0.25 | 80.11 \u00b1 0.18 | | FedProx-FixMatch | 70.02 \u00b1 0.28 | 72.15 \u00b1 0.14 | | * * FedMatch ( Ours ) * * | * * 78.67 \u00b1 0.23 * * | * * 84.32 \u00b1 0.11 * * | - As shown in the above table , our method consistently outperforms all base models with large margins ( around 4 % p-10 % p ) in both scenarios . We further provide test accuracy curves in Figure 8 of the Appendix . In the figure , our method trains faster than the base models and is more stable . We believe that these additional experimental results further strengthen our paper , and thank you for your suggestion . * * ( 5 ) Others ( typos ) * * - Thank you for the helpful suggestion . We have reflected all your comments regarding typos , incorrect references , caption descriptions , etc . ) in the revision ."}, "1": {"review_id": "ce6CFXBh30h-1", "review_text": "In many real-world applications , local data are not always well labeled or fully labeled , and most of them are unlabeled . This paper introduced a novel learning paradigm , Federated Semi-Supervised Learning ( FSSL ) , to handle the federated learning from distributed and partially labeled data within the clients . Under this paradigm , the paper studies two different scenarios where partly labeled data appear on client nodes and merely appear on the sever node . In the introduction , the authors clearly presented the motivations of introducing FSSL and showed the differences between the proposed method and some existing ones . In the FSSL , the key technique is Federated Matching ( FedMath ) , which learns inter-client consistency between clients , and decomposes parameters to reduce both interference between supervised and unsupervised tasks , and communication cost . The Inter-Client Consistency Loss was most directly taken from some existing method . Thus , the critical innovation is the Decomposition of parameters , which has several benefits to the learned models . Basically , the proposed method is intuitively and technically sound . The authors conducted a batch of experiments to evaluate their proposed method on three different tasks under both labels-at-client and labels-at-server , compared with some baselines . The experimental results are positive . However , there are some minor issues in the experiment section . The authors mentioned that the proposed model can work under the scenario that some labels are not correct ( noisy labels ) . However , this point was not verified in the experiment section at different error labeling rates . It is shown that FedProx-UDA/FixMatch degrade when the labeled data increases from 5 to 10 in Figure 6 ( d ) . What causes this degradation and how the proposed method avoid it ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your time and effort in reviewing our paper . We respond to your comments below : ( 1 ) There are some minor issues in the experiment section . The authors mentioned that the proposed model can work under the scenario that some labels are not correct ( noisy labels ) . However , this point was not verified in the experiment section at different error labeling rates . - We double-checked our paper , but we were not able to find a scenario of noisy labels that you pointed out . Could you let us know what line numbers you refer to ? We only introduce two realistic scenarios , which are labels-at-client and labels-at-server scenarios . 1 . * * Labels-at-Client * * scenario : clients learn on both labeled and unlabeled data , while the global server only aggregates learned knowledge from the clients . 2 . * * Labels-at-Server * * scenario : clients learn on unlabeled data , while the global server learns on labeled data . - The proposed scenario with noisy labels seems highly practical , but seems like an orthogonal problem to the federated semi-supervised learning problem we tackle . Yet , we expect our method to be more robust than naive baselines since we have inter-client consistency which pseudo-labels by maximizing agreement . We will evaluate on this scenario if time allows . ( 2 ) It is shown that FedProx-UDA/FixMatch degrade when the labeled data increases from 5 to 10 in Figure 6 ( d ) . What causes this degradation and how the proposed method avoid it ? - This is an insightful comment . The base models suffer from catastrophic forgetting of the knowledge learned on labeled data , as they learn on unlabeled data with pseudo-labels . We conjecture that when the labeled data is small , unlabeled becomes relatively important as we can obtain limited information from labeled data , while it is less effective and even harmful when the amount of labeled data is sufficient . Our FedMatch on the other hand is not affected by the ratio of labeled and unlabeled samples , since it performs * * disjoint learning * * of parameters for labeled and unlabeled data , with * * parameter decomposition * * . - We provide further experimental evidence of our conjecture by additionally experimenting on data with different number of labels per class , using our method without the decomposition technique . Below are the new results : | Number of Labels per Class | 1 | 5 | 10 | 20 | | : :| : -- : | : -- : | : -- : | : -- : | | Methods | Acc . ( % ) | Acc . ( % ) | Acc . ( % ) | Acc . ( % ) | | FedProx-UDA | 31.95 | 47.45 | 41.4 | 47.15 | | FedProx-FixMatch | 30.01 | 47.2 | 34.25 | 44.5 | | FedMatch w/o Decomposition | * * 37.7 * * | 47.51 | 51.15 | 62.7 | | FedMatch | 37.65 | * * 54.5 * * | * * 60.65 * * | * * 66.1 * * | - As shown in the above table , FedMatch without decomposition underperforms FedMatch with decomposition when the number of labels per class increases from 5 to 10 ( around 3.x % p ) than from 1 to 5 ( around 9.x % p ) or from 10 to 20 ( 10.x % p ) , similarly to baselines . - Contrarily , our method with the decomposition technique shows consistent performance improvements with any number of labeled samples . These results can be clear evidence that our decomposition technique is effective in preserving reliable knowledge from labeled data . - We believe that this result will further strengthen our paper as it is another evidence that shows the effectiveness of disjoint learning , and have included it in the Section C.2 of the Appendix , in the revision . Thank you for the insightful comment ."}, "2": {"review_id": "ce6CFXBh30h-2", "review_text": "This paper studies the problem of learning a joint model for different local data sets . Each model is trained individually based on each individual local data set , and the inference is performed by regularizing the results of different models . The main challenges come from the facts that each local data set belongs to different client hence the data is not shared and the labeled data in some local data set may be not sufficient . To utilize the unlabeled and labeled data , the paper proposes using unsupervised , semisupervised and supervised learning models . To incrementally training the model with different types of data , authors propose to decompose the model parameters as supervised and unsupervised model . The description of the problem is complicated and little confusing . The problem seems interesting , but one of my main main concerns is that I do not think authors solve the original challenge . The original challenge is that clients do not want to share data for model training . But the method proposed utilizes all the local data sets to learn the model . My another main concern is that the novel contribution of this work is not strong . My impression of the method proposed in this work is pretraining the model with labeled data and fine-tuning it with unlabeled data with constraints to make the fine-tuned model as close as the pretrained model .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We sincerely thank you for your time and effort in reviewing our paper . We believe that your main concerns are based on misunderstandings of our paper . Please see the response to individual comments below : The problem seems interesting , but one of my main main concerns is that I do not think authors solve the original challenge . The original challenge is that clients do not want to share data for model training . But the method proposed utilizes all the local data sets to learn the model . - This is a critical misunderstanding since in our framework , the clients * * do not share the local data * * with the others at all , and thus there is no * * compromise of data privacy * * at the local clients . Could you elaborate why you thought this would be the case ? What are shared instead in our framework , are the * * parameters * * learned on the local data , as with the conventional federated learning setting . Please see Figure 3 to see how the parameters are communicated across clients . My another main concern is that the novel contribution of this work is not strong . - We believe that you may have missed our contributions on the methodology part . Besides the unique setting we consider for federated semi-supervised learning ( `` Labels-at-Server '' setting ) , we propose both * * inter-client consistency loss * * which makes the prediction across models at multiple clients to be consistent , and the * * parameter decomposition * * for disjoint learning on labeled and unlabeled data . Both are our unique technical contributions that can effectively solve the federated semi-supervised learning problem in the two practical scenarios we described . Please see the second bullet point in the contributions we list in Page 2 , and Section 3.1 ( Inter-client Consistency Loss , Parameter Decomposition for Disjoint Learning ) for more detailed discussions of the contributions . My impression of the method proposed in this work is pretraining the model with labeled data and fine-tuning it with unlabeled data with constraints to make the fine-tuned model as close as the pretrained model . - This is another critical misunderstanding since our model * * does not fine-tune the model pretrained * * on the labeled data , on unlabeled data . The * * fine-tuning approach you describe is rather done by the baseline federated learning models * * ( FedAvg * SL , FedProx * UDA , FedProx * FixMatch ) , which result in severe performance degeneration since they do not prevent the model from forgetting what they have learned about the labeled data ( Please see Figure 5 ( b ) ) . On the other hand , our method does not result in any performance degeneration as it keeps the training of the parameter for labeled data to be * * completely disjoint * * from the unlabeled one with parameter decomposition ( Figure 5 ( b ) , FedMatch ( Ours ) ) , instead of finetuning the model learned on labeled data , with unlabeled data . - Thus there is * * no pretrained model or fine-tuning * * done with our model in neither of the two scenarios . In the Labels-at-Client scenario , both labeled and unlabeled data is generated locally , and the model learns on the two types of data concurrently . There is no fine-tuning happening in the Labels-at-Server scenario either , since the training for the labeled data and unlabeled data is also done at the same time Moreover , with our disjoint learning that allows to learn the parameters for the unlabeled data separately from the parameters for the labeled data , there is almost no interference across the learning for two different types of data ( Please see Figure 3 and Page 5 ) . We hope that these comments clarify your concerns regarding the data privacy preservation and the technical novelty of our work . Please let us know if there are any other concerns ."}, "3": {"review_id": "ce6CFXBh30h-3", "review_text": "The authors propose a new semi-supervised federated learning algorithm ( FedMatch ) . Two scenarios are studies : 1 ) label-at-client ( labeled and unlabeled data are at client ) . 2 ) label-at-server ( labels are at server and unlabeled data are on client ) . The authors propose a disjoint learning which decomposes the model \\theta into supervised \\sigma and unsupervised $ \\psi $ such that $ \\theta = \\sigma + \\psi $ . the model is trained , in an alternating manner , to minimize the two loss functions for the supervised eq ( 4 ) and unsupervised model components eq ( 5 ) with inter-client consistency . The experimental results show an improvement w.r.t.to SOTA in terms of performance . The paper is sometime difficult and its clarity could be improved . I would consider the novelty of the method , from machine learning prospective , to be somewhat borderline as it uses known elements and adapt them to the introduced learning scenarios but overall the proposed solution is interesting as it seems to work well in practice . some detail comments : - $ \\pi $ in one of the terms in the norm needs to be removed in inter-class consistency loss ||p \u03b8 ( y|\u03c0 ( u ) ) \u2212 p \u03b8 ( y|\u03c0 ( u ) ) || - The disjoint learning scenario was not well introduced in paragraph `` Parameter Decomposition for Disjoint Learning '' - a missing section reference in ( see section ? ? ) . - Related to the reduction of communication costs : the bit-encoding of the model difference between the server and client is not clarified , i.e. , for example if the difference is coded using 64 bits and also the same encoding for the models there is no gain in communication . - In Table 1 , the bold should be indicative of the best method per column", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank you for your time and effort in reviewing our paper . We respond to individual comments below : ( 1 ) The paper is sometime difficult and its clarity could be improved - To further improve the clarity of our paper , we have completely reorganized our paper by sequentially introducing the two different scenarios ( `` Labels-at-Client '' and `` Labels-at-Server '' ) rather than elaborating on them simultaneously . - We have also corrected the typos and minor errors you pointed out in the revision ( missing reference , typos , some unclear descriptions ) . We will be grateful if you check our revision for the updates . ( 2 ) I would consider the novelty of the method , from machine learning prospective , to be somewhat borderline as it uses known elements and adapt them to the introduced learning scenarios but overall the proposed solution is interesting as it seems to work well in practice . '' * We appreciate that you find our work interesting . We would like to recap our contributions , as we believe that our work is highly novel in multiple aspects : - * * 1 ) Novel problem : * * We believe that our problem setting is highly novel , and is not a straightforward extension of semi-supervised learning in a federated learning setting , since `` Labels-at-Server '' scenario , where the labels are only available at the server , while clients only receive unlabeled data , is a unique but a realistic setting for federated learning . - * * 2 ) Inter-client consistency : * * The inter-client consistency term is novel since existing approaches enforce consistency across predictions on the perturbations of the same sample ( generated using data augmentation ) , while we enforce consistency across predictions from * * multiple models * * . Further , we do not straightforwardly enforce consistency across all models learned at all clients , since they may learn on heterogeneous data . Rather , we consider the * * task relevance * * to find the models that are helpful for a given task , by * * embedding the models based on their functional similarities * * and retrieving the nearest neighbors in the function space ( Please see the last paragraph of Inter-Client Consistency part in the methodology ) . - * * 3 ) Disjoint learning : * * To our knowledge , our is the first work that * * decomposes the parameters for labeled and unlabeled data * * for semi-supervised learning , and this has effectively * * prevented * * the unreliable knowledge from pseudo-labels from * * negatively affecting * * the reliable knowledge captured with labeled data . We believe that this is an important contribution , since catastrophic forgetting of the knowledge of labeled data is more severe in federated learning scenarios , where the models train on sequential data rather than on batch data . The proposed disjoint learning also provides a natural solution to `` Labels-at-Server '' scenario , where we need to combine knowledge of the labeled data learned at the server , with the knowledge of the unlabeled data learned at the clients . ( 3 ) Detailed Comments * We thank you so much for the detailed and helpful comments . We have reflected the corrections ( missing reference , typos , some unclear descriptions ) in the * * revision * * as follows : - We removed \u03c0 in one of the terms as you mentioned - We added detailed algorithms and corresponding illustrative running examples to complement the explanation of `` Parameter Decomposition for Disjoint Learning '' - We corrected a missing section reference that you mentioned . - Regarding the reduction of communication costs , it is important that we reduce the amount of information that needs to be sent between both server and client . The actual bit-level compression techniques are implementation problems , which are beyond our research topic . However , we added this assumption into our new revision . - We bold our methods , since the SL ( Supervised Learning ) models are not Federated Semi-Supervised Learning ( FSSL ) algorithms . We include them as our upper bound since they learn with full labels . We rather emphasize \u201c SL models learn with full labels \u201c in captions of both Table 1 and 2 ."}}