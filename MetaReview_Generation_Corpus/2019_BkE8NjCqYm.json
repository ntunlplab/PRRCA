{"year": "2019", "forum": "BkE8NjCqYm", "title": "(Unconstrained) Beam Search is Sensitive to Large Search Discrepancies", "decision": "Reject", "meta_review": "This paper examines a concept (also coined by the paper) of \"search discrepancies\" where the search algorithm behaves differently with large beam sizes. It then proposes heuristics to help prevent the model from performing worse when the size of the beam is increased.\n\nI think there are some interesting insights in this paper with respect to how search works in modern neural models, but most reviewers (and me) were concerned by the heuristic approach taken to fix these errors. I still think that within a search paper, a clear separation between modeling errors and search errors is useful, and adding heuristics on top has a potential to making things more complicated down the road when, for example, we change our model or we change our training algorithm.\n\nIt would be nice if the nice insights in the paper could be turned into a more theoretically clean framework that could be re-submitted to a future conference.", "reviews": [{"review_id": "BkE8NjCqYm-0", "review_text": "This paper addresses issues with the beam search decoding algorithm that is commonly applied to recurrent models during inference. In particular, the paper investigates why using larger beam widths, resulting in output sequences with higher log-probabilities, often leads to worse performance on evaluation metrics of interest such as BLEU. The paper argues that this effect is related to \u2018search discrepancies\u2019 (deviations from greedy choices early in decoding), and proposes a constrained decoding mechanism as a heuristic fix. Strengths: - The reduction in performance from using larger beam widths has been often reported and needs more investigation. - The paper views beam search decoding through the lens of heuristic and combinatorial search, and suggests an interesting connection with methods such as limited discrepancy search (Harvey and Ginsberg 1995) that seek to eliminate early \u2018wrong turns\u2019. - In most areas the paper is clear and well-written, although it may help to be more careful about explaining and / or defining terms such as \u2018highly non-greedy\u2019, \u2018search discrepancies\u2019 in the introduction. Weaknesses and suggestions for improvement: - Understanding: The paper does not offer much in the way of a deeper understanding of search discrepancies. For example, are search discrepancies caused by exposure bias or label bias, i.e. an artifact of local normalization at each time step during training, as suggested in the conclusion? Or are they actually a linguistic phenomenon (noting that English, French and German have common roots)? As there are neural network methods that attempt to do approximate global normalization (e.g. https://www.aclweb.org/anthology/P16-1231), there may be ways to investigate this question by looking at whether search discrepancies are reduced in these models (although I haven\u2019t looked deeply into this). - Evaluation: In the empirical evaluation, the results seem quite marginal. Taking the best performing beam size for the proposed method, and comparing the score to the best performing beam size for the baseline, the scores appear to be within around 1% for each task. Although the proposed method allows larger beam widths to be used without degradation during decoding, of course this is not actually beneficial unless the larger beam can improve the score. In the end, the evidence that search discrepancies are the cause of the problems with large beam widths, and therefore the best way to mitigate these problems, is not that strong. - Evaluation metrics and need for human evals: The limitations of automatic linguistic evaluations such as BLEU are well known. For image captioning, the SPICE (ECCV 2016 https://arxiv.org/abs/1607.08822) and CIDEr (CVPR 2015 https://arxiv.org/abs/1411.5726) metrics show much greater correlation with human judgements of caption quality, and should be reported in preference (or in addition) to BLEU. More generally, it is quite possible that the proposed fix based on constraining discrepancies could improve the generated output in the eyes of humans, even if this is not strongly reflected in automatic evaluation metrics. Therefore, it would be interesting to see human evaluations for the generated outputs in each task. - Rare words: The authors reference Koehn and Knowles\u2019 (2017) six challenges for NMT, which includes beam search decoding. One of the other six challenges is low-frequency words. However, the impact of the proposed constrained decoding approach on the generation of rare words is not explored. It seems reasonable that limiting search discrepancies might also further limit the generation of rare words. Therefore, I would like to suggest that an analysis of the diversity of the generated outputs for each approach be included in the evaluation. - Constrained beam search: There is a bunch of prior work on constrained beam search. For example, an algorithm called constrained beam search was introduced at EMNLP 2017 (http://aclweb.org/anthology/D17-1098). This is a general algorithm for decoding RNNs with constraints defined by a finite state acceptor. Other works have also been proposed that are variations on this idea, e.g. http://aclweb.org/anthology/P17-1141, http://aclweb.org/anthology/N18-1119). It might be helpful to identify these in the related work section to help limit confusion when talking about this \u2018constrained beam search\u2019 algorithm. Minor issues: - Section 3. The image captioning splits used by Xu et al. 2015 were actually first proposed by Karpathy & Li, \u2018Deep visual-semantic alignments for generating image descriptions\u2019, CVPR 2015, and should be cited as such. (Some papers actually refer to them as the \u2018Karpathy splits\u2019.) - In Table 4 it is somewhat difficult to interpret the comparison between the baseline results and the constrained beam search methods, because the best results appear in different columns. Bolding the highest score in every row would be helpful. Summary: In summary, improving beam search is an important direction, and to the best of my knowledge the idea of looking at beam search through the lens of search discrepancies is novel. Having said, I don't feel that this paper in it's current form contributes very much to our understanding of RNN decoding, since it is not clear if search discrepancies are actually a problem. Limiting search discrepancies during decoding has minimal impact on BLEU scores, and it seems possible that search discrepancies could just be an aspect of linguistic structure. I rate this paper marginally below acceptance, although I would encourage the authors to keep working in this direction and have tried to provide some suggestions for improvement. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed review . We will incorporate your comments and suggestions in the final version of the paper . We would like to address the reviewer concerns : - Understanding the discrepancy phenomenon and applicability to languages significantly different from English : We believe that the search discrepancies occur due to the combination of exposure and label bias as explained in our discussion . We provide a concrete example in Section 4.6 ( see our response for AnonReviewer 1 for a detailed explanation of this example ) . The reviewer raises a potential concern that the discrepancy phenomenon is actually a linguistic phenomenon associated with English or similar languages . We therefore perform an experiment of generating translations in a language that is significantly different than English : Chinese . We train and evaluate the convolutional translation model by Gehring et al . ( 2017 ) on the WMT'17 En-Zh dataset . We performed the analysis in the paper on this dataset and found similar trends ( results follow ) . Specifically : * The dataset exhibits significant performance degradation for large beam width * Our analysis of the frequency and size of the early discrepancies and the comparison between improved vs. degraded sequences yielded similar trends to the other languages * We used a gap-constrained beam search , tuned on a held-out validation set , and successfully eliminated the performance degradation . We will have results on the rank constraint soon . The results for the baseline vs. the discrepancy-constrained beam search are described in the following table : | Dataset | Method | Threshold | B=1 | B=3 | B=5 | B=25 | B=100 | B=250 | | En-Zh ( BLEU-4 ) | Baseline | | 15.20 | 17.47 | 17.68 | 16.48 | 9.44 | 6.28 | | En-Zh ( BLEU-4 ) | Constr . Gap | \\mathcal { M } =1.0 | 15.20 | 15.49 | 17.71 | 17.73 | 17.79 | 17.83 | - Evaluation metrics and need for human evals : Our work is done from a search perspective and is focused on analyzing , explaining , and eliminating the performance degradation in beam search with respect to a given evaluation metric . Previous work reporting the performance degradation are also based only on automatic evaluation . The reviewer raises a potential concern regarding using BLEU-4 to evaluate image captioning , as there are indication that CIDEr and SPICE are better correlated with human judgment . We therefore present results for these two metrics on the image captioning dataset ( COCO ) . Our analysis finds similar results to the ones found for BLEU-4 : * There is a significant performance degradation for the image captioning tasks , with respect to both CIDEr and SPICE * We used a gap-constrained beam search , tuned on a held-out validation set , and significantly reduced ( and almost eliminated ) the performance degradation . The results for the baseline vs. the discrepancy-constrained beam search are described in the following table : | Dataset | Method | Threshold | B=1 | B=3 | B=5 | B=25 | B=100 | B=250 | | COCO ( CIDEr ) | Baseline | | 0.974 | 1.018 | 1.005 | 0.953 | 0.946 | 0.945 | | COCO ( CIDEr ) | Constr . Gap | \\mathcal { M } =0.4 | 0.974 | 1.016 | 1.018 | 1.016 | 1.016 | 1.016 | | COCO ( SPICE ) | Baseline | | 18.13 | 18.54 | 18.43 | 17.76 | 17.68 | 17.64 | | COCO ( SPICE ) | Constr . Gap | \\mathcal { M } =0.45 | 18.13 | 18.41 | 18.44 | 18.43 | 18.43 | 18.43 | - Rare words : * One main problem with rare words is the limited size of vocabulary ( Koehn and Knowles , 2017 ) . We do not change the vocabulary size . In fact , our machine translation model is using BPE ( Sennrich et al. , 2016 ) to allow translation of rare words using subword units . * Regarding the frequency of rare words that are in the vocabulary , we are definitely reducing the frequency of such rare words compared to the large beams of the baseline : for example , `` copies '' are all sequences of rare words that we eliminate . However , we analyzed the occurrence of rare words that are also in the reference for En-De translation and found no significant difference between the baseline and our algorithm in that respect . Note that the baseline itself poorly represents rare words that are indeed in the reference . * For image captioning and summarization , the related problem of novel captions/summaries vs. ones from the training set is thoroughly discussed in the paper ( see Section 4.5 and Appendix B ) . - Constrained beam search : Thanks for pointing that out . We are familiar with these works but did not include them as they are not directly related ( they are addressing different problems such as forcing the inclusion of a selected token in the sequence ) . However , we agree with the reviewer that mentioning these works in the related work section will help reduce confusion with other constrained variants of beam search . We will add these to the related work section . Please let us know if you have further questions ."}, {"review_id": "BkE8NjCqYm-1", "review_text": " This work does extensive experiments on three different text generation tasks and shows the relationship between wider beam degradation and more and larger early discrepancies. This is an interesting observation but the reason behind the scene are still unclear to me. A lot of the statements in the paper lack of theoretical analysis. The proposed solutions addressing the beam discrepancies are effective, which further proves the relationship between beam size and early discrepancies. My questions/suggestions are as follows: * It\u2019s better to show the dataset statistics along with Fig1,3. So that readers know how much of test set have discrepancies in early steps. * It is not right to conduct your analysis on the test set. You have to be very clear about which results are from test set or dev set. * All the results with BLEU score must include the brevity penalty as well. It is very useful to analyze the length ratio changes between baseline, other methods, and your proposal. * The example in Sec. 4.6 is unclear to me, maybe you could illustrate it more clearly. * Your approaches eliminate the discrepancies along with the diversity with a wider beam. I am curious what if you only apply those constraints on early steps. * I suggest comparing your proposal to the word reward model in [1] since it is also about improving beam search quality. Your threshold-based method is also kind of word reward method. * In eq.2, what do you mean by sequence y \\in V? y is a sequence, V just a set of vocabulary. What do you mean by P (y|x;{y_0..y_t}). Why the whole sequence y is conditioned on a prefix of y? [1] Huang et al, \"When to Finish? Optimal Beam Search for Neural Text Generation\" 2017", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We will incorporate your comments and suggestions in the final version of the paper . We would like to address the reviewer questions and concerns : - The use of test set in our analysis : While the empirical results in Section 4 are on the test set , our algorithms ( discrepancy-constrained beam search variants ) are tuned only on a dev set ( with * no * information from the test set analysis ) . The reason why we present the empirical analysis in Section 4 on the test set is that we are focused on explaining the performance degradation that was previously reported on the test set . This analysis only provides a better understanding of the what is going on : no information from that analysis is later used in our algorithmic improvements . We did perform a similar analysis on the dev set and observed similar trends , however we thought presenting the test set is consistent with the previously reported results . In Section 5 , we propose two discrepancy-constrained variants of beam search . In order to tune these $ \\mathcal { M } $ and $ \\mathcal { N } $ that control the constraints , we used a held-out validation set , and then evaluated the performance on the test set ( we also repeat our analysis of search discrepancy and it can be compared to the one in Section 4 ) . Again , we would like to stress that no information from the test set were used to tune the algorithms ( if the phenomenon did not occur on the dev set , the tuning would not yield useful values that will eliminate the performance degradation in the test set ) . We agree with the reviewer that this was not completely clear in the paper and we will clarify it in our final version . - Length ratio changes : We specifically addressed the topic of length ratio changes in Appendix D , to demonstrate that the performance degradation on the baseline is not due to significant bias in length . Since the reviewer asked about the length ratio changes in our algorithms as well , we provide an updated table that also includes our discrepancy-constrained algorithms . Note that this is not the brevity term but it shows whether the length of the prediction has changed between the different beam widths and when comparing the baseline to our algorithms ( and allows us to analyze other metrics that are not BLEU ) . The table below includes the average length relative to the average length of the top performing baseline configuration ( marked with * ) . As we state in Appendix D , the performance degradation in the baseline is not due to significant change in the length ratio . We can also see that our algorithm keeps nearly the same length ratio across the different beam widths ( and is even marginally more stable than the baseline ) . + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | Dataset | Algorithm | B=1 | B=3 | B=5 | B=25 | B=100 | B=250 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | En-De | Baseline | 0.99 | 1.00 | 1.00 * | 1.00 | 0.99 | 0.98 | | En-De | Constr . Gap | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | | En-De | Constr . Rank | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | En-Fr | Baseline | 0.99 | 1.00 | 1.00 * | 1.00 | 0.99 | 0.91 | | En-Fr | Constr . Gap | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 | | En-Fr | Constr . Rank | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | Gigaword | Baseline | 1.03 | 1.00 * | 0.99 | 0.99 | 1.00 | 1.01 | | Gigaword | Constr . Gap | 1.03 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | | Gigaword | Constr . Rank | 1.03 | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | COCO | Baseline | 1.04 | 1.00 * | 0.99 | 0.98 | 0.98 | 0.98 | | COCO | Constr . Gap | 1.04 | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | | COCO | Constr . Rank | 1.04 | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+"}, {"review_id": "BkE8NjCqYm-2", "review_text": "Pros: - The paper generalizes upon past observations by Ott et al. that NMT models might decode \"copies\" (of the source sentence) when using large beam widths, which results in degraded results. In particular, the present paper observes similar shortcomings in two additional tasks (summarization and captioning), where decoding with large beam widths results in \"training set predictions.\" It's unclear if this observation is novel, but in any case the connection between these observations across NMT and summarization/captioning tasks is novel. - The paper draws a connection between the observed degradation and \"label bias\", whereby prefixes with a low likelihood are selected merely because they lead to (nearly-)deterministic transitions later in decoding. - The paper suggests two simple heuristics for mitigating the observed degradation with large beam widths, and evaluates these heuristics across three tasks. The results are convincing. - The paper is very well written. The analysis throughout the paper is easy to follow and convincing. Cons: - Although the analysis is very valuable, the quantitive impact of the proposed heuristics is relatively minor. Comments/questions: - In Eq. 2, consider using $v$ or $w$ for the max instead of overloading $y$. - To save space, you might compress Figure 1 into a single figure with three differently-styled bars per position that indicate the beam width (somewhat like how Figure 3 is presented). You can do this for Figure 2 as well, and these compressed figures could then be collapsed into a single row. - In Section 5, when describing the \"Discrepancy gap\" constraint, you say that you \"modify Eq. 3 to include the constraint\", but I suspect you meant that you modify Eq. 1 to include this constraint. - In Table 4, why didn't you tune $\\mathcal{M}$ and $\\mathcal{N}$ separately for each beam width?", "rating": "7: Good paper, accept", "reply_text": "We thank the review for the review and the comments . We will incorporate your comments and suggestions in the final version of the paper . Regarding the concern about the minor empirical improvement : This work is focused on studying the previously reported search phenomenon of performance degradation in beam search with large beams . We believe understanding search-related phenomena provides deeper insight into the neural decoding process and can help design better search algorithms . Our proposed variants of discrepancy-constrained beam search are meant to `` fix '' the performance based on our analysis , and the success in doing so validates our analysis ( together with our analysis of search discrepancies on the results of the discrepancy-constrained beam search ) . As such , we did not expect our methods to significantly outperform the best beam width but to eliminate the effect of the beam degradation and validate our analysis of the performance degradation phenomenon . Regarding your questions : - You are correct about the mistake in Section 5 : we do mean `` modify Eq.1 '' instead of `` modify Eq.3 '' .We will fix it . - We can definitely tune a different $ \\mathcal { M } $ and $ \\mathcal { N } $ for each beam width and we expect it to provide further improvement , however we preferred to show the robustness of a relatively simple tuning in effectively eliminating the performance degradation ( this is why we are also interested in the easier-to-tune rank constraint ) . This is , again , consistent with our motivation stated above of studying the performance degradation , explaining this phenomenon , and mitigating it . Please let us know if you have further questions . We also ask the reviewer to see the new results we report in the response to AnonReviewer3 on Chinese translation ( that shows the applicability of our analysis to languages that are significantly different than English ) and the success of our methods with respect to two alternative evaluation metrics in image captioning ."}], "0": {"review_id": "BkE8NjCqYm-0", "review_text": "This paper addresses issues with the beam search decoding algorithm that is commonly applied to recurrent models during inference. In particular, the paper investigates why using larger beam widths, resulting in output sequences with higher log-probabilities, often leads to worse performance on evaluation metrics of interest such as BLEU. The paper argues that this effect is related to \u2018search discrepancies\u2019 (deviations from greedy choices early in decoding), and proposes a constrained decoding mechanism as a heuristic fix. Strengths: - The reduction in performance from using larger beam widths has been often reported and needs more investigation. - The paper views beam search decoding through the lens of heuristic and combinatorial search, and suggests an interesting connection with methods such as limited discrepancy search (Harvey and Ginsberg 1995) that seek to eliminate early \u2018wrong turns\u2019. - In most areas the paper is clear and well-written, although it may help to be more careful about explaining and / or defining terms such as \u2018highly non-greedy\u2019, \u2018search discrepancies\u2019 in the introduction. Weaknesses and suggestions for improvement: - Understanding: The paper does not offer much in the way of a deeper understanding of search discrepancies. For example, are search discrepancies caused by exposure bias or label bias, i.e. an artifact of local normalization at each time step during training, as suggested in the conclusion? Or are they actually a linguistic phenomenon (noting that English, French and German have common roots)? As there are neural network methods that attempt to do approximate global normalization (e.g. https://www.aclweb.org/anthology/P16-1231), there may be ways to investigate this question by looking at whether search discrepancies are reduced in these models (although I haven\u2019t looked deeply into this). - Evaluation: In the empirical evaluation, the results seem quite marginal. Taking the best performing beam size for the proposed method, and comparing the score to the best performing beam size for the baseline, the scores appear to be within around 1% for each task. Although the proposed method allows larger beam widths to be used without degradation during decoding, of course this is not actually beneficial unless the larger beam can improve the score. In the end, the evidence that search discrepancies are the cause of the problems with large beam widths, and therefore the best way to mitigate these problems, is not that strong. - Evaluation metrics and need for human evals: The limitations of automatic linguistic evaluations such as BLEU are well known. For image captioning, the SPICE (ECCV 2016 https://arxiv.org/abs/1607.08822) and CIDEr (CVPR 2015 https://arxiv.org/abs/1411.5726) metrics show much greater correlation with human judgements of caption quality, and should be reported in preference (or in addition) to BLEU. More generally, it is quite possible that the proposed fix based on constraining discrepancies could improve the generated output in the eyes of humans, even if this is not strongly reflected in automatic evaluation metrics. Therefore, it would be interesting to see human evaluations for the generated outputs in each task. - Rare words: The authors reference Koehn and Knowles\u2019 (2017) six challenges for NMT, which includes beam search decoding. One of the other six challenges is low-frequency words. However, the impact of the proposed constrained decoding approach on the generation of rare words is not explored. It seems reasonable that limiting search discrepancies might also further limit the generation of rare words. Therefore, I would like to suggest that an analysis of the diversity of the generated outputs for each approach be included in the evaluation. - Constrained beam search: There is a bunch of prior work on constrained beam search. For example, an algorithm called constrained beam search was introduced at EMNLP 2017 (http://aclweb.org/anthology/D17-1098). This is a general algorithm for decoding RNNs with constraints defined by a finite state acceptor. Other works have also been proposed that are variations on this idea, e.g. http://aclweb.org/anthology/P17-1141, http://aclweb.org/anthology/N18-1119). It might be helpful to identify these in the related work section to help limit confusion when talking about this \u2018constrained beam search\u2019 algorithm. Minor issues: - Section 3. The image captioning splits used by Xu et al. 2015 were actually first proposed by Karpathy & Li, \u2018Deep visual-semantic alignments for generating image descriptions\u2019, CVPR 2015, and should be cited as such. (Some papers actually refer to them as the \u2018Karpathy splits\u2019.) - In Table 4 it is somewhat difficult to interpret the comparison between the baseline results and the constrained beam search methods, because the best results appear in different columns. Bolding the highest score in every row would be helpful. Summary: In summary, improving beam search is an important direction, and to the best of my knowledge the idea of looking at beam search through the lens of search discrepancies is novel. Having said, I don't feel that this paper in it's current form contributes very much to our understanding of RNN decoding, since it is not clear if search discrepancies are actually a problem. Limiting search discrepancies during decoding has minimal impact on BLEU scores, and it seems possible that search discrepancies could just be an aspect of linguistic structure. I rate this paper marginally below acceptance, although I would encourage the authors to keep working in this direction and have tried to provide some suggestions for improvement. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed review . We will incorporate your comments and suggestions in the final version of the paper . We would like to address the reviewer concerns : - Understanding the discrepancy phenomenon and applicability to languages significantly different from English : We believe that the search discrepancies occur due to the combination of exposure and label bias as explained in our discussion . We provide a concrete example in Section 4.6 ( see our response for AnonReviewer 1 for a detailed explanation of this example ) . The reviewer raises a potential concern that the discrepancy phenomenon is actually a linguistic phenomenon associated with English or similar languages . We therefore perform an experiment of generating translations in a language that is significantly different than English : Chinese . We train and evaluate the convolutional translation model by Gehring et al . ( 2017 ) on the WMT'17 En-Zh dataset . We performed the analysis in the paper on this dataset and found similar trends ( results follow ) . Specifically : * The dataset exhibits significant performance degradation for large beam width * Our analysis of the frequency and size of the early discrepancies and the comparison between improved vs. degraded sequences yielded similar trends to the other languages * We used a gap-constrained beam search , tuned on a held-out validation set , and successfully eliminated the performance degradation . We will have results on the rank constraint soon . The results for the baseline vs. the discrepancy-constrained beam search are described in the following table : | Dataset | Method | Threshold | B=1 | B=3 | B=5 | B=25 | B=100 | B=250 | | En-Zh ( BLEU-4 ) | Baseline | | 15.20 | 17.47 | 17.68 | 16.48 | 9.44 | 6.28 | | En-Zh ( BLEU-4 ) | Constr . Gap | \\mathcal { M } =1.0 | 15.20 | 15.49 | 17.71 | 17.73 | 17.79 | 17.83 | - Evaluation metrics and need for human evals : Our work is done from a search perspective and is focused on analyzing , explaining , and eliminating the performance degradation in beam search with respect to a given evaluation metric . Previous work reporting the performance degradation are also based only on automatic evaluation . The reviewer raises a potential concern regarding using BLEU-4 to evaluate image captioning , as there are indication that CIDEr and SPICE are better correlated with human judgment . We therefore present results for these two metrics on the image captioning dataset ( COCO ) . Our analysis finds similar results to the ones found for BLEU-4 : * There is a significant performance degradation for the image captioning tasks , with respect to both CIDEr and SPICE * We used a gap-constrained beam search , tuned on a held-out validation set , and significantly reduced ( and almost eliminated ) the performance degradation . The results for the baseline vs. the discrepancy-constrained beam search are described in the following table : | Dataset | Method | Threshold | B=1 | B=3 | B=5 | B=25 | B=100 | B=250 | | COCO ( CIDEr ) | Baseline | | 0.974 | 1.018 | 1.005 | 0.953 | 0.946 | 0.945 | | COCO ( CIDEr ) | Constr . Gap | \\mathcal { M } =0.4 | 0.974 | 1.016 | 1.018 | 1.016 | 1.016 | 1.016 | | COCO ( SPICE ) | Baseline | | 18.13 | 18.54 | 18.43 | 17.76 | 17.68 | 17.64 | | COCO ( SPICE ) | Constr . Gap | \\mathcal { M } =0.45 | 18.13 | 18.41 | 18.44 | 18.43 | 18.43 | 18.43 | - Rare words : * One main problem with rare words is the limited size of vocabulary ( Koehn and Knowles , 2017 ) . We do not change the vocabulary size . In fact , our machine translation model is using BPE ( Sennrich et al. , 2016 ) to allow translation of rare words using subword units . * Regarding the frequency of rare words that are in the vocabulary , we are definitely reducing the frequency of such rare words compared to the large beams of the baseline : for example , `` copies '' are all sequences of rare words that we eliminate . However , we analyzed the occurrence of rare words that are also in the reference for En-De translation and found no significant difference between the baseline and our algorithm in that respect . Note that the baseline itself poorly represents rare words that are indeed in the reference . * For image captioning and summarization , the related problem of novel captions/summaries vs. ones from the training set is thoroughly discussed in the paper ( see Section 4.5 and Appendix B ) . - Constrained beam search : Thanks for pointing that out . We are familiar with these works but did not include them as they are not directly related ( they are addressing different problems such as forcing the inclusion of a selected token in the sequence ) . However , we agree with the reviewer that mentioning these works in the related work section will help reduce confusion with other constrained variants of beam search . We will add these to the related work section . Please let us know if you have further questions ."}, "1": {"review_id": "BkE8NjCqYm-1", "review_text": " This work does extensive experiments on three different text generation tasks and shows the relationship between wider beam degradation and more and larger early discrepancies. This is an interesting observation but the reason behind the scene are still unclear to me. A lot of the statements in the paper lack of theoretical analysis. The proposed solutions addressing the beam discrepancies are effective, which further proves the relationship between beam size and early discrepancies. My questions/suggestions are as follows: * It\u2019s better to show the dataset statistics along with Fig1,3. So that readers know how much of test set have discrepancies in early steps. * It is not right to conduct your analysis on the test set. You have to be very clear about which results are from test set or dev set. * All the results with BLEU score must include the brevity penalty as well. It is very useful to analyze the length ratio changes between baseline, other methods, and your proposal. * The example in Sec. 4.6 is unclear to me, maybe you could illustrate it more clearly. * Your approaches eliminate the discrepancies along with the diversity with a wider beam. I am curious what if you only apply those constraints on early steps. * I suggest comparing your proposal to the word reward model in [1] since it is also about improving beam search quality. Your threshold-based method is also kind of word reward method. * In eq.2, what do you mean by sequence y \\in V? y is a sequence, V just a set of vocabulary. What do you mean by P (y|x;{y_0..y_t}). Why the whole sequence y is conditioned on a prefix of y? [1] Huang et al, \"When to Finish? Optimal Beam Search for Neural Text Generation\" 2017", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We will incorporate your comments and suggestions in the final version of the paper . We would like to address the reviewer questions and concerns : - The use of test set in our analysis : While the empirical results in Section 4 are on the test set , our algorithms ( discrepancy-constrained beam search variants ) are tuned only on a dev set ( with * no * information from the test set analysis ) . The reason why we present the empirical analysis in Section 4 on the test set is that we are focused on explaining the performance degradation that was previously reported on the test set . This analysis only provides a better understanding of the what is going on : no information from that analysis is later used in our algorithmic improvements . We did perform a similar analysis on the dev set and observed similar trends , however we thought presenting the test set is consistent with the previously reported results . In Section 5 , we propose two discrepancy-constrained variants of beam search . In order to tune these $ \\mathcal { M } $ and $ \\mathcal { N } $ that control the constraints , we used a held-out validation set , and then evaluated the performance on the test set ( we also repeat our analysis of search discrepancy and it can be compared to the one in Section 4 ) . Again , we would like to stress that no information from the test set were used to tune the algorithms ( if the phenomenon did not occur on the dev set , the tuning would not yield useful values that will eliminate the performance degradation in the test set ) . We agree with the reviewer that this was not completely clear in the paper and we will clarify it in our final version . - Length ratio changes : We specifically addressed the topic of length ratio changes in Appendix D , to demonstrate that the performance degradation on the baseline is not due to significant bias in length . Since the reviewer asked about the length ratio changes in our algorithms as well , we provide an updated table that also includes our discrepancy-constrained algorithms . Note that this is not the brevity term but it shows whether the length of the prediction has changed between the different beam widths and when comparing the baseline to our algorithms ( and allows us to analyze other metrics that are not BLEU ) . The table below includes the average length relative to the average length of the top performing baseline configuration ( marked with * ) . As we state in Appendix D , the performance degradation in the baseline is not due to significant change in the length ratio . We can also see that our algorithm keeps nearly the same length ratio across the different beam widths ( and is even marginally more stable than the baseline ) . + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | Dataset | Algorithm | B=1 | B=3 | B=5 | B=25 | B=100 | B=250 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | En-De | Baseline | 0.99 | 1.00 | 1.00 * | 1.00 | 0.99 | 0.98 | | En-De | Constr . Gap | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | | En-De | Constr . Rank | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | En-Fr | Baseline | 0.99 | 1.00 | 1.00 * | 1.00 | 0.99 | 0.91 | | En-Fr | Constr . Gap | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 | | En-Fr | Constr . Rank | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | Gigaword | Baseline | 1.03 | 1.00 * | 0.99 | 0.99 | 1.00 | 1.01 | | Gigaword | Constr . Gap | 1.03 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | | Gigaword | Constr . Rank | 1.03 | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+ | COCO | Baseline | 1.04 | 1.00 * | 0.99 | 0.98 | 0.98 | 0.98 | | COCO | Constr . Gap | 1.04 | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | | COCO | Constr . Rank | 1.04 | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | + -- -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- + -- -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -+ -- -- -- -+"}, "2": {"review_id": "BkE8NjCqYm-2", "review_text": "Pros: - The paper generalizes upon past observations by Ott et al. that NMT models might decode \"copies\" (of the source sentence) when using large beam widths, which results in degraded results. In particular, the present paper observes similar shortcomings in two additional tasks (summarization and captioning), where decoding with large beam widths results in \"training set predictions.\" It's unclear if this observation is novel, but in any case the connection between these observations across NMT and summarization/captioning tasks is novel. - The paper draws a connection between the observed degradation and \"label bias\", whereby prefixes with a low likelihood are selected merely because they lead to (nearly-)deterministic transitions later in decoding. - The paper suggests two simple heuristics for mitigating the observed degradation with large beam widths, and evaluates these heuristics across three tasks. The results are convincing. - The paper is very well written. The analysis throughout the paper is easy to follow and convincing. Cons: - Although the analysis is very valuable, the quantitive impact of the proposed heuristics is relatively minor. Comments/questions: - In Eq. 2, consider using $v$ or $w$ for the max instead of overloading $y$. - To save space, you might compress Figure 1 into a single figure with three differently-styled bars per position that indicate the beam width (somewhat like how Figure 3 is presented). You can do this for Figure 2 as well, and these compressed figures could then be collapsed into a single row. - In Section 5, when describing the \"Discrepancy gap\" constraint, you say that you \"modify Eq. 3 to include the constraint\", but I suspect you meant that you modify Eq. 1 to include this constraint. - In Table 4, why didn't you tune $\\mathcal{M}$ and $\\mathcal{N}$ separately for each beam width?", "rating": "7: Good paper, accept", "reply_text": "We thank the review for the review and the comments . We will incorporate your comments and suggestions in the final version of the paper . Regarding the concern about the minor empirical improvement : This work is focused on studying the previously reported search phenomenon of performance degradation in beam search with large beams . We believe understanding search-related phenomena provides deeper insight into the neural decoding process and can help design better search algorithms . Our proposed variants of discrepancy-constrained beam search are meant to `` fix '' the performance based on our analysis , and the success in doing so validates our analysis ( together with our analysis of search discrepancies on the results of the discrepancy-constrained beam search ) . As such , we did not expect our methods to significantly outperform the best beam width but to eliminate the effect of the beam degradation and validate our analysis of the performance degradation phenomenon . Regarding your questions : - You are correct about the mistake in Section 5 : we do mean `` modify Eq.1 '' instead of `` modify Eq.3 '' .We will fix it . - We can definitely tune a different $ \\mathcal { M } $ and $ \\mathcal { N } $ for each beam width and we expect it to provide further improvement , however we preferred to show the robustness of a relatively simple tuning in effectively eliminating the performance degradation ( this is why we are also interested in the easier-to-tune rank constraint ) . This is , again , consistent with our motivation stated above of studying the performance degradation , explaining this phenomenon , and mitigating it . Please let us know if you have further questions . We also ask the reviewer to see the new results we report in the response to AnonReviewer3 on Chinese translation ( that shows the applicability of our analysis to languages that are significantly different than English ) and the success of our methods with respect to two alternative evaluation metrics in image captioning ."}}