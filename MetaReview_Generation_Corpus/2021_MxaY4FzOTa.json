{"year": "2021", "forum": "MxaY4FzOTa", "title": "High-Capacity Expert Binary Networks", "decision": "Accept (Poster)", "meta_review": "## Summary \nThe paper advances the state of the art in training binary neural networks coming out to first place on ImageNet with a controlled computation budget. While any paper making a new record on ImageNet would be a serious candidate for acceptance, it is positive that this one achieves the goal by putting at work the mechanism of conditional computation, innovative for binary networks, and studying in a systematic and clear way how the network width and configuration can be varied while maintaining the computation budged.\n\n## Review Process and Decision\n\nThe paper was thoroughly discussed by reviewers from different aspects. Several weaker spots have been identified (see below and final reviews), but no critical issues that would indicate a necessity of a major revision. In the end, reviewers agreed on acceptance although in some cases they have decided to keep their original <=5 ranking to reflect the scientific value to them from a more global perspective.\nI think it is an example of well done modeling and experimental work: the work is very clear, uses sound methods, the experimental results are systematic and give interpretable evidence, which is in my experience is rather exceptional for the overall very empirical binary NNs field. I estimate high interest because of the concept of conditional computation put to work here and because of making a new record on ImageNet.\n\n## Details\n\n* Computation Cost\n\nIf such networks are to be deployed in low-power devices, the computation cost might need to be estimated more accurately. An example of such estimation is the work by\nDing et al. (2019) Regularizing Activation Distribution for Training Binarized Deep Networks,\nwhere the energy and area are estimated using information from a semiconductor process design kit.\n\nThere is indeed a number of floating point operations around the binary convolutions: first and last layers, experts, skip connections with scale factors and non-linearities. The latency and cost of these operations may not be negligible on target devices. In particular Ding et al. (2019)  argue that XNOR-Net architecture is 3 times more costly because of floating point scale factors.\nHowever the paper does a fair job in comparing in operation counts, which is a good proxy for many devices. The floating point computations needed in various places can be indeed further reduced to lower bit width, the research on quantization techniques shows this is possible and orthogonal to the contribution.\n\n* Novelty of grouped convolutions design and search\n\nThe work of Phan et al. (CVPR 2020): Binarizing MobileNet via Evolution-based Searching\nalso proposed to search for best grouped convolution under computation budget constraints (evolutionary search method).\nStrict budget constraint and merging results from different groups are somewhat novel and the prior work can be objectively contemporaneous.\n\n* Clarity\n\nClarity of the paper has been improved by the revision. One remaining mysticism is still about the gradient estimator for the experts. The paper states: \"we wish to back-propagate gradients for the non-selected experts\", \"allows meaningful gradients to flow to all experts during training\". The problem is that since $\\varphi(z)$ is binary one-hot on the forward pass, the gradient of the scalar product with $\\varphi$ in (2) results in that in the backward pass only the selected expert receives the training signal and by no means all of them. This is regardless of how the gradient is propagated through $\\varphi$. Maybe something is missing? I hope the authors can clarify in the final version. I do not consider it as a serious flow since this training scheme is not claimed as a contribution in any case.\n\nOne more point on the clarity: The paper claims that using experts increases the network representation power / capacity. While this seems logical, and follows the preceding work in real-valued NNs, the paper could provide additional evidence in terms of training performance of these models. Since the teacher with 76% accuracy is used in the distillation, I assume the training never reaches 100% training accuracy in any of the settings. Does the training accuracy improves with experts? This would be a helpful evidence for further work.\n\n* Search method\n\nThe paper was further criticized for that the manual search of the architecture is a step back from automated search methods (NAS, BATS). However these methods are themselves a relaxation of discrete choices (experts, if you like), that need to keep all possible configurations at the same time, which may be less stable and too costly for real architectures and datasets. The principles of gradient-based architecture search are not entirely clear and the resulting models coming out of these methods typically give no insights regarding good (intelligent) design choices. At present, the systematic exploration with analysis of tradeoffs conducted is seen to have advantages.\n\n\n", "reviews": [{"review_id": "MxaY4FzOTa-0", "review_text": "Summary : The paper addresses the problem of filling the gap between the performance of binary and real-valued networks . The authors propose a series of procedures to improve the model and representation capacity of binary neural networks . Different binary-network architectures are obtained through a new network-growing approach and compared . Strengths : Research on BNNs has a pretty short history if compared with studies on real-valued NN . It is a good idea to start translating some of the main tools from the standard NN literature to the binary setup . According to the authors ' claim , the proposed model greatly outperforms other existing and well-known binary networks . Weaknesses : The majority of the tools proposed for boosting the performance of binary networks are not new and have been already used in standard NNs . A discussion of the technical challenges associated with applying such tools to the binary setup would help understand the main contributions of the paper . The inclusion of real-valued experts seems to make the final network not completely binary and it is not clear whether the advantages of BNN ( e.g.the gain on computational costs ) are preserved . The network design step mainly consists of rearranging a series of pre-defined building blocks . It is not well explained how the architecture space is searched and how to interpret the results in Figure 2 . Questions : - is the cost of Conditional computing included in the total cost when the main results are claimed ( e.g.in `` Without increasing the computational budget of previous works , our method improves upon the state-of-the-art by 6 % '' ) ? More generally , when does the fixed number of BOPs include any training step ? - does the expert selection of the proposed method work better in the binary case than in real-valued networks ? - are the weights in each ` expert ' binary or real ? Is it fair to compare the obtained hybrid model with real-to-bin ? - has Grouped Convolution with a similar scaling factor been already used somewhere ? - what is the difference between the proposed gradient-approximation method and standard `` Straight-Through-Estimator '' ( STE ) ? -- I acknowledge that I read and appreciated the author 's response ( both parts ) . The authors ' reply mainly answers my questions , especially regarding the difference between applying the proposed techniques to the real and binary setups . I agree with all authors comments but would tend to confirm my overall score for two reasons : the architecture search method is not simply a block rearranging but looks more like a heuristic approach than a clear methodological contribution the proposed mixing of real and binary weights may preserve the advantages of fully binary networks but , again , makes less clear the net contribution of the paper from a more theoretical perspective However , as I recognize that the paper contains significant experimental results , I would be happy to support acceptance if all other reviewers agree on that .", "rating": "5: Marginally below acceptance threshold", "reply_text": "$ \\textbf { Q1.1 } $ : `` The majority of the tools proposed for boosting the performance of binary networks are not new and have been already used in standard NNs . A discussion of the technical challenges associated with applying such tools to the binary setup would help understand the main contributions of the paper . '' $ \\textbf { A1.1 } $ : First of all , due to the nature of binarization , architectural or methodological changes that benefit full-precision NNs do not result in accuracy improvements for BNNs . Direct application of techniques that work well for real-valued networks on Binary Networks has very often been unsuccessful . As an example , from our experiments , binarizing the MobileNet architecture performs poorly ( while in [ 1 ] it is shown that it does not converge at all ) . There are a multitude of reasons for this : in brief , this is due to the nature of binarization , where all values are discrete and restricted to 2 states only . This severely limits both the representational power of BNNs and causes issues during training since the true gradients are uninformative and their approximations make training harder to converge . Although conditional convs ( CondConv ) were previously used for standard NN , we show ( Section 5 , \u201c Comparison against CondConv and in Section 2.2 , second paragraph ) that directly applying the CondConv approach to binary networks is unsuitable . This can be attributed to the the fact that CondConv results in a linear combination of binary weights which is non-binary ( see also Section 2.2 , pp2 , last paragraph ) ; hence it requires a second binarization which as we show that doesn \u2019 t work well in practice . We further note that selecting the Best Expert as in our work vs a Mixture of Experts ( MoE ) as in CondConv is a different idea also requiring a different formulation and implementation ( the formulation of Section 4.1 is not required in CondConv which is in general simpler ) . Likewise , while grouped convs are also known , our method uses them in a novel way which is tailored to the nature of BNNs and is concretely supported by discussion and experimental evidence : First of all , we don \u2019 t simply use grouped convs : in Section 4.2 we propose grouped convs combined with a width expansion strategy ( which guarantees the same computational complexity ) followed by aggregation based on 1x1 convolutions . This is proposed for the first time . Secondly , this proposal is largely motivated by the Depth vs Width paragraph ( Section 4.3 , pp5 ) which discusses why and shows experimentally that increasing Width does not have the same impact as increasing Depth for BNNs ( as opposed to real-valued networks ) [ 4 ] . Thirdy , this is the first work on binary networks to jointly study and search the following : the optimal group size per block , its width , the network depth , the layer arrangement . These are non-straightforward observations , proposals and results which ( a ) have not been discussed in prior work , ( b ) are shown for the first time to have such a large impact on BNN accuracy . $ \\textbf { Q1.2 } $ : \u201c The inclusion of real-valued experts seems to make the final network not completely binary and it is not clear whether the advantages of BNN ( e.g.the gain on computational costs ) are preserved \u201d $ \\textbf { A1.2 } $ : The advantages are fully preserved . Their added cost is less than 0.001 % ( in terms of FLOPS ) of the total computing budget . In fact all top performing BNN methods reported in Table 5 do use small amounts of real valued operations reflected in the \u201c FLOPS \u201d column from Table 5 that quantifies the amount of full precision operations ."}, {"review_id": "MxaY4FzOTa-1", "review_text": "Paper summary : This paper proposes three approaches to improve the performance of BNNs . 1 ) Training a super-network ( ensemble of BNNs ) and dynamically selecting one BNNs to execute conditioned on input . 2 ) Widening the layers with group convolution to enhance the representational capacity . 3 ) Designing the architecture using EfficientNet considering the width , depth , groups and layer arrangement configurations simultaneously . The effectiveness of the paper has been justified on ImageNet classification but can be further strengthened . Strength : + The performance of the paper is promising . It will serve as a strong baseline for future works . + Using conditional dynamic routing to improve the BNNs capacity is interesting . Specifically , the paper proposes to learn a supernetwork ( i.e. , ensemble of several experts ) during training and dynamically select the path during testing , which enhances the capacity while preserving the inference efficiency to some extent . Weaknesses : 1 : This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs , which is not significant enough . + The dynamic routing strategy ( conditional on input ) has been widely explored . For example , the proposed dynamic formulation in this paper has been used in several studies [ 2 , 3 ] . + Varying width and depth has been extensively explored in the quantization literature , especially in AutoML based approaches [ Shen et al.2019 , Bulat et al.2020 ] , to design high capacity quantized networks . + The effectiveness of the group convolution in BNNs was initially studied in [ 1 ] . Later works also incorporate the group convolution into the search space in NAS+BNNs methods [ e.g. , Bulat et al.2020a ] to reduce the complexity . 2 : In each layer , the paper introduces a full-precision fully-connected layer to decide which expert to use . However , for deeper networks , such as ResNet-101 , it will include ~100 full-precision layers , which can be very expensive especially in BNNs . As a result , it deteriorates the benefits and practicability of the dynamic routing mechanism . 3 : The actual speedup , memory usage and energy consumption on edge devices ( e.g. , CPU/GPU/FPGA ) or IoT devices must be reported . Even though the full-precision operations only account for a small amount of computations in statistics , it can have a big influence on the efficiency on platforms like FPGA . 4 : This paper proposes to learn the binary gates via gradient-based optimization while exploring the network structure via EfficientNet manner . Then the problem comes . This paper can formulate the < width , depth , groups and layer arrangement > as configuration vectors and optimize them using policy gradients and so on , with the binary gates learning unified in a gradient-based framework . So what is the advantage of the `` semi-automated '' method of EfficientNet over the gradient-based optimization ? In addition , how about learning a policy agent via RL to predict the gates ? I encourage the authors can add comparsions and discussions with these alternatives . 5 : More experiments on deeper networks ( e.g. , ResNet-50 ) and other network structures ( e.g. , MobileNet ) are needed to further strengthen the paper . References : [ 1 ] MoBiNet : A Mobile Binary Network for Image Classification , in WACV 2020 . [ 2 ] Dynamic Channel Pruning : Feature Boosting and Suppression , in ICLR2019 . [ 3 ] Learning Dynamic Routing for Semantic Segmentation , in CVPR2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "$ \\textbf { Q2.1 } $ : `` This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs , which is not significant enough . '' $ \\textbf { A2.1 } $ : We respectfully disagree with the reviewer , we provide concrete answers to all 3 points you raised below . $ \\textbf { Q2.2 } $ : `` The dynamic routing strategy ( conditional on input ) has been widely explored . For example , the proposed dynamic formulation in this paper has been used in several studies [ 2 , 3 ] . '' $ \\textbf { A2.2 } $ : What we claimed is that this is the very first work to explore conditional convolutions for binary networks . We never claimed that this is the first dynamic network in general . In fact , we dedicated the entire 2.2 section to this discussion . The only similarity of our work with [ 2 ] and [ 3 ] is that they use the general idea of dynamic computation . First of all , neither [ 2 ] or [ 3 ] are applied to binary networks . Secondly , compared to our work , both [ 2 ] and [ 3 ] are trying to solve different problems with different problem formulations , solutions and implementations . [ 2 ] is on pruning for dynamically selecting N channels from each layer -- our method selects a single expert ( set of weights ) . Similarly , in [ 3 ] , the authors attempt to dynamically define a network architecture , selecting , N routing direction paths for each active node . Overall , the similarity is superficial , the domain , the goal and the implementation are in fact different . $ \\textbf { Q2.3 } $ : `` Varying width and depth has been extensively explored in the quantization literature , especially in AutoML based approaches [ Shen et al.2019 , Bulat et al.2020 ] , to design high capacity quantized networks . '' $ \\textbf { A2.3 } $ : We respectfully disagree with your comment : In Sections 4.2 and 4.3 , we provide the first comprehensive study and analysis of the optimal values for the width , depth , layer distribution , block structure and number of groups for binary networks . The papers you mentioned do not provide such a study at all . Bulat et al.2020 [ 4 ] doesn \u2019 t study the influence of width or depth . They also do not search for these dimensions . The authors simply report a few results with bigger models ( by means of increasing the width ) , however they do not search nor draw any conclusions regarding this . Note that our method outperforms Bulat et al.2020 [ 4 ] by ~6 % . Similarly in [ Shen et al.2019 ] the authors simply search for the optimal width alone , selecting from a set of 6 possible values . We jointly search for the width , depth , layer arrangement and number of groups per block using also a completely different approach . Note that although [ Shen et al.2019 ] increases complexity ( compared to our method ) by 4-5 times , our method still outperforms it by ~1.5 % ( 69.65 % vs 71.2 % ) . $ \\textbf { Q2.4 } $ : `` The effectiveness of the group convolution in BNNs was initially studied in [ 1 ] . Later works also incorporate the group convolution into the search space in NAS+BNNs methods [ e.g. , Bulat et al.2020a ] to reduce the complexity . '' $ \\textbf { A2.4 } $ : We don \u2019 t claim using grouped convolutions as a novelty of the paper . Our method uses grouped convolutions in a novel way which is tailored to the nature of BNNs and is concretely supported by discussion and experimental evidence : * First of all , we don \u2019 t simply use grouped convs : in Section 4.2 we propose grouped convs combined with a width expansion strategy ( which guarantees the same computational complexity ) followed by aggregation based on 1x1 convolutions . This is proposed for the first time . * Secondly , this proposal is largely motivated by the Depth vs Width paragraph ( Section 4.3 , pp5 ) which discusses why and shows experimentally that increasing Width does not have the same impact as increasing Depth for BNNs ( as opposed to real-valued networks ) [ 7 ] . These are non-straightforward observations , proposals and results which ( a ) have not been discussed in prior work , ( b ) are shown for the first time to have such a large impact on BNN accuracy . * Thirdly , this is the first work on binary networks to jointly study and search the following : the optimal group size per block , its width , the network depth , the layer arrangement . In [ 1 ] the authors , in an attempt to alleviate the issue of binarizing a MobileNet architecture , propose to mix the information and control the dependency between channels using a K-dependency set , that as the authors put it : \u201c has a flavor of group convolutions \u201d . The authors report results for K= { 0,1,2,3 } . They do not search for optimal number of groups . Their methodologies are completely different . Thank you for pointing [ 1 ] , we will cite it . Similarly , Bulat et al . [ 4 ] does not study the influence of the group size , nor the influence of the depth or width . None of these parameters are part of their search . They simply introduce grouped convolutions , with a predefined , fixed , group size ."}, {"review_id": "MxaY4FzOTa-2", "review_text": "The authors improved the performance of BNN by adopting group convolution and data-driven expert binary networks which choose a weight group that takes part in inference . In addition , by searching for the network architecture under the condition of the same number of operations , they achieved SOTA accuracy on ImageNet dataset . The results in the paper are strong . Especially , the SOTA accuracy ( 71.2 % ) of the proposed network which even outperforms the full-precision ResNet-18 is impressive and could be the significant result in BNN research . The adopted data-driven expert network which increases the number of parameters , but maintains that of parameters participating in inference is also interesting . Below is my remaining concerns and questions about the experiments . 1.Let us assume the condition that the proposed BNN which has four experts processes many images ( i.e.multiple batches ) . In worst case , expert weights which a few images ( extremely four ) select by gating function can be all different , which gives rise for processors to have to fetch 4 times more parameters ( i.e.all trained parameters ) compared to single image inference . So , I concern if under the condition of using multiple batches there is the performance degradation in terms of inference speed or energy in hardware compared to other BNN models . 2.Unlikely other previous works , mixup is used even on ImageNet . According to [ 1 ] , it is stated that using mixup on ImageNet slightly degrades the accuracy . If an accuracy of your model trained without mixup ( and if possible , accuracy of other models like Bi-Real Net or Real-to-Bin trained with mixup ) is provided , the proposed model performance will be more clearly shown under the same training condition . Minor comment : 1 . I could not clearly understand the message or meaning of Fig.1b.Detailed explanation of this figure seems to be needed . 2.I think it will be more helpful for readers to understand Fig.2 if the authors provide clearer information about it : the detailed network information for each constellation in Fig.2a and for type of lines in Fig.2b.3.I have a question if the searched network architecture is just optimized to BNN or not . I think this can be shown by measuring the accuracy drop of your model compared to the same full-precision network architecture . 4.As mentioned earlier , the experimental result is impressive , so it seems that other researchers or related people might want to use your model , but the stated training process seems to be little bit complicated . Are you going to make your code public ? Reference [ 1 ] Brais Martinez , et al.Training binary neural networks with real-to-binary convolutions . ICLR , 2020 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "$ \\textbf { Q4.1 } $ : `` Let us assume the condition that the proposed BNN which has four experts processes many images ( i.e.multiple batches ) . In worst case , expert weights which a few images ( extremely four ) select by gating function can be all different , which gives rise for processors to have to fetch 4 times more parameters ( i.e.all trained parameters ) compared to single image inference . So , I concern if under the condition of using multiple batches there is the performance degradation in terms of inference speed or energy in hardware compared to other BNN models . '' $ \\textbf { A4.1 } $ : Thank you , this is a good point for discussion which we will include in the paper . The numbers of BOPs and FLOPs of our binary model will remain constant as the batch size increases since the number of operations itself doesn \u2019 t change . As we mention in our paper ( see Appending A.4 ) , memory requirements do change ( 4x for 4 experts ) ; however what dominates memory usage is the memory consumed by activations rather that of the parameters [ 1 ] . Furthermore , we presume that the 4 binary experts would typically be already loaded into memory . However , you are right that for batch sizes larger than 1 there will be a small cost incurred for the actual reading ( fetching ) of the weights from the memory . However in our tests in pytorch the potential increase was insignificant . Finally , we note that , in practice , for edge devices where BNNs are meant to be deployed the typical batch size will be 1 . We already reported in the supplementary material the case of batch size of 1 where we show that our method comes at little to no extra cost regarding memory consumption . $ \\textbf { Q4.2 } $ : `` Unlikely other previous works , mixup is used even on ImageNet . According to Real-to-BIn , it is stated that using mixup on ImageNet slightly degrades the accuracy . If an accuracy of your model trained without mixup ( and if possible , accuracy of other models like Bi-Real Net or Real-to-Bin trained with mixup ) is provided , the proposed model performance will be more clearly shown under the same training condition . '' $ \\textbf { A4.2 } $ : We did our best to keep the comparisons as fair as possible . As we report in the supplementary material , section A2.2 , mixup gives us a gain of 0.4 % in terms of accuracy . Note that only our model benefits from mix-up so we believe comparisons are fair . For example we already tried Real-to-Bin with mixup and the performance dropped by 0.25 % for step 1 , and 0.8 % for step 2 . We will add this text to section A2.2 . Thank you again for your suggestion ! $ \\textbf { Q4.3 } $ : `` I could not clearly understand the message or meaning of Fig.1b.Detailed explanation of this figure seems to be needed . '' $ \\textbf { A4.3 } $ : In Fig.1b we use t-SNE to compute the similarity between all the samples from the ImageNet validation set based on their features , projecting them on a 2D space . Points located closer to each other are more semantically and visually similar . We color each data point according to the expert selected by the last EBConv from our network . The figures show that our experts learn a preference for certain classes , or groups of classes from Imagenet ( notice the multiple clusters that emerge in the figure ) . This suggests that our EBConv layers learn semantically meaningful representations of the data . $ \\textbf { Q4.4 } $ : `` I think it will be more helpful for readers to understand Fig.2 if the authors provide clearer information about it : the detailed network information for each constellation in Fig.2a and for type of lines in Fig.2b . `` $ \\textbf { A4.4 } $ : Thank you for your suggestion , we will provide for each constellation the structure of the network too in the supplementary material"}, {"review_id": "MxaY4FzOTa-3", "review_text": "This paper proposes some techniques to improve the accuracy of binary networks without adding much computational overhead . To improve model capacity , the author proposes mixture-of-experts convolution with a winner-takes-all gating mechanisms . To deal with the limited representation power of binary activations , the paper proposes utilizing group convolutions . The performance is further improved by careful selection of hyperparameters and improved training techniques . Clarity : This paper is pretty clear . The methodology is well-motivated and the algorithms / experiments are described clearly . Originality : To the best of my knowledge , the utilization of mixture-of-experts and group convolutions for binary neural networks is novel . Significance : The propose techniques , despite being simple , achieves good performance . They can be a new baseline for future research . Overall I think this is a good paper and should be accepted .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for recognizing the novelty of our approach and their importance of our results on research on Binary Networks . We plan to release the code to facilitate future research ."}], "0": {"review_id": "MxaY4FzOTa-0", "review_text": "Summary : The paper addresses the problem of filling the gap between the performance of binary and real-valued networks . The authors propose a series of procedures to improve the model and representation capacity of binary neural networks . Different binary-network architectures are obtained through a new network-growing approach and compared . Strengths : Research on BNNs has a pretty short history if compared with studies on real-valued NN . It is a good idea to start translating some of the main tools from the standard NN literature to the binary setup . According to the authors ' claim , the proposed model greatly outperforms other existing and well-known binary networks . Weaknesses : The majority of the tools proposed for boosting the performance of binary networks are not new and have been already used in standard NNs . A discussion of the technical challenges associated with applying such tools to the binary setup would help understand the main contributions of the paper . The inclusion of real-valued experts seems to make the final network not completely binary and it is not clear whether the advantages of BNN ( e.g.the gain on computational costs ) are preserved . The network design step mainly consists of rearranging a series of pre-defined building blocks . It is not well explained how the architecture space is searched and how to interpret the results in Figure 2 . Questions : - is the cost of Conditional computing included in the total cost when the main results are claimed ( e.g.in `` Without increasing the computational budget of previous works , our method improves upon the state-of-the-art by 6 % '' ) ? More generally , when does the fixed number of BOPs include any training step ? - does the expert selection of the proposed method work better in the binary case than in real-valued networks ? - are the weights in each ` expert ' binary or real ? Is it fair to compare the obtained hybrid model with real-to-bin ? - has Grouped Convolution with a similar scaling factor been already used somewhere ? - what is the difference between the proposed gradient-approximation method and standard `` Straight-Through-Estimator '' ( STE ) ? -- I acknowledge that I read and appreciated the author 's response ( both parts ) . The authors ' reply mainly answers my questions , especially regarding the difference between applying the proposed techniques to the real and binary setups . I agree with all authors comments but would tend to confirm my overall score for two reasons : the architecture search method is not simply a block rearranging but looks more like a heuristic approach than a clear methodological contribution the proposed mixing of real and binary weights may preserve the advantages of fully binary networks but , again , makes less clear the net contribution of the paper from a more theoretical perspective However , as I recognize that the paper contains significant experimental results , I would be happy to support acceptance if all other reviewers agree on that .", "rating": "5: Marginally below acceptance threshold", "reply_text": "$ \\textbf { Q1.1 } $ : `` The majority of the tools proposed for boosting the performance of binary networks are not new and have been already used in standard NNs . A discussion of the technical challenges associated with applying such tools to the binary setup would help understand the main contributions of the paper . '' $ \\textbf { A1.1 } $ : First of all , due to the nature of binarization , architectural or methodological changes that benefit full-precision NNs do not result in accuracy improvements for BNNs . Direct application of techniques that work well for real-valued networks on Binary Networks has very often been unsuccessful . As an example , from our experiments , binarizing the MobileNet architecture performs poorly ( while in [ 1 ] it is shown that it does not converge at all ) . There are a multitude of reasons for this : in brief , this is due to the nature of binarization , where all values are discrete and restricted to 2 states only . This severely limits both the representational power of BNNs and causes issues during training since the true gradients are uninformative and their approximations make training harder to converge . Although conditional convs ( CondConv ) were previously used for standard NN , we show ( Section 5 , \u201c Comparison against CondConv and in Section 2.2 , second paragraph ) that directly applying the CondConv approach to binary networks is unsuitable . This can be attributed to the the fact that CondConv results in a linear combination of binary weights which is non-binary ( see also Section 2.2 , pp2 , last paragraph ) ; hence it requires a second binarization which as we show that doesn \u2019 t work well in practice . We further note that selecting the Best Expert as in our work vs a Mixture of Experts ( MoE ) as in CondConv is a different idea also requiring a different formulation and implementation ( the formulation of Section 4.1 is not required in CondConv which is in general simpler ) . Likewise , while grouped convs are also known , our method uses them in a novel way which is tailored to the nature of BNNs and is concretely supported by discussion and experimental evidence : First of all , we don \u2019 t simply use grouped convs : in Section 4.2 we propose grouped convs combined with a width expansion strategy ( which guarantees the same computational complexity ) followed by aggregation based on 1x1 convolutions . This is proposed for the first time . Secondly , this proposal is largely motivated by the Depth vs Width paragraph ( Section 4.3 , pp5 ) which discusses why and shows experimentally that increasing Width does not have the same impact as increasing Depth for BNNs ( as opposed to real-valued networks ) [ 4 ] . Thirdy , this is the first work on binary networks to jointly study and search the following : the optimal group size per block , its width , the network depth , the layer arrangement . These are non-straightforward observations , proposals and results which ( a ) have not been discussed in prior work , ( b ) are shown for the first time to have such a large impact on BNN accuracy . $ \\textbf { Q1.2 } $ : \u201c The inclusion of real-valued experts seems to make the final network not completely binary and it is not clear whether the advantages of BNN ( e.g.the gain on computational costs ) are preserved \u201d $ \\textbf { A1.2 } $ : The advantages are fully preserved . Their added cost is less than 0.001 % ( in terms of FLOPS ) of the total computing budget . In fact all top performing BNN methods reported in Table 5 do use small amounts of real valued operations reflected in the \u201c FLOPS \u201d column from Table 5 that quantifies the amount of full precision operations ."}, "1": {"review_id": "MxaY4FzOTa-1", "review_text": "Paper summary : This paper proposes three approaches to improve the performance of BNNs . 1 ) Training a super-network ( ensemble of BNNs ) and dynamically selecting one BNNs to execute conditioned on input . 2 ) Widening the layers with group convolution to enhance the representational capacity . 3 ) Designing the architecture using EfficientNet considering the width , depth , groups and layer arrangement configurations simultaneously . The effectiveness of the paper has been justified on ImageNet classification but can be further strengthened . Strength : + The performance of the paper is promising . It will serve as a strong baseline for future works . + Using conditional dynamic routing to improve the BNNs capacity is interesting . Specifically , the paper proposes to learn a supernetwork ( i.e. , ensemble of several experts ) during training and dynamically select the path during testing , which enhances the capacity while preserving the inference efficiency to some extent . Weaknesses : 1 : This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs , which is not significant enough . + The dynamic routing strategy ( conditional on input ) has been widely explored . For example , the proposed dynamic formulation in this paper has been used in several studies [ 2 , 3 ] . + Varying width and depth has been extensively explored in the quantization literature , especially in AutoML based approaches [ Shen et al.2019 , Bulat et al.2020 ] , to design high capacity quantized networks . + The effectiveness of the group convolution in BNNs was initially studied in [ 1 ] . Later works also incorporate the group convolution into the search space in NAS+BNNs methods [ e.g. , Bulat et al.2020a ] to reduce the complexity . 2 : In each layer , the paper introduces a full-precision fully-connected layer to decide which expert to use . However , for deeper networks , such as ResNet-101 , it will include ~100 full-precision layers , which can be very expensive especially in BNNs . As a result , it deteriorates the benefits and practicability of the dynamic routing mechanism . 3 : The actual speedup , memory usage and energy consumption on edge devices ( e.g. , CPU/GPU/FPGA ) or IoT devices must be reported . Even though the full-precision operations only account for a small amount of computations in statistics , it can have a big influence on the efficiency on platforms like FPGA . 4 : This paper proposes to learn the binary gates via gradient-based optimization while exploring the network structure via EfficientNet manner . Then the problem comes . This paper can formulate the < width , depth , groups and layer arrangement > as configuration vectors and optimize them using policy gradients and so on , with the binary gates learning unified in a gradient-based framework . So what is the advantage of the `` semi-automated '' method of EfficientNet over the gradient-based optimization ? In addition , how about learning a policy agent via RL to predict the gates ? I encourage the authors can add comparsions and discussions with these alternatives . 5 : More experiments on deeper networks ( e.g. , ResNet-50 ) and other network structures ( e.g. , MobileNet ) are needed to further strengthen the paper . References : [ 1 ] MoBiNet : A Mobile Binary Network for Image Classification , in WACV 2020 . [ 2 ] Dynamic Channel Pruning : Feature Boosting and Suppression , in ICLR2019 . [ 3 ] Learning Dynamic Routing for Semantic Segmentation , in CVPR2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "$ \\textbf { Q2.1 } $ : `` This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs , which is not significant enough . '' $ \\textbf { A2.1 } $ : We respectfully disagree with the reviewer , we provide concrete answers to all 3 points you raised below . $ \\textbf { Q2.2 } $ : `` The dynamic routing strategy ( conditional on input ) has been widely explored . For example , the proposed dynamic formulation in this paper has been used in several studies [ 2 , 3 ] . '' $ \\textbf { A2.2 } $ : What we claimed is that this is the very first work to explore conditional convolutions for binary networks . We never claimed that this is the first dynamic network in general . In fact , we dedicated the entire 2.2 section to this discussion . The only similarity of our work with [ 2 ] and [ 3 ] is that they use the general idea of dynamic computation . First of all , neither [ 2 ] or [ 3 ] are applied to binary networks . Secondly , compared to our work , both [ 2 ] and [ 3 ] are trying to solve different problems with different problem formulations , solutions and implementations . [ 2 ] is on pruning for dynamically selecting N channels from each layer -- our method selects a single expert ( set of weights ) . Similarly , in [ 3 ] , the authors attempt to dynamically define a network architecture , selecting , N routing direction paths for each active node . Overall , the similarity is superficial , the domain , the goal and the implementation are in fact different . $ \\textbf { Q2.3 } $ : `` Varying width and depth has been extensively explored in the quantization literature , especially in AutoML based approaches [ Shen et al.2019 , Bulat et al.2020 ] , to design high capacity quantized networks . '' $ \\textbf { A2.3 } $ : We respectfully disagree with your comment : In Sections 4.2 and 4.3 , we provide the first comprehensive study and analysis of the optimal values for the width , depth , layer distribution , block structure and number of groups for binary networks . The papers you mentioned do not provide such a study at all . Bulat et al.2020 [ 4 ] doesn \u2019 t study the influence of width or depth . They also do not search for these dimensions . The authors simply report a few results with bigger models ( by means of increasing the width ) , however they do not search nor draw any conclusions regarding this . Note that our method outperforms Bulat et al.2020 [ 4 ] by ~6 % . Similarly in [ Shen et al.2019 ] the authors simply search for the optimal width alone , selecting from a set of 6 possible values . We jointly search for the width , depth , layer arrangement and number of groups per block using also a completely different approach . Note that although [ Shen et al.2019 ] increases complexity ( compared to our method ) by 4-5 times , our method still outperforms it by ~1.5 % ( 69.65 % vs 71.2 % ) . $ \\textbf { Q2.4 } $ : `` The effectiveness of the group convolution in BNNs was initially studied in [ 1 ] . Later works also incorporate the group convolution into the search space in NAS+BNNs methods [ e.g. , Bulat et al.2020a ] to reduce the complexity . '' $ \\textbf { A2.4 } $ : We don \u2019 t claim using grouped convolutions as a novelty of the paper . Our method uses grouped convolutions in a novel way which is tailored to the nature of BNNs and is concretely supported by discussion and experimental evidence : * First of all , we don \u2019 t simply use grouped convs : in Section 4.2 we propose grouped convs combined with a width expansion strategy ( which guarantees the same computational complexity ) followed by aggregation based on 1x1 convolutions . This is proposed for the first time . * Secondly , this proposal is largely motivated by the Depth vs Width paragraph ( Section 4.3 , pp5 ) which discusses why and shows experimentally that increasing Width does not have the same impact as increasing Depth for BNNs ( as opposed to real-valued networks ) [ 7 ] . These are non-straightforward observations , proposals and results which ( a ) have not been discussed in prior work , ( b ) are shown for the first time to have such a large impact on BNN accuracy . * Thirdly , this is the first work on binary networks to jointly study and search the following : the optimal group size per block , its width , the network depth , the layer arrangement . In [ 1 ] the authors , in an attempt to alleviate the issue of binarizing a MobileNet architecture , propose to mix the information and control the dependency between channels using a K-dependency set , that as the authors put it : \u201c has a flavor of group convolutions \u201d . The authors report results for K= { 0,1,2,3 } . They do not search for optimal number of groups . Their methodologies are completely different . Thank you for pointing [ 1 ] , we will cite it . Similarly , Bulat et al . [ 4 ] does not study the influence of the group size , nor the influence of the depth or width . None of these parameters are part of their search . They simply introduce grouped convolutions , with a predefined , fixed , group size ."}, "2": {"review_id": "MxaY4FzOTa-2", "review_text": "The authors improved the performance of BNN by adopting group convolution and data-driven expert binary networks which choose a weight group that takes part in inference . In addition , by searching for the network architecture under the condition of the same number of operations , they achieved SOTA accuracy on ImageNet dataset . The results in the paper are strong . Especially , the SOTA accuracy ( 71.2 % ) of the proposed network which even outperforms the full-precision ResNet-18 is impressive and could be the significant result in BNN research . The adopted data-driven expert network which increases the number of parameters , but maintains that of parameters participating in inference is also interesting . Below is my remaining concerns and questions about the experiments . 1.Let us assume the condition that the proposed BNN which has four experts processes many images ( i.e.multiple batches ) . In worst case , expert weights which a few images ( extremely four ) select by gating function can be all different , which gives rise for processors to have to fetch 4 times more parameters ( i.e.all trained parameters ) compared to single image inference . So , I concern if under the condition of using multiple batches there is the performance degradation in terms of inference speed or energy in hardware compared to other BNN models . 2.Unlikely other previous works , mixup is used even on ImageNet . According to [ 1 ] , it is stated that using mixup on ImageNet slightly degrades the accuracy . If an accuracy of your model trained without mixup ( and if possible , accuracy of other models like Bi-Real Net or Real-to-Bin trained with mixup ) is provided , the proposed model performance will be more clearly shown under the same training condition . Minor comment : 1 . I could not clearly understand the message or meaning of Fig.1b.Detailed explanation of this figure seems to be needed . 2.I think it will be more helpful for readers to understand Fig.2 if the authors provide clearer information about it : the detailed network information for each constellation in Fig.2a and for type of lines in Fig.2b.3.I have a question if the searched network architecture is just optimized to BNN or not . I think this can be shown by measuring the accuracy drop of your model compared to the same full-precision network architecture . 4.As mentioned earlier , the experimental result is impressive , so it seems that other researchers or related people might want to use your model , but the stated training process seems to be little bit complicated . Are you going to make your code public ? Reference [ 1 ] Brais Martinez , et al.Training binary neural networks with real-to-binary convolutions . ICLR , 2020 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "$ \\textbf { Q4.1 } $ : `` Let us assume the condition that the proposed BNN which has four experts processes many images ( i.e.multiple batches ) . In worst case , expert weights which a few images ( extremely four ) select by gating function can be all different , which gives rise for processors to have to fetch 4 times more parameters ( i.e.all trained parameters ) compared to single image inference . So , I concern if under the condition of using multiple batches there is the performance degradation in terms of inference speed or energy in hardware compared to other BNN models . '' $ \\textbf { A4.1 } $ : Thank you , this is a good point for discussion which we will include in the paper . The numbers of BOPs and FLOPs of our binary model will remain constant as the batch size increases since the number of operations itself doesn \u2019 t change . As we mention in our paper ( see Appending A.4 ) , memory requirements do change ( 4x for 4 experts ) ; however what dominates memory usage is the memory consumed by activations rather that of the parameters [ 1 ] . Furthermore , we presume that the 4 binary experts would typically be already loaded into memory . However , you are right that for batch sizes larger than 1 there will be a small cost incurred for the actual reading ( fetching ) of the weights from the memory . However in our tests in pytorch the potential increase was insignificant . Finally , we note that , in practice , for edge devices where BNNs are meant to be deployed the typical batch size will be 1 . We already reported in the supplementary material the case of batch size of 1 where we show that our method comes at little to no extra cost regarding memory consumption . $ \\textbf { Q4.2 } $ : `` Unlikely other previous works , mixup is used even on ImageNet . According to Real-to-BIn , it is stated that using mixup on ImageNet slightly degrades the accuracy . If an accuracy of your model trained without mixup ( and if possible , accuracy of other models like Bi-Real Net or Real-to-Bin trained with mixup ) is provided , the proposed model performance will be more clearly shown under the same training condition . '' $ \\textbf { A4.2 } $ : We did our best to keep the comparisons as fair as possible . As we report in the supplementary material , section A2.2 , mixup gives us a gain of 0.4 % in terms of accuracy . Note that only our model benefits from mix-up so we believe comparisons are fair . For example we already tried Real-to-Bin with mixup and the performance dropped by 0.25 % for step 1 , and 0.8 % for step 2 . We will add this text to section A2.2 . Thank you again for your suggestion ! $ \\textbf { Q4.3 } $ : `` I could not clearly understand the message or meaning of Fig.1b.Detailed explanation of this figure seems to be needed . '' $ \\textbf { A4.3 } $ : In Fig.1b we use t-SNE to compute the similarity between all the samples from the ImageNet validation set based on their features , projecting them on a 2D space . Points located closer to each other are more semantically and visually similar . We color each data point according to the expert selected by the last EBConv from our network . The figures show that our experts learn a preference for certain classes , or groups of classes from Imagenet ( notice the multiple clusters that emerge in the figure ) . This suggests that our EBConv layers learn semantically meaningful representations of the data . $ \\textbf { Q4.4 } $ : `` I think it will be more helpful for readers to understand Fig.2 if the authors provide clearer information about it : the detailed network information for each constellation in Fig.2a and for type of lines in Fig.2b . `` $ \\textbf { A4.4 } $ : Thank you for your suggestion , we will provide for each constellation the structure of the network too in the supplementary material"}, "3": {"review_id": "MxaY4FzOTa-3", "review_text": "This paper proposes some techniques to improve the accuracy of binary networks without adding much computational overhead . To improve model capacity , the author proposes mixture-of-experts convolution with a winner-takes-all gating mechanisms . To deal with the limited representation power of binary activations , the paper proposes utilizing group convolutions . The performance is further improved by careful selection of hyperparameters and improved training techniques . Clarity : This paper is pretty clear . The methodology is well-motivated and the algorithms / experiments are described clearly . Originality : To the best of my knowledge , the utilization of mixture-of-experts and group convolutions for binary neural networks is novel . Significance : The propose techniques , despite being simple , achieves good performance . They can be a new baseline for future research . Overall I think this is a good paper and should be accepted .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for recognizing the novelty of our approach and their importance of our results on research on Binary Networks . We plan to release the code to facilitate future research ."}}