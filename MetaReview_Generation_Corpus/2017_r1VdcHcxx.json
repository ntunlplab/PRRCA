{"year": "2017", "forum": "r1VdcHcxx", "title": "Recurrent Batch Normalization", "decision": "Accept (Poster)", "meta_review": "The reviewers believe this paper is of significant interest to the ICLR community, as it demonstrates how to get the popular batch normalization method to work in the recurrent setting. The fact that it has already been cited a variety of times also speaks to its interest within the community. The extensive experiments are convincing that the method works. One common criticism is that the authors don't address enough the added computational cost of the method in the text or empirically. Plots showing loss as a function of wall-clock time instead of training iteration would be more informative to readers deciding whether to use batch norm. \n \n Pros: \n - Gets batch normalization to work on recurrent networks (which had been elusive to many)\n - The experiments are thorough and demonstrate the method reduces training time (as a function of training iterations)\n - The paper is well written and accessible\n \n Cons\n - The contribution is relatively incremental (several tweaks to an existing method)\n - The major disadvantage to the approach is the added computational cost, but this is conspicuously not addressed.", "reviews": [{"review_id": "r1VdcHcxx-0", "review_text": "The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence. The paper is well written and the idea well presented. i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your feedback , 1 ) We evaluate our BN-LSTM proposal on 5 different datasets , two sequential classification tasks , two language modelling tasks and one large-scale question answering task . The different datasets also cover a variety of inputs , including characters ( PTB and Text8 ) , words ( CNN question-answering ) , and pixels ( MNIST and pMNIST ) . In addition , all the proposed tasks have been extensively used in the RNN literature and can be considered as standard benchmarks for such models . We think that the proposed set of tasks is fairly representative of the usual problems on which RNN model are applied . 2 ) Hyperparameters have not been picked in favor of one of the models . We performed a grid search for both the vanilla LSTM and BN-LSTM , on the learning rate for all the tasks , on model capacity for text8 and CNN and on type of initialization methods for MNIST/pMNIST . We will clarify this in the paper . To the best of our knowledge , our hyperparameters exploration allowed us to obtain the best performance with a straight-up vanilla LSTM model on the MNIST/pMNIST , Penntreebank and Text8 tasks , showing that we consider strong and competitive baselines . Also , we would like to emphasize that using a different learning rate does not lead to an \u201c equally fast convergence for vanilla LSTM \u201d . More specifically on PennTreebank , increasing the learning rate in the vanilla LSTM baseline led to unstable training that diverged after a given number of epoch . Decreasing the learning rate on the other hand , made the overall optimization slower . We observed similar behavior on the other tasks as well . We believe that our experimental evaluation is fair and accurately demonstrates the advantage of BN-LSTM compared to a vanilla LSTM ."}, {"review_id": "r1VdcHcxx-1", "review_text": "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN question-answering task), and pixel level (MNIST and pMNIST). The supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration. The experiment on pMNIST also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies. I additionally also appreciated the gradient flow insight, specifically the impact of unit variance on tanh derivatives. Showing it not just for batch normalization but additionally the \"toy task\" (Figure 1b) was hugely useful. Overall I find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review and positive feedback ,"}, {"review_id": "r1VdcHcxx-2", "review_text": "Contributions The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering. Novelty+Significance Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs. The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison. As RNNs are used across many tasks, this work is of interest to many. However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn\u2019t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? Clarity The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly. Summary Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead. Pros - Shows batch normalization to work for RNNs where previous works have not succeeded - Good empirical analysis of hyper-parameter choices and of the activations - Experiments on multiple tasks - Clarity Cons - Relatively incremental - Several \u2018hacks\u2019 for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization) - No mention of computational overhead - Only character or pixel-level tasks, what about word-level?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed feedback . We address some of your concerns below . \u201c ... why would it also improve generalization ? \u201d This is one of the effects of batch normalization in feed-forward networks . We believe that generalization improvement is due to the use of batch statistics . As the estimated means and variances through the batch are not perfect , they introduce some noise in the training process that have a regularizing effect . We experimentally verified this effect by varying the minibatch size . When the minibatch size is increased , noise is reduced in the estimated means and variances . Consequently , we observed faster optimization , but it also tends to increase the model overfitting . By reducing the batch size , we observe a gain in generalization , but a relative slowdown in the optimization . \u201c ... Several \u2018 hacks \u2019 for the method ( per-time step statistics , adding noise for exploding variance , sequence-wise normalization ) \u201d We would like to clarify the motivations behind the use of per-time step statistics and sequence-normalization . When BN is applied to an LSTM , one needs to consider the normalization of the state activation and the inputs . While LSTM activations do converge to a stationary distribution , we observe that their statistics during the initial transient differ significantly ( see Figure 5 in Appendix A ) . It is therefore necessary to leverage per time step statistics for estimating the mean and variance of the * state activation * , to take into account this initial transient . Note that we always use per-time step statistics for the state activations in our experiments . On the other hand , inputs means and variances are not necessarily time-dependent . You can therefore share the statistics over the different time-step leading to the sequence-wise normalization for the * input * . Finally , adding noise is a way to artificially increase the variance , when the estimated variance is exactly 0 . This problem occurs in the in-order MNIST task due to the first hundred or so pixels being black . Increasing the $ \\epsilon $ in ( eq.5 ) would be an alternative way to deal with 0 variances , however , we empirically observe that increasing $ \\epsilon $ also tends to degrade the optimization . \u201c ... computational overhead \u201d Introducing BN indeed implies a computational overhead as you need to compute the mean and variance statistics and normalize the activation . One SGD update of our BN-LSTM implementation is usually about 50 % slower than a vanilla LSTM . However , since BN eases optimization , it typically requires fewer epochs than the LSTM baseline to converge . In addition , we observed that introducing BN made it possible to train models with lower capacity , which are faster to compute . We were not able to train models with similar reduced capacity using a baseline LSTM . In particular , on the CNN task , it took roughly three weeks to train the LSTM baseline with a capacity of 240 units . We tried to reduce the capacity of the baseline , but we were not able to train the baseline models with reduced capacity . By introducing BN , we were able to train a model with a capacity of 120 units on this task , reducing the total training time to about a week . \u201c ... Only character or pixel-level tasks , what about word-level ? \u201d We also report on the CNN dataset which is a word-level question answering task ."}], "0": {"review_id": "r1VdcHcxx-0", "review_text": "The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence. The paper is well written and the idea well presented. i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your feedback , 1 ) We evaluate our BN-LSTM proposal on 5 different datasets , two sequential classification tasks , two language modelling tasks and one large-scale question answering task . The different datasets also cover a variety of inputs , including characters ( PTB and Text8 ) , words ( CNN question-answering ) , and pixels ( MNIST and pMNIST ) . In addition , all the proposed tasks have been extensively used in the RNN literature and can be considered as standard benchmarks for such models . We think that the proposed set of tasks is fairly representative of the usual problems on which RNN model are applied . 2 ) Hyperparameters have not been picked in favor of one of the models . We performed a grid search for both the vanilla LSTM and BN-LSTM , on the learning rate for all the tasks , on model capacity for text8 and CNN and on type of initialization methods for MNIST/pMNIST . We will clarify this in the paper . To the best of our knowledge , our hyperparameters exploration allowed us to obtain the best performance with a straight-up vanilla LSTM model on the MNIST/pMNIST , Penntreebank and Text8 tasks , showing that we consider strong and competitive baselines . Also , we would like to emphasize that using a different learning rate does not lead to an \u201c equally fast convergence for vanilla LSTM \u201d . More specifically on PennTreebank , increasing the learning rate in the vanilla LSTM baseline led to unstable training that diverged after a given number of epoch . Decreasing the learning rate on the other hand , made the overall optimization slower . We observed similar behavior on the other tasks as well . We believe that our experimental evaluation is fair and accurately demonstrates the advantage of BN-LSTM compared to a vanilla LSTM ."}, "1": {"review_id": "r1VdcHcxx-1", "review_text": "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN question-answering task), and pixel level (MNIST and pMNIST). The supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration. The experiment on pMNIST also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies. I additionally also appreciated the gradient flow insight, specifically the impact of unit variance on tanh derivatives. Showing it not just for batch normalization but additionally the \"toy task\" (Figure 1b) was hugely useful. Overall I find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review and positive feedback ,"}, "2": {"review_id": "r1VdcHcxx-2", "review_text": "Contributions The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering. Novelty+Significance Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs. The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison. As RNNs are used across many tasks, this work is of interest to many. However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn\u2019t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? Clarity The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly. Summary Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead. Pros - Shows batch normalization to work for RNNs where previous works have not succeeded - Good empirical analysis of hyper-parameter choices and of the activations - Experiments on multiple tasks - Clarity Cons - Relatively incremental - Several \u2018hacks\u2019 for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization) - No mention of computational overhead - Only character or pixel-level tasks, what about word-level?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed feedback . We address some of your concerns below . \u201c ... why would it also improve generalization ? \u201d This is one of the effects of batch normalization in feed-forward networks . We believe that generalization improvement is due to the use of batch statistics . As the estimated means and variances through the batch are not perfect , they introduce some noise in the training process that have a regularizing effect . We experimentally verified this effect by varying the minibatch size . When the minibatch size is increased , noise is reduced in the estimated means and variances . Consequently , we observed faster optimization , but it also tends to increase the model overfitting . By reducing the batch size , we observe a gain in generalization , but a relative slowdown in the optimization . \u201c ... Several \u2018 hacks \u2019 for the method ( per-time step statistics , adding noise for exploding variance , sequence-wise normalization ) \u201d We would like to clarify the motivations behind the use of per-time step statistics and sequence-normalization . When BN is applied to an LSTM , one needs to consider the normalization of the state activation and the inputs . While LSTM activations do converge to a stationary distribution , we observe that their statistics during the initial transient differ significantly ( see Figure 5 in Appendix A ) . It is therefore necessary to leverage per time step statistics for estimating the mean and variance of the * state activation * , to take into account this initial transient . Note that we always use per-time step statistics for the state activations in our experiments . On the other hand , inputs means and variances are not necessarily time-dependent . You can therefore share the statistics over the different time-step leading to the sequence-wise normalization for the * input * . Finally , adding noise is a way to artificially increase the variance , when the estimated variance is exactly 0 . This problem occurs in the in-order MNIST task due to the first hundred or so pixels being black . Increasing the $ \\epsilon $ in ( eq.5 ) would be an alternative way to deal with 0 variances , however , we empirically observe that increasing $ \\epsilon $ also tends to degrade the optimization . \u201c ... computational overhead \u201d Introducing BN indeed implies a computational overhead as you need to compute the mean and variance statistics and normalize the activation . One SGD update of our BN-LSTM implementation is usually about 50 % slower than a vanilla LSTM . However , since BN eases optimization , it typically requires fewer epochs than the LSTM baseline to converge . In addition , we observed that introducing BN made it possible to train models with lower capacity , which are faster to compute . We were not able to train models with similar reduced capacity using a baseline LSTM . In particular , on the CNN task , it took roughly three weeks to train the LSTM baseline with a capacity of 240 units . We tried to reduce the capacity of the baseline , but we were not able to train the baseline models with reduced capacity . By introducing BN , we were able to train a model with a capacity of 120 units on this task , reducing the total training time to about a week . \u201c ... Only character or pixel-level tasks , what about word-level ? \u201d We also report on the CNN dataset which is a word-level question answering task ."}}