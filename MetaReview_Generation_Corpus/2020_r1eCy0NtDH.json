{"year": "2020", "forum": "r1eCy0NtDH", "title": "Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks", "decision": "Reject", "meta_review": "This paper focuses on studying the impact of initialization and activation functions on the Neural Tangent Kernel (NTK) type analysis. The authors claim to make a connection between NTK and edge of chaos analysis. The reviewers had some concern about (1) impact of smooth activations \"any NTK-based training method for DNNs should use a Smooth Activation Function from the class S and the network should be initialized on the EOC\" (2) proofs of residual networks (3) and why mixing NTK with EOC is interesting. Some of these concerns were addressed in the response. I do share the reviewer concerns about (2). The authors need to give a clear proof. I think this combination of NTK and EOC could be interesting but needs to be better motivated. As a result I do not recommend publication.", "reviews": [{"review_id": "r1eCy0NtDH-0", "review_text": "This paper studies the role of weight/bias variance of neural network\u2019s trainability via analyzing large depth behaviour of Neural Tangent Kernels(NTK). Recently NTK has been a popular topic of study in theoretical deep learning as it describes exact gradient descent dynamics of infinitely wide networks. Original NTK paper (Jacot et al. (2018)) and other follow up papers often gloss over role of weight/bias scales whereas in the setting of signal propagation or NNGP covariance, Schoenholz et al. (2017), Lee et al. (2018), Novak et al. (2019), Hayou et al. (2019) have shown understanding initialization scale is quite important as networks becomes deeper. This paper brings those analysis for NTK and discovers few interesting results. The good weight/bias initialization scale that propagates signal for very deep network is denoted edge of chaos (EOC). The authors show that 1) for fully connected feed forward networks, outside initialization edge of chaos (EOC) the NTK converges exponentially to constant kernel. This indicates non-trainability. 2) With EOC initialization the convergence is polynomial and the NTK remains invertible for very large depth implying trainability. 3) Certain class of activation function (denoted class S, including ELU/Tanh/Swish) it has even slower convergence(O(log(L)/L)) compare to ReLU (O(1/L)). 4) Residual FC networks NTK has polynomial convergence for all weight and bias variance. In terms of novelty, the paper is combining existing techniques and objects to study large depth behaviour of NTK. However the contribution of the paper is interesting and worth the ICLR audience to know about, especially with the current surge of interest in NTK. One main weakness is that the experiments are weak and have a very weak connection to the early part of the paper. Section 5.1 is direct convergence comparison, which provides fair evidence. In section 5.2, I\u2019m not certain whether experiments displayed connects to asymptotic NTK analysis. First of all, in order to obtain NTK in the infinite width limit, one has to use `NTK parameterization\u2019 where one scales out 1/sqrt(fan_in) in network definition and not in weight initialization. It seems (by mention of He/Glorot init, and choice of learning rate O(1e-3/1e-4)) the authors experimented with standard parameterization. In this case the connection to very deep behaviour of NTK is not straightforward to actual network training since the training dynamics will be different. I suggest authors try experiments in NTK parameterization and see if results are similar. Few comments: It should be emphasized that the infinite width is taken first before taking infinite depth limit. There are subtle effects when depth/width are taken to infinity at the same time. The analysis on asymptotic behaviour of NTK at infinite depth is only valid after width is taken to infinity. First sentence of the abstract is too strong in the sense that NTK has limitations. NTK certainly does not work for any kind of network, so I suggest authors to down tone the sentence. P4 paragraph after Lemma1 `'K^L(x, x\u2019) = cte\u2019 is a typo? In section 5.2, the authors\u2019 claim full batch GD is practically impossible. One could accumulate gradients over minibatch to simulate full batch GD. P13 typo in last paragraph, ` NTKl' EDITS POST AUTHOR RESPONSE: Regarding point 5) I was referring to solving the memory problem. In practice, if one is sampling without replacement, there is no randomness accumulating gradient of the full epoch. I also thank the authors for the clarification. My score still remains the same. ", "rating": "6: Weak Accept", "reply_text": "We thank Reviewer 1 for the detailed and insightful feedback . We address them hereafter . 1 ) \u201c One main weakness is that the experiments are weak and have a very weak connection to the early part of the paper . Section 5.1 is direct convergence comparison , which provides fair evidence . In section 5.2 , I \u2019 m not certain whether experiments displayed connects to asymptotic NTK analysis ... \u201d We agree that the dynamics might be slightly different . However , in practice both parametrizations give similar results in terms of Trainability and Training Speed ( which are our focus in this paper ) since they both have the same EOC ( same EOC curve in ( \\sigma_b , \\sigma_w ) plane ) . More precisely , the NTK is the same for both parametrization at initialization , and in practice we use small learning rates for standard parametrization ( 1e-3 ) compared to the learning rates used with NTK parametrization ( to compensate for the scaling ) , we believe that the resulting training dynamics should not change much , unless we use small learning rates for the NTK parametrization which would result in very slow training . We added in the appendix ( Appendix E ) an example of training a network of depth 100 and width 100 with both parametrizations with an EOC initialization ( we used ELU in this example ) , it shows that both parametrizations have similar behaviour on the EOC . This is also true on the Ordered/Chaotic phase where the model is stuck at the random classifier accuracy ~10 % for both parametrizations . We will add more experimental results with NTK Parametrization in the appendix of the final version of the paper . 2 ) \u201c It should be emphasized that the infinite width is taken first before taking infinite depth limit . There are subtle effects when depth/width are taken to infinity at the same time . The analysis on asymptotic behaviour of NTK at infinite depth is only valid after width is taken to infinity . \u201c We have mentioned that we only deal with the infinite width networks in \u201c Motivation and Related work \u201d . We have changed it in the revised version to make it clearer that the width is considered infinite before taking the limit of large depth . 3 ) \u201d First sentence of the abstract is too strong in the sense that NTK has limitations . NTK certainly does not work for any kind of network , so I suggest authors to down tone the sentence . \u201d In the first sentence of the abstract , we are only referring to the exact NTK and not Mean-Field NTK . The Exact NTK is just another formulation of the Gradient Descent optimization problem as a Gradient in the Functional space , the exact NTK changes with training time t. We agree that when considering the Mean-Field NTK , that sentence is no longer correct . 4 ) \u201c P4 paragraph after Lemma1 ` ' K^L ( x , x \u2019 ) = cte \u2019 is a typo ? \u201d and \u201c P13 typo in last paragraph , ` NTKl ' \u201d Thank you . We have fixed the typos in the revised version 5 ) \u201c In section 5.2 , the authors \u2019 claim full batch GD is practically impossible . One could accumulate gradients over minibatch to simulate full batch GD . \u201d This is a very interesting suggestion . Just to make sure we understand correctly what you mean : by simulating the GD using minibatch gradients , we will still have some randomness , isn \u2019 t that equivalent to SGD with bigger batchsize ? Or are you suggesting this could potentially solve the memory cost problem ?"}, {"review_id": "r1eCy0NtDH-1", "review_text": "This paper discussed the property of the NTK with the increasing depth L with the help of the Edge of Chaos initialization. The authors show that if deep neural networks are not properly initialized, the NTK can have a large condition number, which leads to the poor performance of training and generalization. Moreover, the authors also introduce the conditions that make the neural network trainable by decreasing the convergence rate to a nearly constant kernel w.r.t the depth L by using the specific Edge of Chaos initialization as well as different activations and use residual connections. Experiment results show that the theoretical results are well aligned with the practice. Detailed Comments: 1. At the paragraph after Lemma 1, the authors claimed that f_t(x) is entirely given by f_0(x), which means a generalization error of order O(1). This is a little inaccurate I think, as NTK can only characterize the training process and does not directly indicate generalization. Also, as t to infinity, the coefficient of initialization f_0(x) can be arbitrary small and thus the f_t(x) is entirely decided by initialization is not accurate as well. To analyze generalization with NTK, we need a little more, see [1]. This is not some core issue, but I think the authors should make the description more accurate. 2. The authors should explain what richer limiting NTK means at the end of the Sec 3. It is unclear how the convergence rate of the NTK related to the richer limiting NTK. 3. A typo in the first paragraph in Sec 4, ouerselvs to ourselves. 4. In definition 1, the second-order derivative of \\phi is defined via the indicator function 1_{A_i}? It\u2019s better to use another notation like \\mathbf{1} or \\mathbb{1} to make it more clear. 5. I think for clarity the authors should give a formal definition of ANTK, or if it is unnecessary, better directly use the original NTK. Also, it is better to show the invertibility of the NTK is equivalent to ANTK more formally before Proposition 2. This conclusion is not so straightforward at the first view. Meanwhile, I think the invertibility of ANTK can just indicate a bad condition number of NTK? I don\u2019t think it directly related to the invertibility of NTK. 6. There are several typos in the appendix. Please go through the appendix and fix them. 7. In the proof of Lemma 3 and Lemma 4, I don\u2019t get why |a_l| < l + |a_0|. It may depends on the property of q? For sufficiently large q, if a_0=0, a_1 = q + O(1) which is not necessarily smaller than l=1? But to get a_l/l bounded, it is not necessary to use this. Also, I think it\u2019s better not omit the constant in the dynamic system of Lemma 3. 8. Better use the Stirling\u2019s approximation rather than k to infinity in the calculation of k!/(k-\\alpha-1)! 9. The lemma number of Appendix C is in a chaos. And I don\u2019t quite get the idea behinds Appendix C. Is it relevant to the contents in the main text? 10. I also feel the content in Appendix D is unnecessary for this paper. Overall, this paper is interesting and gives a unified perspective on the recently developed NTK and Edge of Chaos initialization. It also sheds light on the impact of different activation function on NTK by generalizing the results of [2]. I feel this paper can help the communities have more understanding on the properties of deep neural networks. However, this paper can be better if the authors can polish their paper and reorganize the appendix part. [1] Arora, Sanjeev, et al. \"Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.\" International Conference on Machine Learning. 2019. [2] Hayou, S., Doucet, A. & Rousseau, J.. (2019). On the Impact of the Activation function on Deep Neural Networks Training. Proceedings of the 36th International Conference on Machine Learning, in PMLR 97:2672-2680 ", "rating": "6: Weak Accept", "reply_text": "We thank Reviewer 2 for the detailed and insightful comments . We address them hereafter . 1 ) \u201c At the paragraph after Lemma 1 , the authors claimed that f_t ( x ) is entirely given by f_0 ( x ) , which means a generalization error of order O ( 1 ) . This is a little inaccurate I think , as NTK can only characterize the training process and does not directly indicate generalization . Also , as t to infinity , the coefficient of initialization f_0 ( x ) can be arbitrary small and thus the f_t ( x ) is entirely decided by initialization is not accurate as well . To analyze generalization with NTK , we need a little more , see [ 1 ] . This is not some core issue , but I think the authors should make the description more accurate . \u201d We have clarified the text in the revised version of the paper . By \u2018 generalization error \u2019 , we were only referring to the generalization of the linear model given by equation ( 9 ) and not the generalization of the neural network . This linear model can be a good approximation of the true dynamics of a neural network ( see Lee et al ( 2019 ) for example ) . As t grows to infinity in equation ( 9 ) , assuming the NTK remains invertible , we end up with a formula of f_t ( x ) that depends only on f_0 and the kernel ( which we assume to have constant covariances ) , therefore f_t ( x ) is entirely determined by f_0 ( x ) . Moreover , as Reviewer 2 mentioned , discussing the generalization error of a neural network from an NTK point of view would require a different technical analysis and is beyond the scope of this paper . 2 ) \u201c The authors should explain what richer limiting NTK means at the end of the Sec 3 . It is unclear how the convergence rate of the NTK related to the richer limiting NTK . \u201d By \u2018 richer \u2019 kernel we meant that with smooth activation functions , the NTK loses less information compared to when ReLU-like activations are used as the depth grows to infinity . We have rewritten this part of the paper . 3 ) \u201c I think for clarity the authors should give a formal definition of ANTK , or if it is unnecessary , better directly use the original NTK . Also , it is better to show the invertibility of the NTK is equivalent to ANTK more formally before Proposition 2 . This conclusion is not so straightforward at the first view . Meanwhile , I think the invertibility of ANTK can just indicate a bad condition number of NTK ? I don \u2019 t think it directly related to the invertibility of NTK . \u201d The ANTK is just a scaled version of the NTK ( ANTK = NTK / L ) . So the invertibility of the NTK is equivalent to that of the ANTK , and the condition numbers of both the ANTK and the NTK are the same ( here we define the condition number to be largest_eigenvalue / smallest_eigenvalue ) . We have clarified the definition of the ANTK in the revised version of the paper . 4 ) \u201c In the proof of Lemma 3 and Lemma 4 , I don \u2019 t get why |a_l| < l + |a_0| . It may depends on the property of q ? For sufficiently large q , if a_0=0 , a_1 = q + O ( 1 ) which is not necessarily smaller than l=1 ? But to get a_l/l bounded , it is not necessary to use this . Also , I think it \u2019 s better not omit the constant in the dynamic system of Lemma 3 . \u201d Thank you for pointing this out . Indeed , it should be \u201c constant * l \u201d in the right hand part . We have corrected this error in the revised version . 5 ) \u201c Better use the Stirling \u2019 s approximation rather than k to infinity in the calculation of k ! / ( k-\\alpha-1 ) ! \u201d We used an easy equivalent since it is a sum of a divergent series with positive terms , but Stirling \u2019 s formula would work too . 6 ) \u201c The lemma number of Appendix C is in a chaos . And I don \u2019 t quite get the idea behinds Appendix C. Is it relevant to the contents in the main text ? \u201d Thank you , we have fixed the numbering in the revised version . Results in Appendix C were introduced to confirm the impact of the EOC on the output function of the neural network . We agree that this is not directly linked to the NTK . However , as the NTK is a covariance matrix of the Gradient of the output function , we believe it is interesting to understand the behaviour of the output function for different initializations . 7 ) \u201c I also feel the content in Appendix D is unnecessary for this paper . \u201d So far , the NTK has only been studied with GD . The randomness of SGD makes it difficult to understand the limiting behaviour of the NTK in the infinite width limit , and we might even end up with a random NTK in some cases . We introduced Appendix D to shed light on this problem and pave the way for future projects in this direction . [ 1 ] Lee et al . ( 2019 ) \u201c Wide neural networks of any depth evolve as linear models under gradient descent . \u201d"}, {"review_id": "r1eCy0NtDH-2", "review_text": "The paper studies the limiting behavior of neural tangent kernels when the depth grows to infinity. They show that the obtained limit kernels are trivial (a constant) unless one uses 'edge of caos' initialization, in which case they are close to the identity. The authors compare the convergence for different activations, showing a slower convergence (hence better propagation) for some piecewise smooth activations compared to ReLU. For residual networks, the 'edge of caos' behavior is claimed to be in place regardless of the initialization. The detailed characterization of these limiting kernels for different activations and initializations is interesting. Yet, the work is quite incremental and of limited significance, in that such EOC initialization was known to be important for controlling propagation of both activations and gradients with such networks, so it is no surprise that the NTK has similar properties, given that it basically consists of the sum of similar gradient covariances. some comments: - title: isn't the \"mean-field behavior\" already subsumed in the definition of NTK? - contribution 4.: \"more suitable\" seems a bit strong if it's just a log(L) factor? - after lemma 1: generalization may not provide a strong argument here unless further discussed: the constant kernel is obviously bad for learning anything, but the EOC limiting kernel is also pretty bad for predicting anything outside the training data - section 3: typo \"ourselves\" - prop 3: specify that phi is the relu - the proof of prop 3 should be more detailed. Also, it is not obvious that you are giving the correct formula for the NTK of resnets - Table 1: is it meaningful to compare performance with such low accuracies?", "rating": "3: Weak Reject", "reply_text": "( this is part 2 of the response , please read part 1 first ) 6 ) \u201c - Table 1 : is it meaningful to compare performance with such low accuracies ? \u201d The goal here is mainly trainability and training speed of neural networks , and we approach this problem from a NTK point of view . We want to confirm the findings by training neural networks with different activations on MNIST and CIFAR10 . We compare accuracies after 10 epochs and 100 epochs of training to see whether the model is trainable first and which configuration is training faster . This table shows two facts : when the NTK converges exponentially fast to a constant non-invertible kernel ( which is the case for Sigmoid and Softplus since they do not have an EOC ) , the training is impossible and the model is stuck at a low accuracy of ~10 % which is the accuracy of a random classifier . The second fact is that , on the EOC , while for all activations ( that have an EOC ) the model is trainable , smooth activation functions like ELU and Tanh perform better than ReLU , Leaky-ReLU and PReLU . This confirms the results of Proposition 2 . [ 1 ] Lee et al . ( 2019 ) \u201c Wide neural networks of any depth evolve as linear models under gradient descent. \u201d [ 2 ] Schoenholz et al . ( 2017 ) \u201c Deep information propagation \u201d [ 3 ] Hayou et al . ( 2019 ) \u201c On the impact of the activation function on deep neural networks training \u201d"}], "0": {"review_id": "r1eCy0NtDH-0", "review_text": "This paper studies the role of weight/bias variance of neural network\u2019s trainability via analyzing large depth behaviour of Neural Tangent Kernels(NTK). Recently NTK has been a popular topic of study in theoretical deep learning as it describes exact gradient descent dynamics of infinitely wide networks. Original NTK paper (Jacot et al. (2018)) and other follow up papers often gloss over role of weight/bias scales whereas in the setting of signal propagation or NNGP covariance, Schoenholz et al. (2017), Lee et al. (2018), Novak et al. (2019), Hayou et al. (2019) have shown understanding initialization scale is quite important as networks becomes deeper. This paper brings those analysis for NTK and discovers few interesting results. The good weight/bias initialization scale that propagates signal for very deep network is denoted edge of chaos (EOC). The authors show that 1) for fully connected feed forward networks, outside initialization edge of chaos (EOC) the NTK converges exponentially to constant kernel. This indicates non-trainability. 2) With EOC initialization the convergence is polynomial and the NTK remains invertible for very large depth implying trainability. 3) Certain class of activation function (denoted class S, including ELU/Tanh/Swish) it has even slower convergence(O(log(L)/L)) compare to ReLU (O(1/L)). 4) Residual FC networks NTK has polynomial convergence for all weight and bias variance. In terms of novelty, the paper is combining existing techniques and objects to study large depth behaviour of NTK. However the contribution of the paper is interesting and worth the ICLR audience to know about, especially with the current surge of interest in NTK. One main weakness is that the experiments are weak and have a very weak connection to the early part of the paper. Section 5.1 is direct convergence comparison, which provides fair evidence. In section 5.2, I\u2019m not certain whether experiments displayed connects to asymptotic NTK analysis. First of all, in order to obtain NTK in the infinite width limit, one has to use `NTK parameterization\u2019 where one scales out 1/sqrt(fan_in) in network definition and not in weight initialization. It seems (by mention of He/Glorot init, and choice of learning rate O(1e-3/1e-4)) the authors experimented with standard parameterization. In this case the connection to very deep behaviour of NTK is not straightforward to actual network training since the training dynamics will be different. I suggest authors try experiments in NTK parameterization and see if results are similar. Few comments: It should be emphasized that the infinite width is taken first before taking infinite depth limit. There are subtle effects when depth/width are taken to infinity at the same time. The analysis on asymptotic behaviour of NTK at infinite depth is only valid after width is taken to infinity. First sentence of the abstract is too strong in the sense that NTK has limitations. NTK certainly does not work for any kind of network, so I suggest authors to down tone the sentence. P4 paragraph after Lemma1 `'K^L(x, x\u2019) = cte\u2019 is a typo? In section 5.2, the authors\u2019 claim full batch GD is practically impossible. One could accumulate gradients over minibatch to simulate full batch GD. P13 typo in last paragraph, ` NTKl' EDITS POST AUTHOR RESPONSE: Regarding point 5) I was referring to solving the memory problem. In practice, if one is sampling without replacement, there is no randomness accumulating gradient of the full epoch. I also thank the authors for the clarification. My score still remains the same. ", "rating": "6: Weak Accept", "reply_text": "We thank Reviewer 1 for the detailed and insightful feedback . We address them hereafter . 1 ) \u201c One main weakness is that the experiments are weak and have a very weak connection to the early part of the paper . Section 5.1 is direct convergence comparison , which provides fair evidence . In section 5.2 , I \u2019 m not certain whether experiments displayed connects to asymptotic NTK analysis ... \u201d We agree that the dynamics might be slightly different . However , in practice both parametrizations give similar results in terms of Trainability and Training Speed ( which are our focus in this paper ) since they both have the same EOC ( same EOC curve in ( \\sigma_b , \\sigma_w ) plane ) . More precisely , the NTK is the same for both parametrization at initialization , and in practice we use small learning rates for standard parametrization ( 1e-3 ) compared to the learning rates used with NTK parametrization ( to compensate for the scaling ) , we believe that the resulting training dynamics should not change much , unless we use small learning rates for the NTK parametrization which would result in very slow training . We added in the appendix ( Appendix E ) an example of training a network of depth 100 and width 100 with both parametrizations with an EOC initialization ( we used ELU in this example ) , it shows that both parametrizations have similar behaviour on the EOC . This is also true on the Ordered/Chaotic phase where the model is stuck at the random classifier accuracy ~10 % for both parametrizations . We will add more experimental results with NTK Parametrization in the appendix of the final version of the paper . 2 ) \u201c It should be emphasized that the infinite width is taken first before taking infinite depth limit . There are subtle effects when depth/width are taken to infinity at the same time . The analysis on asymptotic behaviour of NTK at infinite depth is only valid after width is taken to infinity . \u201c We have mentioned that we only deal with the infinite width networks in \u201c Motivation and Related work \u201d . We have changed it in the revised version to make it clearer that the width is considered infinite before taking the limit of large depth . 3 ) \u201d First sentence of the abstract is too strong in the sense that NTK has limitations . NTK certainly does not work for any kind of network , so I suggest authors to down tone the sentence . \u201d In the first sentence of the abstract , we are only referring to the exact NTK and not Mean-Field NTK . The Exact NTK is just another formulation of the Gradient Descent optimization problem as a Gradient in the Functional space , the exact NTK changes with training time t. We agree that when considering the Mean-Field NTK , that sentence is no longer correct . 4 ) \u201c P4 paragraph after Lemma1 ` ' K^L ( x , x \u2019 ) = cte \u2019 is a typo ? \u201d and \u201c P13 typo in last paragraph , ` NTKl ' \u201d Thank you . We have fixed the typos in the revised version 5 ) \u201c In section 5.2 , the authors \u2019 claim full batch GD is practically impossible . One could accumulate gradients over minibatch to simulate full batch GD . \u201d This is a very interesting suggestion . Just to make sure we understand correctly what you mean : by simulating the GD using minibatch gradients , we will still have some randomness , isn \u2019 t that equivalent to SGD with bigger batchsize ? Or are you suggesting this could potentially solve the memory cost problem ?"}, "1": {"review_id": "r1eCy0NtDH-1", "review_text": "This paper discussed the property of the NTK with the increasing depth L with the help of the Edge of Chaos initialization. The authors show that if deep neural networks are not properly initialized, the NTK can have a large condition number, which leads to the poor performance of training and generalization. Moreover, the authors also introduce the conditions that make the neural network trainable by decreasing the convergence rate to a nearly constant kernel w.r.t the depth L by using the specific Edge of Chaos initialization as well as different activations and use residual connections. Experiment results show that the theoretical results are well aligned with the practice. Detailed Comments: 1. At the paragraph after Lemma 1, the authors claimed that f_t(x) is entirely given by f_0(x), which means a generalization error of order O(1). This is a little inaccurate I think, as NTK can only characterize the training process and does not directly indicate generalization. Also, as t to infinity, the coefficient of initialization f_0(x) can be arbitrary small and thus the f_t(x) is entirely decided by initialization is not accurate as well. To analyze generalization with NTK, we need a little more, see [1]. This is not some core issue, but I think the authors should make the description more accurate. 2. The authors should explain what richer limiting NTK means at the end of the Sec 3. It is unclear how the convergence rate of the NTK related to the richer limiting NTK. 3. A typo in the first paragraph in Sec 4, ouerselvs to ourselves. 4. In definition 1, the second-order derivative of \\phi is defined via the indicator function 1_{A_i}? It\u2019s better to use another notation like \\mathbf{1} or \\mathbb{1} to make it more clear. 5. I think for clarity the authors should give a formal definition of ANTK, or if it is unnecessary, better directly use the original NTK. Also, it is better to show the invertibility of the NTK is equivalent to ANTK more formally before Proposition 2. This conclusion is not so straightforward at the first view. Meanwhile, I think the invertibility of ANTK can just indicate a bad condition number of NTK? I don\u2019t think it directly related to the invertibility of NTK. 6. There are several typos in the appendix. Please go through the appendix and fix them. 7. In the proof of Lemma 3 and Lemma 4, I don\u2019t get why |a_l| < l + |a_0|. It may depends on the property of q? For sufficiently large q, if a_0=0, a_1 = q + O(1) which is not necessarily smaller than l=1? But to get a_l/l bounded, it is not necessary to use this. Also, I think it\u2019s better not omit the constant in the dynamic system of Lemma 3. 8. Better use the Stirling\u2019s approximation rather than k to infinity in the calculation of k!/(k-\\alpha-1)! 9. The lemma number of Appendix C is in a chaos. And I don\u2019t quite get the idea behinds Appendix C. Is it relevant to the contents in the main text? 10. I also feel the content in Appendix D is unnecessary for this paper. Overall, this paper is interesting and gives a unified perspective on the recently developed NTK and Edge of Chaos initialization. It also sheds light on the impact of different activation function on NTK by generalizing the results of [2]. I feel this paper can help the communities have more understanding on the properties of deep neural networks. However, this paper can be better if the authors can polish their paper and reorganize the appendix part. [1] Arora, Sanjeev, et al. \"Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.\" International Conference on Machine Learning. 2019. [2] Hayou, S., Doucet, A. & Rousseau, J.. (2019). On the Impact of the Activation function on Deep Neural Networks Training. Proceedings of the 36th International Conference on Machine Learning, in PMLR 97:2672-2680 ", "rating": "6: Weak Accept", "reply_text": "We thank Reviewer 2 for the detailed and insightful comments . We address them hereafter . 1 ) \u201c At the paragraph after Lemma 1 , the authors claimed that f_t ( x ) is entirely given by f_0 ( x ) , which means a generalization error of order O ( 1 ) . This is a little inaccurate I think , as NTK can only characterize the training process and does not directly indicate generalization . Also , as t to infinity , the coefficient of initialization f_0 ( x ) can be arbitrary small and thus the f_t ( x ) is entirely decided by initialization is not accurate as well . To analyze generalization with NTK , we need a little more , see [ 1 ] . This is not some core issue , but I think the authors should make the description more accurate . \u201d We have clarified the text in the revised version of the paper . By \u2018 generalization error \u2019 , we were only referring to the generalization of the linear model given by equation ( 9 ) and not the generalization of the neural network . This linear model can be a good approximation of the true dynamics of a neural network ( see Lee et al ( 2019 ) for example ) . As t grows to infinity in equation ( 9 ) , assuming the NTK remains invertible , we end up with a formula of f_t ( x ) that depends only on f_0 and the kernel ( which we assume to have constant covariances ) , therefore f_t ( x ) is entirely determined by f_0 ( x ) . Moreover , as Reviewer 2 mentioned , discussing the generalization error of a neural network from an NTK point of view would require a different technical analysis and is beyond the scope of this paper . 2 ) \u201c The authors should explain what richer limiting NTK means at the end of the Sec 3 . It is unclear how the convergence rate of the NTK related to the richer limiting NTK . \u201d By \u2018 richer \u2019 kernel we meant that with smooth activation functions , the NTK loses less information compared to when ReLU-like activations are used as the depth grows to infinity . We have rewritten this part of the paper . 3 ) \u201c I think for clarity the authors should give a formal definition of ANTK , or if it is unnecessary , better directly use the original NTK . Also , it is better to show the invertibility of the NTK is equivalent to ANTK more formally before Proposition 2 . This conclusion is not so straightforward at the first view . Meanwhile , I think the invertibility of ANTK can just indicate a bad condition number of NTK ? I don \u2019 t think it directly related to the invertibility of NTK . \u201d The ANTK is just a scaled version of the NTK ( ANTK = NTK / L ) . So the invertibility of the NTK is equivalent to that of the ANTK , and the condition numbers of both the ANTK and the NTK are the same ( here we define the condition number to be largest_eigenvalue / smallest_eigenvalue ) . We have clarified the definition of the ANTK in the revised version of the paper . 4 ) \u201c In the proof of Lemma 3 and Lemma 4 , I don \u2019 t get why |a_l| < l + |a_0| . It may depends on the property of q ? For sufficiently large q , if a_0=0 , a_1 = q + O ( 1 ) which is not necessarily smaller than l=1 ? But to get a_l/l bounded , it is not necessary to use this . Also , I think it \u2019 s better not omit the constant in the dynamic system of Lemma 3 . \u201d Thank you for pointing this out . Indeed , it should be \u201c constant * l \u201d in the right hand part . We have corrected this error in the revised version . 5 ) \u201c Better use the Stirling \u2019 s approximation rather than k to infinity in the calculation of k ! / ( k-\\alpha-1 ) ! \u201d We used an easy equivalent since it is a sum of a divergent series with positive terms , but Stirling \u2019 s formula would work too . 6 ) \u201c The lemma number of Appendix C is in a chaos . And I don \u2019 t quite get the idea behinds Appendix C. Is it relevant to the contents in the main text ? \u201d Thank you , we have fixed the numbering in the revised version . Results in Appendix C were introduced to confirm the impact of the EOC on the output function of the neural network . We agree that this is not directly linked to the NTK . However , as the NTK is a covariance matrix of the Gradient of the output function , we believe it is interesting to understand the behaviour of the output function for different initializations . 7 ) \u201c I also feel the content in Appendix D is unnecessary for this paper . \u201d So far , the NTK has only been studied with GD . The randomness of SGD makes it difficult to understand the limiting behaviour of the NTK in the infinite width limit , and we might even end up with a random NTK in some cases . We introduced Appendix D to shed light on this problem and pave the way for future projects in this direction . [ 1 ] Lee et al . ( 2019 ) \u201c Wide neural networks of any depth evolve as linear models under gradient descent . \u201d"}, "2": {"review_id": "r1eCy0NtDH-2", "review_text": "The paper studies the limiting behavior of neural tangent kernels when the depth grows to infinity. They show that the obtained limit kernels are trivial (a constant) unless one uses 'edge of caos' initialization, in which case they are close to the identity. The authors compare the convergence for different activations, showing a slower convergence (hence better propagation) for some piecewise smooth activations compared to ReLU. For residual networks, the 'edge of caos' behavior is claimed to be in place regardless of the initialization. The detailed characterization of these limiting kernels for different activations and initializations is interesting. Yet, the work is quite incremental and of limited significance, in that such EOC initialization was known to be important for controlling propagation of both activations and gradients with such networks, so it is no surprise that the NTK has similar properties, given that it basically consists of the sum of similar gradient covariances. some comments: - title: isn't the \"mean-field behavior\" already subsumed in the definition of NTK? - contribution 4.: \"more suitable\" seems a bit strong if it's just a log(L) factor? - after lemma 1: generalization may not provide a strong argument here unless further discussed: the constant kernel is obviously bad for learning anything, but the EOC limiting kernel is also pretty bad for predicting anything outside the training data - section 3: typo \"ourselves\" - prop 3: specify that phi is the relu - the proof of prop 3 should be more detailed. Also, it is not obvious that you are giving the correct formula for the NTK of resnets - Table 1: is it meaningful to compare performance with such low accuracies?", "rating": "3: Weak Reject", "reply_text": "( this is part 2 of the response , please read part 1 first ) 6 ) \u201c - Table 1 : is it meaningful to compare performance with such low accuracies ? \u201d The goal here is mainly trainability and training speed of neural networks , and we approach this problem from a NTK point of view . We want to confirm the findings by training neural networks with different activations on MNIST and CIFAR10 . We compare accuracies after 10 epochs and 100 epochs of training to see whether the model is trainable first and which configuration is training faster . This table shows two facts : when the NTK converges exponentially fast to a constant non-invertible kernel ( which is the case for Sigmoid and Softplus since they do not have an EOC ) , the training is impossible and the model is stuck at a low accuracy of ~10 % which is the accuracy of a random classifier . The second fact is that , on the EOC , while for all activations ( that have an EOC ) the model is trainable , smooth activation functions like ELU and Tanh perform better than ReLU , Leaky-ReLU and PReLU . This confirms the results of Proposition 2 . [ 1 ] Lee et al . ( 2019 ) \u201c Wide neural networks of any depth evolve as linear models under gradient descent. \u201d [ 2 ] Schoenholz et al . ( 2017 ) \u201c Deep information propagation \u201d [ 3 ] Hayou et al . ( 2019 ) \u201c On the impact of the activation function on deep neural networks training \u201d"}}