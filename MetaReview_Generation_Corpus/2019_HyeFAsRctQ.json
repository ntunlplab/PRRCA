{"year": "2019", "forum": "HyeFAsRctQ", "title": "Verification of Non-Linear Specifications for Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of neural networks under adversarial examples.\n\nThe reviewers were unanimous in their vote to accept the paper. Note: the remaining score of 5 belongs to a reviewer who agreed to acceptance in the discussion.", "reviews": [{"review_id": "HyeFAsRctQ-0", "review_text": "- Summary: This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of the network under adversarial examples. Experimental results are shown for semantic specifications for CIFAR, errors in predicting sum of two digits and conservation of energy in a simple pendulum. - Clarity and correctness: It is a well-written and well-organized paper. Notations and expressions are clear. The math seems to be correct. - Significance: The paper claims to have introduced a class of convex-relaxable specifications which constitute specifications that can be verified using a convex relaxation. However, as described later in the paper, it is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts (it would be better to tone down the claims in the abstract and introduction parts.) - Novelty: The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical. This paper introduces some nice ideas to generalize linear verification functions to a larger class of convex-relaxable functions, however, it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results. ** More detailed comments: ** The idea of generalizing verifications to a convex-relaxable set is interesting, however, applying it in general is not very clear -- as the authors worked on a case by case basis in section 3.1. ** One of my main concerns is regarding the relaxation step. There is no discussion on the effects of the tightness of the relaxation on the actual results of the models; when in reality, there is an infinite pool of candidates for 'convexifying' the verification functions. It would be nice to see that analysis as well as a discussion on how much are we willing to lose w.r.t. to the tightness of the bounds -- especially when there is a trade-off between better approximation to the verification function and tightness of the bound. ** I barely found the experimental results satisfying. To find \"reasonable\" inputs to the model, authors considered perturbing points in the test set. However, I am not sure if this is a reasonable assumption when there would be no access to test data points when training a neural network with robustness to adversarial examples. And if bounding them is a very hard task, I am wondering if that is a reasonable assumption to begin with. ** It is hard to have a sense of how good the results are in Figure 1 due to lack of benchmark results (I could not find them in the Appendix either.) ** The experimental results in section 4.4 are very limited. I suggest that the authors consider running more experiments on more data sets and re-running them with more settings (N=2 for digit sums looks very limited, and if increasing N has some effects, it would be nice to see them or discuss those effects.) ** Page 2, \"if they do a find a proof\" should be --> \"if they do find a proof\" ** Page 5, \"(as described in Section (Bunel et al., 2017; Dvijotham et al., 2018)\", \"Section\" should be omitted. ****************************************************** After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring. One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed feedback and criticism . We made adjustments to the paper to address all your concerns and detail the changes below . We hope the changes clarify the concerns regarding the generality of our algorithm and the requested additional experiments . Comment 1 : [ The method is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts ( it would be better to tone down the claims in the abstract and introduction parts . ) ] Answer 1 : We want to clarify that although we have demonstrated most of the results on ReLU feedforward neural networks , it is not limited to such networks . The feedforward nature is indeed required but the ReLU activation function can be replaced with arbitrary activation functions , for example tanh or sigmoid activations ( please see https : //arxiv.org/abs/1803.06567 for more details ) . We initially used the ReLU example for clarity of presentation , as a result , maybe the generality of our result is not clear . To address this we have updated Sections 3.1 , 3.3 and 3.4 . Specifically , we changed the equation : X_ { k+1 } = ReLU ( W_k x_k + b_k ) to X_ { k+1 } = g_k ( W_k x_k + b_k ) . The only change required going from the ReLU equation to the more general equation is the way the bounds ( [ l_k , u_k ] ) are propagated through the network and the relaxations applied on the activation functions . For a more general overview of the bound propagation techniques and relaxation of arbitrary activation functions we refer to the following papers https : //arxiv.org/abs/1803.06567 , https : //arxiv.org/pdf/1610.06940.pdf , https : //arxiv.org/abs/1805.12514 . We would also like to clarify that this paper provides a framework for general nonlinear specifications that are convex-relaxable . Although we presented softmax and quadratic specifications this algorithm is not limited to these two cases . To demonstrate this further , we have added Appendix I where we find a convex-relaxation to the entropy of a softmax distribution from a classifier network and use it to verify that a given network is never overly confident . In other words ; we would like to verify that a threshold on the entropy of the class probabilities is never violated . The specification at hand is the following : F ( x , y ) = E + \\sum_i exp ( y_i ) / ( \\sum_j exp ( y_j ) ) log ( exp ( y_i ) / ( \\sum_j exp ( y_j ) ) ) < =0 which is a non-convex function of the network outputs . Comment 2 : Novelty : The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical . This paper introduces some nice ideas to generalize linear verification functions [ ... ] it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results . Answer 2 : We emphasize that our method is not limited to quadratic expressions and label semantics and refer to Answer 1 , above , for comments regarding the generality . Regarding your concerns wrt the novelty of the approach : as far as we are aware there is no prior paper considering the problem of verifying nonlinear specifications for neural networks . Regarding the presentation of results : We refer to Answer 6 , below , for a detailed justification of our experimental procedure . Additionally we want to highlight that our verification tool was a useful diagnostic in finding the failure modes of pendulum and CIFAR10 models . An example is that when we are able to verify that the pendulum model satisfies energy conservation more - the long term dynamics of the model always reaches a stable equilibrium ."}, {"review_id": "HyeFAsRctQ-1", "review_text": "This paper uses convex relaxation to verify a larger class of specifications for neural network's properties. Many previous papers use convex relaxations on the ReLU activation function and solve a relaxed convex problem to give verification bounds. However, most papers consider the verification specification simply as an affine transformation of neural network's output. This paper extends the verification specifications to a larger family of functions that can be efficiently relaxed. The author demonstrates three use cases for non-linear specifications, including verifying specifications involving label semantics, physic laws and down-stream tasks, and show some experiments that the proposed verification method can find non-vacuous bound for these problems. Additionally, this paper shows some interesting experiments on the value of verification - a more verifiable model seems to provide more interpretable results. Overall, the proposed method seems to be a straightforward extension to existing works like [2]. However the demonstrated applications of non-linear specifications are indeed interesting, and the proposed method works well on these tasks. I have some minor questions regarding this paper: 1) For some non-linear specifications, we can convert these non-linear elements into activation functions, and build an equivalent network for verification such that the final verification specification becomes linear. For example, for verifying the quadratic specification in physics we can add a \"quadratic activation function\" to the network and deal with it using techniques in [1] or [2]. The authors should distinguish the proposed technique with these existing techniques. My understanding is that the proposed method is more general, but the authors should better discussing more on the differences in this paper. 2) The authors should report the details on how they solve the relaxed convex problem, and report verification time. Are there any tricks used to improve solving time? What is the largest scale of network that the algorithm can handle within a reasonable time? 3) The detailed network architecture (Model A, Model B) is not shown. How many layers and neurons are there in these networks? This is important to show the scalability of the proposed method. 4) For the Mujoco experiment, I am not sure how to interpret the delta values in Figure 1. For CIFAR I know it is the delta of pixel values but it is not clear about the delta in Mujoco model. What is the normal range of predicted numbers in this model? How does the delta compare to it? Is the delta very small or trivial? 5) Is it possible to show how loose the convex relaxation is for a small toy example? For example, the specification involving quadratic function is a good candidate. There are some small glitches in equations: * In (4), k is undefined * In (20), I am not sure if it is equivalent to the four inequalities after (22). There are 4 inequalities after (22) but only 3 in (20). Many papers uses convex relaxations for neural network verification. However very few of them can deal with general non-linear units in neural networks. ReLU activation is usually the only non-linear element than we can handle in most neural network verification works. Currently the only works that can handle other general non-linear elements are [1][2]. This paper uses more general convex relaxations than these previous approaches, and it can handle non-separable non-linear specifications. This is a unique contribution to this field. I recommend accepting this paper as long as the minor issues mentioned above can be fixed. [1] \"Efficient Neural Network Robustness Certification with General Activation Functions\" by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel. NIPS 2018 [2] \"A dual approach to scalable verification of deep networks.\" by Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. UAI 2018. ", "rating": "7: Good paper, accept", "reply_text": "Comment 1 : [ The authors should distinguish the proposed technique to techniques from [ 1 ] and [ 2 ] which could be used to convert some non-linear specifications to linear specifications . ] Answer 1 : We thank the reviewer for highlighting this point , we have now added a paragraph in the section \u2018 Specifications Beyond Robustness \u2019 to distinguish between existing techniques and convex relaxable specifications . The reviewer is correct in pointing out that some non-linearities can indeed be linearized through the use of different element-wise activation functions . However , in terms of generality as the reviewer mentioned , this mechanism does not work in many cases - an example is the softmax function , which needs every input in the layer to give it \u2019 s output . In this particular case , it is a non-separable nonlinear function and current literature does not support verification with such non-linearities ."}, {"review_id": "HyeFAsRctQ-2", "review_text": "This paper considers more general non-linear verifications, which can be convexified, for neural networks, and demonstrate that the proposed methodology is capable of modeling several important properties, including the conversation law, semantic consistency, and bounding errors. A few other comments *) Is it critical that the non-linear verifications need to be convex relaxable. Recently, people have observed that a lot of nonconvex optimization problems also have good local solutions. Is it true that the convex relaxable condition is only required for provable algorithm? As the neural network itself is nonconvex, constraining the specification to be convex is a little awkward to me. *) The paper contains the example specification functions derived for three specific purpose, I'm wondering how broad the proposed technique could be. Say if I need my neural network to satisfy other additional properties, is there a general recipe or guideline. If not, what's the difficulty intuitively speaking? The paper needs to be carefully proofread, and a lot of commas are missing.", "rating": "5: Marginally below acceptance threshold", "reply_text": "From your comments it seems that there was a misunderstanding regarding the general applicability of our method . We have updated the paper and provided extensive additional explanations below ( please also consider our reply to all authors ) . To address the comments you have made : Comment 1 : Is it critical that the non-linear verifications need to be convex relaxable . Recently , people have observed that a lot of nonconvex optimization problems also have good local solutions . Is it true that the convex relaxable condition is only required for provable algorithm ? As the neural network itself is nonconvex , constraining the specification to be convex is a little awkward to me . Answer 1 : For verification purposes it is indeed critical that we have either the global optimum value or an upper bound on the global optimum value . Verification of neural networks tries to find a proof that the specification , F ( x , y ) < = 0 , is satisfied for all x and y within a bounded set ( https : //arxiv.org/abs/1803.06567 ) . Note that this condition is equivalent to max_ { x , y } F ( x , y ) < = 0 , thus if we have the global maximum - the problem is solved . However , to find the global optimum value is often NP-hard even for ReLU networks ( https : //arxiv.org/abs/1705.01320 ) . We can try to find a lower bound to the global optimum value by doing gradient descent to maximize the value of F ( x , y ) . This is called a falsification procedure ( as explained in Section 3.1 ) . However , even if the value found is not greater than zero this is not sufficient to give a guarantee that there exists no x and y which can violate the specification , as the value is always a lower bound to the global optimum . Thus , we are motivated to find provable upper bounds on max F ( x , y ) , ie , a number U such that F ( x , y ) < = U for all x , y in the input and output domain . If this U < =0 then we have found a guarantee that the specification is never violated . In order to do this , we study convex relaxations of this problem that enable computation of provable upper bounds . We also do not require the specification to be convex ( for example the physics specification isn \u2019 t if Q is not a semi-definite matrix ) , the specification can be some complicated nonlinear function - we just require that it be convex-relaxable , which is a weaker requirement . We slightly rephrased Section 3 to make this point more obvious . Comment 2 : The paper contains the example specification functions derived for three specific purpose , I 'm wondering how broad the proposed technique could be . Say if I need my neural network to satisfy other additional properties , is there a general recipe or guideline . If not , what 's the difficulty intuitively speaking ? Answer 2 : This proposed technique is capable handling all specifications which are convex-relaxable , i.e.any specification for which the set of values that ( x , y , F ( x , y ) ) can take can be bounded by a convex set . The difficulty here is always getting a tight convex set on the specification you would like to verify for . There is a lot of literature in finding tight convex sets ( https : //eng.uok.ac.ir/mfathi/Courses/Advanced % 20Eng % 20Math/Linear % 20and % 20Nonlinear % 20Programming.pdf ) , we have chosen to demonstrate the generality of our framework with three specifications that we deem to be important . In general any convex-relaxable specification can be treated in the same manner as in the paper but , of course , finding a tight convex set should be done on a case-by-case basis . We added an additional example , going beyond quadratic constraints in Appendix I . Here we verify that a given classifier is never overly confident , in other words ; we would like to verify that a threshold on the entropy of the class probabilities is never violated . We would also like to emphasize that this paper is aimed to do post-hoc verification , where we consider a scenario that we are given a pre-trained neural network . Thus this is different to training your neural network to satisfy desirable properties , it is rather a safety measure before the network is put into deployment for real world applications . Comment 3 : [ The reviewer also commented on a lack of commas ] Could you please expand upon this point ?"}], "0": {"review_id": "HyeFAsRctQ-0", "review_text": "- Summary: This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of the network under adversarial examples. Experimental results are shown for semantic specifications for CIFAR, errors in predicting sum of two digits and conservation of energy in a simple pendulum. - Clarity and correctness: It is a well-written and well-organized paper. Notations and expressions are clear. The math seems to be correct. - Significance: The paper claims to have introduced a class of convex-relaxable specifications which constitute specifications that can be verified using a convex relaxation. However, as described later in the paper, it is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts (it would be better to tone down the claims in the abstract and introduction parts.) - Novelty: The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical. This paper introduces some nice ideas to generalize linear verification functions to a larger class of convex-relaxable functions, however, it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results. ** More detailed comments: ** The idea of generalizing verifications to a convex-relaxable set is interesting, however, applying it in general is not very clear -- as the authors worked on a case by case basis in section 3.1. ** One of my main concerns is regarding the relaxation step. There is no discussion on the effects of the tightness of the relaxation on the actual results of the models; when in reality, there is an infinite pool of candidates for 'convexifying' the verification functions. It would be nice to see that analysis as well as a discussion on how much are we willing to lose w.r.t. to the tightness of the bounds -- especially when there is a trade-off between better approximation to the verification function and tightness of the bound. ** I barely found the experimental results satisfying. To find \"reasonable\" inputs to the model, authors considered perturbing points in the test set. However, I am not sure if this is a reasonable assumption when there would be no access to test data points when training a neural network with robustness to adversarial examples. And if bounding them is a very hard task, I am wondering if that is a reasonable assumption to begin with. ** It is hard to have a sense of how good the results are in Figure 1 due to lack of benchmark results (I could not find them in the Appendix either.) ** The experimental results in section 4.4 are very limited. I suggest that the authors consider running more experiments on more data sets and re-running them with more settings (N=2 for digit sums looks very limited, and if increasing N has some effects, it would be nice to see them or discuss those effects.) ** Page 2, \"if they do a find a proof\" should be --> \"if they do find a proof\" ** Page 5, \"(as described in Section (Bunel et al., 2017; Dvijotham et al., 2018)\", \"Section\" should be omitted. ****************************************************** After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring. One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed feedback and criticism . We made adjustments to the paper to address all your concerns and detail the changes below . We hope the changes clarify the concerns regarding the generality of our algorithm and the requested additional experiments . Comment 1 : [ The method is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts ( it would be better to tone down the claims in the abstract and introduction parts . ) ] Answer 1 : We want to clarify that although we have demonstrated most of the results on ReLU feedforward neural networks , it is not limited to such networks . The feedforward nature is indeed required but the ReLU activation function can be replaced with arbitrary activation functions , for example tanh or sigmoid activations ( please see https : //arxiv.org/abs/1803.06567 for more details ) . We initially used the ReLU example for clarity of presentation , as a result , maybe the generality of our result is not clear . To address this we have updated Sections 3.1 , 3.3 and 3.4 . Specifically , we changed the equation : X_ { k+1 } = ReLU ( W_k x_k + b_k ) to X_ { k+1 } = g_k ( W_k x_k + b_k ) . The only change required going from the ReLU equation to the more general equation is the way the bounds ( [ l_k , u_k ] ) are propagated through the network and the relaxations applied on the activation functions . For a more general overview of the bound propagation techniques and relaxation of arbitrary activation functions we refer to the following papers https : //arxiv.org/abs/1803.06567 , https : //arxiv.org/pdf/1610.06940.pdf , https : //arxiv.org/abs/1805.12514 . We would also like to clarify that this paper provides a framework for general nonlinear specifications that are convex-relaxable . Although we presented softmax and quadratic specifications this algorithm is not limited to these two cases . To demonstrate this further , we have added Appendix I where we find a convex-relaxation to the entropy of a softmax distribution from a classifier network and use it to verify that a given network is never overly confident . In other words ; we would like to verify that a threshold on the entropy of the class probabilities is never violated . The specification at hand is the following : F ( x , y ) = E + \\sum_i exp ( y_i ) / ( \\sum_j exp ( y_j ) ) log ( exp ( y_i ) / ( \\sum_j exp ( y_j ) ) ) < =0 which is a non-convex function of the network outputs . Comment 2 : Novelty : The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical . This paper introduces some nice ideas to generalize linear verification functions [ ... ] it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results . Answer 2 : We emphasize that our method is not limited to quadratic expressions and label semantics and refer to Answer 1 , above , for comments regarding the generality . Regarding your concerns wrt the novelty of the approach : as far as we are aware there is no prior paper considering the problem of verifying nonlinear specifications for neural networks . Regarding the presentation of results : We refer to Answer 6 , below , for a detailed justification of our experimental procedure . Additionally we want to highlight that our verification tool was a useful diagnostic in finding the failure modes of pendulum and CIFAR10 models . An example is that when we are able to verify that the pendulum model satisfies energy conservation more - the long term dynamics of the model always reaches a stable equilibrium ."}, "1": {"review_id": "HyeFAsRctQ-1", "review_text": "This paper uses convex relaxation to verify a larger class of specifications for neural network's properties. Many previous papers use convex relaxations on the ReLU activation function and solve a relaxed convex problem to give verification bounds. However, most papers consider the verification specification simply as an affine transformation of neural network's output. This paper extends the verification specifications to a larger family of functions that can be efficiently relaxed. The author demonstrates three use cases for non-linear specifications, including verifying specifications involving label semantics, physic laws and down-stream tasks, and show some experiments that the proposed verification method can find non-vacuous bound for these problems. Additionally, this paper shows some interesting experiments on the value of verification - a more verifiable model seems to provide more interpretable results. Overall, the proposed method seems to be a straightforward extension to existing works like [2]. However the demonstrated applications of non-linear specifications are indeed interesting, and the proposed method works well on these tasks. I have some minor questions regarding this paper: 1) For some non-linear specifications, we can convert these non-linear elements into activation functions, and build an equivalent network for verification such that the final verification specification becomes linear. For example, for verifying the quadratic specification in physics we can add a \"quadratic activation function\" to the network and deal with it using techniques in [1] or [2]. The authors should distinguish the proposed technique with these existing techniques. My understanding is that the proposed method is more general, but the authors should better discussing more on the differences in this paper. 2) The authors should report the details on how they solve the relaxed convex problem, and report verification time. Are there any tricks used to improve solving time? What is the largest scale of network that the algorithm can handle within a reasonable time? 3) The detailed network architecture (Model A, Model B) is not shown. How many layers and neurons are there in these networks? This is important to show the scalability of the proposed method. 4) For the Mujoco experiment, I am not sure how to interpret the delta values in Figure 1. For CIFAR I know it is the delta of pixel values but it is not clear about the delta in Mujoco model. What is the normal range of predicted numbers in this model? How does the delta compare to it? Is the delta very small or trivial? 5) Is it possible to show how loose the convex relaxation is for a small toy example? For example, the specification involving quadratic function is a good candidate. There are some small glitches in equations: * In (4), k is undefined * In (20), I am not sure if it is equivalent to the four inequalities after (22). There are 4 inequalities after (22) but only 3 in (20). Many papers uses convex relaxations for neural network verification. However very few of them can deal with general non-linear units in neural networks. ReLU activation is usually the only non-linear element than we can handle in most neural network verification works. Currently the only works that can handle other general non-linear elements are [1][2]. This paper uses more general convex relaxations than these previous approaches, and it can handle non-separable non-linear specifications. This is a unique contribution to this field. I recommend accepting this paper as long as the minor issues mentioned above can be fixed. [1] \"Efficient Neural Network Robustness Certification with General Activation Functions\" by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel. NIPS 2018 [2] \"A dual approach to scalable verification of deep networks.\" by Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. UAI 2018. ", "rating": "7: Good paper, accept", "reply_text": "Comment 1 : [ The authors should distinguish the proposed technique to techniques from [ 1 ] and [ 2 ] which could be used to convert some non-linear specifications to linear specifications . ] Answer 1 : We thank the reviewer for highlighting this point , we have now added a paragraph in the section \u2018 Specifications Beyond Robustness \u2019 to distinguish between existing techniques and convex relaxable specifications . The reviewer is correct in pointing out that some non-linearities can indeed be linearized through the use of different element-wise activation functions . However , in terms of generality as the reviewer mentioned , this mechanism does not work in many cases - an example is the softmax function , which needs every input in the layer to give it \u2019 s output . In this particular case , it is a non-separable nonlinear function and current literature does not support verification with such non-linearities ."}, "2": {"review_id": "HyeFAsRctQ-2", "review_text": "This paper considers more general non-linear verifications, which can be convexified, for neural networks, and demonstrate that the proposed methodology is capable of modeling several important properties, including the conversation law, semantic consistency, and bounding errors. A few other comments *) Is it critical that the non-linear verifications need to be convex relaxable. Recently, people have observed that a lot of nonconvex optimization problems also have good local solutions. Is it true that the convex relaxable condition is only required for provable algorithm? As the neural network itself is nonconvex, constraining the specification to be convex is a little awkward to me. *) The paper contains the example specification functions derived for three specific purpose, I'm wondering how broad the proposed technique could be. Say if I need my neural network to satisfy other additional properties, is there a general recipe or guideline. If not, what's the difficulty intuitively speaking? The paper needs to be carefully proofread, and a lot of commas are missing.", "rating": "5: Marginally below acceptance threshold", "reply_text": "From your comments it seems that there was a misunderstanding regarding the general applicability of our method . We have updated the paper and provided extensive additional explanations below ( please also consider our reply to all authors ) . To address the comments you have made : Comment 1 : Is it critical that the non-linear verifications need to be convex relaxable . Recently , people have observed that a lot of nonconvex optimization problems also have good local solutions . Is it true that the convex relaxable condition is only required for provable algorithm ? As the neural network itself is nonconvex , constraining the specification to be convex is a little awkward to me . Answer 1 : For verification purposes it is indeed critical that we have either the global optimum value or an upper bound on the global optimum value . Verification of neural networks tries to find a proof that the specification , F ( x , y ) < = 0 , is satisfied for all x and y within a bounded set ( https : //arxiv.org/abs/1803.06567 ) . Note that this condition is equivalent to max_ { x , y } F ( x , y ) < = 0 , thus if we have the global maximum - the problem is solved . However , to find the global optimum value is often NP-hard even for ReLU networks ( https : //arxiv.org/abs/1705.01320 ) . We can try to find a lower bound to the global optimum value by doing gradient descent to maximize the value of F ( x , y ) . This is called a falsification procedure ( as explained in Section 3.1 ) . However , even if the value found is not greater than zero this is not sufficient to give a guarantee that there exists no x and y which can violate the specification , as the value is always a lower bound to the global optimum . Thus , we are motivated to find provable upper bounds on max F ( x , y ) , ie , a number U such that F ( x , y ) < = U for all x , y in the input and output domain . If this U < =0 then we have found a guarantee that the specification is never violated . In order to do this , we study convex relaxations of this problem that enable computation of provable upper bounds . We also do not require the specification to be convex ( for example the physics specification isn \u2019 t if Q is not a semi-definite matrix ) , the specification can be some complicated nonlinear function - we just require that it be convex-relaxable , which is a weaker requirement . We slightly rephrased Section 3 to make this point more obvious . Comment 2 : The paper contains the example specification functions derived for three specific purpose , I 'm wondering how broad the proposed technique could be . Say if I need my neural network to satisfy other additional properties , is there a general recipe or guideline . If not , what 's the difficulty intuitively speaking ? Answer 2 : This proposed technique is capable handling all specifications which are convex-relaxable , i.e.any specification for which the set of values that ( x , y , F ( x , y ) ) can take can be bounded by a convex set . The difficulty here is always getting a tight convex set on the specification you would like to verify for . There is a lot of literature in finding tight convex sets ( https : //eng.uok.ac.ir/mfathi/Courses/Advanced % 20Eng % 20Math/Linear % 20and % 20Nonlinear % 20Programming.pdf ) , we have chosen to demonstrate the generality of our framework with three specifications that we deem to be important . In general any convex-relaxable specification can be treated in the same manner as in the paper but , of course , finding a tight convex set should be done on a case-by-case basis . We added an additional example , going beyond quadratic constraints in Appendix I . Here we verify that a given classifier is never overly confident , in other words ; we would like to verify that a threshold on the entropy of the class probabilities is never violated . We would also like to emphasize that this paper is aimed to do post-hoc verification , where we consider a scenario that we are given a pre-trained neural network . Thus this is different to training your neural network to satisfy desirable properties , it is rather a safety measure before the network is put into deployment for real world applications . Comment 3 : [ The reviewer also commented on a lack of commas ] Could you please expand upon this point ?"}}