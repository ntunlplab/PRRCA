{"year": "2021", "forum": "M9hdyCNlWaf", "title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "decision": "Reject", "meta_review": "The scores for this paper have been borderline, however the decision has been greatly facilitated by the participation of the authors and reviewers to the discussion and, more importantly, by active private discussion among reviewers and AC. Specifically, from the private discussion it seems that the reviewers find interesting ideas in this paper, but are overall are not entirely convinced about its significance, at least in the way the paper is currently positioned and motivated. \n\nMore specifically, the reviewers found the main idea of using inducing weights interesting, both technically (e.g. associated sampling scheme) but also in terms of application (sparsity). The results are insightful from a theoretical perspective. That is, the inducing weights and their treatment does seem to result in interesting and potentially useful statistical properties for the model. On the other hand, it is important to note that the high-level idea of variational inducing weights, with usage of matrix normals in this setting, as well as connection to deep GPs has been studied before, as pointed out by R2 (refs [1,2]). Furthermore, even after discussions the motivation is still not entirely convincing, especially in conjunction with the experiments. Although various interesting ideas exist in the paper, both R2 and R3 in particular remain unconvinced about what is the main benefit (e.g.  pruning or runtime efficiency) stemming out of the proposed idea. Another reviewer agreed with this point in the private discussions.  Apart from overall clearer positioning of the paper, the claimed benefit would need to be supported by experiments tailored to illustrate this main point. The authors argued against some of the suggested comparisons (e.g. past pruning methods), and further discuss that there is no established experimental benchmark for the parameter efficiency of BNNs. I indeed sympathize with both of these arguments; however, I believe that if the reviewers' suggestions for extra experiments are rejected, it remains the responsibility of the authors to find a slightly different way of motivating their work and demonstrating its efficiency in some convincing, meaningful and more well-defined setting with the appropriate benchmarks.  \n", "reviews": [{"review_id": "M9hdyCNlWaf-0", "review_text": "# summary This paper proposed a method on uncertainty estimation in deep neural networks . Compared with BNN and deep ensemble , the proposed approach in this work has a storage advantage . Furthermore , this work provides a better trade-off between accuracy and calibration . # pros 1.The approach in this work is quite interesting . The idea of augmenting weights with auxiliary low-dimensional latent variables in a deep neural network seems natural at first sight , but this approach is novel as far as my knowledge is concerned . Although VI with local latent variables is an old technique , this application in deep neural network is novel . 2.The authors also proposed an efficient approach that can sample from the variational approximation conditioning on the latent variable . Since the original weight is large , such a sampling is necessary . 3.This paper provides extensive empirical results and sufficient theoretical results . Experimental results show the proposed approach achieve a good balance between accuracy , calibration and memory requirements . # cons 1.My major concern is all experiments are conducted on ResNet18 , and this is not a typical choice for practical problems . I think experiments on other larger net such as ResNet56 on CIFAR10 will make this paper more convincing . 2.It is not clear to me how the authors choose the `` mixture of delta measures '' , i.e.how to choose U < sup > k < /sup > ? Can the authors comment on this ? In this case , q ( U ) is a categorical distribution . It seems better if q ( U ) also depends on input data , however , the authors choose to use a fixed q ( U ) . Can the authors comment on this choice ? Overall speaking , the idea and the presentation of this work are great in my opinion .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your encouraging review . We are particularly pleased that you appreciate the novelty of the idea and the clarity of the presentation . * * Architecture size * * In order to avoid possible confusion regarding the architecture , we would like to emphasize that the Resnet18 architecture we use is based on the torchvision implementation , i.e.the smallest ImageNet-sized architecture ( > 11M parameters ) , * * not * * the CIFAR-sized Resnet20 architecture from the original Resnet paper ( < 1M parameters ) . The Resnet56 architecture you suggest was also developed for CIFAR ( < 1M parameters ) -- did you mean Resnet50 ( > 22M parameters ) or were you under the impression that we were using the smaller CIFAR architectures ? While we fully agree that Resnet50 would be more representative of an architecture that a practitioner would use , we do believe that Resnet18 captures the most relevant properties of a modern deep neural network . For example , Osawa et al . ( 2019 ) also based their Bayesian deep learning experiments on Resnet18 . We are currently running some preliminary experiments for FFG-W , FFG-U and Ensemble-U on Resnet50 for CIFAR10 . Those take slightly more than twice as long compared to the Resnet18 experiments due to the larger network architecture . Given the short discussion period , we wish to provide initial results in response to your question , and later on we will update the full set of experiments in the camera-ready version . Based on our first runs , we can confirm that the inducing weight method works on the larger architecture and accuracy increases as expected while maintaining a low ECE . The results we have available for Resnet50 at this point are as follows : |Method | Accuracy ( % ) | ECE ( % ) | # params | % params of det . net ( including BatchNorm ) | # seeds | ||||||| |Deterministic | 94.47 | 4.59 | 23,520,842 | 100 | 5 | |FFG-U ( M=128 ) | 94.72 | 1.61 | 12,253,366 | 52.1 | 3 | |FFG-U ( M=64 ) | 94.33 | 0.66 | 5,710,902 | 24.28 | 2 | |FFG-W | 93.24 | 0.66 | 46,988,564 | 199.8 | 3 | |Ensemble-U ( M=128 ) | 95.28 | 1.86 | 14,907,574 | 63.4 | 3 | |Ensemble-W ( K=5 ) | 95.58 | 1.32 | 117,604,210 | 500 | 1 ( from det.seeds ) * * Ensemble-U * * By \u201c mixture of delta measures \u201d we mean having $ k $ sets of continuous \u201c parameter \u201d values in inducing space . You can think of this as equivalent to a classical ensemble of neural networks , except that the ensemble is in the lower-dimensional U space and then each set of inducing weights is ( stochastically ) projected into the original parameter space for a separate forward pass ( the projection parameters are shared ) . Having q ( U ) depend on the input would be a change of the model from globally shared parameters to parameters local to each data point as in the classical mixture density networks ( Bishop , 1994 ) . We have opted for the global weight model mostly to avoid an additional factor of variation between the two inference methods we compare . Introducing an input dependence would certainly be an interesting extension of our work and is a direction that has received attention recently e.g.in ( Kristiadi et al. , 2019 ) and ( Karaletsos & Bui , 2020 ) . We hope our response has been helpful and are of course happy to further clarify any additional questions you may have . References : Osawa et al.Practical Deep Learning with Bayesian Principles . In NeurIPS 2019 . Bishop.Mixture density networks . 1994.Kristiadi et al.Predictive Uncertainty Quantification with Compound Density Networks . arxiv preprint arXiv:1902.01080 . Karaletsos & Bui . Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights . NeurIPS 2020 ."}, {"review_id": "M9hdyCNlWaf-1", "review_text": "The paper proposes to define the weights approximate posterior of a NN with inducing variables . The main benefit of the approach lies in the compactness of the description ( Fig4 , right ) . However , if memory size is the main issue , entropy should be considered instead of just a count of parameters . Moreover , if complexity and memory footprint is the main concern , MC dropout sounds like a reasonable alternative to Bayesian NN . Why do the authors do not compare their proposal to a conventional MC dropout approach ? Overall , the contribution of the paper appears to be valid but relatively limited in scope compared to what exists in the literature ( inducing variables are known , Bayesian NN are known , ensemble methods are known ) . What is the gain in knowledge brought by the paper ? I might have missed a piece of the contribution , but the way the paper is written does not help . It is not self-contained ( many previous works have to be read to follow the story ) , and remains relatively opaque to the non-expert . It lacks a clear and eye-bird picture of the approach ( including both training and inference steps ) , to position and compare it to existing works . The overhead in optimizing ( 1 ) or ( 2 ) , compared to training a deterministic NN , is not discussed . Variables d_in^l is used in Section 2 , but only defined in Section 3 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your review . While the other reviewers have found the technical exposition novel and clear enough , it is valuable feedback that to the non-expert reader the paper is difficult to follow . Our aim is of course to make our work accessible to a more general machine learning audience than just the Bayesian deep learning research community . We appreciate that the write-up is dense in its current state and we already had to move some technical material to the appendix that we would have liked to include in the main text . Given that we now have an additional page available , we will include some more high-level discussion in our next revision . However , we hope you understand that it is not feasible to write a self-contained research paper within nine pages and we have to refer the reader to other works in some places . We do not consider MC dropout as a baseline because it does not reduce the parameter count compared to a deterministic network . One would have to either store the full set of trained weights and perform dropout at test time , which would require storing the same number of parameters as a deterministic network , or one would have store an ensemble of smaller networks where the weights that have been zeroed-out by dropout have been removed , which , depending on the dropout rate , would quickly exceed the storage requirements of a deterministic network . Further , dropout is not used in all architectures , for example Resnets do not use dropout , limiting its applicability compared to deep ensembles . While our work builds on existing methods , we believe -- in which the other reviewers ( as experts in Bayesian deep learning ) also agreed -- that our approach is highly novel , and our contribution is significant . In addition to our empirical evaluations , we make the following original technical contributions : * An efficient approximate inference method for Bayesian neural networks that uses * * fewer * * parameters than a deterministic network . To our knowledge , this is the first such method that has been applied successfully to modern NN architectures . * An extension of Matheron \u2019 s rule to efficiently sample from conditional Gaussian distributions that have a joint covariance matrix with Khatri-Rao product structure . This may be of use beyond variational inference and Bayesian neural networks . * An interpretation of the proposed inducing weight approach as related to function-space inference and ( deep ) Gaussian processes . To our knowledge , this is the first approach that explicitly introduces the inducing variable method ( which has been hugely successful in Gaussian process literature ) to deep neural networks , and our inducing weight method nicely connects weight-space and function-space methods for uncertainty estimation in deep learning . We hope this has clarified the contribution of our work and that you will consider adjusting your score ."}, {"review_id": "M9hdyCNlWaf-2", "review_text": "# # # Summary This work proposes a specific parametrisation for the Gaussian prior and approximate posterior distribution in variational Bayesian neural networks in terms of inducing weights . The general idea is an instance of the sparse variational inference scheme for GPs proposed by Titsias back in 2009 ; for a given model with a prior p ( W ) perform variational inference on an extended model with a hierarchical prior p ( U ) p ( W | U ) , that has the same marginal p ( W ) = \\int p ( U ) p ( W | U ) dU as the original model . The authors then consider \u201c U \u201d to be auxiliary weights that are jointly Gaussian with the actual weights \u201c W \u201d and then use the decomposition p ( W|U ) p ( U ) , q ( W|U ) q ( U ) for the prior and approximate posterior ( which can easily be computed via the conditional Gaussian rules ) . Furthermore , they \u201c tie \u201d ( almost ) all of the parameters between q ( W|U ) and p ( W|U ) ( similarly to Titsias , 2009 ) . The main benefit from these two things is that since the mean and covariance of the Gaussian distribution over W conditioned on U can be efficiently represented as functions of U , whenever dim ( U ) < < dim ( W ) we get reductions in memory for storing the distributions over the parameters in the network . The authors furthermore , discuss how to efficiently parametrize the joint distribution over W , U , discuss different choices for q ( U ) ( that can lead to either traditional VI or something like deep ensembles ) . In addition , they also discuss how more efficient sampling from q ( W|U ) can be realised via an extension of the Matheron \u2019 s rule to the case of matrix random variables . Finally , they evaluate their method against traditional mean field variational Bayesian neural networks and deep ensembles on several tasks that include regression , classification , calibration and OOD performance . # # # Pros - The method provides a novel way to induce parameter efficiency in variational bayesian neural networks - It connects to the sparse GP literature and the trick seems to be general enough , in that it can be applied to any distribution that admits some parametrisation in terms of a Gaussian random variable . - The extension of the Matheron \u2019 s rule can be of independent interest - Extensive set of experimental tasks , with a nice ablation study for lambda_max and sigma_max # # # Cons - Comparison against alternative approaches that induce parameter efficiency are missing - Results are mixed and sometimes not very convincing # # # Recommendation While I find the main idea very interesting and the presentation of it relatively clear , I unfortunately can not recommend acceptance of this work as is . The main motivation behind improving parameter efficiency can also be performed with other , perhaps much simpler , ways and comparisons against such approaches is missing . Furthermore , the results , at least in their current state , are a bit mixed and thus not very convincing . # # # Detailed feedback Overall , this work is interesting and relatively easy to follow . Using inducing variables in neural networks in not a new concept , as it has been used before in , e.g. , [ 1 , 2 ] , but the motivation of using them as a means towards reducing the number of parameters of each distribution is new . The authors explain the main idea behind them in a clear manner . Furthermore , they clearly explain the need for using matrix normal distributions , explaining how the Kronecker product factorisation of the covariance improves the parametrization efficiency , along with how efficient sampling from q ( W|U ) can be performed . In addition , performing parameter efficient deep ensembles in U space is a nice bonus of this formulation . Section 3.3 however is dense and can be a bit hard to follow ( whereas Appendix D is clearer ) . I would therefore suggest that the authors briefly describe the main idea in a couple of sentences and refer the readers to Appendix D instead . My main point for feedback is for the experimental section . The authors argue at the beginning of section 4 that \u201c the goal is to demonstrate competitive performance \u2026 . While remaining computationally efficient. \u201d Is that claim for the training or evaluation phases ? From what I see , during the training phase , the inducing weights framework is not faster or more efficient than the baselines . The main advantage of the inducing weight is instead memory reduction in terms of parameters which translates to \u201c memory efficiency \u201d and not \u201c computational \u201c efficiency \u201d . If then memory efficiency is the main target , I believe that some reasonable baselines are missing . For the case of a variational Bayesian neural network a simple baseline that performs pruning post-hoc ( e.g. , the one presented at the FFG-W paper or the one from [ 3 ] ) in order to reduce the parameter count ; remove weights by either setting them to exact zero or equal to the prior and then use the variational posterior for those that survive . It would be interesting to see how such an approach would fare not just on accuracy , but also on the ECE and OOD . Similarly for deep ensembles , a baseline where you perform pruning ( e.g. , simple magnitude based ) would also serve as a better baseline for the Ensemble-U . Both of these baselines would provide a more complete picture and would be a better signal in understanding whether the inducing weights framework is a better choice overall . As for other points - At figure 3 you only show FCG-U and not the FFG-U which is used in all of the other experiments . How does the uncertainty look like with FFG-U ? - Sometimes , both FFG-U and Ensemble-U underperform compared to FFG-W and Ensemble-W and it would be good to know if it is due to less free parameters to optimise or due to the model definition itself . How does FFG-U perform when increasing dim ( U ) such that the parameter count is the same as FFG-W ? Similarly for Ensemble-U vs Ensemble-W. - The second point at the end of page 2 only makes sense in hindsight ( i.e. , after one reads section 3 ) . Perhaps the authors could expand on them a bit better so that it is self-contained ( e.g. , explain what you mean with \u201c q ( theta|alpha ) can be efficiently parametrised \u201d ) . [ 1 ] Structured and efficient variational deep learning with matrix Gaussian posteriors , C. Louizos & M. Welling , 2016 [ 2 ] Global inducing point variational posteriors for Bayesian neural networks and deep gaussian processes , S. Ober & L. Aitchison , 2020 [ 3 ] \u0010 Practical Variational Inference for Neural Networks , A. Graves , 2011", "rating": "6: Marginally above acceptance threshold", "reply_text": "Regarding your other points : * We will add the corresponding figure to the appendix . The uncertainty is similar to FFG-W. * Overall we would say that the performance differences are rather small . The main factor that determines the number of parameters is the dimensionality of the inducing space . We only ran our ablation study for the dimensionality of the inducing space up to half the parameters of the deterministic network ( or a quarter of the number of mean-field parameters ) , but given that both accuracy and ECE start to saturate ( Figure 5 ; right column ) at the value of M=128 that we use in our main figures and tables , we would expect that matching the number of parameters would at most give a marginal improvement for the performance of the inducing weight methods . * Thanks for pointing this out . We had to move more material than we would have liked to the appendix to be within the page limit , but will upload a revised version that makes use of the additional page that is now available . Similarly we will revise section 3.3 to make it clearer ."}, {"review_id": "M9hdyCNlWaf-3", "review_text": "Bayesian deep learning attempts to incorporate uncertainty estimation in the modern neural network models . However , common approaches such as Bayesian neural networks and Deep Ensembles incur large memory overhead because of the increased parameter sizes . Motivated by the inducing points approach in the sparse variational Gaussian Processes area , this paper proposes the Gaussian variational posterior relying on the compact * inducing weights * $ U $ and the conditional distribution $ p ( W|U ) $ . Because the variational parameter shifts from $ W $ to the smaller $ U $ , the resulting model potentially uses even fewer parameters than a deterministic counterpart . In general , I think the paper proposes an interesting approach that could help addressing the storage issues of current Bayesian deep learning models . * * Novelty * * The proposed approach is both novel and elegant . Given the high dimensionality of the parameters , many BNN approaches consider structured covariance for the variational posterior , which involves the balancing between computational costs and approximating capacity . And these approaches all maintain a `` mean parameter '' , thus the memory costs are at least as large as the original network . In comparison , this paper turns to an augmented space using the joint Gaussian distribution , and instead model the variational posterior for the compact * inducing weights * . In consequence , the resulting model could potentially have even fewer parameters than a deterministic counterpart , while being able to conduct uncertainty quantification . Given that the inducing points approach has achieved big successes for scalable GPs , I think the proposed method could be impactful for uncertainty modelling in deep learning models . * * Experiments * * This paper conducts experiments covering both image classification and out-of-distribution detection . Empirically , their model has increased calibration compared to the deterministic network while using $ \\leq 47.9\\ % $ of parameters . However , as shown in Figure 4 , the proposed approach incurs large computational overheads especially when using small number of samples . * * Questions * * 1 . What are the inducing weights for the conv layer ? Are they matrices of shape [ 128 , 128 , kern_size , kern_size ] ? 2.The Ensemble-U approach uses a dirac measure $ \\sum_k \\delta_ { U^k_l } $ in each layer and drops the KL penalty . Are dirac measures of different layers independent , or like deep ensemble , the particles are tied into $ K $ disjoint groups across layers ? If it was the former , I would reckon that the particles of the same layer would converge to the same place ; if it was the latter , it should be made clearer in the paper . 3.How does the sample size $ K $ influence the performance ? In practice , I think only a small number of $ K $ should be used . 4.A hyperparameter $ \\sigma_ { max } ^2 $ is set for the variance of $ q ( W ) $ . And the similar hyperparameter $ \\lambda_ { \\max } $ is set for the variational posterior . Especially for $ \\lambda_ { max } $ , increasing it from $ 0.1 $ to $ 0.3 $ observes large performance drop . Is this an issue due to the expressiveness of the proposed posterior ? * * Clarity * * In spite of my questions regarding the model details , the paper well presents its main methods . Besides , I think Sec3.3 is worthy of more polishing , since it looks confusing when you start by studying $ U_c $ instead of $ U_r $ or $ U $ . And a few typos to be corrected , 1 . Abstract : whichenable 2 . Last paragraph of introduction : our approach achieve 3 ) Related works : function-space inference is appealing to ...", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review . We are pleased to see that you appreciate the novelty and the technical contributions of our paper . We will address your questions below : 1 . We treat convolutional weights as 2d matrices by \u2018 flattening \u2019 the 4d tensors from shape ( out_channels , in_channnels , height , width ) to ( out_channels , in_channels * height * width ) . This corresponds to the interpretation of convolutions as linear layers that are repeated across the image patches with tied weights , see e.g . ( Grosse & Martens , 2016 ) for an equivalent use for second-order optimisation . We will clarify this in our next revision of the paper . 2.We indeed use the latter interpretation , i.e.tie the groups across layers . Again , we will update the paper to better reflect this . 3.Similar to previous work , e.g . ( Ovadia et al. , 2019 ) , we found in preliminary experiments a small sample size of around 5 to be sufficient for capturing most of the performance gains over using a single sample . We used 20 samples for all variational methods , as we believe that this fairly represents the advantage over deterministic ensembles of being able to draw an arbitrary number of samples after training . 4.This is an interesting question and we don \u2019 t have a conclusive answer at this point . It has recently been argued in the BNN literature that a more concentrated \u2018 cold posterior \u2019 ( Wenzel et al. , 2020 ) can give better predictive performance and we would hypothesise that our observations regarding the hyperparameters are related to this phenomenon . Further it has been observed that more expressive variational posteriors can lead to worse performance ( Trippe & Turner , 2017 ) , however since our approximate posterior has a low-rank mean , but a non-diagonal covariance , it is difficult to strictly classify it as more or less expressive than e.g.a mean-field posterior . Overall , we think that further research around the true posterior of deep neural networks is needed and that our low rank approach in the original model makes for an interesting contribution in that direction . We are glad to see from your comments that the main message of our paper is clearly conveyed . Still we will revise our paper , especially for section 3.3 , to further improve clarity . References : Grosse & Martens . A Kronecker-factored approximate Fisher matrix for convolution layers . In ICML 2016 . Ovadia et al.Can You Trust Your Model \u2019 s Uncertainty ? Evaluating Predictive Uncertainty Under Dataset Shift . In NeurIPS 2019 . Wenzel et al.How Good is the Bayes Posterior in Deep Neural Networks Really ? In ICML 2020 . Trippe & Turner . Overpruning in Variational Bayesian Neural Networks . In NeurIPS 2017 Workshop on Advances in Approximate Bayesian Inference ."}], "0": {"review_id": "M9hdyCNlWaf-0", "review_text": "# summary This paper proposed a method on uncertainty estimation in deep neural networks . Compared with BNN and deep ensemble , the proposed approach in this work has a storage advantage . Furthermore , this work provides a better trade-off between accuracy and calibration . # pros 1.The approach in this work is quite interesting . The idea of augmenting weights with auxiliary low-dimensional latent variables in a deep neural network seems natural at first sight , but this approach is novel as far as my knowledge is concerned . Although VI with local latent variables is an old technique , this application in deep neural network is novel . 2.The authors also proposed an efficient approach that can sample from the variational approximation conditioning on the latent variable . Since the original weight is large , such a sampling is necessary . 3.This paper provides extensive empirical results and sufficient theoretical results . Experimental results show the proposed approach achieve a good balance between accuracy , calibration and memory requirements . # cons 1.My major concern is all experiments are conducted on ResNet18 , and this is not a typical choice for practical problems . I think experiments on other larger net such as ResNet56 on CIFAR10 will make this paper more convincing . 2.It is not clear to me how the authors choose the `` mixture of delta measures '' , i.e.how to choose U < sup > k < /sup > ? Can the authors comment on this ? In this case , q ( U ) is a categorical distribution . It seems better if q ( U ) also depends on input data , however , the authors choose to use a fixed q ( U ) . Can the authors comment on this choice ? Overall speaking , the idea and the presentation of this work are great in my opinion .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your encouraging review . We are particularly pleased that you appreciate the novelty of the idea and the clarity of the presentation . * * Architecture size * * In order to avoid possible confusion regarding the architecture , we would like to emphasize that the Resnet18 architecture we use is based on the torchvision implementation , i.e.the smallest ImageNet-sized architecture ( > 11M parameters ) , * * not * * the CIFAR-sized Resnet20 architecture from the original Resnet paper ( < 1M parameters ) . The Resnet56 architecture you suggest was also developed for CIFAR ( < 1M parameters ) -- did you mean Resnet50 ( > 22M parameters ) or were you under the impression that we were using the smaller CIFAR architectures ? While we fully agree that Resnet50 would be more representative of an architecture that a practitioner would use , we do believe that Resnet18 captures the most relevant properties of a modern deep neural network . For example , Osawa et al . ( 2019 ) also based their Bayesian deep learning experiments on Resnet18 . We are currently running some preliminary experiments for FFG-W , FFG-U and Ensemble-U on Resnet50 for CIFAR10 . Those take slightly more than twice as long compared to the Resnet18 experiments due to the larger network architecture . Given the short discussion period , we wish to provide initial results in response to your question , and later on we will update the full set of experiments in the camera-ready version . Based on our first runs , we can confirm that the inducing weight method works on the larger architecture and accuracy increases as expected while maintaining a low ECE . The results we have available for Resnet50 at this point are as follows : |Method | Accuracy ( % ) | ECE ( % ) | # params | % params of det . net ( including BatchNorm ) | # seeds | ||||||| |Deterministic | 94.47 | 4.59 | 23,520,842 | 100 | 5 | |FFG-U ( M=128 ) | 94.72 | 1.61 | 12,253,366 | 52.1 | 3 | |FFG-U ( M=64 ) | 94.33 | 0.66 | 5,710,902 | 24.28 | 2 | |FFG-W | 93.24 | 0.66 | 46,988,564 | 199.8 | 3 | |Ensemble-U ( M=128 ) | 95.28 | 1.86 | 14,907,574 | 63.4 | 3 | |Ensemble-W ( K=5 ) | 95.58 | 1.32 | 117,604,210 | 500 | 1 ( from det.seeds ) * * Ensemble-U * * By \u201c mixture of delta measures \u201d we mean having $ k $ sets of continuous \u201c parameter \u201d values in inducing space . You can think of this as equivalent to a classical ensemble of neural networks , except that the ensemble is in the lower-dimensional U space and then each set of inducing weights is ( stochastically ) projected into the original parameter space for a separate forward pass ( the projection parameters are shared ) . Having q ( U ) depend on the input would be a change of the model from globally shared parameters to parameters local to each data point as in the classical mixture density networks ( Bishop , 1994 ) . We have opted for the global weight model mostly to avoid an additional factor of variation between the two inference methods we compare . Introducing an input dependence would certainly be an interesting extension of our work and is a direction that has received attention recently e.g.in ( Kristiadi et al. , 2019 ) and ( Karaletsos & Bui , 2020 ) . We hope our response has been helpful and are of course happy to further clarify any additional questions you may have . References : Osawa et al.Practical Deep Learning with Bayesian Principles . In NeurIPS 2019 . Bishop.Mixture density networks . 1994.Kristiadi et al.Predictive Uncertainty Quantification with Compound Density Networks . arxiv preprint arXiv:1902.01080 . Karaletsos & Bui . Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights . NeurIPS 2020 ."}, "1": {"review_id": "M9hdyCNlWaf-1", "review_text": "The paper proposes to define the weights approximate posterior of a NN with inducing variables . The main benefit of the approach lies in the compactness of the description ( Fig4 , right ) . However , if memory size is the main issue , entropy should be considered instead of just a count of parameters . Moreover , if complexity and memory footprint is the main concern , MC dropout sounds like a reasonable alternative to Bayesian NN . Why do the authors do not compare their proposal to a conventional MC dropout approach ? Overall , the contribution of the paper appears to be valid but relatively limited in scope compared to what exists in the literature ( inducing variables are known , Bayesian NN are known , ensemble methods are known ) . What is the gain in knowledge brought by the paper ? I might have missed a piece of the contribution , but the way the paper is written does not help . It is not self-contained ( many previous works have to be read to follow the story ) , and remains relatively opaque to the non-expert . It lacks a clear and eye-bird picture of the approach ( including both training and inference steps ) , to position and compare it to existing works . The overhead in optimizing ( 1 ) or ( 2 ) , compared to training a deterministic NN , is not discussed . Variables d_in^l is used in Section 2 , but only defined in Section 3 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your review . While the other reviewers have found the technical exposition novel and clear enough , it is valuable feedback that to the non-expert reader the paper is difficult to follow . Our aim is of course to make our work accessible to a more general machine learning audience than just the Bayesian deep learning research community . We appreciate that the write-up is dense in its current state and we already had to move some technical material to the appendix that we would have liked to include in the main text . Given that we now have an additional page available , we will include some more high-level discussion in our next revision . However , we hope you understand that it is not feasible to write a self-contained research paper within nine pages and we have to refer the reader to other works in some places . We do not consider MC dropout as a baseline because it does not reduce the parameter count compared to a deterministic network . One would have to either store the full set of trained weights and perform dropout at test time , which would require storing the same number of parameters as a deterministic network , or one would have store an ensemble of smaller networks where the weights that have been zeroed-out by dropout have been removed , which , depending on the dropout rate , would quickly exceed the storage requirements of a deterministic network . Further , dropout is not used in all architectures , for example Resnets do not use dropout , limiting its applicability compared to deep ensembles . While our work builds on existing methods , we believe -- in which the other reviewers ( as experts in Bayesian deep learning ) also agreed -- that our approach is highly novel , and our contribution is significant . In addition to our empirical evaluations , we make the following original technical contributions : * An efficient approximate inference method for Bayesian neural networks that uses * * fewer * * parameters than a deterministic network . To our knowledge , this is the first such method that has been applied successfully to modern NN architectures . * An extension of Matheron \u2019 s rule to efficiently sample from conditional Gaussian distributions that have a joint covariance matrix with Khatri-Rao product structure . This may be of use beyond variational inference and Bayesian neural networks . * An interpretation of the proposed inducing weight approach as related to function-space inference and ( deep ) Gaussian processes . To our knowledge , this is the first approach that explicitly introduces the inducing variable method ( which has been hugely successful in Gaussian process literature ) to deep neural networks , and our inducing weight method nicely connects weight-space and function-space methods for uncertainty estimation in deep learning . We hope this has clarified the contribution of our work and that you will consider adjusting your score ."}, "2": {"review_id": "M9hdyCNlWaf-2", "review_text": "# # # Summary This work proposes a specific parametrisation for the Gaussian prior and approximate posterior distribution in variational Bayesian neural networks in terms of inducing weights . The general idea is an instance of the sparse variational inference scheme for GPs proposed by Titsias back in 2009 ; for a given model with a prior p ( W ) perform variational inference on an extended model with a hierarchical prior p ( U ) p ( W | U ) , that has the same marginal p ( W ) = \\int p ( U ) p ( W | U ) dU as the original model . The authors then consider \u201c U \u201d to be auxiliary weights that are jointly Gaussian with the actual weights \u201c W \u201d and then use the decomposition p ( W|U ) p ( U ) , q ( W|U ) q ( U ) for the prior and approximate posterior ( which can easily be computed via the conditional Gaussian rules ) . Furthermore , they \u201c tie \u201d ( almost ) all of the parameters between q ( W|U ) and p ( W|U ) ( similarly to Titsias , 2009 ) . The main benefit from these two things is that since the mean and covariance of the Gaussian distribution over W conditioned on U can be efficiently represented as functions of U , whenever dim ( U ) < < dim ( W ) we get reductions in memory for storing the distributions over the parameters in the network . The authors furthermore , discuss how to efficiently parametrize the joint distribution over W , U , discuss different choices for q ( U ) ( that can lead to either traditional VI or something like deep ensembles ) . In addition , they also discuss how more efficient sampling from q ( W|U ) can be realised via an extension of the Matheron \u2019 s rule to the case of matrix random variables . Finally , they evaluate their method against traditional mean field variational Bayesian neural networks and deep ensembles on several tasks that include regression , classification , calibration and OOD performance . # # # Pros - The method provides a novel way to induce parameter efficiency in variational bayesian neural networks - It connects to the sparse GP literature and the trick seems to be general enough , in that it can be applied to any distribution that admits some parametrisation in terms of a Gaussian random variable . - The extension of the Matheron \u2019 s rule can be of independent interest - Extensive set of experimental tasks , with a nice ablation study for lambda_max and sigma_max # # # Cons - Comparison against alternative approaches that induce parameter efficiency are missing - Results are mixed and sometimes not very convincing # # # Recommendation While I find the main idea very interesting and the presentation of it relatively clear , I unfortunately can not recommend acceptance of this work as is . The main motivation behind improving parameter efficiency can also be performed with other , perhaps much simpler , ways and comparisons against such approaches is missing . Furthermore , the results , at least in their current state , are a bit mixed and thus not very convincing . # # # Detailed feedback Overall , this work is interesting and relatively easy to follow . Using inducing variables in neural networks in not a new concept , as it has been used before in , e.g. , [ 1 , 2 ] , but the motivation of using them as a means towards reducing the number of parameters of each distribution is new . The authors explain the main idea behind them in a clear manner . Furthermore , they clearly explain the need for using matrix normal distributions , explaining how the Kronecker product factorisation of the covariance improves the parametrization efficiency , along with how efficient sampling from q ( W|U ) can be performed . In addition , performing parameter efficient deep ensembles in U space is a nice bonus of this formulation . Section 3.3 however is dense and can be a bit hard to follow ( whereas Appendix D is clearer ) . I would therefore suggest that the authors briefly describe the main idea in a couple of sentences and refer the readers to Appendix D instead . My main point for feedback is for the experimental section . The authors argue at the beginning of section 4 that \u201c the goal is to demonstrate competitive performance \u2026 . While remaining computationally efficient. \u201d Is that claim for the training or evaluation phases ? From what I see , during the training phase , the inducing weights framework is not faster or more efficient than the baselines . The main advantage of the inducing weight is instead memory reduction in terms of parameters which translates to \u201c memory efficiency \u201d and not \u201c computational \u201c efficiency \u201d . If then memory efficiency is the main target , I believe that some reasonable baselines are missing . For the case of a variational Bayesian neural network a simple baseline that performs pruning post-hoc ( e.g. , the one presented at the FFG-W paper or the one from [ 3 ] ) in order to reduce the parameter count ; remove weights by either setting them to exact zero or equal to the prior and then use the variational posterior for those that survive . It would be interesting to see how such an approach would fare not just on accuracy , but also on the ECE and OOD . Similarly for deep ensembles , a baseline where you perform pruning ( e.g. , simple magnitude based ) would also serve as a better baseline for the Ensemble-U . Both of these baselines would provide a more complete picture and would be a better signal in understanding whether the inducing weights framework is a better choice overall . As for other points - At figure 3 you only show FCG-U and not the FFG-U which is used in all of the other experiments . How does the uncertainty look like with FFG-U ? - Sometimes , both FFG-U and Ensemble-U underperform compared to FFG-W and Ensemble-W and it would be good to know if it is due to less free parameters to optimise or due to the model definition itself . How does FFG-U perform when increasing dim ( U ) such that the parameter count is the same as FFG-W ? Similarly for Ensemble-U vs Ensemble-W. - The second point at the end of page 2 only makes sense in hindsight ( i.e. , after one reads section 3 ) . Perhaps the authors could expand on them a bit better so that it is self-contained ( e.g. , explain what you mean with \u201c q ( theta|alpha ) can be efficiently parametrised \u201d ) . [ 1 ] Structured and efficient variational deep learning with matrix Gaussian posteriors , C. Louizos & M. Welling , 2016 [ 2 ] Global inducing point variational posteriors for Bayesian neural networks and deep gaussian processes , S. Ober & L. Aitchison , 2020 [ 3 ] \u0010 Practical Variational Inference for Neural Networks , A. Graves , 2011", "rating": "6: Marginally above acceptance threshold", "reply_text": "Regarding your other points : * We will add the corresponding figure to the appendix . The uncertainty is similar to FFG-W. * Overall we would say that the performance differences are rather small . The main factor that determines the number of parameters is the dimensionality of the inducing space . We only ran our ablation study for the dimensionality of the inducing space up to half the parameters of the deterministic network ( or a quarter of the number of mean-field parameters ) , but given that both accuracy and ECE start to saturate ( Figure 5 ; right column ) at the value of M=128 that we use in our main figures and tables , we would expect that matching the number of parameters would at most give a marginal improvement for the performance of the inducing weight methods . * Thanks for pointing this out . We had to move more material than we would have liked to the appendix to be within the page limit , but will upload a revised version that makes use of the additional page that is now available . Similarly we will revise section 3.3 to make it clearer ."}, "3": {"review_id": "M9hdyCNlWaf-3", "review_text": "Bayesian deep learning attempts to incorporate uncertainty estimation in the modern neural network models . However , common approaches such as Bayesian neural networks and Deep Ensembles incur large memory overhead because of the increased parameter sizes . Motivated by the inducing points approach in the sparse variational Gaussian Processes area , this paper proposes the Gaussian variational posterior relying on the compact * inducing weights * $ U $ and the conditional distribution $ p ( W|U ) $ . Because the variational parameter shifts from $ W $ to the smaller $ U $ , the resulting model potentially uses even fewer parameters than a deterministic counterpart . In general , I think the paper proposes an interesting approach that could help addressing the storage issues of current Bayesian deep learning models . * * Novelty * * The proposed approach is both novel and elegant . Given the high dimensionality of the parameters , many BNN approaches consider structured covariance for the variational posterior , which involves the balancing between computational costs and approximating capacity . And these approaches all maintain a `` mean parameter '' , thus the memory costs are at least as large as the original network . In comparison , this paper turns to an augmented space using the joint Gaussian distribution , and instead model the variational posterior for the compact * inducing weights * . In consequence , the resulting model could potentially have even fewer parameters than a deterministic counterpart , while being able to conduct uncertainty quantification . Given that the inducing points approach has achieved big successes for scalable GPs , I think the proposed method could be impactful for uncertainty modelling in deep learning models . * * Experiments * * This paper conducts experiments covering both image classification and out-of-distribution detection . Empirically , their model has increased calibration compared to the deterministic network while using $ \\leq 47.9\\ % $ of parameters . However , as shown in Figure 4 , the proposed approach incurs large computational overheads especially when using small number of samples . * * Questions * * 1 . What are the inducing weights for the conv layer ? Are they matrices of shape [ 128 , 128 , kern_size , kern_size ] ? 2.The Ensemble-U approach uses a dirac measure $ \\sum_k \\delta_ { U^k_l } $ in each layer and drops the KL penalty . Are dirac measures of different layers independent , or like deep ensemble , the particles are tied into $ K $ disjoint groups across layers ? If it was the former , I would reckon that the particles of the same layer would converge to the same place ; if it was the latter , it should be made clearer in the paper . 3.How does the sample size $ K $ influence the performance ? In practice , I think only a small number of $ K $ should be used . 4.A hyperparameter $ \\sigma_ { max } ^2 $ is set for the variance of $ q ( W ) $ . And the similar hyperparameter $ \\lambda_ { \\max } $ is set for the variational posterior . Especially for $ \\lambda_ { max } $ , increasing it from $ 0.1 $ to $ 0.3 $ observes large performance drop . Is this an issue due to the expressiveness of the proposed posterior ? * * Clarity * * In spite of my questions regarding the model details , the paper well presents its main methods . Besides , I think Sec3.3 is worthy of more polishing , since it looks confusing when you start by studying $ U_c $ instead of $ U_r $ or $ U $ . And a few typos to be corrected , 1 . Abstract : whichenable 2 . Last paragraph of introduction : our approach achieve 3 ) Related works : function-space inference is appealing to ...", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review . We are pleased to see that you appreciate the novelty and the technical contributions of our paper . We will address your questions below : 1 . We treat convolutional weights as 2d matrices by \u2018 flattening \u2019 the 4d tensors from shape ( out_channels , in_channnels , height , width ) to ( out_channels , in_channels * height * width ) . This corresponds to the interpretation of convolutions as linear layers that are repeated across the image patches with tied weights , see e.g . ( Grosse & Martens , 2016 ) for an equivalent use for second-order optimisation . We will clarify this in our next revision of the paper . 2.We indeed use the latter interpretation , i.e.tie the groups across layers . Again , we will update the paper to better reflect this . 3.Similar to previous work , e.g . ( Ovadia et al. , 2019 ) , we found in preliminary experiments a small sample size of around 5 to be sufficient for capturing most of the performance gains over using a single sample . We used 20 samples for all variational methods , as we believe that this fairly represents the advantage over deterministic ensembles of being able to draw an arbitrary number of samples after training . 4.This is an interesting question and we don \u2019 t have a conclusive answer at this point . It has recently been argued in the BNN literature that a more concentrated \u2018 cold posterior \u2019 ( Wenzel et al. , 2020 ) can give better predictive performance and we would hypothesise that our observations regarding the hyperparameters are related to this phenomenon . Further it has been observed that more expressive variational posteriors can lead to worse performance ( Trippe & Turner , 2017 ) , however since our approximate posterior has a low-rank mean , but a non-diagonal covariance , it is difficult to strictly classify it as more or less expressive than e.g.a mean-field posterior . Overall , we think that further research around the true posterior of deep neural networks is needed and that our low rank approach in the original model makes for an interesting contribution in that direction . We are glad to see from your comments that the main message of our paper is clearly conveyed . Still we will revise our paper , especially for section 3.3 , to further improve clarity . References : Grosse & Martens . A Kronecker-factored approximate Fisher matrix for convolution layers . In ICML 2016 . Ovadia et al.Can You Trust Your Model \u2019 s Uncertainty ? Evaluating Predictive Uncertainty Under Dataset Shift . In NeurIPS 2019 . Wenzel et al.How Good is the Bayes Posterior in Deep Neural Networks Really ? In ICML 2020 . Trippe & Turner . Overpruning in Variational Bayesian Neural Networks . In NeurIPS 2017 Workshop on Advances in Approximate Bayesian Inference ."}}