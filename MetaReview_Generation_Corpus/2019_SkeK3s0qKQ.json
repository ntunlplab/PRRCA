{"year": "2019", "forum": "SkeK3s0qKQ", "title": "Episodic Curiosity through Reachability", "decision": "Accept (Poster)", "meta_review": "\nThe authors present a novel method for tackling exploration and exploitation that yields promising results on some hard navigation-like domains. The reviewers were impressed by the contribution and had some suggestions for improvement that should be addressed in the camera ready version.\n", "reviews": [{"review_id": "SkeK3s0qKQ-0", "review_text": "The main idea of this paper is to propose a heuristic method for exploration in deep reinforcement learning. The work is fairly innovative in its approach, where an episodic memory is used to store agent\u2019s observations while rewarding the agent for reaching novel observations not yet stored in memory. The novelty here is determined by a pre-trained network that computes the within k-step-reachability of current observation to the observations stored in memory. The method is quite simple but promising and can be easily integrated with any RL algorithm. They test their method on a pair of 3D environments, VizDoom and DMLab. The experiments are well executed and analysed. Positives: - They do a rigorous analysis of parameters, and explicitly count the pre-training interactions with the environment in their learning curves. - This method does not hurt when dense environmental rewards are present. - The memory buffer is smaller than the episode length, which avoids trivial solutions. - The idea of having a discriminator assess distance between states is interesting. Questions and critics: - The tasks explored in this paper are all navigation based tasks, would this method also apply equally successfully to non-navigation domains such as manipulation? - My main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data. In navigation domains it makes sense that the random policy could cover the space fairly well, however, this will not be the case for more complex tasks involving more complex dynamics. - It was surprising to me that the choice of k does not seem to be that important. As it implicitly defines what \u201cnovelty\u201d means for an environment, I would have expected that its value should be calibrated better. Could that be a function of the navigation tasks considered? - The DMLab results are not great or comparable to the state-of-the-art methods, which may hinder interpreting how good the policies really are. This was perhaps a conscious choice given they are only interested in early training results, but that seems like a confound. - The architecture does not include an RNN which makes certain things very surprising even though they shouldn't (e.g. firing, or moving around a corner, are specifically surprising for ICM) as they cannot be learnt, but perhaps if they had an RNN in the architecture these would be easy to explain? Would be interesting to see what are the authors thoughts on this (apart from their computational complexity argument they mention)? - Having the memory contain only information about the current episode with no information transfer between episodes seems a bit strange to me, I would like to hear the motivation behind this? - The fact that the memory is reset between episodes, and that the buffer is small, can mean that effectively the method implements some sort of complex pseudo count over meta-states per episode? - The embedding network is only trained during the pre-training phase and frozen during the RL task. This sounds a bit limiting to me: what if the agent starts exploring part of the space that was not covered during pre-training? Obviously this could lead to collapses when allowing to fine-tune it, but I feel this is rather restrictive. Again, I feel that the choice of navigation tasks did not magnify this problem, which would arise more in harder exploration tasks. - I think that alluding that their method is similar to babies\u2019 behaviour in their cradle is stretched at best and not a constructive way to motivate their work\u2026 - In Figure 6 and 7, all individual curves from each seed run are shown, which is a bit distracting. Perhaps showing the mean and std would be a cleaner and easier-to-interpret way to report these results? Overall, it is a simple and interesting idea and seems quite easy to implement. However, everything is highly dependent on how varying the environment is, how bad the exploration policy used for pre-training is, how good the embeddings are once frozen, and how k, action repeat and memory buffer size interact. Given that the experiments are all navigation based, it makes it hard for me to assess whether this method can work as well in other domains with harder exploration setups. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their work on the review . > The tasks explored in this paper are all navigation based tasks , would this method also apply equally successfully to non-navigation domains such as manipulation ? This is a great question and we work on experiments in other domains . However , we would like to point out that the tasks we already have in the paper are both non-trivial and more visually complex than in many other works in the field of sparse-reward exploration . > My main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data . In navigation domains it makes sense that the random policy could cover the space fairly well , however , this will not be the case for more complex tasks involving more complex dynamics . We agree that for more complex domains randomly collected data may be insufficient and online training of the R-network may be crucial . To address the concerns of the reviewer , we have implemented a version of our algorithm which performs training of R-network online ( together with the policy ) . Preliminary results indicate that such training is possible and does not collapse . It produces results at least as good as pre-training . Thus , we are able to demonstrate that our approach can function with online training , offering the possibility of functioning in domains where collection of random data may be insufficient . In addition , we would like to point out that the R-network ( Embedding + Comparator ) can generalize beyond what was seen in the pre-training stage . We have such an experiment in the supplementary section S3 `` R-network generalization study '' . In particular , in Table S4 , R-networks trained on levels `` Dense 1 '' and `` Sparse + Doors '' generalize to the `` Very Sparse '' environment . The visual gap is quite significant : please compare https : //youtu.be/C5g10cUl7Ew with https : //youtu.be/9J4CzdOz60I , for example . All this is possible because R-network is solving a simple problem of comparing two observations given access to both observations at the same time . Moreover , in the real world , people typically hand-design the initial exploration policy even for the standard RL methods , let alone the model-based ones ( and our method could be considered partially model-based ) . For example , please take a look at the recent work https : //ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html ( which has just received the best paper award at the Conference on Robotic Learning ) . Another recent work from ICLR \u2019 18 https : //openreview.net/forum ? id=BkisuzWRW also uses hand-crafted policy for the robotic manipulation task to collect data for training the inverse model of the environment . > It was surprising to me that the choice of k does not seem to be that important . As it implicitly defines what \u201c novelty \u201d means for an environment , I would have expected that its value should be calibrated better . Could that be a function of the navigation tasks considered ? Those values of k are still rather small . What we demonstrate in this experiment is that our method is not excessively sensitive to this parameter when it is chosen within a reasonable range . > The DMLab results are not great or comparable to the state-of-the-art methods , which may hinder interpreting how good the policies really are . This was perhaps a conscious choice given they are only interested in early training results , but that seems like a confound . As far as we know , SOTA results are achieved by Impala https : //arxiv.org/abs/1802.01561 at 1B steps ( 250M 4-repeated steps ) . We haven \u2019 t yet run our experiments at this scale : we use 20M 4-repeated steps in our PPO setup with 12 actors on GPU , which already takes 2 days to complete . Furthermore , being more sample efficient is an appealing property of more effective exploration as interactions with an environment might be costly in some environments ."}, {"review_id": "SkeK3s0qKQ-1", "review_text": "This paper proposes a new method to give exploration bonuses in RL algorithms by giving larger bonuses to observations that are farther away (> k) in environment steps to past observations in the current episode, encouraging the agent to visit observations farther away. This is in contrast to existing exploration bonuses based on prediction gain or prediction error, which do not work properly for stochastic transitions. Overall, I very much like the idea, but I found many little pieces of confusing explanations that could be further clarified, and also some questionable implementation details. However the experimental results are very promising, and the approach should be modular and slotable into existing deep RL methods. Section Introduction: I\u2019m confused by how you can define such a bonus if the memory is the current episode. Won\u2019t the shortest-path distance of the next observation always be 1 because it is immediately following the current step, and thus this results in a constant bonus? You explain how you get around this in practice, but intuitively and from a high-level, this idea does not make sense. It would perhaps make more sense if you used a different aggregation, such average, in which case you would be giving bonuses to observations that are farther away from the past on average. Also, while eventually this idea makes sense, it only makes sense within a single episode. If you clear the memory between episodes, then you are relying on some natural stochasticity of the algorithm to avoid revisiting the same states as in the previous episode. Otherwise, it seems like there is not much to be gained from actually resetting and starting a new episode; it would encourage more exploration to just continue the same episode, or not clear memory when starting a new episode. Section 2.2: You say you have a novelty threshold of 0 in practice, doesn\u2019t this mean you end up always adding new observations to the memory? In this case, then it seems like your aggregation method of taking the 90th percentile is really the only mechanism that avoids the issue of always predicting a constant distance of 1 (and relying on the function approximator\u2019s natural errors). I do think you should rework your intuition. It seems to me what you are actually doing is creating some sort of implicit discretization of the observation space, and rewarding observations that you have not seen before under this discretization. This is what would correspond to a shortest-path distance aggregation. Experiments: I like your grid oracle, as it acts as a baseline for using PPO and provides a point of reference for how well an exploration bonus could potentially be. But why aren\u2019t grid oracle results put into your graphs? Your results look good and are very promising. Other points: - The pre-training of the R-network is concerning, but you have already responded with preliminary results. - I do share some of the concerns other reviewers have brought up about generality beyond navigation tasks, e.g. Atari games. To me, it seems like this method can run into difficulty when reachability is not as nice as it is in navigation tasks, for example if the decisions of the task followed a more tree-like structure. This also does not work well with the fact that you reset every episode, so there is nothing to encourage an agent to try different branches of the tree every episode. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their work on the review . Please note that the paper has just been updated as per the request from AR2 , here is an anonymous link to the updated paper : https : //drive.google.com/open ? id=1tUHfBwWWu6W2zuk-De0AyYWxuNQTWa0D . The reviewer \u2019 s questions are addressed below . > Won \u2019 t the shortest-path distance of the next observation always be 1 because it is immediately following the current step , and thus this results in a constant bonus ? In Section 2.2 we introduce a novelty threshold b_novelty for entering the memory buffer which addresses exactly this problem . This threshold implicitly discretizes the embedding space : only sufficiently novel observations are rewarded with a bonus . Very likely , if the current observation is in memory , the next one wo n't be considered novel . Not only the next observation wo n't receive a large bonus , it also wo n't enter the memory buffer . As time passes by , the agent will go further and further away from observations in memory , the reward bonus will increase and at some point exceed b_novelty -- and only then the observation will enter the buffer . Of course , right after that the reward will drop again . Please take a look at this reward visualization : https : //youtu.be/mphIRR6VsbM > It would perhaps make more sense if you used a different aggregation , such average , in which case you would be giving bonuses to observations that are farther away from the past on average . We have tried average in the past , it did not work well . The reason is probably that the average is not robust to outliers -- which are abundant as the visual similarity ca n't be perfect . > Also , while eventually this idea makes sense , it only makes sense within a single episode . If you clear the memory between episodes , then you are relying on some natural stochasticity of the algorithm to avoid revisiting the same states as in the previous episode . Otherwise , it seems like there is not much to be gained from actually resetting and starting a new episode ; it would encourage more exploration to just continue the same episode , or not clear memory when starting a new episode . The typical goal of RL is to maximize the reward throughout the current episode . The information from other episodes might be coming from a completely different environment/maze ( unless you make an assumption that it is the same environment in every episode ) . If you visited some places in one maze , how would it help you to determine novelty of the current observation in another maze ? > Section 2.2 : You say you have a novelty threshold of 0 in practice , doesn \u2019 t this mean you end up always adding new observations to the memory ? As we have an additive factor beta = 0.5 , the bonus b ends up in interval [ -alpha/2 , alpha/2 ] . Thus b_novelty = 0 is the middle of this interval and not all observations end up in the memory . > I do think you should rework your intuition . It seems to me what you are actually doing is creating some sort of implicit discretization of the observation space , and rewarding observations that you have not seen before under this discretization . This is exactly what we are doing -- by introducing b_novelty we implicitly discretize the embedding space . > I like your grid oracle , as it acts as a baseline for using PPO and provides a point of reference for how well an exploration bonus could potentially be . But why aren \u2019 t grid oracle results put into your graphs ? Your results look good and are very promising . At the time of the submission , we didn \u2019 t include Grid-Oracle into the plots because otherwise it was hard to see the difference between the comparable methods ( note that it is not fair to compare Oracle with them ) . In the latest version of our paper , we included it for all DMLab curves ."}, {"review_id": "SkeK3s0qKQ-2", "review_text": "The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems. The bonus is given by an auxillary network which tries to score whether a candidate observation is difficult to reach with respect to all previously observed novel observations which are stored in a memory buffer. The paper considers many experiments on complex 3D environments. The paper is well written and very well illustrated. The method can be clearly understood from the 3 figures and the examples are nice. I think the method is interesting and novel and it is evaluated on a realistic and challenging problem. It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements and related to that if the implementation is cumbersome. I didn\u2019t understand well how the method avoids the issue of old memories leaving the buffer. It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus? For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately) Are there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area. The Grid-Oracle result is very interesting and a contribution on it\u2019s own if similar results for complex 3D environments are not published anywhere else. It demonstrates well that exploration bonuses can help drastically in these tasks. I think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables. Indeed as a general problem the current number of training steps of any methods shown seem to indicate these techniques are too data hungry for non-simulated environments. For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider. I would be interested to know if the authors had some thoughts on this. Overall I lean towards accept, the method is shown to work on relatively very complex problems in DMLab and VizDoom while most sparse reward solutions proposed are typically evaluated on relatively simpler and unrealistic tasks. I would consider to further increase my score if the authors can address some of the comments. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their work on the review . > It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements The most computationally intensive parts of the algorithm are the memory reachability queries . Reachabilities to past memories are computed in parallel via mini-batching . We have shown the algorithm to work reasonably fast with a memory size of 200 . For significantly larger memory sizes , one would need to better parallelize reachability computations -- which should be doable . Memory consumption for the stored memories is very modest ( 400 KB ) , as we only store 200 of 512-float-embeddings , not the observations . > and related to that if the implementation is cumbersome . The implementation is relatively easy . We commit to publishing the source code if the paper is accepted -- this would make adoption even easier . > I didn \u2019 t understand well how the method avoids the issue of old memories leaving the buffer . Forgetting is unavoidable if the storage size is limited . That said , not all old memories are erased . The distribution of memory age is geometric : so older memories are sparser than the recent ones , but still present . Please see our visualization of the memory state : https : //youtu.be/mphIRR6VsbM . Please note that we denote memories by their location only for visualization purposes , the coordinates are not available to our method . > It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus ? This is true : when revisiting a part of state space that the agent hasn \u2019 t been to for a long time , many of the memories from that region may have been discarded and the curiosity bonus may offer more reward for returning to these states . This should not be a problem though : the curiosity bonus would still provide some reactive incentive to move away from recent memories -- because recent memories are always well-represented . And , it is possible that it would be good to incentivise visiting states that haven \u2019 t been seen in a long time . > For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline ( e.g.number of nodes for example and how long they run approximately ) PPO+ICM is 1.09x slower than PPO and PPO+EC ( our method ) is 1.84x slower than PPO . As for the number of parameters , R-network brings 13M trainable variables , while PPO alone was 1.7M and PPO+ICM was 2M . That said , we have spent almost no effort optimizing it in terms of speed/parameters , so it is likely easy to make improvements in this respect . It \u2019 s quite likely that we do not need a Resnet-18 for the R-network -- a much simpler model may work as well . In this paper , we just followed the setup for the R-network from prior work https : //arxiv.org/abs/1803.00653 because it was shown to perform well , but there is no evidence that this setup is necessary . > Are there any potential issues with adapting the method on 2D environments like Atari ? this could permit direct comparisons with several other recently proposed techniques in this area . We have n't tried it for Atari , so it is hard to predict . That said , we try to focus on more visually complex environments . In Atari , there is always a danger that the method would exploit exact observation repeatability . One recent work https : //arxiv.org/pdf/1606.04460.pdf estimated this repeatability to reach 60 % in some games , and > 10 % in many . This creates a dangerous incentive for the exploration algorithms to brute-force this vulnerability . On the other hand , in DMLab , such repeatability was estimated by the same work as < 0.1 % . > The Grid-Oracle result is very interesting and a contribution on it \u2019 s own\u2026 I think if possible it would be interesting to have an idea how fast this method converges ( number of training steps ) We don \u2019 t include Grid-Oracle into the plots because otherwise it is hard to see the difference between the comparable methods ( note that it is not fair to compare Oracle with them ) . That said , Oracle converges faster than any other method -- but requires privileged information . To give specific numbers , after 5M 4-repeated steps Grid-Oracle reaches approximately reward 40 in the `` Sparse '' environment , reward 35 in the `` Very Sparse '' environment and reward 20 in the `` Sparse+Doors '' environment . This is way higher than any other method in our study . We will include those numbers into the manuscript . > For some applications ( e.g.aimed at sim-to-real transfer ) the grid-oracle approach might be a good alternative to consider . The Oracle could be useful in situations where additional information is available about the environment . However , it is not universal , so we have not focused on the possibility of taking advantage of privileged information in the current manuscript ."}, {"review_id": "SkeK3s0qKQ-3", "review_text": "In this paper, the authors study the problem of exploration in RL when the reward process is sparse. They introduce a new curiosity based approach which considers a state novel if it was not visited before and is far from the visited states. They show that their methods perform better than two other approaches, one without curiosity-driven exploration and the second one a one-step curiosity-driven approach. The paper is well-written and easy to follow. The authors motivate this work by bringing an example where the state observation might be novel but important. They show that if part of the environment just changes randomly, then there is no need to explore there as much as vanilla curiosity-driven approaches to suggest. The approach in this paper partially addresses this drawback of curiosity-driven approaches. The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest. The problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves. An efficient explorative/exploitative RL agent explores part of state space more if there is uncertainty in the reward and state distribution rather than not being able to predict a particular sample. In curiosity-driven approaches, if the predictability of the next state is considered, all methods are sentenced to failure in stochastic environments. The approach in this paper partially mitigate this problem but for a very specific environment setup, but still, fails if the environment is stochastic. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest . It would be helpful if the reviewer could be more specific here . > The problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves . Could the reviewer please provide references to such methods , demonstrated on visually-rich 3D environments ? > In curiosity-driven approaches , if the predictability of the next state is considered , all methods are sentenced to failure in stochastic environments . The approach in this paper partially mitigate this problem but for a very specific environment setup , ViZDoom and DMLab are standard benchmarks . We used the standard action sets for those benchmarks . Could the reviewer please elaborate more on what is very specific about our environmental setup ? > but still , fails if the environment is stochastic . Could the reviewer please be more specific here ? That is , how does the method fail in the case that the environment is stochastic ?"}], "0": {"review_id": "SkeK3s0qKQ-0", "review_text": "The main idea of this paper is to propose a heuristic method for exploration in deep reinforcement learning. The work is fairly innovative in its approach, where an episodic memory is used to store agent\u2019s observations while rewarding the agent for reaching novel observations not yet stored in memory. The novelty here is determined by a pre-trained network that computes the within k-step-reachability of current observation to the observations stored in memory. The method is quite simple but promising and can be easily integrated with any RL algorithm. They test their method on a pair of 3D environments, VizDoom and DMLab. The experiments are well executed and analysed. Positives: - They do a rigorous analysis of parameters, and explicitly count the pre-training interactions with the environment in their learning curves. - This method does not hurt when dense environmental rewards are present. - The memory buffer is smaller than the episode length, which avoids trivial solutions. - The idea of having a discriminator assess distance between states is interesting. Questions and critics: - The tasks explored in this paper are all navigation based tasks, would this method also apply equally successfully to non-navigation domains such as manipulation? - My main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data. In navigation domains it makes sense that the random policy could cover the space fairly well, however, this will not be the case for more complex tasks involving more complex dynamics. - It was surprising to me that the choice of k does not seem to be that important. As it implicitly defines what \u201cnovelty\u201d means for an environment, I would have expected that its value should be calibrated better. Could that be a function of the navigation tasks considered? - The DMLab results are not great or comparable to the state-of-the-art methods, which may hinder interpreting how good the policies really are. This was perhaps a conscious choice given they are only interested in early training results, but that seems like a confound. - The architecture does not include an RNN which makes certain things very surprising even though they shouldn't (e.g. firing, or moving around a corner, are specifically surprising for ICM) as they cannot be learnt, but perhaps if they had an RNN in the architecture these would be easy to explain? Would be interesting to see what are the authors thoughts on this (apart from their computational complexity argument they mention)? - Having the memory contain only information about the current episode with no information transfer between episodes seems a bit strange to me, I would like to hear the motivation behind this? - The fact that the memory is reset between episodes, and that the buffer is small, can mean that effectively the method implements some sort of complex pseudo count over meta-states per episode? - The embedding network is only trained during the pre-training phase and frozen during the RL task. This sounds a bit limiting to me: what if the agent starts exploring part of the space that was not covered during pre-training? Obviously this could lead to collapses when allowing to fine-tune it, but I feel this is rather restrictive. Again, I feel that the choice of navigation tasks did not magnify this problem, which would arise more in harder exploration tasks. - I think that alluding that their method is similar to babies\u2019 behaviour in their cradle is stretched at best and not a constructive way to motivate their work\u2026 - In Figure 6 and 7, all individual curves from each seed run are shown, which is a bit distracting. Perhaps showing the mean and std would be a cleaner and easier-to-interpret way to report these results? Overall, it is a simple and interesting idea and seems quite easy to implement. However, everything is highly dependent on how varying the environment is, how bad the exploration policy used for pre-training is, how good the embeddings are once frozen, and how k, action repeat and memory buffer size interact. Given that the experiments are all navigation based, it makes it hard for me to assess whether this method can work as well in other domains with harder exploration setups. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their work on the review . > The tasks explored in this paper are all navigation based tasks , would this method also apply equally successfully to non-navigation domains such as manipulation ? This is a great question and we work on experiments in other domains . However , we would like to point out that the tasks we already have in the paper are both non-trivial and more visually complex than in many other works in the field of sparse-reward exploration . > My main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data . In navigation domains it makes sense that the random policy could cover the space fairly well , however , this will not be the case for more complex tasks involving more complex dynamics . We agree that for more complex domains randomly collected data may be insufficient and online training of the R-network may be crucial . To address the concerns of the reviewer , we have implemented a version of our algorithm which performs training of R-network online ( together with the policy ) . Preliminary results indicate that such training is possible and does not collapse . It produces results at least as good as pre-training . Thus , we are able to demonstrate that our approach can function with online training , offering the possibility of functioning in domains where collection of random data may be insufficient . In addition , we would like to point out that the R-network ( Embedding + Comparator ) can generalize beyond what was seen in the pre-training stage . We have such an experiment in the supplementary section S3 `` R-network generalization study '' . In particular , in Table S4 , R-networks trained on levels `` Dense 1 '' and `` Sparse + Doors '' generalize to the `` Very Sparse '' environment . The visual gap is quite significant : please compare https : //youtu.be/C5g10cUl7Ew with https : //youtu.be/9J4CzdOz60I , for example . All this is possible because R-network is solving a simple problem of comparing two observations given access to both observations at the same time . Moreover , in the real world , people typically hand-design the initial exploration policy even for the standard RL methods , let alone the model-based ones ( and our method could be considered partially model-based ) . For example , please take a look at the recent work https : //ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html ( which has just received the best paper award at the Conference on Robotic Learning ) . Another recent work from ICLR \u2019 18 https : //openreview.net/forum ? id=BkisuzWRW also uses hand-crafted policy for the robotic manipulation task to collect data for training the inverse model of the environment . > It was surprising to me that the choice of k does not seem to be that important . As it implicitly defines what \u201c novelty \u201d means for an environment , I would have expected that its value should be calibrated better . Could that be a function of the navigation tasks considered ? Those values of k are still rather small . What we demonstrate in this experiment is that our method is not excessively sensitive to this parameter when it is chosen within a reasonable range . > The DMLab results are not great or comparable to the state-of-the-art methods , which may hinder interpreting how good the policies really are . This was perhaps a conscious choice given they are only interested in early training results , but that seems like a confound . As far as we know , SOTA results are achieved by Impala https : //arxiv.org/abs/1802.01561 at 1B steps ( 250M 4-repeated steps ) . We haven \u2019 t yet run our experiments at this scale : we use 20M 4-repeated steps in our PPO setup with 12 actors on GPU , which already takes 2 days to complete . Furthermore , being more sample efficient is an appealing property of more effective exploration as interactions with an environment might be costly in some environments ."}, "1": {"review_id": "SkeK3s0qKQ-1", "review_text": "This paper proposes a new method to give exploration bonuses in RL algorithms by giving larger bonuses to observations that are farther away (> k) in environment steps to past observations in the current episode, encouraging the agent to visit observations farther away. This is in contrast to existing exploration bonuses based on prediction gain or prediction error, which do not work properly for stochastic transitions. Overall, I very much like the idea, but I found many little pieces of confusing explanations that could be further clarified, and also some questionable implementation details. However the experimental results are very promising, and the approach should be modular and slotable into existing deep RL methods. Section Introduction: I\u2019m confused by how you can define such a bonus if the memory is the current episode. Won\u2019t the shortest-path distance of the next observation always be 1 because it is immediately following the current step, and thus this results in a constant bonus? You explain how you get around this in practice, but intuitively and from a high-level, this idea does not make sense. It would perhaps make more sense if you used a different aggregation, such average, in which case you would be giving bonuses to observations that are farther away from the past on average. Also, while eventually this idea makes sense, it only makes sense within a single episode. If you clear the memory between episodes, then you are relying on some natural stochasticity of the algorithm to avoid revisiting the same states as in the previous episode. Otherwise, it seems like there is not much to be gained from actually resetting and starting a new episode; it would encourage more exploration to just continue the same episode, or not clear memory when starting a new episode. Section 2.2: You say you have a novelty threshold of 0 in practice, doesn\u2019t this mean you end up always adding new observations to the memory? In this case, then it seems like your aggregation method of taking the 90th percentile is really the only mechanism that avoids the issue of always predicting a constant distance of 1 (and relying on the function approximator\u2019s natural errors). I do think you should rework your intuition. It seems to me what you are actually doing is creating some sort of implicit discretization of the observation space, and rewarding observations that you have not seen before under this discretization. This is what would correspond to a shortest-path distance aggregation. Experiments: I like your grid oracle, as it acts as a baseline for using PPO and provides a point of reference for how well an exploration bonus could potentially be. But why aren\u2019t grid oracle results put into your graphs? Your results look good and are very promising. Other points: - The pre-training of the R-network is concerning, but you have already responded with preliminary results. - I do share some of the concerns other reviewers have brought up about generality beyond navigation tasks, e.g. Atari games. To me, it seems like this method can run into difficulty when reachability is not as nice as it is in navigation tasks, for example if the decisions of the task followed a more tree-like structure. This also does not work well with the fact that you reset every episode, so there is nothing to encourage an agent to try different branches of the tree every episode. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their work on the review . Please note that the paper has just been updated as per the request from AR2 , here is an anonymous link to the updated paper : https : //drive.google.com/open ? id=1tUHfBwWWu6W2zuk-De0AyYWxuNQTWa0D . The reviewer \u2019 s questions are addressed below . > Won \u2019 t the shortest-path distance of the next observation always be 1 because it is immediately following the current step , and thus this results in a constant bonus ? In Section 2.2 we introduce a novelty threshold b_novelty for entering the memory buffer which addresses exactly this problem . This threshold implicitly discretizes the embedding space : only sufficiently novel observations are rewarded with a bonus . Very likely , if the current observation is in memory , the next one wo n't be considered novel . Not only the next observation wo n't receive a large bonus , it also wo n't enter the memory buffer . As time passes by , the agent will go further and further away from observations in memory , the reward bonus will increase and at some point exceed b_novelty -- and only then the observation will enter the buffer . Of course , right after that the reward will drop again . Please take a look at this reward visualization : https : //youtu.be/mphIRR6VsbM > It would perhaps make more sense if you used a different aggregation , such average , in which case you would be giving bonuses to observations that are farther away from the past on average . We have tried average in the past , it did not work well . The reason is probably that the average is not robust to outliers -- which are abundant as the visual similarity ca n't be perfect . > Also , while eventually this idea makes sense , it only makes sense within a single episode . If you clear the memory between episodes , then you are relying on some natural stochasticity of the algorithm to avoid revisiting the same states as in the previous episode . Otherwise , it seems like there is not much to be gained from actually resetting and starting a new episode ; it would encourage more exploration to just continue the same episode , or not clear memory when starting a new episode . The typical goal of RL is to maximize the reward throughout the current episode . The information from other episodes might be coming from a completely different environment/maze ( unless you make an assumption that it is the same environment in every episode ) . If you visited some places in one maze , how would it help you to determine novelty of the current observation in another maze ? > Section 2.2 : You say you have a novelty threshold of 0 in practice , doesn \u2019 t this mean you end up always adding new observations to the memory ? As we have an additive factor beta = 0.5 , the bonus b ends up in interval [ -alpha/2 , alpha/2 ] . Thus b_novelty = 0 is the middle of this interval and not all observations end up in the memory . > I do think you should rework your intuition . It seems to me what you are actually doing is creating some sort of implicit discretization of the observation space , and rewarding observations that you have not seen before under this discretization . This is exactly what we are doing -- by introducing b_novelty we implicitly discretize the embedding space . > I like your grid oracle , as it acts as a baseline for using PPO and provides a point of reference for how well an exploration bonus could potentially be . But why aren \u2019 t grid oracle results put into your graphs ? Your results look good and are very promising . At the time of the submission , we didn \u2019 t include Grid-Oracle into the plots because otherwise it was hard to see the difference between the comparable methods ( note that it is not fair to compare Oracle with them ) . In the latest version of our paper , we included it for all DMLab curves ."}, "2": {"review_id": "SkeK3s0qKQ-2", "review_text": "The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems. The bonus is given by an auxillary network which tries to score whether a candidate observation is difficult to reach with respect to all previously observed novel observations which are stored in a memory buffer. The paper considers many experiments on complex 3D environments. The paper is well written and very well illustrated. The method can be clearly understood from the 3 figures and the examples are nice. I think the method is interesting and novel and it is evaluated on a realistic and challenging problem. It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements and related to that if the implementation is cumbersome. I didn\u2019t understand well how the method avoids the issue of old memories leaving the buffer. It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus? For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately) Are there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area. The Grid-Oracle result is very interesting and a contribution on it\u2019s own if similar results for complex 3D environments are not published anywhere else. It demonstrates well that exploration bonuses can help drastically in these tasks. I think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables. Indeed as a general problem the current number of training steps of any methods shown seem to indicate these techniques are too data hungry for non-simulated environments. For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider. I would be interested to know if the authors had some thoughts on this. Overall I lean towards accept, the method is shown to work on relatively very complex problems in DMLab and VizDoom while most sparse reward solutions proposed are typically evaluated on relatively simpler and unrealistic tasks. I would consider to further increase my score if the authors can address some of the comments. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their work on the review . > It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements The most computationally intensive parts of the algorithm are the memory reachability queries . Reachabilities to past memories are computed in parallel via mini-batching . We have shown the algorithm to work reasonably fast with a memory size of 200 . For significantly larger memory sizes , one would need to better parallelize reachability computations -- which should be doable . Memory consumption for the stored memories is very modest ( 400 KB ) , as we only store 200 of 512-float-embeddings , not the observations . > and related to that if the implementation is cumbersome . The implementation is relatively easy . We commit to publishing the source code if the paper is accepted -- this would make adoption even easier . > I didn \u2019 t understand well how the method avoids the issue of old memories leaving the buffer . Forgetting is unavoidable if the storage size is limited . That said , not all old memories are erased . The distribution of memory age is geometric : so older memories are sparser than the recent ones , but still present . Please see our visualization of the memory state : https : //youtu.be/mphIRR6VsbM . Please note that we denote memories by their location only for visualization purposes , the coordinates are not available to our method . > It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus ? This is true : when revisiting a part of state space that the agent hasn \u2019 t been to for a long time , many of the memories from that region may have been discarded and the curiosity bonus may offer more reward for returning to these states . This should not be a problem though : the curiosity bonus would still provide some reactive incentive to move away from recent memories -- because recent memories are always well-represented . And , it is possible that it would be good to incentivise visiting states that haven \u2019 t been seen in a long time . > For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline ( e.g.number of nodes for example and how long they run approximately ) PPO+ICM is 1.09x slower than PPO and PPO+EC ( our method ) is 1.84x slower than PPO . As for the number of parameters , R-network brings 13M trainable variables , while PPO alone was 1.7M and PPO+ICM was 2M . That said , we have spent almost no effort optimizing it in terms of speed/parameters , so it is likely easy to make improvements in this respect . It \u2019 s quite likely that we do not need a Resnet-18 for the R-network -- a much simpler model may work as well . In this paper , we just followed the setup for the R-network from prior work https : //arxiv.org/abs/1803.00653 because it was shown to perform well , but there is no evidence that this setup is necessary . > Are there any potential issues with adapting the method on 2D environments like Atari ? this could permit direct comparisons with several other recently proposed techniques in this area . We have n't tried it for Atari , so it is hard to predict . That said , we try to focus on more visually complex environments . In Atari , there is always a danger that the method would exploit exact observation repeatability . One recent work https : //arxiv.org/pdf/1606.04460.pdf estimated this repeatability to reach 60 % in some games , and > 10 % in many . This creates a dangerous incentive for the exploration algorithms to brute-force this vulnerability . On the other hand , in DMLab , such repeatability was estimated by the same work as < 0.1 % . > The Grid-Oracle result is very interesting and a contribution on it \u2019 s own\u2026 I think if possible it would be interesting to have an idea how fast this method converges ( number of training steps ) We don \u2019 t include Grid-Oracle into the plots because otherwise it is hard to see the difference between the comparable methods ( note that it is not fair to compare Oracle with them ) . That said , Oracle converges faster than any other method -- but requires privileged information . To give specific numbers , after 5M 4-repeated steps Grid-Oracle reaches approximately reward 40 in the `` Sparse '' environment , reward 35 in the `` Very Sparse '' environment and reward 20 in the `` Sparse+Doors '' environment . This is way higher than any other method in our study . We will include those numbers into the manuscript . > For some applications ( e.g.aimed at sim-to-real transfer ) the grid-oracle approach might be a good alternative to consider . The Oracle could be useful in situations where additional information is available about the environment . However , it is not universal , so we have not focused on the possibility of taking advantage of privileged information in the current manuscript ."}, "3": {"review_id": "SkeK3s0qKQ-3", "review_text": "In this paper, the authors study the problem of exploration in RL when the reward process is sparse. They introduce a new curiosity based approach which considers a state novel if it was not visited before and is far from the visited states. They show that their methods perform better than two other approaches, one without curiosity-driven exploration and the second one a one-step curiosity-driven approach. The paper is well-written and easy to follow. The authors motivate this work by bringing an example where the state observation might be novel but important. They show that if part of the environment just changes randomly, then there is no need to explore there as much as vanilla curiosity-driven approaches to suggest. The approach in this paper partially addresses this drawback of curiosity-driven approaches. The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest. The problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves. An efficient explorative/exploitative RL agent explores part of state space more if there is uncertainty in the reward and state distribution rather than not being able to predict a particular sample. In curiosity-driven approaches, if the predictability of the next state is considered, all methods are sentenced to failure in stochastic environments. The approach in this paper partially mitigate this problem but for a very specific environment setup, but still, fails if the environment is stochastic. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest . It would be helpful if the reviewer could be more specific here . > The problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves . Could the reviewer please provide references to such methods , demonstrated on visually-rich 3D environments ? > In curiosity-driven approaches , if the predictability of the next state is considered , all methods are sentenced to failure in stochastic environments . The approach in this paper partially mitigate this problem but for a very specific environment setup , ViZDoom and DMLab are standard benchmarks . We used the standard action sets for those benchmarks . Could the reviewer please elaborate more on what is very specific about our environmental setup ? > but still , fails if the environment is stochastic . Could the reviewer please be more specific here ? That is , how does the method fail in the case that the environment is stochastic ?"}}