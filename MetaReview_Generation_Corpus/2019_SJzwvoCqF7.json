{"year": "2019", "forum": "SJzwvoCqF7", "title": "On Tighter Generalization Bounds for Deep Neural Networks: CNNs, ResNets, and Beyond", "decision": "Reject", "meta_review": "I'm quite concerned by the conversation with Anonymous, entitled \"Why is the dependence...\". My issues concern the empirical Rademacher complexity (ERC) and in particular the choice of the loss class for which the ERC is being computed. This  class is obviously data dependent, but the Reviewers concerns centers on the nature of its data dependence. It is not valid to define the classes by the Jacobian's norm on the input data, as this _structure_ over the space of classes is data dependent, which is not kosher. The reviewer was gently pushing the authors towards a very strong assumption... i'm guessing that the jacobian norm over all data sets was bounded by a particular constant. This seems like a whopping assumption. The fact that I can so easily read this concern off of the reviewer's comments and the authors seem to not be able to understand what the reviewer is getting at, concerns me.\n\nBesides this concern, it seems that this paper has undergone a rather significant revision. I'm not convinced the new version has been properly reviewed. For a theory paper, I'm concerned about letting work through that's not properly vetted, and I'm really not certain this has been. I suggest the authors consider sending it to COLT.", "reviews": [{"review_id": "SJzwvoCqF7-0", "review_text": "The rebutal and the revision of the paper solve my comments. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ The paper presents a new characterization of generalization error bound for general deep neural networks in terms of the depth and width of the networks and the spectral norm of weight matrices. The proof follows the setting of Bartlett et al. 2017 with new development on the Lipschitz properties of neural networks. Pros: 1. The paper provides a solid improvement over previous bounds on generalization error. 2. The presentation of the result and proofs is clear and easy to follow. 3. It does case studies specially on widely used network structures CNN, ResNet, etc. Con: 1. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018. If it is vacuous, it is hard to justify the claims that given the generalization error, which is vacuous for all bounds, the paper's bound allows the choices of larger dimensions of parameters and larger spectral norms of weight matrices. 2. The L_w has the factor \"products of B_{d,2}s\" which, however, does not show up in the final generalization bound (The equation right above Appendix B). This products may introduce an additional $D$ under sqrt changing D to D^2 under the sqrt, which changes the order. The authors should give some explanation on this. 3. Is the assumption on the orthogonal and normalized filters in CNN a must thing for the argument or just for convenience of the presentation? The paper should be clearer about this point. 4. The RHS of the equation in Lemma 2 misses terms related with B_{d,2}. 5. Typos: Find one typo in Page 3 \"de\ufb01ed as\" ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for recognizing our contributions . 1.As in existing norm based bounds , it can be vacuous ( e.g. , when the norms of weight matrices are large ) if we do not have further structural conditions on the networks . Our effort here is to tighten the potentially vacuous bound from two directions : ( 1 ) reduce the dependence in terms of the depth and width ; ( 2 ) reduce the product of norms to the loss function output when it is bounded . With these improvements , we can obtain non-vacuous bounds . For example , in the case of CNNs with a bounded loss , our generalization bound can be < < 1 given a moderate training sample size . An extended numerical result is provided in Appendix A.1 to support this claim . 2.We updated the proof of Theorem 1 in terms of the L_w using the spectral norm of Jacobian operators instead of the product of spectral norms of weight matrices . This is a tighter result since the spectral norm of Jacobian is significantly smaller than the product of spectral norms of weight matrices . For example , when we constrain the network to be obtained from stochastic gradient descent using randomly initialized weights , the spectral norm of Jacobian is significantly smaller than the product of spectral norms of weight matrices that can be exponential on depth in general . To illustrate this , we provided an extended numerical result on the empirical distributions of the norm of Jacobian and the product of spectral norms in Appendix A.2 and A.3 . We can observe that the values of the norm of Jacobian are orders smaller than the values product of norms , and the former quantity increases significantly slower than the latter when the depth increases ( even slower than some low degree polynomial on the depth ) . When we consider the network functions obtained using proper training procedures , we do have sqrt ( D ) dependence in ERC rather than D. 3 . The orthogonal and normalized filters in CNNs have shown comparable or even improved empirical results than their non-orthogonal counterpart [ 1,2 ] . This motivates us to analyze this setting for CNNs . In addition , the orthogonal filters also provide a way to quantify the spectral of weight matrices for CNNs tightly based on the filter size and stride size . We further clarified this in the revision . On the other hand , the orthogonality is not a must as we discussed in the numerical evaluation ( Section 4.5 ) . When the filters are not orthogonal , our bound reduces to the number of total parameters in filters ( # filters * filter size ) . This is one of the benefits of our analytic pipeline that allows obtaining potentially tighter bounds based on the number of parameters rather than the width of weight matrices . 4 and 5.The typos are corrected . [ 1 ] Huang et al.Orthogonal weight normalization : Solution to optimization over multiple dependent stiefel manifolds in deep neural networks . [ 2 ] Xie et al.All you need is beyond a good init : Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation ."}, {"review_id": "SJzwvoCqF7-1", "review_text": "After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ This paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. The proposed work is shown to provide a tighter generalization error bounds compared with a few existing literatures. Pros: This paper makes theoretical contributions to the understanding of DNNs. This is an important but difficult task. As a theoretical paper, this one is relatively easy to follow. Cons: In spite of its theoretical contributions, this paper has a few major issues. Q1: This paper fails to fairly compare with the most recent work, Arora et al. (2018), Zhou and Feng (2018). For instance, Arora et al. (2018) uses error-resilience parameters instead of the norms of weight matrices to obtain a better generalization error. The authors claim that the error-resilience parameters are less interpretable than the norms of weight matrices. This claim could be subjective and is not convincing. Q2: The error bounds of Bartlett et al. (2017), Neyshabur et al. (2017) could be improved for low-rank weight matrices, in which case the proposed Theorem 1 is tighter only if $p \\le D^2$. This holds only when DNN is very deep. Can theorem 1 be improved by similarly considering the low-rankness of weight matrices? Q3: In Corollary 2, the error bound for CNN, the authors assume that the filters are orthogonal with unit norm. Can the authors provide some justification on the orthogonal filters? In addition, Zhou and Feng (2018) have achieved similar bound for CNN. Can the authors provide some justification why this latest result is not included in Table 2? ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the careful reading and recognizing our contributions . 1.We have added some discussion to compare with the [ 1,2 ] after corollary 1 in the bounded loss case . Moreover , we extended the numerical result to compare with [ 1,2 ] in Appendix A.1 for the trained network on CIFAR10 . Please refer further details therein . Our results show that our bound is tighter than the existing works based on the bounded function [ 1,2 ] . 2.Thanks to the reviewer for pointing this out , we updated our analysis to incorporate the rank of weight matrices in the revision . In the updated results , we have sqrt ( pr ) ( r is the rank ) dependence , which is of the same order with [ 3,4 ] in terms of the width . To this end , our bound is strictly better than [ 3,4 ] by a margin at least D in all scenarios of ranks . 3.The orthogonal and normalized filters in CNNs have shown comparable or even improved empirical results than their non-orthogonal counterpart [ 5,6 ] . This motivates us to analyze this setting for CNNs , which also provide a way to quantify the spectral of weight matrices for CNNs tightly based on the filter size and stride size . We further clarified this in the revision . In addition , we provided further discussion to compare with [ 2 ] in Section 3.2 and extended numerical evaluations in Appendix A.1 in the bounded loss case . This support that our new bound is tighter than [ 2 ] in this case . Also , note that the orthogonality is not a must as we discussed in the numerical evaluation ( Section 4.5 ) . When the filters are not orthogonal , our bound reduces to the number of total parameters in filters ( # filters * filter size ) . This is one of the benefits of our analytic pipeline that allows obtaining tighter bounds based on the number of parameters rather than the width of weight matrices . [ 1 ] Arora et al.Stronger generalization bounds for deep nets via a compression approach . [ 2 ] Zhou and Feng . Understanding generalization and optimization performance of deep cnns . [ 3 ] Bartlett et al.Spectrally-normalized margin bounds for neural networks . [ 4 ] Neyshabur et al.A pac-bayesian approach to spectrally-normalized margin bounds for neural networks . [ 5 ] Huang et al.Orthogonal weight normalization : Solution to optimization over multiple dependent stiefel manifolds in deep neural networks . [ 6 ] Xie et al.All you need is beyond a good init : Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation ."}, {"review_id": "SJzwvoCqF7-2", "review_text": "The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network as a parametric function in Depth * Width ^2 number of parameters, and then using standard L-2 covering and Dudley Integral. They extend this technique for CNNs, Resnets, Hyper-spherical Networks, etc and provide specialized bounds for each case. In the end, the authors provide comparisons to the existing bounds. Although intended, the bound in Theorem-1 depends on the number of parameters and hold only if m > d * (p^2) = number of parameters (from the last line of proof of Lemma 3, we need \\beta < \\alpha and thus m > h). Such bounds are already know in the literature (see Anthony and Bartlett, 1999). Adaptive (completely norm dependent, like Bartlett et. al. 2017) bounds will be better than explicit dimension dependent bounds. The comparison in Figure-1 which suggest their bound to be better is unfair because they are comparing their specialized bounds for CNN to generic bounds for standard feedforward networks. Same for comparison in Table-2. It was already established in Theorem 3.4 (Bartlett et al. 2017) that spectral norms are necessary for any generalization bounds for Deep Neural Networks, thus voiding the claims made in the paper (and discussion) about the importance of spectral norms. Typos / Errors : 1. Statement of Lemma 2 does not contain the spectral norms terms. 2. The third equation in Page 13 should be K <= \\sqrt{pD} max B_{d, 2}; and this changes the bound further. The paper introduces some new techniques on mathematical analysis of specialized neural networks. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for careful reading and helpful comments . We do have the same dependence with the VC type of bound in terms of the number of parameters , but we are considering essentially a smaller class of network functions ( than that considered in the VC type of bound ) with norm constraints . In the case when all norms are bounded ( by 1 ) or the loss function is bounded ( e.g. , the ramp loss < 1 ) , then our resulting bound can be smaller than the VC type of bound . We want to remark that we have updated our analysis for DNNs in terms of the rank of weight matrices , which may lead to tighter results when the weight matrices are low-rank , analogous to [ 1,2 ] . Our result for CNNs is a direct derivation from Theorem 1 ( for DNNs ) . This is one benefit of our analysis based on the Lipschitz property on the parameters , which allows us to directly reduce from pr ( r is the rank of W ) to k^2 in the CNNs case . For the other comparing results in Figure 1 , to the best of our knowledge , there is no such direct simplification from the general case to the CNNs case from their analysis . In other words , they have the same order of bounds for DNNs and CNNs . Note that the ranks are of the same order with width p in CNNs when the filters are linear independent , and we already simplified the norms in the other comparing results as in our result for CNNs in Table 2 . Moreover , we want to remark that Thm 3.4 in [ 1 ] is the lower bound of the ERC for the neural network function , which did not take the loss function into consideration . In other words , it does not conflict our case when the loss is bounded , which can lead to tighter dependence in terms of the loss function bound rather than the product of norm . Note that our result shown in Figure 1 is the worse case of Corollary 1 . In practice , the loss function for a trained network has a significantly smaller scale than \\prod B_ { d,2 } , which mean our result in Figure 1 is in fact significantly smaller than what is shown . This is another benefit of our analysis that allows us to only depend on the output of loss function , rather than the product of norms that the comparing results in Figure 1 can not avoid due to the nature of their analyses . We have added an extended experiment in Appendix A.1 for the bounded loss case . One can find that the generalization bound in the bounded loss case can be significantly smaller than the norm based result . We also added a discussion in Section 3.2 and Appendix A.1 to compare with existing output based bounds [ 3,4 ] and show that our output based result is tighter . In addition , from our observation on the trained network using real data , the dependence on the network sizes in other norm based results ( e.g. , the terms in Bound 1 and 2 in Figure 1 excluding the term of the product of norms ) are significantly larger than the number of parameters . Based on the updated Theorem 1 , [ 1,2 ] are always larger than our bound by a margin at least D , including the low-rank cases . The typos are corrected . [ 1 ] Bartlett et al.Spectrally-normalized margin bounds for neural networks . [ 2 ] Neyshabur et al.A pac-bayesian approach to spectrally-normalized margin bounds for neural networks . [ 3 ] Arora et al.Stronger generalization bounds for deep nets via a compression approach . [ 4 ] Zhou and Feng . Understanding generalization and optimization performance of deep cnns ."}], "0": {"review_id": "SJzwvoCqF7-0", "review_text": "The rebutal and the revision of the paper solve my comments. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ The paper presents a new characterization of generalization error bound for general deep neural networks in terms of the depth and width of the networks and the spectral norm of weight matrices. The proof follows the setting of Bartlett et al. 2017 with new development on the Lipschitz properties of neural networks. Pros: 1. The paper provides a solid improvement over previous bounds on generalization error. 2. The presentation of the result and proofs is clear and easy to follow. 3. It does case studies specially on widely used network structures CNN, ResNet, etc. Con: 1. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018. If it is vacuous, it is hard to justify the claims that given the generalization error, which is vacuous for all bounds, the paper's bound allows the choices of larger dimensions of parameters and larger spectral norms of weight matrices. 2. The L_w has the factor \"products of B_{d,2}s\" which, however, does not show up in the final generalization bound (The equation right above Appendix B). This products may introduce an additional $D$ under sqrt changing D to D^2 under the sqrt, which changes the order. The authors should give some explanation on this. 3. Is the assumption on the orthogonal and normalized filters in CNN a must thing for the argument or just for convenience of the presentation? The paper should be clearer about this point. 4. The RHS of the equation in Lemma 2 misses terms related with B_{d,2}. 5. Typos: Find one typo in Page 3 \"de\ufb01ed as\" ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for recognizing our contributions . 1.As in existing norm based bounds , it can be vacuous ( e.g. , when the norms of weight matrices are large ) if we do not have further structural conditions on the networks . Our effort here is to tighten the potentially vacuous bound from two directions : ( 1 ) reduce the dependence in terms of the depth and width ; ( 2 ) reduce the product of norms to the loss function output when it is bounded . With these improvements , we can obtain non-vacuous bounds . For example , in the case of CNNs with a bounded loss , our generalization bound can be < < 1 given a moderate training sample size . An extended numerical result is provided in Appendix A.1 to support this claim . 2.We updated the proof of Theorem 1 in terms of the L_w using the spectral norm of Jacobian operators instead of the product of spectral norms of weight matrices . This is a tighter result since the spectral norm of Jacobian is significantly smaller than the product of spectral norms of weight matrices . For example , when we constrain the network to be obtained from stochastic gradient descent using randomly initialized weights , the spectral norm of Jacobian is significantly smaller than the product of spectral norms of weight matrices that can be exponential on depth in general . To illustrate this , we provided an extended numerical result on the empirical distributions of the norm of Jacobian and the product of spectral norms in Appendix A.2 and A.3 . We can observe that the values of the norm of Jacobian are orders smaller than the values product of norms , and the former quantity increases significantly slower than the latter when the depth increases ( even slower than some low degree polynomial on the depth ) . When we consider the network functions obtained using proper training procedures , we do have sqrt ( D ) dependence in ERC rather than D. 3 . The orthogonal and normalized filters in CNNs have shown comparable or even improved empirical results than their non-orthogonal counterpart [ 1,2 ] . This motivates us to analyze this setting for CNNs . In addition , the orthogonal filters also provide a way to quantify the spectral of weight matrices for CNNs tightly based on the filter size and stride size . We further clarified this in the revision . On the other hand , the orthogonality is not a must as we discussed in the numerical evaluation ( Section 4.5 ) . When the filters are not orthogonal , our bound reduces to the number of total parameters in filters ( # filters * filter size ) . This is one of the benefits of our analytic pipeline that allows obtaining potentially tighter bounds based on the number of parameters rather than the width of weight matrices . 4 and 5.The typos are corrected . [ 1 ] Huang et al.Orthogonal weight normalization : Solution to optimization over multiple dependent stiefel manifolds in deep neural networks . [ 2 ] Xie et al.All you need is beyond a good init : Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation ."}, "1": {"review_id": "SJzwvoCqF7-1", "review_text": "After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ This paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. The proposed work is shown to provide a tighter generalization error bounds compared with a few existing literatures. Pros: This paper makes theoretical contributions to the understanding of DNNs. This is an important but difficult task. As a theoretical paper, this one is relatively easy to follow. Cons: In spite of its theoretical contributions, this paper has a few major issues. Q1: This paper fails to fairly compare with the most recent work, Arora et al. (2018), Zhou and Feng (2018). For instance, Arora et al. (2018) uses error-resilience parameters instead of the norms of weight matrices to obtain a better generalization error. The authors claim that the error-resilience parameters are less interpretable than the norms of weight matrices. This claim could be subjective and is not convincing. Q2: The error bounds of Bartlett et al. (2017), Neyshabur et al. (2017) could be improved for low-rank weight matrices, in which case the proposed Theorem 1 is tighter only if $p \\le D^2$. This holds only when DNN is very deep. Can theorem 1 be improved by similarly considering the low-rankness of weight matrices? Q3: In Corollary 2, the error bound for CNN, the authors assume that the filters are orthogonal with unit norm. Can the authors provide some justification on the orthogonal filters? In addition, Zhou and Feng (2018) have achieved similar bound for CNN. Can the authors provide some justification why this latest result is not included in Table 2? ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the careful reading and recognizing our contributions . 1.We have added some discussion to compare with the [ 1,2 ] after corollary 1 in the bounded loss case . Moreover , we extended the numerical result to compare with [ 1,2 ] in Appendix A.1 for the trained network on CIFAR10 . Please refer further details therein . Our results show that our bound is tighter than the existing works based on the bounded function [ 1,2 ] . 2.Thanks to the reviewer for pointing this out , we updated our analysis to incorporate the rank of weight matrices in the revision . In the updated results , we have sqrt ( pr ) ( r is the rank ) dependence , which is of the same order with [ 3,4 ] in terms of the width . To this end , our bound is strictly better than [ 3,4 ] by a margin at least D in all scenarios of ranks . 3.The orthogonal and normalized filters in CNNs have shown comparable or even improved empirical results than their non-orthogonal counterpart [ 5,6 ] . This motivates us to analyze this setting for CNNs , which also provide a way to quantify the spectral of weight matrices for CNNs tightly based on the filter size and stride size . We further clarified this in the revision . In addition , we provided further discussion to compare with [ 2 ] in Section 3.2 and extended numerical evaluations in Appendix A.1 in the bounded loss case . This support that our new bound is tighter than [ 2 ] in this case . Also , note that the orthogonality is not a must as we discussed in the numerical evaluation ( Section 4.5 ) . When the filters are not orthogonal , our bound reduces to the number of total parameters in filters ( # filters * filter size ) . This is one of the benefits of our analytic pipeline that allows obtaining tighter bounds based on the number of parameters rather than the width of weight matrices . [ 1 ] Arora et al.Stronger generalization bounds for deep nets via a compression approach . [ 2 ] Zhou and Feng . Understanding generalization and optimization performance of deep cnns . [ 3 ] Bartlett et al.Spectrally-normalized margin bounds for neural networks . [ 4 ] Neyshabur et al.A pac-bayesian approach to spectrally-normalized margin bounds for neural networks . [ 5 ] Huang et al.Orthogonal weight normalization : Solution to optimization over multiple dependent stiefel manifolds in deep neural networks . [ 6 ] Xie et al.All you need is beyond a good init : Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation ."}, "2": {"review_id": "SJzwvoCqF7-2", "review_text": "The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network as a parametric function in Depth * Width ^2 number of parameters, and then using standard L-2 covering and Dudley Integral. They extend this technique for CNNs, Resnets, Hyper-spherical Networks, etc and provide specialized bounds for each case. In the end, the authors provide comparisons to the existing bounds. Although intended, the bound in Theorem-1 depends on the number of parameters and hold only if m > d * (p^2) = number of parameters (from the last line of proof of Lemma 3, we need \\beta < \\alpha and thus m > h). Such bounds are already know in the literature (see Anthony and Bartlett, 1999). Adaptive (completely norm dependent, like Bartlett et. al. 2017) bounds will be better than explicit dimension dependent bounds. The comparison in Figure-1 which suggest their bound to be better is unfair because they are comparing their specialized bounds for CNN to generic bounds for standard feedforward networks. Same for comparison in Table-2. It was already established in Theorem 3.4 (Bartlett et al. 2017) that spectral norms are necessary for any generalization bounds for Deep Neural Networks, thus voiding the claims made in the paper (and discussion) about the importance of spectral norms. Typos / Errors : 1. Statement of Lemma 2 does not contain the spectral norms terms. 2. The third equation in Page 13 should be K <= \\sqrt{pD} max B_{d, 2}; and this changes the bound further. The paper introduces some new techniques on mathematical analysis of specialized neural networks. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for careful reading and helpful comments . We do have the same dependence with the VC type of bound in terms of the number of parameters , but we are considering essentially a smaller class of network functions ( than that considered in the VC type of bound ) with norm constraints . In the case when all norms are bounded ( by 1 ) or the loss function is bounded ( e.g. , the ramp loss < 1 ) , then our resulting bound can be smaller than the VC type of bound . We want to remark that we have updated our analysis for DNNs in terms of the rank of weight matrices , which may lead to tighter results when the weight matrices are low-rank , analogous to [ 1,2 ] . Our result for CNNs is a direct derivation from Theorem 1 ( for DNNs ) . This is one benefit of our analysis based on the Lipschitz property on the parameters , which allows us to directly reduce from pr ( r is the rank of W ) to k^2 in the CNNs case . For the other comparing results in Figure 1 , to the best of our knowledge , there is no such direct simplification from the general case to the CNNs case from their analysis . In other words , they have the same order of bounds for DNNs and CNNs . Note that the ranks are of the same order with width p in CNNs when the filters are linear independent , and we already simplified the norms in the other comparing results as in our result for CNNs in Table 2 . Moreover , we want to remark that Thm 3.4 in [ 1 ] is the lower bound of the ERC for the neural network function , which did not take the loss function into consideration . In other words , it does not conflict our case when the loss is bounded , which can lead to tighter dependence in terms of the loss function bound rather than the product of norm . Note that our result shown in Figure 1 is the worse case of Corollary 1 . In practice , the loss function for a trained network has a significantly smaller scale than \\prod B_ { d,2 } , which mean our result in Figure 1 is in fact significantly smaller than what is shown . This is another benefit of our analysis that allows us to only depend on the output of loss function , rather than the product of norms that the comparing results in Figure 1 can not avoid due to the nature of their analyses . We have added an extended experiment in Appendix A.1 for the bounded loss case . One can find that the generalization bound in the bounded loss case can be significantly smaller than the norm based result . We also added a discussion in Section 3.2 and Appendix A.1 to compare with existing output based bounds [ 3,4 ] and show that our output based result is tighter . In addition , from our observation on the trained network using real data , the dependence on the network sizes in other norm based results ( e.g. , the terms in Bound 1 and 2 in Figure 1 excluding the term of the product of norms ) are significantly larger than the number of parameters . Based on the updated Theorem 1 , [ 1,2 ] are always larger than our bound by a margin at least D , including the low-rank cases . The typos are corrected . [ 1 ] Bartlett et al.Spectrally-normalized margin bounds for neural networks . [ 2 ] Neyshabur et al.A pac-bayesian approach to spectrally-normalized margin bounds for neural networks . [ 3 ] Arora et al.Stronger generalization bounds for deep nets via a compression approach . [ 4 ] Zhou and Feng . Understanding generalization and optimization performance of deep cnns ."}}