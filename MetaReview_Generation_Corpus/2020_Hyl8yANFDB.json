{"year": "2020", "forum": "Hyl8yANFDB", "title": "Assessing Generalization in TD methods for Deep Reinforcement Learning", "decision": "Reject", "meta_review": "This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back-and-forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1's comment that it is overall a \"potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper,\" and feel the paper really needs a revision and another round of peer review before publication. ", "reviews": [{"review_id": "Hyl8yANFDB-0", "review_text": "Summary: This paper performs an empirical evaluation of generalization by TD methods with neural nets as function approximators. To quantify generalization, the paper considers the change in the loss function at similar states to the one where the update rule is being applied (where \u201csimilar\u201d is usually defined as nearby in time). It comes to a variety of conclusions including that TD(0) does not induce much generalization, that TD(0) does not induce as much generalization as supervised learning, and that the choice of optimizer and objective changes the behavior according to their generalization criteria in various ways. Decision: This paper should be rejected because (1) the motivation is unsubstantiated, (2) the main metrics used (\u201cgain\u201d at nearby states and difference since last visit) are of questionable importance, and (3) the conclusions are often vague and not informative. Main argument: Motivation: - The paper motivates the need for an evaluation of \u201cgradient update generalization\u201d by claiming that it is related to sample complexity and brittleness of RL algorithms. While I agree that this is plausible, there is nothing empirical or theoretical to support this claim in the paper or in the references. This is a significant problem since this assumed connection underlies everything in the paper. - Also, it is this sort of generalization that differentiates the function approximation setting (where there are no convergence guarantees for TD without strong assumptions) from the tabular setting (where there are convergence and sample complexity guarantees). Metrics: - The main metric used is TD gain on temporally nearby states. The TD gain is defined as the reduction in the squared TD error at some state s\u2019 when an update is applied at some other state s. Note this metric does not capture all update generalization, but only update generalization as it effects the TD error. - It is not evident, nor supported by the paper, that improvements in this metric at nearby states necessarily improve performance of the algorithm. This is especially true when there is a tradeoff between improvement at very nearby vs. somewhat nearby states, it is not clear which behavior is preferable (and this behavior seems to occur in experiments). As a result there is no clear way to use this metric to determine which algorithms are preferable. - The other metric used in the paper is the change in value function at a state since the last time that state was sampled from the buffer. It is also not clear whether this measurement is necessarily important for the same reasons as above. Conclusions: - In general the results are presented in plots that do not give clear implications and are difficult to read. I understand that we cannot expect completely clean results on such empirical questions, but the results would be potentially much more convincing and clear if the hypotheses were clearly stated and then one plot could summarize the result with respect to each hypothesis. For example, the gain plots are difficult to compare and interpret clearly and the memorization plots do not give such clear results (eg. 3a and 3d look visually fairly similar). Another example is the comparisons between optimizers, which are fine to have as a specific point in one section, but do not need to be in every plot. - The main result claimed in the paper is that there is little generalization from TD(0) when compared to supervised learning. This seems to be born out by the difference between figure 1a and 2a, but the difference in scale makes it a bit difficult. It is also not clear how to quantify this result or what the implications are. - The plots are averaged across all states over all of training and all environments, while this is somewhat rationalized in figure 11, I am worried that this may be covering some additional complexity/ambiguity in the results. - The result about TD(lambda) seems to be born out by figures 3e and f and 5, but is also unsurprising since the objective explicitly averages across temporally nearby states. Again it is not clear how this temporal consistency of updates should be interpreted in terms of the goals of the RL algorithms. At a higher level, the paper feels like a solid preliminary set of experiments rather than a paper organized around a clear motivating idea with clear hypotheses to test. The results would become more interesting if the metrics used could be connected back (either empirically or theoretically) to an objective. For example, does generalization in the sense defined in the paper give better performance at value estimation or return maximization? Can these results be quantified in a more direct way than the plots presented in the paper? Additional feedback: - The plots all have different scales which makes them difficult to compare - Using \u201cdistance\u201d to refer to the relative tilmestep of samples in the replay buffer is confusing (distance cannot be negative) - There are strange visual artifacts (horizontal lines) in Figures 3, 5, 10, 12, 13, and 14 - Sections 3.7 and 3.8 very briefly present results that seem to distract from the main thread of the paper - Using one network architecture across all experiments seems like it may have a significant impact on the results. I understand that the architecture chosen is standard and testing different architectures is time consuming, but making broad claims about the algorithms based on only one architecture is potentially dangerous. - I assume that the replay buffer is always being sampled uniformly, but I could not find this detail in the paper. ", "rating": "3: Weak Reject", "reply_text": "Dear reviewer , thank you for your feedback . The nuance of the phenomenon we are attempting to characterize does bring challenges , and requires simultaneously considering new questions , new experiments , and new metrics . We welcome your suggestions for developing and presenting findings in a clearer manner . > On ( 1 ) and ( 2 ) , motivation and importance of the metric A previous version of this paper had a scatter plot showing the link between the average magnitude of update gain and lifetime rewards of an agent . It appeared obvious to us that this link existed , but we will reintroduce this plot , which is empirical evidence of $ Y^ { near } $ being indicative of speed of learning ( correlation coefficient of r=0.433 , see Fig 15 in revised version ) . > On ( 3 ) , vague and uninformative conclusions We believe we have identified a behaviour which , at least to us and most RL researchers with whom we discussed this , is surprising and unexpected . We have attempted in many ways to understand how this behaviour arises , ruling out many hypotheses , unfortunately without much success so far . Knowing its cause would of course bring even greater insight , but we nonetheless strongly believe that publishing this unexpected result would be highly valuable for the community . We disagree that these results are uninformative . Most papers have a conclusion along the lines of \u201c do this \u201d or \u201c don \u2019 t do that \u201d ; our paper instead suggests that the Deep RL community may be unaware of a problem with large ramifications . It is our opinion that identifying directions of research is valuable . > On difference since last visit A common observation of memorizing deep neural networks is that they act like nearest-neighbour classifiers or even tabular lookups . This measure simply aims to disprove this for DQN by showing that , unlike for a DNN trained to memorize , states change value without being \u201c visited \u201d for a gradient update . So while the first results suggest that DQN could just be \u201c memorizing \u201d states , these results suggest that DQN doesn \u2019 t 100 % memorize . > On averaging plots over states and environments We tried many different plots to see if patterns would emerge . For example , we tried separating curves by how far the next reward was , or by distance to a terminal state . We were unable to see any robust pattern specific to some characterization of the MDP/trajectories . For environments , there is some amount of difference , usually depending on how dense rewards are , but these differences did not strongly affect the shape nor the magnitude of the update gain curves . As such , we simply averaged over all environments . > On plot scale We agree that this should be clearer in the text . Figure 1 and Figure 2 \u2019 s y axes are an order of magnitude apart ( Figure 2a \u2019 s y axis is 10x smaller , Figure 2b \u2019 s 100x ) . Showing them both in the same plot is feasible but it seemed to us more informative to separate the two . > On using \u201c distance \u201d You are right , we will replace \u201c distance \u201d with \u201c offset \u201d . > On visual artifacts We are sorry to hear this . We are unable to reproduce this problem , but will investigate on other browsers/OS . Here is a rasterized version of the pdf if you need it : https : //imgur.com/a/Qdv0JLt > Architectures As mentioned previously we will add a scatter plot showing the effect of varying the architecture and capacity . > Replay buffer Yes , the buffer is sampled uniformly . We also briefly experimented with Prioritized Experience Replay , which had slightly larger update gain but still in the same order of magnitude ."}, {"review_id": "Hyl8yANFDB-1", "review_text": "The manuscript is analyzing the \"generalization\" in TD(lambda) methods. It includes supervised learning from trajectories, on-policy imitation learning, and basic RL setting. Moreover, memoization performance has also been measured. Main conclusion is the fact that TD(0) performs very similar to tabular learning failing to transfer inductive biases between states. There are also additional surprising results about optimization. The empirical study is rather complete and significant. It raises interesting questions for the community and states some clear open problems. Results are conclusive and interesting. I believe it is a study which a practitioner using TD-based method should be aware of. Hence, I believe it is impactful. On the other hand, the manuscript has some significant issues which need to be resolved as follows: - One major issue is calling the analyzed metric \"generalization\". Generalization by definition requires something beyond what is seen. I believe the quantity defined in (9) is generalization. However, it can not be computed. Hence, calling its empirical version, \"generalization\" is confusing and a clear misuse of the term. I strongly urge authors to call the observed quantity something else. \"Empirical expected improvement\", \"gradient regularity\", \"expected gain\", etc. are some candidates come to my mind. - The optimization aspect is very interesting; however, it confuses the exposition significantly. I think it is better to give all results using adam first, and then showing the comparisons between adam and rmsprop later would be much more readable and easier to understand. - There are some clarity issues in the explanation of the experiments. Figure 3 is very confusing and it requires multiple reading to be understandable. A clearer visualization or a better explanation would improve the paper. - I am puzzled about why the authors did not use Q_MC in policy evaluation experiments (Section 3.3). I think it can very well be used in a straightforward manner. It would be an interesting addition to the experiments. Minor Nitpicks: - Memorization section is not clear. The discussion on N is very confusing as \"14.4% for N = 2 and of 16.1% for N = 50\" does not match any of \"10.5%, 22.7%, and 34.2%\" Can you give full error table in appendix? Overall, I like the study and suggest to accept it hoping authors can fix the issues I raise during rebuttal period.", "rating": "6: Weak Accept", "reply_text": "Dear reviewer , thank you for your feedback . We agree that there are many nuances to our work that make presentation challenging , and any suggestions to improve this are welcome . > On the metric name We acknowledge that calling ( 9 ) a measure of generalization conflicts somewhat with existing notions . We gave a lot of thought to what would be an appropriate name for this phenomenon , in the end we settled for the lengthy \u201c gradient update generalization \u201d . \u201c Gradient \u201d ecompasses the parameter sharing aspect of this , \u201c update \u201d refers to the measure being related to a single discrete update , and \u201c generalization \u201d refers to the desire to improve on more than some given samples ( which is related to generalization in the classical sense ) . Would something like \u201c gradient update improvement \u201d or \u201c gradient update gain \u201d make more sense ? > On optimization aspects We agree that the exposition of the optimizers adds complexity . The current presentation is meant to be more compact , and to highlight the consistent differences we found between optimizers that are also consistent with the findings of other papers . > On Figure 3 We will revise the caption of this Figure as well as the references to it within the text . Here is the current new caption : \u201c Figure 3 : Policy evaluation on Atari : evolution of the distribution of $ \\Delta $ , the difference since last visit , during training . In ( a ) the DNN is forced to memorize , as such the density of $ \\Delta $ is concentrated around 0 ( thin red/white band ) . In ( b-c ) , Q-Learning and Sarsa , the density is much less peaked at 0 ( larger yellow/green bands ) as the DNN learns about states without visiting them . In ( d ) the DNN learns quickly presumably without memorizing ( the distribution of $ \\Delta $ is more spread out and not as concentrated around 0 , seen by the larger yellow/green band ) , as it is trained on Monte-Carlo returns , and quickly converges as can be seen by the high density of positive $ \\Delta $ s early . In ( e , f ) we see the effect of using $ \\lambda $ returns ( see appendix A.6 for all values of $ \\lambda $ ) . \u201d > On Q_MC Unless Q_MC refers to something else , we already have this experiment . In section 2.2 we consider Q_MC to be a \u201c supervised \u201d task , using eq ( 10 ) , and present its results in Figure 1a which we simply refer to as MC . Should we replot the MC curve for 1a in Figure 2 for completeness ? ( both experiments are using the same data ) > Memorization section In retrospect the numbers indeed did seem unlikely . We investigated and found a typo in the code . Here is the full table ( added in appendix , Table 1 , page 13 ) : D / N 2 10 50 10k 5.7 8.7 8.5 100k 38.8 45.6 56.5 500k 44.7 79.3 85.8 ( error % )"}, {"review_id": "Hyl8yANFDB-2", "review_text": "This paper studies the generalization property of DRL. Fundamentally, this is a very interesting problem. The authors experimentally analyze this issue through the lens of memorization, and showed that it can be observed directly during training. This paper presents the measure of gradient update generalization, best understood as a side-effect of neural networks sharing parameters over the entire input space. This paper is very written, and well organized. The experiments are quite solid. However I may be not capable in judging the novelties and contributions of this paper, since I did not conduct research on this topic. ", "rating": "6: Weak Accept", "reply_text": "Dear reviewer , we very much agree that this is an interesting problem . If you have any further questions we will be happy to answer them ."}], "0": {"review_id": "Hyl8yANFDB-0", "review_text": "Summary: This paper performs an empirical evaluation of generalization by TD methods with neural nets as function approximators. To quantify generalization, the paper considers the change in the loss function at similar states to the one where the update rule is being applied (where \u201csimilar\u201d is usually defined as nearby in time). It comes to a variety of conclusions including that TD(0) does not induce much generalization, that TD(0) does not induce as much generalization as supervised learning, and that the choice of optimizer and objective changes the behavior according to their generalization criteria in various ways. Decision: This paper should be rejected because (1) the motivation is unsubstantiated, (2) the main metrics used (\u201cgain\u201d at nearby states and difference since last visit) are of questionable importance, and (3) the conclusions are often vague and not informative. Main argument: Motivation: - The paper motivates the need for an evaluation of \u201cgradient update generalization\u201d by claiming that it is related to sample complexity and brittleness of RL algorithms. While I agree that this is plausible, there is nothing empirical or theoretical to support this claim in the paper or in the references. This is a significant problem since this assumed connection underlies everything in the paper. - Also, it is this sort of generalization that differentiates the function approximation setting (where there are no convergence guarantees for TD without strong assumptions) from the tabular setting (where there are convergence and sample complexity guarantees). Metrics: - The main metric used is TD gain on temporally nearby states. The TD gain is defined as the reduction in the squared TD error at some state s\u2019 when an update is applied at some other state s. Note this metric does not capture all update generalization, but only update generalization as it effects the TD error. - It is not evident, nor supported by the paper, that improvements in this metric at nearby states necessarily improve performance of the algorithm. This is especially true when there is a tradeoff between improvement at very nearby vs. somewhat nearby states, it is not clear which behavior is preferable (and this behavior seems to occur in experiments). As a result there is no clear way to use this metric to determine which algorithms are preferable. - The other metric used in the paper is the change in value function at a state since the last time that state was sampled from the buffer. It is also not clear whether this measurement is necessarily important for the same reasons as above. Conclusions: - In general the results are presented in plots that do not give clear implications and are difficult to read. I understand that we cannot expect completely clean results on such empirical questions, but the results would be potentially much more convincing and clear if the hypotheses were clearly stated and then one plot could summarize the result with respect to each hypothesis. For example, the gain plots are difficult to compare and interpret clearly and the memorization plots do not give such clear results (eg. 3a and 3d look visually fairly similar). Another example is the comparisons between optimizers, which are fine to have as a specific point in one section, but do not need to be in every plot. - The main result claimed in the paper is that there is little generalization from TD(0) when compared to supervised learning. This seems to be born out by the difference between figure 1a and 2a, but the difference in scale makes it a bit difficult. It is also not clear how to quantify this result or what the implications are. - The plots are averaged across all states over all of training and all environments, while this is somewhat rationalized in figure 11, I am worried that this may be covering some additional complexity/ambiguity in the results. - The result about TD(lambda) seems to be born out by figures 3e and f and 5, but is also unsurprising since the objective explicitly averages across temporally nearby states. Again it is not clear how this temporal consistency of updates should be interpreted in terms of the goals of the RL algorithms. At a higher level, the paper feels like a solid preliminary set of experiments rather than a paper organized around a clear motivating idea with clear hypotheses to test. The results would become more interesting if the metrics used could be connected back (either empirically or theoretically) to an objective. For example, does generalization in the sense defined in the paper give better performance at value estimation or return maximization? Can these results be quantified in a more direct way than the plots presented in the paper? Additional feedback: - The plots all have different scales which makes them difficult to compare - Using \u201cdistance\u201d to refer to the relative tilmestep of samples in the replay buffer is confusing (distance cannot be negative) - There are strange visual artifacts (horizontal lines) in Figures 3, 5, 10, 12, 13, and 14 - Sections 3.7 and 3.8 very briefly present results that seem to distract from the main thread of the paper - Using one network architecture across all experiments seems like it may have a significant impact on the results. I understand that the architecture chosen is standard and testing different architectures is time consuming, but making broad claims about the algorithms based on only one architecture is potentially dangerous. - I assume that the replay buffer is always being sampled uniformly, but I could not find this detail in the paper. ", "rating": "3: Weak Reject", "reply_text": "Dear reviewer , thank you for your feedback . The nuance of the phenomenon we are attempting to characterize does bring challenges , and requires simultaneously considering new questions , new experiments , and new metrics . We welcome your suggestions for developing and presenting findings in a clearer manner . > On ( 1 ) and ( 2 ) , motivation and importance of the metric A previous version of this paper had a scatter plot showing the link between the average magnitude of update gain and lifetime rewards of an agent . It appeared obvious to us that this link existed , but we will reintroduce this plot , which is empirical evidence of $ Y^ { near } $ being indicative of speed of learning ( correlation coefficient of r=0.433 , see Fig 15 in revised version ) . > On ( 3 ) , vague and uninformative conclusions We believe we have identified a behaviour which , at least to us and most RL researchers with whom we discussed this , is surprising and unexpected . We have attempted in many ways to understand how this behaviour arises , ruling out many hypotheses , unfortunately without much success so far . Knowing its cause would of course bring even greater insight , but we nonetheless strongly believe that publishing this unexpected result would be highly valuable for the community . We disagree that these results are uninformative . Most papers have a conclusion along the lines of \u201c do this \u201d or \u201c don \u2019 t do that \u201d ; our paper instead suggests that the Deep RL community may be unaware of a problem with large ramifications . It is our opinion that identifying directions of research is valuable . > On difference since last visit A common observation of memorizing deep neural networks is that they act like nearest-neighbour classifiers or even tabular lookups . This measure simply aims to disprove this for DQN by showing that , unlike for a DNN trained to memorize , states change value without being \u201c visited \u201d for a gradient update . So while the first results suggest that DQN could just be \u201c memorizing \u201d states , these results suggest that DQN doesn \u2019 t 100 % memorize . > On averaging plots over states and environments We tried many different plots to see if patterns would emerge . For example , we tried separating curves by how far the next reward was , or by distance to a terminal state . We were unable to see any robust pattern specific to some characterization of the MDP/trajectories . For environments , there is some amount of difference , usually depending on how dense rewards are , but these differences did not strongly affect the shape nor the magnitude of the update gain curves . As such , we simply averaged over all environments . > On plot scale We agree that this should be clearer in the text . Figure 1 and Figure 2 \u2019 s y axes are an order of magnitude apart ( Figure 2a \u2019 s y axis is 10x smaller , Figure 2b \u2019 s 100x ) . Showing them both in the same plot is feasible but it seemed to us more informative to separate the two . > On using \u201c distance \u201d You are right , we will replace \u201c distance \u201d with \u201c offset \u201d . > On visual artifacts We are sorry to hear this . We are unable to reproduce this problem , but will investigate on other browsers/OS . Here is a rasterized version of the pdf if you need it : https : //imgur.com/a/Qdv0JLt > Architectures As mentioned previously we will add a scatter plot showing the effect of varying the architecture and capacity . > Replay buffer Yes , the buffer is sampled uniformly . We also briefly experimented with Prioritized Experience Replay , which had slightly larger update gain but still in the same order of magnitude ."}, "1": {"review_id": "Hyl8yANFDB-1", "review_text": "The manuscript is analyzing the \"generalization\" in TD(lambda) methods. It includes supervised learning from trajectories, on-policy imitation learning, and basic RL setting. Moreover, memoization performance has also been measured. Main conclusion is the fact that TD(0) performs very similar to tabular learning failing to transfer inductive biases between states. There are also additional surprising results about optimization. The empirical study is rather complete and significant. It raises interesting questions for the community and states some clear open problems. Results are conclusive and interesting. I believe it is a study which a practitioner using TD-based method should be aware of. Hence, I believe it is impactful. On the other hand, the manuscript has some significant issues which need to be resolved as follows: - One major issue is calling the analyzed metric \"generalization\". Generalization by definition requires something beyond what is seen. I believe the quantity defined in (9) is generalization. However, it can not be computed. Hence, calling its empirical version, \"generalization\" is confusing and a clear misuse of the term. I strongly urge authors to call the observed quantity something else. \"Empirical expected improvement\", \"gradient regularity\", \"expected gain\", etc. are some candidates come to my mind. - The optimization aspect is very interesting; however, it confuses the exposition significantly. I think it is better to give all results using adam first, and then showing the comparisons between adam and rmsprop later would be much more readable and easier to understand. - There are some clarity issues in the explanation of the experiments. Figure 3 is very confusing and it requires multiple reading to be understandable. A clearer visualization or a better explanation would improve the paper. - I am puzzled about why the authors did not use Q_MC in policy evaluation experiments (Section 3.3). I think it can very well be used in a straightforward manner. It would be an interesting addition to the experiments. Minor Nitpicks: - Memorization section is not clear. The discussion on N is very confusing as \"14.4% for N = 2 and of 16.1% for N = 50\" does not match any of \"10.5%, 22.7%, and 34.2%\" Can you give full error table in appendix? Overall, I like the study and suggest to accept it hoping authors can fix the issues I raise during rebuttal period.", "rating": "6: Weak Accept", "reply_text": "Dear reviewer , thank you for your feedback . We agree that there are many nuances to our work that make presentation challenging , and any suggestions to improve this are welcome . > On the metric name We acknowledge that calling ( 9 ) a measure of generalization conflicts somewhat with existing notions . We gave a lot of thought to what would be an appropriate name for this phenomenon , in the end we settled for the lengthy \u201c gradient update generalization \u201d . \u201c Gradient \u201d ecompasses the parameter sharing aspect of this , \u201c update \u201d refers to the measure being related to a single discrete update , and \u201c generalization \u201d refers to the desire to improve on more than some given samples ( which is related to generalization in the classical sense ) . Would something like \u201c gradient update improvement \u201d or \u201c gradient update gain \u201d make more sense ? > On optimization aspects We agree that the exposition of the optimizers adds complexity . The current presentation is meant to be more compact , and to highlight the consistent differences we found between optimizers that are also consistent with the findings of other papers . > On Figure 3 We will revise the caption of this Figure as well as the references to it within the text . Here is the current new caption : \u201c Figure 3 : Policy evaluation on Atari : evolution of the distribution of $ \\Delta $ , the difference since last visit , during training . In ( a ) the DNN is forced to memorize , as such the density of $ \\Delta $ is concentrated around 0 ( thin red/white band ) . In ( b-c ) , Q-Learning and Sarsa , the density is much less peaked at 0 ( larger yellow/green bands ) as the DNN learns about states without visiting them . In ( d ) the DNN learns quickly presumably without memorizing ( the distribution of $ \\Delta $ is more spread out and not as concentrated around 0 , seen by the larger yellow/green band ) , as it is trained on Monte-Carlo returns , and quickly converges as can be seen by the high density of positive $ \\Delta $ s early . In ( e , f ) we see the effect of using $ \\lambda $ returns ( see appendix A.6 for all values of $ \\lambda $ ) . \u201d > On Q_MC Unless Q_MC refers to something else , we already have this experiment . In section 2.2 we consider Q_MC to be a \u201c supervised \u201d task , using eq ( 10 ) , and present its results in Figure 1a which we simply refer to as MC . Should we replot the MC curve for 1a in Figure 2 for completeness ? ( both experiments are using the same data ) > Memorization section In retrospect the numbers indeed did seem unlikely . We investigated and found a typo in the code . Here is the full table ( added in appendix , Table 1 , page 13 ) : D / N 2 10 50 10k 5.7 8.7 8.5 100k 38.8 45.6 56.5 500k 44.7 79.3 85.8 ( error % )"}, "2": {"review_id": "Hyl8yANFDB-2", "review_text": "This paper studies the generalization property of DRL. Fundamentally, this is a very interesting problem. The authors experimentally analyze this issue through the lens of memorization, and showed that it can be observed directly during training. This paper presents the measure of gradient update generalization, best understood as a side-effect of neural networks sharing parameters over the entire input space. This paper is very written, and well organized. The experiments are quite solid. However I may be not capable in judging the novelties and contributions of this paper, since I did not conduct research on this topic. ", "rating": "6: Weak Accept", "reply_text": "Dear reviewer , we very much agree that this is an interesting problem . If you have any further questions we will be happy to answer them ."}}