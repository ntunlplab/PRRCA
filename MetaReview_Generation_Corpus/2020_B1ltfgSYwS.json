{"year": "2020", "forum": "B1ltfgSYwS", "title": "Few-Shot One-Class Classification via Meta-Learning", "decision": "Reject", "meta_review": "The authors present a combination of few-shot learning with one-class classification model of problems. The authors use the existing MAML algorithm and build upon it to present a learning algorithm for the problem. As pointed out by the reviewers, the technical contributions of the paper are quite minimal and after the author response period the reviewers have not changed their minds. However, the authors have significantly changed the paper from its initial submission and as of now it needs to be reviewed again. I recommend authors to resubmit their paper to another conference. As of now, I recommend rejection.", "reviews": [{"review_id": "B1ltfgSYwS-0", "review_text": " This paper tackles an interesting problem, one-class classification or anomaly detection, using a meta-learning approach. The main contribution is to introduce a parameter such that the inner-loop of the meta-learning algorithm better reflects the imbalance which occurs during meta-testing. Results are shown comparing a few simple baselines to both MAML and the modified variant, on a few datasets such as image-based ones (MNIST, miniImageNet), a synthetic dataset, and a real-world time-series example from CNC milling machines. Overall, the paper presents an interesting problem and awareness that meta-learning might be general enough to solve it well, but provides no real novelty in the approach. The datasets and comparison to other state of art methods (including both other anomaly detection methods and out of distribution methods) is lacking. I suggest the authors perform more rigorous experimentation and focus the paper to be a paper about an understudied problem with rigorous experiments/findings, or improve their method beond the small modification made. Due to these weaknesses, I vote for rejection at this time. Detailed comments are below. Strengths - The problem is interesting and under-studied in the context of deep learning and transferable methods from similar ML problems (e.g. few-shot learning) - The method is simple and adapts a state of art in few-shot learning (meta-learning, and specifically MAML) Weaknesses - While I enjoyed reading the paper since it tackles an under-explored problem, it is hard to justify publishing the method/approach at a top machine learning conference. Changing the balance in meta-learning is a relatively obvious modification that one would do to better reflect the problem; I don't think it results in general scientific/ML principles that can be used elsewhere. - The relationship to out-of-distribution detection (which some of the experiments, e.g. Multi-task MNIST and miniImagenet essentially test) is not discussed or compared to. How are anomalies defined and is it really different than just being out-of-distribution? - The datasets are limited. The MNIST dataset seems to choose a fixed two specific categories for meta-validation and meta-testing, as opposed to doing cross-validation. Results on just one meta-testing seems limited in this case with just one class. In terms of time-series, anomaly detection has been studied for a long time; is there a reason that the authors create a new synthetic dataset? For the milling example, how were anomalies provoked? - The baselines do not represent any state of art anomaly detection (e.g. density based, isolation forests, etc.) nor out of distribution detection; the latter especially would likely do extremely well for the simple image examples. - There is no analysis of what the difference is in representation (initialization) learning due to the differences between the OCC and FS setup. What are the characteristics of the improved initialization? One minor comment not reflecting the decision: - Exposition: Define the one-class classification problem; it's not common so it would be good to define in the abstract, or mention anomaly detection which is a better-known term. ", "rating": "1: Reject", "reply_text": "Weakness 5 : '' There is no analysis of what the difference is in representation ( initialization ) learning due to the differences between the OCC and FS setup . What are the characteristics of the improved initialization ? '' We adress the FS-OCC problem in our present work . We assume that your question is about the difference between the initialization yielded by FS-OCC meta-training ( OC-MAML ) and FS-class-balanced meta-training ( MAML ) . We cover this concern directly in our revised version of the paper ( Section 2.3.2 ) : By analyzing the approximated loss gradients used for the MAML and OC-MAML updates , we come to the following finding : For a given task , OC-MAML optimizes for increasing the inner product of the gradients computed on different minibatches with different class-imbalance rates , namely minibatches containing data from only one class and a class-balanced minibatch ( meta-update ) . If the inner product of the gradients computed on two different minibatches is positive , taking one gradient step using one minibatch leads to an increase in performance on the other minibatch . Consequently , OC-MAML optimizes for a parameter initialization from which taking one ( or few ) gradient step ( s ) with minibatch ( es ) including only normal class data results in a performance increase on class-balanced data . In contrast , MAML optimizes for a parameter initialization that requires class-balanced minibatches to yield the same effect . When adapting to OCC tasks , however , only examples from one class are available . We conclude , therefore , that using minibatches with different class-imbalance rates for meta-training , as done in OC-MAML , yields parameter initializations that are more suitable for adapting to OCC tasks . We also find that the second-order derivatives are essential to do so , which explains why OC-FOMAML ( its first-order approximation ) fails to adapt to FS-OCC tasks . Please refer to section 2.3.2 in the revised paper version for a more detailed theoretical analysis of the gradients of the different meta-learning algorithms , in the OCC case . In the answer to Reviewer 1 , we further discuss how batch normalization after the last feature-producing layer can be used to partially counteract this shortcoming . However , despite this modification , the first order meta-learning methods are still outperformed by OC-MAML by a significanat margin . -- -- Minor comment : `` Exposition : Define the one-class classification problem ; it 's not common so it would be good to define in the abstract , or mention anomaly detection which is a better-known term . '' Thank you for the comment . We did this in the revised version of the paper . -- -- Thank you again for taking the time to thoroughly read our paper . We noticed that you judged our work with the lowest score . This is really unfortunate since we believe that the topic of few/one-shot one-class classification really deserves a greater attention by the research community due to its wide applicability in many areas where data is naturally scarce and has an extreme class-imbalance . As we address most of your concerns and additional concerns of the other reviewers which clearly improved our paper and our contribution we would really appreciate if you could spare additional time and reevaluate our revised version of the paper and its contributions . Also , if you have further remarks and concerns please feel free to let us know so we can further improve our research contribution ."}, {"review_id": "B1ltfgSYwS-1", "review_text": "In this paper, the authors have investigated the few shot one classification problem. They have presented a meta-learning approach that requires only few data examples from only one class to adapt to unseen tasks. The proposed method builds upon the model-agnostic meta-learning (MAML) algorithm. I think the topic itself is interesting and I have the following concerns. (1) The first is about the real requirement of this learning scenario. Although the authors have pointed out some real applications, I think they have been introduced separated. In other words, since this setting is the combination of two previous areas, i.e., one class classification and few-shot learning, I fell that the authors have introduced it by just a combination. What are the unique challenges of this problem? I think these problems should be clarified at first. (2) The second one is the algorithms itself. Although I have not checked the details, I fell that the authors have prepared this paper in a rough way. The authors have only described the method, without deep analyses answering the question why. For example, the method seems heuristic, without theoretical analysis. In summary, I think this paper likes a technical report, not a research paper. (3) Although I can catch the main meaning of this paper, it seems that the writing style is not so fluently. I suggested the authors to recognize the presentation. ", "rating": "3: Weak Reject", "reply_text": "Concern 2.1 : '' The second one is the algorithms itself . Although I have not checked the details , I fell that the authors have prepared this paper in a rough way . '' We would be grateful for concrete examples of sentences or sections , where you felt that the paper was written in a `` rough '' way , so that we can improve them . -- -- Concern 2.2 : '' The authors have only described the method , without deep analyses answering the question why . For example , the method seems heuristic , without theoretical analysis . '' We added a section ( section 2.3.2 ) in the revised version of the paper , where we give a theoretical explanation of why OC-MAML works . In the following , we briefly summarize our findings . Furthermore , we conducted additional experiments to validate our theoretical analysis empirically . For the results of these experiments please see Table 2 in the revised paper version . By analyzing the approximated loss gradients used in the MAML and OC-MAML updates , we come to the following finding : For a given task , OC-MAML optimizes for increasing the inner product of the gradients computed on different minibatches with different class-imbalance rates , namely minibatches containing data from only one class and a class-balanced minibatch ( meta-update ) . If the inner product of the gradients computed on two different minibatches is positive , taking one gradient step using one minibatch leads to an increase in performance on the other minibatch . Consequently , OC-MAML optimizes for a parameter initialization from which taking one ( or few ) gradient step ( s ) with minibatch ( es ) including only normal class data results in a performance increase on class-balanced data . In contrast , MAML optimizes for a parameter initialization that requires class-balanced minibatches to yield the same effect . When adapting to OCC tasks , however , only examples from one class are available . We conclude , therefore , that using minibatches with different class-imbalance rates for meta-training , as done in OC-MAML , yields parameter initializations that are more suitable for adapting to OCC tasks . We also find that the second-order derivative term is essential to do so , which explains why OC-FOMAML ( its first-order approximation ) fails to adapt to FS-OCC tasks . Please refer to section 2.3.2 of the revised paper version for a more detailed theoretical analysis of the gradients of the different meta-learning algorithms , in the OCC case . -- -- Concern 2.3 : '' In summary , I think this paper likes a technical report , not a research paper . '' Our work emerged from a practical and technical situation , where the few-shot one-class regime is common , namely industrial manufacturing . However , we developed a method that is applicable in multiple data domains , i.e.time-series ( in particular , sensor data ) and images , and therefore can be adopted beyond the initial technical problem . We aim for our approach to be a first and strong baseline for the , as recognized by both other reviewers , relevant and under-studied research problem of few-shot one-class classification in general , i.e.not to the specific case of few-shot anomaly detection on sensor data . We believe that our approach can be of great value to the research community that will explore this challenging and important problem further in the future . We hope that our extensions of the paper give it a stronger research character . -- -- Concern3 : `` Although I can catch the main meaning of this paper , it seems that the writing style is not so fluently . I suggested the authors to recognize the presentation . '' As mentioned above , we would be grateful if you could point out some concrete examples of sentences or sections , of which we should improve the writing style . Could you also please elaborate on the last sentence ? Since you mentioned that you have not thoroughly read our paper , we would really appreciate if you could spare the time to reevaluate our revised paper version . Please feel free to raise any further concerns or add some additional comments . As we believe that we have addressed most of your concerns we would also be very grateful if you could reconsider the rather low scoring for our research contribution ."}, {"review_id": "B1ltfgSYwS-2", "review_text": "One of promising approach to tackle the few-shot problems is to use meta-learning so that the learner can quickly generalize to an unseen task. One-class classification requires only a set of positive examples to discriminate negative examples from positive examples. The current paper addresses a method of meta-training one-class classifiers in the MAML framework when only a handful of positive examples are available. ---Strength--- - Few-shot one-class classification is a timely subject, which has not be studied yet. - Meta-training one-class classifiers in the MAML framework seems to be sound. ---Weakness--- - MAML is quite a general meta-training framework, which can be used when parameterized base-learners are updated using gradient methods. Thus, when parameterized models for one-class classification are used, it is rather easy to meta-train one-class classifiers in the MAML framework. - Regarding episodic training, in contrast to few-shot classification problems, support sets in episodes have similar positive examples. Thus, fine-tuning baseline method could work well, even without using MAML. Please compare it with the fine-tuning method. ---Comments--- - I assume that the query set in each episode include negative examples, while support sets have only positive examples. Right? What is the value of c (class imbalance rate) in the query set? - Wouldn't it be better to focus on experiments with c=0% since one-class classification requires the training with only positive examples? - What was the baseline one-class classifier? One-class SVM? - It was mentioned that CLEAR was an earlier work. Then, the empirical comparison with CLEAR should be included when image data is considered.", "rating": "3: Weak Reject", "reply_text": "Comment 1 : `` I assume that the query set in each episode include negative examples , while support sets have only positive examples . Right ? What is the value of c ( class imbalance rate ) in the query set ? '' Yes , support sets include only positive examples , i.e.belonging to the majority ( non anomalous ) class . Query sets are class-balanced ( c=50 % ) in order to evaluate the model on both classes in the outer loop after adaptation , i.e.the inner loop updates . This way , we optimize the performance of the model on both classes equally after adaptation using only one class . -- -- Comment 2 : `` Would n't it be better to focus on experiments with c=0 % since one-class classification requires the training with only positive examples ? '' We showed the results of c=50 % in order to give the reader some reference numbers from the class-balanced case . For example , these results answer the question `` How much accuracy gain would having data from both classes yield '' . As for the experiments with c=1 % , we conducted them to show that our approach is not only applicable in the extreme case of one-class classification , but also in the general class-imabalance case . We will focus more on the c=0 % case in the revised version of the paper . -- -- Comment 3 : '' What was the baseline one-class classifier ? One-class SVM ? '' As we mentioned in the paper , we did not compare to the classical one-class classification approaches , since they require high amounts of data and are therefore not applicable in the few-shot regime that we address . It should be noted that OC-SVM and other shallow approaches sometimes completely fail in one-class classification even when high amounts of data are available . For example in [ 3 ] , OC-SVM yields a AUC-ROC of 50 % on some of the CIFAR-10 classes , when 5000 datapoints from this class are used for learning . Upon your request and the request of Reviewer 2 , we conducted additional experiments using the classical OCC approaches OC-SVM and Isolation Forest ( IF ) on the few-shot one-class test tasks . Hereby , we apply PCA to the data where we choose the minimum number of eigenvectors sothat at least 95 % of the variance is conserved , as done in [ 3 ] . For OC-SVM , we additionally tune the inverse length scale ( gamma ) by using 10 % of the test set , as done in [ 3 ] . This gives OC-SVM a supervised advantage , compared to the other baselines . For a fairer comparison , where these methods also benefit from the data available in the meta-training tasks , we additionally conducted experiments on the embeddings inferred by the feature extractors of both the `` Finetune '' baseline and the Multi-Task-Learning ( MTL ) baseline . The results can be seen in Table 1 of the the revised paper version . As expected , the baselines fail to generalize to unseen examples when only few datapoints from the normal class are available . The only exception is the MT-MNIST dataset , where the shallow baselines trained on the extracted embeddings yield good performance K=10 examples of the normal class are available . We explain this by the fact that in the MT-MNIST dataset , the feature extractor models ( MTL and `` Finetune '' ) are exposed to most of the anomalies of the test task ( 8 out of the 9 digit classes present in the test task ) during training . Hence , useful embeddings are extracted . We note that , even on the MT-MNIST dataset , OC-MAML still outperfoms all baselines by a significant margin . -- -- Comment 4 : '' It was mentioned that CLEAR was an earlier work . Then , the empirical comparison with CLEAR should be included when image data is considered . '' CLEAR [ 4 ] uses a feature extractor trained on ImageNet . Comparing to it on the MiniImageNet dataset would not be fair , as the feature extractor was trained on the test classes . The other datasets that we tested our approach on , MNIST and Omniglot , are composed of grey-scale images . We will not be able to run experiments on datasets that were tested in the CLEAR due to the short rebuttal time . We would like , however , to point out that OC-MAML is data-type-agnostic and was successfully validated on time-series data , to which CLEAR is not applicable . As we have addressed most of your concerns which clearly improved the quality of the paper , we would really appreciate if you could spare the time to reevaluate our revised paper and adapt the current score accordingly . Please feel free to further comment our additions and raise further concerns ."}], "0": {"review_id": "B1ltfgSYwS-0", "review_text": " This paper tackles an interesting problem, one-class classification or anomaly detection, using a meta-learning approach. The main contribution is to introduce a parameter such that the inner-loop of the meta-learning algorithm better reflects the imbalance which occurs during meta-testing. Results are shown comparing a few simple baselines to both MAML and the modified variant, on a few datasets such as image-based ones (MNIST, miniImageNet), a synthetic dataset, and a real-world time-series example from CNC milling machines. Overall, the paper presents an interesting problem and awareness that meta-learning might be general enough to solve it well, but provides no real novelty in the approach. The datasets and comparison to other state of art methods (including both other anomaly detection methods and out of distribution methods) is lacking. I suggest the authors perform more rigorous experimentation and focus the paper to be a paper about an understudied problem with rigorous experiments/findings, or improve their method beond the small modification made. Due to these weaknesses, I vote for rejection at this time. Detailed comments are below. Strengths - The problem is interesting and under-studied in the context of deep learning and transferable methods from similar ML problems (e.g. few-shot learning) - The method is simple and adapts a state of art in few-shot learning (meta-learning, and specifically MAML) Weaknesses - While I enjoyed reading the paper since it tackles an under-explored problem, it is hard to justify publishing the method/approach at a top machine learning conference. Changing the balance in meta-learning is a relatively obvious modification that one would do to better reflect the problem; I don't think it results in general scientific/ML principles that can be used elsewhere. - The relationship to out-of-distribution detection (which some of the experiments, e.g. Multi-task MNIST and miniImagenet essentially test) is not discussed or compared to. How are anomalies defined and is it really different than just being out-of-distribution? - The datasets are limited. The MNIST dataset seems to choose a fixed two specific categories for meta-validation and meta-testing, as opposed to doing cross-validation. Results on just one meta-testing seems limited in this case with just one class. In terms of time-series, anomaly detection has been studied for a long time; is there a reason that the authors create a new synthetic dataset? For the milling example, how were anomalies provoked? - The baselines do not represent any state of art anomaly detection (e.g. density based, isolation forests, etc.) nor out of distribution detection; the latter especially would likely do extremely well for the simple image examples. - There is no analysis of what the difference is in representation (initialization) learning due to the differences between the OCC and FS setup. What are the characteristics of the improved initialization? One minor comment not reflecting the decision: - Exposition: Define the one-class classification problem; it's not common so it would be good to define in the abstract, or mention anomaly detection which is a better-known term. ", "rating": "1: Reject", "reply_text": "Weakness 5 : '' There is no analysis of what the difference is in representation ( initialization ) learning due to the differences between the OCC and FS setup . What are the characteristics of the improved initialization ? '' We adress the FS-OCC problem in our present work . We assume that your question is about the difference between the initialization yielded by FS-OCC meta-training ( OC-MAML ) and FS-class-balanced meta-training ( MAML ) . We cover this concern directly in our revised version of the paper ( Section 2.3.2 ) : By analyzing the approximated loss gradients used for the MAML and OC-MAML updates , we come to the following finding : For a given task , OC-MAML optimizes for increasing the inner product of the gradients computed on different minibatches with different class-imbalance rates , namely minibatches containing data from only one class and a class-balanced minibatch ( meta-update ) . If the inner product of the gradients computed on two different minibatches is positive , taking one gradient step using one minibatch leads to an increase in performance on the other minibatch . Consequently , OC-MAML optimizes for a parameter initialization from which taking one ( or few ) gradient step ( s ) with minibatch ( es ) including only normal class data results in a performance increase on class-balanced data . In contrast , MAML optimizes for a parameter initialization that requires class-balanced minibatches to yield the same effect . When adapting to OCC tasks , however , only examples from one class are available . We conclude , therefore , that using minibatches with different class-imbalance rates for meta-training , as done in OC-MAML , yields parameter initializations that are more suitable for adapting to OCC tasks . We also find that the second-order derivatives are essential to do so , which explains why OC-FOMAML ( its first-order approximation ) fails to adapt to FS-OCC tasks . Please refer to section 2.3.2 in the revised paper version for a more detailed theoretical analysis of the gradients of the different meta-learning algorithms , in the OCC case . In the answer to Reviewer 1 , we further discuss how batch normalization after the last feature-producing layer can be used to partially counteract this shortcoming . However , despite this modification , the first order meta-learning methods are still outperformed by OC-MAML by a significanat margin . -- -- Minor comment : `` Exposition : Define the one-class classification problem ; it 's not common so it would be good to define in the abstract , or mention anomaly detection which is a better-known term . '' Thank you for the comment . We did this in the revised version of the paper . -- -- Thank you again for taking the time to thoroughly read our paper . We noticed that you judged our work with the lowest score . This is really unfortunate since we believe that the topic of few/one-shot one-class classification really deserves a greater attention by the research community due to its wide applicability in many areas where data is naturally scarce and has an extreme class-imbalance . As we address most of your concerns and additional concerns of the other reviewers which clearly improved our paper and our contribution we would really appreciate if you could spare additional time and reevaluate our revised version of the paper and its contributions . Also , if you have further remarks and concerns please feel free to let us know so we can further improve our research contribution ."}, "1": {"review_id": "B1ltfgSYwS-1", "review_text": "In this paper, the authors have investigated the few shot one classification problem. They have presented a meta-learning approach that requires only few data examples from only one class to adapt to unseen tasks. The proposed method builds upon the model-agnostic meta-learning (MAML) algorithm. I think the topic itself is interesting and I have the following concerns. (1) The first is about the real requirement of this learning scenario. Although the authors have pointed out some real applications, I think they have been introduced separated. In other words, since this setting is the combination of two previous areas, i.e., one class classification and few-shot learning, I fell that the authors have introduced it by just a combination. What are the unique challenges of this problem? I think these problems should be clarified at first. (2) The second one is the algorithms itself. Although I have not checked the details, I fell that the authors have prepared this paper in a rough way. The authors have only described the method, without deep analyses answering the question why. For example, the method seems heuristic, without theoretical analysis. In summary, I think this paper likes a technical report, not a research paper. (3) Although I can catch the main meaning of this paper, it seems that the writing style is not so fluently. I suggested the authors to recognize the presentation. ", "rating": "3: Weak Reject", "reply_text": "Concern 2.1 : '' The second one is the algorithms itself . Although I have not checked the details , I fell that the authors have prepared this paper in a rough way . '' We would be grateful for concrete examples of sentences or sections , where you felt that the paper was written in a `` rough '' way , so that we can improve them . -- -- Concern 2.2 : '' The authors have only described the method , without deep analyses answering the question why . For example , the method seems heuristic , without theoretical analysis . '' We added a section ( section 2.3.2 ) in the revised version of the paper , where we give a theoretical explanation of why OC-MAML works . In the following , we briefly summarize our findings . Furthermore , we conducted additional experiments to validate our theoretical analysis empirically . For the results of these experiments please see Table 2 in the revised paper version . By analyzing the approximated loss gradients used in the MAML and OC-MAML updates , we come to the following finding : For a given task , OC-MAML optimizes for increasing the inner product of the gradients computed on different minibatches with different class-imbalance rates , namely minibatches containing data from only one class and a class-balanced minibatch ( meta-update ) . If the inner product of the gradients computed on two different minibatches is positive , taking one gradient step using one minibatch leads to an increase in performance on the other minibatch . Consequently , OC-MAML optimizes for a parameter initialization from which taking one ( or few ) gradient step ( s ) with minibatch ( es ) including only normal class data results in a performance increase on class-balanced data . In contrast , MAML optimizes for a parameter initialization that requires class-balanced minibatches to yield the same effect . When adapting to OCC tasks , however , only examples from one class are available . We conclude , therefore , that using minibatches with different class-imbalance rates for meta-training , as done in OC-MAML , yields parameter initializations that are more suitable for adapting to OCC tasks . We also find that the second-order derivative term is essential to do so , which explains why OC-FOMAML ( its first-order approximation ) fails to adapt to FS-OCC tasks . Please refer to section 2.3.2 of the revised paper version for a more detailed theoretical analysis of the gradients of the different meta-learning algorithms , in the OCC case . -- -- Concern 2.3 : '' In summary , I think this paper likes a technical report , not a research paper . '' Our work emerged from a practical and technical situation , where the few-shot one-class regime is common , namely industrial manufacturing . However , we developed a method that is applicable in multiple data domains , i.e.time-series ( in particular , sensor data ) and images , and therefore can be adopted beyond the initial technical problem . We aim for our approach to be a first and strong baseline for the , as recognized by both other reviewers , relevant and under-studied research problem of few-shot one-class classification in general , i.e.not to the specific case of few-shot anomaly detection on sensor data . We believe that our approach can be of great value to the research community that will explore this challenging and important problem further in the future . We hope that our extensions of the paper give it a stronger research character . -- -- Concern3 : `` Although I can catch the main meaning of this paper , it seems that the writing style is not so fluently . I suggested the authors to recognize the presentation . '' As mentioned above , we would be grateful if you could point out some concrete examples of sentences or sections , of which we should improve the writing style . Could you also please elaborate on the last sentence ? Since you mentioned that you have not thoroughly read our paper , we would really appreciate if you could spare the time to reevaluate our revised paper version . Please feel free to raise any further concerns or add some additional comments . As we believe that we have addressed most of your concerns we would also be very grateful if you could reconsider the rather low scoring for our research contribution ."}, "2": {"review_id": "B1ltfgSYwS-2", "review_text": "One of promising approach to tackle the few-shot problems is to use meta-learning so that the learner can quickly generalize to an unseen task. One-class classification requires only a set of positive examples to discriminate negative examples from positive examples. The current paper addresses a method of meta-training one-class classifiers in the MAML framework when only a handful of positive examples are available. ---Strength--- - Few-shot one-class classification is a timely subject, which has not be studied yet. - Meta-training one-class classifiers in the MAML framework seems to be sound. ---Weakness--- - MAML is quite a general meta-training framework, which can be used when parameterized base-learners are updated using gradient methods. Thus, when parameterized models for one-class classification are used, it is rather easy to meta-train one-class classifiers in the MAML framework. - Regarding episodic training, in contrast to few-shot classification problems, support sets in episodes have similar positive examples. Thus, fine-tuning baseline method could work well, even without using MAML. Please compare it with the fine-tuning method. ---Comments--- - I assume that the query set in each episode include negative examples, while support sets have only positive examples. Right? What is the value of c (class imbalance rate) in the query set? - Wouldn't it be better to focus on experiments with c=0% since one-class classification requires the training with only positive examples? - What was the baseline one-class classifier? One-class SVM? - It was mentioned that CLEAR was an earlier work. Then, the empirical comparison with CLEAR should be included when image data is considered.", "rating": "3: Weak Reject", "reply_text": "Comment 1 : `` I assume that the query set in each episode include negative examples , while support sets have only positive examples . Right ? What is the value of c ( class imbalance rate ) in the query set ? '' Yes , support sets include only positive examples , i.e.belonging to the majority ( non anomalous ) class . Query sets are class-balanced ( c=50 % ) in order to evaluate the model on both classes in the outer loop after adaptation , i.e.the inner loop updates . This way , we optimize the performance of the model on both classes equally after adaptation using only one class . -- -- Comment 2 : `` Would n't it be better to focus on experiments with c=0 % since one-class classification requires the training with only positive examples ? '' We showed the results of c=50 % in order to give the reader some reference numbers from the class-balanced case . For example , these results answer the question `` How much accuracy gain would having data from both classes yield '' . As for the experiments with c=1 % , we conducted them to show that our approach is not only applicable in the extreme case of one-class classification , but also in the general class-imabalance case . We will focus more on the c=0 % case in the revised version of the paper . -- -- Comment 3 : '' What was the baseline one-class classifier ? One-class SVM ? '' As we mentioned in the paper , we did not compare to the classical one-class classification approaches , since they require high amounts of data and are therefore not applicable in the few-shot regime that we address . It should be noted that OC-SVM and other shallow approaches sometimes completely fail in one-class classification even when high amounts of data are available . For example in [ 3 ] , OC-SVM yields a AUC-ROC of 50 % on some of the CIFAR-10 classes , when 5000 datapoints from this class are used for learning . Upon your request and the request of Reviewer 2 , we conducted additional experiments using the classical OCC approaches OC-SVM and Isolation Forest ( IF ) on the few-shot one-class test tasks . Hereby , we apply PCA to the data where we choose the minimum number of eigenvectors sothat at least 95 % of the variance is conserved , as done in [ 3 ] . For OC-SVM , we additionally tune the inverse length scale ( gamma ) by using 10 % of the test set , as done in [ 3 ] . This gives OC-SVM a supervised advantage , compared to the other baselines . For a fairer comparison , where these methods also benefit from the data available in the meta-training tasks , we additionally conducted experiments on the embeddings inferred by the feature extractors of both the `` Finetune '' baseline and the Multi-Task-Learning ( MTL ) baseline . The results can be seen in Table 1 of the the revised paper version . As expected , the baselines fail to generalize to unseen examples when only few datapoints from the normal class are available . The only exception is the MT-MNIST dataset , where the shallow baselines trained on the extracted embeddings yield good performance K=10 examples of the normal class are available . We explain this by the fact that in the MT-MNIST dataset , the feature extractor models ( MTL and `` Finetune '' ) are exposed to most of the anomalies of the test task ( 8 out of the 9 digit classes present in the test task ) during training . Hence , useful embeddings are extracted . We note that , even on the MT-MNIST dataset , OC-MAML still outperfoms all baselines by a significant margin . -- -- Comment 4 : '' It was mentioned that CLEAR was an earlier work . Then , the empirical comparison with CLEAR should be included when image data is considered . '' CLEAR [ 4 ] uses a feature extractor trained on ImageNet . Comparing to it on the MiniImageNet dataset would not be fair , as the feature extractor was trained on the test classes . The other datasets that we tested our approach on , MNIST and Omniglot , are composed of grey-scale images . We will not be able to run experiments on datasets that were tested in the CLEAR due to the short rebuttal time . We would like , however , to point out that OC-MAML is data-type-agnostic and was successfully validated on time-series data , to which CLEAR is not applicable . As we have addressed most of your concerns which clearly improved the quality of the paper , we would really appreciate if you could spare the time to reevaluate our revised paper and adapt the current score accordingly . Please feel free to further comment our additions and raise further concerns ."}}