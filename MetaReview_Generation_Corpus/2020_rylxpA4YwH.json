{"year": "2020", "forum": "rylxpA4YwH", "title": "On the Evaluation of Conditional GANs", "decision": "Reject", "meta_review": "The paper presents an extension of FID for conditional generation settings. While it's an important problem to address, the reviewers were concerned about the novelty and advantage of the proposed method over the existing methods. The evaluation is reported on toy datasets, and the significance is limited.", "reviews": [{"review_id": "rylxpA4YwH-0", "review_text": "Summary: This paper extends the Fr\u00e9chet Inception distance (FID) to the conditional distribution. To this end, the authors use an additional embedding for the condition variables (class, image, text), and concatenate to the data embedding. The proposed metric, name the Fr\u00e9chet joint distance (FJD), captures three desired properties of conditional generative models: sample quality, conditional consistency, and sample diversity. The authors demonstrate that the proposed metric indeed captures the properties using a synthetic (dSprite) dataset, and shows reasonable values for real datasets. Pros: - FJD is an intuitive extension of FID for conditional generative models. - FJD can be applied to various types of conditions (e.g., image and text), which cannot be done by prior work (e.g., [1]). - The paper is easy to read and experimental details are clearly stated. Cons: 1. FJD is a straightforward extension of FID. FJD simply follows the FID formula but concatenates the condition embedding to the original data embedding. It is a straightforward extension of FID and suffers from the design choice problems due to the concatenation, as stated below. 2. FJD requires many design choices and not theoretically justified. As FJD requires an additional embedding function h, balancing parameter \\alpha, and merging function g, it raises a burden of design choices. While the authors give some suggestions, they are not theoretically justified. Also, one may use the statistical distances [2] between data distribution p_data(x,c) and model distribution p_g(x,c) to evaluate conditional generative models in a principled way, e.g., measure the KL-divergence using the density ratios [3]. The advantage of FJD over such metrics is unclear, as stated below. 3. The advantage over the prior work is not clear. FJD and FID show the same trend in all reported experiments (Table 2, 3, 4), hence the advantage of FJD is unclear. Also, one may measure the FID score on conditional distributions, i.e., \\sum_c FID( p_data(x|c), p_g(x|c) ). It also captures the desired three properties and would be a strong baseline for FJD. Besides, while the authors aim to design a single metric to stand the models in a line, identifying the trade-offs of models may also be useful. For example, Improved PRD [4] provides the precision-recall trade-offs of generative models, which provides some insights for the models. [1] Ravuri and Vinyals. Classification Accuracy Score for Conditional Generative Models. NeurIPS 2019. [2] https://en.wikipedia.org/wiki/Statistical_distance [3] Uehara et al. Generative Adversarial Nets from a Density Ratio Estimation Perspective. arXiv 2016. [4] Kynk\u00e4\u00e4nniemi et al. Improved Precision and Recall Metric for Assessing Generative Models. NeurIPS 2019.", "rating": "3: Weak Reject", "reply_text": "We would like to thank the reviewer for the thorough analysis of our paper and the insightful comments . We have made several improvements to the paper based on your feedback , and have attempted to address your concerns below . Concern : FJD is a straightforward extension of FID . We think that the straightforward extension is beneficial , since many practitioners in the field are already familiar with FID , and , as the reviewer previously pointed out , the extension is intuitive . In fact , we see this more as a strength of our approach rather than a concern , and the reviewer seems to partially agree with this assessment ( \u201c FJD is an intuitive extension of FID for conditional generative models \u201d ) . Moreover , to the best of our knowledge , we are the first to propose and demonstrate ( in a series of carefully crafted and executed experiments ) that such an extension to FD can actually fulfill the requirements of the evaluation of conditional image generation models and is a useful model selection technique . Concern : FJD requires many design choices and not theoretically justified . To be clear , FJD requires only three , not many , design choices beyond that of FID and other evaluation techniques ( more details below ) . While we do not provide theoretical justification for our design choices , we did evaluate them empirically to confirm their suitability . Embedding function : All evaluation techniques require some sort of embedding function for dimensionality reduction , whether it is learned or pre-trained ( consider the Inception score , FID , KID , Improved PR , CAS , etc . ) . This seems to be an inevitable requirement for evaluation metrics . In our preliminary experiments we found that FJD was fairly insensitive to the particular embedding function used , with different functions yielding similar trends . Alpha : In the paper we present a well-motivated heuristic for selecting alpha , which is based on the reasonable assumption that we desire a model which equally balances image quality and conditional consistency . As such , we present a simple scaling rule in which alpha is selected such that it balances the weight of the conditioning embedding with that of the image embedding . This has the added desirable effect of normalizing FJD values across embedding dimension and magnitude . We validated our design decision by generating multiple conditioning embedding spaces with varying dimensions and magnitudes . Without our scaling rule , each embedding space produces a different FJD value given the same input . However , when our scaling rule was applied , we observed that the FJD value was much more uniform across embeddings , which suggests that the scaling rule is performing as expected . Merging function : We evaluated several different options for the merging function , including random projection , addition , and multiplication ( in the case that the image and conditioning embedding have the same number of dimensions ) . We measured the correlation between resulting FJD scores and conditional consistency , and found that concatenation consistently yielded the best correlation among the tested options . Concern : \u201c Also , one may use the statistical distances [ 2 ] between data distribution p_data ( x , c ) and model distribution p_g ( x , c ) to evaluate conditional generative models in a principled way. \u201d We agree . In fact , this is essentially what FJD is doing . Frechet distance ( aka Wasserstein-2 distance ) , which we use to measure the distance between data and model distributions , is a statistical distance . Concern : FJD and FID show the same trend in all reported experiments ( Table 2 , 3 , 4 ) , hence the advantage of FJD is unclear . Please see our general comment above regarding similarity in ranking between FID and FJD ."}, {"review_id": "rylxpA4YwH-1", "review_text": "Summary This paper mention that there are some critical drawbacks existing in IS (Inception Score) and FID (Fr\u00e9chet Inception Distance) which are two popular metrics to measure image generation quality. However, IS and FID scores are initially designed for measuring unconditional distribution, which fails to capture the conditional consistency of conditional distribution. Thus, the authors propose to concatenate conditioned embedding h(y) with image feature vector f(x) to extend the FID metric. The authors also implement the method on a toy dataset to show the sensitivity of FJD on conditional consistency and several popular cGAN models to show the efficiency of FJD on real data. Paper Strengths 1. The method is intuitive and easy to implement. Paper Weaknesses 1. Although this paper shows the problem of FID for capturing the conditional consistency sprightly with the toy dataset, however, this problem does not obviously show up on real data. Basically, FID can also give a good comparison of the different model as FJD", "rating": "1: Reject", "reply_text": "Thank you for your review . Since your single concern was shared by the other reviewers , please see our general comment above , where we have addressed it ."}, {"review_id": "rylxpA4YwH-2", "review_text": "This paper proposes a variant of the use of Frechet Inception Distance (FID) for the evaluation and benchmarking of conditional GAN models. FID is a popular measure for comparing image distributions in the Inception v3 feature space, in terms of the means and variances of multivariate Gaussians fit to data samples from each distribution. The authors argue that FID is ill-suited for use with cGANs, in that they do not explicitly take into account conditional consistency or intra-conditioning diversity. The main contribution and basic idea of this paper is to create joint image-conditioning distributions from the image embedding and the conditioning embedding in the Inception embedding, and then to combine them (by default, through vector concatenation). These joint image-conditioning distributions are then fed to FID as per standard usage on image distributions alone. The authors refer to their approach as FJD (Frechet Joint Distance). Although the authors propose FJD as a new technique, it should more properly be regarded as the direct use of FID on joint distributions. The main practical contribution of the paper thus reduces to the notion of concatenating the learned conditioning representation with the image representation. As a research contribution, this is in itself not very substantial. However, in their experimentation the authors do take care to show through examples the effect of their joint image-conditioning approach in assessing image quality, conditional consistency, and intra-conditioning diversity, under a variety of conditionings. There are some issues that are not adequately addressed: 1) In all experimental cases FJD scores and FID scores correlate well, which undercuts the argument that FJD is superior to FID in assessing the performance of cGAN models. Can situations be experimentally demonstrated where that is not the case? In particular, what happens when the fit in image representation is dramatically better / worse than that of the conditioning representation? This situation is interesting, but not considered in this paper. 2) The effect of the dimensionality of the learned representations is not addressed. When concatenating vectors to produce a joint image-conditioning representation, the dimensionality increases, which would tend to produce larger FJD values than their corresponding FID values. The experimental results of this paper seem to confirm this. However, is it really valid then to declare FJD as being somehow more sensitive to the conditioning simply by virtue of obtaining larger values and larger spreads than FID? It should be remembered that FID and FJD are *not* unitless measures. Overall, in its current state the paper appears to be below the acceptance threshold. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for their careful reading of our paper and remarks . Below we address the concerns that were raised . Concern : the authors propose FJD as a new technique , it should more properly be regarded as the direct use of FID on joint distributions . We would like to clarify that in the paper we introduced FJD as a direct application of Fr\u00e9chet distance ( FD ) to a joint embedding space ( see Section 4 ) . Similarly , FID is a direct application to FD to image embedding space obtained with an Inceptionv3 model ( see Section 3 ) . Moreover , when discussing our contribution we clearly state : ( 1 ) in the abstract - \u201c In this paper , we propose the Fr\u00e9chet Joint Distance ( FJD ) , which is defined as the Fr\u00e9chet distance between joint distributions of images and conditioning , allowing it to implicitly capture the aforementioned properties in a single metric \u201d , and ( 2 ) in the introduction - \u201c FJD computes the Fr\u00e9chet distance on an embedding of the joint image-conditioning distribution , and introduces only small computational overhead over FID compared to alternative methods \u201d . Concern : The main practical contribution of the paper thus reduces to the notion of concatenating the learned conditioning representation with the image representation . As a research contribution , this is in itself not very substantial . While our approach is very intuitive ( aka simple ) , we see it rather as a strength of the approach and not limitation . Moreover , to the best of our knowledge , we are the first to propose and demonstrate ( in a series of carefully crafted and executed experiments ) that such extension to FD can actually fulfill the requirements of the evaluation of conditional image generation models and is a useful model selection technique . Thus , we argue that our contribution is novel and substantial . Concern : FJD scores and FID scores correlate well . Can situations be experimentally demonstrated where that is not the case ? Please see our general comment above regarding similarity in ranking between FID and FJD . Question : what happens when the fit in image representation is dramatically better / worse than that of the conditioning representation ? We were uncertain how to interpret this question . Is the reviewer referring to scenarios where samples have reduced image quality but good conditional consistency , and vice versa ? Experiments we conducted in Section 5.2 and 5.3 cover both of these scenarios with synthetic examples . Our new experiment in Appendix H also demonstrates a real world example of GANs with good image quality and bad conditional consistency , as well as bad image quality and good conditional consistency . If the reviewer is referring to a relative \u201c strength \u201d of the image representation to the conditioning representation , please refer to Appendix F , where we study the behaviour of FJD as a function of different alpha parameter values . If this was not what you were referring to , please indicate so and we will do our best to clarify . Concern : the effect of the dimensionality of the learned representations is not addressed . However , is it really valid then to declare FJD as being somehow more sensitive to the conditioning simply by virtue of obtaining larger values and larger spreads than FID ? We would like to clarify that the only correct way to use FJD is to compare values for a fixed conditioning modality and fixed embedding function ( thus the embedding dimensionality is constant ) . One should not compare numerical values between FID and FJD as well as for two FJDs obtained with different embedding functions or using different modalities . Thus , when discussing the results from Section 6 , we draw our observations based on model rankings ( within a single metric ) . However , we do recognize that the wording used in Sections 5.2 and 5.4 might indicate that it is correct to compare FJD to FID for a fixed model . We clarified it in the paper and corrected the discussion of the results in both Sections . We thank the reviewer for spotting this issue . Nevertheless , we would argue that by design FJD is sensitive to conditional inconsistencies while FID is blind to them ( see the definitions of the metrics in Sections 3 and 4 ) and we prove this experimentally in Section 5 ( e.g.see Figure 3 ) and in Appendix D ."}], "0": {"review_id": "rylxpA4YwH-0", "review_text": "Summary: This paper extends the Fr\u00e9chet Inception distance (FID) to the conditional distribution. To this end, the authors use an additional embedding for the condition variables (class, image, text), and concatenate to the data embedding. The proposed metric, name the Fr\u00e9chet joint distance (FJD), captures three desired properties of conditional generative models: sample quality, conditional consistency, and sample diversity. The authors demonstrate that the proposed metric indeed captures the properties using a synthetic (dSprite) dataset, and shows reasonable values for real datasets. Pros: - FJD is an intuitive extension of FID for conditional generative models. - FJD can be applied to various types of conditions (e.g., image and text), which cannot be done by prior work (e.g., [1]). - The paper is easy to read and experimental details are clearly stated. Cons: 1. FJD is a straightforward extension of FID. FJD simply follows the FID formula but concatenates the condition embedding to the original data embedding. It is a straightforward extension of FID and suffers from the design choice problems due to the concatenation, as stated below. 2. FJD requires many design choices and not theoretically justified. As FJD requires an additional embedding function h, balancing parameter \\alpha, and merging function g, it raises a burden of design choices. While the authors give some suggestions, they are not theoretically justified. Also, one may use the statistical distances [2] between data distribution p_data(x,c) and model distribution p_g(x,c) to evaluate conditional generative models in a principled way, e.g., measure the KL-divergence using the density ratios [3]. The advantage of FJD over such metrics is unclear, as stated below. 3. The advantage over the prior work is not clear. FJD and FID show the same trend in all reported experiments (Table 2, 3, 4), hence the advantage of FJD is unclear. Also, one may measure the FID score on conditional distributions, i.e., \\sum_c FID( p_data(x|c), p_g(x|c) ). It also captures the desired three properties and would be a strong baseline for FJD. Besides, while the authors aim to design a single metric to stand the models in a line, identifying the trade-offs of models may also be useful. For example, Improved PRD [4] provides the precision-recall trade-offs of generative models, which provides some insights for the models. [1] Ravuri and Vinyals. Classification Accuracy Score for Conditional Generative Models. NeurIPS 2019. [2] https://en.wikipedia.org/wiki/Statistical_distance [3] Uehara et al. Generative Adversarial Nets from a Density Ratio Estimation Perspective. arXiv 2016. [4] Kynk\u00e4\u00e4nniemi et al. Improved Precision and Recall Metric for Assessing Generative Models. NeurIPS 2019.", "rating": "3: Weak Reject", "reply_text": "We would like to thank the reviewer for the thorough analysis of our paper and the insightful comments . We have made several improvements to the paper based on your feedback , and have attempted to address your concerns below . Concern : FJD is a straightforward extension of FID . We think that the straightforward extension is beneficial , since many practitioners in the field are already familiar with FID , and , as the reviewer previously pointed out , the extension is intuitive . In fact , we see this more as a strength of our approach rather than a concern , and the reviewer seems to partially agree with this assessment ( \u201c FJD is an intuitive extension of FID for conditional generative models \u201d ) . Moreover , to the best of our knowledge , we are the first to propose and demonstrate ( in a series of carefully crafted and executed experiments ) that such an extension to FD can actually fulfill the requirements of the evaluation of conditional image generation models and is a useful model selection technique . Concern : FJD requires many design choices and not theoretically justified . To be clear , FJD requires only three , not many , design choices beyond that of FID and other evaluation techniques ( more details below ) . While we do not provide theoretical justification for our design choices , we did evaluate them empirically to confirm their suitability . Embedding function : All evaluation techniques require some sort of embedding function for dimensionality reduction , whether it is learned or pre-trained ( consider the Inception score , FID , KID , Improved PR , CAS , etc . ) . This seems to be an inevitable requirement for evaluation metrics . In our preliminary experiments we found that FJD was fairly insensitive to the particular embedding function used , with different functions yielding similar trends . Alpha : In the paper we present a well-motivated heuristic for selecting alpha , which is based on the reasonable assumption that we desire a model which equally balances image quality and conditional consistency . As such , we present a simple scaling rule in which alpha is selected such that it balances the weight of the conditioning embedding with that of the image embedding . This has the added desirable effect of normalizing FJD values across embedding dimension and magnitude . We validated our design decision by generating multiple conditioning embedding spaces with varying dimensions and magnitudes . Without our scaling rule , each embedding space produces a different FJD value given the same input . However , when our scaling rule was applied , we observed that the FJD value was much more uniform across embeddings , which suggests that the scaling rule is performing as expected . Merging function : We evaluated several different options for the merging function , including random projection , addition , and multiplication ( in the case that the image and conditioning embedding have the same number of dimensions ) . We measured the correlation between resulting FJD scores and conditional consistency , and found that concatenation consistently yielded the best correlation among the tested options . Concern : \u201c Also , one may use the statistical distances [ 2 ] between data distribution p_data ( x , c ) and model distribution p_g ( x , c ) to evaluate conditional generative models in a principled way. \u201d We agree . In fact , this is essentially what FJD is doing . Frechet distance ( aka Wasserstein-2 distance ) , which we use to measure the distance between data and model distributions , is a statistical distance . Concern : FJD and FID show the same trend in all reported experiments ( Table 2 , 3 , 4 ) , hence the advantage of FJD is unclear . Please see our general comment above regarding similarity in ranking between FID and FJD ."}, "1": {"review_id": "rylxpA4YwH-1", "review_text": "Summary This paper mention that there are some critical drawbacks existing in IS (Inception Score) and FID (Fr\u00e9chet Inception Distance) which are two popular metrics to measure image generation quality. However, IS and FID scores are initially designed for measuring unconditional distribution, which fails to capture the conditional consistency of conditional distribution. Thus, the authors propose to concatenate conditioned embedding h(y) with image feature vector f(x) to extend the FID metric. The authors also implement the method on a toy dataset to show the sensitivity of FJD on conditional consistency and several popular cGAN models to show the efficiency of FJD on real data. Paper Strengths 1. The method is intuitive and easy to implement. Paper Weaknesses 1. Although this paper shows the problem of FID for capturing the conditional consistency sprightly with the toy dataset, however, this problem does not obviously show up on real data. Basically, FID can also give a good comparison of the different model as FJD", "rating": "1: Reject", "reply_text": "Thank you for your review . Since your single concern was shared by the other reviewers , please see our general comment above , where we have addressed it ."}, "2": {"review_id": "rylxpA4YwH-2", "review_text": "This paper proposes a variant of the use of Frechet Inception Distance (FID) for the evaluation and benchmarking of conditional GAN models. FID is a popular measure for comparing image distributions in the Inception v3 feature space, in terms of the means and variances of multivariate Gaussians fit to data samples from each distribution. The authors argue that FID is ill-suited for use with cGANs, in that they do not explicitly take into account conditional consistency or intra-conditioning diversity. The main contribution and basic idea of this paper is to create joint image-conditioning distributions from the image embedding and the conditioning embedding in the Inception embedding, and then to combine them (by default, through vector concatenation). These joint image-conditioning distributions are then fed to FID as per standard usage on image distributions alone. The authors refer to their approach as FJD (Frechet Joint Distance). Although the authors propose FJD as a new technique, it should more properly be regarded as the direct use of FID on joint distributions. The main practical contribution of the paper thus reduces to the notion of concatenating the learned conditioning representation with the image representation. As a research contribution, this is in itself not very substantial. However, in their experimentation the authors do take care to show through examples the effect of their joint image-conditioning approach in assessing image quality, conditional consistency, and intra-conditioning diversity, under a variety of conditionings. There are some issues that are not adequately addressed: 1) In all experimental cases FJD scores and FID scores correlate well, which undercuts the argument that FJD is superior to FID in assessing the performance of cGAN models. Can situations be experimentally demonstrated where that is not the case? In particular, what happens when the fit in image representation is dramatically better / worse than that of the conditioning representation? This situation is interesting, but not considered in this paper. 2) The effect of the dimensionality of the learned representations is not addressed. When concatenating vectors to produce a joint image-conditioning representation, the dimensionality increases, which would tend to produce larger FJD values than their corresponding FID values. The experimental results of this paper seem to confirm this. However, is it really valid then to declare FJD as being somehow more sensitive to the conditioning simply by virtue of obtaining larger values and larger spreads than FID? It should be remembered that FID and FJD are *not* unitless measures. Overall, in its current state the paper appears to be below the acceptance threshold. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for their careful reading of our paper and remarks . Below we address the concerns that were raised . Concern : the authors propose FJD as a new technique , it should more properly be regarded as the direct use of FID on joint distributions . We would like to clarify that in the paper we introduced FJD as a direct application of Fr\u00e9chet distance ( FD ) to a joint embedding space ( see Section 4 ) . Similarly , FID is a direct application to FD to image embedding space obtained with an Inceptionv3 model ( see Section 3 ) . Moreover , when discussing our contribution we clearly state : ( 1 ) in the abstract - \u201c In this paper , we propose the Fr\u00e9chet Joint Distance ( FJD ) , which is defined as the Fr\u00e9chet distance between joint distributions of images and conditioning , allowing it to implicitly capture the aforementioned properties in a single metric \u201d , and ( 2 ) in the introduction - \u201c FJD computes the Fr\u00e9chet distance on an embedding of the joint image-conditioning distribution , and introduces only small computational overhead over FID compared to alternative methods \u201d . Concern : The main practical contribution of the paper thus reduces to the notion of concatenating the learned conditioning representation with the image representation . As a research contribution , this is in itself not very substantial . While our approach is very intuitive ( aka simple ) , we see it rather as a strength of the approach and not limitation . Moreover , to the best of our knowledge , we are the first to propose and demonstrate ( in a series of carefully crafted and executed experiments ) that such extension to FD can actually fulfill the requirements of the evaluation of conditional image generation models and is a useful model selection technique . Thus , we argue that our contribution is novel and substantial . Concern : FJD scores and FID scores correlate well . Can situations be experimentally demonstrated where that is not the case ? Please see our general comment above regarding similarity in ranking between FID and FJD . Question : what happens when the fit in image representation is dramatically better / worse than that of the conditioning representation ? We were uncertain how to interpret this question . Is the reviewer referring to scenarios where samples have reduced image quality but good conditional consistency , and vice versa ? Experiments we conducted in Section 5.2 and 5.3 cover both of these scenarios with synthetic examples . Our new experiment in Appendix H also demonstrates a real world example of GANs with good image quality and bad conditional consistency , as well as bad image quality and good conditional consistency . If the reviewer is referring to a relative \u201c strength \u201d of the image representation to the conditioning representation , please refer to Appendix F , where we study the behaviour of FJD as a function of different alpha parameter values . If this was not what you were referring to , please indicate so and we will do our best to clarify . Concern : the effect of the dimensionality of the learned representations is not addressed . However , is it really valid then to declare FJD as being somehow more sensitive to the conditioning simply by virtue of obtaining larger values and larger spreads than FID ? We would like to clarify that the only correct way to use FJD is to compare values for a fixed conditioning modality and fixed embedding function ( thus the embedding dimensionality is constant ) . One should not compare numerical values between FID and FJD as well as for two FJDs obtained with different embedding functions or using different modalities . Thus , when discussing the results from Section 6 , we draw our observations based on model rankings ( within a single metric ) . However , we do recognize that the wording used in Sections 5.2 and 5.4 might indicate that it is correct to compare FJD to FID for a fixed model . We clarified it in the paper and corrected the discussion of the results in both Sections . We thank the reviewer for spotting this issue . Nevertheless , we would argue that by design FJD is sensitive to conditional inconsistencies while FID is blind to them ( see the definitions of the metrics in Sections 3 and 4 ) and we prove this experimentally in Section 5 ( e.g.see Figure 3 ) and in Appendix D ."}}