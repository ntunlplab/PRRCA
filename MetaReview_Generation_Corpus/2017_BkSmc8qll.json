{"year": "2017", "forum": "BkSmc8qll", "title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "decision": "Reject", "meta_review": "This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.", "reviews": [{"review_id": "BkSmc8qll-0", "review_text": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM. Pros: + Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers + Encouraging results on the curriculum learning on hybrid discrete and continuous attentions Cons: - Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance? - Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable feedback and comments about our paper . > Very weak NTM baseline ... The experimental setup followed on the bAbI dataset in DNC paper ( Graves et al 2016 ) and our paper is very different . Firstly , in Graves et al 2016 , they report results only on joint-training . However , in our paper , we have only trained the models on each task separately . Potentially , the transfer between different tasks in joint training can have a huge impact on the results and it can improve generalization . Most importantly , 20 % error across over all the tasks which `` Graves et al 2016 '' reports is the result obtained with the BEST NETWORK on the validation set after 20 RUNS ( please see the caption of Table 1 ) , whereas our results are obtained only from a single-run . However , in DNC paper ( see Table 1 ) , they also report mean results of their networks along with the best network after the multiple runs , their mean result is very close to ours 28.5 % with std of +/- 2.9 . Nevertheless , please note that the 31 % result which we report in our paper is very close to the mean result of the NTM in the DNC paper ( within a single standard deviation of their mean result ) . Lastly , we use the representation of GRU over the tokens in each fact of every story in our paper . However , in Graves et al 2016 , their models emit the word embeddings for all the facts in the story . > Section 3 of the paper is hard to follow\u2026 Thanks for the comment , we have improved the readability of both the Section 2 and Section 3 to fix the issue that you have pointed out and improved the clarity of our paper . We will further improve those two sections and upload a newer version of the paper to openreview again . If these two concerns are the main reasons for you to give a low score to our paper , we would kindly ask you to re-evaluate your decision ."}, {"review_id": "BkSmc8qll-1", "review_text": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory. The model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik). The model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years. In terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026). Overall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your feedback . > Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model . Please note that 6 of the terms of our cost is for the REINFORCE with entropy regularization . REINFORCE with entropy regularization is a very standard way to train neural networks with discrete stochastic decision variables and has been used in many other papers [ 1,2 ... ] . One of the term is the original cross-entropy cost function for predicting the answer to the question about the story . The other two terms are for the regularizations , which are justified in the paper . Thus if we include the original cost , REINFORCE with entropy regularization and the other two regularizations which we have proposed in this paper ( next fact prediction and the read/write consistency ) we would have 9 terms . However , only TWO OF THE TERMS of the cost are introduced in our paper . The rest of the terms are very typical for neural networks trained with REINFORCE . We have justified all the design choices we have taken in our paper and achieved significant improvements over LSTMs despite not using tricks such as linear restart , multi-runs with different seeds ( or hyperparameters ) , joint training , etc \u2026 which have been used in most of the recent memory augmented neural networks related papers . > slightly above those of a vanilla LSTM On bAbI task , LSTM got 36.41 % and this result is reported in Sukhbaatar 2015 for non-joint training ( with joint training it is possible to improve this result , but that would not be a fair comparison ) . Our model with GRU controller got 21.8 and using feedforward controller we got 12.8 percent error over all tasks . We find the improvement from 36.41 % to 12.8 % error to a significant improvement . > There is no code available nor plan to release it ( afaik ) . We have released our code on github : https : //github.com/caglar/dntm . We are improving our codebase and adding more documentation into our repository at the moment . > The equations are hard to read , using non standard notation ( e.g. , \u201c softplus \u201d ) \u2026 We find \u201c softplus \u201d to be a widely adopted term for log ( exp ( x ) +1 ) and many papers in machine learning literature has used it [ 3 ] . However , we agree that some people still may not be very familiar with it . We have already improved the readability of our equations in the direction that you have suggested . Please check the new revised version of the paper . We will make further clarifications ( mostly in Section 2 and 3 ) and upload another version of the paper soon as well . [ 1 ] Xu , K. , Ba , J. , Kiros , R. , Cho , K. , Courville , A. , Salakhutdinov , R. , ... & Bengio , Y . ( 2015 ) .Show , attend and tell : Neural image caption generation with visual attention . arXiv preprint arXiv:1502.03044 , 2 ( 3 ) , 5 . [ 2 ] Vezhnevets A , Mnih V , Osindero S , Graves A , Vinyals O , Agapiou J . Strategic attentive writer for learning macro-actions . In Advances in Neural Information Processing Systems 2016 ( pp.3486-3494 ) . [ 3 ] https : //scholar.google.ca/scholar ? q=softplus+machine+learning & btnG= & hl=en & as_sdt=0 % 2C5"}, {"review_id": "BkSmc8qll-2", "review_text": "The paper extends the NTM by a trainable memory addressing scheme. The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms. Pros: * Extension to NTM with trainable addressing. * Experiments with discrete addressing. * Experiments on bAbI QA tasks. Cons: * Big gap to MemN2N and DMN+ in performance. * Code not available. * There could be more experiments on other real-world tasks. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your valuable feedback and review . > Big gap to MemN2N and DMN+ in performance . Both MemN2N and DMN+ computes the attention over the whole input context , such that its memory grows linearly as the input sequence grows . For the bAbI task , very likely that the optimal strategy is to perform attention over all facts in the story . However , D-NTM keeps a limited-size memory and it learns to write into the memory as well which is much more difficult . In particular , on tasks which involve ambiguities , it is very easy for the controller to learn a deficient explicit memory usage that does not generalize well for the other tasks . Because the part of the memory to read from or write into becomes ambiguous as well . Furthermore , reading depends on the writing as well , if the controller fails to write into the correct location in the memory , reading from that part of the memory becomes futile . The advantage of using limited external memory becomes more evident when the input sequence gets very long , such that computation of attention over the whole input sequence would be infeasible . > Code not available . We made our code freely available at : https : //github.com/caglar/dntm > There could be more exp\u2026 Thanks for noticing this we will add more results on more realistic tasks . We plan to add one more experiment on SNLI task to the paper ."}], "0": {"review_id": "BkSmc8qll-0", "review_text": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM. Pros: + Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers + Encouraging results on the curriculum learning on hybrid discrete and continuous attentions Cons: - Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance? - Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable feedback and comments about our paper . > Very weak NTM baseline ... The experimental setup followed on the bAbI dataset in DNC paper ( Graves et al 2016 ) and our paper is very different . Firstly , in Graves et al 2016 , they report results only on joint-training . However , in our paper , we have only trained the models on each task separately . Potentially , the transfer between different tasks in joint training can have a huge impact on the results and it can improve generalization . Most importantly , 20 % error across over all the tasks which `` Graves et al 2016 '' reports is the result obtained with the BEST NETWORK on the validation set after 20 RUNS ( please see the caption of Table 1 ) , whereas our results are obtained only from a single-run . However , in DNC paper ( see Table 1 ) , they also report mean results of their networks along with the best network after the multiple runs , their mean result is very close to ours 28.5 % with std of +/- 2.9 . Nevertheless , please note that the 31 % result which we report in our paper is very close to the mean result of the NTM in the DNC paper ( within a single standard deviation of their mean result ) . Lastly , we use the representation of GRU over the tokens in each fact of every story in our paper . However , in Graves et al 2016 , their models emit the word embeddings for all the facts in the story . > Section 3 of the paper is hard to follow\u2026 Thanks for the comment , we have improved the readability of both the Section 2 and Section 3 to fix the issue that you have pointed out and improved the clarity of our paper . We will further improve those two sections and upload a newer version of the paper to openreview again . If these two concerns are the main reasons for you to give a low score to our paper , we would kindly ask you to re-evaluate your decision ."}, "1": {"review_id": "BkSmc8qll-1", "review_text": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory. The model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik). The model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years. In terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026). Overall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your feedback . > Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model . Please note that 6 of the terms of our cost is for the REINFORCE with entropy regularization . REINFORCE with entropy regularization is a very standard way to train neural networks with discrete stochastic decision variables and has been used in many other papers [ 1,2 ... ] . One of the term is the original cross-entropy cost function for predicting the answer to the question about the story . The other two terms are for the regularizations , which are justified in the paper . Thus if we include the original cost , REINFORCE with entropy regularization and the other two regularizations which we have proposed in this paper ( next fact prediction and the read/write consistency ) we would have 9 terms . However , only TWO OF THE TERMS of the cost are introduced in our paper . The rest of the terms are very typical for neural networks trained with REINFORCE . We have justified all the design choices we have taken in our paper and achieved significant improvements over LSTMs despite not using tricks such as linear restart , multi-runs with different seeds ( or hyperparameters ) , joint training , etc \u2026 which have been used in most of the recent memory augmented neural networks related papers . > slightly above those of a vanilla LSTM On bAbI task , LSTM got 36.41 % and this result is reported in Sukhbaatar 2015 for non-joint training ( with joint training it is possible to improve this result , but that would not be a fair comparison ) . Our model with GRU controller got 21.8 and using feedforward controller we got 12.8 percent error over all tasks . We find the improvement from 36.41 % to 12.8 % error to a significant improvement . > There is no code available nor plan to release it ( afaik ) . We have released our code on github : https : //github.com/caglar/dntm . We are improving our codebase and adding more documentation into our repository at the moment . > The equations are hard to read , using non standard notation ( e.g. , \u201c softplus \u201d ) \u2026 We find \u201c softplus \u201d to be a widely adopted term for log ( exp ( x ) +1 ) and many papers in machine learning literature has used it [ 3 ] . However , we agree that some people still may not be very familiar with it . We have already improved the readability of our equations in the direction that you have suggested . Please check the new revised version of the paper . We will make further clarifications ( mostly in Section 2 and 3 ) and upload another version of the paper soon as well . [ 1 ] Xu , K. , Ba , J. , Kiros , R. , Cho , K. , Courville , A. , Salakhutdinov , R. , ... & Bengio , Y . ( 2015 ) .Show , attend and tell : Neural image caption generation with visual attention . arXiv preprint arXiv:1502.03044 , 2 ( 3 ) , 5 . [ 2 ] Vezhnevets A , Mnih V , Osindero S , Graves A , Vinyals O , Agapiou J . Strategic attentive writer for learning macro-actions . In Advances in Neural Information Processing Systems 2016 ( pp.3486-3494 ) . [ 3 ] https : //scholar.google.ca/scholar ? q=softplus+machine+learning & btnG= & hl=en & as_sdt=0 % 2C5"}, "2": {"review_id": "BkSmc8qll-2", "review_text": "The paper extends the NTM by a trainable memory addressing scheme. The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms. Pros: * Extension to NTM with trainable addressing. * Experiments with discrete addressing. * Experiments on bAbI QA tasks. Cons: * Big gap to MemN2N and DMN+ in performance. * Code not available. * There could be more experiments on other real-world tasks. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your valuable feedback and review . > Big gap to MemN2N and DMN+ in performance . Both MemN2N and DMN+ computes the attention over the whole input context , such that its memory grows linearly as the input sequence grows . For the bAbI task , very likely that the optimal strategy is to perform attention over all facts in the story . However , D-NTM keeps a limited-size memory and it learns to write into the memory as well which is much more difficult . In particular , on tasks which involve ambiguities , it is very easy for the controller to learn a deficient explicit memory usage that does not generalize well for the other tasks . Because the part of the memory to read from or write into becomes ambiguous as well . Furthermore , reading depends on the writing as well , if the controller fails to write into the correct location in the memory , reading from that part of the memory becomes futile . The advantage of using limited external memory becomes more evident when the input sequence gets very long , such that computation of attention over the whole input sequence would be infeasible . > Code not available . We made our code freely available at : https : //github.com/caglar/dntm > There could be more exp\u2026 Thanks for noticing this we will add more results on more realistic tasks . We plan to add one more experiment on SNLI task to the paper ."}}