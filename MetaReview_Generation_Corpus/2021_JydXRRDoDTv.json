{"year": "2021", "forum": "JydXRRDoDTv", "title": "Optimistic Policy Optimization with General Function Approximations", "decision": "Reject", "meta_review": "While the reviewers appreciated the aim of the work, they found the technical contribution to be too incremental to be of sufficient interest and the exploration of the problem and its significance to be incomplete in the paper's current state.", "reviews": [{"review_id": "JydXRRDoDTv-0", "review_text": "This paper propose a novel policy optimization algorithm that allows general function approximation ( i.e. , kernel function approximation , neural function approximation ) that go beyond linear approximation , the algorithm achieves exploration by absorbing optimism into policy evaluation . An \\sqrt { T } regret bound of the proposed algorithm is obtained . Overall , this paper is very well-written , the problem is well-motivated , and the claims and proofs looks solid .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your review of our work . We have addressed the issues raised by the other reviewers and revised our work accordingly ."}, {"review_id": "JydXRRDoDTv-1", "review_text": "* * Summary : * * This paper proves a regret bound for an optimistic variant of a policy optimization algorithm in an advsersarial reward setting . The paper extends prior work in this setting by considering function classes with bounded Eluder dimension instead of linear functions . This yields guarantees for a kernel-based variant of the algorithm ( which also apply to neural kernels like the NTK ) . -- * * Strengths : * * 1 . More general than prior work . The main strength of this paper is to extend the work of Cai et al . ( 2019 ) beyond the Linear MDP setting to one with a kernelized transition matrix . This essentially requires introducing some results based on the Eluder dimension from Russo and Van Roy ( 2014 ) . The paper helps to fill out this line of research by showing that the algorithm from Cai et al . ( 2019 ) does indeed work when the linear MDP is replaced by a kernelized one . 2.The proofs seem to be correct . -- * * Weaknesses : * * 1 . Setting seems unrealistic . The paper assumes that the * entire reward function * is revealed after each episode rather than only seeing the rewards at the visited states . This is integral to the algorithm since this reward function is then queried at new state , action pairs . I understand that this allows for adversarial reward functions and is the same assumption from Cai et al . ( 2019 ) , but it seems unrealistic and unmotivated . I can not think of a problem where the * entire reward function * would be revealed at the end of each episode , and without this assumption the entire algorithm and argument seem to break down . 2.Calling this algorithm policy optimization seems misleading . The algorithm requires using the kernel-based approximator to learn not just a model , but an entire confidence set over models . In the usual RL nomenclature this would seem to be a model-based algorithm . While the policy is not simply attained by planning in the learned model , having to learn these models seems to make the algorithm indeed model-based . This would not be so important except a major motivation for the paper in the abstract and intro is to provide guarantees for model-free policy optimization algorithms like the policy gradient algorithms that are often used in practice . This motivation does not align with the algorithm the paper is actually analyzing . 3.The main assumption is not clearly explained . The main assumption in the paper is that the transition kernel is in some RKHS , but the paper never explains what this assumption means and which sorts of MDPs it may apply to . It is more general than the Linear MDP , but how much more general ? Moreover , the introduction makes it sound like the function approximation assumption is merely for the policy class , but in fact the approximator must be able to represent the entire transition kernel , which is potentially much more complicated . 4.Novelty.To me the paper is only an incremental improvement over the results of Cai et al . ( 2019 ) .Specifically , extending results from a linear setting to a kernel setting provides a technical challenge , but it is not clear how much insight is gained . The main result follows quickly from combining the results on Eluder dimension from Russo and Van Roy ( 2014 ) with thos of Cai et al . ( 2019 ) .5.Algorithm seems computationally infeasible . The algorithm requires maximizing an integral over the entire state space against over a set of measures defined by a confidence set over models in the RKHS at every step . The paper argues that the algorithm can be implemented with supervised learning oracles , but having to solve several supervised learning problems at each step of training over all historical data seems quite inefficient . In short , this is not an algorithm that a practitioner would attempt to implement . -- * * Recommendation : * * I recommend to reject this paper , and gave it a score of 4 . While the paper is a logical extension of some prior work , I am not convinced of the usefulness of the setting or algorithm proposed . If the authors can provide better motivation for the setting , novelty , and practical value of the contribution , I would consider raising my score .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the valuable review and suggestions . We have revised our work accordingly . In what follows , we address your concerns in detail . 1 . ( unrealistic reward function setting ) The instant reward function in reinforcement learning is usually known or defined by human in the modeling phase , and thus it is quite common to assume it to be known . Actually , in practice , oftentimes it is up to the learner to design a reward function , which is known as the reward shaping problem . After the reward is designed , a reinforcement learning problem is solved with that reward . As a concrete example , in many Atari games , reward functions are directly given by the rules and are usually trivial indicator functions of the states . In our setting , since we consider adversarial rewards , which can depend on previous actions , we assume the revealing of the reward function is delayed . Moreover , our algorithm and proof can be easily adapted to the case of stationary reward function with stochastic feedback . For such a case , we can directly estimate the reward function via least-squares regression and add another bonus function in the policy evaluation step to address the uncertainty in reward function estimation . Moreover , it is possible to allow stochastic feedbacks under the adversarial reward setting . We can also construct confidence sets based on estimates of the reward function using techniques from the bandit literature , e.g. , [ 3 ] . Furthermore , we would emphasize that the core difficulty of reinforcement learning is to make optimal sequential decisions with the transition model unknown . Such a challenge is known as `` deep exploration \u2019 \u2019 [ 4 ] which means that an exploration algorithm needs to consider the effect of any action on future learning , which is propagated by the unknown transition model . Even when the reward function is known , such a fundamental challenge persists due to the unknown transition . This is the fundamental difference from the bandit setting , where exploration only focuses on the immediate effect of an action . Thus , even though we assume the reward function is known , such a setting is realistic from both practical and theoretical perspectives . 2 . ( name of policy optimization ) The algorithm of the paper is model-based , and we have made it clear in the abstract and introduction . We would like to emphasize that the model-free versus model-based classification of RL methods is orthogonal to the policy-based versus value-based algorithm . An algorithm is policy-based simply means that the algorithm updates the policy directly by an optimization method ( e.g.policy gradient ) , and the update direction can be calculated by either model-based or model-free methods . Our algorithm is policy-based because the algorithm constructs a sequence of policies by KL-divergence regularized optimization , which is a local greedy update similar to the natural policy gradient method . This algorithm is model-based because the descent direction , namely the critic , is obtained by model-based policy evaluation . See also Point 1 for Reviewer 5 . In the paper , we emphasize that our algorithm is a policy optimization method in order to distinguish it from purely value-based methods , where the learned policy is always the greedy policy with respect to the value function , and these methods update the policy indirectly by updating the value functions instead . In contrast , policy optimization essentially searches policies within the stochastic policy space . 3 . ( assumption of transition kernel in some RKHS ) RKHS is widely used in machine learning and known as a rich class of functions , which covers the linear function as a special case by choosing the kernel to be the inner product kernel . As a result , our RKHS transition kernel covers the class of linear transitions as a special case . Moreover , note that the inner product kernel ( the kernel of the linear case ) is of finite-rank . Thus , the transition kernels expressed by linear class is only finite-dimensional functions . In contrast , by considering RKHS functions in general , we cover __infinite-dimensional__ transition functions . Thus , the model considered in our work is __strictly much more general__ than that in [ 1 ] . One example is the transition kernel can be represented using the random feature model [ 5 ] induced by a given kernel , which is also closely related to the case of NTK in our paper . Such a class of functions is infinite-dimensional and can not belong to the finite-dimensional linear model ."}, {"review_id": "JydXRRDoDTv-2", "review_text": "Summary : The paper claims to study the sample efficiency in policy optimization with general function approximation . Specifically , the authors propose an algorithm with transition model approximation and analyze the regret when adopting RKHS or function classes with bounded Eluder dimension . The results also apply to neural networks in the NTK regime . -- Reasons for score : The theoretical understanding of RL with function approximation is an important issue . It is nice to see that the paper provides a wide-ranging discussion under this topic . My major concerns are about the novelty and clarity . The paper investigates many interesting scenarios , but disappointingly , none of them is satisfyingly addressed . -- Pros : * The paper goes beyond tabular MDP and finite-dimensional linear function setting . The use of RKHS and Eluder dimension makes the results more general . * The paper connects reinforcement learning with neural network and attempts to provide a unified theoretical explanation for the empirical successes in deep RL . -- Cons : * In introduction , the authors claim to study a policy-based approach but their algorithm turns out to be a model-based one . * The paper seems only a combination of Cai et al . ( 2019 ) and Ayoub et at . ( 2020 ) using RKHS and Eluder dimension techniques . Admittedly , the extension needs some work , however , the results do not provide much more insights compared with Cai et al. ( 2019 ) . * Assumption 4.2 is too restrictive . It assumes the eigenvalues of kernel to decay exponentially fast . A discussion of power law spectral decay would definitely improve the quality of the paper . In Assumption 4.6 , it is also unclear why exponential spectral decay is a reasonable assumption on NTK . The authors refer to Srinivas et al . ( 2019 ) and Yang & Salman ( 2019 ) to justify their assumptions . However , it seems to the reviewer that Srinivas et al . ( 2019 ) considers all finite spectrum , exponential spectral decay and power law spectral decay . Also , Theorem 3.1 in Yang & Salman ( 2019 ) shows a power law for NTK and does not support Assumption 4.6 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We appreciate the valuable review and suggestions . We have revised our work accordingly . In what follows , we address your concerns in detail . 1 . ( policy-based approach ) The algorithm of the paper is model-based , and we have made it clear in the abstract and introduction . However , it seems that the reviewer wrongly claims that an RL algorithm can not be both model-based and policy-based . We would like to point out that an RL method is model-based or model-free only depends on whether the algorithm estimates the transition or not . While an algorithm is policy-based if it directly updates the policy via optimization methods where the update direction can be computed by either model-based or model-free methods . Thus , they are orthogonal aspects of an RL algorithm , and an algorithm can be simultaneously model-based and policy-based . This is exactly the case of our algorithm we estimate the model via value-targeted regression , use the estimated model to estimate the critic , and update the policy by a KL-divergence based optimization problem based on the estimated critic . 2 . ( Novelty ) Our paper is NOT simply a combination of [ 1 ] and [ 2 ] . Compared with [ 2 ] , we for the first time introduce RKHS ( and the NTK regime of overparameterized neural networks ) into the eluder framework , which is not studied before even in bandit . [ 2 ] and other previous work on the eluder framework only analyzes the eluder dimension of finite-dimensional linear models or generalized finite-dimensional linear models with very limited link functions . Bringing infinite-dimensional RKHS and overparameterized neural networks require nontrivial efforts . Moreover , in terms of algorithms , [ 2 ] propose a UCRL-type algorithm whereas we study policy optimization algorithm . These two algorithms have diverse properties -\u2013 the value functions constructed by [ 2 ] are always upper bounds on the globally optimal value function $ Q^ { \\pi * } $ whereas the value functions constructed by OPPO are upper bounds for policy evaluation problems , which can be much smaller than $ Q^ { \\pi * } $ . In addition , [ 1 ] only studies the linear function approximation setting and both the algorithm and theory are specific to such a linear setting . In comparison , we extend its algorithm to the general function approximation setting which includes linear , RKHS , overparameterized neural networks as a special case . We emphasize that the algorithm in [ 1 ] can not be directly applied to the general function approximation setting . In addition to the novel algorithm , we also establish a unified theoretical framework that covers linear , RKHS , and overparameterized neural networks as special cases . Finally , we emphasize that our result can not be directly obtained by combining [ 1 ] and [ 2 ] for two reasons . First , RKHS and overparameterized neural network cases are not covered in [ 1 ] or [ 2 ] . Second , the algorithm in [ 2 ] can not be directly combined with [ 1 ] . Thus , we propose a novel optimistic policy optimization problem to tackle such a challenge . Specifically , the optimistic policy evaluation subroutine is novel and has not appeared in [ 1 ] or [ 2 ] . To the best of our knowledge , our work establishes for the first time a policy optimization algorithm that provably explores under the general function approximation setting . To analyze this algorithm , we provide a general theorey framework that bridges policy optimization and general function approximation under the eluder dimension framework and establishes a regret bound . Our theory covers the result in [ 1 ] as a special example while also cover powerful function approximators such as RKHS functions and overparameterized neural networks . To this end , we further enrich the theory of the eluder dimension and demonstrate the power of our theory framework by establishing the eluder dimension of RKHS , and overparametrized neural networks . Furthermore , in terms of technical contribution , more importantly , our paper is the first that incorporates RKHS ( and the NTK regime of overparametrized neural networks ) into the eluder framework , which is not studied before even for bandits . Such a novel eluder theory is not only limited to policy optimization but also can contribute to the theoretical study of other RL algorithms with kernel or neural network function approximation including UCRL [ 2 ] and model-based value iteration ."}, {"review_id": "JydXRRDoDTv-3", "review_text": "The paper proposes an optimistic policy optimization algorithm . It is theoretically shown that the algorithm has sublinear regret for multiple model classes such as kernel function and NTK . The policy update rule also allows the algorithm to have sublinear regret for adversarial reward function . The technique for analyzing nonparametric model classes can potentially be extended to other reinforcement learning algorithms based on optimism . While the algorithm itself is neat and clean , the paragraph describing its implementation is not very rigorous . In particular , how is Line 13 computed ? Note that for general value function approximation , the state space is usually large ( or even infinite ) . If I understand correctly , there is no assumption about the structure of Q-function in this paper . How is the Q-function even stored , or parameterized ? A similar question is that , the last few sentences before Sec 4 says `` it suffices to solve a least-square regression problem '' . Does it mean that $ Q_h^k ( \\cdot\\mid\\cdot ) $ is computed by doing a regression problem where the target is r+clamp ... ? If this is the case , can the Q-function be solved exactly ? Can the algorithm work for the case where the regression error is small but not zero ? It seems to me that the analysis for general function class is different to those for kernel function and NTK , so I 'm wondering that are there any upper/lower bounds for the Eluder dimension of the two non-parametric classes described in this paper ? Besides that , the regret upper bound given by Theorem 4.3 and 4.7 depends on parameter $ \\gamma $ . In what scenario the parameter $ \\gamma $ is lower bounded by some constant ? Minor issue : - In Eq . ( 3.1 ) , Page 3 : should the policy be $ \\pi_h^k ( a\\mid s ) =\\frac { \\exp ( E_h^k ( s , a ) ) } { \\sum \\exp ( E_h^k ( s , a ) ) } $ ? Overall , the results are solid and interesting . Therefore I would recommend a acceptance .", "rating": "7: Good paper, accept", "reply_text": "We appreciate the valuable review and suggestions . We have revised our work accordingly . In what follows , we address your concerns in detail . 1 . ( implementation of the algorithm ) Yes , we will need another parametrization to store the updated Q-functions , which will be trained through least square regressions to fit the target . We note that in such a regression problem , we have as much training data as we need , and thus the error can usually be reduced to any sufficient accuracy , for example , using a non-parametric model . The computational error , if considered , can be trivially separated as a part of the regret and will be dominated by the terms in our main theorems given enough computational power . 2 . ( typos ) Yes , equation ( 3.1 ) should be written in that form . We have corrected it in the revised pdf ."}], "0": {"review_id": "JydXRRDoDTv-0", "review_text": "This paper propose a novel policy optimization algorithm that allows general function approximation ( i.e. , kernel function approximation , neural function approximation ) that go beyond linear approximation , the algorithm achieves exploration by absorbing optimism into policy evaluation . An \\sqrt { T } regret bound of the proposed algorithm is obtained . Overall , this paper is very well-written , the problem is well-motivated , and the claims and proofs looks solid .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your review of our work . We have addressed the issues raised by the other reviewers and revised our work accordingly ."}, "1": {"review_id": "JydXRRDoDTv-1", "review_text": "* * Summary : * * This paper proves a regret bound for an optimistic variant of a policy optimization algorithm in an advsersarial reward setting . The paper extends prior work in this setting by considering function classes with bounded Eluder dimension instead of linear functions . This yields guarantees for a kernel-based variant of the algorithm ( which also apply to neural kernels like the NTK ) . -- * * Strengths : * * 1 . More general than prior work . The main strength of this paper is to extend the work of Cai et al . ( 2019 ) beyond the Linear MDP setting to one with a kernelized transition matrix . This essentially requires introducing some results based on the Eluder dimension from Russo and Van Roy ( 2014 ) . The paper helps to fill out this line of research by showing that the algorithm from Cai et al . ( 2019 ) does indeed work when the linear MDP is replaced by a kernelized one . 2.The proofs seem to be correct . -- * * Weaknesses : * * 1 . Setting seems unrealistic . The paper assumes that the * entire reward function * is revealed after each episode rather than only seeing the rewards at the visited states . This is integral to the algorithm since this reward function is then queried at new state , action pairs . I understand that this allows for adversarial reward functions and is the same assumption from Cai et al . ( 2019 ) , but it seems unrealistic and unmotivated . I can not think of a problem where the * entire reward function * would be revealed at the end of each episode , and without this assumption the entire algorithm and argument seem to break down . 2.Calling this algorithm policy optimization seems misleading . The algorithm requires using the kernel-based approximator to learn not just a model , but an entire confidence set over models . In the usual RL nomenclature this would seem to be a model-based algorithm . While the policy is not simply attained by planning in the learned model , having to learn these models seems to make the algorithm indeed model-based . This would not be so important except a major motivation for the paper in the abstract and intro is to provide guarantees for model-free policy optimization algorithms like the policy gradient algorithms that are often used in practice . This motivation does not align with the algorithm the paper is actually analyzing . 3.The main assumption is not clearly explained . The main assumption in the paper is that the transition kernel is in some RKHS , but the paper never explains what this assumption means and which sorts of MDPs it may apply to . It is more general than the Linear MDP , but how much more general ? Moreover , the introduction makes it sound like the function approximation assumption is merely for the policy class , but in fact the approximator must be able to represent the entire transition kernel , which is potentially much more complicated . 4.Novelty.To me the paper is only an incremental improvement over the results of Cai et al . ( 2019 ) .Specifically , extending results from a linear setting to a kernel setting provides a technical challenge , but it is not clear how much insight is gained . The main result follows quickly from combining the results on Eluder dimension from Russo and Van Roy ( 2014 ) with thos of Cai et al . ( 2019 ) .5.Algorithm seems computationally infeasible . The algorithm requires maximizing an integral over the entire state space against over a set of measures defined by a confidence set over models in the RKHS at every step . The paper argues that the algorithm can be implemented with supervised learning oracles , but having to solve several supervised learning problems at each step of training over all historical data seems quite inefficient . In short , this is not an algorithm that a practitioner would attempt to implement . -- * * Recommendation : * * I recommend to reject this paper , and gave it a score of 4 . While the paper is a logical extension of some prior work , I am not convinced of the usefulness of the setting or algorithm proposed . If the authors can provide better motivation for the setting , novelty , and practical value of the contribution , I would consider raising my score .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the valuable review and suggestions . We have revised our work accordingly . In what follows , we address your concerns in detail . 1 . ( unrealistic reward function setting ) The instant reward function in reinforcement learning is usually known or defined by human in the modeling phase , and thus it is quite common to assume it to be known . Actually , in practice , oftentimes it is up to the learner to design a reward function , which is known as the reward shaping problem . After the reward is designed , a reinforcement learning problem is solved with that reward . As a concrete example , in many Atari games , reward functions are directly given by the rules and are usually trivial indicator functions of the states . In our setting , since we consider adversarial rewards , which can depend on previous actions , we assume the revealing of the reward function is delayed . Moreover , our algorithm and proof can be easily adapted to the case of stationary reward function with stochastic feedback . For such a case , we can directly estimate the reward function via least-squares regression and add another bonus function in the policy evaluation step to address the uncertainty in reward function estimation . Moreover , it is possible to allow stochastic feedbacks under the adversarial reward setting . We can also construct confidence sets based on estimates of the reward function using techniques from the bandit literature , e.g. , [ 3 ] . Furthermore , we would emphasize that the core difficulty of reinforcement learning is to make optimal sequential decisions with the transition model unknown . Such a challenge is known as `` deep exploration \u2019 \u2019 [ 4 ] which means that an exploration algorithm needs to consider the effect of any action on future learning , which is propagated by the unknown transition model . Even when the reward function is known , such a fundamental challenge persists due to the unknown transition . This is the fundamental difference from the bandit setting , where exploration only focuses on the immediate effect of an action . Thus , even though we assume the reward function is known , such a setting is realistic from both practical and theoretical perspectives . 2 . ( name of policy optimization ) The algorithm of the paper is model-based , and we have made it clear in the abstract and introduction . We would like to emphasize that the model-free versus model-based classification of RL methods is orthogonal to the policy-based versus value-based algorithm . An algorithm is policy-based simply means that the algorithm updates the policy directly by an optimization method ( e.g.policy gradient ) , and the update direction can be calculated by either model-based or model-free methods . Our algorithm is policy-based because the algorithm constructs a sequence of policies by KL-divergence regularized optimization , which is a local greedy update similar to the natural policy gradient method . This algorithm is model-based because the descent direction , namely the critic , is obtained by model-based policy evaluation . See also Point 1 for Reviewer 5 . In the paper , we emphasize that our algorithm is a policy optimization method in order to distinguish it from purely value-based methods , where the learned policy is always the greedy policy with respect to the value function , and these methods update the policy indirectly by updating the value functions instead . In contrast , policy optimization essentially searches policies within the stochastic policy space . 3 . ( assumption of transition kernel in some RKHS ) RKHS is widely used in machine learning and known as a rich class of functions , which covers the linear function as a special case by choosing the kernel to be the inner product kernel . As a result , our RKHS transition kernel covers the class of linear transitions as a special case . Moreover , note that the inner product kernel ( the kernel of the linear case ) is of finite-rank . Thus , the transition kernels expressed by linear class is only finite-dimensional functions . In contrast , by considering RKHS functions in general , we cover __infinite-dimensional__ transition functions . Thus , the model considered in our work is __strictly much more general__ than that in [ 1 ] . One example is the transition kernel can be represented using the random feature model [ 5 ] induced by a given kernel , which is also closely related to the case of NTK in our paper . Such a class of functions is infinite-dimensional and can not belong to the finite-dimensional linear model ."}, "2": {"review_id": "JydXRRDoDTv-2", "review_text": "Summary : The paper claims to study the sample efficiency in policy optimization with general function approximation . Specifically , the authors propose an algorithm with transition model approximation and analyze the regret when adopting RKHS or function classes with bounded Eluder dimension . The results also apply to neural networks in the NTK regime . -- Reasons for score : The theoretical understanding of RL with function approximation is an important issue . It is nice to see that the paper provides a wide-ranging discussion under this topic . My major concerns are about the novelty and clarity . The paper investigates many interesting scenarios , but disappointingly , none of them is satisfyingly addressed . -- Pros : * The paper goes beyond tabular MDP and finite-dimensional linear function setting . The use of RKHS and Eluder dimension makes the results more general . * The paper connects reinforcement learning with neural network and attempts to provide a unified theoretical explanation for the empirical successes in deep RL . -- Cons : * In introduction , the authors claim to study a policy-based approach but their algorithm turns out to be a model-based one . * The paper seems only a combination of Cai et al . ( 2019 ) and Ayoub et at . ( 2020 ) using RKHS and Eluder dimension techniques . Admittedly , the extension needs some work , however , the results do not provide much more insights compared with Cai et al. ( 2019 ) . * Assumption 4.2 is too restrictive . It assumes the eigenvalues of kernel to decay exponentially fast . A discussion of power law spectral decay would definitely improve the quality of the paper . In Assumption 4.6 , it is also unclear why exponential spectral decay is a reasonable assumption on NTK . The authors refer to Srinivas et al . ( 2019 ) and Yang & Salman ( 2019 ) to justify their assumptions . However , it seems to the reviewer that Srinivas et al . ( 2019 ) considers all finite spectrum , exponential spectral decay and power law spectral decay . Also , Theorem 3.1 in Yang & Salman ( 2019 ) shows a power law for NTK and does not support Assumption 4.6 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We appreciate the valuable review and suggestions . We have revised our work accordingly . In what follows , we address your concerns in detail . 1 . ( policy-based approach ) The algorithm of the paper is model-based , and we have made it clear in the abstract and introduction . However , it seems that the reviewer wrongly claims that an RL algorithm can not be both model-based and policy-based . We would like to point out that an RL method is model-based or model-free only depends on whether the algorithm estimates the transition or not . While an algorithm is policy-based if it directly updates the policy via optimization methods where the update direction can be computed by either model-based or model-free methods . Thus , they are orthogonal aspects of an RL algorithm , and an algorithm can be simultaneously model-based and policy-based . This is exactly the case of our algorithm we estimate the model via value-targeted regression , use the estimated model to estimate the critic , and update the policy by a KL-divergence based optimization problem based on the estimated critic . 2 . ( Novelty ) Our paper is NOT simply a combination of [ 1 ] and [ 2 ] . Compared with [ 2 ] , we for the first time introduce RKHS ( and the NTK regime of overparameterized neural networks ) into the eluder framework , which is not studied before even in bandit . [ 2 ] and other previous work on the eluder framework only analyzes the eluder dimension of finite-dimensional linear models or generalized finite-dimensional linear models with very limited link functions . Bringing infinite-dimensional RKHS and overparameterized neural networks require nontrivial efforts . Moreover , in terms of algorithms , [ 2 ] propose a UCRL-type algorithm whereas we study policy optimization algorithm . These two algorithms have diverse properties -\u2013 the value functions constructed by [ 2 ] are always upper bounds on the globally optimal value function $ Q^ { \\pi * } $ whereas the value functions constructed by OPPO are upper bounds for policy evaluation problems , which can be much smaller than $ Q^ { \\pi * } $ . In addition , [ 1 ] only studies the linear function approximation setting and both the algorithm and theory are specific to such a linear setting . In comparison , we extend its algorithm to the general function approximation setting which includes linear , RKHS , overparameterized neural networks as a special case . We emphasize that the algorithm in [ 1 ] can not be directly applied to the general function approximation setting . In addition to the novel algorithm , we also establish a unified theoretical framework that covers linear , RKHS , and overparameterized neural networks as special cases . Finally , we emphasize that our result can not be directly obtained by combining [ 1 ] and [ 2 ] for two reasons . First , RKHS and overparameterized neural network cases are not covered in [ 1 ] or [ 2 ] . Second , the algorithm in [ 2 ] can not be directly combined with [ 1 ] . Thus , we propose a novel optimistic policy optimization problem to tackle such a challenge . Specifically , the optimistic policy evaluation subroutine is novel and has not appeared in [ 1 ] or [ 2 ] . To the best of our knowledge , our work establishes for the first time a policy optimization algorithm that provably explores under the general function approximation setting . To analyze this algorithm , we provide a general theorey framework that bridges policy optimization and general function approximation under the eluder dimension framework and establishes a regret bound . Our theory covers the result in [ 1 ] as a special example while also cover powerful function approximators such as RKHS functions and overparameterized neural networks . To this end , we further enrich the theory of the eluder dimension and demonstrate the power of our theory framework by establishing the eluder dimension of RKHS , and overparametrized neural networks . Furthermore , in terms of technical contribution , more importantly , our paper is the first that incorporates RKHS ( and the NTK regime of overparametrized neural networks ) into the eluder framework , which is not studied before even for bandits . Such a novel eluder theory is not only limited to policy optimization but also can contribute to the theoretical study of other RL algorithms with kernel or neural network function approximation including UCRL [ 2 ] and model-based value iteration ."}, "3": {"review_id": "JydXRRDoDTv-3", "review_text": "The paper proposes an optimistic policy optimization algorithm . It is theoretically shown that the algorithm has sublinear regret for multiple model classes such as kernel function and NTK . The policy update rule also allows the algorithm to have sublinear regret for adversarial reward function . The technique for analyzing nonparametric model classes can potentially be extended to other reinforcement learning algorithms based on optimism . While the algorithm itself is neat and clean , the paragraph describing its implementation is not very rigorous . In particular , how is Line 13 computed ? Note that for general value function approximation , the state space is usually large ( or even infinite ) . If I understand correctly , there is no assumption about the structure of Q-function in this paper . How is the Q-function even stored , or parameterized ? A similar question is that , the last few sentences before Sec 4 says `` it suffices to solve a least-square regression problem '' . Does it mean that $ Q_h^k ( \\cdot\\mid\\cdot ) $ is computed by doing a regression problem where the target is r+clamp ... ? If this is the case , can the Q-function be solved exactly ? Can the algorithm work for the case where the regression error is small but not zero ? It seems to me that the analysis for general function class is different to those for kernel function and NTK , so I 'm wondering that are there any upper/lower bounds for the Eluder dimension of the two non-parametric classes described in this paper ? Besides that , the regret upper bound given by Theorem 4.3 and 4.7 depends on parameter $ \\gamma $ . In what scenario the parameter $ \\gamma $ is lower bounded by some constant ? Minor issue : - In Eq . ( 3.1 ) , Page 3 : should the policy be $ \\pi_h^k ( a\\mid s ) =\\frac { \\exp ( E_h^k ( s , a ) ) } { \\sum \\exp ( E_h^k ( s , a ) ) } $ ? Overall , the results are solid and interesting . Therefore I would recommend a acceptance .", "rating": "7: Good paper, accept", "reply_text": "We appreciate the valuable review and suggestions . We have revised our work accordingly . In what follows , we address your concerns in detail . 1 . ( implementation of the algorithm ) Yes , we will need another parametrization to store the updated Q-functions , which will be trained through least square regressions to fit the target . We note that in such a regression problem , we have as much training data as we need , and thus the error can usually be reduced to any sufficient accuracy , for example , using a non-parametric model . The computational error , if considered , can be trivially separated as a part of the regret and will be dominated by the terms in our main theorems given enough computational power . 2 . ( typos ) Yes , equation ( 3.1 ) should be written in that form . We have corrected it in the revised pdf ."}}