{"year": "2021", "forum": "8Xi5MLFE_IW", "title": "Episodic Memory for Learning Subjective-Timescale Models", "decision": "Reject", "meta_review": "This paper uses a free-energy formulation to develop an approach to learning \"jumpy\" transition models, which predict surprising future states. This transition model is used in combination with MCTS and applied to a scavenging task in the Animal AI Olympics, outperforming two baselines.\n\nWhile the reviewers praised the importance of the problem tackled, and the novelty of using a free energy approach, there was a general sense amongst the reviewers that the paper wasn't totally clear (especially for an RL audience). R1 also felt that some of the claims of the paper weren't sufficiently evaluated enough, and several reviewers indicated that they felt the baselines were insufficient (or, at a minimum, not described in enough detail to evaluate whether they were sufficient). Given these points, I feel the paper is not quite ready for publication at ICLR. I encourage the authors to flesh out their analysis a bit more, better describe the baselines (and possibly compare to other existing approaches as mentioned by R4), and overall to frame the paper a bit more for the RL community.\n\nOne additional reference the authors may be interested in: Gregor et al (2018). Temporal difference variational auto-encoder.", "reviews": [{"review_id": "8Xi5MLFE_IW-0", "review_text": "Summary : Most model-based RL algorithms learn dynamics models that predicts the next timestep . However , because of model-bias , frequency of timesteps , and objective timescales , the dynamics models can accumulate errors and limited by timescales . The authors propose subjective-timescale model ( STM ) that instead of predicting the next timesteps they find the `` surprising '' subsequences of the trajectories and learn temporal-skipping dynamics models over them . The paper shows the improvement over single-step prediction baselines in a first-person navigation domain . Pros : The method aims to address a very important problem in model-based RL . The idea of using variational free energy with model-based RL seems novel to me , and has not been widely explored . The qualitative visualization ( figure 4 and 5 ) provides a nice understanding of what the method is doing as well as what it is capable of in first-person navigation . Cons : -- Methods -- The main sections do not contain sufficient information regarding how the actions are obtained from learned STM models . I find one paragraph in section 3 in sufficient . The experimental sections also do not mention how MCTS and MPC baselines differ . Please clarify . Also , how do you recover low-level action sequences from the aggregated actions after MCTS ? I do not find the answer from the paper . The keyframe selection method requires more justification . It is unclear how using the KL divergence for measuring surprise will improve over model 's prediction error studied in previous work . It would be amazing to provide a theoretical justification for the heurstics toward , e.g. , saying something about the end task performance , if possible . How sensitive is it to the KL threshold ? Please provide this study . The action sequence aggregation is domain specific which seems a bit unfair to compare against the baselines which * * do not * * have access to the same information . There should be more baselines or ablation studies to disentangle the improvement of the method from this domain-specific assumption . The paper uses different indexing styles which make the method more confusing than it should have been . Please choose one between indexing tau or arithematics on tau . -- Experiments -- As metioned briefly before , more baselines or ablation will be critical to judge the importance of the proposed model ? What about compare against other sliency approaches such as prediction error for memory accumulation ? Also , it would be helpful to have more than 1 environment to show the genrality of the approach . The experimental results do not provide enough information to understand what tasks can be solved and what can not be solved in Animal AI environment . Can the we provide the success rates and categorized by difficulty levels ? These information will be helpful in understanding what STM can and can not do . Perhaps , having a link to some videos will also help . Figure 3 shows that the reward is going up ; how far can the rward go ? It is still increasing . In figure 3 , how is the reward computed ? In figure 5 , is there a groundturth trajectory comparison ? -- Others -- Personally , I would appreciate more background and intuition on each term in the variational free energy formula . Conclusion : Again , I believe that this work is addressing a very important question with an interesting idea , but it may require a little bit more work to make the case . I appreciate the authors thinking about this problem , and hope the authors are encouraged to continue their work .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer3 , Thank you for your review . We are happy to address the concerns you outlined above . * * Q1 * * : * \u201c The main sections do not contain sufficient information regarding how the actions are obtained from learned STM models. \u201d * + * \u201c how MCTS and MPC baselines differ \u201d * * * A * * : We understand your concern here . We will extend these explanations in the next version of the manuscript . * * Q2 * * : * \u201c how do you recover low-level action sequences \u201d * * * A * * : Our model does not involve a hierarchical approach for retrieving action sequences . While we acknowledge that this may seem as a disadvantage ( since there is no one-to-one correspondence in the predicted future states and the ground-truth states ) , the retrieval of low-level sequences of actions is not necessary to observe a visible improvement in the system \u2019 s performance . As mentioned in another response , we are now working on more sophisticated methods for retrieving useful long-term action sequences and hope this will improve the results further . * * Q3 * * : * \u201c It is unclear how using the KL divergence for measuring surprise will improve over model 's prediction error studied in previous work. \u201d * * * A * * : Could you please let us know some of the exact references you are referring to so we can better investigate this point . As we understand it , KL divergence ( in this case , variational free energy of the dynamics model ) can also be considered to be the prediction error produced by the probabilistic transition dynamics model . Within active inference , it is used to quantify a measure of belief updating and thus , loosely speaking , quantifies the prediction error , as well . As we mentioned in the paper , empirical evidence from neuroscience suggests that prediction error and event saliency is one of the decisive factors on whether an episodic memory will be formed . * * Q4 * * : * \u201c How sensitive is it to the KL threshold ? \u201d * * * A * * : Thank you for raising this point , we agree with you that more should be said about the threshold value choice . Currently , the threshold was treated as a hyperparameter of the model and was manually chosen by inspecting the distribution of transition model surprise values . * * Q5 * * : * \u201c unfair to compare against the baselines which do not have access to the same information. \u201d * * * A * * : We are not entirely sure what you mean by this . Both the baseline and the STM agents have access to exactly the same amount of information . The action sequence aggregation is formed out of the actions performed by the agent , which the objective timescale agent also has access to ( though of course stored in the objective timescale ) . * * Q6 * * : * \u201c it would be helpful to have more than 1 environment to show the generality of the approach. \u201d * * * A * * : We certainly agree with this . However , we believe that the presented results do show that the idea is sound and strongly suggest that it has potential in other domains . We are currently working on further experiments across more benchmarks/environments . To be continued in the next post ..."}, {"review_id": "8Xi5MLFE_IW-1", "review_text": "Summary : The authors propose to train a model not on the objective time-scale of a sequence of frames , but on the subjective time-scale dictated by how surprising events are ( where surprise here is defined as being above a certain energy free threshold ) . The trained action-conditioned model learns to slow down time for complex scenes , and fast forward when things are easily predicted . The overall topic is an important one , most model based methods suffer from accumulating errors . The introduction is well written and offers a strong motivation for the rest of the paper . I like the explanation in terms of a distinction between objective and subjective perception of time and events . There are lots and lots of references to work by Friston et al. , but I am not sure I see the connection with active inference as being that strong or even necessary . Regarding the part \u201c Furthermore , for long-term predictions STM systematically performs temporal jumps ( skipping intermediary steps ) , thus providing more informative future predictions and reducing the detrimental effects of one-step prediction error accumulation. \u201d There are two potentially relevant references ( Neitz et al NeurIPS 2018 ) , and ( Darajaman et al , ICML 2019 ) , as both learn models ( not necessarily action-conditioned though ) that can skip an adaptive number of steps into the future , with similar consequences ( i.e.preventing error accumulation and increasing rollout speed ) . P4 : \u201c The habitual network acts as a model-free component of the system , learning to map inferred states directly to actions \u201d . Shouldn \u2019 t $ q ( a_t ; \u03c6_a ) $ be a function of $ s $ as well ? P6 : The angle heuristic might deserve a more in-depth discussion . What if the agent goes in a circle ? Or does a U turn ? Or a complex sequence of movements in a maze ? Since the goal of STMs is ( at least in part ) to reduce progressively the length of S-sequences ( such that they start spanning longer and longer horizons ) , the actions that bridge two episodic memory need to be summarised in a way that is expressive enough . Could one think of applying the same STM principle that is already applied to states , to actions as well ? \u201c Importantly , [ the ] function $ f_ { \\theta_ { s } } $ is deterministic and serves only to encode information about preceding\u2026 \u201d . Where does $ f_ { \\theta_ { s } } $ appear in this context ? I can \u2019 t see it anywhere between eq.4 and 6 This AAI environment is not so established . Perhaps the author could add ( even in the appendix ) a top view of a typical setting that corresponds to one episode ? Also , it appears that the setting chosen by the authors does not reflect the characteristics that they planned to showcase with their method . Given the presence of one sphere per colour and a relatively small environment , how can we appreciate that the agent learns to do planning over long horizons ? Perhaps an environment with longer horizons would be a more adequate testbed ? I very much liked that the authors showed the additional results in Appendix C , I think they are extremely important for the paper . However , I disagree with the claims made in the text , as it appears that they are not substantiated by the figures they reference . - \u201c Figure 10 : random roll-outs generated by the system . These diverse roll-outs demonstrate that STM is able to : i ) make correct action-conditioned predictions , ii ) speed up its prediction timescale when objects are far away , iii ) slow down the prediction timescale when objects are nearby \u201d If I interpret Figure 10 correctly , it seems to suggest that the imagined roll-outs are not very consistent , as in almost every single case the color of the spheres changes from green to yellow and vice-versa during the rollout . - Figure 11 : several transitions appear not to be realistic , objects appear out of nowhere instead of smoothly while turning . - Figure 12 : while it is true that the baseline model does not generate any sphere , it appears to me that the physics is significantly more consistent than with STM . The last sentence in the conclusion seems to suggest that the model is not progressively expanding its horizon the more it trains . How come ? Overall , my impression is that the paper has a very interesting motivation , but the execution could be significantly improved . The results are not so convincing , due to : 1 ) figures ( 10-11-12 ) that do not soundly corroborate the claims , 2 ) evaluation setting that does not allow to really test the claim ( i.e.not enough opportunities for increasingly longer horizons as the model improves ) 3 ) the lack of established baselines and benchmarks 4 ) the lack of an algorithm box to present the method Minor : - Simplify language where possible ( utilised - > used ; are capable of - > can ; etc . ) - Several figures ( 4,5 , 8 ) are corrupted using preview on a Mac ( not sure if it \u2019 s just my computer ) . I could see them correctly by using Chrome to open the PDF .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer4 , Thank you for your comments and feedback . We would love to address the issues you bring up in your review . * * Q1 * * : * \u201c I am not sure I see the connection with active inference as being that strong or even necessary \u201d * * * A * * : Active inference is a model-based cognitive framework that we chose mainly for the reasons of biological plausibility and its intrinsic Bayesian nature . This , as well , is consistent with the Bayesian predictive processing model for time perception used in Fountas et al . ( 2020 ) [ 1 ] . Regardless , we would kindly want to ask you to clarify a little more on this concern , particularly with respect to the connection with active inference not being \u201c that strong \u201d . * * Q2 * * : * \u201c There are two potentially relevant references [ ... ] \u201d * * * A * * : Thank you for these ! However , we were unable to locate the Darajaman et al.in the proceedings of ICML 2019 . Could you please link the paper you are referencing ? * * Q3 * * : * \u201c Shouldn \u2019 t * $ q ( a_t ; \\phi_a ) $ * be a function of s as well ? \u201d * * * A * * : You are correct in saying that it is also a function of s. Here , however , we follow standard notation from variational inference which drops the conditioning , when denoting the approximate posterior . We chose to stick with the conventions in the literature , but agree that writing it out in full may be more clear . * * Q4 * * : * \u201c The angle heuristic might deserve a more in-depth discussion . What if the agent goes in a circle ? Or does a U turn ? Or a complex sequence of movements in a maze ? \u201d * * * A * * : Absolutely ! We certainly agree with you that the angle heuristic deserves more attention , and we plan to address it more concretely in our future work . The purpose of this paper , however , was to showcase the usefulness of defining a subjective timescale as the top priority . Therefore , we chose a very simple heuristic that was necessary to provide the agent with information to learn action-conditioned predictions . This is not to say that they are perfect and we would expect them to get worse in more complex configurations of the environment ; however , we believe that this heuristic was enough to demonstrate the effectiveness of subjective-timescale models . * * Q5 * * : * \u201c Could one think of applying the same STM principle that is already applied to states , to actions as well ? \u201d * * * A * * : Could you please clarify the suggestion as there are many ways this might be possible , and it 's unclear to us exactly how you are imagining this to work . * * Q6 * * : * \u201c Where does $ f_ { \\theta_s } $ appear in this context ? \u201d * * * A * * : Thank you for pointing it out \u2013 it is a typo and should be $ f_ { \\theta_h } $ , instead . * * Q7 * * : * \u201c Perhaps the author could add ( even in the appendix ) a top view of a typical setting that corresponds to one episode ? \u201d * * * A * * : Definitely , thank you for the suggestion . * * Q8 * * : * \u201c the setting chosen by the authors does not reflect the characteristics that they planned to showcase with their method\u2026 . [ ... ] how can we appreciate that the agent learns to do planning over long horizons ? \u201d * * * A * * : Although it may seem that the environment is small , we argue that the most important bit is the underlying temporal dynamics \u2013 e.g.how much does an agent progress forward given a forward action ? In AAI , it is very slow , requiring the agent to take hundreds of steps ( > 500 ) to go to the opposite side of the sandbox . Furthermore , sparse rewards were chosen deliberately to encourage both long- and short-term planning . This becomes even more challenging , as our agents were only allowed to take 500 steps , after which the environment would terminate and the next configuration would be chosen . Therefore , we believe that the chosen set-up , on the contrary , helps with testing our agents for both short- and long-term planning abilities . Now that the idea has been shown to be successful we plan to address other benchmarks/environments that are likely to display more traditional long-term planning behaviour . To be continued in the next post ... [ 1 ] https : //www.biorxiv.org/content/10.1101/2020.02.17.953133v1"}, {"review_id": "8Xi5MLFE_IW-2", "review_text": "In this paper , the authors describe a variable-timescale prediction model for planning in the context of a deep active inference agent . They show that this agent outperforms a baseline in a scavenging task in a 3D first person environment . They show example rollouts of the baseline and variable-timescale models . Until reading this paper I was n't familiar with deep active inference agents , which apparently enable the extension of free energy methods to more complex settings . It seems like an intriguing alternative to deep RL . It 's not clear to me whether it has the scaling potential that has been demonstrated for deep RL . Although the experiments reported here are larger scale than experiments with active inference systems prior to Fountas et al . ( 2020 ) , they seem to be quite simple in comparison to tasks on which deep RL agents excel ( e.g.typical Vizdoom and DMLab tasks ) . I do n't mean this as a criticism , but instead as a question mark : this difference in demonstrated scale might simply be due to the massive difference in resources that have been devoted to deep RL agents vs active inference agents . As a result , I tried to evaluate the paper on its merits in the context of active inference systems , instead of via comparison with deep RL . As such , I 'd be happy with a convincing demonstration that ( 1 ) the variable timescale model outperforms a strong time-locked model in the scavenging task and ( 2 ) that its performance is due to the selection of useful frames for planning that MCTS then uses effectively . Figure 3 seems to answer ( 1 ) in the affirmative . I 'm basically trusting the authors that theirs is a reasonably strong baseline , since I do n't have experience training active inference agents or with this scavenging environment . This will be reflected in my confidence score . However , ( 2 ) is not substantiated well by the paper 's analysis section . The only evidence for this is in the form of two pairs of example rollouts for the time-locked and variable-time models . Are these cherry picked examples , or are they actually reflective of general trends ? I 'd be much more comfortable if the authors supplied some aggregate results to substantiate this claim : `` As a result , our agent consistently predicts farther into the future in the absence of any nearby objects , and slows its timescale , predicting at finer temporal rate , when the objects are close . '' I 'd really need aggregate results demonstrating that the S-sequences are summarizing long trajectories in a sensible way over a large set over episodes , to feel confident that the system is providing the benefits its purported to . Even better would be to analyze the MCTS search trees to show that the search trajectories over S-seqences have desirable properties . Perhaps more concerningly , the authors say that `` As a result , the STM agent is less prone to get stuck in a sub-optimal state , which was commonly observed in the baseline system , and is more inclined to explore the environment beyond its current position '' . But , as far as I can tell , there 's no evidence in the main paper for this . One concrete concern is that the time-locked agent fails simply because it 's rollouts are n't long enough to find the rewarding object . Maybe it would be sufficient to simply randomly drop out timesteps from the trajectory to reach the STM-MCTS performance level . The current analyses ( and baseline results ) do n't rule out this hypothesis . If the authors can provide stronger evidence on these points , I 'd be very happy to increase my rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your thoughtful feedback \u2013 you raise some important points , which we would be happy to address . * * Q1 * * : * \u201c [ ... ] this difference in demonstrated scale might simply be due to the massive difference in resources that have been devoted to deep RL agents vs active inference agents. \u201d * * * A * * : We certainly agree with this statement and believe that deep active inference research could greatly benefit from more computational resources and efforts of scaling it further . * * Q2 * * : * \u201c I 'm basically trusting the authors that theirs is a reasonably strong baseline \u201d * * * A * * : Our baseline is a deep active inference agent devised by Fountas et al . ( NeurIPS 2020 ) , which was compared to several model-free RL agents , such as DQN , A2C , and PPO2 . We believe it can be considered the current state of the art for deep active inference agents . Furthermore , the parameters of this baseline were tuned to yield better performance . We would also like to stress that the only difference between the baseline and our STM agent is the transition model . As such , we can much more confidently attribute the improvement in the performance to the subjective-timescale model . Moving on , you made several points about being uncertain whether the STM agent \u2019 s better performance is indeed due to the subjective-timescale model . Accordingly , we plan to address these in the next version of the manuscript and we thank you for pointing it out . Nevertheless , in response to your concern , we again emphasise that the baseline and STM agent share the weights of all the networks , except for the transition dynamics model . This was done deliberately to address the exact point that you raise . * * Q3 * * : * \u201c Are these cherry picked examples , or are they actually reflective of general trends ? \u201d * * * A * * : These are indeed the general trends that we observed by performing random roll-outs in a variety of different settings . For more examples of random roll-outs , please see Figures 10 and 11 in the Appendix . Nevertheless , we believe your concerns about the analysis of the results , such as demonstrating * \u201c aggregate results \u201d * and * \u201c analyz [ ing ] the MCTS search trees \u201d * , are well-justified . We are currently working towards creating more convincing quantitative and qualitative metrics , by which these improvements could be judged . * * Q4 * * : * \u201c [ With regards to baseline agents getting stuck in sub-optimal states ] , there 's no evidence in the main paper for this. \u201d * * * A * * : This observation was based on numerous runs we performed and analysed qualitatively . We can find ways to support this observation more quantitatively in the next version . * * Q5 * * : Further , you mention that the hypothesis that * \u201c [ ... ] the time-locked agent fails simply because it 's rollouts are n't long enough to find the rewarding object \u201d * is not ruled out by the analysis . * * A * * : There are several points to address here : ( 1 ) We did experiment with both longer and shorter roll-outs for the time-locked agent , and found that performing longer roll-outs does not result in a better performance \u2013 likely related to the problem of error accumulation for one-step prediction models ( as can be observed in Figure 4 ) . ( 2 ) Time-locked models can indeed yield roll-outs that are not long enough ; however , it is exactly one of the issues that our STM model can effectively address \u2013 but do so without resorting to any explicit mechanisms of varying the temporal extent of predictions . ( 3 ) Longer roll-outs result in higher computational complexity of the planning process \u2013 something that is addressed via the subjective-timescale modelling . ( 4 ) As shown in Figures 5 and 11 , STM agents can additionally imagine objects ( affordances that would allow for optimal minimisation of the free energy ) , which is in stark contrast to the objective-timescale agent in Figures 5 and 12 . We argue that this systematic characteristic allows the STM agent to have richer information about its environment ( potential affordances ) , encouraging imagination-driven exploration , while the time-locked agent is deprived of such ability . We again thank you for your useful feedback , and we will continue working on improving our paper in the meantime . Best regards ."}], "0": {"review_id": "8Xi5MLFE_IW-0", "review_text": "Summary : Most model-based RL algorithms learn dynamics models that predicts the next timestep . However , because of model-bias , frequency of timesteps , and objective timescales , the dynamics models can accumulate errors and limited by timescales . The authors propose subjective-timescale model ( STM ) that instead of predicting the next timesteps they find the `` surprising '' subsequences of the trajectories and learn temporal-skipping dynamics models over them . The paper shows the improvement over single-step prediction baselines in a first-person navigation domain . Pros : The method aims to address a very important problem in model-based RL . The idea of using variational free energy with model-based RL seems novel to me , and has not been widely explored . The qualitative visualization ( figure 4 and 5 ) provides a nice understanding of what the method is doing as well as what it is capable of in first-person navigation . Cons : -- Methods -- The main sections do not contain sufficient information regarding how the actions are obtained from learned STM models . I find one paragraph in section 3 in sufficient . The experimental sections also do not mention how MCTS and MPC baselines differ . Please clarify . Also , how do you recover low-level action sequences from the aggregated actions after MCTS ? I do not find the answer from the paper . The keyframe selection method requires more justification . It is unclear how using the KL divergence for measuring surprise will improve over model 's prediction error studied in previous work . It would be amazing to provide a theoretical justification for the heurstics toward , e.g. , saying something about the end task performance , if possible . How sensitive is it to the KL threshold ? Please provide this study . The action sequence aggregation is domain specific which seems a bit unfair to compare against the baselines which * * do not * * have access to the same information . There should be more baselines or ablation studies to disentangle the improvement of the method from this domain-specific assumption . The paper uses different indexing styles which make the method more confusing than it should have been . Please choose one between indexing tau or arithematics on tau . -- Experiments -- As metioned briefly before , more baselines or ablation will be critical to judge the importance of the proposed model ? What about compare against other sliency approaches such as prediction error for memory accumulation ? Also , it would be helpful to have more than 1 environment to show the genrality of the approach . The experimental results do not provide enough information to understand what tasks can be solved and what can not be solved in Animal AI environment . Can the we provide the success rates and categorized by difficulty levels ? These information will be helpful in understanding what STM can and can not do . Perhaps , having a link to some videos will also help . Figure 3 shows that the reward is going up ; how far can the rward go ? It is still increasing . In figure 3 , how is the reward computed ? In figure 5 , is there a groundturth trajectory comparison ? -- Others -- Personally , I would appreciate more background and intuition on each term in the variational free energy formula . Conclusion : Again , I believe that this work is addressing a very important question with an interesting idea , but it may require a little bit more work to make the case . I appreciate the authors thinking about this problem , and hope the authors are encouraged to continue their work .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer3 , Thank you for your review . We are happy to address the concerns you outlined above . * * Q1 * * : * \u201c The main sections do not contain sufficient information regarding how the actions are obtained from learned STM models. \u201d * + * \u201c how MCTS and MPC baselines differ \u201d * * * A * * : We understand your concern here . We will extend these explanations in the next version of the manuscript . * * Q2 * * : * \u201c how do you recover low-level action sequences \u201d * * * A * * : Our model does not involve a hierarchical approach for retrieving action sequences . While we acknowledge that this may seem as a disadvantage ( since there is no one-to-one correspondence in the predicted future states and the ground-truth states ) , the retrieval of low-level sequences of actions is not necessary to observe a visible improvement in the system \u2019 s performance . As mentioned in another response , we are now working on more sophisticated methods for retrieving useful long-term action sequences and hope this will improve the results further . * * Q3 * * : * \u201c It is unclear how using the KL divergence for measuring surprise will improve over model 's prediction error studied in previous work. \u201d * * * A * * : Could you please let us know some of the exact references you are referring to so we can better investigate this point . As we understand it , KL divergence ( in this case , variational free energy of the dynamics model ) can also be considered to be the prediction error produced by the probabilistic transition dynamics model . Within active inference , it is used to quantify a measure of belief updating and thus , loosely speaking , quantifies the prediction error , as well . As we mentioned in the paper , empirical evidence from neuroscience suggests that prediction error and event saliency is one of the decisive factors on whether an episodic memory will be formed . * * Q4 * * : * \u201c How sensitive is it to the KL threshold ? \u201d * * * A * * : Thank you for raising this point , we agree with you that more should be said about the threshold value choice . Currently , the threshold was treated as a hyperparameter of the model and was manually chosen by inspecting the distribution of transition model surprise values . * * Q5 * * : * \u201c unfair to compare against the baselines which do not have access to the same information. \u201d * * * A * * : We are not entirely sure what you mean by this . Both the baseline and the STM agents have access to exactly the same amount of information . The action sequence aggregation is formed out of the actions performed by the agent , which the objective timescale agent also has access to ( though of course stored in the objective timescale ) . * * Q6 * * : * \u201c it would be helpful to have more than 1 environment to show the generality of the approach. \u201d * * * A * * : We certainly agree with this . However , we believe that the presented results do show that the idea is sound and strongly suggest that it has potential in other domains . We are currently working on further experiments across more benchmarks/environments . To be continued in the next post ..."}, "1": {"review_id": "8Xi5MLFE_IW-1", "review_text": "Summary : The authors propose to train a model not on the objective time-scale of a sequence of frames , but on the subjective time-scale dictated by how surprising events are ( where surprise here is defined as being above a certain energy free threshold ) . The trained action-conditioned model learns to slow down time for complex scenes , and fast forward when things are easily predicted . The overall topic is an important one , most model based methods suffer from accumulating errors . The introduction is well written and offers a strong motivation for the rest of the paper . I like the explanation in terms of a distinction between objective and subjective perception of time and events . There are lots and lots of references to work by Friston et al. , but I am not sure I see the connection with active inference as being that strong or even necessary . Regarding the part \u201c Furthermore , for long-term predictions STM systematically performs temporal jumps ( skipping intermediary steps ) , thus providing more informative future predictions and reducing the detrimental effects of one-step prediction error accumulation. \u201d There are two potentially relevant references ( Neitz et al NeurIPS 2018 ) , and ( Darajaman et al , ICML 2019 ) , as both learn models ( not necessarily action-conditioned though ) that can skip an adaptive number of steps into the future , with similar consequences ( i.e.preventing error accumulation and increasing rollout speed ) . P4 : \u201c The habitual network acts as a model-free component of the system , learning to map inferred states directly to actions \u201d . Shouldn \u2019 t $ q ( a_t ; \u03c6_a ) $ be a function of $ s $ as well ? P6 : The angle heuristic might deserve a more in-depth discussion . What if the agent goes in a circle ? Or does a U turn ? Or a complex sequence of movements in a maze ? Since the goal of STMs is ( at least in part ) to reduce progressively the length of S-sequences ( such that they start spanning longer and longer horizons ) , the actions that bridge two episodic memory need to be summarised in a way that is expressive enough . Could one think of applying the same STM principle that is already applied to states , to actions as well ? \u201c Importantly , [ the ] function $ f_ { \\theta_ { s } } $ is deterministic and serves only to encode information about preceding\u2026 \u201d . Where does $ f_ { \\theta_ { s } } $ appear in this context ? I can \u2019 t see it anywhere between eq.4 and 6 This AAI environment is not so established . Perhaps the author could add ( even in the appendix ) a top view of a typical setting that corresponds to one episode ? Also , it appears that the setting chosen by the authors does not reflect the characteristics that they planned to showcase with their method . Given the presence of one sphere per colour and a relatively small environment , how can we appreciate that the agent learns to do planning over long horizons ? Perhaps an environment with longer horizons would be a more adequate testbed ? I very much liked that the authors showed the additional results in Appendix C , I think they are extremely important for the paper . However , I disagree with the claims made in the text , as it appears that they are not substantiated by the figures they reference . - \u201c Figure 10 : random roll-outs generated by the system . These diverse roll-outs demonstrate that STM is able to : i ) make correct action-conditioned predictions , ii ) speed up its prediction timescale when objects are far away , iii ) slow down the prediction timescale when objects are nearby \u201d If I interpret Figure 10 correctly , it seems to suggest that the imagined roll-outs are not very consistent , as in almost every single case the color of the spheres changes from green to yellow and vice-versa during the rollout . - Figure 11 : several transitions appear not to be realistic , objects appear out of nowhere instead of smoothly while turning . - Figure 12 : while it is true that the baseline model does not generate any sphere , it appears to me that the physics is significantly more consistent than with STM . The last sentence in the conclusion seems to suggest that the model is not progressively expanding its horizon the more it trains . How come ? Overall , my impression is that the paper has a very interesting motivation , but the execution could be significantly improved . The results are not so convincing , due to : 1 ) figures ( 10-11-12 ) that do not soundly corroborate the claims , 2 ) evaluation setting that does not allow to really test the claim ( i.e.not enough opportunities for increasingly longer horizons as the model improves ) 3 ) the lack of established baselines and benchmarks 4 ) the lack of an algorithm box to present the method Minor : - Simplify language where possible ( utilised - > used ; are capable of - > can ; etc . ) - Several figures ( 4,5 , 8 ) are corrupted using preview on a Mac ( not sure if it \u2019 s just my computer ) . I could see them correctly by using Chrome to open the PDF .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer4 , Thank you for your comments and feedback . We would love to address the issues you bring up in your review . * * Q1 * * : * \u201c I am not sure I see the connection with active inference as being that strong or even necessary \u201d * * * A * * : Active inference is a model-based cognitive framework that we chose mainly for the reasons of biological plausibility and its intrinsic Bayesian nature . This , as well , is consistent with the Bayesian predictive processing model for time perception used in Fountas et al . ( 2020 ) [ 1 ] . Regardless , we would kindly want to ask you to clarify a little more on this concern , particularly with respect to the connection with active inference not being \u201c that strong \u201d . * * Q2 * * : * \u201c There are two potentially relevant references [ ... ] \u201d * * * A * * : Thank you for these ! However , we were unable to locate the Darajaman et al.in the proceedings of ICML 2019 . Could you please link the paper you are referencing ? * * Q3 * * : * \u201c Shouldn \u2019 t * $ q ( a_t ; \\phi_a ) $ * be a function of s as well ? \u201d * * * A * * : You are correct in saying that it is also a function of s. Here , however , we follow standard notation from variational inference which drops the conditioning , when denoting the approximate posterior . We chose to stick with the conventions in the literature , but agree that writing it out in full may be more clear . * * Q4 * * : * \u201c The angle heuristic might deserve a more in-depth discussion . What if the agent goes in a circle ? Or does a U turn ? Or a complex sequence of movements in a maze ? \u201d * * * A * * : Absolutely ! We certainly agree with you that the angle heuristic deserves more attention , and we plan to address it more concretely in our future work . The purpose of this paper , however , was to showcase the usefulness of defining a subjective timescale as the top priority . Therefore , we chose a very simple heuristic that was necessary to provide the agent with information to learn action-conditioned predictions . This is not to say that they are perfect and we would expect them to get worse in more complex configurations of the environment ; however , we believe that this heuristic was enough to demonstrate the effectiveness of subjective-timescale models . * * Q5 * * : * \u201c Could one think of applying the same STM principle that is already applied to states , to actions as well ? \u201d * * * A * * : Could you please clarify the suggestion as there are many ways this might be possible , and it 's unclear to us exactly how you are imagining this to work . * * Q6 * * : * \u201c Where does $ f_ { \\theta_s } $ appear in this context ? \u201d * * * A * * : Thank you for pointing it out \u2013 it is a typo and should be $ f_ { \\theta_h } $ , instead . * * Q7 * * : * \u201c Perhaps the author could add ( even in the appendix ) a top view of a typical setting that corresponds to one episode ? \u201d * * * A * * : Definitely , thank you for the suggestion . * * Q8 * * : * \u201c the setting chosen by the authors does not reflect the characteristics that they planned to showcase with their method\u2026 . [ ... ] how can we appreciate that the agent learns to do planning over long horizons ? \u201d * * * A * * : Although it may seem that the environment is small , we argue that the most important bit is the underlying temporal dynamics \u2013 e.g.how much does an agent progress forward given a forward action ? In AAI , it is very slow , requiring the agent to take hundreds of steps ( > 500 ) to go to the opposite side of the sandbox . Furthermore , sparse rewards were chosen deliberately to encourage both long- and short-term planning . This becomes even more challenging , as our agents were only allowed to take 500 steps , after which the environment would terminate and the next configuration would be chosen . Therefore , we believe that the chosen set-up , on the contrary , helps with testing our agents for both short- and long-term planning abilities . Now that the idea has been shown to be successful we plan to address other benchmarks/environments that are likely to display more traditional long-term planning behaviour . To be continued in the next post ... [ 1 ] https : //www.biorxiv.org/content/10.1101/2020.02.17.953133v1"}, "2": {"review_id": "8Xi5MLFE_IW-2", "review_text": "In this paper , the authors describe a variable-timescale prediction model for planning in the context of a deep active inference agent . They show that this agent outperforms a baseline in a scavenging task in a 3D first person environment . They show example rollouts of the baseline and variable-timescale models . Until reading this paper I was n't familiar with deep active inference agents , which apparently enable the extension of free energy methods to more complex settings . It seems like an intriguing alternative to deep RL . It 's not clear to me whether it has the scaling potential that has been demonstrated for deep RL . Although the experiments reported here are larger scale than experiments with active inference systems prior to Fountas et al . ( 2020 ) , they seem to be quite simple in comparison to tasks on which deep RL agents excel ( e.g.typical Vizdoom and DMLab tasks ) . I do n't mean this as a criticism , but instead as a question mark : this difference in demonstrated scale might simply be due to the massive difference in resources that have been devoted to deep RL agents vs active inference agents . As a result , I tried to evaluate the paper on its merits in the context of active inference systems , instead of via comparison with deep RL . As such , I 'd be happy with a convincing demonstration that ( 1 ) the variable timescale model outperforms a strong time-locked model in the scavenging task and ( 2 ) that its performance is due to the selection of useful frames for planning that MCTS then uses effectively . Figure 3 seems to answer ( 1 ) in the affirmative . I 'm basically trusting the authors that theirs is a reasonably strong baseline , since I do n't have experience training active inference agents or with this scavenging environment . This will be reflected in my confidence score . However , ( 2 ) is not substantiated well by the paper 's analysis section . The only evidence for this is in the form of two pairs of example rollouts for the time-locked and variable-time models . Are these cherry picked examples , or are they actually reflective of general trends ? I 'd be much more comfortable if the authors supplied some aggregate results to substantiate this claim : `` As a result , our agent consistently predicts farther into the future in the absence of any nearby objects , and slows its timescale , predicting at finer temporal rate , when the objects are close . '' I 'd really need aggregate results demonstrating that the S-sequences are summarizing long trajectories in a sensible way over a large set over episodes , to feel confident that the system is providing the benefits its purported to . Even better would be to analyze the MCTS search trees to show that the search trajectories over S-seqences have desirable properties . Perhaps more concerningly , the authors say that `` As a result , the STM agent is less prone to get stuck in a sub-optimal state , which was commonly observed in the baseline system , and is more inclined to explore the environment beyond its current position '' . But , as far as I can tell , there 's no evidence in the main paper for this . One concrete concern is that the time-locked agent fails simply because it 's rollouts are n't long enough to find the rewarding object . Maybe it would be sufficient to simply randomly drop out timesteps from the trajectory to reach the STM-MCTS performance level . The current analyses ( and baseline results ) do n't rule out this hypothesis . If the authors can provide stronger evidence on these points , I 'd be very happy to increase my rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your thoughtful feedback \u2013 you raise some important points , which we would be happy to address . * * Q1 * * : * \u201c [ ... ] this difference in demonstrated scale might simply be due to the massive difference in resources that have been devoted to deep RL agents vs active inference agents. \u201d * * * A * * : We certainly agree with this statement and believe that deep active inference research could greatly benefit from more computational resources and efforts of scaling it further . * * Q2 * * : * \u201c I 'm basically trusting the authors that theirs is a reasonably strong baseline \u201d * * * A * * : Our baseline is a deep active inference agent devised by Fountas et al . ( NeurIPS 2020 ) , which was compared to several model-free RL agents , such as DQN , A2C , and PPO2 . We believe it can be considered the current state of the art for deep active inference agents . Furthermore , the parameters of this baseline were tuned to yield better performance . We would also like to stress that the only difference between the baseline and our STM agent is the transition model . As such , we can much more confidently attribute the improvement in the performance to the subjective-timescale model . Moving on , you made several points about being uncertain whether the STM agent \u2019 s better performance is indeed due to the subjective-timescale model . Accordingly , we plan to address these in the next version of the manuscript and we thank you for pointing it out . Nevertheless , in response to your concern , we again emphasise that the baseline and STM agent share the weights of all the networks , except for the transition dynamics model . This was done deliberately to address the exact point that you raise . * * Q3 * * : * \u201c Are these cherry picked examples , or are they actually reflective of general trends ? \u201d * * * A * * : These are indeed the general trends that we observed by performing random roll-outs in a variety of different settings . For more examples of random roll-outs , please see Figures 10 and 11 in the Appendix . Nevertheless , we believe your concerns about the analysis of the results , such as demonstrating * \u201c aggregate results \u201d * and * \u201c analyz [ ing ] the MCTS search trees \u201d * , are well-justified . We are currently working towards creating more convincing quantitative and qualitative metrics , by which these improvements could be judged . * * Q4 * * : * \u201c [ With regards to baseline agents getting stuck in sub-optimal states ] , there 's no evidence in the main paper for this. \u201d * * * A * * : This observation was based on numerous runs we performed and analysed qualitatively . We can find ways to support this observation more quantitatively in the next version . * * Q5 * * : Further , you mention that the hypothesis that * \u201c [ ... ] the time-locked agent fails simply because it 's rollouts are n't long enough to find the rewarding object \u201d * is not ruled out by the analysis . * * A * * : There are several points to address here : ( 1 ) We did experiment with both longer and shorter roll-outs for the time-locked agent , and found that performing longer roll-outs does not result in a better performance \u2013 likely related to the problem of error accumulation for one-step prediction models ( as can be observed in Figure 4 ) . ( 2 ) Time-locked models can indeed yield roll-outs that are not long enough ; however , it is exactly one of the issues that our STM model can effectively address \u2013 but do so without resorting to any explicit mechanisms of varying the temporal extent of predictions . ( 3 ) Longer roll-outs result in higher computational complexity of the planning process \u2013 something that is addressed via the subjective-timescale modelling . ( 4 ) As shown in Figures 5 and 11 , STM agents can additionally imagine objects ( affordances that would allow for optimal minimisation of the free energy ) , which is in stark contrast to the objective-timescale agent in Figures 5 and 12 . We argue that this systematic characteristic allows the STM agent to have richer information about its environment ( potential affordances ) , encouraging imagination-driven exploration , while the time-locked agent is deprived of such ability . We again thank you for your useful feedback , and we will continue working on improving our paper in the meantime . Best regards ."}}