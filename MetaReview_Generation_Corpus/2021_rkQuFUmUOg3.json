{"year": "2021", "forum": "rkQuFUmUOg3", "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets", "decision": "Accept (Poster)", "meta_review": "The authors proposed a meta learning framework for NAS, namely MetaD2A (Meta Dataset-to-Architecture), that can stochastically generate graphs (architectures) from a given set (dataset) via a dataset-architecture latent space learned with amortized meta-learning. Each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder. MetaD2A is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. While the set encoder and graph decoder for NAS have been introduced by existing work, the main contribution of the paper is to show that the meta-learning of a \"dataset-conditioned architecture generation\" framework can enable fast generation of a good architecture without training on the target dataset. The proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of real-world problems. I strongly encourage the authors to include experiments on a larger pool of architectures than the NAS-Bench-201 search space to show the strength of their proposed method in generating good architectures. While training MetaD2A with pairs of MetaImageNet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesn't show that it can successfully meta-learn to produce better architectures for a new task from an existing pool of good architectures. \n\nWe believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well (e.g., R3, in his last post, seems to suggest that his review should be updated but it has not happened).", "reviews": [{"review_id": "rkQuFUmUOg3-0", "review_text": "The authors propose a novel framework named MetaD2A . Their motivation is interesting to me and they clarify their difference compared with traditional metaNAS in Figure 1 . There are mainly three components in MetaD2A : a set encoder , a graph decoder and a meta-performance predictor . I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets , while the components are all proposed before in different NAS scenarios . In summary , the paper has following drawbacks that need to be further concerned : 1 . Since the predictor consists of two linear layers , we can not take architecture with different nodes as input . Thus , it will limit the generalization ability of the whole algorithm . Moreover , there are also different kinds of performance predictors in NAS field like LSTM and GCN predictor [ 1 ] . And I prefer to see the effect of predictor part with different predictors . 2.In Table 1 , MetaD2A is pretrained on Meta-ImageNet while other baselines are trained from the scratch . I think this is unfair to compare them . It 's better to compare MetaD2A with some MetaNAS methods listed in related work part such as [ 2 ] . In my view , I think MetaD2A pays more emphasis on meta-learning in NAS field . 2.In ablation study section , they only compare MetaD2A with random , which is a somewhat weak baseline . However , the improvement of their method is not significant . I think there are some other perspectives to perform ablation study . For instance , they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component . [ 1 ] Shi , Han , et al . `` Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS . '' Neural Information Processing Systems . 2020 . [ 2 ] Lian , Dongze , et al . `` Towards fast adaptation of neural architectures with meta learning . '' International Conference on Learning Representations . 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * ( 3 ) In ablation study section , they only compare MetaD2A with random , which is a somewhat weak baseline . However , the improvement of their method is not significant . I think there are some other perspectives to perform ablation study . For instance , they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component . * * - We * * do provide the ablation study in Table 5 * * for a total of 4 modules including random ( random , random with predictor , generator without predictor , generator with predictor ) . In addition , we perform the ablation study to analyze components of the input type , the encoder , and the decoder in Table 6 . - Moreover , in ablation study of Table 5 , our method achieves * * 22.16 % * * higher performance over random on aircraft , which we believe is a more than significant improvement . - We * * do provide the ablation study on the flat set encoding in Table 6 * * ( Please see the fourth and the fifth row of Table 6 ) . We can see that the hierarchical set encoding and bi-directional graph encoding both improves the performance of the predictor . We hope that the above responses clear up your confusion . Please let us know if there is anything else we should provide ."}, {"review_id": "rkQuFUmUOg3-1", "review_text": "Pros : 1.This paper proposes new scene , where a meta model is pre-trained on some datasets , and transfer the learned meta feature onto other datasets to do fast adoption . This scene can be applied in variety of domains . 2.The experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance . 3.The paper is well-organized , the paper structure is clear . Cons : 1.The three parts of your model are of little novelty . The dataset encoding part is just borrowed and there is no improvement to adapt NAS tasks . Similar graph decoder is proposed in previous NAS works [ 1 ] and performance predictor are proposed more times . It seems that the proposed framework is just putting existing models together . 2.No comparing to other methods on fast adaptation by NAS such as [ 2 ] . Besides , meta-learning methods may also be compared . 3.According to Figure 8 and Figure 9 , it is likely that your graph decoder can only generate one type of edge connections . Your graph decoder may fail . Since your framework needs other methods ( GDAS / NAS-Bench-201 ) to provide training material . These materials are all good architectures . It is possible that your framework just gives architectures the same as those good architectures rather than using meta features . Your experiment should prove that your model can generate variety of architectures . Overall Review : This paper proposes a new scene of fast adaption of NAS , which may be a good direction of NAS & meta-learning . The paper proposes a framework to generate good architectures according to the datasets . However , the model may need to improved and more experiment need to be done to solve the problems mentioned above . [ 1 ] Does unsupervised architecture representation learning help neural architecture search ? NeurIPS 20 \u2019 [ 2 ] Fast neural network adaptation via parameter remapping and architecture search ICLR 20 \u2019", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * ( 3-1 ) According to Figure 8 and Figure 9 , it is likely that your graph decoder can only generate one type of edge connections . Your graph decoder may fail . * * - This is a misunderstanding and our graph decoder did not fail . We use * * nodes to represent the operation types * * , and thus our graph decoder generates the graph with fixed edges with * * variable node types * * such as the graphs in Figure 8 and Figure 9 . Note that this is different from [ Liu et al.19 and Dong & Yang 20 ] where the operations are represented with edges . Thus our model did generate diverse neural networks with different operations . * * ( 3-2 ) Since your framework needs other methods ( GDAS / NAS-Bench-201 ) to provide training material . These materials are all good architectures . It is possible that your framework just gives architectures the same as those good architectures rather than using meta features . Your experiment should prove that your model can generate variety of architectures . * * - Note that our method is a probabilistic approach and can generate multiple architectures even for a * * single * * dataset , rather than simply retrieving a memorized architecture from the training set . We further performed an additional experiment to show that MetaD2A can generate diverse architectures , instead of retrieving a memorized architecture from the training set . To this end , we generated 10000 neural architecture samples with our MetaD2A on the meta-training set of the MetaImageNet dataset , and measured the quality of the generated architecture by the 1 ) Validity and 2 ) Novelty , following [ Zhang et al.19 ] .- * * 1 ) Validity * * measures the proportion of the neural architectures that are valid ( e.g.graphs with all nodes connected to some other nodes ) . - * * 2 ) Novelty * * measures the proportion of the valid neural architectures that are never seen in the training set . - The result in the Table below shows that MetaD2A can generate * * diverse and novel * * architectures that * * never appeared in the training set * * . # # # # * * The Proportion of Novelty Graphs ( % ) * * | Samples | Validity | Novelty | | : - : | : -- : | : - : | | 10000 | 100 % | 67.31 % | [ Zhang et al.19 ] D-VAE : A Variational autoencoder for directed acyclic graphs . NeurIPS 2019 . We have included this result in the Experiments Section 4.3 of the revision . We appreciate your constructive comments ."}, {"review_id": "rkQuFUmUOg3-2", "review_text": "The authors address neural architecture search ( NAS ) scenarios . In particular , a framework , MetaD2A , is proposed , which yields a neural architecture for a new dataset . In a nutshell , the framework learns a `` dataset-to-neural-network-architecture '' transformation using a database of datasets and architectures . Each dataset is encoded via a `` set encode '' and the architecutres are obtained via a `` graph decoder '' . The experiments demonstrate the usefullness of the approach and its improvements over conventual NAS approaches . The approach could be described Positive : - The results look very solid and indiciate improvements ( time/prediction performance ) over existing approaches - The paper is well written and structured - Additional details ( e.g. , implementation details ) are provided in the appendix Negative : - The authors claim that NAS with meta learning has only been done with small datasets in the past . However , the authors do not really use big datasets as well ( see Appendix C ; the ImageNet subset considered is small as well , if I understand this correctly ) To sum up , I think the work might be a suitable candidate for being accepted at ICLR . I have to admit that this is not precisely an area of my expertise , so I might be missing something .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive comments . We address your comments below : * * Negative : The authors claim that NAS with meta-learning has only been done with small datasets in the past . However , the authors do not really use big datasets as well ( see Appendix C ; the ImageNet subset considered is small as well , if I understand this correctly ) * * - Existing Meta-NAS approaches [ Lian et al.19 , Shaw et al.19 , Elsken et al.20 ] can not scale to such a large task with a large number of samples , since they rely on gradient-based meta-learning with rollout gradient steps . For few-shot learning , taking a few gradient steps is more than enough since there are only a few samples from which we can compute the gradients , but for large-scale learning , taking few gradient steps with minibatch sampling will only consider a very small portion of the samples in the task , which will be largely insufficient for the model to converge . Thus the model needs to take a large number of gradient steps with large tasks ( with a large number of samples ) . However , this is extremely costly when we want to train over a large number of tasks ( we consider * * 1,679 tasks * * to train the MetaD2A generator , and * * 3,752 tasks * * for the meta-performance predictor ) , and is almost infeasible when computing second-order derivatives as in MAML . - On the contrary , MetaD2A can handle the large-way many-shot problem tackled by general classification tasks , since it does not require any rollout gradient steps when inferring the architecture . For example , CIFAR-100 in Table 2 is orders of magnitudes larger than the tasks considered in a few-shot classification ( m-way k-shot classification ) existing Meta-NAS methods target , where the number of samples is $ m\\times { k } $ . For conventional settings ( 5-way 5-shot ) , the number of samples per task is 25 . Contrarily , the number of samples of CIFAR-100 used is * * 100 $ \\times $ 20=2,000 * * , which is * * $ 80 $ times larger * * than the tasks in 5-way 5-shot classification . Further , the number of instances ( e.g.20 ) can be set to a much larger number . Please let us know if there is anything else that you want us to clarify or provide ."}], "0": {"review_id": "rkQuFUmUOg3-0", "review_text": "The authors propose a novel framework named MetaD2A . Their motivation is interesting to me and they clarify their difference compared with traditional metaNAS in Figure 1 . There are mainly three components in MetaD2A : a set encoder , a graph decoder and a meta-performance predictor . I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets , while the components are all proposed before in different NAS scenarios . In summary , the paper has following drawbacks that need to be further concerned : 1 . Since the predictor consists of two linear layers , we can not take architecture with different nodes as input . Thus , it will limit the generalization ability of the whole algorithm . Moreover , there are also different kinds of performance predictors in NAS field like LSTM and GCN predictor [ 1 ] . And I prefer to see the effect of predictor part with different predictors . 2.In Table 1 , MetaD2A is pretrained on Meta-ImageNet while other baselines are trained from the scratch . I think this is unfair to compare them . It 's better to compare MetaD2A with some MetaNAS methods listed in related work part such as [ 2 ] . In my view , I think MetaD2A pays more emphasis on meta-learning in NAS field . 2.In ablation study section , they only compare MetaD2A with random , which is a somewhat weak baseline . However , the improvement of their method is not significant . I think there are some other perspectives to perform ablation study . For instance , they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component . [ 1 ] Shi , Han , et al . `` Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS . '' Neural Information Processing Systems . 2020 . [ 2 ] Lian , Dongze , et al . `` Towards fast adaptation of neural architectures with meta learning . '' International Conference on Learning Representations . 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * ( 3 ) In ablation study section , they only compare MetaD2A with random , which is a somewhat weak baseline . However , the improvement of their method is not significant . I think there are some other perspectives to perform ablation study . For instance , they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component . * * - We * * do provide the ablation study in Table 5 * * for a total of 4 modules including random ( random , random with predictor , generator without predictor , generator with predictor ) . In addition , we perform the ablation study to analyze components of the input type , the encoder , and the decoder in Table 6 . - Moreover , in ablation study of Table 5 , our method achieves * * 22.16 % * * higher performance over random on aircraft , which we believe is a more than significant improvement . - We * * do provide the ablation study on the flat set encoding in Table 6 * * ( Please see the fourth and the fifth row of Table 6 ) . We can see that the hierarchical set encoding and bi-directional graph encoding both improves the performance of the predictor . We hope that the above responses clear up your confusion . Please let us know if there is anything else we should provide ."}, "1": {"review_id": "rkQuFUmUOg3-1", "review_text": "Pros : 1.This paper proposes new scene , where a meta model is pre-trained on some datasets , and transfer the learned meta feature onto other datasets to do fast adoption . This scene can be applied in variety of domains . 2.The experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance . 3.The paper is well-organized , the paper structure is clear . Cons : 1.The three parts of your model are of little novelty . The dataset encoding part is just borrowed and there is no improvement to adapt NAS tasks . Similar graph decoder is proposed in previous NAS works [ 1 ] and performance predictor are proposed more times . It seems that the proposed framework is just putting existing models together . 2.No comparing to other methods on fast adaptation by NAS such as [ 2 ] . Besides , meta-learning methods may also be compared . 3.According to Figure 8 and Figure 9 , it is likely that your graph decoder can only generate one type of edge connections . Your graph decoder may fail . Since your framework needs other methods ( GDAS / NAS-Bench-201 ) to provide training material . These materials are all good architectures . It is possible that your framework just gives architectures the same as those good architectures rather than using meta features . Your experiment should prove that your model can generate variety of architectures . Overall Review : This paper proposes a new scene of fast adaption of NAS , which may be a good direction of NAS & meta-learning . The paper proposes a framework to generate good architectures according to the datasets . However , the model may need to improved and more experiment need to be done to solve the problems mentioned above . [ 1 ] Does unsupervised architecture representation learning help neural architecture search ? NeurIPS 20 \u2019 [ 2 ] Fast neural network adaptation via parameter remapping and architecture search ICLR 20 \u2019", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * ( 3-1 ) According to Figure 8 and Figure 9 , it is likely that your graph decoder can only generate one type of edge connections . Your graph decoder may fail . * * - This is a misunderstanding and our graph decoder did not fail . We use * * nodes to represent the operation types * * , and thus our graph decoder generates the graph with fixed edges with * * variable node types * * such as the graphs in Figure 8 and Figure 9 . Note that this is different from [ Liu et al.19 and Dong & Yang 20 ] where the operations are represented with edges . Thus our model did generate diverse neural networks with different operations . * * ( 3-2 ) Since your framework needs other methods ( GDAS / NAS-Bench-201 ) to provide training material . These materials are all good architectures . It is possible that your framework just gives architectures the same as those good architectures rather than using meta features . Your experiment should prove that your model can generate variety of architectures . * * - Note that our method is a probabilistic approach and can generate multiple architectures even for a * * single * * dataset , rather than simply retrieving a memorized architecture from the training set . We further performed an additional experiment to show that MetaD2A can generate diverse architectures , instead of retrieving a memorized architecture from the training set . To this end , we generated 10000 neural architecture samples with our MetaD2A on the meta-training set of the MetaImageNet dataset , and measured the quality of the generated architecture by the 1 ) Validity and 2 ) Novelty , following [ Zhang et al.19 ] .- * * 1 ) Validity * * measures the proportion of the neural architectures that are valid ( e.g.graphs with all nodes connected to some other nodes ) . - * * 2 ) Novelty * * measures the proportion of the valid neural architectures that are never seen in the training set . - The result in the Table below shows that MetaD2A can generate * * diverse and novel * * architectures that * * never appeared in the training set * * . # # # # * * The Proportion of Novelty Graphs ( % ) * * | Samples | Validity | Novelty | | : - : | : -- : | : - : | | 10000 | 100 % | 67.31 % | [ Zhang et al.19 ] D-VAE : A Variational autoencoder for directed acyclic graphs . NeurIPS 2019 . We have included this result in the Experiments Section 4.3 of the revision . We appreciate your constructive comments ."}, "2": {"review_id": "rkQuFUmUOg3-2", "review_text": "The authors address neural architecture search ( NAS ) scenarios . In particular , a framework , MetaD2A , is proposed , which yields a neural architecture for a new dataset . In a nutshell , the framework learns a `` dataset-to-neural-network-architecture '' transformation using a database of datasets and architectures . Each dataset is encoded via a `` set encode '' and the architecutres are obtained via a `` graph decoder '' . The experiments demonstrate the usefullness of the approach and its improvements over conventual NAS approaches . The approach could be described Positive : - The results look very solid and indiciate improvements ( time/prediction performance ) over existing approaches - The paper is well written and structured - Additional details ( e.g. , implementation details ) are provided in the appendix Negative : - The authors claim that NAS with meta learning has only been done with small datasets in the past . However , the authors do not really use big datasets as well ( see Appendix C ; the ImageNet subset considered is small as well , if I understand this correctly ) To sum up , I think the work might be a suitable candidate for being accepted at ICLR . I have to admit that this is not precisely an area of my expertise , so I might be missing something .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive comments . We address your comments below : * * Negative : The authors claim that NAS with meta-learning has only been done with small datasets in the past . However , the authors do not really use big datasets as well ( see Appendix C ; the ImageNet subset considered is small as well , if I understand this correctly ) * * - Existing Meta-NAS approaches [ Lian et al.19 , Shaw et al.19 , Elsken et al.20 ] can not scale to such a large task with a large number of samples , since they rely on gradient-based meta-learning with rollout gradient steps . For few-shot learning , taking a few gradient steps is more than enough since there are only a few samples from which we can compute the gradients , but for large-scale learning , taking few gradient steps with minibatch sampling will only consider a very small portion of the samples in the task , which will be largely insufficient for the model to converge . Thus the model needs to take a large number of gradient steps with large tasks ( with a large number of samples ) . However , this is extremely costly when we want to train over a large number of tasks ( we consider * * 1,679 tasks * * to train the MetaD2A generator , and * * 3,752 tasks * * for the meta-performance predictor ) , and is almost infeasible when computing second-order derivatives as in MAML . - On the contrary , MetaD2A can handle the large-way many-shot problem tackled by general classification tasks , since it does not require any rollout gradient steps when inferring the architecture . For example , CIFAR-100 in Table 2 is orders of magnitudes larger than the tasks considered in a few-shot classification ( m-way k-shot classification ) existing Meta-NAS methods target , where the number of samples is $ m\\times { k } $ . For conventional settings ( 5-way 5-shot ) , the number of samples per task is 25 . Contrarily , the number of samples of CIFAR-100 used is * * 100 $ \\times $ 20=2,000 * * , which is * * $ 80 $ times larger * * than the tasks in 5-way 5-shot classification . Further , the number of instances ( e.g.20 ) can be set to a much larger number . Please let us know if there is anything else that you want us to clarify or provide ."}}