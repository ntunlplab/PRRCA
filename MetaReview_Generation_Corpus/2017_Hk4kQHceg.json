{"year": "2017", "forum": "Hk4kQHceg", "title": "Multiplicative LSTM for sequence modelling", "decision": "Invite to Workshop Track", "meta_review": "The paper presents a new way of doing multiplicative / tensored recurrent weights in RNNs. The multiplicative weights are input dependent. Results are presented on language modeling (PTB and Hutter). We found the paper to be clearly written, and the idea well motivated. However, as pointed out by the reviewers, the results were not state of the art. We feel that is that this is because the authors did not make a strong attempt at regularizing the training. Better results on a larger set of tasks would have probably made this paper easier to accept. \n \n Pros:\n - interesting idea, and reasonable results\n Cons:\n - only shown on language modeling tasks\n - results were not very strong, when compared to other methods (which typically used strong regularization and training like batch normalization etc).\n - reviewers did not find the experiments convincing enough, and felt that a fair comparison would be to compare with dynamic weights on the competing RNNs.", "reviews": [{"review_id": "Hk4kQHceg-0", "review_text": "* Brief Summary: This paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear. * Criticisms: - In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper. - The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. - There are some non-standard choices on modifications on the standard algorithms, such as \"l\" parameter of RMSProp and multiplying output gate before the nonlinearity. - The experimental results are only limited to character-level language modeling only. * An Overview of the Review: Pros: - A simple modification that seems to reasonably well in practice. - Well-written. Cons: - Lack of good enough experimental results. - Not enough contributions (almost trivial extension over existing algorithms). - Non-standard modifications over the existing algorithms. [1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864). [2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . While the modifications could be considered trivial , we justify them clearly in the paper , in terms of architectural expressiveness . The presented approach achieves better results than those models that use similar but not the same modifications . Our results are behind recurrent highway networks [ 1 ] and bytenet [ 2 ] on Wikipedia ( without dynamic evaluation ) - we use a much simpler model that is able to achieve competitive results with much less depth , and without any regularization method . Recurrent highway networks actually achieved a similar result to mLSTM ( both 1.42 bits/char on Wikipedia ) before recurrent highway networks were updated to include recurrent batch normalization . Also , both of these results were submitted to arXiv around the same time as our paper was submitted to ICLR . [ 1 ] Zilly et al.Recurrent Highway Networks . arXiv:1607.03474 [ 2 ] Kalchbrenner et al.Neural Machine Translation in Linear Time . arXiv:1610.10099"}, {"review_id": "Hk4kQHceg-1", "review_text": "This paper proposes an extension of the multiplicative RNN [1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM. The paper proposes some interesting tricks, but none of them seems to be very crucial. For instance, in Eq. (16), the authors propose to multiply the output gate inside the activation function in order to alleviate the saturation problem in logistic sigmoid or hyperbolic tangent. Also, the authors share m_t across the inference of different gating units and cell-state candidates, at the end this brings only 1.25 times increase on the number of model parameters. Lastly, the authors use a variant of RMSProp where they add an additional hyper-parameter $\\ell$ and schedule it across the training time. It would be nicer to apply the same tricks to other baseline models and show the improvement with regard to each trick. With the new architectural modification to the LSTM and all the tricks combined, the performance is not as great as we would expect. Why didn\u2019t the authors apply batch normalization, layer normalization or zoneout to their models? Was there any issue with applying one of those regularization or optimization techniques? At the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading. I find it a bit hard to connect dynamic evaluation as a variant of fast weights. Fast weights do not use test error signal. In the paper, the authors claim that \u201cdynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted\u201d, and I am afraid to tell that this assumption is very misleading. We should never assume that test label information is given at the inference time. The test label information is there to evaluate the generalization performance of the model. In some applications, we may get the label information at test time, e.g., stock prediction, weather forecasting, however, in many other applications, we don\u2019t. For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting. Also, it would be fair to apply dynamic evaluation to all the other baseline models as well to compare with the BPC score 1.19 achieved by the proposed mLSTM. The quality of the work is not that bad, but the novelty of the paper is not that good either. The performance of the proposed model is oftentime worse than other methods, and it is only better when dynamic evaluation is coupled together. However, dynamic evaluation can improve the other methods as well. [1] Ilya et al., \u201cGenerating Text with Recurrent Neural Networks\u201d, ICML\u201911", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . The review contains misunderstandings about the scope of dynamic evaluation . Dynamic evaluation can be applied without the ground truth labels by using labels generated by the model . This approach has been extensively studied in speech recognition and machine translation , through cache-based n-gram language models [ 1,2 ] and dynamic adaptation of recurrent neural networks [ 3 ] . This is similar to how a generative RNN can either use ground truth or generated sequence elements to update its hidden state . If applying dynamic evaluation to the ground truth sequence achieves a lower perplexity , it means that the network has a higher probability of generating the correct sequence from scratch by applying dynamic evaluation to sequence elements generated by the network , and would likely achieve a better word error rate , because it assigns a higher density to the correct answer . We included dynamic evaluation in the paper because we felt it added to the paper 's concept of being able to adjust to unexpected inputs . [ 1 ] Kuhn et al.A cache-based natural language model for speech recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 12 , no.6 , pp.570-583 , Jun 1990 . [ 2 ] Nepveu et al.Adaptive Language and Translation Models for Interactive Machine Translation . EMNLP-2004 . [ 3 ] Kombrink et al.Recurrent Neural Network Based Language Modeling in Meeting Recognition . Interspeech-2011 ."}, {"review_id": "Hk4kQHceg-2", "review_text": "Pros: * Clearly written. * New model mLSTM which seems to be useful according to the results. * Some interesting experiments on big data. Cons: * Number of parameters in comparisons of different models is missing. * mLSTM is behind some other models in most tasks. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Similar to our response to another reviewer , we would point out that mLSTM is a much simpler architectural modification that uses less depth than models that outperform it ."}], "0": {"review_id": "Hk4kQHceg-0", "review_text": "* Brief Summary: This paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear. * Criticisms: - In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper. - The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. - There are some non-standard choices on modifications on the standard algorithms, such as \"l\" parameter of RMSProp and multiplying output gate before the nonlinearity. - The experimental results are only limited to character-level language modeling only. * An Overview of the Review: Pros: - A simple modification that seems to reasonably well in practice. - Well-written. Cons: - Lack of good enough experimental results. - Not enough contributions (almost trivial extension over existing algorithms). - Non-standard modifications over the existing algorithms. [1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864). [2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . While the modifications could be considered trivial , we justify them clearly in the paper , in terms of architectural expressiveness . The presented approach achieves better results than those models that use similar but not the same modifications . Our results are behind recurrent highway networks [ 1 ] and bytenet [ 2 ] on Wikipedia ( without dynamic evaluation ) - we use a much simpler model that is able to achieve competitive results with much less depth , and without any regularization method . Recurrent highway networks actually achieved a similar result to mLSTM ( both 1.42 bits/char on Wikipedia ) before recurrent highway networks were updated to include recurrent batch normalization . Also , both of these results were submitted to arXiv around the same time as our paper was submitted to ICLR . [ 1 ] Zilly et al.Recurrent Highway Networks . arXiv:1607.03474 [ 2 ] Kalchbrenner et al.Neural Machine Translation in Linear Time . arXiv:1610.10099"}, "1": {"review_id": "Hk4kQHceg-1", "review_text": "This paper proposes an extension of the multiplicative RNN [1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM. The paper proposes some interesting tricks, but none of them seems to be very crucial. For instance, in Eq. (16), the authors propose to multiply the output gate inside the activation function in order to alleviate the saturation problem in logistic sigmoid or hyperbolic tangent. Also, the authors share m_t across the inference of different gating units and cell-state candidates, at the end this brings only 1.25 times increase on the number of model parameters. Lastly, the authors use a variant of RMSProp where they add an additional hyper-parameter $\\ell$ and schedule it across the training time. It would be nicer to apply the same tricks to other baseline models and show the improvement with regard to each trick. With the new architectural modification to the LSTM and all the tricks combined, the performance is not as great as we would expect. Why didn\u2019t the authors apply batch normalization, layer normalization or zoneout to their models? Was there any issue with applying one of those regularization or optimization techniques? At the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading. I find it a bit hard to connect dynamic evaluation as a variant of fast weights. Fast weights do not use test error signal. In the paper, the authors claim that \u201cdynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted\u201d, and I am afraid to tell that this assumption is very misleading. We should never assume that test label information is given at the inference time. The test label information is there to evaluate the generalization performance of the model. In some applications, we may get the label information at test time, e.g., stock prediction, weather forecasting, however, in many other applications, we don\u2019t. For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting. Also, it would be fair to apply dynamic evaluation to all the other baseline models as well to compare with the BPC score 1.19 achieved by the proposed mLSTM. The quality of the work is not that bad, but the novelty of the paper is not that good either. The performance of the proposed model is oftentime worse than other methods, and it is only better when dynamic evaluation is coupled together. However, dynamic evaluation can improve the other methods as well. [1] Ilya et al., \u201cGenerating Text with Recurrent Neural Networks\u201d, ICML\u201911", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . The review contains misunderstandings about the scope of dynamic evaluation . Dynamic evaluation can be applied without the ground truth labels by using labels generated by the model . This approach has been extensively studied in speech recognition and machine translation , through cache-based n-gram language models [ 1,2 ] and dynamic adaptation of recurrent neural networks [ 3 ] . This is similar to how a generative RNN can either use ground truth or generated sequence elements to update its hidden state . If applying dynamic evaluation to the ground truth sequence achieves a lower perplexity , it means that the network has a higher probability of generating the correct sequence from scratch by applying dynamic evaluation to sequence elements generated by the network , and would likely achieve a better word error rate , because it assigns a higher density to the correct answer . We included dynamic evaluation in the paper because we felt it added to the paper 's concept of being able to adjust to unexpected inputs . [ 1 ] Kuhn et al.A cache-based natural language model for speech recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 12 , no.6 , pp.570-583 , Jun 1990 . [ 2 ] Nepveu et al.Adaptive Language and Translation Models for Interactive Machine Translation . EMNLP-2004 . [ 3 ] Kombrink et al.Recurrent Neural Network Based Language Modeling in Meeting Recognition . Interspeech-2011 ."}, "2": {"review_id": "Hk4kQHceg-2", "review_text": "Pros: * Clearly written. * New model mLSTM which seems to be useful according to the results. * Some interesting experiments on big data. Cons: * Number of parameters in comparisons of different models is missing. * mLSTM is behind some other models in most tasks. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Similar to our response to another reviewer , we would point out that mLSTM is a much simpler architectural modification that uses less depth than models that outperform it ."}}