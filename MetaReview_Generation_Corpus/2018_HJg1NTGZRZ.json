{"year": "2018", "forum": "HJg1NTGZRZ", "title": "Bit-Regularized Optimization of Neural Nets", "decision": "Reject", "meta_review": "Pros:\n+ The idea of end-to-end training that simultaneously learns the weights and appropriate precision for those weights is very appealing.\n\nCons:\n- Experimental results are far from the state-of-the-art, which makes the empirical evaluation unconvincing.\n- More justification is needed for the update of the number of bits using the sign of the gradient.\n", "reviews": [{"review_id": "HJg1NTGZRZ-0", "review_text": "This paper proposes a direct way to learn low-bit neural nets. The idea is introduced clearly and rather straightforward. pros: (1) The idea is introduced clearly and rather straightforward. (2) The introduction and related work are well written. cons: The provided experiments are weak to demonstrate the effectiveness of the proposed method. (1) only small networks on relatively small datasets are tested. (2) the results on MNIST and CIFAR 10 are not good enough for practical deployment.", "rating": "4: Ok but not good enough - rejection", "reply_text": "1- Only small networks on relatively small datasets are tested . > The results on VGG networks ( larger networks ) is being computed and will be included in the camera ready submission . 2-The results on MNIST and CIFAR-10 are not good enough ... > We found that our low performance on MNIST was caused by using 4X4 pooling layers . We have changed this experiment to use a the standard neural net architecture as a baseline and results on MNIST are shown in ( Figure 1a ) and CIFAR-10 ( Figure 1b ) ."}, {"review_id": "HJg1NTGZRZ-1", "review_text": "This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4. 1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong. 2. The experimental section of this paper needs improvement. a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with. b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc. c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these. d. No wall-time and real memory numbers are reported.", "rating": "3: Clear rejection", "reply_text": "Re : 1.After equation ( 5 ) , I do n't understand how the gradient of L ( tilde_W ) w.r.t.B ( i ) is computed . B ( i ) is discrete . This seems to be mistaken . Internally B ( i ) is a real number that is restricted to take integral values through this update rule using the sign function . It is easy to see how the gradient of L ( tilde_W ) is computed wrt B ( i ) , ie . the gradient of q ( W , B ( i ) ) wrt B ( i ) . ( n.b.q ( . ) is continuous and piecewise differentiable ) . To differentiate q wrt B ( i ) we need to differentiate tilde_W wrt B ( i ) . For a fixed W , you can write tilde_W as a case expression with outputs \\alpha+1\\delta , \\alpha+2\\delta etc for different conditions for w \\in W. dtilde_W/db is differentiated piecewise , and same as d\\delta/db , \\delta is a function of 1/ ( 2 * * B ( i ) ) ."}, {"review_id": "HJg1NTGZRZ-2", "review_text": "The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used). Updates are made to the parameter representing the # of bits via the sign of its gradient. Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10. Overall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. I have a few concerns: First, I find the discussion around the training methodology insufficient. Inherently, the objective is discontinuous since # of bits is a discrete parameter. This is worked around by updating the parameter using the sign of its gradient. This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume. It's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss). We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either. I would be less concerned about the above points if I found the experiments compelling. Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful. Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad. Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve. Overall, while I like the and think the goal is good, I think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning. I can't recommend acceptance. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "1- It 's also difficult for me to understand how this interacts with the other terms in the objective ( quantization error and loss ) ... it 's not at all clear that the gradient of either the loss or the quantization error w.r.t.the number of bits will in general suggest increasing the number of bit . > We have clarified this in the current revision . It is reasonable to assume that the classification accuracy is similar for similar values of the parameters . The error due to the local linear approximation drops at a rate of 1/2^B . In the worst case , we use a fine grained approximation using 32 bits . This is clearly not needed as we show in the experiments , that a 5-6 bit local linear approximation gives good accuracy . 2- It 's unclear to me how effectively accuracy and precision are balanced by this training strategy . > We have clarified this in the current revision . As we show in our experiments , the accuracy and precision trade-off varies with different values for \\lambda_1 and \\lambda_2 . 3-The results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful . > We believe the reviewer is referring to Table 1 . These are after 30 epochs only for MNIST . Our final error rate was 3 % on MNIST . Please note that we are using a small learning rate ( 1e-3 ) in order to show the big impact of bit regularization . In Figure 3 ( a ) ( right panel ) , we showed that increasing the learning rate does indeed improve the performance to about a 2 % error rate . > In the initial submission , we showed LeNet32 and BitNet with about 4 % test error rate . We found that our low performance on MNIST was also caused by using 4X4 pooling layers . We have changed this experiment to use a 2X2 pooling and now show error rates of 2-3 % on MNIST ( Figure 1a ) and 27-29 % on CIFAR-10 ( Figure 1b ) ."}], "0": {"review_id": "HJg1NTGZRZ-0", "review_text": "This paper proposes a direct way to learn low-bit neural nets. The idea is introduced clearly and rather straightforward. pros: (1) The idea is introduced clearly and rather straightforward. (2) The introduction and related work are well written. cons: The provided experiments are weak to demonstrate the effectiveness of the proposed method. (1) only small networks on relatively small datasets are tested. (2) the results on MNIST and CIFAR 10 are not good enough for practical deployment.", "rating": "4: Ok but not good enough - rejection", "reply_text": "1- Only small networks on relatively small datasets are tested . > The results on VGG networks ( larger networks ) is being computed and will be included in the camera ready submission . 2-The results on MNIST and CIFAR-10 are not good enough ... > We found that our low performance on MNIST was caused by using 4X4 pooling layers . We have changed this experiment to use a the standard neural net architecture as a baseline and results on MNIST are shown in ( Figure 1a ) and CIFAR-10 ( Figure 1b ) ."}, "1": {"review_id": "HJg1NTGZRZ-1", "review_text": "This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4. 1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong. 2. The experimental section of this paper needs improvement. a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with. b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc. c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these. d. No wall-time and real memory numbers are reported.", "rating": "3: Clear rejection", "reply_text": "Re : 1.After equation ( 5 ) , I do n't understand how the gradient of L ( tilde_W ) w.r.t.B ( i ) is computed . B ( i ) is discrete . This seems to be mistaken . Internally B ( i ) is a real number that is restricted to take integral values through this update rule using the sign function . It is easy to see how the gradient of L ( tilde_W ) is computed wrt B ( i ) , ie . the gradient of q ( W , B ( i ) ) wrt B ( i ) . ( n.b.q ( . ) is continuous and piecewise differentiable ) . To differentiate q wrt B ( i ) we need to differentiate tilde_W wrt B ( i ) . For a fixed W , you can write tilde_W as a case expression with outputs \\alpha+1\\delta , \\alpha+2\\delta etc for different conditions for w \\in W. dtilde_W/db is differentiated piecewise , and same as d\\delta/db , \\delta is a function of 1/ ( 2 * * B ( i ) ) ."}, "2": {"review_id": "HJg1NTGZRZ-2", "review_text": "The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used). Updates are made to the parameter representing the # of bits via the sign of its gradient. Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10. Overall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. I have a few concerns: First, I find the discussion around the training methodology insufficient. Inherently, the objective is discontinuous since # of bits is a discrete parameter. This is worked around by updating the parameter using the sign of its gradient. This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume. It's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss). We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either. I would be less concerned about the above points if I found the experiments compelling. Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful. Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad. Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve. Overall, while I like the and think the goal is good, I think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning. I can't recommend acceptance. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "1- It 's also difficult for me to understand how this interacts with the other terms in the objective ( quantization error and loss ) ... it 's not at all clear that the gradient of either the loss or the quantization error w.r.t.the number of bits will in general suggest increasing the number of bit . > We have clarified this in the current revision . It is reasonable to assume that the classification accuracy is similar for similar values of the parameters . The error due to the local linear approximation drops at a rate of 1/2^B . In the worst case , we use a fine grained approximation using 32 bits . This is clearly not needed as we show in the experiments , that a 5-6 bit local linear approximation gives good accuracy . 2- It 's unclear to me how effectively accuracy and precision are balanced by this training strategy . > We have clarified this in the current revision . As we show in our experiments , the accuracy and precision trade-off varies with different values for \\lambda_1 and \\lambda_2 . 3-The results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful . > We believe the reviewer is referring to Table 1 . These are after 30 epochs only for MNIST . Our final error rate was 3 % on MNIST . Please note that we are using a small learning rate ( 1e-3 ) in order to show the big impact of bit regularization . In Figure 3 ( a ) ( right panel ) , we showed that increasing the learning rate does indeed improve the performance to about a 2 % error rate . > In the initial submission , we showed LeNet32 and BitNet with about 4 % test error rate . We found that our low performance on MNIST was also caused by using 4X4 pooling layers . We have changed this experiment to use a 2X2 pooling and now show error rates of 2-3 % on MNIST ( Figure 1a ) and 27-29 % on CIFAR-10 ( Figure 1b ) ."}}