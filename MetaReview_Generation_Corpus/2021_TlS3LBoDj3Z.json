{"year": "2021", "forum": "TlS3LBoDj3Z", "title": "QTRAN++: Improved Value Transformation for Cooperative Multi-Agent Reinforcement Learning", "decision": "Reject", "meta_review": "This paper proposes practical improvements to theoretically well founded QTRAN, which is a state-of-the-art technique of cooperative multi-agent reinforcement learning.  The improvements include new designs of loss function and action-value estimator, which might be widely applicable beyond QTRAN.  However, it is not obvious if the proposed improvements actually improves the performance of QTRAN, and experimental evaluation is essential to this work.  After the discussion, there remain some major concerns about the experimental results.  In particular, the performance of baselines in the experiments is not consistent with those reported in the prior work.", "reviews": [{"review_id": "TlS3LBoDj3Z-0", "review_text": "# # # Summary and claims This work proposes a MARL ( multi-agent reinforcement learning ) algorithm . In the MARL setting , multiple agents have to make choices based on independent information to maximize a common objective . An existing algorithm in this space is QTRAN . The authors propose several modifications to QTRAN : changing the architecture , adding two additional constraints to the loss function and also allowing gradients to flow from the QTRAN objective into the `` true '' action-value estimator . The claims of the paper are : 1 ) QTRAN++ achieves better performance than QTRAN 2 ) The modifications introduced stabilize training compared to QTRAN It took me some time to fully understand all of the components that go into QTRAN++ and I find the complexity of the overall algorithm pretty surprising , especially considering that other algorithms in this space are as simple as `` add up all the Q values of the individual agents '' . I think the proposed changes consist of : - Rather than directly training action-value networks , a QMIX-like hypernet approach is used - The network that estimates the `` true '' ( combined ) action-value is implemented through what the authors call a `` semi-monotonic mixing network '' , which is the sum of a non-monotonic ( regular ) hypernet and a monotonic hypernet as used in QMIX . This seems pretty arbitrary . Is n't the original idea behind QTRAN that this would accurately track the true values ? - In QTRAN the separate network that aggregates the Q values of the individual agents is trained to track the `` true '' action-value . In QTRAN++ this is done through multiple hypernetworks ( the authors call these `` heads '' ) . - The loss function is modified to impose two additional constraints on the transforming value function . - Gradients are now also backpropagated from the `` tracking loss '' into the `` true '' action-value estimator , which makes it somewhat unclear what it is actually representing . # # # Relation to prior work The paper is positioned sufficiently with respect to prior work . I 've noticed that there is a larger section on related work in the appendix . I 'm not sure what the purpose of moving the related work into the appendix is , especially if some of the papers mentioned there are not actually related to the work presented in this paper . I think it would be good to try to move as much as possible of that section into the main text , leaving out prior work that is not sufficiently related . Some of the additions in QTRAN++ seem very similar to ideas proposed in QMIX , but this is not directly acknowledged as far as I can tell . It would be good to point out which parts of the architecture come from QMIX . # # # Are the claims supported ? The experiments presented in the paper are reasonably thorough and show that QTRAN++ consistently outperforms QTRAN on the tasks that were tested . SMAC ( StarCraft Multiagent Challenge ) is a nontrivial benchmark , so I would agree that claim 1 ) has been shown sufficiently . But I 'm not sure whether the ablation studies are thorough enough to really demonstrate that all of the components of the ( rather complex ) proposed algorithm are really needed . The authors often make claims about improved stability and other properties of the algorithm throughout the paper , but these are not supported by any empirical evidence . If the authors want to claim that QTRAN++ outperforms QTRAN because of a specific mechanism then it would be good to provide some sort of empirical evidence or proof ( the proof in appendix A does n't count since it does n't make any statements about stability ) . Therefore I think that claim 2 ) is currently not well supported and it would be good to either support it better or soften the statements in the paper to make statements in the form of `` we believe that the algorithm has improved stability '' . # # # Presentation and clarity The paper is reasonably clear and understandable . There are some cases where incorrect grammar or word choice made a sentence difficult to understand . For example the choice of `` affluent '' to describe a class of estimators . It would be good to address cases like this to improve the clarity of the paper . # # # Conclusions The main claim of the paper ( that QTRAN++ is an improvement over QTRAN ) has been demonstrated sufficiently . But the high complexity of the approach ( with several additions to the algorithm feeling somewhat arbitrary ) and the fact that the paper `` merely '' presents an upgrade to QTRAN could be potential arguments against accepting it . # # # * Edit after author comments : * I have read the author comments and the latest paper revision . The authors have noticeably improved the clarity of the paper in several places and adjusted their claims about the stability of the algorithm , and the improved ablation studies are appreciated . Unfortunately , after thinking it through very carefully and despite the author comments , I have not been able to understand some aspects of the model , for example why gradients from the tracking loss are backpropagated into the value function that is supposed to track the `` true '' action-values . Several parts of the architecture seem to have a complicated dual purpose , which makes it difficult to understand what is going on and why the model is performing better . I suspect that other readers might also encounter similar issues , which makes it difficult for me to raise my rating . I 've decided to leave the rating at 6 ( marginal accept ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * 4.I 'm not sure whether the ablation studies are thorough enough . * * To alleviate your concerns , we extended our ablation studies from 3 maps to 10 maps . We report the experimental results in Figure 4 of the revised paper . In the figure , one can observe how each component of QTRAN++ provides a solid improvement . Especially , removing any component of QTRAN++ leads to significantly worse performance for at least one of the scenarios . Especially , as mentioned in our previous response , Fix-QTRAN++ underperforms significantly compared to QTRAN++ for all of the considered scenarios . * * 5.The authors often make claims about improved stability of the algorithm . It would be good to soften the statements in the paper to make statements in the form of `` we believe that the algorithm has improved stability '' . * * Thank you for pointing this out . We originally described QTRAN++ to improve the \u201c stability \u201d of QTRAN to reflect how our objective provides a denser training signal compared to QTRAN . Nevertheless , we deeply resonate with your concern and revised our paper to soften our claims on improving the stability of the QTRAN++ . * * 6.There are some cases where incorrect grammar or word choice made a sentence difficult to understand . For example , the choice of `` affluent '' to describe a class of estimators . * * Thank you for the helpful suggestion . To alleviate your concern , we carefully revised the paper to remove any incorrect grammar or word choice that makes a sentence difficult to understand . For example , we replaced the phrase \u201c more affluent class of estimators \u201d with \u201c a larger class of estimators \u201d in our revised paper ."}, {"review_id": "TlS3LBoDj3Z-1", "review_text": "# # # Summary This paper presents an improved version of QTRAN [ 1 ] . The design is based on new loss function design , as well as new action-value estimator designs . The paper claims superior perfromance gains compared to previous methods on the Starcraft Multi-Agent Challenge ( SMAC ) environment . # # # Strengths + The ideas proposed to improve the previous QTRAN ( or might be applied to other MARL algorithms as well ) seems novel and general . + The authors perform comprehensive ablation studies for different components they proposed . + The empirical performance on the SMAC benchmark is better and more stable across different runs . + The writing is clear and easy to follow . # # # Weaknesses - Only one environment is evaluated , which might not be that convincing . It would be good to see more results on different benchmarks . - In Figure 3 , some results seem to be not converged yet . Since the metric is the win rate , which is bounded , it would be interesting to see given enough training steps , whether all methods can actually converge to similar winning rate , or inherently the proposed scheme can lead to better results . # # # Minor issues Typo : Page 6 , `` ... for being \u201c selfish. \u201d This ... '' - > `` ... for being \u201c selfish \u201d . This ... '' Considering all the aspects , I tend to accept the paper in the current stage . # # # Reference 1 . Qtran : Learning to factorize with transformation for cooperative multi-agent reinforcement learning . 2019.- * * Updates * * : After reading the other reviews and the rebuttal , I still maintain my current score . The additional experiments on the converged results are good to me . As I 'm not very familiar with the performance in MARL literatures , I have decreased my confidence from 3 to 2 to reflect some of the concerns of other reviewers involving the performance for the baselines .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We express our deep appreciation for your time and insightful comments . We are grateful for all the positive comments : providing a practical improvement for the theoretically important algorithm ( by R2 and R4 ) , strong empirical performance ( by you , R3 and R4 ) , novel and general ideas ( by you ) , and clear writing ( by you and R3 ) . In the revised manuscript , we have substantially updated or newly added ( Section 2 , Section 3 , Figure 3 , Figure 4 , Appendix B , D , E , F ) according to the initial reviews and colored them red . In the following , we address your comments one by one . * * 1.It would be good to see more results on different benchmarks . * * Thank you for the suggestion . We consider the SMAC benchmark to be sufficient in our experiments since it allows evaluating agents under diverse scenarios . Indeed , the scenarios have varying numbers and types of agents , and require the agents to learn diverse skills such as \u201c kiting \u201d and \u201c focus fire. \u201d For this reason , most prior works such as QMIX [ 1 ] , and MAVEN [ 2 ] only considered the SMAC environment for evaluation . Nevertheless , to incorporate your comments , we additionally evaluated our QTRAN++ using the multi-domain Gaussian squeeze benchmark [ 3 ] . The corresponding results are reported in Appendix F of our revised paper . In the experiments , one can observe how QTRAN++ consistently achieves the best result compared to the baselines . This result is especially significant since the benchmark was designed specifically for demonstrating the strength of the original QTRAN algorithm [ 3 ] . This demonstrates that QTRAN++ achieves state-of-the-art performance across different benchmarks and still retains the main strengths of QTRAN . * * 2.In Figure 3 , some results seem to be not converged yet . It would be interesting to see experiments using enough training steps . * * Thank you for the suggestion . We follow the same number of training steps for the SMAC environment as the prior works , e.g. , QMIX [ 1 ] and Weighted QMIX [ 4 ] . This can avoid a potential reproducibility issue when comparing with the prior works . Nevertheless , to incorporate your comments , we evaluated our QTRAN++ by increasing the number of training steps twice ( two million to four million steps ) for the scenarios where algorithms have not converged in Figure 3 . The corresponding results are reported in Appendix E of our revised paper . In the experiments , one can observe how our QTRAN++ maintains state-of-the-art performance even after the considered algorithms converge . * * 3.Typo : Page 6 , `` ... for being \u201c selfish. \u201d This ... '' - > `` ... for being \u201c selfish \u201d . This ... '' * * Thank you for pointing this out . We rephrased the corresponding sentence to alleviate your concern . * * References * * [ 1 ] Rashid et al.Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning , JMLR 2020 [ 2 ] Mahajan et al. , MAVEN : Multi-Agent Variational Exploration , NeurIPS 2019 [ 3 ] Son et al.QTRAN : Learning to factorize with transformation for cooperative multi-agent reinforcement learning , ICML 2019 [ 4 ] Rashid et al.Weighted QMIX : Expanding Monotonic Value Function Factorisation , NeurIPS 2020"}, {"review_id": "TlS3LBoDj3Z-2", "review_text": "This paper provides good improvements that make QTRAN more practical and can be applied to problems other than matrix games . Since QTRAN is a significant improvement for value-based multi-agent reinforcement learning after QMIX , the practical implementation of QTRAN is expected for a long time in the community . However , after the publication of QTRAN , especially in 2020 , some other works have explored the question of how to extend QMIX to the full IGM function class . Due to these works ( QPLEX is the major concern , given that weighted-QMIX has been compared in the experiments ) , the expectation of the advanced version of QTRAN is higher than before . I have several concerns regarding several core contributions of QTRAN++ . QTRAN++ relies heavily on the true joint action-value function . ( 1.1 ) However , learning joint action-value functions is not an adorable choice in multi-agent problems . ( 1.2 ) To ease the training and representation of joint action-value functions , the authors condition $ Q_ { jt } $ on individual q values and use a semi-monotonic structure . However , it is difficult to tell the contribution of the monotonic part . It has been shown that monotonic functions can not represent some Q-values . Why should this part be included ? I expect that I can find the answer from ablation studies , but on two out of three scenarios , FC-QTRAN++ is very similar to QTRAN++ . The authors can provide a more serious discussion of this part to make their paper stronger . About the training of $ Q_ { tran } ^ { i } $ . I have two questions about the training of this value function . ( 2.1 ) When training $ Q_ { tran } ^ { i } $ , whether local utility function of agent $ j $ ( $ q_j $ using the notation from the paper ) is updated ? ( 2.2 ) The training scheme is a midpoint between VDN and QMIX , which is similar to an attention mechanism that has been explored in multi-agent value decomposition settings . The formulation is quite different from previous papers ( DOP [ Wang et al. , 2020 ] and REFIL [ Iqbal et al.2020 ] ) , but based on the results from these previous work , I think the multi-head structure may not improve the performance . Although the authors use a matrix game to illustrate their idea , which I appreciate , I can hardly tell whether this example is specially designed . I was expecting a convincing ablation study on SMAC , but I do not find them sufficient : ( 1 ) The authors did not record how many random seeds did they test , and SMAC tasks are typically sensitive to random seeds . ( 2 ) The gap between QTRAN and QTRAN++ is not significant . If the authors can provide results with more random seeds on more maps , I will consider revising my rating . My last concern is about QPLEX , as cited by the authors . Similar to QTRAN , QPLEX provides full expressivity for the IGM function class . Nevertheless , the implementation of QPLEX seems to be much more lightweight than QTRAN++ . Since QPLEX has provided codes that can be freely tested on the SMAC benchmark , I was wondering why the authors cited this paper but did not compare to it . At least , a detailed discussion of the differences can make the contribution of QTRAN++ clearer . * * A minor concern about experiments . It seems that the authors are using an older version of QMIX . In the latest version , QMIX can achieve a win rate pf 80 % on MMM2 . This fact is unknown for many , because the journal version of QMIX reports the same win rate as in this paper . [ Wang et al. , 2020 ] Wang , Y. , Han , B. , Wang , T. , Dong , H. and Zhang , C. , 2020 . Off-Policy Multi-Agent Decomposed Policy Gradients . arXiv preprint arXiv:2007.12322 . [ Iqbal et al.2020 ] Iqbal , S. , de Witt , C.A.S. , Peng , B. , B\u00f6hmer , W. , Whiteson , S. and Sha , F. , 2020 . AI-QMIX : Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning . arXiv preprint arXiv:2006.04222 . # # # # # UPDATE = Thanks for the authors ' clarifications . After a careful re-evaluation of the paper , I have many concerns about the performance of baselines on the StarCraft II benchmark tasks . The reported performance is not consistent with those reported in the SMAC benchmark paper ( see Figure 4,5,6 in [ 1 ] ) and QPLEX paper ( Figure 5,8,19 in [ 2 ] ) . Moreover , I also evaluate the available GitHub codes of baselines on my own , which is consistent with [ 1,2 ] . Using results in [ 1,2 ] , QTRAN++ significantly underperforms the baselines on the StarCraft II benchmark tasks . Moreover , the paper claims that it uses the standard StarCraft II benchmark , the latest version of SC2 , and the default baseline codes . Due to these concerns , I tend to lower my rating . [ 1 ] Samvelyan M , Rashid T , de Witt C S , et al.The starcraft multi-agent challenge . arXiv preprint arXiv:1902.04043 , 2019 . [ 2 ] Jianhao Wang , Zhizhou Ren , Terry Liu , Yu Yang , and Chongjie Zhang . Qplex : Duplex dueling multi-agent Q-learning . ICLR submission . https : //openreview.net/forum ? id=Rcmk0xxIQV", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * 6.The ablation studies are not sufficient ( for validating the contribution of the multi-head structure ) . The authors did not record how many random seeds they test . Authors can provide results with more random seeds on more maps . * * Thank you for the suggestion . To address your concern , we extended our ablation studies from 3 maps to 10 maps . We report the experimental results in Figure 4 of the revised paper . In the figure , one can observe how the multi-head component provides a concrete improvement to QTRAN++ . To be specific , QTRAN++ performs significantly better than Mix-QTRAN++ ( QTRAN++ without the multi-head component ) in the 5m_vs_6m and 5m_vs_6m ( negative ) scenarios and performs at least as good as Mix-QTRAN++ for the other eight scenarios . Furthermore , we recorded that we use five random seeds for all the experiments in Section 4.1 of our paper ( before revision ) . If you find this number of random seeds insufficient , we will be happy to run more experiments and increase the number of random seeds . * * 7.The gap between QTRAN and QTRAN++ is not significant . * * We do believe that the gap between QTRAN and QTRAN++ is significant . As one can see in Figure 3 of our ( original or revised ) paper , QTRAN++ outperforms QTRAN for all the scenarios . Especially , QTRAN performs the worst among the baselines in five out of ten scenarios , i.e. , MMM2 , 3s5z , 3s5z ( negative ) , 3s_vs_5z , 3s_vs_5z ( negative ) . In contrast , QTRAN++ achieves the best performance for all ten scenarios . * * 8.I was wondering why the authors did not compare with QPLEX . A detailed discussion of the differences between QPLEX and QTRAN++ can make the contribution clearer . * * Thank you for the suggestion . To address your concern , we additionally considered QPLEX ( a concurrent submission at ICLR 2021 , https : //openreview.net/forum ? id=Rcmk0xxIQV ) as a baseline in Figure 3 . One can observe how our QTRAN++ outperforms QPLEX for four scenarios , i.e. , MMM2 , 10m_vs_11m ( negative ) , 5m_vs_6m ( negative ) , 3z_vs_5z , and performs at least as good as QPLEX for the other six scenarios . To further incorporate your suggestion , we discuss the algorithmic difference between QPLEX and QTRAN++ . As you mentioned , QPLEX is similar to QTRAN++ since it removes the restriction on the true action-value estimator . However , they use different regularization for achieving this goal . To be specific , QTRAN++ imposes an additional loss function between the true and the transformed action-value estimators . In contrast , QPLEX introduces a structural constraint : the true action-value estimator is expressed as a summation of utility functions and a non-positive advantage estimator . A more detailed discussion is incorporated in Appendix B of our revised paper . * * 9.It seems that the authors are using an older version of QMIX . In the latest version , QMIX can achieve a win rate of 80 % on MMM2 . * * After careful inspection , we checked that our version of QMIX is identical to your suggested version . We hypothesize that the performance gap comes from different exploration hyperparameters and version of Starcraft II ( performance of the algorithms may be sensitive to the version of Starcraft II as stated in [ 7 ] ) . To be specific , we use the same experimental setting across all algorithms for a fair comparison . Especially , we use the setting employed by the previous state-of-the-art work [ 4 ] to make the comparison more competitive . To incorporate your comment , we revised Appendix D of our paper by adding this discussion accordingly . * * References * * [ 1 ] Sunehag et al.Value-Decomposition Networks For Cooperative Multi-Agent Learning , AAMAS 2018 [ 2 ] Rashid et al.Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning , JMLR 2020 [ 3 ] Son et al.Qtran : Learning to factorize with transformation for cooperative multi-agent reinforcement learning , ICML 2019 [ 4 ] Rashid et al.Weighted QMIX : Expanding Monotonic Value Function Factorisation , NeurIPS 2020 [ 5 ] Wang et al.Off-Policy Multi-Agent Decomposed Policy Gradients , preprint 2020 [ 6 ] Iqbal et al.AI-QMIX : Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning , preprint 2020 [ 7 ] Python MARL framework ( https : //github.com/oxwhirl/pymarl )"}, {"review_id": "TlS3LBoDj3Z-3", "review_text": "# # Summary This paper addresses the domain of cooperative multiagent learning with centralised learning and decentralised execution . Specifically , it improves on the QTRAN algorithm , a theoretically justified algorithm which previously had not produced strong learning performance . With these improvements , QTRAN++ outperforms baselines on the SMAC environments . I recommend accepting this paper . It delivers strong performance on a popular benchmark for complex , cooperative multiagent learning ( SMAC ) . While the algorithmic contribution is incremental , it still delivers insight into how to improve the empirical performance of a theoretically interesting and well-justified algorithm . # # Positives The problem addressed - cooperative multiagent environments with CTDE - is a widely studied and important one . It is well set up in the paper , including discussion of related algorithms . The base algorithm - QTRAN - should theoretically perform well in a wider variety of environments than other algorithms for these problems , so improving its performance is particularly valuable . The improvements made to the algorithm are clear and well motivated ; section 3.1 in particular explains clearly the difference the modified loss is intended to make . The empirical studies in the paper are strong . They show that QTRAN++ outperforms several baselines in data efficiency and final performance across a variety of domains . Further , a comprehensive ablation study shows that each of the improvements made to QTRAN is independently important ( in at least some domains ) . # # Negatives The algorithmic contribution of the paper is relatively minor , since it provides fairly simple modifications to an existing algorithm . Experimentally , it would be good to see experiments on similar domains to those addressed in the original QTRAN paper , which are designed to probe the advantages QTRAN has over related algorithms . This would demonstrate that QTRAN++ retains the benefits of QTRAN in non-monotonic factorisable environments . # # Sources of reviewer uncertainty I am not knowledgeable enough in this domain to be certain of the coverage of the baselines and domains in the paper . Since the empirical performance of the algorithm is central to the paper , this is important .", "rating": "7: Good paper, accept", "reply_text": "We express our deep appreciation for your time and insightful comments . We are grateful for all the positive comments : providing a practical improvement for the theoretically important algorithm ( by you and R2 ) , strong empirical performance ( by you , R1 and R3 ) , novel and general ideas ( by R1 ) , and clear writing ( by R1 and R3 ) . In the revised manuscript , we have substantially updated or newly added ( Section 2 , Section 3 , Figure 3 , Figure 4 , Appendix B , D , E , F ) according to the initial reviews and colored them red . In the following , we address your comments one by one . * * 1.The algorithmic contribution of the paper is relatively minor since it provides fairly simple modifications to an existing algorithm . * * Despite being simple , we believe our QTRAN++ to deliver a significant contribution by closing the gap between theory and practice , i.e. , QTRAN++ improves the empirical performance of a theoretically interesting algorithm . Furthermore , the algorithmic contribution of QTRAN++ stems not only from proposing the simple modifications but also from effectively combining the modifications to yield a large overall gain in performance . Indeed , as shown in Figure 4 , the modifications are complementary , and omitting just one of our modifications results in a degradation of performance for at least one of the considered scenarios . We added the respective discussion in Section 1 of our revised paper . * * 2.It would be good to see experiments on similar domains to those addressed in the original QTRAN paper . * * Thank you for the suggestion . To incorporate your comments , we evaluated our QTRAN++ using the multi-domain Gaussian squeeze environment from the original QTRAN paper [ 1 ] . The corresponding results are reported in Appendix F of our revised paper . In the experiments , one can observe how QTRAN++ consistently achieves the best result compared to the baselines including QTRAN . Hence , one may conclude that QTRAN++ achieves state-of-the-art performance across different environments and still retains the main strengths of QTRAN . * * References * * [ 1 ] Son et al.QTRAN : Learning to factorize with transformation for cooperative multi-agent reinforcement learning , ICML 2019"}], "0": {"review_id": "TlS3LBoDj3Z-0", "review_text": "# # # Summary and claims This work proposes a MARL ( multi-agent reinforcement learning ) algorithm . In the MARL setting , multiple agents have to make choices based on independent information to maximize a common objective . An existing algorithm in this space is QTRAN . The authors propose several modifications to QTRAN : changing the architecture , adding two additional constraints to the loss function and also allowing gradients to flow from the QTRAN objective into the `` true '' action-value estimator . The claims of the paper are : 1 ) QTRAN++ achieves better performance than QTRAN 2 ) The modifications introduced stabilize training compared to QTRAN It took me some time to fully understand all of the components that go into QTRAN++ and I find the complexity of the overall algorithm pretty surprising , especially considering that other algorithms in this space are as simple as `` add up all the Q values of the individual agents '' . I think the proposed changes consist of : - Rather than directly training action-value networks , a QMIX-like hypernet approach is used - The network that estimates the `` true '' ( combined ) action-value is implemented through what the authors call a `` semi-monotonic mixing network '' , which is the sum of a non-monotonic ( regular ) hypernet and a monotonic hypernet as used in QMIX . This seems pretty arbitrary . Is n't the original idea behind QTRAN that this would accurately track the true values ? - In QTRAN the separate network that aggregates the Q values of the individual agents is trained to track the `` true '' action-value . In QTRAN++ this is done through multiple hypernetworks ( the authors call these `` heads '' ) . - The loss function is modified to impose two additional constraints on the transforming value function . - Gradients are now also backpropagated from the `` tracking loss '' into the `` true '' action-value estimator , which makes it somewhat unclear what it is actually representing . # # # Relation to prior work The paper is positioned sufficiently with respect to prior work . I 've noticed that there is a larger section on related work in the appendix . I 'm not sure what the purpose of moving the related work into the appendix is , especially if some of the papers mentioned there are not actually related to the work presented in this paper . I think it would be good to try to move as much as possible of that section into the main text , leaving out prior work that is not sufficiently related . Some of the additions in QTRAN++ seem very similar to ideas proposed in QMIX , but this is not directly acknowledged as far as I can tell . It would be good to point out which parts of the architecture come from QMIX . # # # Are the claims supported ? The experiments presented in the paper are reasonably thorough and show that QTRAN++ consistently outperforms QTRAN on the tasks that were tested . SMAC ( StarCraft Multiagent Challenge ) is a nontrivial benchmark , so I would agree that claim 1 ) has been shown sufficiently . But I 'm not sure whether the ablation studies are thorough enough to really demonstrate that all of the components of the ( rather complex ) proposed algorithm are really needed . The authors often make claims about improved stability and other properties of the algorithm throughout the paper , but these are not supported by any empirical evidence . If the authors want to claim that QTRAN++ outperforms QTRAN because of a specific mechanism then it would be good to provide some sort of empirical evidence or proof ( the proof in appendix A does n't count since it does n't make any statements about stability ) . Therefore I think that claim 2 ) is currently not well supported and it would be good to either support it better or soften the statements in the paper to make statements in the form of `` we believe that the algorithm has improved stability '' . # # # Presentation and clarity The paper is reasonably clear and understandable . There are some cases where incorrect grammar or word choice made a sentence difficult to understand . For example the choice of `` affluent '' to describe a class of estimators . It would be good to address cases like this to improve the clarity of the paper . # # # Conclusions The main claim of the paper ( that QTRAN++ is an improvement over QTRAN ) has been demonstrated sufficiently . But the high complexity of the approach ( with several additions to the algorithm feeling somewhat arbitrary ) and the fact that the paper `` merely '' presents an upgrade to QTRAN could be potential arguments against accepting it . # # # * Edit after author comments : * I have read the author comments and the latest paper revision . The authors have noticeably improved the clarity of the paper in several places and adjusted their claims about the stability of the algorithm , and the improved ablation studies are appreciated . Unfortunately , after thinking it through very carefully and despite the author comments , I have not been able to understand some aspects of the model , for example why gradients from the tracking loss are backpropagated into the value function that is supposed to track the `` true '' action-values . Several parts of the architecture seem to have a complicated dual purpose , which makes it difficult to understand what is going on and why the model is performing better . I suspect that other readers might also encounter similar issues , which makes it difficult for me to raise my rating . I 've decided to leave the rating at 6 ( marginal accept ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * 4.I 'm not sure whether the ablation studies are thorough enough . * * To alleviate your concerns , we extended our ablation studies from 3 maps to 10 maps . We report the experimental results in Figure 4 of the revised paper . In the figure , one can observe how each component of QTRAN++ provides a solid improvement . Especially , removing any component of QTRAN++ leads to significantly worse performance for at least one of the scenarios . Especially , as mentioned in our previous response , Fix-QTRAN++ underperforms significantly compared to QTRAN++ for all of the considered scenarios . * * 5.The authors often make claims about improved stability of the algorithm . It would be good to soften the statements in the paper to make statements in the form of `` we believe that the algorithm has improved stability '' . * * Thank you for pointing this out . We originally described QTRAN++ to improve the \u201c stability \u201d of QTRAN to reflect how our objective provides a denser training signal compared to QTRAN . Nevertheless , we deeply resonate with your concern and revised our paper to soften our claims on improving the stability of the QTRAN++ . * * 6.There are some cases where incorrect grammar or word choice made a sentence difficult to understand . For example , the choice of `` affluent '' to describe a class of estimators . * * Thank you for the helpful suggestion . To alleviate your concern , we carefully revised the paper to remove any incorrect grammar or word choice that makes a sentence difficult to understand . For example , we replaced the phrase \u201c more affluent class of estimators \u201d with \u201c a larger class of estimators \u201d in our revised paper ."}, "1": {"review_id": "TlS3LBoDj3Z-1", "review_text": "# # # Summary This paper presents an improved version of QTRAN [ 1 ] . The design is based on new loss function design , as well as new action-value estimator designs . The paper claims superior perfromance gains compared to previous methods on the Starcraft Multi-Agent Challenge ( SMAC ) environment . # # # Strengths + The ideas proposed to improve the previous QTRAN ( or might be applied to other MARL algorithms as well ) seems novel and general . + The authors perform comprehensive ablation studies for different components they proposed . + The empirical performance on the SMAC benchmark is better and more stable across different runs . + The writing is clear and easy to follow . # # # Weaknesses - Only one environment is evaluated , which might not be that convincing . It would be good to see more results on different benchmarks . - In Figure 3 , some results seem to be not converged yet . Since the metric is the win rate , which is bounded , it would be interesting to see given enough training steps , whether all methods can actually converge to similar winning rate , or inherently the proposed scheme can lead to better results . # # # Minor issues Typo : Page 6 , `` ... for being \u201c selfish. \u201d This ... '' - > `` ... for being \u201c selfish \u201d . This ... '' Considering all the aspects , I tend to accept the paper in the current stage . # # # Reference 1 . Qtran : Learning to factorize with transformation for cooperative multi-agent reinforcement learning . 2019.- * * Updates * * : After reading the other reviews and the rebuttal , I still maintain my current score . The additional experiments on the converged results are good to me . As I 'm not very familiar with the performance in MARL literatures , I have decreased my confidence from 3 to 2 to reflect some of the concerns of other reviewers involving the performance for the baselines .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We express our deep appreciation for your time and insightful comments . We are grateful for all the positive comments : providing a practical improvement for the theoretically important algorithm ( by R2 and R4 ) , strong empirical performance ( by you , R3 and R4 ) , novel and general ideas ( by you ) , and clear writing ( by you and R3 ) . In the revised manuscript , we have substantially updated or newly added ( Section 2 , Section 3 , Figure 3 , Figure 4 , Appendix B , D , E , F ) according to the initial reviews and colored them red . In the following , we address your comments one by one . * * 1.It would be good to see more results on different benchmarks . * * Thank you for the suggestion . We consider the SMAC benchmark to be sufficient in our experiments since it allows evaluating agents under diverse scenarios . Indeed , the scenarios have varying numbers and types of agents , and require the agents to learn diverse skills such as \u201c kiting \u201d and \u201c focus fire. \u201d For this reason , most prior works such as QMIX [ 1 ] , and MAVEN [ 2 ] only considered the SMAC environment for evaluation . Nevertheless , to incorporate your comments , we additionally evaluated our QTRAN++ using the multi-domain Gaussian squeeze benchmark [ 3 ] . The corresponding results are reported in Appendix F of our revised paper . In the experiments , one can observe how QTRAN++ consistently achieves the best result compared to the baselines . This result is especially significant since the benchmark was designed specifically for demonstrating the strength of the original QTRAN algorithm [ 3 ] . This demonstrates that QTRAN++ achieves state-of-the-art performance across different benchmarks and still retains the main strengths of QTRAN . * * 2.In Figure 3 , some results seem to be not converged yet . It would be interesting to see experiments using enough training steps . * * Thank you for the suggestion . We follow the same number of training steps for the SMAC environment as the prior works , e.g. , QMIX [ 1 ] and Weighted QMIX [ 4 ] . This can avoid a potential reproducibility issue when comparing with the prior works . Nevertheless , to incorporate your comments , we evaluated our QTRAN++ by increasing the number of training steps twice ( two million to four million steps ) for the scenarios where algorithms have not converged in Figure 3 . The corresponding results are reported in Appendix E of our revised paper . In the experiments , one can observe how our QTRAN++ maintains state-of-the-art performance even after the considered algorithms converge . * * 3.Typo : Page 6 , `` ... for being \u201c selfish. \u201d This ... '' - > `` ... for being \u201c selfish \u201d . This ... '' * * Thank you for pointing this out . We rephrased the corresponding sentence to alleviate your concern . * * References * * [ 1 ] Rashid et al.Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning , JMLR 2020 [ 2 ] Mahajan et al. , MAVEN : Multi-Agent Variational Exploration , NeurIPS 2019 [ 3 ] Son et al.QTRAN : Learning to factorize with transformation for cooperative multi-agent reinforcement learning , ICML 2019 [ 4 ] Rashid et al.Weighted QMIX : Expanding Monotonic Value Function Factorisation , NeurIPS 2020"}, "2": {"review_id": "TlS3LBoDj3Z-2", "review_text": "This paper provides good improvements that make QTRAN more practical and can be applied to problems other than matrix games . Since QTRAN is a significant improvement for value-based multi-agent reinforcement learning after QMIX , the practical implementation of QTRAN is expected for a long time in the community . However , after the publication of QTRAN , especially in 2020 , some other works have explored the question of how to extend QMIX to the full IGM function class . Due to these works ( QPLEX is the major concern , given that weighted-QMIX has been compared in the experiments ) , the expectation of the advanced version of QTRAN is higher than before . I have several concerns regarding several core contributions of QTRAN++ . QTRAN++ relies heavily on the true joint action-value function . ( 1.1 ) However , learning joint action-value functions is not an adorable choice in multi-agent problems . ( 1.2 ) To ease the training and representation of joint action-value functions , the authors condition $ Q_ { jt } $ on individual q values and use a semi-monotonic structure . However , it is difficult to tell the contribution of the monotonic part . It has been shown that monotonic functions can not represent some Q-values . Why should this part be included ? I expect that I can find the answer from ablation studies , but on two out of three scenarios , FC-QTRAN++ is very similar to QTRAN++ . The authors can provide a more serious discussion of this part to make their paper stronger . About the training of $ Q_ { tran } ^ { i } $ . I have two questions about the training of this value function . ( 2.1 ) When training $ Q_ { tran } ^ { i } $ , whether local utility function of agent $ j $ ( $ q_j $ using the notation from the paper ) is updated ? ( 2.2 ) The training scheme is a midpoint between VDN and QMIX , which is similar to an attention mechanism that has been explored in multi-agent value decomposition settings . The formulation is quite different from previous papers ( DOP [ Wang et al. , 2020 ] and REFIL [ Iqbal et al.2020 ] ) , but based on the results from these previous work , I think the multi-head structure may not improve the performance . Although the authors use a matrix game to illustrate their idea , which I appreciate , I can hardly tell whether this example is specially designed . I was expecting a convincing ablation study on SMAC , but I do not find them sufficient : ( 1 ) The authors did not record how many random seeds did they test , and SMAC tasks are typically sensitive to random seeds . ( 2 ) The gap between QTRAN and QTRAN++ is not significant . If the authors can provide results with more random seeds on more maps , I will consider revising my rating . My last concern is about QPLEX , as cited by the authors . Similar to QTRAN , QPLEX provides full expressivity for the IGM function class . Nevertheless , the implementation of QPLEX seems to be much more lightweight than QTRAN++ . Since QPLEX has provided codes that can be freely tested on the SMAC benchmark , I was wondering why the authors cited this paper but did not compare to it . At least , a detailed discussion of the differences can make the contribution of QTRAN++ clearer . * * A minor concern about experiments . It seems that the authors are using an older version of QMIX . In the latest version , QMIX can achieve a win rate pf 80 % on MMM2 . This fact is unknown for many , because the journal version of QMIX reports the same win rate as in this paper . [ Wang et al. , 2020 ] Wang , Y. , Han , B. , Wang , T. , Dong , H. and Zhang , C. , 2020 . Off-Policy Multi-Agent Decomposed Policy Gradients . arXiv preprint arXiv:2007.12322 . [ Iqbal et al.2020 ] Iqbal , S. , de Witt , C.A.S. , Peng , B. , B\u00f6hmer , W. , Whiteson , S. and Sha , F. , 2020 . AI-QMIX : Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning . arXiv preprint arXiv:2006.04222 . # # # # # UPDATE = Thanks for the authors ' clarifications . After a careful re-evaluation of the paper , I have many concerns about the performance of baselines on the StarCraft II benchmark tasks . The reported performance is not consistent with those reported in the SMAC benchmark paper ( see Figure 4,5,6 in [ 1 ] ) and QPLEX paper ( Figure 5,8,19 in [ 2 ] ) . Moreover , I also evaluate the available GitHub codes of baselines on my own , which is consistent with [ 1,2 ] . Using results in [ 1,2 ] , QTRAN++ significantly underperforms the baselines on the StarCraft II benchmark tasks . Moreover , the paper claims that it uses the standard StarCraft II benchmark , the latest version of SC2 , and the default baseline codes . Due to these concerns , I tend to lower my rating . [ 1 ] Samvelyan M , Rashid T , de Witt C S , et al.The starcraft multi-agent challenge . arXiv preprint arXiv:1902.04043 , 2019 . [ 2 ] Jianhao Wang , Zhizhou Ren , Terry Liu , Yu Yang , and Chongjie Zhang . Qplex : Duplex dueling multi-agent Q-learning . ICLR submission . https : //openreview.net/forum ? id=Rcmk0xxIQV", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * 6.The ablation studies are not sufficient ( for validating the contribution of the multi-head structure ) . The authors did not record how many random seeds they test . Authors can provide results with more random seeds on more maps . * * Thank you for the suggestion . To address your concern , we extended our ablation studies from 3 maps to 10 maps . We report the experimental results in Figure 4 of the revised paper . In the figure , one can observe how the multi-head component provides a concrete improvement to QTRAN++ . To be specific , QTRAN++ performs significantly better than Mix-QTRAN++ ( QTRAN++ without the multi-head component ) in the 5m_vs_6m and 5m_vs_6m ( negative ) scenarios and performs at least as good as Mix-QTRAN++ for the other eight scenarios . Furthermore , we recorded that we use five random seeds for all the experiments in Section 4.1 of our paper ( before revision ) . If you find this number of random seeds insufficient , we will be happy to run more experiments and increase the number of random seeds . * * 7.The gap between QTRAN and QTRAN++ is not significant . * * We do believe that the gap between QTRAN and QTRAN++ is significant . As one can see in Figure 3 of our ( original or revised ) paper , QTRAN++ outperforms QTRAN for all the scenarios . Especially , QTRAN performs the worst among the baselines in five out of ten scenarios , i.e. , MMM2 , 3s5z , 3s5z ( negative ) , 3s_vs_5z , 3s_vs_5z ( negative ) . In contrast , QTRAN++ achieves the best performance for all ten scenarios . * * 8.I was wondering why the authors did not compare with QPLEX . A detailed discussion of the differences between QPLEX and QTRAN++ can make the contribution clearer . * * Thank you for the suggestion . To address your concern , we additionally considered QPLEX ( a concurrent submission at ICLR 2021 , https : //openreview.net/forum ? id=Rcmk0xxIQV ) as a baseline in Figure 3 . One can observe how our QTRAN++ outperforms QPLEX for four scenarios , i.e. , MMM2 , 10m_vs_11m ( negative ) , 5m_vs_6m ( negative ) , 3z_vs_5z , and performs at least as good as QPLEX for the other six scenarios . To further incorporate your suggestion , we discuss the algorithmic difference between QPLEX and QTRAN++ . As you mentioned , QPLEX is similar to QTRAN++ since it removes the restriction on the true action-value estimator . However , they use different regularization for achieving this goal . To be specific , QTRAN++ imposes an additional loss function between the true and the transformed action-value estimators . In contrast , QPLEX introduces a structural constraint : the true action-value estimator is expressed as a summation of utility functions and a non-positive advantage estimator . A more detailed discussion is incorporated in Appendix B of our revised paper . * * 9.It seems that the authors are using an older version of QMIX . In the latest version , QMIX can achieve a win rate of 80 % on MMM2 . * * After careful inspection , we checked that our version of QMIX is identical to your suggested version . We hypothesize that the performance gap comes from different exploration hyperparameters and version of Starcraft II ( performance of the algorithms may be sensitive to the version of Starcraft II as stated in [ 7 ] ) . To be specific , we use the same experimental setting across all algorithms for a fair comparison . Especially , we use the setting employed by the previous state-of-the-art work [ 4 ] to make the comparison more competitive . To incorporate your comment , we revised Appendix D of our paper by adding this discussion accordingly . * * References * * [ 1 ] Sunehag et al.Value-Decomposition Networks For Cooperative Multi-Agent Learning , AAMAS 2018 [ 2 ] Rashid et al.Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning , JMLR 2020 [ 3 ] Son et al.Qtran : Learning to factorize with transformation for cooperative multi-agent reinforcement learning , ICML 2019 [ 4 ] Rashid et al.Weighted QMIX : Expanding Monotonic Value Function Factorisation , NeurIPS 2020 [ 5 ] Wang et al.Off-Policy Multi-Agent Decomposed Policy Gradients , preprint 2020 [ 6 ] Iqbal et al.AI-QMIX : Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning , preprint 2020 [ 7 ] Python MARL framework ( https : //github.com/oxwhirl/pymarl )"}, "3": {"review_id": "TlS3LBoDj3Z-3", "review_text": "# # Summary This paper addresses the domain of cooperative multiagent learning with centralised learning and decentralised execution . Specifically , it improves on the QTRAN algorithm , a theoretically justified algorithm which previously had not produced strong learning performance . With these improvements , QTRAN++ outperforms baselines on the SMAC environments . I recommend accepting this paper . It delivers strong performance on a popular benchmark for complex , cooperative multiagent learning ( SMAC ) . While the algorithmic contribution is incremental , it still delivers insight into how to improve the empirical performance of a theoretically interesting and well-justified algorithm . # # Positives The problem addressed - cooperative multiagent environments with CTDE - is a widely studied and important one . It is well set up in the paper , including discussion of related algorithms . The base algorithm - QTRAN - should theoretically perform well in a wider variety of environments than other algorithms for these problems , so improving its performance is particularly valuable . The improvements made to the algorithm are clear and well motivated ; section 3.1 in particular explains clearly the difference the modified loss is intended to make . The empirical studies in the paper are strong . They show that QTRAN++ outperforms several baselines in data efficiency and final performance across a variety of domains . Further , a comprehensive ablation study shows that each of the improvements made to QTRAN is independently important ( in at least some domains ) . # # Negatives The algorithmic contribution of the paper is relatively minor , since it provides fairly simple modifications to an existing algorithm . Experimentally , it would be good to see experiments on similar domains to those addressed in the original QTRAN paper , which are designed to probe the advantages QTRAN has over related algorithms . This would demonstrate that QTRAN++ retains the benefits of QTRAN in non-monotonic factorisable environments . # # Sources of reviewer uncertainty I am not knowledgeable enough in this domain to be certain of the coverage of the baselines and domains in the paper . Since the empirical performance of the algorithm is central to the paper , this is important .", "rating": "7: Good paper, accept", "reply_text": "We express our deep appreciation for your time and insightful comments . We are grateful for all the positive comments : providing a practical improvement for the theoretically important algorithm ( by you and R2 ) , strong empirical performance ( by you , R1 and R3 ) , novel and general ideas ( by R1 ) , and clear writing ( by R1 and R3 ) . In the revised manuscript , we have substantially updated or newly added ( Section 2 , Section 3 , Figure 3 , Figure 4 , Appendix B , D , E , F ) according to the initial reviews and colored them red . In the following , we address your comments one by one . * * 1.The algorithmic contribution of the paper is relatively minor since it provides fairly simple modifications to an existing algorithm . * * Despite being simple , we believe our QTRAN++ to deliver a significant contribution by closing the gap between theory and practice , i.e. , QTRAN++ improves the empirical performance of a theoretically interesting algorithm . Furthermore , the algorithmic contribution of QTRAN++ stems not only from proposing the simple modifications but also from effectively combining the modifications to yield a large overall gain in performance . Indeed , as shown in Figure 4 , the modifications are complementary , and omitting just one of our modifications results in a degradation of performance for at least one of the considered scenarios . We added the respective discussion in Section 1 of our revised paper . * * 2.It would be good to see experiments on similar domains to those addressed in the original QTRAN paper . * * Thank you for the suggestion . To incorporate your comments , we evaluated our QTRAN++ using the multi-domain Gaussian squeeze environment from the original QTRAN paper [ 1 ] . The corresponding results are reported in Appendix F of our revised paper . In the experiments , one can observe how QTRAN++ consistently achieves the best result compared to the baselines including QTRAN . Hence , one may conclude that QTRAN++ achieves state-of-the-art performance across different environments and still retains the main strengths of QTRAN . * * References * * [ 1 ] Son et al.QTRAN : Learning to factorize with transformation for cooperative multi-agent reinforcement learning , ICML 2019"}}