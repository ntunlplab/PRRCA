{"year": "2019", "forum": "SkgQBn0cF7", "title": "Modeling the Long Term Future in Model-Based Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper explores the use of multi-step latent variable models of the dynamics in imitation learning, planning, and finding sub-goals. The reviewers found the approach to be interesting. The initial experiments were a main weakpoint in the initial submission. However, the authors updated the experimental results to address these concerns to a significant degree. The reviewers all agree that the paper is above the bar for acceptance. I recommend accept.", "reviews": [{"review_id": "SkgQBn0cF7-0", "review_text": "After the rebuttal and the authors providing newer experimental results, I've increased my score. They have addressed both the issue with the phrasing of the auxiliary loss, which I'm very happy they did as well as provided more solid experimental results, which in my opinion make the paper strong enough for publication. ##### The paper proposes a variational framework for learning a Model of both the environment and the actor's policy in Reinforcement Learning. Specifically, the model is a deterministic RNN which at every step takes as input also a new stochastic latent variable z_t. Compared to more standard approaches, the prior over z_t is not standard normal but depends on the previously hidden state. The inference model combines information from the forward generative hidden state and a backward RNN that looks only at future observations. Finally, an auxiliary loss is added to the model that tries to predict the future states of the backward RNN using the latent variable z_t. The idea of the paper is quite well presented and concise. The paper tests the proposed framework on several RL benchmarks. Using it for imitation learning outperforms two baseline models: behaviour cloning and behaviour cloning trained with an auxiliary loss of predicting the next observation. Although the results are good, it would have been much better if there was also a comparison against a Generative model (identical to the one proposed) without the auxiliary loss added? The authors claim that the results of the experiment suggest that the auxiliary loss is indeed helping, where I find the evidence unconvincing given that there is no comparison against this obvious baseline. Extra comparison against the method from [1] or GAIL would make the results even stronger, but it is understandable that one can not compare against everything, hence I do not see this as a major issue. The authors also compare on long-horizon video prediction. Although their method outperforms the method proposed in Ha & Schmidhuber, this by no means suggests that the method is really that superior. I would argue that in terms of future video prediction that [3] provides significantly better results than the World Models, nevertheless, at least one more baseline would have supported the authors claims much better. On the Model-Based planning, the authors outperform SeCTAR model on the BabyAI tasks and the Wheeled locomotion. This result is indeed interesting and shows that the method is viable for planning. However, given that similar result has been shown in [1] regarding the planning framework it is unclear how novel the result is. In conclusion, the paper presents a generative model for training a model-based approach with an auxiliary loss. The results look promising, however, stronger baselines and better ablation of how do different components actually contribute would make the paper significantly stronger than it is at the moment. Below are a few further comments on some specific parts of the paper. A few comments regarding relevant literature: Both in the introduction and during the main text the authors have not cited [1] which I think is a very closely related method. In this work similarly, a generative model of future segments is learned using a variational framework. In addition, the MPC procedure that the authors present in this paper is not novel, but has already been proposed and tried in [1] - optimizing over the latent variables rather than the actions directly, and there have been named Latent Action Priors. The data gathering process is also not a new idea and using the error in a dynamics model for exploration is a well-known method, usually referred to as curiosity, for instance see [2] and some of the cited papers as Pathak et. al., Stadie et. al. - these all should be at least cited in section 3.2.2 as well not only in the background section regarding different topics. On the auxiliary loss: The authors claim that they train the auxiliary loss using Variational Inference, yet they drop the KL term, which is \"kinda\" an important feature of VI. Auxiliary losses are well understood that often help in RL, hence there is no need to over-conceptualize the idea of adding the extra term log p(b|z) as a VI and then doing something else. It would be much more clear and concise just to introduce it as an extra term and motivate it without referring to the VI framework, which the authors do not use for it (they still use it for the main generative model). The only way that this would have been acceptable if the experiment section contained experiments with the full VI objective as equation (6) suggest and without the sharing of the variational priors and posteriors and compared them against what they have done in the current version of the manuscript. A minor mistake seems to be that equation (5) and (7) have double counted log p(z_t|h_t-1) since they are written as an explicit term as well as they appear in the KL(q(z_t|..)|p(z_t|h_t-1)). [1] Prediction and Control with Temporal Segment Models [Nikhil Mishra, Pieter Abbeel, Igor Mordatch, 2017] [2] Large-Scale Study of Curiosity-Driven Learning [Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A. Efros, 2018] [3] Action-Conditional Video Prediction using Deep Networks in Atari Games [Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh, 2015] ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for such a detailed feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions . We briefly summarize the key idea of the paper and then address the specific concerns . Q : \u201c it would have been much better if there was also a comparison against a Generative model ( identical to the one proposed ) without the auxiliary loss added ? \u201c We thank the reviewer for pointing this out . We have updated our paper with comparisons to a generative model identical to the one we proposed but without the auxiliary cost being added . We ran comparisons against the proposed baseline ( our model without auxiliary cost ) both on imitation learning ( Reacher , CarRacing ) and RL ( wheeled locomotion ) . Our method outperforms the baseline in all cases . Q : \u201c However , given that similar result has been shown in [ 1 ] regarding the planning framework it is unclear how novel the result is . \u201c The reviewer is right . We are not suggesting the use of the proposed method is novel for planning . The novelty comes from using bidirectional inference network and using the auxiliary cost for exploration . We showed that the method learns a better inference network by using the proposed method for planning , and showing that the proposed method outperforms more complicated and state of the art methods like [ 1 ] , [ 2 ] . [ 1 ] Sectar . https : //arxiv.org/abs/1806.02813 [ 2 ] Learning and Querying Generative Models for RL https : //arxiv.org/abs/1802.03006"}, {"review_id": "SkgQBn0cF7-1", "review_text": "The authors claim that long-term prediction as a key issue in model-based reinforcement learning. Based on that, they propose a fairly specific model to which is then improved with Z-forcing to achieve better performance. ## Major The main issue with the paper is that the premise is not convincing to me. It is based on four works which (to me) appear to focus on auto-regressive models. In this submission, latent variable models are considered. The basis for sequential LVMs suffering from these problems is therefore not given by the literature. That alone would not be much of an issue, since the problem could also be shown to exist in this context in the paper. But the way I understand the experimental section, the approach without the auxiliary cost is not even evaluated. Therefore, we cannot assess if it is that alone which improves the method. The central hypothesis of the paper is not properly tested. Apart from that, the paper appears to have been written in haste. There are numerous typos in text and in equations (e.g. $dz$ missing from integrals). To reconsider my assessment, I think it should be shown that the problem of long-term future prediction exists in the context of sequential LVMs. Maybe this is obvious for ppl more knowledgeable in the field, but this paper fails to make that point by either pointing out relevant references or containing the necessary experiments. Especially since other works have made model-based control work in challenging environments: - Buesing, Lars, et al. \"Learning and Querying Fast Generative Models for Reinforcement Learning.\" *arXiv preprint arXiv:1802.03006* (2018). - Karl, M., Soelch, M., Becker-Ehmck, P., Benbouzid, D., van der Smagt, P., & Bayer, J. (2017). Unsupervised Real-Time Control through Variational Empowerment. *arXiv preprint arXiv:1710.05101*. ## Minor - The authors chose to use the latent states for planning. This turns the optimisation into a POMDP problem. How is the latent state inferred at run time? How do we assure that the policy is still optimal? - Application of learning models to RL is not novel, see references above. But maybe this is a misunderstanding on my side, as the Buesing paper is cited in the related work. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for such a detailed feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have added the extra references as per the reviewer 's suggestions . Q : \u201c The main issue with the paper is that the premise is not convincing to me\u2026. , latent variable models are considered . The basis for sequential LVMs suffering from these problems is therefore not given by the literature. \u201d We thank the reviewer for pointing this out . There exists a rich literature which advocates learning sequential LVMs , and mentions that its difficult to learn sequential LVMs . In particular , [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] , [ 6 ] , [ 7 ] , [ 8 ] observes that it is difficult extracting meaningful representations for latent variables when a powerful autoregressive model is used . We will update the paper to reflect these . [ 1 ] A recurrent latent variable model for sequential data , https : //arxiv.org/abs/1506.02216 [ 2 ] Variational Lossy Autoencoder , https : //arxiv.org/abs/1611.02731 [ 3 ] Generating sentences from a continuous space , https : //arxiv.org/abs/1511.06349 [ 4 ] Sequential neural models with stochastic layers https : //arxiv.org/abs/1605.07571 [ 5 ] A hierarchical latent variable encoder-decoder model for generating dialogues . https : //arxiv.org/abs/1605.06069 [ 6 ] Gulrajani , I. , Kumar , K. , Ahmed , F. , Taiga , A . A. , Visin , F. , Vazquez , D. , and Courville , A . ( 2016 ) .Pixelvae : A latent variable model for natural images . arXiv preprint arXiv:1611.05013 . [ 7 ] Variational Autoencoders for semi supervised text classification https : //arxiv.org/abs/1603.02514 [ 8 ] . GOYAL , Anirudh Goyal ALIAS PARTH , et al . `` Z-Forcing : Training stochastic recurrent networks . '' Advances in Neural Information Processing Systems . 2017.This was also observed in our paper , as our method without auxiliary cost noticeably under-performs compared to our method with auxiliary cost which suggests that auxiliary cost acts as an important regularizer . Q : \u201c That alone would not be much of an issue , since the problem could also be shown to exist in this context in the paper . But the way I understand the experimental section , the approach without the auxiliary cost is not even evaluated. \u201d We thank the reviewer for pointing this out and we agree that this is an important baseline . We ran comparisons against the proposed baseline ( our model without auxiliary cost ) both on imitation learning ( Reacher ) and RL ( wheeled locomotion ) . Our method outperforms the baseline in all cases . We also ran other experiments to compare the proposed method with state of the art methods [ 1 ] like the one mentioned by the reviewer . [ 1 ] Buesing , Lars , et al . `` Learning and Querying Fast Generative Models for Reinforcement Learning . '' * arXiv preprint arXiv:1802.03006 * ( 2018 ) . Q : \u201c There are numerous typos in text and in equations ( e.g. $ dz $ missing from integrals ) . \u201d We thank the reviewer for pointing these out . We have corrected these typos in the updated paper ."}, {"review_id": "SkgQBn0cF7-2", "review_text": "The paper introduces an interesting approach to model learning for imitation and RL. Given the problem of maintaining multi-step predictions in the context of sequential decision making process, and deficiencies faced during planning with one-step models [1][2], it\u2019s imperative to explore approaches that do multi-step predictions. This paper combines ideas from learning sequential latent models with making multi-step future predictions as an auxiliary loss to improve imitation learning performance, efficiency of planning and finding sub-goals in a partially observed domain. From what I understand there are quite a few components in the architecture. The generative part uses the latent variables z_t and LSTM hidden state h_t to find the factored autoregressive distribution p_\\theta. It\u2019s slightly unclear how their parameters are structured and what parameters are shared (if any). I understand these are hard to describe in text, so hopefully the source code for the experiments will be made available. On the inference side, the paper makes a few choices to make the posterior approximation. It would be useful to describe the intuitions behind the choices especially the dependence of the posterior on actions a_{t-1}:T because it seems like the actions _should_ be fairly important for modeling the dynamics in a stochastic system. In the auxiliary cost, it\u2019s unclear what q(z|h) you are referring to in the primary model. It\u2019s only when I carefully read Eq 7, that I realized that it\u2019s p_\\theta(z|h) from the generator. Slightly unsure about the details of the imitation and RL (MPC + PPO + Model learning) experiments. How large is the replay buffer? What\u2019s the value of k? It would be interesting how the value of k affects learning performance. It\u2019s unclear how many seeds experiments were repeated with. Overall it\u2019s an interesting paper. Not sure if the ideas really do scale to \u201clong-horizon\u201d problems. The MuJoCo tasks don\u2019t need good long horizon models and the BabyAI problem seems fairly small. - Minor points Sec 2.3: not sensitive *to* how different Algorithm 2: *replay* buffer [1]: https://arxiv.org/abs/1612.06018 [2]: https://arxiv.org/abs/1806.01825", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for such a detailed feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have added the extra references as per the reviewer 's suggestions . We have conducted additional experiments to compare the proposed model to the state of the art state space model . We ask the reviewer to refer to the heading `` Comparison with state of the art state space model - ALL REVIEWERS '' Q : \u201c From what I understand there are quite a few components in the architecture . The generative part uses the latent variables z_t and LSTM hidden state h_t to find the factored autoregressive distribution p_\\theta . It \u2019 s slightly unclear how their parameters are structured and what parameters are shared ( if any ) . I understand these are hard to describe in text , so hopefully the source code for the experiments will be made available. \u201d We thank the reviewer for pointing this out . We will publish our source code . Our model consists of a forward and backward RNN , with the forward and backward RNNs do not share parameters . Q : \u201c On the inference side , the paper makes a few choices to make the posterior approximation . It would be useful to describe the intuitions behind the choices especially the dependence of the posterior on actions a_ { t-1 } : T because it seems like the actions _should_ be fairly important for modeling the dynamics in a stochastic system. \u2018 We thank the reviewer for pointing this out . In principle , the posterior should depend on future actions . To take into account the dependence on future actions as well as future observations , we can use the LSTM that processes the observation-action sequence backwards . In pilot trials , we conducted experiments with and without the dependencies on actions for the backward LSTM and we didn \u2019 t notice a noticeable difference in terms of performance . We hence chose to drop the dependencies on actions in the backward LSTM to simplify the code . We have updated the paper ( appendix ) to clarify this difference ."}], "0": {"review_id": "SkgQBn0cF7-0", "review_text": "After the rebuttal and the authors providing newer experimental results, I've increased my score. They have addressed both the issue with the phrasing of the auxiliary loss, which I'm very happy they did as well as provided more solid experimental results, which in my opinion make the paper strong enough for publication. ##### The paper proposes a variational framework for learning a Model of both the environment and the actor's policy in Reinforcement Learning. Specifically, the model is a deterministic RNN which at every step takes as input also a new stochastic latent variable z_t. Compared to more standard approaches, the prior over z_t is not standard normal but depends on the previously hidden state. The inference model combines information from the forward generative hidden state and a backward RNN that looks only at future observations. Finally, an auxiliary loss is added to the model that tries to predict the future states of the backward RNN using the latent variable z_t. The idea of the paper is quite well presented and concise. The paper tests the proposed framework on several RL benchmarks. Using it for imitation learning outperforms two baseline models: behaviour cloning and behaviour cloning trained with an auxiliary loss of predicting the next observation. Although the results are good, it would have been much better if there was also a comparison against a Generative model (identical to the one proposed) without the auxiliary loss added? The authors claim that the results of the experiment suggest that the auxiliary loss is indeed helping, where I find the evidence unconvincing given that there is no comparison against this obvious baseline. Extra comparison against the method from [1] or GAIL would make the results even stronger, but it is understandable that one can not compare against everything, hence I do not see this as a major issue. The authors also compare on long-horizon video prediction. Although their method outperforms the method proposed in Ha & Schmidhuber, this by no means suggests that the method is really that superior. I would argue that in terms of future video prediction that [3] provides significantly better results than the World Models, nevertheless, at least one more baseline would have supported the authors claims much better. On the Model-Based planning, the authors outperform SeCTAR model on the BabyAI tasks and the Wheeled locomotion. This result is indeed interesting and shows that the method is viable for planning. However, given that similar result has been shown in [1] regarding the planning framework it is unclear how novel the result is. In conclusion, the paper presents a generative model for training a model-based approach with an auxiliary loss. The results look promising, however, stronger baselines and better ablation of how do different components actually contribute would make the paper significantly stronger than it is at the moment. Below are a few further comments on some specific parts of the paper. A few comments regarding relevant literature: Both in the introduction and during the main text the authors have not cited [1] which I think is a very closely related method. In this work similarly, a generative model of future segments is learned using a variational framework. In addition, the MPC procedure that the authors present in this paper is not novel, but has already been proposed and tried in [1] - optimizing over the latent variables rather than the actions directly, and there have been named Latent Action Priors. The data gathering process is also not a new idea and using the error in a dynamics model for exploration is a well-known method, usually referred to as curiosity, for instance see [2] and some of the cited papers as Pathak et. al., Stadie et. al. - these all should be at least cited in section 3.2.2 as well not only in the background section regarding different topics. On the auxiliary loss: The authors claim that they train the auxiliary loss using Variational Inference, yet they drop the KL term, which is \"kinda\" an important feature of VI. Auxiliary losses are well understood that often help in RL, hence there is no need to over-conceptualize the idea of adding the extra term log p(b|z) as a VI and then doing something else. It would be much more clear and concise just to introduce it as an extra term and motivate it without referring to the VI framework, which the authors do not use for it (they still use it for the main generative model). The only way that this would have been acceptable if the experiment section contained experiments with the full VI objective as equation (6) suggest and without the sharing of the variational priors and posteriors and compared them against what they have done in the current version of the manuscript. A minor mistake seems to be that equation (5) and (7) have double counted log p(z_t|h_t-1) since they are written as an explicit term as well as they appear in the KL(q(z_t|..)|p(z_t|h_t-1)). [1] Prediction and Control with Temporal Segment Models [Nikhil Mishra, Pieter Abbeel, Igor Mordatch, 2017] [2] Large-Scale Study of Curiosity-Driven Learning [Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A. Efros, 2018] [3] Action-Conditional Video Prediction using Deep Networks in Atari Games [Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh, 2015] ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for such a detailed feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions . We briefly summarize the key idea of the paper and then address the specific concerns . Q : \u201c it would have been much better if there was also a comparison against a Generative model ( identical to the one proposed ) without the auxiliary loss added ? \u201c We thank the reviewer for pointing this out . We have updated our paper with comparisons to a generative model identical to the one we proposed but without the auxiliary cost being added . We ran comparisons against the proposed baseline ( our model without auxiliary cost ) both on imitation learning ( Reacher , CarRacing ) and RL ( wheeled locomotion ) . Our method outperforms the baseline in all cases . Q : \u201c However , given that similar result has been shown in [ 1 ] regarding the planning framework it is unclear how novel the result is . \u201c The reviewer is right . We are not suggesting the use of the proposed method is novel for planning . The novelty comes from using bidirectional inference network and using the auxiliary cost for exploration . We showed that the method learns a better inference network by using the proposed method for planning , and showing that the proposed method outperforms more complicated and state of the art methods like [ 1 ] , [ 2 ] . [ 1 ] Sectar . https : //arxiv.org/abs/1806.02813 [ 2 ] Learning and Querying Generative Models for RL https : //arxiv.org/abs/1802.03006"}, "1": {"review_id": "SkgQBn0cF7-1", "review_text": "The authors claim that long-term prediction as a key issue in model-based reinforcement learning. Based on that, they propose a fairly specific model to which is then improved with Z-forcing to achieve better performance. ## Major The main issue with the paper is that the premise is not convincing to me. It is based on four works which (to me) appear to focus on auto-regressive models. In this submission, latent variable models are considered. The basis for sequential LVMs suffering from these problems is therefore not given by the literature. That alone would not be much of an issue, since the problem could also be shown to exist in this context in the paper. But the way I understand the experimental section, the approach without the auxiliary cost is not even evaluated. Therefore, we cannot assess if it is that alone which improves the method. The central hypothesis of the paper is not properly tested. Apart from that, the paper appears to have been written in haste. There are numerous typos in text and in equations (e.g. $dz$ missing from integrals). To reconsider my assessment, I think it should be shown that the problem of long-term future prediction exists in the context of sequential LVMs. Maybe this is obvious for ppl more knowledgeable in the field, but this paper fails to make that point by either pointing out relevant references or containing the necessary experiments. Especially since other works have made model-based control work in challenging environments: - Buesing, Lars, et al. \"Learning and Querying Fast Generative Models for Reinforcement Learning.\" *arXiv preprint arXiv:1802.03006* (2018). - Karl, M., Soelch, M., Becker-Ehmck, P., Benbouzid, D., van der Smagt, P., & Bayer, J. (2017). Unsupervised Real-Time Control through Variational Empowerment. *arXiv preprint arXiv:1710.05101*. ## Minor - The authors chose to use the latent states for planning. This turns the optimisation into a POMDP problem. How is the latent state inferred at run time? How do we assure that the policy is still optimal? - Application of learning models to RL is not novel, see references above. But maybe this is a misunderstanding on my side, as the Buesing paper is cited in the related work. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for such a detailed feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have added the extra references as per the reviewer 's suggestions . Q : \u201c The main issue with the paper is that the premise is not convincing to me\u2026. , latent variable models are considered . The basis for sequential LVMs suffering from these problems is therefore not given by the literature. \u201d We thank the reviewer for pointing this out . There exists a rich literature which advocates learning sequential LVMs , and mentions that its difficult to learn sequential LVMs . In particular , [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] , [ 6 ] , [ 7 ] , [ 8 ] observes that it is difficult extracting meaningful representations for latent variables when a powerful autoregressive model is used . We will update the paper to reflect these . [ 1 ] A recurrent latent variable model for sequential data , https : //arxiv.org/abs/1506.02216 [ 2 ] Variational Lossy Autoencoder , https : //arxiv.org/abs/1611.02731 [ 3 ] Generating sentences from a continuous space , https : //arxiv.org/abs/1511.06349 [ 4 ] Sequential neural models with stochastic layers https : //arxiv.org/abs/1605.07571 [ 5 ] A hierarchical latent variable encoder-decoder model for generating dialogues . https : //arxiv.org/abs/1605.06069 [ 6 ] Gulrajani , I. , Kumar , K. , Ahmed , F. , Taiga , A . A. , Visin , F. , Vazquez , D. , and Courville , A . ( 2016 ) .Pixelvae : A latent variable model for natural images . arXiv preprint arXiv:1611.05013 . [ 7 ] Variational Autoencoders for semi supervised text classification https : //arxiv.org/abs/1603.02514 [ 8 ] . GOYAL , Anirudh Goyal ALIAS PARTH , et al . `` Z-Forcing : Training stochastic recurrent networks . '' Advances in Neural Information Processing Systems . 2017.This was also observed in our paper , as our method without auxiliary cost noticeably under-performs compared to our method with auxiliary cost which suggests that auxiliary cost acts as an important regularizer . Q : \u201c That alone would not be much of an issue , since the problem could also be shown to exist in this context in the paper . But the way I understand the experimental section , the approach without the auxiliary cost is not even evaluated. \u201d We thank the reviewer for pointing this out and we agree that this is an important baseline . We ran comparisons against the proposed baseline ( our model without auxiliary cost ) both on imitation learning ( Reacher ) and RL ( wheeled locomotion ) . Our method outperforms the baseline in all cases . We also ran other experiments to compare the proposed method with state of the art methods [ 1 ] like the one mentioned by the reviewer . [ 1 ] Buesing , Lars , et al . `` Learning and Querying Fast Generative Models for Reinforcement Learning . '' * arXiv preprint arXiv:1802.03006 * ( 2018 ) . Q : \u201c There are numerous typos in text and in equations ( e.g. $ dz $ missing from integrals ) . \u201d We thank the reviewer for pointing these out . We have corrected these typos in the updated paper ."}, "2": {"review_id": "SkgQBn0cF7-2", "review_text": "The paper introduces an interesting approach to model learning for imitation and RL. Given the problem of maintaining multi-step predictions in the context of sequential decision making process, and deficiencies faced during planning with one-step models [1][2], it\u2019s imperative to explore approaches that do multi-step predictions. This paper combines ideas from learning sequential latent models with making multi-step future predictions as an auxiliary loss to improve imitation learning performance, efficiency of planning and finding sub-goals in a partially observed domain. From what I understand there are quite a few components in the architecture. The generative part uses the latent variables z_t and LSTM hidden state h_t to find the factored autoregressive distribution p_\\theta. It\u2019s slightly unclear how their parameters are structured and what parameters are shared (if any). I understand these are hard to describe in text, so hopefully the source code for the experiments will be made available. On the inference side, the paper makes a few choices to make the posterior approximation. It would be useful to describe the intuitions behind the choices especially the dependence of the posterior on actions a_{t-1}:T because it seems like the actions _should_ be fairly important for modeling the dynamics in a stochastic system. In the auxiliary cost, it\u2019s unclear what q(z|h) you are referring to in the primary model. It\u2019s only when I carefully read Eq 7, that I realized that it\u2019s p_\\theta(z|h) from the generator. Slightly unsure about the details of the imitation and RL (MPC + PPO + Model learning) experiments. How large is the replay buffer? What\u2019s the value of k? It would be interesting how the value of k affects learning performance. It\u2019s unclear how many seeds experiments were repeated with. Overall it\u2019s an interesting paper. Not sure if the ideas really do scale to \u201clong-horizon\u201d problems. The MuJoCo tasks don\u2019t need good long horizon models and the BabyAI problem seems fairly small. - Minor points Sec 2.3: not sensitive *to* how different Algorithm 2: *replay* buffer [1]: https://arxiv.org/abs/1612.06018 [2]: https://arxiv.org/abs/1806.01825", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for such a detailed feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have added the extra references as per the reviewer 's suggestions . We have conducted additional experiments to compare the proposed model to the state of the art state space model . We ask the reviewer to refer to the heading `` Comparison with state of the art state space model - ALL REVIEWERS '' Q : \u201c From what I understand there are quite a few components in the architecture . The generative part uses the latent variables z_t and LSTM hidden state h_t to find the factored autoregressive distribution p_\\theta . It \u2019 s slightly unclear how their parameters are structured and what parameters are shared ( if any ) . I understand these are hard to describe in text , so hopefully the source code for the experiments will be made available. \u201d We thank the reviewer for pointing this out . We will publish our source code . Our model consists of a forward and backward RNN , with the forward and backward RNNs do not share parameters . Q : \u201c On the inference side , the paper makes a few choices to make the posterior approximation . It would be useful to describe the intuitions behind the choices especially the dependence of the posterior on actions a_ { t-1 } : T because it seems like the actions _should_ be fairly important for modeling the dynamics in a stochastic system. \u2018 We thank the reviewer for pointing this out . In principle , the posterior should depend on future actions . To take into account the dependence on future actions as well as future observations , we can use the LSTM that processes the observation-action sequence backwards . In pilot trials , we conducted experiments with and without the dependencies on actions for the backward LSTM and we didn \u2019 t notice a noticeable difference in terms of performance . We hence chose to drop the dependencies on actions in the backward LSTM to simplify the code . We have updated the paper ( appendix ) to clarify this difference ."}}