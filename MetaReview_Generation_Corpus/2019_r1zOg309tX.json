{"year": "2019", "forum": "r1zOg309tX", "title": "Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets", "decision": "Reject", "meta_review": "The paper investigates problems that can arise for a certain version of the dual form of the Wasserstein distance, which is proved in Appendix I. While the theoretical analysis seems correct, the significance of the distribution is limited by the fact, that the specific dual form analysed is not commonly used in other works. Furthermore, the assumption that the optimal function is differentiable is often not fulfilled neither. The paper would herefore be significantly strengthen  by making more clear to which methods used in practice the insights carry over. \n", "reviews": [{"review_id": "r1zOg309tX-0", "review_text": "The authors study the fundamental problems with GAN training. By performing a gradient analysis of the value surface of the optimal discriminator, the authors identify several key issues. In particular, for a fixed GAN objective they consider the optimal discriminator f* and analyze the gradients of f* at points x ~ P_g and x~P_d. The gradient decouples into the magnitude and direction terms. In previous work, the gradient vanishing issue was identified and the authors show that it is fundamentally only controlling the magnitude. Furthermore, controlling the magnitude doesn\u2019t suffice as the gradient direction itself might be non-informative to move P_g to P_d. The authors proceed to analyze two cases: (1) No overlap between P_g and P_d where they show that the original GAN formulation, as well as the Wasserstein GAN will suffer from this issue, unless Lipschitzness is enforced. (2) For the case where P_g and P_d have overlap, the gradients will be locally useful which the authors identify as the fundamental source of mode collapse. The main theoretical result suggests that (1) penalizing the discriminator proportionally to the square of the Lipschitz constant is the key -- the choice of divergence is not. This readily implies that pure Wasserstein divergence may fail to provide useful gradients, as well as that other divergences combined with Lipschitz penalties (precise technical details in the paper) might succeed. Furthermore, it also implies that one can mix and match the components of the objective function for the discriminator, as long as the penalty is present, giving rise to many objectives which are not necessarily proper divergences. Finally, one can explain the recent success of many methods in practice: While the degenerate examples showing deficiencies of current methods can be derived, in practice we implement discriminators as some deep neural networks which induce relatively smooth value surfaces which in turn make the gradients more meaningful. Pro: - Clear setup and analysis of the considered cases. Interesting discussion from the perspective of the optimal discriminator and divergence minimization. The experiments on the toy data are definitely interesting and confirm some of the theoretical results. - A convincing discussion of why Wasserstein distance is not the key, but rather it is the Lipschitz constant. This brings some light on why the gradient penalty or spectral normalization help even for the non-saturating loss [2]. - Discussion on why 1-Lip is sufficient, but might be too strong. The authors suggest that instead of requiring 1-Lip on the entire space, it suffices to require Lipschitz continuity in the blending region of the marginal distributions. Con: - Practical considerations: I appreciate the theoretical implications of this work. However, how can we exploit this knowledge in practice? As stated by the authors, many of these issues are sidestepped by our current inductive biases in neural architectures. - Can you provide more detail on your main theorem, in particular property (d). Doesn't it imply that the discriminator is constant? - Which currently known objectives do not satisfy the assumptions of the theorem? - The work would benefit from a polishing pass. ======== Thank you for the response. Given that there is no consensus on the questions posed by AnonReviewer2, there will be no update to the score.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive feedback . Q : How can we exploit this knowledge in practice ? As stated by the authors , many of these issues are sidestepped by our current inductive biases in neural architectures . > > Although fine-tuning the neural architectures could possibly make GANs work , it is also the fundamental reason why GANs are hard to train and easily broken . Investigating theoretically-sound GAN formulation is thus important from our perspective . > > Our work actually has a few practical implications . For example , we tested a few new objectives which are also sound according to our theorems . And we found that , compared with W-distance , the outputs of the discriminator with some new objectives are more stable and the final qualities of generated samples are also consistently higher than those produced by W-distance . Q : Can you provide more detail on your main theorem , in particular , property ( d ) . Does n't it imply that the discriminator is constant ? > > The main theorem describes the properties of the optimal discriminative function under Lipschitz constraint . In a nutshell , the discriminator would stretch the value surface by adjusting the value of f ( x ) for each x ( from P_g and P_r ) till |f ( x ) -f ( y ) |=|x-y| for some y . The key insight behind the properties is that at optimum , for every x , f * ( x ) must hold a zero gradient with respect to the objective or it is bounded by the Lipschitz constraint . > > Property- ( d ) states that at the only Nash Equilibrium , it holds P_g = P_r . And it can also be proved that at the Nash Equilibrium state , k ( f * ) must be zero . k ( f * ) =0 means the discriminative function is constant , but it only appears at the converged state P_g = P_r . Constant discriminative function at convergence is actually a good/desired property , where the generator receives zero gradients and hence will not shift from the convergence point . Q : Which currently known objectives do not satisfy the assumptions of the theorem ? > > There exists a few practically used instances of GAN objectives that do not satisfy the assumptions of the theorem . For example , the objective of Least-Square GAN and the hinge loss used in [ 1 ] [ 2 ] [ 3 ] . More generally , any objective that \u2018 \u2018 holds a zero gradient at certain point \u2019 \u2019 does not satisfy the assumptions of the theorem ( check Eq.12 ) . [ 1 ] Geometric GAN [ 2 ] Energy-based Generative Adversarial Network [ 3 ] Spectral Normalization for Generative Adversarial Networks"}, {"review_id": "r1zOg309tX-1", "review_text": "The authors try to claim that Lipschitz continuity of the discriminator is a fundamental solution of GANs, and that current methods do not satisfy this approach in principle. There are several false statements in this paper. In particular, sections 2.3 and 4.4 are wrong (and most of the paper is based on statements made there). The necessary constraint for the Wasserstein distance is NOT f(x) - f(y) <= d(x, y) for all x ~ Pr, y ~ Pg. It has to actually be 1-Lipschitz in the entire space. See Chapters 5 and 6 of [1], for example remark 6.4 or particular cases 5.16 and 5.4. Indeed, this is how it is written in all of the literature this reviewer is aware off, and it's a fact well used in the literature. Indeed, all the smoothness results for optimal transport in [1] heavily exploit the fact that the gradient of the critic is in the direction of the optimal transport map, which wouldn't be the case in the situation the authors try to claim of 'f not being defined outside of the support of Pr or Pg'. Furthermore, the relationship between Lipschitz continuity and having a gradient is elaborated in [2] https://arxiv.org/abs/1701.07875 , for example figure 2 clearly show this. Furthermore, and contrary to what section 4.5 tries to claim, the idea that most conclusions of wgan hold *without* the Wasserstein distance, but with Lipschitz continuity are already elaborated in the wgan paper. See in fact, appendix G.1 [3], where this is described in detail. [1]: http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf [2]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf [3]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your constructive feedback . Q : The idea that most conclusions of WGAN hold * without * the Wasserstein distance , but with Lipschitz continuity are already elaborated in the WGAN paper . See in fact , Appendix G.1 [ 3 ] , where this is described in detail . > > Given Wasserstein GAN is one of the most important papers in GAN community and we have realized that its argument in the main text does not hold very well , we believe it is necessary to highlight the point . Currently , most people tend to believe that a good distance metric is the key to the convergence or stability of GANs . One contribution of our paper is that it thoroughly expounded that : the property of the gradient in terms of \\nabla_x f * ( x ) is substantially different from the property of a distance metric ; and to ensure the convergence of GANs or design new formulation for GANs , one should carefully check whether the gradient \\nabla_x f * ( x ) is reliable . > > Regarding Appendix G.1 [ 3 ] , although it states that Lipschitz might be generally applicable , the discussion there is far from enough : ( i ) the discussion in Appendix G.1 is limited to the objective of the original GAN ; our theorem , in contrast , elaborated a family of GAN objectives and characterized the necessary condition where Lipschitz condition ensures the convergence . ( ii ) the discussion in Appendix G.1 ignores the $ \\log $ term in the objective of original GAN ; our theorem is , however , directly applicable to the whole objective of the original GAN . * ( iii ) according to our theorem , for any objective other than W-distance , it is theoretically necessary to penalize the Lipschitz constant k ( f ) to ensure the convergence ; though Appendix G.1 mentioned that to avoid the saturation , k need to be small , it fails to cover the other fold that \u2018 \u2018 \\nabla_x f * ( x ) =0 for all x \u2019 \u2019 might also happen even if there is no saturation region or saturation region is not touched . * * [ 1 ] : Optimal transport , old and new [ 2 ] : http : //proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf [ 3 ] : http : //proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf [ 4 ] : Improved Training of Wasserstein GANs * In Appendix G.1 , it discusses the properties of f with bounded value-range ( to simulate a classifier ) , while in our paper , f is assumed to have unbounded value range and loss metrics are applied to the unbounded f. Therefore , the arguments are actually quite different . * * That is to say , small k is not enough to guarantee the convergence , and penalizing/decreasing the Lipschitz constant is necessary . Given a fixed Lipschitz constant k , according to our analysis , the following state is possible : \u2018 \u2018 Pg ! =Pr \u2019 \u2019 and \u2018 \u2018 for each x , f * ( x ) is optimal \u2019 \u2019 , but there does not exist two points x , y such that |f ( x ) -f ( y ) | > k|x-y| . In this case , \u2018 \u2018 \\nabla_x f * ( x ) =0 for all x \u2019 \u2019 and the generator stop learning , however , Pg does not equal to Pr ."}, {"review_id": "r1zOg309tX-2", "review_text": " [pros] - It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases. - It also proposes use of the penalty term in terms of the Lipschitz constant of the discriminative function. [cons] - Some of the arguments on the Wasserstein distance and on WGAN are not sound. - Theorem 3 does not make sense. - The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017). [Quality] I found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below. [Clarity] The main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions. [Originality] Despite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\\phi,\\varphi,\\psi$ and the form of the gradient penalty, $\\max \\|\\nabla f(x)\\|_2^2$ in this paper versus $E[(\\|\\nabla f(x)\\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal. [Significance] This paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning. Detailed comments: In Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation. It is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the \"critic\" $f$ via a multilayer neural network with weight clipping. One can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading. On optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\\sim P_g}[f(x)]-E_{x\\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. I do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes \"$\\forall x \\not= y$\", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\\|x-y\\|$ to obtain $f(x)-f(y)=k\\|y-x\\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \\not= y$ such that $f(y)-f(x)=k\\|x-y\\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\\|x-y\\|$ under the Lipschitz condition. Appendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\\partial J_D/\\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as \"arg min\" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function. Page 5, line 36: $J_D(x)$ appears without explicit definition. Page 23, lines 34 and 38: Cluttered expression $\\frac{\\partial [}{\\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive feedback . Q : In Section 2.3 , the authors criticize the use of the Wasserstein distance as the distance function of GANs , but their criticism is off the point . This is not the problem of the Wasserstein distance itself , but the problem regarding how the dual problem is implemented in learning of GANs . In practice , however , one may regard that $ f $ satisfies the condition $ f ( x ) -f ( y ) \\le d ( x , y ) $ not only on the supports of $ P_g $ and $ P_r $ but throughout the entire space $ \\mathbb { R } ^n $ . > > We have refined our statements to make it more rigorous . We argued that Wasserstein distance in the dual form with compacted constraint will fail to provide valid gradients . We have revised related sections . It is much clearer now . > > The practical solution of WGAN , which usually involves Lipschitz constraint , is sound . What we want to emphasize is that the explanation on the underlining working mechanism in the Wasserstein GAN paper does not hold very well . It is not Wasserstein distance which can properly measure the distance that makes it work , but the Lipschitz constraint . As we showed in the paper , in the compact dual form ( without Lipschitz ) of Wasserstein distance , $ \\nabla_x f * ( x ) $ may also fail to provide a meaningful gradient ; and with Lipschitz , lots of GAN objectives can guarantee a meaningful gradient in terms of $ \\nabla_x f * ( x ) $ , not limited to Wasserstein distance . Q : The claim in Theorem 3 does not make sense . If we literally take its statement , it would imply $ k=0 $ , and consequently , $ f $ is constant . One would be able to prove that if there exists $ ( x , y ) $ with $ x \\not= y $ such that $ f ( y ) -f ( x ) =k\\|x-y\\| $ holds then the gradient of $ f $ at $ x_t $ is equal to $ k ( y-x ) /\\|x-y\\| $ under the Lipschitz condition . > > Sorry , this is a typo . We indeed mean \u201c $ \\forall x \\not= y $ , if $ f ( y ) -f ( x ) =k\\|x-y\\| $ \u201d as what you have guessed , but it was miswritten as \u201c if $ f ( y ) -f ( x ) =k\\|x-y\\| $ , $ \\forall x \\neq y $ \u201d . We have revised it in our new version . Q : On optimizing $ k $ , I do not agree with the authors ' claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $ f $ even when $ P_g=P_r $ . Indeed , when $ P_g=P_r $ , for any measurable function $ f $ one trivially has $ J_D [ f ] =E_ { x\\sim P_g } [ f ( x ) ] -E_ { x\\sim P_r } [ f ( x ) ] =0 $ , so that the functional derivative of $ J_D $ with respect to $ f $ does vanish identically . > > Sorry for the typo . It should be \u2018 \u2018 $ \\nabla_x f * ( x ) $ may not be zero '' . We have miswritten it as $ \\nabla_f * $ . For WGAN , when $ P_g=P_r $ , f * can be arbitrary , so $ \\nabla_x f * ( x ) $ may not be zero . We have revised it in our new version . -- -- - Minor Points -- -- - Q : Page 5 , line 36 : $ J_D ( x ) $ appears without explicit definition . > > Thanks for pointing this out . We have added the definition for $ J_D ( x ) $ in the revised version . Q : Appendix G : Some notations should be made more precise . For example , in the definition of $ J_D $ the variable of integration $ x $ has been integrated out so that $ J_D $ no longer has $ x $ as its variable . The expression $ \\partial J_D/\\partial x $ does not make any sense . > > By $ \\partial J_D/\\partial x $ , we want to refer to the term that is being integrated over $ x $ . In the current form , it is analogous to the gradient of indefinite integral . i.e. , F = \\int_x f ( x ) dx and F \u2019 = f ( x ) . We have replaced the confusing derivative form with a simple notation . Q : Appendix G : $ J_D^ * ( k ) $ is defined as `` arg min '' of $ J_D $ , implying as if $ J_D^ * ( k ) $ were a $ k $ -Lipschitz function . > > $ J_D^ * ( k ) $ should be defined as the min value , not arg min . We have corrected it . Q : Page 23 , lines 34 and 38 : cluttered expression $ \\frac { \\partial [ } { \\partial 2 } ] $ makes the statements not understandable . It also appears on page 24 several times . > > The $ \\frac { \\partial [ } { \\partial 2 } ] $ comes from a breaking \\newcommand for second-order derivation in latex . We have fixed it . Thanks a lot for the careful reading of our paper and the detailed comments , which are very helpful ."}], "0": {"review_id": "r1zOg309tX-0", "review_text": "The authors study the fundamental problems with GAN training. By performing a gradient analysis of the value surface of the optimal discriminator, the authors identify several key issues. In particular, for a fixed GAN objective they consider the optimal discriminator f* and analyze the gradients of f* at points x ~ P_g and x~P_d. The gradient decouples into the magnitude and direction terms. In previous work, the gradient vanishing issue was identified and the authors show that it is fundamentally only controlling the magnitude. Furthermore, controlling the magnitude doesn\u2019t suffice as the gradient direction itself might be non-informative to move P_g to P_d. The authors proceed to analyze two cases: (1) No overlap between P_g and P_d where they show that the original GAN formulation, as well as the Wasserstein GAN will suffer from this issue, unless Lipschitzness is enforced. (2) For the case where P_g and P_d have overlap, the gradients will be locally useful which the authors identify as the fundamental source of mode collapse. The main theoretical result suggests that (1) penalizing the discriminator proportionally to the square of the Lipschitz constant is the key -- the choice of divergence is not. This readily implies that pure Wasserstein divergence may fail to provide useful gradients, as well as that other divergences combined with Lipschitz penalties (precise technical details in the paper) might succeed. Furthermore, it also implies that one can mix and match the components of the objective function for the discriminator, as long as the penalty is present, giving rise to many objectives which are not necessarily proper divergences. Finally, one can explain the recent success of many methods in practice: While the degenerate examples showing deficiencies of current methods can be derived, in practice we implement discriminators as some deep neural networks which induce relatively smooth value surfaces which in turn make the gradients more meaningful. Pro: - Clear setup and analysis of the considered cases. Interesting discussion from the perspective of the optimal discriminator and divergence minimization. The experiments on the toy data are definitely interesting and confirm some of the theoretical results. - A convincing discussion of why Wasserstein distance is not the key, but rather it is the Lipschitz constant. This brings some light on why the gradient penalty or spectral normalization help even for the non-saturating loss [2]. - Discussion on why 1-Lip is sufficient, but might be too strong. The authors suggest that instead of requiring 1-Lip on the entire space, it suffices to require Lipschitz continuity in the blending region of the marginal distributions. Con: - Practical considerations: I appreciate the theoretical implications of this work. However, how can we exploit this knowledge in practice? As stated by the authors, many of these issues are sidestepped by our current inductive biases in neural architectures. - Can you provide more detail on your main theorem, in particular property (d). Doesn't it imply that the discriminator is constant? - Which currently known objectives do not satisfy the assumptions of the theorem? - The work would benefit from a polishing pass. ======== Thank you for the response. Given that there is no consensus on the questions posed by AnonReviewer2, there will be no update to the score.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive feedback . Q : How can we exploit this knowledge in practice ? As stated by the authors , many of these issues are sidestepped by our current inductive biases in neural architectures . > > Although fine-tuning the neural architectures could possibly make GANs work , it is also the fundamental reason why GANs are hard to train and easily broken . Investigating theoretically-sound GAN formulation is thus important from our perspective . > > Our work actually has a few practical implications . For example , we tested a few new objectives which are also sound according to our theorems . And we found that , compared with W-distance , the outputs of the discriminator with some new objectives are more stable and the final qualities of generated samples are also consistently higher than those produced by W-distance . Q : Can you provide more detail on your main theorem , in particular , property ( d ) . Does n't it imply that the discriminator is constant ? > > The main theorem describes the properties of the optimal discriminative function under Lipschitz constraint . In a nutshell , the discriminator would stretch the value surface by adjusting the value of f ( x ) for each x ( from P_g and P_r ) till |f ( x ) -f ( y ) |=|x-y| for some y . The key insight behind the properties is that at optimum , for every x , f * ( x ) must hold a zero gradient with respect to the objective or it is bounded by the Lipschitz constraint . > > Property- ( d ) states that at the only Nash Equilibrium , it holds P_g = P_r . And it can also be proved that at the Nash Equilibrium state , k ( f * ) must be zero . k ( f * ) =0 means the discriminative function is constant , but it only appears at the converged state P_g = P_r . Constant discriminative function at convergence is actually a good/desired property , where the generator receives zero gradients and hence will not shift from the convergence point . Q : Which currently known objectives do not satisfy the assumptions of the theorem ? > > There exists a few practically used instances of GAN objectives that do not satisfy the assumptions of the theorem . For example , the objective of Least-Square GAN and the hinge loss used in [ 1 ] [ 2 ] [ 3 ] . More generally , any objective that \u2018 \u2018 holds a zero gradient at certain point \u2019 \u2019 does not satisfy the assumptions of the theorem ( check Eq.12 ) . [ 1 ] Geometric GAN [ 2 ] Energy-based Generative Adversarial Network [ 3 ] Spectral Normalization for Generative Adversarial Networks"}, "1": {"review_id": "r1zOg309tX-1", "review_text": "The authors try to claim that Lipschitz continuity of the discriminator is a fundamental solution of GANs, and that current methods do not satisfy this approach in principle. There are several false statements in this paper. In particular, sections 2.3 and 4.4 are wrong (and most of the paper is based on statements made there). The necessary constraint for the Wasserstein distance is NOT f(x) - f(y) <= d(x, y) for all x ~ Pr, y ~ Pg. It has to actually be 1-Lipschitz in the entire space. See Chapters 5 and 6 of [1], for example remark 6.4 or particular cases 5.16 and 5.4. Indeed, this is how it is written in all of the literature this reviewer is aware off, and it's a fact well used in the literature. Indeed, all the smoothness results for optimal transport in [1] heavily exploit the fact that the gradient of the critic is in the direction of the optimal transport map, which wouldn't be the case in the situation the authors try to claim of 'f not being defined outside of the support of Pr or Pg'. Furthermore, the relationship between Lipschitz continuity and having a gradient is elaborated in [2] https://arxiv.org/abs/1701.07875 , for example figure 2 clearly show this. Furthermore, and contrary to what section 4.5 tries to claim, the idea that most conclusions of wgan hold *without* the Wasserstein distance, but with Lipschitz continuity are already elaborated in the wgan paper. See in fact, appendix G.1 [3], where this is described in detail. [1]: http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf [2]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf [3]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your constructive feedback . Q : The idea that most conclusions of WGAN hold * without * the Wasserstein distance , but with Lipschitz continuity are already elaborated in the WGAN paper . See in fact , Appendix G.1 [ 3 ] , where this is described in detail . > > Given Wasserstein GAN is one of the most important papers in GAN community and we have realized that its argument in the main text does not hold very well , we believe it is necessary to highlight the point . Currently , most people tend to believe that a good distance metric is the key to the convergence or stability of GANs . One contribution of our paper is that it thoroughly expounded that : the property of the gradient in terms of \\nabla_x f * ( x ) is substantially different from the property of a distance metric ; and to ensure the convergence of GANs or design new formulation for GANs , one should carefully check whether the gradient \\nabla_x f * ( x ) is reliable . > > Regarding Appendix G.1 [ 3 ] , although it states that Lipschitz might be generally applicable , the discussion there is far from enough : ( i ) the discussion in Appendix G.1 is limited to the objective of the original GAN ; our theorem , in contrast , elaborated a family of GAN objectives and characterized the necessary condition where Lipschitz condition ensures the convergence . ( ii ) the discussion in Appendix G.1 ignores the $ \\log $ term in the objective of original GAN ; our theorem is , however , directly applicable to the whole objective of the original GAN . * ( iii ) according to our theorem , for any objective other than W-distance , it is theoretically necessary to penalize the Lipschitz constant k ( f ) to ensure the convergence ; though Appendix G.1 mentioned that to avoid the saturation , k need to be small , it fails to cover the other fold that \u2018 \u2018 \\nabla_x f * ( x ) =0 for all x \u2019 \u2019 might also happen even if there is no saturation region or saturation region is not touched . * * [ 1 ] : Optimal transport , old and new [ 2 ] : http : //proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf [ 3 ] : http : //proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf [ 4 ] : Improved Training of Wasserstein GANs * In Appendix G.1 , it discusses the properties of f with bounded value-range ( to simulate a classifier ) , while in our paper , f is assumed to have unbounded value range and loss metrics are applied to the unbounded f. Therefore , the arguments are actually quite different . * * That is to say , small k is not enough to guarantee the convergence , and penalizing/decreasing the Lipschitz constant is necessary . Given a fixed Lipschitz constant k , according to our analysis , the following state is possible : \u2018 \u2018 Pg ! =Pr \u2019 \u2019 and \u2018 \u2018 for each x , f * ( x ) is optimal \u2019 \u2019 , but there does not exist two points x , y such that |f ( x ) -f ( y ) | > k|x-y| . In this case , \u2018 \u2018 \\nabla_x f * ( x ) =0 for all x \u2019 \u2019 and the generator stop learning , however , Pg does not equal to Pr ."}, "2": {"review_id": "r1zOg309tX-2", "review_text": " [pros] - It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases. - It also proposes use of the penalty term in terms of the Lipschitz constant of the discriminative function. [cons] - Some of the arguments on the Wasserstein distance and on WGAN are not sound. - Theorem 3 does not make sense. - The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017). [Quality] I found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below. [Clarity] The main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions. [Originality] Despite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\\phi,\\varphi,\\psi$ and the form of the gradient penalty, $\\max \\|\\nabla f(x)\\|_2^2$ in this paper versus $E[(\\|\\nabla f(x)\\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal. [Significance] This paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning. Detailed comments: In Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation. It is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the \"critic\" $f$ via a multilayer neural network with weight clipping. One can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading. On optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\\sim P_g}[f(x)]-E_{x\\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. I do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes \"$\\forall x \\not= y$\", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\\|x-y\\|$ to obtain $f(x)-f(y)=k\\|y-x\\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \\not= y$ such that $f(y)-f(x)=k\\|x-y\\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\\|x-y\\|$ under the Lipschitz condition. Appendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\\partial J_D/\\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as \"arg min\" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function. Page 5, line 36: $J_D(x)$ appears without explicit definition. Page 23, lines 34 and 38: Cluttered expression $\\frac{\\partial [}{\\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive feedback . Q : In Section 2.3 , the authors criticize the use of the Wasserstein distance as the distance function of GANs , but their criticism is off the point . This is not the problem of the Wasserstein distance itself , but the problem regarding how the dual problem is implemented in learning of GANs . In practice , however , one may regard that $ f $ satisfies the condition $ f ( x ) -f ( y ) \\le d ( x , y ) $ not only on the supports of $ P_g $ and $ P_r $ but throughout the entire space $ \\mathbb { R } ^n $ . > > We have refined our statements to make it more rigorous . We argued that Wasserstein distance in the dual form with compacted constraint will fail to provide valid gradients . We have revised related sections . It is much clearer now . > > The practical solution of WGAN , which usually involves Lipschitz constraint , is sound . What we want to emphasize is that the explanation on the underlining working mechanism in the Wasserstein GAN paper does not hold very well . It is not Wasserstein distance which can properly measure the distance that makes it work , but the Lipschitz constraint . As we showed in the paper , in the compact dual form ( without Lipschitz ) of Wasserstein distance , $ \\nabla_x f * ( x ) $ may also fail to provide a meaningful gradient ; and with Lipschitz , lots of GAN objectives can guarantee a meaningful gradient in terms of $ \\nabla_x f * ( x ) $ , not limited to Wasserstein distance . Q : The claim in Theorem 3 does not make sense . If we literally take its statement , it would imply $ k=0 $ , and consequently , $ f $ is constant . One would be able to prove that if there exists $ ( x , y ) $ with $ x \\not= y $ such that $ f ( y ) -f ( x ) =k\\|x-y\\| $ holds then the gradient of $ f $ at $ x_t $ is equal to $ k ( y-x ) /\\|x-y\\| $ under the Lipschitz condition . > > Sorry , this is a typo . We indeed mean \u201c $ \\forall x \\not= y $ , if $ f ( y ) -f ( x ) =k\\|x-y\\| $ \u201d as what you have guessed , but it was miswritten as \u201c if $ f ( y ) -f ( x ) =k\\|x-y\\| $ , $ \\forall x \\neq y $ \u201d . We have revised it in our new version . Q : On optimizing $ k $ , I do not agree with the authors ' claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $ f $ even when $ P_g=P_r $ . Indeed , when $ P_g=P_r $ , for any measurable function $ f $ one trivially has $ J_D [ f ] =E_ { x\\sim P_g } [ f ( x ) ] -E_ { x\\sim P_r } [ f ( x ) ] =0 $ , so that the functional derivative of $ J_D $ with respect to $ f $ does vanish identically . > > Sorry for the typo . It should be \u2018 \u2018 $ \\nabla_x f * ( x ) $ may not be zero '' . We have miswritten it as $ \\nabla_f * $ . For WGAN , when $ P_g=P_r $ , f * can be arbitrary , so $ \\nabla_x f * ( x ) $ may not be zero . We have revised it in our new version . -- -- - Minor Points -- -- - Q : Page 5 , line 36 : $ J_D ( x ) $ appears without explicit definition . > > Thanks for pointing this out . We have added the definition for $ J_D ( x ) $ in the revised version . Q : Appendix G : Some notations should be made more precise . For example , in the definition of $ J_D $ the variable of integration $ x $ has been integrated out so that $ J_D $ no longer has $ x $ as its variable . The expression $ \\partial J_D/\\partial x $ does not make any sense . > > By $ \\partial J_D/\\partial x $ , we want to refer to the term that is being integrated over $ x $ . In the current form , it is analogous to the gradient of indefinite integral . i.e. , F = \\int_x f ( x ) dx and F \u2019 = f ( x ) . We have replaced the confusing derivative form with a simple notation . Q : Appendix G : $ J_D^ * ( k ) $ is defined as `` arg min '' of $ J_D $ , implying as if $ J_D^ * ( k ) $ were a $ k $ -Lipschitz function . > > $ J_D^ * ( k ) $ should be defined as the min value , not arg min . We have corrected it . Q : Page 23 , lines 34 and 38 : cluttered expression $ \\frac { \\partial [ } { \\partial 2 } ] $ makes the statements not understandable . It also appears on page 24 several times . > > The $ \\frac { \\partial [ } { \\partial 2 } ] $ comes from a breaking \\newcommand for second-order derivation in latex . We have fixed it . Thanks a lot for the careful reading of our paper and the detailed comments , which are very helpful ."}}