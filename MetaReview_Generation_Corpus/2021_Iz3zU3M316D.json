{"year": "2021", "forum": "Iz3zU3M316D", "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights", "decision": "Accept (Poster)", "meta_review": "Clarity: The paper is well-written with illustrative figures.\n\nOriginality: The originality of the paper is relatively restricted, mainly due to the resemblance with the work [1]. However, there are important differences, that the authors nicely pointed out, and we encourage them to include these in the final version of the paper.\n\nSignificance: The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum-based optimization algorithms in training deep neural networks. While the paper could be considered \"another algorithms for training NNs\", the papers illustrates nicely the main arguments, and is backed up with more than sufficient experimental results.\n\nMain pros:\n- In the main pros, AC and reviewers admit the phenomenal job in responding to reviewers' questions and requests\n- The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method.\n- After the reviews, The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e-4 was the best value.\n\nMain cons:\n- One reviewer requires more explanation why the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum-based update in equation (8).", "reviews": [{"review_id": "Iz3zU3M316D-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper shows that momentum-based gradient descent optimizers reduce the effective step size in training scale-invariant models including deep neural networks normalized by batch normalization , layer normaliztion , instance normalization and group normalization . The authors then propose a solution that projects the update at each step in gradient descent onto the tangent space of the model parameters . Theoretical results are provided to show that this projection operator only adjusts the effective learning rate but does not change the effective update directions . Empirical results on various tasks are provided to justify the advantage of the proposed method over the baseline momentum-based ( stochastic ) gradient descent and Adam . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reason for the Score : Overall , this paper could be an interesting algorithmic contribution . However , there are relevant points needed to be clarified on the theory and experiments . My first main concern is that theoretically it is hard to justify that the proposed projection-based update yields smaller parameter norms than the baseline momentum-based update . My second main concern is some baseline results in the experiments do not match those in existing literature , and no error bars are provided in the empirical results even though the improvements of the proposed methods over the baseline methods are small . Currently , I am leaning toward rejecting the paper . However , given additional clarifications on the two main concerns above in an author response , I would be willing to increase the score . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strong points : 1 . The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum-based optimization algorithms in training deep neural networks . 2.The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method . 3.The paper is well-written with illustrative figures . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Weak points : 1 . It is not clear to me that the proposed update in equation ( 12 ) yields smaller norms ||w_ { t+1 } || than the momentum-based update in equation ( 8 ) . The parameters of the model evolve differently under these two update rules . Throughout the training , the update p_t in equation ( 11 ) is different from the update p_t in equation ( 8 ) . As a result , it is hard to compare ||q_t|| in equation ( 12 ) and ||p_t|| plus all the terms ||p_k|| in equation ( 8 ) . 2.The improvements of the proposed SGDP and AdamP over the baseline SGD and Adam are small across experiments , and thus error bars are needed to validate that these improvements are not due to randomness . However , no error bars are provided for the empirical results in the paper . 3.The reported baseline results for audio classification are worse than those reported in ( Won et al. , 2019 ) . 4.The baseline results for adversarial robustness seems to be much higher than reported results in ( Madry et al. , 2018 ) . Also why are the values of epsilon used in the paper quite small ( 80/255 and 4/255 vs. 8 in ( Madry et al. , 2018 ) ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Additional Concerns and Questions for the Authors : 1 . Adam normalizes the gradient by its cumulative norm . This can help eliminate the small step size issue since the norms of the gradients become smaller during training . Can you provide a similar simulation as in Figure 3 but using Adam , AdamW , and AdamP ? 2.What are the baseline results , reported in existing literature , on ImageNet for ResNet18 and ResNet50 trained with the cosine learning rate schedule in 100 epochs ? Can you please link me to the previous papers that report those results ? The paper you cite , ( Loshchilov & Hutter , 2016 ) , does not report those results . 3.In section 4.1 , the authors say \u201c For ResNet , we employ the training hyperparameters in ( He , 2016 ) \u201d . However , the training hyperparameters for ResNet used in the paper are not from ( He , 2016 ) . In ( He , 2016 ) , the models are trained for only 90 epochs without using cosine learning rate . 4.The proposed update is more expensive than the baseline momentum-update . The paper also reports that the proposed update incurs 8 % extra training time on top of the baselines for ResNet18 on ImageNet classification while resulting in only small improvements over the baselines . It is needed to compare the proposed optimizer with the baseline momentum-based optimizer trained with more epochs and potentially with an additional learning rate decay . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor Comments that did not Impact the Score : 1 . The paper proposes not only AdamP , but also SGDP . It is better if the authors remove AdamP in the title . 2.In figure 4 , is the y-axis the test or train accuracy ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # References : Minz Won , Sanghyuk Chun , and Xavier Serra . Toward interpretable music tagging with self- attention . arXiv preprint arXiv:1906.04972 , 2019 . Aleksander Madry , Aleksandar Makelov , Ludwig Schmidt , Dimitris Tsipras , and Adrian Vladu . Towards deep learning models resistant to adversarial attacks . In International Conference on Learning Representations ( ICLR ) , 2018 . URL https : //openreview.net/forum ? id= rJzIBfZAb . Ilya Loshchilov and Frank Hutter . SGDR : Stochastic gradient descent with warm restarts . arXiv preprint arXiv:1608.03983 , 2016 . Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2016 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Post Discussion Score : After reading the rebuttal from the author and the comments from other reviewers , I am still not clear if the proposed update in equation ( 12 ) yields smaller norms ||w_ { t+1 } || than the momentum-based update in equation ( 8 ) . However , the authors have addressed all of my other concerns . I decided to increase my score for this paper from 4 to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the high-quality review and the extensive verification of experimental setups . We have answered most of your questions , except for the ones that require several days to finish training . They will be updated as soon as the results arrive . In the meantime , we would be happy to discuss further , so please do not hesitate to comment and ask follow-up questions as necessary . * * It is not clear to me that the proposed update in equation ( 12 ) yields smaller norms ||w_ { t+1 } || than the momentum-based update in equation ( 8 ) . * * We note that the size comparison of $ \\boldsymbol { p } _t $ and $ \\boldsymbol { q } _t $ is not presented in the current version ; we will include it in the revision . We claim that $ || \\boldsymbol { q } _t ||_2 $ is always smaller than $ || \\boldsymbol { p } _t ||_2 $ . This observation follows quickly from that fact that $ \\boldsymbol { q } _t $ is an output of a projection operation on $ \\boldsymbol { p } _t $ . Since a projection mapping has eigenvalues either 0 or 1 , it * always * decreases the norm of the input vector after transformation . Another way to see this is as follows : \\\\ [ || \\boldsymbol { q } _t ||_2 = || \\boldsymbol { p } _t - ( \\hat { \\boldsymbol { w } } \\cdot \\boldsymbol { p } _t ) \\hat { \\boldsymbol { w } } ||_2 = \\sqrt { \\boldsymbol { p } _t \\cdot \\boldsymbol { p } _t - ( \\hat { \\boldsymbol { w } } \\cdot \\boldsymbol { p } _t ) ^2 } \\leq || \\boldsymbol { p } _t ||_2 \\\\ ] This inequality leads to the conclusion that : \\\\ [ || { \\boldsymbol { w } _t } ||_2^2 + \\eta^2 || { \\boldsymbol { q } } _t ||_2^2 \\leq || \\boldsymbol { w } _t ||_2^2 + \\eta^2 || { \\boldsymbol { p } } _t ||_2^2 \\leq || \\boldsymbol { w } _t ||_2^2 + \\eta^2 || { \\boldsymbol { p } } _t ||_2^2 + 2 \\eta^2 \\sum_k \\beta^ { t-k } || { \\boldsymbol { p } } _k||^2_2 \\\\ ] Thus , equation ( 12 ) yields smaller norms than equation ( 8 ) . We will add these formulas in the paper . * * The improvements of the proposed SGDP and AdamP over the baseline SGD and Adam are small across experiments , and thus error bars are needed to validate * * The error bars for Tables 3 , 5 and D.2 are as follows . - Table 3 | |\\| Music Tagging \\||\\| Music Tagging \\||\\| Keyword Spotting \\||\\| Sound Event Tagging \\|| | : :| : - : | : - : | : - : | : - : | | | ROC-AUC | PR-AUC | Accuracy | F1 score | | Adam + SGD | \u00b1 0.055 | \u00b1 0.025 | - | - | | AdamW | \u00b1 0.081 | \u00b1 0.189 | \u00b1 0.055 | \u00b1 0.394 | | AdamP ( ours ) | \u00b1 0.074 | \u00b1 0.269 | \u00b1 0.061 | \u00b1 0.833 | - Table 5 | |\\| CUB \\||\\| CUB \\||\\| Cars-196 \\||\\| Cars-196 \\||\\| InShop \\||\\| InShop \\||\\| SOP \\||\\| SOP \\|| | : :| : - : | : :| : -- : | : -- : | : - : | : :| : - : | : :| | | Triplet | PA | Triplet | PA | Triplet | PA | Triplet | PA | | AdamW | \u00b1 0.91 | \u00b1 0.24 | \u00b1 0.82 | \u00b1 0.09 | \u00b1 0.52 | \u00b1 0.08 | \u00b1 0.71 | \u00b1 0.04 | | AdamP ( ours ) | \u00b1 0.62 | \u00b1 0.77 | \u00b1 1.35 | \u00b1 0.19 | \u00b1 0.82 | \u00b1 0.02 | \u00b1 0.74 | \u00b1 0.09 | - Table D.2 Biased MNIST Unbiased acc . at \u03c1 |\\| \u03c1 \\||\\| .999 \\||\\| .997 \\||\\| .995 \\||\\| .990 \\|| | : :| : :| : :| : :| : :| | Adam | \u00b1 6.04 | \u00b1 0.48 | \u00b1 0.70 | \u00b1 0.48 | | AdamP ( ours ) | \u00b1 1.73 | \u00b1 1.31 | \u00b1 2.16 | \u00b1 0.40 | 9-Class ImageNet | |\\| Biased \\||\\| UnBiased \\||\\| ImageNet-A \\|| | : :| : :| : -- : | : - : | | AdamW | \u00b1 0.27 | \u00b1 0.40 | \u00b1 1.06 | | AdamP ( ours ) | \u00b1 0.21 | \u00b1 0.27 | \u00b1 0.82 | For the other experiments , we have reported the averaged results of 3 independent trials ( randomizing data shuffling for stochastic optimizers , as well as other random factors in data augmentation ) . However , the data has been lost due to the limited capacity of our shared servers . We are running new training sessions to recover the error bars . We will share the results and update the paper ( ETA : November 18 ) ."}, {"review_id": "Iz3zU3M316D-1", "review_text": "# # # Summary : This paper studies the hurtful effect of momentum on scale-invariant parameters ( such as weight matrices of linear layers followed by Batch Normalization ( BN ) ) : Indeed , momentum tend to increase the norm of the parameters , but the effective update size of scale-invariant parameters is inversely proportional to their norm , which leads to `` _premature decay of effective step size_ '' . The authors propose the improved SGDP and AdamP to reduce this issue , and show how it improves the performances of a wide variety of models ( ResNets , MobileNets , Transformers , etc ... ) on many different tasks ( ImageNet classification , adversarial training , audio classification , etc ... ) # # # Strengths : + This paper addresses the optimization of scale-invariant parameters , which can be found in pretty much all the state-of-the-art models . + The paper is well written and contains a good balance of illustrative examples , theoretical analysis and experiments . Good job ! + The empirical evaluation of the proposed algorithms is quite large , and many tasks and architectures are considered . + I also really like the `` cos ( w , Grad_w ) '' hack to automatically detect scale invariant parameters in the model . This is a very neat trick ! # # # Concern : My main concern is with the statement that `` _our paper is first to delve into the issue in the widely-used combination of scale-invariant parameters and momentum-based optimizers '' _ . Indeed the authors have missed [ 1 ] , which also adapts SGD with momentum and Adam to work on scale-invariant parameters , and showed performance improvement over vanilla SGD and Adam . Now , the approach of [ 1 ] is slightly different , in the sense that they propose to keep the scale-invariant parameters on the unit sphere S^1 . Also , [ 1 ] performs the projection onto the tangent space before computing the momentum term ( and second order moment in Adam ) , while this projection is done after in SGDP and AdamP . In any case , I do believe proper comparison with [ 1 ] throughout the paper is required , due to the conceptual similarity of both methods . It would be particularly relevant to show how the angles between SGDP and the SGD of [ 1 ] compare in Figure 1 ( I think they would be identical , although not 100 % certain ) . Also , I 'm curious to see if the proposed SGDP and AdamP work better than the algorithms proposed in [ 1 ] , as the nice automatic learning rate decay of BN disappears when keeping the scale-invariant parameters on the unit sphere ( which would be an advantage of the proposed SGDP and AdamP ) . # # # Reasons for score : Overall I really liked that paper , but I do n't think it would be fair to accept it without an in-depth comparison with [ 1 ] . # # # Questions / Comments : - I like the toy example in Figure 1 , although I 'm not sure I understand what you mean by : `` _Compared to GD , GD+Momentum speeds up the norm growth , resulting in a slower effective convergence in S^1 , though being faster in the nominal space R^2_ '' . It seems to me that the angle between the optimal solution and GD+momentum is smaller than the angle between the optimal solution and GD . I 'm not sure to understand how do you observe a slower effective convergence in S^1 ? - ( 3.1 , real-world experiments and F ) What happens if you increase WD when using SGD ? It seems that the best values reported for SGD in Table 1 and Figures F. * are with WD=1e-4 , which is also the highest value reported . One can imagine even better results could be obtained using WD=3e-4 or 1e-3 . It would be nice to have this result for completeness . - Reporting mean and standard deviation across several seeds would have been nice , but I 'm going to let this one slide , since results are reported on a lot of different tasks and architectures . [ 1 ] Cho , Minhyung and Lee , Jaehyung , _Riemannian approach to batch normalization_ , NIPS 2017 ___ # # # After author response and paper revision First , I would like to congratulate the authors for their amazing work during the rebuttal period . My main concern , the comparison against [ 1 ] , has been perfectly addressed in appendix G ( both `` conceptually '' , by highlighting the differences between both methods , and showing the advantages of the proposed SGDP and AdamP , but also empirically on ImageNet , where a fair comparison with proper hyper-parameter tuning has been performed ) . The authors also reinforced their empirical investigation by reporting standard deviation of the results , which allows to better appreciate the performances of SGDP and AdamP . Finally , they also added the experiments with higher weight decay , showing that indeed 1e-4 was the best value . With all these changes , I think the paper is good and I recommend its acceptance .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the high-quality expert review . We first address the concern regarding the comparison against an important prior work . We answer the rest of your questions , some of which may take days to get the experimental results . They will be updated as soon as the results arrive . In the meantime , we would be happy to discuss further , so please do not hesitate to comment and ask follow-up questions as necessary . * * Comparison with `` Cho & Lee , Riemannian approach to batch normalization '' [ 1 ] * * Thank you for pointing out an important reference , and sorry for missing a discussion about this work ; we were not aware of this paper . Indeed , both [ A ] and ours are motivated from the scale invariance of certain parameters in a neural network , and both propose a solution that reduces the radial component of the optimization steps . However , our contributions are still not significantly eclipsed by this paper . Our optimization algorithm is different from that of [ A ] , with a different set of theories to justify the methodologies . And this difference results in a practical edge for our optimizers . The crucial difference between [ A ] and ours is in the space where the optimization is performed . [ A ] performs the gradient steps on the Riemannian manifold . Ours project the updates on the tangent planes of the manifold . Thus , ours operates on the same Euclidean space where SGD and Adam operate . From a theory point of view , [ A ] has made contributions to the optimization theory on a Riemannian manifold . Our contributions are along a different axis : we focus on the norm growth when the updates are projected onto the tangent spaces ( Sections 2.4 , 3.1 , and 3.2 ) . We contribute present theoretical findings that are not covered by [ A ] . From the practicality point of view , we note that changing the very nature of space from Euclidean to Riemannian requires users to find the sensible ranges for many optimization hyperparameters again . For example , [ A ] has \u201c used different learning rates for the weights in Euclidean space and on Grassmann [ Riemannian ] manifolds \u201d ( page 7 of [ A ] ) , while in our case hyperparameters are largely compatible between scale-invariant and scale-variant parameters , for they are both accommodated in the same kind of space . We have shown that SGDP and AdamP outperform the SGD and Adam baselines with exactly the same optimization hyperparameters ( Section E.2 ) . The widely used Xavier or Gaussian initializations are no longer available in the spherical Riemannian manifold , necessitating changes in the code defining parameters and their initialization schemes ( e.g . [ A ] has employed a dedicated initialization based on truncated normals . See https : //github.com/MinhyungCho/riemannian-batch-normalization/blob/d1ac938ca5af8af1b7c1d4f708c1aacd2d8cbab9/gutils.py # L65 ) . Finally , [ A ] requires users to manually register scale-invariant hyperparameters . This procedure is not scalable , as the networks nowadays are becoming deeper and more complex and the architectures are becoming more machine-designed than handcrafted . Our optimizers automatically detect scale invariances through the orthogonality test ( eq.11 ) , and users do not need to register anything by themselves . The sum of all the pain points above is an optimization algorithm that is not readily applicable in many practical scenarios , as hinted in their Github manual ( https : //github.com/MinhyungCho/riemannian-batch-normalization # to-apply-this-algorithm-to-your-model ) where users are instructed to go through five convoluted steps to apply the algorithm . In stark contrast , users wanting to apply SGDP or AdamP only need to ( 1 ) pip install , ( 2 ) import , and ( 3 ) replace the \u201c torch.optim.SGD ( Adam ) \u201d with an \u201c SGDP ( AdamP ) \u201d class instantiation . No change in hyperparameters , design choices , or model codes is required . We recognize [ A ] as a great contribution to the field , and we are glad to confirm that another group of researchers have been motivated from a similar problem and have come up with a solution that shares certain similarities with ours . We believe that our contribution is not diminished by [ A ] though . Our solution is based on a different optimization space ( Euclidean as opposed to Riemannian ) with a different algorithm ( projection onto the tangent space as opposed to moving along the manifold ) and this provides significant practicality benefits for users . As an addition to the above points , we are conducting experiments on the toy examples ( Section 3.2 ) and ImageNet to empirically compare ours against [ A ] . We will share the results as soon as they are out ( ETA : November 18 ) . We will revise the paper with the discussion above as well as the experiments that are being run . [ A ] Cho , Minhyung , and Jaehyung Lee . `` Riemannian approach to batch normalization . '' Advances in Neural Information Processing Systems . 2017 ."}, {"review_id": "Iz3zU3M316D-2", "review_text": "This paper points out that momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights . To solve the problem , two algorithms called SGDP and AdamP are proposed , which project the updates to tangent space of the parameter . Experiments on several tasks including image classification , language modeling , etc show the effectiveness of the proposed algorithms . The idea to study the integration of BN and momentum is interesting . The analyses and proposed algorithms provide guidance to practitioners . Questions : ( 1 ) What is the difference between the proposed algorithm and the algorithm in paper `` Cho & Lee , Riemannian approach to batch normalization '' ? Eq . ( 10 ) in your paper is exactly the same with Eq . ( 6 ) in [ Cho & Lee ] . It is an important related work which should be cited and compared with . ( 2 ) What does `` lies on the geodesic '' mean in Proposition 3.1 ? What is the relation between Proposition 3.1 and the convergence of the algorithm ? ( 3 ) It is not clear which results are proposed in related work and which results are proposed in this paper . Is Lemma 2.1 a new result ? If not , it needs a citation . ( 4 ) The theory part analyzes the effect of momentum , while the experiments shows the effect of weight decay . How does weight decay influence the norm growth theoretically ? Why not conduct experiments with varying momentum coefficient ? ( 5 ) The theory part shows the negative effect of momentum . Does it mean that `` SGD is a better choice than momentum SGD '' ? Momentum SGD is a standard algorithm to train deep neural networks with BN in practice but not vanilla SGD . How to explain it ? ( 6 ) What is the additional computational cost of the proposed algorithms compared with the baselines ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the high-quality review with spot-on questions . We include the requested comparison against the important missing prior work . We answer the rest of your questions as thoroughly as possible . We would be happy to discuss further , so please do not hesitate to comment and ask follow-up questions . * * Difference from `` Cho & Lee , Riemannian approach to batch normalization '' [ A ] ? * * Thank you for pointing out an important reference , and sorry for missing a discussion about this work ; we were not aware of this paper . Indeed , both [ A ] and ours are motivated from the scale invariance of certain parameters in a neural network , and both propose a solution that reduces the radial component of the optimization steps . However , our contributions are still not significantly eclipsed by this paper . Our optimization algorithm is different from that of [ A ] , with a different set of theories to justify the methodologies . And this difference results in a practical edge for our optimizers . The crucial difference between [ A ] and ours is in the space where the optimization is performed . [ A ] performs the gradient steps on the Riemannian manifold . Ours project the updates on the tangent planes of the manifold . Thus , ours operates on the same Euclidean space where SGD and Adam operate . From a theory point of view , [ A ] has made contributions to the optimization theory on a Riemannian manifold . Our contributions are along a different axis : we focus on the norm growth when the updates are projected onto the tangent spaces ( Sections 2.4 , 3.1 , and 3.2 ) . We contribute present theoretical findings that are not covered by [ A ] . From the practicality point of view , we note that changing the very nature of space from Euclidean to Riemannian requires users to find the sensible ranges for many optimization hyperparameters again . For example , [ A ] has \u201c used different learning rates for the weights in Euclidean space and on Grassmann [ Riemannian ] manifolds \u201d ( page 7 of [ A ] ) , while in our case hyperparameters are largely compatible between scale-invariant and scale-variant parameters , for they are both accommodated in the same kind of space . We have shown that SGDP and AdamP outperform the SGD and Adam baselines with exactly the same optimization hyperparameters ( Section E.2 ) . The widely used Xavier or Gaussian initializations are no longer available in the spherical Riemannian manifold , necessitating changes in the code defining parameters and their initialization schemes ( e.g . [ A ] has employed a dedicated initialization based on truncated normals . See https : //github.com/MinhyungCho/riemannian-batch-normalization/blob/d1ac938ca5af8af1b7c1d4f708c1aacd2d8cbab9/gutils.py # L65 ) . Finally , [ A ] requires users to manually register scale-invariant hyperparameters . This procedure is not scalable , as the networks nowadays are becoming deeper and more complex and the architectures are becoming more machine-designed than handcrafted . Our optimizers automatically detect scale invariances through the orthogonality test ( eq.11 ) , and users do not need to register anything by themselves . The sum of all the pain points above is an optimization algorithm that is not readily applicable in many practical scenarios , as hinted in their Github manual ( https : //github.com/MinhyungCho/riemannian-batch-normalization # to-apply-this-algorithm-to-your-model ) where users are instructed to go through five convoluted steps to apply the algorithm . In stark contrast , users wanting to apply SGDP or AdamP only need to ( 1 ) pip install , ( 2 ) import , and ( 3 ) replace the \u201c torch.optim.SGD ( Adam ) \u201d with an \u201c SGDP ( AdamP ) \u201d class instantiation . No change in hyperparameters , design choices , or model codes is required . We recognize [ A ] as a great contribution to the field , and we are glad to confirm that another group of researchers have been motivated from a similar problem and have come up with a solution that shares certain similarities with ours . We believe that our contribution is not diminished by [ A ] though . Our solution is based on a different optimization space ( Euclidean as opposed to Riemannian ) with a different algorithm ( projection onto the tangent space as opposed to moving along the manifold ) and this provides significant practicality benefits for users . As an addition to the above points , we are conducting experiments on the toy examples ( Section 3.2 ) and ImageNet to empirically compare ours against [ A ] . We will share the results as soon as they are out ( ETA : November 18 ) . We will revise the paper with the discussion above as well as the experiments that are being run . [ A ] Cho , Minhyung , and Jaehyung Lee . `` Riemannian approach to batch normalization . '' Advances in Neural Information Processing Systems . 2017 ."}, {"review_id": "Iz3zU3M316D-3", "review_text": "Summary : Based on the assumption that the rapidly decrease step size \\delta \\omega leads to a solve effective convergence of \\omega . This paper proposes to use a projection step to remove the radial component , and thus reduce the norm of training parameters , and faster the training procedure . The experiments look good to me while the derivation of this paper based on many assumptions and conjectures . I tend to accept this paper at this time . However , I am not an expert at this area I will not be sad if this paper is reject by other reviews . minor problems : 1 ) Can the author explain more on how to derive eq . ( 4 ) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for spending time reading a paper out of your domain of expertise . We benefit a lot from reviewers from other domains , as we may learn how to make the paper interesting for a broader audience . We answer your question below . Please do not hesitate to ask for further clarifications for any part of the paper , should there be points that are not so obvious . * * How to derive eq . ( 4 ) * * We start from the observation that if f ( w ) is scale invariant with respect to w , then \\\\ [ \\frac { \\partial f ( c\\boldsymbol { w } ) } { \\partial c } =0 . \\\\ ] This follows from the fact that the rate of change along w is always zero . Now , by the multivariate chain rule , we compute \\\\ [ \\frac { \\partial f ( c\\boldsymbol { w } ) } { \\partial c } = \\sum_i \\left [ \\frac { \\partial f } { \\partial v_i } \\frac { \\partial v_i } { \\partial c } \\right ] _ { v = c\\boldsymbol { w } } \\\\ ] where $ \\boldsymbol { v } =c\\boldsymbol { w } $ . We continue the derivation as follows : \\\\ [ \\sum_i \\left [ \\frac { \\partial f } { \\partial v_i } \\frac { \\partial v_i } { \\partial c } \\right ] _ { v = c\\boldsymbol { w } } = \\sum_i \\nabla f ( \\boldsymbol { v } ) _i w_i = \\sum_i \\nabla f ( \\boldsymbol { w } ) _i w_i = \\boldsymbol { w } ^\\top \\nabla f ( \\boldsymbol { w } ) .\\\\ ] Hence , it follows that the scale-invariance leads to the orthogonality between gradient and the radial direction : \\\\ [ \\boldsymbol { w } ^\\top \\nabla f ( \\boldsymbol { w } ) = 0 . \\\\ ]"}], "0": {"review_id": "Iz3zU3M316D-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper shows that momentum-based gradient descent optimizers reduce the effective step size in training scale-invariant models including deep neural networks normalized by batch normalization , layer normaliztion , instance normalization and group normalization . The authors then propose a solution that projects the update at each step in gradient descent onto the tangent space of the model parameters . Theoretical results are provided to show that this projection operator only adjusts the effective learning rate but does not change the effective update directions . Empirical results on various tasks are provided to justify the advantage of the proposed method over the baseline momentum-based ( stochastic ) gradient descent and Adam . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reason for the Score : Overall , this paper could be an interesting algorithmic contribution . However , there are relevant points needed to be clarified on the theory and experiments . My first main concern is that theoretically it is hard to justify that the proposed projection-based update yields smaller parameter norms than the baseline momentum-based update . My second main concern is some baseline results in the experiments do not match those in existing literature , and no error bars are provided in the empirical results even though the improvements of the proposed methods over the baseline methods are small . Currently , I am leaning toward rejecting the paper . However , given additional clarifications on the two main concerns above in an author response , I would be willing to increase the score . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strong points : 1 . The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum-based optimization algorithms in training deep neural networks . 2.The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method . 3.The paper is well-written with illustrative figures . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Weak points : 1 . It is not clear to me that the proposed update in equation ( 12 ) yields smaller norms ||w_ { t+1 } || than the momentum-based update in equation ( 8 ) . The parameters of the model evolve differently under these two update rules . Throughout the training , the update p_t in equation ( 11 ) is different from the update p_t in equation ( 8 ) . As a result , it is hard to compare ||q_t|| in equation ( 12 ) and ||p_t|| plus all the terms ||p_k|| in equation ( 8 ) . 2.The improvements of the proposed SGDP and AdamP over the baseline SGD and Adam are small across experiments , and thus error bars are needed to validate that these improvements are not due to randomness . However , no error bars are provided for the empirical results in the paper . 3.The reported baseline results for audio classification are worse than those reported in ( Won et al. , 2019 ) . 4.The baseline results for adversarial robustness seems to be much higher than reported results in ( Madry et al. , 2018 ) . Also why are the values of epsilon used in the paper quite small ( 80/255 and 4/255 vs. 8 in ( Madry et al. , 2018 ) ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Additional Concerns and Questions for the Authors : 1 . Adam normalizes the gradient by its cumulative norm . This can help eliminate the small step size issue since the norms of the gradients become smaller during training . Can you provide a similar simulation as in Figure 3 but using Adam , AdamW , and AdamP ? 2.What are the baseline results , reported in existing literature , on ImageNet for ResNet18 and ResNet50 trained with the cosine learning rate schedule in 100 epochs ? Can you please link me to the previous papers that report those results ? The paper you cite , ( Loshchilov & Hutter , 2016 ) , does not report those results . 3.In section 4.1 , the authors say \u201c For ResNet , we employ the training hyperparameters in ( He , 2016 ) \u201d . However , the training hyperparameters for ResNet used in the paper are not from ( He , 2016 ) . In ( He , 2016 ) , the models are trained for only 90 epochs without using cosine learning rate . 4.The proposed update is more expensive than the baseline momentum-update . The paper also reports that the proposed update incurs 8 % extra training time on top of the baselines for ResNet18 on ImageNet classification while resulting in only small improvements over the baselines . It is needed to compare the proposed optimizer with the baseline momentum-based optimizer trained with more epochs and potentially with an additional learning rate decay . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor Comments that did not Impact the Score : 1 . The paper proposes not only AdamP , but also SGDP . It is better if the authors remove AdamP in the title . 2.In figure 4 , is the y-axis the test or train accuracy ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # References : Minz Won , Sanghyuk Chun , and Xavier Serra . Toward interpretable music tagging with self- attention . arXiv preprint arXiv:1906.04972 , 2019 . Aleksander Madry , Aleksandar Makelov , Ludwig Schmidt , Dimitris Tsipras , and Adrian Vladu . Towards deep learning models resistant to adversarial attacks . In International Conference on Learning Representations ( ICLR ) , 2018 . URL https : //openreview.net/forum ? id= rJzIBfZAb . Ilya Loshchilov and Frank Hutter . SGDR : Stochastic gradient descent with warm restarts . arXiv preprint arXiv:1608.03983 , 2016 . Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2016 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Post Discussion Score : After reading the rebuttal from the author and the comments from other reviewers , I am still not clear if the proposed update in equation ( 12 ) yields smaller norms ||w_ { t+1 } || than the momentum-based update in equation ( 8 ) . However , the authors have addressed all of my other concerns . I decided to increase my score for this paper from 4 to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the high-quality review and the extensive verification of experimental setups . We have answered most of your questions , except for the ones that require several days to finish training . They will be updated as soon as the results arrive . In the meantime , we would be happy to discuss further , so please do not hesitate to comment and ask follow-up questions as necessary . * * It is not clear to me that the proposed update in equation ( 12 ) yields smaller norms ||w_ { t+1 } || than the momentum-based update in equation ( 8 ) . * * We note that the size comparison of $ \\boldsymbol { p } _t $ and $ \\boldsymbol { q } _t $ is not presented in the current version ; we will include it in the revision . We claim that $ || \\boldsymbol { q } _t ||_2 $ is always smaller than $ || \\boldsymbol { p } _t ||_2 $ . This observation follows quickly from that fact that $ \\boldsymbol { q } _t $ is an output of a projection operation on $ \\boldsymbol { p } _t $ . Since a projection mapping has eigenvalues either 0 or 1 , it * always * decreases the norm of the input vector after transformation . Another way to see this is as follows : \\\\ [ || \\boldsymbol { q } _t ||_2 = || \\boldsymbol { p } _t - ( \\hat { \\boldsymbol { w } } \\cdot \\boldsymbol { p } _t ) \\hat { \\boldsymbol { w } } ||_2 = \\sqrt { \\boldsymbol { p } _t \\cdot \\boldsymbol { p } _t - ( \\hat { \\boldsymbol { w } } \\cdot \\boldsymbol { p } _t ) ^2 } \\leq || \\boldsymbol { p } _t ||_2 \\\\ ] This inequality leads to the conclusion that : \\\\ [ || { \\boldsymbol { w } _t } ||_2^2 + \\eta^2 || { \\boldsymbol { q } } _t ||_2^2 \\leq || \\boldsymbol { w } _t ||_2^2 + \\eta^2 || { \\boldsymbol { p } } _t ||_2^2 \\leq || \\boldsymbol { w } _t ||_2^2 + \\eta^2 || { \\boldsymbol { p } } _t ||_2^2 + 2 \\eta^2 \\sum_k \\beta^ { t-k } || { \\boldsymbol { p } } _k||^2_2 \\\\ ] Thus , equation ( 12 ) yields smaller norms than equation ( 8 ) . We will add these formulas in the paper . * * The improvements of the proposed SGDP and AdamP over the baseline SGD and Adam are small across experiments , and thus error bars are needed to validate * * The error bars for Tables 3 , 5 and D.2 are as follows . - Table 3 | |\\| Music Tagging \\||\\| Music Tagging \\||\\| Keyword Spotting \\||\\| Sound Event Tagging \\|| | : :| : - : | : - : | : - : | : - : | | | ROC-AUC | PR-AUC | Accuracy | F1 score | | Adam + SGD | \u00b1 0.055 | \u00b1 0.025 | - | - | | AdamW | \u00b1 0.081 | \u00b1 0.189 | \u00b1 0.055 | \u00b1 0.394 | | AdamP ( ours ) | \u00b1 0.074 | \u00b1 0.269 | \u00b1 0.061 | \u00b1 0.833 | - Table 5 | |\\| CUB \\||\\| CUB \\||\\| Cars-196 \\||\\| Cars-196 \\||\\| InShop \\||\\| InShop \\||\\| SOP \\||\\| SOP \\|| | : :| : - : | : :| : -- : | : -- : | : - : | : :| : - : | : :| | | Triplet | PA | Triplet | PA | Triplet | PA | Triplet | PA | | AdamW | \u00b1 0.91 | \u00b1 0.24 | \u00b1 0.82 | \u00b1 0.09 | \u00b1 0.52 | \u00b1 0.08 | \u00b1 0.71 | \u00b1 0.04 | | AdamP ( ours ) | \u00b1 0.62 | \u00b1 0.77 | \u00b1 1.35 | \u00b1 0.19 | \u00b1 0.82 | \u00b1 0.02 | \u00b1 0.74 | \u00b1 0.09 | - Table D.2 Biased MNIST Unbiased acc . at \u03c1 |\\| \u03c1 \\||\\| .999 \\||\\| .997 \\||\\| .995 \\||\\| .990 \\|| | : :| : :| : :| : :| : :| | Adam | \u00b1 6.04 | \u00b1 0.48 | \u00b1 0.70 | \u00b1 0.48 | | AdamP ( ours ) | \u00b1 1.73 | \u00b1 1.31 | \u00b1 2.16 | \u00b1 0.40 | 9-Class ImageNet | |\\| Biased \\||\\| UnBiased \\||\\| ImageNet-A \\|| | : :| : :| : -- : | : - : | | AdamW | \u00b1 0.27 | \u00b1 0.40 | \u00b1 1.06 | | AdamP ( ours ) | \u00b1 0.21 | \u00b1 0.27 | \u00b1 0.82 | For the other experiments , we have reported the averaged results of 3 independent trials ( randomizing data shuffling for stochastic optimizers , as well as other random factors in data augmentation ) . However , the data has been lost due to the limited capacity of our shared servers . We are running new training sessions to recover the error bars . We will share the results and update the paper ( ETA : November 18 ) ."}, "1": {"review_id": "Iz3zU3M316D-1", "review_text": "# # # Summary : This paper studies the hurtful effect of momentum on scale-invariant parameters ( such as weight matrices of linear layers followed by Batch Normalization ( BN ) ) : Indeed , momentum tend to increase the norm of the parameters , but the effective update size of scale-invariant parameters is inversely proportional to their norm , which leads to `` _premature decay of effective step size_ '' . The authors propose the improved SGDP and AdamP to reduce this issue , and show how it improves the performances of a wide variety of models ( ResNets , MobileNets , Transformers , etc ... ) on many different tasks ( ImageNet classification , adversarial training , audio classification , etc ... ) # # # Strengths : + This paper addresses the optimization of scale-invariant parameters , which can be found in pretty much all the state-of-the-art models . + The paper is well written and contains a good balance of illustrative examples , theoretical analysis and experiments . Good job ! + The empirical evaluation of the proposed algorithms is quite large , and many tasks and architectures are considered . + I also really like the `` cos ( w , Grad_w ) '' hack to automatically detect scale invariant parameters in the model . This is a very neat trick ! # # # Concern : My main concern is with the statement that `` _our paper is first to delve into the issue in the widely-used combination of scale-invariant parameters and momentum-based optimizers '' _ . Indeed the authors have missed [ 1 ] , which also adapts SGD with momentum and Adam to work on scale-invariant parameters , and showed performance improvement over vanilla SGD and Adam . Now , the approach of [ 1 ] is slightly different , in the sense that they propose to keep the scale-invariant parameters on the unit sphere S^1 . Also , [ 1 ] performs the projection onto the tangent space before computing the momentum term ( and second order moment in Adam ) , while this projection is done after in SGDP and AdamP . In any case , I do believe proper comparison with [ 1 ] throughout the paper is required , due to the conceptual similarity of both methods . It would be particularly relevant to show how the angles between SGDP and the SGD of [ 1 ] compare in Figure 1 ( I think they would be identical , although not 100 % certain ) . Also , I 'm curious to see if the proposed SGDP and AdamP work better than the algorithms proposed in [ 1 ] , as the nice automatic learning rate decay of BN disappears when keeping the scale-invariant parameters on the unit sphere ( which would be an advantage of the proposed SGDP and AdamP ) . # # # Reasons for score : Overall I really liked that paper , but I do n't think it would be fair to accept it without an in-depth comparison with [ 1 ] . # # # Questions / Comments : - I like the toy example in Figure 1 , although I 'm not sure I understand what you mean by : `` _Compared to GD , GD+Momentum speeds up the norm growth , resulting in a slower effective convergence in S^1 , though being faster in the nominal space R^2_ '' . It seems to me that the angle between the optimal solution and GD+momentum is smaller than the angle between the optimal solution and GD . I 'm not sure to understand how do you observe a slower effective convergence in S^1 ? - ( 3.1 , real-world experiments and F ) What happens if you increase WD when using SGD ? It seems that the best values reported for SGD in Table 1 and Figures F. * are with WD=1e-4 , which is also the highest value reported . One can imagine even better results could be obtained using WD=3e-4 or 1e-3 . It would be nice to have this result for completeness . - Reporting mean and standard deviation across several seeds would have been nice , but I 'm going to let this one slide , since results are reported on a lot of different tasks and architectures . [ 1 ] Cho , Minhyung and Lee , Jaehyung , _Riemannian approach to batch normalization_ , NIPS 2017 ___ # # # After author response and paper revision First , I would like to congratulate the authors for their amazing work during the rebuttal period . My main concern , the comparison against [ 1 ] , has been perfectly addressed in appendix G ( both `` conceptually '' , by highlighting the differences between both methods , and showing the advantages of the proposed SGDP and AdamP , but also empirically on ImageNet , where a fair comparison with proper hyper-parameter tuning has been performed ) . The authors also reinforced their empirical investigation by reporting standard deviation of the results , which allows to better appreciate the performances of SGDP and AdamP . Finally , they also added the experiments with higher weight decay , showing that indeed 1e-4 was the best value . With all these changes , I think the paper is good and I recommend its acceptance .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the high-quality expert review . We first address the concern regarding the comparison against an important prior work . We answer the rest of your questions , some of which may take days to get the experimental results . They will be updated as soon as the results arrive . In the meantime , we would be happy to discuss further , so please do not hesitate to comment and ask follow-up questions as necessary . * * Comparison with `` Cho & Lee , Riemannian approach to batch normalization '' [ 1 ] * * Thank you for pointing out an important reference , and sorry for missing a discussion about this work ; we were not aware of this paper . Indeed , both [ A ] and ours are motivated from the scale invariance of certain parameters in a neural network , and both propose a solution that reduces the radial component of the optimization steps . However , our contributions are still not significantly eclipsed by this paper . Our optimization algorithm is different from that of [ A ] , with a different set of theories to justify the methodologies . And this difference results in a practical edge for our optimizers . The crucial difference between [ A ] and ours is in the space where the optimization is performed . [ A ] performs the gradient steps on the Riemannian manifold . Ours project the updates on the tangent planes of the manifold . Thus , ours operates on the same Euclidean space where SGD and Adam operate . From a theory point of view , [ A ] has made contributions to the optimization theory on a Riemannian manifold . Our contributions are along a different axis : we focus on the norm growth when the updates are projected onto the tangent spaces ( Sections 2.4 , 3.1 , and 3.2 ) . We contribute present theoretical findings that are not covered by [ A ] . From the practicality point of view , we note that changing the very nature of space from Euclidean to Riemannian requires users to find the sensible ranges for many optimization hyperparameters again . For example , [ A ] has \u201c used different learning rates for the weights in Euclidean space and on Grassmann [ Riemannian ] manifolds \u201d ( page 7 of [ A ] ) , while in our case hyperparameters are largely compatible between scale-invariant and scale-variant parameters , for they are both accommodated in the same kind of space . We have shown that SGDP and AdamP outperform the SGD and Adam baselines with exactly the same optimization hyperparameters ( Section E.2 ) . The widely used Xavier or Gaussian initializations are no longer available in the spherical Riemannian manifold , necessitating changes in the code defining parameters and their initialization schemes ( e.g . [ A ] has employed a dedicated initialization based on truncated normals . See https : //github.com/MinhyungCho/riemannian-batch-normalization/blob/d1ac938ca5af8af1b7c1d4f708c1aacd2d8cbab9/gutils.py # L65 ) . Finally , [ A ] requires users to manually register scale-invariant hyperparameters . This procedure is not scalable , as the networks nowadays are becoming deeper and more complex and the architectures are becoming more machine-designed than handcrafted . Our optimizers automatically detect scale invariances through the orthogonality test ( eq.11 ) , and users do not need to register anything by themselves . The sum of all the pain points above is an optimization algorithm that is not readily applicable in many practical scenarios , as hinted in their Github manual ( https : //github.com/MinhyungCho/riemannian-batch-normalization # to-apply-this-algorithm-to-your-model ) where users are instructed to go through five convoluted steps to apply the algorithm . In stark contrast , users wanting to apply SGDP or AdamP only need to ( 1 ) pip install , ( 2 ) import , and ( 3 ) replace the \u201c torch.optim.SGD ( Adam ) \u201d with an \u201c SGDP ( AdamP ) \u201d class instantiation . No change in hyperparameters , design choices , or model codes is required . We recognize [ A ] as a great contribution to the field , and we are glad to confirm that another group of researchers have been motivated from a similar problem and have come up with a solution that shares certain similarities with ours . We believe that our contribution is not diminished by [ A ] though . Our solution is based on a different optimization space ( Euclidean as opposed to Riemannian ) with a different algorithm ( projection onto the tangent space as opposed to moving along the manifold ) and this provides significant practicality benefits for users . As an addition to the above points , we are conducting experiments on the toy examples ( Section 3.2 ) and ImageNet to empirically compare ours against [ A ] . We will share the results as soon as they are out ( ETA : November 18 ) . We will revise the paper with the discussion above as well as the experiments that are being run . [ A ] Cho , Minhyung , and Jaehyung Lee . `` Riemannian approach to batch normalization . '' Advances in Neural Information Processing Systems . 2017 ."}, "2": {"review_id": "Iz3zU3M316D-2", "review_text": "This paper points out that momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights . To solve the problem , two algorithms called SGDP and AdamP are proposed , which project the updates to tangent space of the parameter . Experiments on several tasks including image classification , language modeling , etc show the effectiveness of the proposed algorithms . The idea to study the integration of BN and momentum is interesting . The analyses and proposed algorithms provide guidance to practitioners . Questions : ( 1 ) What is the difference between the proposed algorithm and the algorithm in paper `` Cho & Lee , Riemannian approach to batch normalization '' ? Eq . ( 10 ) in your paper is exactly the same with Eq . ( 6 ) in [ Cho & Lee ] . It is an important related work which should be cited and compared with . ( 2 ) What does `` lies on the geodesic '' mean in Proposition 3.1 ? What is the relation between Proposition 3.1 and the convergence of the algorithm ? ( 3 ) It is not clear which results are proposed in related work and which results are proposed in this paper . Is Lemma 2.1 a new result ? If not , it needs a citation . ( 4 ) The theory part analyzes the effect of momentum , while the experiments shows the effect of weight decay . How does weight decay influence the norm growth theoretically ? Why not conduct experiments with varying momentum coefficient ? ( 5 ) The theory part shows the negative effect of momentum . Does it mean that `` SGD is a better choice than momentum SGD '' ? Momentum SGD is a standard algorithm to train deep neural networks with BN in practice but not vanilla SGD . How to explain it ? ( 6 ) What is the additional computational cost of the proposed algorithms compared with the baselines ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the high-quality review with spot-on questions . We include the requested comparison against the important missing prior work . We answer the rest of your questions as thoroughly as possible . We would be happy to discuss further , so please do not hesitate to comment and ask follow-up questions . * * Difference from `` Cho & Lee , Riemannian approach to batch normalization '' [ A ] ? * * Thank you for pointing out an important reference , and sorry for missing a discussion about this work ; we were not aware of this paper . Indeed , both [ A ] and ours are motivated from the scale invariance of certain parameters in a neural network , and both propose a solution that reduces the radial component of the optimization steps . However , our contributions are still not significantly eclipsed by this paper . Our optimization algorithm is different from that of [ A ] , with a different set of theories to justify the methodologies . And this difference results in a practical edge for our optimizers . The crucial difference between [ A ] and ours is in the space where the optimization is performed . [ A ] performs the gradient steps on the Riemannian manifold . Ours project the updates on the tangent planes of the manifold . Thus , ours operates on the same Euclidean space where SGD and Adam operate . From a theory point of view , [ A ] has made contributions to the optimization theory on a Riemannian manifold . Our contributions are along a different axis : we focus on the norm growth when the updates are projected onto the tangent spaces ( Sections 2.4 , 3.1 , and 3.2 ) . We contribute present theoretical findings that are not covered by [ A ] . From the practicality point of view , we note that changing the very nature of space from Euclidean to Riemannian requires users to find the sensible ranges for many optimization hyperparameters again . For example , [ A ] has \u201c used different learning rates for the weights in Euclidean space and on Grassmann [ Riemannian ] manifolds \u201d ( page 7 of [ A ] ) , while in our case hyperparameters are largely compatible between scale-invariant and scale-variant parameters , for they are both accommodated in the same kind of space . We have shown that SGDP and AdamP outperform the SGD and Adam baselines with exactly the same optimization hyperparameters ( Section E.2 ) . The widely used Xavier or Gaussian initializations are no longer available in the spherical Riemannian manifold , necessitating changes in the code defining parameters and their initialization schemes ( e.g . [ A ] has employed a dedicated initialization based on truncated normals . See https : //github.com/MinhyungCho/riemannian-batch-normalization/blob/d1ac938ca5af8af1b7c1d4f708c1aacd2d8cbab9/gutils.py # L65 ) . Finally , [ A ] requires users to manually register scale-invariant hyperparameters . This procedure is not scalable , as the networks nowadays are becoming deeper and more complex and the architectures are becoming more machine-designed than handcrafted . Our optimizers automatically detect scale invariances through the orthogonality test ( eq.11 ) , and users do not need to register anything by themselves . The sum of all the pain points above is an optimization algorithm that is not readily applicable in many practical scenarios , as hinted in their Github manual ( https : //github.com/MinhyungCho/riemannian-batch-normalization # to-apply-this-algorithm-to-your-model ) where users are instructed to go through five convoluted steps to apply the algorithm . In stark contrast , users wanting to apply SGDP or AdamP only need to ( 1 ) pip install , ( 2 ) import , and ( 3 ) replace the \u201c torch.optim.SGD ( Adam ) \u201d with an \u201c SGDP ( AdamP ) \u201d class instantiation . No change in hyperparameters , design choices , or model codes is required . We recognize [ A ] as a great contribution to the field , and we are glad to confirm that another group of researchers have been motivated from a similar problem and have come up with a solution that shares certain similarities with ours . We believe that our contribution is not diminished by [ A ] though . Our solution is based on a different optimization space ( Euclidean as opposed to Riemannian ) with a different algorithm ( projection onto the tangent space as opposed to moving along the manifold ) and this provides significant practicality benefits for users . As an addition to the above points , we are conducting experiments on the toy examples ( Section 3.2 ) and ImageNet to empirically compare ours against [ A ] . We will share the results as soon as they are out ( ETA : November 18 ) . We will revise the paper with the discussion above as well as the experiments that are being run . [ A ] Cho , Minhyung , and Jaehyung Lee . `` Riemannian approach to batch normalization . '' Advances in Neural Information Processing Systems . 2017 ."}, "3": {"review_id": "Iz3zU3M316D-3", "review_text": "Summary : Based on the assumption that the rapidly decrease step size \\delta \\omega leads to a solve effective convergence of \\omega . This paper proposes to use a projection step to remove the radial component , and thus reduce the norm of training parameters , and faster the training procedure . The experiments look good to me while the derivation of this paper based on many assumptions and conjectures . I tend to accept this paper at this time . However , I am not an expert at this area I will not be sad if this paper is reject by other reviews . minor problems : 1 ) Can the author explain more on how to derive eq . ( 4 ) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for spending time reading a paper out of your domain of expertise . We benefit a lot from reviewers from other domains , as we may learn how to make the paper interesting for a broader audience . We answer your question below . Please do not hesitate to ask for further clarifications for any part of the paper , should there be points that are not so obvious . * * How to derive eq . ( 4 ) * * We start from the observation that if f ( w ) is scale invariant with respect to w , then \\\\ [ \\frac { \\partial f ( c\\boldsymbol { w } ) } { \\partial c } =0 . \\\\ ] This follows from the fact that the rate of change along w is always zero . Now , by the multivariate chain rule , we compute \\\\ [ \\frac { \\partial f ( c\\boldsymbol { w } ) } { \\partial c } = \\sum_i \\left [ \\frac { \\partial f } { \\partial v_i } \\frac { \\partial v_i } { \\partial c } \\right ] _ { v = c\\boldsymbol { w } } \\\\ ] where $ \\boldsymbol { v } =c\\boldsymbol { w } $ . We continue the derivation as follows : \\\\ [ \\sum_i \\left [ \\frac { \\partial f } { \\partial v_i } \\frac { \\partial v_i } { \\partial c } \\right ] _ { v = c\\boldsymbol { w } } = \\sum_i \\nabla f ( \\boldsymbol { v } ) _i w_i = \\sum_i \\nabla f ( \\boldsymbol { w } ) _i w_i = \\boldsymbol { w } ^\\top \\nabla f ( \\boldsymbol { w } ) .\\\\ ] Hence , it follows that the scale-invariance leads to the orthogonality between gradient and the radial direction : \\\\ [ \\boldsymbol { w } ^\\top \\nabla f ( \\boldsymbol { w } ) = 0 . \\\\ ]"}}