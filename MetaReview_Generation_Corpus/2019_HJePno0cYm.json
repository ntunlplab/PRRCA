{"year": "2019", "forum": "HJePno0cYm", "title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "decision": "Reject", "meta_review": "despite the (significant) improvement in language modelling, it has always been a thorny issue whether better language models (at this level) lead to better performance in the downstream task or whether such a technique could be used to build a better conditional language model which often focuses on the aspect of generation. in this context, the reviewers found it difficult to see the merit of the proposed approach, as the technique itself may be considered a rather trivial application of earlier approaches such as truncated backprop. it would be good to apply this technique to e.g. document-level generation and see if the proposed approach can strike an amazing balance between computational efficiency and generation performance.", "reviews": [{"review_id": "HJePno0cYm-0", "review_text": "This paper proposes a variant of transformer to train language model, it uses two modifications, one is the segment level recurrence with state reuse, the other is relative positional encoding, which significantly enhances the power to model long range dependency. Extensive experiments in terms of perplexity results are reported, specially on WikiText-103 corpus, significant perplexity reduction has been achieved. Perplexity is not a gold standard for language model, the authors are encouraged to report experimental results on real world applications such as word rate reduction ASR on BLEU score improvement machine translation. Ciprian Chelba and Frederick Jelinek, Structured language modeling. Computer Speech and Language (2000) 14, 283\u2013332. Peng Xu, Frederick Jelinek: Random forests and the data sparseness problem in language modeling. Computer Speech & Language 21(1): 105-152 (2007).", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable comments ! As far as we know , almost all language models were evaluated by perplexity in previous work . Please see our comments above regarding the importance of language modeling on its own ."}, {"review_id": "HJePno0cYm-1", "review_text": "This paper proposes a Transformer based RNN structure \"Transformer-XL\" to capture long-range contextual relations and targets on language model task. The idea is straightforward: it splits the input sequence into equal and fixed length segments, and recurrently apply the Transformer over the sequence of segments, in which the hidden states for the previous segment are treated as a memory to attend for the next segment. This paper is well-organized and well-written, and easy to follow. The empirical results also demonstrate the proposed model can achieve SoTA performance on several word- and character-based language model benchmarks. Pros: 1. The model is designed based on a careful engineering: 1) taking into account the history hidden states for long-term dependency modeling and 2) alignment scores calculated from multiple perspectives for relative position modeling and global significance capturing. In addition, in contrast to the previous Transformer-based language model, benefiting from the recurrent architecture, both training and decoding can be accelerated. 2. The experimental results show that the proposed Transformer-XL can surpass the baseline model and achieve new state-of-the-art perplexity or bpc on word- or char-based language model task. And, based on the proposed new metric, RECL, the analysis for context length modeling verifies the proposed model can make the best of long-range dependencies. Cons: 1. The proposed model is ad-hoc and is only compatible with language model task. Is it possible to extend the proposed model to more general and practical tasks (e.g., seq2seq tasks)? 2. The absence of a popular language model benchmark, WikiText-2, which has been evaluated in most previous papers. 3. It is notable that there are no ubiquitous decoding techniques for the language used in both the proposed model and baselines, such as dynamical evaluation and continuous cache pointer. However, these techniques are essential for the RNN-LM baselines to achieve state-of-the-art performance, and has been standardly used in most previous works. Therefore, the comparison seems unfair. Minor comments: In Figure 1 and 2, it is better to include a legend explaining the meaning of different colors for different nodes. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable comments ! [ WT2 ] WT2 shares the same test set as WT103 , and the only difference is that WT103 has more training data . Since language modeling has almost unlimited training data in nature , we believe it brings less benefit to compare models on more small-scale datasets as we already have results on Penn Treebank which is also a small dataset . [ Test-time evaluation techniques ] In Table 1 , we show that our method without any test-time evaluation techniques is still 21+ points better than Grave et al which employs test-time continuous cache on WT103 . On enwiki8 , mLSTM + dynamic eval [ 1 ] achieves a BPC of 1.08 , which is still 0.09 worse than Transformer-XL without dynamic evaluation . On One Billion Word , the best previous result did not use test-time evaluation techniques . The only exception is Penn Treebank , where we exclude results with test-time techniques to focus on comparing different architectures . This is fair comparison because all considered models do not use test-time techniques . Moreover , according to previous results , test-time evaluation techniques bring consistent improvement to different architectures ( Yang et al 2017 , Merity et al 2017 ) . Please see our comments above regarding the importance of language modeling on its own . [ 1 ] Ben Krause , Emmanuel Kahembwe , Iain Murray , and Steve Renals . Dynamic evaluation of neural sequence models ."}, {"review_id": "HJePno0cYm-2", "review_text": "This paper puts forward a new schema for language modeling, especially for relationship between two parts far apart. The experimental results on WikiText-103 are good, improving the STOA PPL by 9.0. On the other three datasets, however, there's little or no gain. The speed comparison should be carried out over more LM models, as Al-Rfou is not the fastest. The writing is not very clear, especially around equations. Overall the contribution of this paper is marginally incremental: 1. The major proposed idea is just to add one no-grad previous segment into the prediction for next segment. This is similar to Residual network idea but more simplified. 2. Using relative positional encoding is not a new idea, e.g. https://arxiv.org/pdf/1803.02155.pdf. 3. Reusing previous level/segment computation with gradient fixed is also not a big innovation. typo: 1. end of page 3, and \"W.\" denotes\". 2. The speed experiment should be put in the main text.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your valuable comments ! [ Speed Comparison ] As shown in our paper , Transformer is the state-of-the-art model on language modeling , and Al Rfou et al was the previous SoTA of Transformer language models . The main argument of our results on computational time is that Transformer-XL substantially improves the speed while getting even better results . It is less interesting to obtain speedup over a poorly performing model . On the other hand , as our speedup techniques specifically target Transformers , we believe Al Rfou et al is the most appropriate baseline to test the effects of our proposed methods . Please see our comments above regarding the significance and novelty of our contributions ."}], "0": {"review_id": "HJePno0cYm-0", "review_text": "This paper proposes a variant of transformer to train language model, it uses two modifications, one is the segment level recurrence with state reuse, the other is relative positional encoding, which significantly enhances the power to model long range dependency. Extensive experiments in terms of perplexity results are reported, specially on WikiText-103 corpus, significant perplexity reduction has been achieved. Perplexity is not a gold standard for language model, the authors are encouraged to report experimental results on real world applications such as word rate reduction ASR on BLEU score improvement machine translation. Ciprian Chelba and Frederick Jelinek, Structured language modeling. Computer Speech and Language (2000) 14, 283\u2013332. Peng Xu, Frederick Jelinek: Random forests and the data sparseness problem in language modeling. Computer Speech & Language 21(1): 105-152 (2007).", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable comments ! As far as we know , almost all language models were evaluated by perplexity in previous work . Please see our comments above regarding the importance of language modeling on its own ."}, "1": {"review_id": "HJePno0cYm-1", "review_text": "This paper proposes a Transformer based RNN structure \"Transformer-XL\" to capture long-range contextual relations and targets on language model task. The idea is straightforward: it splits the input sequence into equal and fixed length segments, and recurrently apply the Transformer over the sequence of segments, in which the hidden states for the previous segment are treated as a memory to attend for the next segment. This paper is well-organized and well-written, and easy to follow. The empirical results also demonstrate the proposed model can achieve SoTA performance on several word- and character-based language model benchmarks. Pros: 1. The model is designed based on a careful engineering: 1) taking into account the history hidden states for long-term dependency modeling and 2) alignment scores calculated from multiple perspectives for relative position modeling and global significance capturing. In addition, in contrast to the previous Transformer-based language model, benefiting from the recurrent architecture, both training and decoding can be accelerated. 2. The experimental results show that the proposed Transformer-XL can surpass the baseline model and achieve new state-of-the-art perplexity or bpc on word- or char-based language model task. And, based on the proposed new metric, RECL, the analysis for context length modeling verifies the proposed model can make the best of long-range dependencies. Cons: 1. The proposed model is ad-hoc and is only compatible with language model task. Is it possible to extend the proposed model to more general and practical tasks (e.g., seq2seq tasks)? 2. The absence of a popular language model benchmark, WikiText-2, which has been evaluated in most previous papers. 3. It is notable that there are no ubiquitous decoding techniques for the language used in both the proposed model and baselines, such as dynamical evaluation and continuous cache pointer. However, these techniques are essential for the RNN-LM baselines to achieve state-of-the-art performance, and has been standardly used in most previous works. Therefore, the comparison seems unfair. Minor comments: In Figure 1 and 2, it is better to include a legend explaining the meaning of different colors for different nodes. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable comments ! [ WT2 ] WT2 shares the same test set as WT103 , and the only difference is that WT103 has more training data . Since language modeling has almost unlimited training data in nature , we believe it brings less benefit to compare models on more small-scale datasets as we already have results on Penn Treebank which is also a small dataset . [ Test-time evaluation techniques ] In Table 1 , we show that our method without any test-time evaluation techniques is still 21+ points better than Grave et al which employs test-time continuous cache on WT103 . On enwiki8 , mLSTM + dynamic eval [ 1 ] achieves a BPC of 1.08 , which is still 0.09 worse than Transformer-XL without dynamic evaluation . On One Billion Word , the best previous result did not use test-time evaluation techniques . The only exception is Penn Treebank , where we exclude results with test-time techniques to focus on comparing different architectures . This is fair comparison because all considered models do not use test-time techniques . Moreover , according to previous results , test-time evaluation techniques bring consistent improvement to different architectures ( Yang et al 2017 , Merity et al 2017 ) . Please see our comments above regarding the importance of language modeling on its own . [ 1 ] Ben Krause , Emmanuel Kahembwe , Iain Murray , and Steve Renals . Dynamic evaluation of neural sequence models ."}, "2": {"review_id": "HJePno0cYm-2", "review_text": "This paper puts forward a new schema for language modeling, especially for relationship between two parts far apart. The experimental results on WikiText-103 are good, improving the STOA PPL by 9.0. On the other three datasets, however, there's little or no gain. The speed comparison should be carried out over more LM models, as Al-Rfou is not the fastest. The writing is not very clear, especially around equations. Overall the contribution of this paper is marginally incremental: 1. The major proposed idea is just to add one no-grad previous segment into the prediction for next segment. This is similar to Residual network idea but more simplified. 2. Using relative positional encoding is not a new idea, e.g. https://arxiv.org/pdf/1803.02155.pdf. 3. Reusing previous level/segment computation with gradient fixed is also not a big innovation. typo: 1. end of page 3, and \"W.\" denotes\". 2. The speed experiment should be put in the main text.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your valuable comments ! [ Speed Comparison ] As shown in our paper , Transformer is the state-of-the-art model on language modeling , and Al Rfou et al was the previous SoTA of Transformer language models . The main argument of our results on computational time is that Transformer-XL substantially improves the speed while getting even better results . It is less interesting to obtain speedup over a poorly performing model . On the other hand , as our speedup techniques specifically target Transformers , we believe Al Rfou et al is the most appropriate baseline to test the effects of our proposed methods . Please see our comments above regarding the significance and novelty of our contributions ."}}