{"year": "2019", "forum": "rkgpCoRctm", "title": "Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics", "decision": "Reject", "meta_review": "The paper proposes a simple method for detecting out-of-distribution samples. The authors' major finding is that mean and standard deviation within feature maps can be used as an input for classifying out-of-distribution (OOD) samples. The proposed method is simple and practical.\n\nThe reviewers and AC note the following potential weaknesses: (1) limited novelty and somewhat ad-hoc approach, i.e., it is not too surprising to expect that such statistics can be useful for the purpose. Some theoretical justification might help. (2) arguable experimental settings, i.e., the performance highly varies depending on validation (even in the revised draft), and sometimes irrationally good. It also depends on the choice of classifier.\n\nFor (2), I think the whole evaluation should be done assuming that we don't know how it looks the OOD set. Under the setting, the authors should compare the proposed method and existing ones for fair comparisons. AC understands the authors follows the same experimental settings of some previous work addressing this problem, but it's time that this is changed. Indeed, a recent paper by Lee at al. 2018 considers such a setting for detecting more general types of abnormal samples including OOD.\n\nIn overall, the proposed idea is simple and easy to use. However, AC decided that the authors need more significant works to publish the work.\n", "reviews": [{"review_id": "rkgpCoRctm-0", "review_text": "The authors present a simple algorithm based on the statistics of neural activations of deep networks to detect out-of-distribution samples. The idea is to use the existing running estimate of mean and variance within BatchNorm layers to construct feature representations that are later fed into a simple linear classifier. The authors demonstrate superior performance over the previous state-of-art in the standard evaluation setting and provide fascinating insights and empirical analysis of their method. There are several aspects of this work that I admire. - The authors evaluate the generalization of their OOD detection model through evaluation against unseen OOD samples. This critical evaluation strategy is not typical in this literature and is much needed. - The organization of the material and the depth of the discussion is of high quality. They discuss and connect the previous work, they clearly explain the idea and provide empirical results to support the design decisions, and run several experiments to evaluate their method from different angles followed by interesting discussions. - The proposed method is easy to implement and has a minimal runtime complexity with no adverse effect on the underlying classifier. - The source code is already included in the submission. My only concern is that the feature pooling strategy first averages the input spatially, then across the channels. This feature size reduction is necessary because we have to ensure the following OOD classifier does not overfit in the validation stage. However, this reduction also introduces a permutation invariance in the feature space that is not desirable in OOD sample detection. I think it would make the work more valuable if the authors also take a critical look at the possible failure cases -- a short discussion of the weaknesses and assumptions. Overall, the paper is technically sound and well-organized with sufficient coverage of the previous work. A thorough series of evaluations support the claims. It is a novel combination of existing techniques. The empirical evidence is strong and insightful. Given the simplicity of the method, I would expect a quick adoption by the community. ------ Rev. In light of the rebuttal and the following discussions I have updated my rating to 7.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed analysis of the method . Regarding your concern , we added a short note discussing the implications of the feature reduction ( pooling and averaging ) , the method assumptions and its weakness/limitations in Section 3.2 : \u201c Using averages of the low-order statistics could lead to issues in deeper layers , where activations are in general concentrated over a fewer number of channels . In this case , the mean of the statistics over channels might not be an appropriate data reduction function . Nevertheless , as we show in the experiments section , this effect has not impacted the performance of the proposed method , but more investigation is warranted. \u201d Bests , Authors ."}, {"review_id": "rkgpCoRctm-1", "review_text": "Summary: A relatively simple approach for detecting out-of-distribution samples by having a parallel logistic regression model using simple statistics (mean and variance) over output of each batch normalisation layer, in order to discriminate between in-distribution and out-of-distribution samples. Results are appealing but presentation is lacking clarity at time and some doubts on the correctness of the experiments remain. With the goal of detecting out-of-distribution sets, the authors propose to use logistic regression over simple statistics (mean and variance) of each batch normalization layer of CNN in order to discriminate between in-distribution (ID) and out-of-distribution (OOD) samples. They argue that ID and OOD samples can be discriminated with these statistics. Quality: The motivations of the paper are clear, it aims at having better capacity to detect OOD samples with a method that involves less computations. However, the quality of the experiments is not good enough and I have doubts on their validity. Originality: Ok. The proposal is relatively simple and is based on the intuition that statistics for the batch normalization is useful to detect OOD samples. The problem is not new, the approach is relatively ad hoc, but it works. Significance of the work: The results reported are unreasonably good. Although the authors claim the improvement of detection of OOD is significant, the results achieved by detecting **all** the out-distribution samples sounds weird and irrational. How rejecting all Tiny-ImageNet is possible while there are several overlap between the classes presented in TinyImageNet vs. Cifar10/Cifar100 (cf. Table 8)? To me, it looks like the model either overfitted something else than the content of the images, maybe the background noise or similar regarding the nature of the data. More experiments with different in-distribution datasets should be made to be convincing. All experiments reported are using either Cifar10 or Cifar100 as in-distribution datasets. The author also claim using few samples from a single OOD set is enough for training the regressor that provides OOD-ness score. Is it true for any OOD set or only a carefully chosen OOD set can demonstrate this behavior? What is the criteria for selecting a good OOD set for training the regressor? Despite of the fact that the proposed method is heavily dependent on the threshold, the authors barely discuss of it. I am assuming that threshold is on OOD-ness score, is that correct? How does look like the OOD-ness score for an ID set over different OOD sets? Providing the OOD-ness score for ID and OOD could reflect how the proposed method is sensitive to a selected threshold. In other words, is selecting a fix threshold will to the TPR / FPR across different OODs. The overall writing style is perfectible. I did not found the paper super clear in the presentation and it is difficult to really get all useful information for it. However, the authors appear knowledgeable of the literature and the overall structure is clear. An example of lack of clarity in the explanations: in Table 7, I have difficulty to make sense of the 100% achieved for \u201cOurs (pair)\u201d vs \u201cOurs\u201d. Is the \u201cOurs (pair)\u201d the rate obtained with the exact pair used for adjusting the threshold, while \u201cOurs\u201d is on another dataset? If not, what this mean? Moreover, reporting columns all with 100% is not a good practice, it seems to be a stunt to impress the reader, while not carrying much in term of content and understanding. In Table 8, I do not understand what the values in parenthesis means. Another element: why for training the regressor, the IN and OOD samples are not selected from their corresponding training sets instead of splitting their test sets to a validation and test sets? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ Part 2/2 ] The choice of OOD dataset for this experiment was done based on the performance on separate validation samples . In order to highlight the impact of selecting a given OOD dataset for training the logistic regressor , we \u2019 ve added Table 6 to the manuscript . Note that this is an advantage w.r.t.the published SOTA ( ODIN [ 2 ] ) , where they fit the parameters to the same OOD dataset they use to test , whereas our method generalizes to unseen OOD datasets . To address your concern even further , we performed unsupervised training of our model using one-class SVM , meaning no OOD samples are used during training time , only samples from the ID distribution . As one can see in the new results added to Table 4 , the method is still able to separate the ID samples with reasonable performance without ever seeing any OOD sample during training . Q4 ) \u201c Despite of the fact that the proposed method is heavily dependent on the threshold , the authors barely discuss of it . I am assuming that threshold is on OOD-ness score , is that correct ? How does look like the OOD-ness score for an ID set over different OOD sets ? Providing the OOD-ness score for ID and OOD could reflect how the proposed method is sensitive to a selected threshold . In other words , is selecting a fix threshold will to the TPR / FPR across different OODs. \u201d The chosen threshold is always set to obtain 95 % TPR in the dataset used to train the logistic regressor . We do not further adjust the threshold for each OOD . We thank the reviewer for pointing this out . We realized this was not clear in the manuscript , and we amended the manuscript to clarify this point . It should also be pointed out that the AUROC value , shown in Table 7 , provides a metric that does not depend on selecting a threshold . Q5 ) \u201c An example of lack of clarity in the explanations : in Table 7 , I have difficulty to make sense of the 100 % achieved for \u201c Ours ( pair ) \u201d vs \u201c Ours \u201d . Is the \u201c Ours ( pair ) \u201d the rate obtained with the exact pair used for adjusting the threshold , while \u201c Ours \u201d is on another dataset ? If not , what this mean ? \u201c We agree that this was misleading in the submitted manuscript . We reorganized the experimental section to make it clearer . For clarification , \u201c Ours ( pair ) \u201d mean that we fitted the linear classifier in a validation partition of the OOD validation set and tested in another partition of the OOD , while \u201c Ours \u201d means that we fitted both the parameters and the threshold to one OOD validation set ( the combined validation partitions of TinyImageNet ( c ) and Gaussian noise ) and tested on all other OOD dataset \u2019 s test partition . Q6 ) \u201c Moreover , reporting columns all with 100 % is not a good practice , it seems to be a stunt to impress the reader , while not carrying much in term of content and understanding. \u201d The results in this table follow the standard protocol in the OOD detection literature , by setting the threshold to 95 % TPR [ Hendrycks2017 , Liang2018 , Lee2018a , Lee2018b ] . We added an explicit comment explaining this . In other words , we are only picking one point in the ROC curve that . Please refer to our answer to Q1 where we show that the 100 % values stem from this arbitrary threshold . Q7 ) `` In Table 8 , I do not understand what the values in parenthesis means . '' Thanks for pointing this out . In fact , we realized we this was n't enough stressed in the manuscript . .They correspond to the standard deviation after running each experiment four times ( each one on a different backbone model trained from scratch on the ID dataset ) to understand how the results varied across different trained models . Note that this is not common in literature where only one run is reported . Q8 ) \u201c Another element : why for training the regressor , the IN and OOD samples are not selected from their corresponding training sets instead of splitting their test sets to a validation and test sets ? \u201d To do a fair comparison , we follow the same data split as previous works . Some experiments that we did internally indicate that the feature distribution of training and validation samples is essentially the same . Just to state some numbers , for a DenseNet BC 100-12 model using CIFAR-100 as ID model , we trained the logistic regressor using 1000 arbitrary samples from the training set ( i.e. , 1000 samples chosen randomly over 50000 images ) + 2000 validation samples from OOD datasets ( TinyImageNet crop + Gaussian ) . We ran this experiments 5 times and we got the results displayed below as mean ( std ) calculated over the runs : OOD dataset | TNR @ 95 TPR TinyImageNet ( c ) | 100.0 ( 0.0 ) TinyImageNet ( r ) | 93.6 ( 2.9 ) LSUN ( c ) | 100.0 ( 0.0 ) LSUN ( r ) | 97.8 ( 2.1 ) iSUN | 95.9 ( 2.8 ) Uniform | 100.0 ( 0.0 ) Gaussian | 100.0 ( 0.0 ) Which is a similar result ( given the variance ) to the one we reported in Table 7 ( using the 1000 validation samples ) . Best , The authors ."}, {"review_id": "rkgpCoRctm-2", "review_text": "There has been recent interest in using statistics and information summary measures to evaluate what deep nets are trying to do. Following the line of work, the paper suggests to use mean and variance of Z-scores accumulated across all layers/channels as features to distinguish ID and OOD samples. Simple idea but needs some work in its current format. Firstly, the bulk of content in Sections 2 and 3 can be reduced/shortened since the importance of normalized statistics to understand learning models is well known, and not novel. 1) The choice of datasets/netowrks needs to be understood here. How is the OOD summary changing as more layers are added into computing the score (since the score is basically averaging all layers'/channels contribution)? 2) What happens if we split the ID itself into two datasets and train on one, while using the other as OOD? 3) (r) is random and (c) is not is it for the TinyImages? Seems to be the other way around. 4) What is the influence of the dataset? Since the summaries are first order statistics, there can be significant dependance of the 'coverage' of training data (i.e., how many and how good of instances are present for each class)? This is purely a sampling problem and it may reciprocate in the OOD scores (back to first order statistics). This needs to be tested. 5) Statistical tests of significance needs to be reported for the performance summaries shown in the Tables. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We revisited sections 2 and 3 to make sure they are as concise as possible . 1.The ID/OOD datasets , as well as the networks ( WRN 28-10 , DenseNet BC 100-12 ) , were chosen to be able to compare our results with the state-of-the-art . Figure 7 ( in the Appendix ) shows how the TNR changes as more layers are added to the logistic regressor . For the WRN 28-10 , using only \u2153 of the layers gave us already more than 90 % of TNR @ 95 % of TPR , and adding more layers ( rightmost bins ) is really helpful to the OOD-ness score . 2.In our experiments , we always split the ID dataset between training ( used for training the backbone ) , validation ( used for training the logistic regressor ) and test partitions ( used for testing our OOD detector , unseen by both backbone and logistic regressor ) , and the samples from the ID dataset are correctly classified as such ( see Table 6 , 7 , 11 ) . We also tested training the logistic regressor with arbitrary samples from the training dataset ( Reviewer 2 , Q8 answer ) , and the results did not change , indicating that the features do not change significantly between the partitions . Also , the tSNE plot ( Figure 3 ) shows that ID/OOD samples are different in our proposed feature space . We hope this answers your doubt . 3 . ( r ) stands for resize , while ( c ) stands for crop . 4.To address your concern , we now show on new Table 6 the results of using the summaries from one dataset to evaluate on another one . 5.We trained 4 models for each network architecture ( each one initialized with a different random seed ) and all results reported consider the standard deviation of the results ( through averaging , median , and other relevant statistics ) . In the revised manuscript , we specify the notation in each table . We would like to note that none of the previous works reported results considering different realizations of each backbone model . Bests , Authors ."}], "0": {"review_id": "rkgpCoRctm-0", "review_text": "The authors present a simple algorithm based on the statistics of neural activations of deep networks to detect out-of-distribution samples. The idea is to use the existing running estimate of mean and variance within BatchNorm layers to construct feature representations that are later fed into a simple linear classifier. The authors demonstrate superior performance over the previous state-of-art in the standard evaluation setting and provide fascinating insights and empirical analysis of their method. There are several aspects of this work that I admire. - The authors evaluate the generalization of their OOD detection model through evaluation against unseen OOD samples. This critical evaluation strategy is not typical in this literature and is much needed. - The organization of the material and the depth of the discussion is of high quality. They discuss and connect the previous work, they clearly explain the idea and provide empirical results to support the design decisions, and run several experiments to evaluate their method from different angles followed by interesting discussions. - The proposed method is easy to implement and has a minimal runtime complexity with no adverse effect on the underlying classifier. - The source code is already included in the submission. My only concern is that the feature pooling strategy first averages the input spatially, then across the channels. This feature size reduction is necessary because we have to ensure the following OOD classifier does not overfit in the validation stage. However, this reduction also introduces a permutation invariance in the feature space that is not desirable in OOD sample detection. I think it would make the work more valuable if the authors also take a critical look at the possible failure cases -- a short discussion of the weaknesses and assumptions. Overall, the paper is technically sound and well-organized with sufficient coverage of the previous work. A thorough series of evaluations support the claims. It is a novel combination of existing techniques. The empirical evidence is strong and insightful. Given the simplicity of the method, I would expect a quick adoption by the community. ------ Rev. In light of the rebuttal and the following discussions I have updated my rating to 7.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed analysis of the method . Regarding your concern , we added a short note discussing the implications of the feature reduction ( pooling and averaging ) , the method assumptions and its weakness/limitations in Section 3.2 : \u201c Using averages of the low-order statistics could lead to issues in deeper layers , where activations are in general concentrated over a fewer number of channels . In this case , the mean of the statistics over channels might not be an appropriate data reduction function . Nevertheless , as we show in the experiments section , this effect has not impacted the performance of the proposed method , but more investigation is warranted. \u201d Bests , Authors ."}, "1": {"review_id": "rkgpCoRctm-1", "review_text": "Summary: A relatively simple approach for detecting out-of-distribution samples by having a parallel logistic regression model using simple statistics (mean and variance) over output of each batch normalisation layer, in order to discriminate between in-distribution and out-of-distribution samples. Results are appealing but presentation is lacking clarity at time and some doubts on the correctness of the experiments remain. With the goal of detecting out-of-distribution sets, the authors propose to use logistic regression over simple statistics (mean and variance) of each batch normalization layer of CNN in order to discriminate between in-distribution (ID) and out-of-distribution (OOD) samples. They argue that ID and OOD samples can be discriminated with these statistics. Quality: The motivations of the paper are clear, it aims at having better capacity to detect OOD samples with a method that involves less computations. However, the quality of the experiments is not good enough and I have doubts on their validity. Originality: Ok. The proposal is relatively simple and is based on the intuition that statistics for the batch normalization is useful to detect OOD samples. The problem is not new, the approach is relatively ad hoc, but it works. Significance of the work: The results reported are unreasonably good. Although the authors claim the improvement of detection of OOD is significant, the results achieved by detecting **all** the out-distribution samples sounds weird and irrational. How rejecting all Tiny-ImageNet is possible while there are several overlap between the classes presented in TinyImageNet vs. Cifar10/Cifar100 (cf. Table 8)? To me, it looks like the model either overfitted something else than the content of the images, maybe the background noise or similar regarding the nature of the data. More experiments with different in-distribution datasets should be made to be convincing. All experiments reported are using either Cifar10 or Cifar100 as in-distribution datasets. The author also claim using few samples from a single OOD set is enough for training the regressor that provides OOD-ness score. Is it true for any OOD set or only a carefully chosen OOD set can demonstrate this behavior? What is the criteria for selecting a good OOD set for training the regressor? Despite of the fact that the proposed method is heavily dependent on the threshold, the authors barely discuss of it. I am assuming that threshold is on OOD-ness score, is that correct? How does look like the OOD-ness score for an ID set over different OOD sets? Providing the OOD-ness score for ID and OOD could reflect how the proposed method is sensitive to a selected threshold. In other words, is selecting a fix threshold will to the TPR / FPR across different OODs. The overall writing style is perfectible. I did not found the paper super clear in the presentation and it is difficult to really get all useful information for it. However, the authors appear knowledgeable of the literature and the overall structure is clear. An example of lack of clarity in the explanations: in Table 7, I have difficulty to make sense of the 100% achieved for \u201cOurs (pair)\u201d vs \u201cOurs\u201d. Is the \u201cOurs (pair)\u201d the rate obtained with the exact pair used for adjusting the threshold, while \u201cOurs\u201d is on another dataset? If not, what this mean? Moreover, reporting columns all with 100% is not a good practice, it seems to be a stunt to impress the reader, while not carrying much in term of content and understanding. In Table 8, I do not understand what the values in parenthesis means. Another element: why for training the regressor, the IN and OOD samples are not selected from their corresponding training sets instead of splitting their test sets to a validation and test sets? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ Part 2/2 ] The choice of OOD dataset for this experiment was done based on the performance on separate validation samples . In order to highlight the impact of selecting a given OOD dataset for training the logistic regressor , we \u2019 ve added Table 6 to the manuscript . Note that this is an advantage w.r.t.the published SOTA ( ODIN [ 2 ] ) , where they fit the parameters to the same OOD dataset they use to test , whereas our method generalizes to unseen OOD datasets . To address your concern even further , we performed unsupervised training of our model using one-class SVM , meaning no OOD samples are used during training time , only samples from the ID distribution . As one can see in the new results added to Table 4 , the method is still able to separate the ID samples with reasonable performance without ever seeing any OOD sample during training . Q4 ) \u201c Despite of the fact that the proposed method is heavily dependent on the threshold , the authors barely discuss of it . I am assuming that threshold is on OOD-ness score , is that correct ? How does look like the OOD-ness score for an ID set over different OOD sets ? Providing the OOD-ness score for ID and OOD could reflect how the proposed method is sensitive to a selected threshold . In other words , is selecting a fix threshold will to the TPR / FPR across different OODs. \u201d The chosen threshold is always set to obtain 95 % TPR in the dataset used to train the logistic regressor . We do not further adjust the threshold for each OOD . We thank the reviewer for pointing this out . We realized this was not clear in the manuscript , and we amended the manuscript to clarify this point . It should also be pointed out that the AUROC value , shown in Table 7 , provides a metric that does not depend on selecting a threshold . Q5 ) \u201c An example of lack of clarity in the explanations : in Table 7 , I have difficulty to make sense of the 100 % achieved for \u201c Ours ( pair ) \u201d vs \u201c Ours \u201d . Is the \u201c Ours ( pair ) \u201d the rate obtained with the exact pair used for adjusting the threshold , while \u201c Ours \u201d is on another dataset ? If not , what this mean ? \u201c We agree that this was misleading in the submitted manuscript . We reorganized the experimental section to make it clearer . For clarification , \u201c Ours ( pair ) \u201d mean that we fitted the linear classifier in a validation partition of the OOD validation set and tested in another partition of the OOD , while \u201c Ours \u201d means that we fitted both the parameters and the threshold to one OOD validation set ( the combined validation partitions of TinyImageNet ( c ) and Gaussian noise ) and tested on all other OOD dataset \u2019 s test partition . Q6 ) \u201c Moreover , reporting columns all with 100 % is not a good practice , it seems to be a stunt to impress the reader , while not carrying much in term of content and understanding. \u201d The results in this table follow the standard protocol in the OOD detection literature , by setting the threshold to 95 % TPR [ Hendrycks2017 , Liang2018 , Lee2018a , Lee2018b ] . We added an explicit comment explaining this . In other words , we are only picking one point in the ROC curve that . Please refer to our answer to Q1 where we show that the 100 % values stem from this arbitrary threshold . Q7 ) `` In Table 8 , I do not understand what the values in parenthesis means . '' Thanks for pointing this out . In fact , we realized we this was n't enough stressed in the manuscript . .They correspond to the standard deviation after running each experiment four times ( each one on a different backbone model trained from scratch on the ID dataset ) to understand how the results varied across different trained models . Note that this is not common in literature where only one run is reported . Q8 ) \u201c Another element : why for training the regressor , the IN and OOD samples are not selected from their corresponding training sets instead of splitting their test sets to a validation and test sets ? \u201d To do a fair comparison , we follow the same data split as previous works . Some experiments that we did internally indicate that the feature distribution of training and validation samples is essentially the same . Just to state some numbers , for a DenseNet BC 100-12 model using CIFAR-100 as ID model , we trained the logistic regressor using 1000 arbitrary samples from the training set ( i.e. , 1000 samples chosen randomly over 50000 images ) + 2000 validation samples from OOD datasets ( TinyImageNet crop + Gaussian ) . We ran this experiments 5 times and we got the results displayed below as mean ( std ) calculated over the runs : OOD dataset | TNR @ 95 TPR TinyImageNet ( c ) | 100.0 ( 0.0 ) TinyImageNet ( r ) | 93.6 ( 2.9 ) LSUN ( c ) | 100.0 ( 0.0 ) LSUN ( r ) | 97.8 ( 2.1 ) iSUN | 95.9 ( 2.8 ) Uniform | 100.0 ( 0.0 ) Gaussian | 100.0 ( 0.0 ) Which is a similar result ( given the variance ) to the one we reported in Table 7 ( using the 1000 validation samples ) . Best , The authors ."}, "2": {"review_id": "rkgpCoRctm-2", "review_text": "There has been recent interest in using statistics and information summary measures to evaluate what deep nets are trying to do. Following the line of work, the paper suggests to use mean and variance of Z-scores accumulated across all layers/channels as features to distinguish ID and OOD samples. Simple idea but needs some work in its current format. Firstly, the bulk of content in Sections 2 and 3 can be reduced/shortened since the importance of normalized statistics to understand learning models is well known, and not novel. 1) The choice of datasets/netowrks needs to be understood here. How is the OOD summary changing as more layers are added into computing the score (since the score is basically averaging all layers'/channels contribution)? 2) What happens if we split the ID itself into two datasets and train on one, while using the other as OOD? 3) (r) is random and (c) is not is it for the TinyImages? Seems to be the other way around. 4) What is the influence of the dataset? Since the summaries are first order statistics, there can be significant dependance of the 'coverage' of training data (i.e., how many and how good of instances are present for each class)? This is purely a sampling problem and it may reciprocate in the OOD scores (back to first order statistics). This needs to be tested. 5) Statistical tests of significance needs to be reported for the performance summaries shown in the Tables. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We revisited sections 2 and 3 to make sure they are as concise as possible . 1.The ID/OOD datasets , as well as the networks ( WRN 28-10 , DenseNet BC 100-12 ) , were chosen to be able to compare our results with the state-of-the-art . Figure 7 ( in the Appendix ) shows how the TNR changes as more layers are added to the logistic regressor . For the WRN 28-10 , using only \u2153 of the layers gave us already more than 90 % of TNR @ 95 % of TPR , and adding more layers ( rightmost bins ) is really helpful to the OOD-ness score . 2.In our experiments , we always split the ID dataset between training ( used for training the backbone ) , validation ( used for training the logistic regressor ) and test partitions ( used for testing our OOD detector , unseen by both backbone and logistic regressor ) , and the samples from the ID dataset are correctly classified as such ( see Table 6 , 7 , 11 ) . We also tested training the logistic regressor with arbitrary samples from the training dataset ( Reviewer 2 , Q8 answer ) , and the results did not change , indicating that the features do not change significantly between the partitions . Also , the tSNE plot ( Figure 3 ) shows that ID/OOD samples are different in our proposed feature space . We hope this answers your doubt . 3 . ( r ) stands for resize , while ( c ) stands for crop . 4.To address your concern , we now show on new Table 6 the results of using the summaries from one dataset to evaluate on another one . 5.We trained 4 models for each network architecture ( each one initialized with a different random seed ) and all results reported consider the standard deviation of the results ( through averaging , median , and other relevant statistics ) . In the revised manuscript , we specify the notation in each table . We would like to note that none of the previous works reported results considering different realizations of each backbone model . Bests , Authors ."}}