{"year": "2021", "forum": "avBunqDXFS", "title": "Memory-Efficient Semi-Supervised Continual Learning: The World is its Own Replay Buffer", "decision": "Reject", "meta_review": "This paper proposes a semi-supervised setting to reduce memory budget in replay-based continual learning.\nIt uses unlabeled data in the environment for replaying which requires no storage, and generates pseudo-labels where unlabeled data is connected to labeled one.\nThe method was validated on the proposed tasks.\n\nPros:\n- The semi-supervised continual learning setting is novel and interesting.\n- The proposed approach is memory efficient, since it does not need exemplars to replay past tasks.\n\nCons:\n- The scale of experiment is small. It lacks evaluation in real world environment.\n- The novelty is limited, because it is a combination of existing technologies: pseudo-labeling, consistency regularization, Out-of-Distribution (OoD) detection, and knowledge distillation.\n- The comparison might not be fair due to different settings.\n\nThe authors addressed the fairness and scalability with additional experiments\nand leave some suggestions of reviewers for future work.\nR3 had a concern on the error propagation of pseudo-labels which I also share. The authors agreed that this is a challenge for all CL methods.\n\nIn summary, the reviews are mixed. All reviewers agree that the semi-supervised continual learning setting is novel and interesting, and some have concerns on scalability and novelty of the method which I also share. So at present time I believe there is much room for the authors to improve their method and experiments before publication.\n", "reviews": [{"review_id": "avBunqDXFS-0", "review_text": "This paper comes up with a novel scenario where the unlabled data are available as well as labeled data in the continual learning scenario . # # # Overall - Based on my understanding , the major contribution is the proposal of a task scenario , aka , experimental setting . The novelty of DistillMatch is an incremental modification of previous work . - The task setting sidesteps the learning with non-stationarity problem than solving it . - Further , this setting potentially makes the task easier for the proposed method . To verify whether this is true , more information are needed . - The presentation of the paper needs polishing , I listed a few points below . # # # Pros - The novel scenario of semisupervised continual learning is proposed . The argument is that in several realistic scenarios old data are often re-observed without label ( The funiture labeling example ) . Therefore instead of storing a coreset , one may make use of the unlabeled data for pseudo-rehearsal/distillation . It is reasonable to make use of it when this assumption is true . - With the setting the author proposed , the DistillMatch method is able to perform better than previous methods . # # # Cons 1 . The novelty mostly comes from the task scenario , the DistillMatch method is incremental . 2.Although SSCL is a new scenario , and the author argues it is more realistic . IMO taking this assumption sidesteps the problem of continual learning rather than solving it . The central problem of continual learning IMO is to learn under non-stationary distribution , the assumption made in this submission makes the distribution more stationary . 3.It is true that this assumption should be utilized when available . However , the only dataset used is manually constructed from CIFAR100 , contradicting the initial motivation to move towards a more realistic scenario . 4.There 's a lack of information on how the compared methods are adapted to the new scenario . I searched the supplementary but failed to find a detailed documentation . With the given information , it is hard to tell whether the comparison is fair . My concerns are following , * * increasing from 3 - > 4 as this point is resolved in the rebuttal * * - In the RandomClasses setting , it is stated that no coreset is used , if the compared methods depends on coreset to replay , it would be unfair . If that 's the case , the only conclusion we can draw is that replay is better than no replay , which seems trivial to me . - GD depends on internet crawled data , is it replaced with the unlabeled data since it is available in the experiment setting ? If not , then I think it is just the setting that favors DistillMatch . - With the above said , I suggest the author to list clearly the objectives , replay buffer sizes or even pseudo code for each of the compared method and their own method in a table , which will help the reader identify what major component in the proposed method is making the contribution . Regarding the quality and clarity , I found myself confused and making guesses sometimes while reading it . To list a few : - introduction paragraph 2 , ... to determine which unlabeled data is relevant to the incremental task ... , I guess the incremental task means learning the newly observed data , but then for rehearsal we 'll pick the unlabeled data which is from the distribution of past tasks . - section 1 , ... save up to 0.23 stored images per processed image over naive rehearsal ( compared to Lee ) ... , here seems Lee et al is the naive rehearsal . But then `` which only saved 0.08 '' confuses me , seems to be saying Lee saves 0.08 compared to naive rehearsal . - section 3 , ... where data distributions reflect object class correlations between , and among , the labeled and unlabeled data distributions ... not enough information to infer what `` reflect '' and `` object class correlation '' means here . - section 4 , ... Let S_ { n-1 } denote the score of our OoD detector for valid classes of our pseudo-label model ... what is the `` valid classes '' needs to be clarified . As I understand it , S_ { n-1 } measures how likely the unlabeled data is in the distribution of past tasks . - Super class / Parent class are not defined clear enough .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the constructive comments . We have replied to your points below . ( 1 ) \u201c The novelty mostly comes from the task scenario , the DistillMatch method is incremental. \u201d * * Response * * : We agree that a significant component of our contribution is the proposed setting . From a methodological viewpoint , we contribute the following : we are the first to explore hard knowledge distillation in the continual learning setting ( i.e.using one-hot label supervision from a teacher ) ; first to apply Out-of-Distribution ( OoD ) detection in the continual learning setting ( which requires a non-intuitive training and calibration strategy described in Section 4 ) ; first to use unlabeled data to balance the classifier in this setting ; state-of-the-art performance on all baselines . ( 2+3 ) \u201c Although SSCL is a new scenario , and the author argues it is more realistic . IMO taking this assumption sidesteps the problem of continual learning rather than solving it . The central problem of continual learning IMO is to learn under non-stationary distribution , the assumption made in this submission makes the distribution more stationary\u2026 It is true that this assumption should be utilized when available . However , the only dataset used is manually constructed from CIFAR100 , contradicting the initial motivation to move towards a more realistic scenario. \u201d * * Response * * : Our setting is a realistic realization of the continual learning problem ; the motivation for the SSCL method is not to avoid the challenge of continual learning but instead to propose that a slight modification to the problem , SSCL , is more representative of the real-world applications associated with continual learning while remaining challenging . In the real world , where a continual agent \u2019 s learning task is a product of its environment , there is an intrinsic correlation between this learning task and the existing unlabeled datastream due to the underlying structure of the environment . We explore the case where the unlabeled data is non-stationary , as Table 2.a explores several realistic non-stationary unlabeled data sequences . We show that prior methods work well only under some of these distributions , and we develop a method that can work across many different types of unlabeled distributions . The aspect of realism that we focus on in the super-class experiments is a task order that is not uniform random , and that as such an unknown correlation exists between the continual learner \u2019 s labeled and unlabeled data . In this way we acknowledge that the SSCL setting is not literally the real world , but instead a more realistic continual learning setting that , by design , can be studied with any conventional dataset . We chose to rigorously explore CIFAR-100 because the super-class structure , key to our realistic setting , is well defined , allowing us to design and evaluate a method in the research setting . ( 4 ) \u201c there 's a lack of information on how the compared methods are adapted to the new scenario . I searched the supplementary but failed to find a detailed documentation . With the given information , it is hard to tell whether the comparison is fair. \u201d * * Response * * : The compared methods are adapted to use unlabeled data for their respective distillation losses . We have highlighted this in the newly added Table 8 ( Supplementary I ) . Note that E2E , DR , and GD all use both coreset ( if available ) and unlabeled data from the environment , so the comparison is fair . ( 5 ) \u201c In the RandomClasses setting , it is stated that no coreset is used , if the compared methods depends on coreset to replay , it would be unfair . If that 's the case , the only conclusion we can draw is that replay is better than no replay , which seems trivial to me. \u201d * * Response * * : The compared methods ( GD , DR , E2E ) leverage unlabeled data for their respective distillation losses , just like our method , so the comparison is fair . The poor performance reflects that the distillation losses for these methods can not take full advantage of the unlabeled data , indicating the need for our method . We do coreset experiments , and they show that while coresets are important they are not singularly important , as the prior work baselines outperform naive rehearsal , or replay , of the coreset , and our method outperforms naive rehearsal and all prior work baselines ."}, {"review_id": "avBunqDXFS-1", "review_text": "- Summary : This paper proposes class-incremental learning with unlabeled data correlated to labeled data , and a method to tackle it . The task can be considered as a variant of [ Lee et al . ] , which has no assumption on the unlabeled dataset , while this paper assumes the correlation between labeled and unlabeled dataset explicitly . The proposed method is inspired by state-of-the-art class-incremental learning , semi-supervised learning , and out-of-distribution ( OoD ) detection methods : local distillation [ Li and Hoiem ] , OoD detection [ Hsu et al . ] , consistency regularization and pseudo labeling ( or hard distillation ) [ Sohn et al . ] , and loss balancing based on class statistics [ Lee et al . ] . Experimental results support that the proposed method outperforms prior works in the proposed task . - Reasons for score : 1 . Extending continual learning to the semi-supervised setting is natural , given that the extension to self-taught learning has already been considered in [ Lee et al . ] . However , I can not agree that semi-supervised learning is more realistic than self-taught learning , which is emphasized throughout the paper 18 times . In an early work of [ Raina et al . ] , self-taught learning is proposed to make the scenario of learning with unlabeled data `` widely applicable to many practical learning problems . '' [ Oliver et al . ] also argued that `` ( unlabeled data from out-of-distribution ) violates the strict definition of semi-supervised learning , but it nevertheless represents a common use-case for semi-supervised learning ( for example , augmenting a face recognition dataset with unlabeled images of people not in the labeled set ) . '' I am not saying that semi-supervised learning is unrealistic , but the argument in this paper sounds overclaimed . I believe both semi-supervised and self-taught learning are realistic in some cases . I also recommend to provide real world scenarios that the proposed task ( correlation between labeled and unlabeled data exists and no memory for coreset is available ) is useful in practice . 2.The proposed method is not novel , which is essentially the combination of state-of-the-art methods in relevant tasks . But I do not discount this much , because this work would be valuable as the proposed task is interesting but not investigated before . However , the name of task might need to be changed , because a similar name , `` semi-supervised incremental learning '' is already taken by a kind of semi-supervised learning , which incrementally incorporates unlabeled data to training . 3.Though the improvement over prior class-incremental learning methods is impressive , the overall performance is still too low . In fact , the scale of the experimental setting is too small , so I doubt it is scalable . All experiments are bounded on CIFAR-100 , and even only 20 % of training data are used as labeled one . Frankly , in this small-scale setting ( in both number of data and image resolution ) , keeping all data is just fine , as the coreset size is negligible compared to the model size . I recommend to experiment in large-scale settings , e.g. , on ImageNet . Also , I recommend to compare the oracle setting as well , which keeps all previous training data . 4.In addition to small-scale experimental setting , the architecture is larger than the prior work [ Lee et al . ] : WRN-28-2 vs. WRN-16-2 . In the worst case scenario , it is possible that the best performance of the proposed method is simply from the complexity of their learning objective , i.e. , all methods overfit to training data , but the proposed method did not have enough updates to overfit to them . 5.In Figure 3 , why do GD and DM not have a coreset ? I think there is no reason to give an unfair constraint to them . I recommend to draw curves with respect to increasing number of coreset for those methods as well . 6.Could you provide results on the self-taught learning setting like [ Lee et al . ] ? It would also be interesting to see the performance of the proposed method in the setting . 7.Hyperparameter sweep results provided in Table 4 are either minimum or maximum of the range , so you could improve the performance by enlarging the range . - Minor Comments : 8 . Subscripts of theta often are dropped . Is theta equal to $ \\theta_ { n,1 : n } $ ? 9 . `` the parameters of no more than three models '' - > I believe it is four , because you need to temporarily store gradients during training . 10. $ \\hat { q } $ is not a probability vector , which makes eq . ( 2 ) mathematically do not make sense . 11.Citation format issue : you can use \\citet for noun and \\citep for adverb . 12. typo on page 5 : statoe - > state 13 . Table 4 : what is TPR here ? threshold for consistency regularization ? [ Raina et al . ] Self-taught Learning : Transfer Learning from Unlabeled Data . In ICML , 2007 . [ Li and Hoiem ] Learning without forgetting . In TPAMI , 2017 . [ Oliver et al . ] Realistic Evaluation of Deep Semi-Supervised Learning Algorithms . In NeurIPS , 2018 . [ Lee et al . ] Overcoming catastrophic forgetting with unlabeled data in the wild . In ICCV , 2019 . [ Hsu et al . ] Generalized odin : Detecting out-of-distribution image without learning from out-of-distribution data . In CVPR , 2020 . [ Sohn et al . ] Fixmatch : Simplifying semi-supervised learning with consistency and confidence . In NeurIPS , 2020 . * * After rebuttal * * I 'd like to thank authors for their efforts to address my concerns . They have addressed most of them , so I increased my score from 5 to 6 . However , there are two concerns that could n't be resolved during the rebuttal period : ( 1 ) I am still not sure if the proposed task is practical . At glance it looks realistic , but I could n't find a detailed scenario that can only be solved by the proposed task . Any real world scenario I can think of is closer to [ Lee et al . ] , which is a prior work of this paper . Authors provided an exploring robot example in the thread of responses , but I think [ Lee et al . ] fits better for the provided one . I recommend authors to find a concrete use-case in real-world applications , which can only be solved by the proposed setting ( or at least [ Lee et al . ] is not applicable ; in the revised intro , you may emphasize that there are some real-world problems that [ Lee et al . ] is not applicable but yours is ) . R1 and R4 seem to have a similar concern . ( 2 ) the scale of experiment is too small . As CIFAR-10/100 have a limited number of data for your purpose , you can borrow some data from tinyimages ( FYI , CIFAR-10/100 are a subset of 80M tinyimages ) or focus on ImageNet . I am okay with the lack of novelty on the proposed method . For a newly proposed task , I think proposing a simple and effective baseline is good enough . However , because of the two concerns above , I can not strongly agree with its acceptance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the constructive comments . We have replied to your points below . ( 1 ) \u201c Extending continual learning to the semi-supervised setting is natural , given that the extension to self-taught learning has already been considered in [ Lee et al . ] . However , I can not agree that semi-supervised learning is more realistic than self-taught learning , which is emphasized throughout the paper 18 times . In an early work of [ Raina et al . ] , self-taught learning is proposed to make the scenario of learning with unlabeled data `` widely applicable to many practical learning problems . '' [ Oliver et al . ] also argued that `` ( unlabeled data from out-of-distribution ) violates the strict definition of semi-supervised learning , but it nevertheless represents a common use-case for semi-supervised learning ( for example , augmenting a face recognition dataset with unlabeled images of people not in the labeled set ) . '' I am not saying that semi-supervised learning is unrealistic , but the argument in this paper sounds overclaimed . I believe both semi-supervised and self-taught learning are realistic in some cases . I also recommend to provide real world scenarios that the proposed task ( correlation between labeled and unlabeled data exists and no memory for coreset is available ) is useful in practice. \u201d * * Response * * : We address the comment on self-taught learning below in ( 6 ) . Practical examples which reflect our realistic SSCL setting include robotics which continually explore the world and have memory constraints or protected data which can not be stored for legal reasons . We are not arguing that semi-supervised learning is more realistic than self-taught learning , this is a mistake of terminology and we apologize for the confusion . Like Lee et . al.we are closer in spirit to self-taught learning as each task \u2019 s unlabeled data is not drawn from the same distribution as the task \u2019 s label data . ( 2 ) \u201c The proposed method is not novel , which is essentially the combination of state-of-the-art methods in relevant tasks . But I do not discount this much , because this work would be valuable as the proposed task is interesting but not investigated before . However , the name of task might need to be changed , because a similar name , `` semi-supervised incremental learning '' is already taken by a kind of semi-supervised learning , which incrementally incorporates unlabeled data to training. \u201d * * Response * * : We agree that a significant component of our contribution is the proposed setting . From a methodological viewpoint , we contribute the following : we are the first to explore hard knowledge distillation in the continual learning setting ( i.e.using one-hot label supervision from a teacher ) ; first to apply Out-of-Distribution ( OoD ) detection in the continual learning setting ( which requires a non-intuitive training and calibration strategy described in Section 4 ) ; first to use unlabeled data to balance the classifier in this setting ; state-of-the-art performance on all baselines . Thank you for the feedback on the name of the task , we will consider this . ( 3 ) \u201c Though the improvement over prior class-incremental learning methods is impressive , the overall performance is still too low . In fact , the scale of the experimental setting is too small , so I doubt it is scalable . All experiments are bounded on CIFAR-100 , and even only 20 % of training data are used as labeled one . Frankly , in this small-scale setting ( in both number of data and image resolution ) , keeping all data is just fine , as the coreset size is negligible compared to the model size . I recommend to experiment in large-scale settings , e.g. , on ImageNet . Also , I recommend to compare the oracle setting as well , which keeps all previous training data. \u201d * * Response * * : We use the Omega metric to report performance with respect to the upper bound ( i.e.oracle setting ) , and have added the oracle performance to the results tables , thank you for the suggestion . The oracle final accuracy is 56.7 % . We chose the CIFAR-100 dataset due to the well defined super-class structure , which we consider the most important aspect of our experiment section . It allows us to rigorously explore our underlying setting and research question ( i.e.with and without class correlations in the unlabeled set , etc . ) . Moreover CIFAR-100 is a staple both in the semi-supervised and continual learning communities ."}, {"review_id": "avBunqDXFS-2", "review_text": "This paper investigates a semi-supervised continual learning ( SSCL ) setting and proposes a new method called DistillMatch for this setting . The major contributions are : ( 1 ) The authors carefully design a realistic SSCL setting where object-object correlations between labeled and unlabeled sets are maintained through a label super-class structure . And then , they develop the DistillMatch method combining knowledge distillation , pseudo-labels , out of distribution detection , and consistency regularization . ( 2 ) They show that DistillMatch outperforms other existing methods on CIFAR-100 dataset , and ablation study results are shown also . However , there are some downsides that should be considered before its publication . ( 1 ) In abstract the authors claim that they can significantly reduce the memory budget ( of labeled training data ) by leveraging unlabeled data ( perhaps with large volume ) . This motivation seems to be contradictive . ( 2 ) From a methodological viewpoint , the proposed DistillMatch method is just a combination of existing methods ( listed as in above ) . So where is the novelty of this `` new '' method ? ( 3 ) In experiments , the chosen baseline algorithm is very weak . There are some strong baseline methods such as GEM , A-GEM , and ER . So I wonder to know the real improvements over state-of-the-art methods for continual learning . ( 4 ) The label super-class structure existed in CIFAR-100 has been used in their experiments . But this is not very common for other more realistic datasets such as miniImageNet . If there is no super-class structure , we do n't know how to apply the proposed DistillMatch method . In summary , I think this semi-supervised continual learning setting is interesting , but the proposed DistillMatch method can not persuade me that this method is a novel significant contribution to this problem . So at present time I believe there is much room for the authors to improve their method before publication .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the constructive comments and interest in our setting . We have replied to your points below . ( 1 ) \u201c In abstract the authors claim that they can significantly reduce the memory budget ( of labeled training data ) by leveraging unlabeled data ( perhaps with large volume ) . This motivation seems to be contradictive. \u201d * * Response * * : Our claim is that we significantly reduce the memory budget ( and not the computational budget ) by leveraging unlabeled data existing in the continual learning agent \u2019 s environment ( i.e.this unlabeled data is never stored , and never needs to be ) . The assumption we make is that the agent is operating in an \u201c environment \u201d containing both labeled and unlabeled data , not unlike a room in a house with some objects labeled , and unlabeled data is observed , can be used , and discarded , hence incurring no no memory cost ; this is highlighted in Fig.1 ) .While this assumption does not hold for all continual learning applications , it does hold for many important applications such as robots/agents exploring and continuously learning in an environment , and situations in which data is legally sensitive and can not be stored ( like internet content taken down by content creators ) . ( 2 ) \u201c From a methodological viewpoint , the proposed DistillMatch method is just a combination of existing methods ( listed as in above ) . So where is the novelty of this `` new '' method ? \u201d * * Response * * : We contribute the following : we are the first to explore hard knowledge distillation in the continual learning setting ( i.e.using one-hot label supervision from a teacher ) ; first to apply Out-of-Distribution ( OoD ) detection in the continual learning setting ( which requires a non-intuitive training and calibration strategy described in Section 4 ) ; first to use unlabeled data to balance the classifier in this setting ; state-of-the-art performance on all baselines . ( 3 ) \u201c In experiments , the chosen baseline algorithm is very weak . There are some strong baseline methods such as GEM , A-GEM , and ER . So I wonder to know the real improvements over state-of-the-art methods for continual learning. \u201d * * Response * * : We choose to compare to methods which best fit in our setting , which are algorithms that can mitigate catastrophic forgetting with and without a coreset ( also called \u201c replay \u201d ) and can leverage unlabeled data for fair comparisons . GD , E2E , and DR can all leverage the unlabeled data in their respective distillation losses , and GD was state-of-the-art for continual learning methods that leverage unlabeled data prior to our work . If by ER the reviewer is referring to naive rehearsal , sometimes called \u201c experience rehearsal \u201d or \u201c experience replay \u201d , we do include results for naive rehearsal , listed as \u201c Base \u201d in the experiments with a coreset . If the author is referring to the CLEAR algorithm from [ 1 ] , sometimes referred to as ER , this algorithm targets the reinforcement learning domain , which is not considered in our work . Replay-focused methods such as GEM and A-GEM can not work in experiments where no coreset is present because they rely on replay . Moreover , they have no mechanism to learn from the unlabeled data ; as such it would be an unfair comparison to GEM and A-GEM if we outperformed them using unlabeled data when they could not use any , so the comparison has no null hypothesis . Additionally , the unique contributions of replay-focused methods like GEM and A-GEM are orthogonal to our contribution and can be combined with our approach ( and the competing approaches ) for better performance . We have added a brief acknowledgement of these details in Section 5 ."}, {"review_id": "avBunqDXFS-3", "review_text": "The paper presents a novel semi-supervised continual learning ( SSCL ) setting , where labeled data is scarce and unlabeled data is plentiful . The proposed framework is built on pseudo-labeling , consistency regularization , Out-of-Distribution ( OoD ) detection , and knowledge distillation in order to reduce the catastrophic forgetting in the proposed setting . The paper is in general clear and well-written . The contributions are clearly highlighted and the proposed approach is conveniently compared with other state of the art methods , demonstrating its superiority . Positive aspects : - the definition of a realistic , semi-supervised setting for continual learning - a novel approach for continual learning in order to cope with 'catastrophic forgetting ' - the proposed approach is memory efficient , since it does not need exemplars to replay past tasks Negative aspects : - the OoD implemented in this paper rejects the unknown samples . In other words , all unknown samples are considered a single class . It would have been a plus to distinguish between several unknown classes and somehow introduce them in the framework - the lack of recabilibration step after a number of tasks ( in the case of pseudo-labeled samples ) , could lead to an undesired error propagation which is not quantified in the paper However , I have some questions : 1 . What is the relationship between the 'fi ' and 'theta ' models ( section 4 ) ? Are they completely separate or there is a relationship between them ? For instance , when 'theta ' is extended with a new task , is 'fi ' extended accordingly ? Or is 'fi ' trained off-line from the beginning ( with all tasks ) ? 2.There are some different source of errors : distilation , pseudo-labels ... Do you perform any kind of system re-calibration ? After how many tasks ? I mean , do you make a study of error propagation of pseudo-labeled data ? Or at some point do you have a human-in-the-loop to correct mis-classification ? What is the mis-classification error of pseudo-labeled samples ? 3.Do you assume that labeled and unlabeled data come from different distributions or you have a single distribution which is divided in labeled and unlabeled data at the beginning of the process ? 4.Does your scenario foresee that when learning a new task T , all the previous tasks are represented ( 1 .. T-1 ) in the unlabebeld data or only a subpart ? ( i.e.kind of selective replay ) 5 . When the number of tasks increases , the number of unlabeled data per task remains constant or is scaled accordingly ( i.e.reduced ) ? 6.Would be interesting to test your approach in a real-world scenario , i.e.robot navigation .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the constructive comments and appreciation of our novel setting , approach , and memory efficiency . We have replied to your points below . ( A ) \u201c the OoD implemented in this paper rejects the unknown samples . In other words , all unknown samples are considered a single class . It would have been a plus to distinguish between several unknown classes and somehow introduce them in the framework \u201d * * Response * * : Thank you for the suggestion . This would require a clustering-like mechanism to separate semantic categories , and maintain them over time , which is not a trivial problem . This is a great idea to explore in follow-up work . ( B ) \u201c the lack of recabilibration step after a number of tasks ( in the case of pseudo-labeled samples ) , could lead to an undesired error propagation which is not quantified in the paper \u201d * * Response * * : Thanks for pointing this out ; we can discuss this in the paper . Because pseudo-labels in task n are generated using theta_ { n-1 } , then the degradation of pseudo-labels can be quantified with the accuracy/forgetting metrics of theta . We consider this to be part of the continual learning paradigm , and argue that other soft distillation methods have a similar weakness ( where the distilling copy is presenting unreliable knowledge ) . In practice , our method is more robust to this effect ( based on the performance ) but of course not immune to it . Adding more sophisticated calibration across the tasks is an interesting area of research . ( Q1 ) \u201c What is the relationship between the 'fi ' and 'theta ' models ( section 4 ) ? Are they completely separate or there is a relationship between them ? For instance , when 'theta ' is extended with a new task , is 'fi ' extended accordingly ? Or is 'fi ' trained off-line from the beginning ( with all tasks ) ? \u201d * * Response * * : These are separate models . We train \u2018 fi \u2019 with only a subset of the training data so that the OoD detection threshold can be calibrated . The classification accuracy of \u2018 fi \u2019 would be worse compared to \u2018 theta \u2019 , but this does not matter because \u2018 fi \u2019 is only used for OoD detection . ( Q2 ) \u201c There are some different source of errors : distillation , pseudo-labels ... Do you perform any kind of system re-calibration ? After how many tasks ? I mean , do you make a study of error propagation of pseudo-labeled data ? Or at some point do you have a human-in-the-loop to correct mis-classification ? What is the mis-classification error of pseudo-labeled samples ? \u201d * * Response * * : We consider this to be a fundamental challenge part of the continual learning paradigm , wherein the forgetting is amplified through task sequences . The other methods have a similar weakness ( discussed above in B ) . Ideally in continual learning , an agent would revisit a task to recalibrate the system , but we do not explore this scenario ( and we are not aware of related works which do ) . However , these are all very interesting proposals for future work . ( Q3 ) \u201c Do you assume that labeled and unlabeled data come from different distributions or you have a single distribution which is divided in labeled and unlabeled data at the beginning of the process ? \u201d * * Response * * : We explore both scenarios in Table 2.a . Importantly , our method performs SOTA regardless of whether the distributions are the same , different , or mixed . We show that prior methods are specialized to some distributions over others , in the sense that their performance degrades under some distributions . A key contribution of our method is that we show that our method works well across many distributions ."}], "0": {"review_id": "avBunqDXFS-0", "review_text": "This paper comes up with a novel scenario where the unlabled data are available as well as labeled data in the continual learning scenario . # # # Overall - Based on my understanding , the major contribution is the proposal of a task scenario , aka , experimental setting . The novelty of DistillMatch is an incremental modification of previous work . - The task setting sidesteps the learning with non-stationarity problem than solving it . - Further , this setting potentially makes the task easier for the proposed method . To verify whether this is true , more information are needed . - The presentation of the paper needs polishing , I listed a few points below . # # # Pros - The novel scenario of semisupervised continual learning is proposed . The argument is that in several realistic scenarios old data are often re-observed without label ( The funiture labeling example ) . Therefore instead of storing a coreset , one may make use of the unlabeled data for pseudo-rehearsal/distillation . It is reasonable to make use of it when this assumption is true . - With the setting the author proposed , the DistillMatch method is able to perform better than previous methods . # # # Cons 1 . The novelty mostly comes from the task scenario , the DistillMatch method is incremental . 2.Although SSCL is a new scenario , and the author argues it is more realistic . IMO taking this assumption sidesteps the problem of continual learning rather than solving it . The central problem of continual learning IMO is to learn under non-stationary distribution , the assumption made in this submission makes the distribution more stationary . 3.It is true that this assumption should be utilized when available . However , the only dataset used is manually constructed from CIFAR100 , contradicting the initial motivation to move towards a more realistic scenario . 4.There 's a lack of information on how the compared methods are adapted to the new scenario . I searched the supplementary but failed to find a detailed documentation . With the given information , it is hard to tell whether the comparison is fair . My concerns are following , * * increasing from 3 - > 4 as this point is resolved in the rebuttal * * - In the RandomClasses setting , it is stated that no coreset is used , if the compared methods depends on coreset to replay , it would be unfair . If that 's the case , the only conclusion we can draw is that replay is better than no replay , which seems trivial to me . - GD depends on internet crawled data , is it replaced with the unlabeled data since it is available in the experiment setting ? If not , then I think it is just the setting that favors DistillMatch . - With the above said , I suggest the author to list clearly the objectives , replay buffer sizes or even pseudo code for each of the compared method and their own method in a table , which will help the reader identify what major component in the proposed method is making the contribution . Regarding the quality and clarity , I found myself confused and making guesses sometimes while reading it . To list a few : - introduction paragraph 2 , ... to determine which unlabeled data is relevant to the incremental task ... , I guess the incremental task means learning the newly observed data , but then for rehearsal we 'll pick the unlabeled data which is from the distribution of past tasks . - section 1 , ... save up to 0.23 stored images per processed image over naive rehearsal ( compared to Lee ) ... , here seems Lee et al is the naive rehearsal . But then `` which only saved 0.08 '' confuses me , seems to be saying Lee saves 0.08 compared to naive rehearsal . - section 3 , ... where data distributions reflect object class correlations between , and among , the labeled and unlabeled data distributions ... not enough information to infer what `` reflect '' and `` object class correlation '' means here . - section 4 , ... Let S_ { n-1 } denote the score of our OoD detector for valid classes of our pseudo-label model ... what is the `` valid classes '' needs to be clarified . As I understand it , S_ { n-1 } measures how likely the unlabeled data is in the distribution of past tasks . - Super class / Parent class are not defined clear enough .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the constructive comments . We have replied to your points below . ( 1 ) \u201c The novelty mostly comes from the task scenario , the DistillMatch method is incremental. \u201d * * Response * * : We agree that a significant component of our contribution is the proposed setting . From a methodological viewpoint , we contribute the following : we are the first to explore hard knowledge distillation in the continual learning setting ( i.e.using one-hot label supervision from a teacher ) ; first to apply Out-of-Distribution ( OoD ) detection in the continual learning setting ( which requires a non-intuitive training and calibration strategy described in Section 4 ) ; first to use unlabeled data to balance the classifier in this setting ; state-of-the-art performance on all baselines . ( 2+3 ) \u201c Although SSCL is a new scenario , and the author argues it is more realistic . IMO taking this assumption sidesteps the problem of continual learning rather than solving it . The central problem of continual learning IMO is to learn under non-stationary distribution , the assumption made in this submission makes the distribution more stationary\u2026 It is true that this assumption should be utilized when available . However , the only dataset used is manually constructed from CIFAR100 , contradicting the initial motivation to move towards a more realistic scenario. \u201d * * Response * * : Our setting is a realistic realization of the continual learning problem ; the motivation for the SSCL method is not to avoid the challenge of continual learning but instead to propose that a slight modification to the problem , SSCL , is more representative of the real-world applications associated with continual learning while remaining challenging . In the real world , where a continual agent \u2019 s learning task is a product of its environment , there is an intrinsic correlation between this learning task and the existing unlabeled datastream due to the underlying structure of the environment . We explore the case where the unlabeled data is non-stationary , as Table 2.a explores several realistic non-stationary unlabeled data sequences . We show that prior methods work well only under some of these distributions , and we develop a method that can work across many different types of unlabeled distributions . The aspect of realism that we focus on in the super-class experiments is a task order that is not uniform random , and that as such an unknown correlation exists between the continual learner \u2019 s labeled and unlabeled data . In this way we acknowledge that the SSCL setting is not literally the real world , but instead a more realistic continual learning setting that , by design , can be studied with any conventional dataset . We chose to rigorously explore CIFAR-100 because the super-class structure , key to our realistic setting , is well defined , allowing us to design and evaluate a method in the research setting . ( 4 ) \u201c there 's a lack of information on how the compared methods are adapted to the new scenario . I searched the supplementary but failed to find a detailed documentation . With the given information , it is hard to tell whether the comparison is fair. \u201d * * Response * * : The compared methods are adapted to use unlabeled data for their respective distillation losses . We have highlighted this in the newly added Table 8 ( Supplementary I ) . Note that E2E , DR , and GD all use both coreset ( if available ) and unlabeled data from the environment , so the comparison is fair . ( 5 ) \u201c In the RandomClasses setting , it is stated that no coreset is used , if the compared methods depends on coreset to replay , it would be unfair . If that 's the case , the only conclusion we can draw is that replay is better than no replay , which seems trivial to me. \u201d * * Response * * : The compared methods ( GD , DR , E2E ) leverage unlabeled data for their respective distillation losses , just like our method , so the comparison is fair . The poor performance reflects that the distillation losses for these methods can not take full advantage of the unlabeled data , indicating the need for our method . We do coreset experiments , and they show that while coresets are important they are not singularly important , as the prior work baselines outperform naive rehearsal , or replay , of the coreset , and our method outperforms naive rehearsal and all prior work baselines ."}, "1": {"review_id": "avBunqDXFS-1", "review_text": "- Summary : This paper proposes class-incremental learning with unlabeled data correlated to labeled data , and a method to tackle it . The task can be considered as a variant of [ Lee et al . ] , which has no assumption on the unlabeled dataset , while this paper assumes the correlation between labeled and unlabeled dataset explicitly . The proposed method is inspired by state-of-the-art class-incremental learning , semi-supervised learning , and out-of-distribution ( OoD ) detection methods : local distillation [ Li and Hoiem ] , OoD detection [ Hsu et al . ] , consistency regularization and pseudo labeling ( or hard distillation ) [ Sohn et al . ] , and loss balancing based on class statistics [ Lee et al . ] . Experimental results support that the proposed method outperforms prior works in the proposed task . - Reasons for score : 1 . Extending continual learning to the semi-supervised setting is natural , given that the extension to self-taught learning has already been considered in [ Lee et al . ] . However , I can not agree that semi-supervised learning is more realistic than self-taught learning , which is emphasized throughout the paper 18 times . In an early work of [ Raina et al . ] , self-taught learning is proposed to make the scenario of learning with unlabeled data `` widely applicable to many practical learning problems . '' [ Oliver et al . ] also argued that `` ( unlabeled data from out-of-distribution ) violates the strict definition of semi-supervised learning , but it nevertheless represents a common use-case for semi-supervised learning ( for example , augmenting a face recognition dataset with unlabeled images of people not in the labeled set ) . '' I am not saying that semi-supervised learning is unrealistic , but the argument in this paper sounds overclaimed . I believe both semi-supervised and self-taught learning are realistic in some cases . I also recommend to provide real world scenarios that the proposed task ( correlation between labeled and unlabeled data exists and no memory for coreset is available ) is useful in practice . 2.The proposed method is not novel , which is essentially the combination of state-of-the-art methods in relevant tasks . But I do not discount this much , because this work would be valuable as the proposed task is interesting but not investigated before . However , the name of task might need to be changed , because a similar name , `` semi-supervised incremental learning '' is already taken by a kind of semi-supervised learning , which incrementally incorporates unlabeled data to training . 3.Though the improvement over prior class-incremental learning methods is impressive , the overall performance is still too low . In fact , the scale of the experimental setting is too small , so I doubt it is scalable . All experiments are bounded on CIFAR-100 , and even only 20 % of training data are used as labeled one . Frankly , in this small-scale setting ( in both number of data and image resolution ) , keeping all data is just fine , as the coreset size is negligible compared to the model size . I recommend to experiment in large-scale settings , e.g. , on ImageNet . Also , I recommend to compare the oracle setting as well , which keeps all previous training data . 4.In addition to small-scale experimental setting , the architecture is larger than the prior work [ Lee et al . ] : WRN-28-2 vs. WRN-16-2 . In the worst case scenario , it is possible that the best performance of the proposed method is simply from the complexity of their learning objective , i.e. , all methods overfit to training data , but the proposed method did not have enough updates to overfit to them . 5.In Figure 3 , why do GD and DM not have a coreset ? I think there is no reason to give an unfair constraint to them . I recommend to draw curves with respect to increasing number of coreset for those methods as well . 6.Could you provide results on the self-taught learning setting like [ Lee et al . ] ? It would also be interesting to see the performance of the proposed method in the setting . 7.Hyperparameter sweep results provided in Table 4 are either minimum or maximum of the range , so you could improve the performance by enlarging the range . - Minor Comments : 8 . Subscripts of theta often are dropped . Is theta equal to $ \\theta_ { n,1 : n } $ ? 9 . `` the parameters of no more than three models '' - > I believe it is four , because you need to temporarily store gradients during training . 10. $ \\hat { q } $ is not a probability vector , which makes eq . ( 2 ) mathematically do not make sense . 11.Citation format issue : you can use \\citet for noun and \\citep for adverb . 12. typo on page 5 : statoe - > state 13 . Table 4 : what is TPR here ? threshold for consistency regularization ? [ Raina et al . ] Self-taught Learning : Transfer Learning from Unlabeled Data . In ICML , 2007 . [ Li and Hoiem ] Learning without forgetting . In TPAMI , 2017 . [ Oliver et al . ] Realistic Evaluation of Deep Semi-Supervised Learning Algorithms . In NeurIPS , 2018 . [ Lee et al . ] Overcoming catastrophic forgetting with unlabeled data in the wild . In ICCV , 2019 . [ Hsu et al . ] Generalized odin : Detecting out-of-distribution image without learning from out-of-distribution data . In CVPR , 2020 . [ Sohn et al . ] Fixmatch : Simplifying semi-supervised learning with consistency and confidence . In NeurIPS , 2020 . * * After rebuttal * * I 'd like to thank authors for their efforts to address my concerns . They have addressed most of them , so I increased my score from 5 to 6 . However , there are two concerns that could n't be resolved during the rebuttal period : ( 1 ) I am still not sure if the proposed task is practical . At glance it looks realistic , but I could n't find a detailed scenario that can only be solved by the proposed task . Any real world scenario I can think of is closer to [ Lee et al . ] , which is a prior work of this paper . Authors provided an exploring robot example in the thread of responses , but I think [ Lee et al . ] fits better for the provided one . I recommend authors to find a concrete use-case in real-world applications , which can only be solved by the proposed setting ( or at least [ Lee et al . ] is not applicable ; in the revised intro , you may emphasize that there are some real-world problems that [ Lee et al . ] is not applicable but yours is ) . R1 and R4 seem to have a similar concern . ( 2 ) the scale of experiment is too small . As CIFAR-10/100 have a limited number of data for your purpose , you can borrow some data from tinyimages ( FYI , CIFAR-10/100 are a subset of 80M tinyimages ) or focus on ImageNet . I am okay with the lack of novelty on the proposed method . For a newly proposed task , I think proposing a simple and effective baseline is good enough . However , because of the two concerns above , I can not strongly agree with its acceptance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the constructive comments . We have replied to your points below . ( 1 ) \u201c Extending continual learning to the semi-supervised setting is natural , given that the extension to self-taught learning has already been considered in [ Lee et al . ] . However , I can not agree that semi-supervised learning is more realistic than self-taught learning , which is emphasized throughout the paper 18 times . In an early work of [ Raina et al . ] , self-taught learning is proposed to make the scenario of learning with unlabeled data `` widely applicable to many practical learning problems . '' [ Oliver et al . ] also argued that `` ( unlabeled data from out-of-distribution ) violates the strict definition of semi-supervised learning , but it nevertheless represents a common use-case for semi-supervised learning ( for example , augmenting a face recognition dataset with unlabeled images of people not in the labeled set ) . '' I am not saying that semi-supervised learning is unrealistic , but the argument in this paper sounds overclaimed . I believe both semi-supervised and self-taught learning are realistic in some cases . I also recommend to provide real world scenarios that the proposed task ( correlation between labeled and unlabeled data exists and no memory for coreset is available ) is useful in practice. \u201d * * Response * * : We address the comment on self-taught learning below in ( 6 ) . Practical examples which reflect our realistic SSCL setting include robotics which continually explore the world and have memory constraints or protected data which can not be stored for legal reasons . We are not arguing that semi-supervised learning is more realistic than self-taught learning , this is a mistake of terminology and we apologize for the confusion . Like Lee et . al.we are closer in spirit to self-taught learning as each task \u2019 s unlabeled data is not drawn from the same distribution as the task \u2019 s label data . ( 2 ) \u201c The proposed method is not novel , which is essentially the combination of state-of-the-art methods in relevant tasks . But I do not discount this much , because this work would be valuable as the proposed task is interesting but not investigated before . However , the name of task might need to be changed , because a similar name , `` semi-supervised incremental learning '' is already taken by a kind of semi-supervised learning , which incrementally incorporates unlabeled data to training. \u201d * * Response * * : We agree that a significant component of our contribution is the proposed setting . From a methodological viewpoint , we contribute the following : we are the first to explore hard knowledge distillation in the continual learning setting ( i.e.using one-hot label supervision from a teacher ) ; first to apply Out-of-Distribution ( OoD ) detection in the continual learning setting ( which requires a non-intuitive training and calibration strategy described in Section 4 ) ; first to use unlabeled data to balance the classifier in this setting ; state-of-the-art performance on all baselines . Thank you for the feedback on the name of the task , we will consider this . ( 3 ) \u201c Though the improvement over prior class-incremental learning methods is impressive , the overall performance is still too low . In fact , the scale of the experimental setting is too small , so I doubt it is scalable . All experiments are bounded on CIFAR-100 , and even only 20 % of training data are used as labeled one . Frankly , in this small-scale setting ( in both number of data and image resolution ) , keeping all data is just fine , as the coreset size is negligible compared to the model size . I recommend to experiment in large-scale settings , e.g. , on ImageNet . Also , I recommend to compare the oracle setting as well , which keeps all previous training data. \u201d * * Response * * : We use the Omega metric to report performance with respect to the upper bound ( i.e.oracle setting ) , and have added the oracle performance to the results tables , thank you for the suggestion . The oracle final accuracy is 56.7 % . We chose the CIFAR-100 dataset due to the well defined super-class structure , which we consider the most important aspect of our experiment section . It allows us to rigorously explore our underlying setting and research question ( i.e.with and without class correlations in the unlabeled set , etc . ) . Moreover CIFAR-100 is a staple both in the semi-supervised and continual learning communities ."}, "2": {"review_id": "avBunqDXFS-2", "review_text": "This paper investigates a semi-supervised continual learning ( SSCL ) setting and proposes a new method called DistillMatch for this setting . The major contributions are : ( 1 ) The authors carefully design a realistic SSCL setting where object-object correlations between labeled and unlabeled sets are maintained through a label super-class structure . And then , they develop the DistillMatch method combining knowledge distillation , pseudo-labels , out of distribution detection , and consistency regularization . ( 2 ) They show that DistillMatch outperforms other existing methods on CIFAR-100 dataset , and ablation study results are shown also . However , there are some downsides that should be considered before its publication . ( 1 ) In abstract the authors claim that they can significantly reduce the memory budget ( of labeled training data ) by leveraging unlabeled data ( perhaps with large volume ) . This motivation seems to be contradictive . ( 2 ) From a methodological viewpoint , the proposed DistillMatch method is just a combination of existing methods ( listed as in above ) . So where is the novelty of this `` new '' method ? ( 3 ) In experiments , the chosen baseline algorithm is very weak . There are some strong baseline methods such as GEM , A-GEM , and ER . So I wonder to know the real improvements over state-of-the-art methods for continual learning . ( 4 ) The label super-class structure existed in CIFAR-100 has been used in their experiments . But this is not very common for other more realistic datasets such as miniImageNet . If there is no super-class structure , we do n't know how to apply the proposed DistillMatch method . In summary , I think this semi-supervised continual learning setting is interesting , but the proposed DistillMatch method can not persuade me that this method is a novel significant contribution to this problem . So at present time I believe there is much room for the authors to improve their method before publication .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the constructive comments and interest in our setting . We have replied to your points below . ( 1 ) \u201c In abstract the authors claim that they can significantly reduce the memory budget ( of labeled training data ) by leveraging unlabeled data ( perhaps with large volume ) . This motivation seems to be contradictive. \u201d * * Response * * : Our claim is that we significantly reduce the memory budget ( and not the computational budget ) by leveraging unlabeled data existing in the continual learning agent \u2019 s environment ( i.e.this unlabeled data is never stored , and never needs to be ) . The assumption we make is that the agent is operating in an \u201c environment \u201d containing both labeled and unlabeled data , not unlike a room in a house with some objects labeled , and unlabeled data is observed , can be used , and discarded , hence incurring no no memory cost ; this is highlighted in Fig.1 ) .While this assumption does not hold for all continual learning applications , it does hold for many important applications such as robots/agents exploring and continuously learning in an environment , and situations in which data is legally sensitive and can not be stored ( like internet content taken down by content creators ) . ( 2 ) \u201c From a methodological viewpoint , the proposed DistillMatch method is just a combination of existing methods ( listed as in above ) . So where is the novelty of this `` new '' method ? \u201d * * Response * * : We contribute the following : we are the first to explore hard knowledge distillation in the continual learning setting ( i.e.using one-hot label supervision from a teacher ) ; first to apply Out-of-Distribution ( OoD ) detection in the continual learning setting ( which requires a non-intuitive training and calibration strategy described in Section 4 ) ; first to use unlabeled data to balance the classifier in this setting ; state-of-the-art performance on all baselines . ( 3 ) \u201c In experiments , the chosen baseline algorithm is very weak . There are some strong baseline methods such as GEM , A-GEM , and ER . So I wonder to know the real improvements over state-of-the-art methods for continual learning. \u201d * * Response * * : We choose to compare to methods which best fit in our setting , which are algorithms that can mitigate catastrophic forgetting with and without a coreset ( also called \u201c replay \u201d ) and can leverage unlabeled data for fair comparisons . GD , E2E , and DR can all leverage the unlabeled data in their respective distillation losses , and GD was state-of-the-art for continual learning methods that leverage unlabeled data prior to our work . If by ER the reviewer is referring to naive rehearsal , sometimes called \u201c experience rehearsal \u201d or \u201c experience replay \u201d , we do include results for naive rehearsal , listed as \u201c Base \u201d in the experiments with a coreset . If the author is referring to the CLEAR algorithm from [ 1 ] , sometimes referred to as ER , this algorithm targets the reinforcement learning domain , which is not considered in our work . Replay-focused methods such as GEM and A-GEM can not work in experiments where no coreset is present because they rely on replay . Moreover , they have no mechanism to learn from the unlabeled data ; as such it would be an unfair comparison to GEM and A-GEM if we outperformed them using unlabeled data when they could not use any , so the comparison has no null hypothesis . Additionally , the unique contributions of replay-focused methods like GEM and A-GEM are orthogonal to our contribution and can be combined with our approach ( and the competing approaches ) for better performance . We have added a brief acknowledgement of these details in Section 5 ."}, "3": {"review_id": "avBunqDXFS-3", "review_text": "The paper presents a novel semi-supervised continual learning ( SSCL ) setting , where labeled data is scarce and unlabeled data is plentiful . The proposed framework is built on pseudo-labeling , consistency regularization , Out-of-Distribution ( OoD ) detection , and knowledge distillation in order to reduce the catastrophic forgetting in the proposed setting . The paper is in general clear and well-written . The contributions are clearly highlighted and the proposed approach is conveniently compared with other state of the art methods , demonstrating its superiority . Positive aspects : - the definition of a realistic , semi-supervised setting for continual learning - a novel approach for continual learning in order to cope with 'catastrophic forgetting ' - the proposed approach is memory efficient , since it does not need exemplars to replay past tasks Negative aspects : - the OoD implemented in this paper rejects the unknown samples . In other words , all unknown samples are considered a single class . It would have been a plus to distinguish between several unknown classes and somehow introduce them in the framework - the lack of recabilibration step after a number of tasks ( in the case of pseudo-labeled samples ) , could lead to an undesired error propagation which is not quantified in the paper However , I have some questions : 1 . What is the relationship between the 'fi ' and 'theta ' models ( section 4 ) ? Are they completely separate or there is a relationship between them ? For instance , when 'theta ' is extended with a new task , is 'fi ' extended accordingly ? Or is 'fi ' trained off-line from the beginning ( with all tasks ) ? 2.There are some different source of errors : distilation , pseudo-labels ... Do you perform any kind of system re-calibration ? After how many tasks ? I mean , do you make a study of error propagation of pseudo-labeled data ? Or at some point do you have a human-in-the-loop to correct mis-classification ? What is the mis-classification error of pseudo-labeled samples ? 3.Do you assume that labeled and unlabeled data come from different distributions or you have a single distribution which is divided in labeled and unlabeled data at the beginning of the process ? 4.Does your scenario foresee that when learning a new task T , all the previous tasks are represented ( 1 .. T-1 ) in the unlabebeld data or only a subpart ? ( i.e.kind of selective replay ) 5 . When the number of tasks increases , the number of unlabeled data per task remains constant or is scaled accordingly ( i.e.reduced ) ? 6.Would be interesting to test your approach in a real-world scenario , i.e.robot navigation .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the constructive comments and appreciation of our novel setting , approach , and memory efficiency . We have replied to your points below . ( A ) \u201c the OoD implemented in this paper rejects the unknown samples . In other words , all unknown samples are considered a single class . It would have been a plus to distinguish between several unknown classes and somehow introduce them in the framework \u201d * * Response * * : Thank you for the suggestion . This would require a clustering-like mechanism to separate semantic categories , and maintain them over time , which is not a trivial problem . This is a great idea to explore in follow-up work . ( B ) \u201c the lack of recabilibration step after a number of tasks ( in the case of pseudo-labeled samples ) , could lead to an undesired error propagation which is not quantified in the paper \u201d * * Response * * : Thanks for pointing this out ; we can discuss this in the paper . Because pseudo-labels in task n are generated using theta_ { n-1 } , then the degradation of pseudo-labels can be quantified with the accuracy/forgetting metrics of theta . We consider this to be part of the continual learning paradigm , and argue that other soft distillation methods have a similar weakness ( where the distilling copy is presenting unreliable knowledge ) . In practice , our method is more robust to this effect ( based on the performance ) but of course not immune to it . Adding more sophisticated calibration across the tasks is an interesting area of research . ( Q1 ) \u201c What is the relationship between the 'fi ' and 'theta ' models ( section 4 ) ? Are they completely separate or there is a relationship between them ? For instance , when 'theta ' is extended with a new task , is 'fi ' extended accordingly ? Or is 'fi ' trained off-line from the beginning ( with all tasks ) ? \u201d * * Response * * : These are separate models . We train \u2018 fi \u2019 with only a subset of the training data so that the OoD detection threshold can be calibrated . The classification accuracy of \u2018 fi \u2019 would be worse compared to \u2018 theta \u2019 , but this does not matter because \u2018 fi \u2019 is only used for OoD detection . ( Q2 ) \u201c There are some different source of errors : distillation , pseudo-labels ... Do you perform any kind of system re-calibration ? After how many tasks ? I mean , do you make a study of error propagation of pseudo-labeled data ? Or at some point do you have a human-in-the-loop to correct mis-classification ? What is the mis-classification error of pseudo-labeled samples ? \u201d * * Response * * : We consider this to be a fundamental challenge part of the continual learning paradigm , wherein the forgetting is amplified through task sequences . The other methods have a similar weakness ( discussed above in B ) . Ideally in continual learning , an agent would revisit a task to recalibrate the system , but we do not explore this scenario ( and we are not aware of related works which do ) . However , these are all very interesting proposals for future work . ( Q3 ) \u201c Do you assume that labeled and unlabeled data come from different distributions or you have a single distribution which is divided in labeled and unlabeled data at the beginning of the process ? \u201d * * Response * * : We explore both scenarios in Table 2.a . Importantly , our method performs SOTA regardless of whether the distributions are the same , different , or mixed . We show that prior methods are specialized to some distributions over others , in the sense that their performance degrades under some distributions . A key contribution of our method is that we show that our method works well across many distributions ."}}