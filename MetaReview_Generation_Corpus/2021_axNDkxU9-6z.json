{"year": "2021", "forum": "axNDkxU9-6z", "title": "MDP Playground: Controlling Orthogonal Dimensions of Hardness in Toy Environments", "decision": "Reject", "meta_review": "I thank the authors for their submission and very active participation in the author response period. The reviewers and I acknowledge the importance of designing toy environments that allow the community to systematically investigate strengths and weaknesses of RL approaches. That said, the reviewers have criticized that it is unclear whether experiments on the proposed toy MDPs would transfer to more complex standard RL benchmarks (such as Atari) [R1 & R2], and that the proposed metrics and axes of variation seem not well motivated or systematic [R1,R2,R3 & R4], thus casting doubts regarding what insights the community will be able to gain from experiments on MDP Playground. In particular, I agree with the reviewers R1's and R4's assessment that proposing many dimensions of variation, even if they are orthogonal, without a well formulated motivation and grounding in actual tasks the community cares about is not particularly helpful for advancing our understanding of current challenges in RL. Post rebuttal, R2 and R4 stand by their strong stance against acceptance; and R1 has increased their score as a result of the improvements of the updated paper, but they still lean towards rejection. Thus, I recommend rejection.", "reviews": [{"review_id": "axNDkxU9-6z-0", "review_text": "-- POST-REBUTTAL COMMENTS As a result of the discussion the paper has improved , so I 'm increasing my score . However , the core issue , that the proposed benchmarks do n't seem to capture the difficulty structure of either real problems or more complex benchmarks , remains . -- SUMMARY : The paper introduces MDP Playground , a parameterized suite of MDPs with low computational requirements that nonetheless present significant challenges for existing RL algorithms . The authors argue that MDP Playground is a valuable testbed for developing and evaluating ne RL algorithms . The paper also assess how the identified dimensions of hardness that can be exercised in MDP Playground transfer to more complex benchmarks . The paper also uses MDP Playground to evaluate several rllib algorithms . HIGH-LEVEL COMMENTS : This work has a lot of potential . It targets an important problem in RL research : studying the behavior of RL algorithms as an environment changes along various dimensions of hardness . It is valuable to have a benchmark suite of the sort this work aims to deliver , one that allows varying these dimensions in a controlled way and consists of problems that are simple enough for debugging . However , in its current form this work has non-trivial weaknesses in contribution and presentation that make publication at ICLR or similar conferences premature : 1 ) MDP Playground 's problems are completely unintuitive . They are randomly generated and are n't inspired by any real scenarios . This raises the question of how meaningful they are . In ML , it is understood from results such as the no-free-lunch theorem that no learner can do well across all possible tasks , and the sensible approach is to make ML algorithms work well on meaningful data distributions that are encountered in reality . Unfortunately , RL as a field has n't been good at sticking to this principle : many of its existing benchmarks , such as videogames , look `` interesting '' and difficult , but in a very different way that real decision-making scenarios are . MDP Playground exacerbates this issue -- since its MDPs are randomly generated and do n't have a natural interpretation , it 's difficult to get even an intuition for a good behavior in them , and difficult to understand how to generate problems that that have a combination of hardness along different dimensions that is representative of realistic scenarios . A case in point is the paper 's omission of MDP non-ergodicity ( and presence of constraints on the desired policy as a common real factor that causes non-ergodicity ) from its list of hardness dimensions . In reality , and even in some existing benchmarks , the learner can reach irrecoverable ( absorbing ) failure states , such as robots damaging themselves or objects they interact with , unless they are very careful . There are goal-directed MDP models with such states -- see , e.g. , reference [ a ] at the end of the review -- and learning in their presence in realistic scenarios requires costly resets -- see , e.g. , reference [ b ] . MDP Playground does n't help with researching this aspect , e.g. , by having knobs for probabilities of entering such states , penalties for doing so and the cost of recovery , despite it being ubiquitous in reality . Another major omission is the fact that in reality agent actions have durations that can make it very hard to learn a policy within a given time budget . The existing benchmark does n't allow studying the effects of action duration on learning time . Given these gaps , I doubt that MDP Playground in its current form adds much value over the existing RL benchmarks , which have some drawbacks but have interpretability as a big asset for debugging . 2 ) The paper 's analysis of dimensions of hardness is quite unprincipled , contrary to the paper 's claims . What makes many decision-making problems ( both existing ones and those introduced in MDP Playground ) hard is partial observability , but the paper never formally states what POMDPs are , nor even mentions this term . If it defined POMDPs formally , the incorrectness of some of the statements the paper makes about problems with partial observability ( see the detailed comments below ) would become obvious , and the connections between partial observability and hardness would become much clearer mathematically . See , e.g. , reference [ c ] for a formal but accessible treatment of POMDPs . TECHNICAL ISSUES/RELATED WORK : -- In the intro , the paper claims that `` partial observability [ is ] when the underlying environment is assumed to be an MDP , however , the state formulation , i.e. , the observation used by the agent is not Markovian '' . This is an extremely inaccurate statement at best . In POMDPs , observations are Markovian -- their probability depends only on the current ( hidden ) state . `` State formulation '' is also Markovian , as it is fully observable MDPs . So are the belief states . What is non-Markovian in POMDPs is the optimal policy w.r.t.the observations : an optimal policy can depend on the entire observation history . However , again , each observation history maps to a belief state , and in the belief state space the optimal policies are Markovian . -- The same goes for several other statements , e.g . `` performance degrades in environments where the delayed reward induces partial observability and hence makes the state used by the algorithm non-Markovian '' . How can delayed reward introduce partial observability ? How can partial observability make `` the state used by the algorithm '' ( an imprecise term in its own right ) non-Markovian ? -- Out of the dimensions of hardness the paper does identify , why is `` sequence length '' a distinct dimension , rather than being an instance of reward delay or reward sparsity ? -- In the related work , a notable omission is reference [ d ] , which looks at aspects that MDP hard and analyzes existing benchmarks w.r.t.those aspects . -- I was n't sure what the benefit of the `` varying reprsentations '' experiment that tries to enforce various kinds of invariances was . First of all , the experiment is about applying various types of data augmentation to images , not about varying representations . Second , the conclusion that `` this indicates that shift and other types of invariance do not come for free and that one needs to have sufficient amount of samples for the algorithm to become invariant to the transforms we desire . '' is rather obvious and well-known . -- A similar remark goes for several other experiments : varying time units , target variance , irrelevant features . Their results are completely expected , and while they might be a useful sanity check , the results description can be condensed to 1 line each and placed in the figure captions . -- A similar comment goes for the entire content of Section 4.3 as well -- it is very verbose and can be greatly condensed and structured into a short list with bullet points summraizing the findings . Typos : `` Very low cost execution '' -- > `` Very low cost of execution '' [ a ] Kolobov , Mausam , Weld , `` A Theory of Goal-Oriented MDPs with Dead Ends '' , UAI-2012 [ b ] Eysenbach , Gu , Ibarz , Levine , `` Leave no Trace : Learning to Reset for Safe and Autonomous Reinforcement Learning '' , ICLR-2018 [ c ] Kochenderfer , `` Decision Making Under Uncertainty : Theory and Application '' , MIT Press , 2015 [ d ] Maillard , Mann , Mannor `` \u201c How hard is my MDP ? \u201d The distribution-norm to the rescue '' , NIPS-2014", "rating": "5: Marginally below acceptance threshold", "reply_text": "> In ML , it is understood from results such as the no-free-lunch theorem that no learner can do well across all possible tasks , and the sensible approach is to make ML algorithms work well on meaningful data distributions that are encountered in reality . This is true for empirical research on complex environments . However , for theoretical development of algorithms , making algorithms work on meaningful data distributions that are encountered in reality may not always be a requirement . For example , Q-learning is agnostic to the structure in * P * and * R * and should work in theory for * * any * * kind of * P * and * R * . > Unfortunately , RL as a field has n't been good at sticking to this principle : many of its existing benchmarks , such as videogames , look `` interesting '' and difficult , but in a very different way that real decision-making scenarios are . > MDP Playground exacerbates this issue -- since its MDPs are randomly generated and do n't have a natural interpretation , it 's difficult to get even an intuition for a good behavior in them , and difficult to understand how to generate problems that that have a combination of hardness along different dimensions that is representative of realistic scenarios . We have made a case for how we want to help researchers identify inductive biases in 3.1 in the common response to reviewers . To do this , even random data can work as long as the data have the important features in there . Also as mentioned there , most RL algorithms that we are aware of are agnostic to the structure of * P * and * R * , so we believe it \u2019 s not wrong to test them on randomly generated * P * s and * R * s. The bsuite debugging example we have given in Appendix E.1 supports our claim . We only needed the key feature of varying reward sparsity there . > A case in point is the paper 's omission of MDP non-ergodicity ( and presence of constraints on the desired policy as a common real factor that causes non-ergodicity ) from its list of hardness dimensions . In reality , and even in some existing benchmarks , the learner can reach irrecoverable ( absorbing ) failure states , such as robots damaging themselves or objects they interact with , unless they are very careful . > MDP Playground does n't help with researching this aspect , e.g. , by having knobs for probabilities of entering such states , penalties for doing so and the cost of recovery , despite it being ubiquitous in reality . Thank you , we have a couple of responses to this . Firstly , we allow setting * terminal state density * which achieves non-ergodicity by having terminal states which only transition back to themselves . Further , we have * terminal state costs * which can also be specified , so at least in some form non-ergodicity * is * captured . Secondly , as we have said in principle ( 1 ) above , we aim to allow users to specify their own * P * and * R * and this is easily ready by the end of the rebuttal . Thirdly , we are also happy to consider adding it as a separate dimension according to our commitment ( 2 ) above . To that end , could you please point out if you \u2019 re not satisfied with the first two proposals here ? > Another major omission is the fact that in reality agent actions have durations that can make it very hard to learn a policy within a given time budget . The existing benchmark does n't allow studying the effects of action duration on learning time . We are confused by this statement . We believe from your description , that the included dimension * time unit * is exactly this feature ( please see the last paragraph of Section 2 in the paper for the relevant equation ) . It 's the time that the actions last for in the continuous environments . Could you please clarify what you believe to be missing here ? > The paper 's analysis of dimensions of hardness is quite unprincipled , contrary to the paper 's claims . We have explicitly stated our principle for selecting the dimensions here . Could you please tell us if that makes it better for you ? > In the intro , the paper claims that `` partial observability [ is ] when the underlying environment is assumed to be an MDP , however , the state formulation , i.e. , the observation used by the agent is not Markovian '' . This is an extremely inaccurate statement at best . In POMDPs , observations are Markovian -- their probability depends only on the current ( hidden ) state . `` State formulation '' is also Markovian , as it is fully observable MDPs . So are the belief states . What is non-Markovian in POMDPs is the optimal policy w.r.t.the observations : an optimal policy can depend on the entire observation history . However , again , each observation history maps to a belief state , and in the belief state space the optimal policies are Markovian . Thank you for pointing this out . We 're sorry if our statements were inaccurate . We \u2019 re happy to reframe it to be accurate for the rebuttal ."}, {"review_id": "axNDkxU9-6z-1", "review_text": "The paper describes a new benchmark for evaluating reinforcement learning techniques ( MDP-playground ) . It can be seen as a toolbox allowing to generate different MDPs with different characteristics . Each MDP will then be used to probe a particular ability of learning algorithms , resulting in a comparison of methods over multiple dimensions . Proposed dimensions are reward sparsity , stochasticity , delayed reward , etc .... In addition to this toolbox , the authors also evaluate some of the classical algorithms in the domain . == Comments First of all , the idea of evaluating RL techniques over multiple dimensions is very interesting , since right now the comparison of RL techniques is very weak , and providing simple tools in that direction is crucial to make advances in the field . The MDP-playground approach is a good approach toward this goal and proposes a large number of different metrics on both continuous and discrete MDPs , making this platform the most complete in the domain . But I identify two drawbacks in the proposed toolbox : first of all , if many metrics are proposed , it is very difficult to know which of these metrics are really relevant , and which are not . Said otherwise , the methodology would gain if these metrics could be connected to real use-cases e.g what are the relevant dimensions underlying atari environments , robotics ones , etc ... Right now , imagine I evaluate my model over all the different metrics , and compare my model to other models , I still do n't know which model I have to choose for solving a concrete use-case . A second drawback is readability : the approach is somehow proposing too many metrics such that being able to understand which model is good and which model is bad is very difficult . I would propose the authors to think about organizing these metrics in a way that they can be easily presented to users ( e.g using spider plots ? by using a hierarchy ? ) At last , the paper is just comparing a few models over these metrics , while I would expect to have a more complete benchmark of existing models . To conclude , if I really like the approach proposed in this paper , and if I think that it is a nice step toward a better evaluation of RL algorithms , I find that the paper is lacking some important characteristics to make MDP-playground really usable : i ) a good way to summarize the performance of RL algorithms too many metrics allowing a good understanding of the methods ii ) a comparison of more algorithms and iii ) a link between the proposed metrics and classical benchmarks . == Considering the modifications made on the paper , I increase my score", "rating": "6: Marginally above acceptance threshold", "reply_text": "> first of all , if many metrics are proposed , it is very difficult to know which of these metrics are really relevant , and which are not . This is just the nature of RL - there are too many dimensions and that is why RL is so hard . In different application areas , different dimensions are more relevant and that is the domain of more specific benchmarks - they are meant to capture and instantiate more specific problems . In fact as our qbert example ( from 3.1 ) in common response to all reviewers ) shows , different dimensions can be relevant even within the same environment at different points in time and space and this is also true of the real world . Another example we would like to mention is that of Atari . It comes in deterministic and non-deterministic versions . While for some researchers , performing well on deterministic Atari could be a worthwhile cause , for others they might want to only really work with non-determinism . For the former , noise would not be relevant at all . So , any kind of weighting of relevances of metrics is specific to the application at hand and we have refrained from doing it . > Said otherwise , the methodology would gain if these metrics could be connected to real use-cases e.g what are the relevant dimensions underlying atari environments , robotics ones , etc . We are not really sure whether this concern is regarding textually describing underlying dimensions which we have done by motivating the dimensions how these may be relevant in different domains . We could improve and detail that description if you like . If this concern is regarding being able to measure the \u201c amounts \u201d of these dimensions in the complex environments , then we have addressed it in 3.1 and 4.2 in the common response to reviewers . > I would propose the authors to think about organizing these metrics in a way that they can be easily presented to users ( e.g using spider plots ? by using a hierarchy ? ) Thank you for the suggestion . However , one drawback of the spider plots is that it forces us to impose a subjective valuation of the different dimensions . That 's why we refrain from setting a fixed grid of values for a dimension and allow fine-grained control over the amount of R noise ( or any other dimension ) that a user can inject into the environments . The experiments we have plotted in the paper are just a sample of the experiments possible with MDP Playground . Users could easily choose a different grid for another experiment . We do , however , also see the benefit of including spider plots with user-chosen weighting of relevance of values of the dimensions and are working on adding those for the rebuttal . Would that work for you ? > At last , the paper is just comparing a few models over these metrics , while I would expect to have a more complete benchmark of existing models . We have compared 3 SOTA deep RL methods and also 3 tabular methods in the appendix . The transfer experiments to more complex environments for the deep RL methods has led to extensive resource usage on our side for the Atari and Mujoco environments and this is apart from the hyperparameter tuning which in itself is expensive . We are happy to run more agents on the toy environments though . Would that be alright for you ? > since right now the comparison of RL techniques is very weak , and providing simple tools in that direction is crucial to make advances in the field . The MDP-playground approach is a good approach toward this goal and proposes a large number of different metrics on both continuous and discrete MDPs , making this platform the most complete in the domain . Thank you for the kind words ."}, {"review_id": "axNDkxU9-6z-2", "review_text": "This paper proposes a suite of benchmark tasks designed to test ( and possibly debug ) reinforcement learning algorithms . Deemed the MDP Playground , these environments are applicable to both discrete and continuous RL agents and allow tuning of various dimensions of complexity - reward delays , reward sparsity , stochasticity , etc . The authors demonstrate their framework by evaluating the performance of many well-known RL agents across a variety of these playground environments . Additionally they conduct similar experiments on Atari and Mujoco tasks and observe similar trends in agent performance when injecting noise , reward delays , and varying action max values . Finally , the MDP Playground is very quick to run and facilitates fast experimentation . It 's my view that the efficacy of a testing and debugging suite like MDP Playground is measured by the actionable insights that can be generated with it . To this end the authors describe some findings that may be applicable in the design of new environments ( such as action max needing tuning in continuous action environments ) , but little is shown about new insights gained toward understanding shortcomings of existing algorithms or routes for building better RL agents . Additionally , why do we believe that the structure of the MDPs generated by MDP Playground will resemble that of the problems that RL practitioners in the community are interested in solving ? Specifically for discrete environments having a completely connected transition function consisting of 8 states and 8 actions seems like it may not resemble more complicated environments like Atari . Similarly , moving a pointmass in a 2D plane likely has many differences from learning how to locomote a multi-jointed robot . It 's not clear that insights gained from Playground environments will transfer to more complex environments . Tangentially , it might be interesting to have a tool that could analyze a particular complex environment and automatically generate a corresponding Playground MDP that would somehow capture the same measures of difficulty , such that agents could be debugged/optimized in this low-cost environment before being transferred back to the complex environment . I have read the author response and stand by my original score of the paper .", "rating": "4: Ok but not good enough - rejection", "reply_text": "> but little is shown about new insights gained toward understanding shortcomings of existing algorithms or routes for building better RL agents . Thank you for pointing that out . We can discuss more in detail about shortcomings of existing algorithms , e.g. , how most of the algorithms we have surveyed can not identify these dimensions in environments but we as humans can give a rough measure of each of these dimensions in the situations we are faced with . And we could also discuss some new research directions ( we \u2019 ve already done a bit of this in 3.1 in common response to reviewers ) , however , all the breadth and diversity of RL is a very open research area with ( too ) many dimensions and we believe putting these together in a highly tunable platform is itself a very important tool that promotes quick development of algorithms . Identifying the dimensions and allowing fine-grained control over them is not an easy undertaking . Further , we would like to ask you to read the other reviewers \u2019 comments about wanting a more detailed description of the dimensions themselves . So , if you would like we can discuss insights in more detail but we would kindly request you to discuss with the other reviewers whether we should rather add more insights or more description of the dimensions in the main paper . > \u2026 more complicated environments like Atari . Similarly , moving a pointmass in a 2D plane likely has many differences from learning how to locomote a multi-jointed robot.It 's not clear that insights gained from Playground environments will transfer to more complex environments . We \u2019 re sorry that you feel this way , we have performed extensive experiments showing high-level transfer of trends to more complex environments in Section 4.3 . We have further discussed what kinds of trends we think will not transfer to more complex environments . For instance , we believe our environment can not capture complex dynamics interactions between different joints and have said in the text \u201c We believe this is due to correlations within the multiple degrees of freedom as opposed to a rigid object in the toy environment. \u201d . The cases you have mentioned are too specific for a toy benchmark and our platform wasn \u2019 t designed with those in mind . Like we have said in 3 in the common response to all reviewers , MDP Playground is meant as a bridge between theory and practice . We apologise if that was not clear enough . We \u2019 re not trying to claim that it will capture complex/high variance aspects like multi-jointed robots but it does capture general aspects like the significance of the time unit which we have shown transfer experiments for ."}, {"review_id": "axNDkxU9-6z-3", "review_text": "This paper presents `` MDP Playground '' , a family of procedurally generated MDPs that can be used to benchmark certain dimension of difficulty considered by the authors to be challenging to current RL algorithms . The paper presents the effects of the various perturbations to the MDP on state-of-the-art learning algorithms and discusses particular dimension of interest in the paper . A full and exhaustive analysis of results is presented in the appendix . The `` MDP Playground '' is slated to be open-sourced so that the community can benchmark against it . Pros : I think that high-level difficulties of MDPs are under-studied and that the over-reliance on benchmarks such as Atari or Mujoco make people look more at per-environment/game performance instead of thinking about high-level issues with the MDPs ( exploration , delays et.c ) . Although this is done in a hand-wavy manner , all attempts at formalising this seem essential to better understand what the actual degrees of difficulty are for particular tasks and whether novel proposed approaches are really tackling the challenge they claim to be tackling . I find the use of two procedurally generated MDP , one continuous and the other discreet , in a very simple task setup to be a good design choice , and also allows for quick iteration . Cons : Although this is an opinionated position , I 'm not super happy with the choice of perturbations . Some are very general , such as delays or stochasticity , while others are very specific such as target radius , action max , or action loss weight . I feel the nomenclature around the dimensions of 'hardness ' ( nit , perhaps 'difficulty ' would be a better word here ) is not very clear . The proposed dimensions seemed to be inspired by some tasks the authors are working with , but in that case it would make more sense to ground their choice by describing the tasks and arguing why these are particularly important . For example , target radius seems to be completely arbitrary , I could define an infinite number of reward functions that describe a goal and use all sorts of topologies to window my reward and shape it as the agent nears the goal , is this really a general problem for RL though ? I remain unconvinced . With relation to bsuite I would have also like to see more discussion on why the additional dimensions of difficulty make sense . For example bsuite already proposes noise as an evaluation dimension , how does MDP Playground 's noise differ ? More generally , not all dimensions were clearly described , in particular stochasticity , it was n't clear to me how this was defined . I would have appreciated more time spent on describing the challenges rather than the analysis of the results on all sorts of environments , in the end the core contribution here is the framework and its structure , the analysis is slightly out of scope given the length of the paper . There seems to also be very similar work in this space [ https : //arxiv.org/abs/2003.11881 ] that also proposes an open-source benchmark , it would be interesting to compare the choice of hardness dimensions to the ones used here . Conclusion : Overall I think this is a good direction of work , but it is a bit too unprincipled , and the paper structure kind of confusing . I would prefer perhaps less degrees of hardness , or perhaps a couple 'families ' to make your thought process easier to understand . Then , further discussion and grounding for each family of tasks would be great , to understand where these dimensions of hardness would manifest themselves . Finally , spending more time on describing each hardness dimension clearly instead of compacting it all into the end of Section 2 would also make this an easier read . I think this will be hard to achieve for the rebuttal phase , but I encourage continued work in this domain and look forward to seeing the authors ' response .", "rating": "4: Ok but not good enough - rejection", "reply_text": "> I feel the nomenclature around the dimensions of 'hardness ' ( nit , perhaps 'difficulty ' would be a better word here ) is not very clear . It is very important for us to have a clear nomenclature since a central goal of MDP Playground is adoption by the community . We welcome any discussion to find better terminology . We previously discussed between \u201c hardness \u201d and \u201c difficulty \u201d and decided to go for 'hardness ' as it sounded intuitively better to us . But that was a subjective preference on our part . Could you please provide some reasoning as to why you think \u2018 difficulty \u2019 is a better fit here ? > Some are very general , such as delays or stochasticity , while others are very specific such as target radius , action max , or action loss weight . \u2026 The proposed dimensions seemed to be inspired by some tasks the authors are working with , but in that case it would make more sense to ground their choice by describing the tasks and arguing why these are particularly important . For example , target radius seems to be completely arbitrary , I could define an infinite number of reward functions that describe a goal and use all sorts of topologies to window my reward and shape it as the agent nears the goal , is this really a general problem for RL though ? I remain unconvinced . It is true that there is likely some bias in our selections because our research is still on a finite number of sources and we apologise for any bias that may have crept in . However , we kindly point out to the reviewer that the proposed dimensions are not inspired by any tasks we work on and we even tried to survey environments we haven \u2019 t previously worked with . * target radius * , * action max * , or * action loss weight * were introduced when we surveyed the continuous environments in the literature , most of them here ( https : //github.com/openai/gym/tree/master/gym/envs/mujoco ) , and were consistent with our principle of making the environments as parameterisable as possible . They * are * , however , specific to the continuous environments and not there for discrete environments . Regarding * target radius * , we use just one number specifying the distance from the goal to decide whether we have reached the goal or not . To the best of our knowledge , any other way to decide if we have reached a goal requires more than a single scalar . Consequently , we believe an n-dimensional sphere having a target radius is the only property which may be considered objective . We are curious about how you would define a reward function in this instance and are happy to include your suggestion . Regarding * action max * , maybe our terminology was confusing . By * action max * , we actually meant the action range * [ action_min , action_max ] * , just that in our case we decided to keep * action_min = - action_max * because the toy environment is symmetric . The action range itself is always present in continuous systems , so unless there was some confusion due to our poor terminology , we believe it to be general . If you disagree , could you be so kind as to elaborate ? Regarding * action loss weight * , we can see how this might be seen as a subjective inclusion but this parameter is so frequently present in continuous control environments , that we feel omitting it would have left a common use case unaddressed ( for instance , almost every environment in the Mujoco link above has the action loss weight ) . This can easily be turned off by setting it to 0 ( the default ) . While , just like the reviewer , we would also love to be perfectly general and as objective as possible , for practical reasons we have included some dimensions which we felt were very common but these can always be turned off ( which is usually the default ) . We welcome any constructive discussions and actionable suggestions to make our design decisions and selection of dimensions as objective and general as possible . Assuming that MDP Playground is designed to be improved based on debates like these , we hope the reviewer does not deem the proposed dimensions to be arbitrary but instead can appreciate the nature and the consequences of making design decisions and having to impose subjectivity in some instances ."}], "0": {"review_id": "axNDkxU9-6z-0", "review_text": "-- POST-REBUTTAL COMMENTS As a result of the discussion the paper has improved , so I 'm increasing my score . However , the core issue , that the proposed benchmarks do n't seem to capture the difficulty structure of either real problems or more complex benchmarks , remains . -- SUMMARY : The paper introduces MDP Playground , a parameterized suite of MDPs with low computational requirements that nonetheless present significant challenges for existing RL algorithms . The authors argue that MDP Playground is a valuable testbed for developing and evaluating ne RL algorithms . The paper also assess how the identified dimensions of hardness that can be exercised in MDP Playground transfer to more complex benchmarks . The paper also uses MDP Playground to evaluate several rllib algorithms . HIGH-LEVEL COMMENTS : This work has a lot of potential . It targets an important problem in RL research : studying the behavior of RL algorithms as an environment changes along various dimensions of hardness . It is valuable to have a benchmark suite of the sort this work aims to deliver , one that allows varying these dimensions in a controlled way and consists of problems that are simple enough for debugging . However , in its current form this work has non-trivial weaknesses in contribution and presentation that make publication at ICLR or similar conferences premature : 1 ) MDP Playground 's problems are completely unintuitive . They are randomly generated and are n't inspired by any real scenarios . This raises the question of how meaningful they are . In ML , it is understood from results such as the no-free-lunch theorem that no learner can do well across all possible tasks , and the sensible approach is to make ML algorithms work well on meaningful data distributions that are encountered in reality . Unfortunately , RL as a field has n't been good at sticking to this principle : many of its existing benchmarks , such as videogames , look `` interesting '' and difficult , but in a very different way that real decision-making scenarios are . MDP Playground exacerbates this issue -- since its MDPs are randomly generated and do n't have a natural interpretation , it 's difficult to get even an intuition for a good behavior in them , and difficult to understand how to generate problems that that have a combination of hardness along different dimensions that is representative of realistic scenarios . A case in point is the paper 's omission of MDP non-ergodicity ( and presence of constraints on the desired policy as a common real factor that causes non-ergodicity ) from its list of hardness dimensions . In reality , and even in some existing benchmarks , the learner can reach irrecoverable ( absorbing ) failure states , such as robots damaging themselves or objects they interact with , unless they are very careful . There are goal-directed MDP models with such states -- see , e.g. , reference [ a ] at the end of the review -- and learning in their presence in realistic scenarios requires costly resets -- see , e.g. , reference [ b ] . MDP Playground does n't help with researching this aspect , e.g. , by having knobs for probabilities of entering such states , penalties for doing so and the cost of recovery , despite it being ubiquitous in reality . Another major omission is the fact that in reality agent actions have durations that can make it very hard to learn a policy within a given time budget . The existing benchmark does n't allow studying the effects of action duration on learning time . Given these gaps , I doubt that MDP Playground in its current form adds much value over the existing RL benchmarks , which have some drawbacks but have interpretability as a big asset for debugging . 2 ) The paper 's analysis of dimensions of hardness is quite unprincipled , contrary to the paper 's claims . What makes many decision-making problems ( both existing ones and those introduced in MDP Playground ) hard is partial observability , but the paper never formally states what POMDPs are , nor even mentions this term . If it defined POMDPs formally , the incorrectness of some of the statements the paper makes about problems with partial observability ( see the detailed comments below ) would become obvious , and the connections between partial observability and hardness would become much clearer mathematically . See , e.g. , reference [ c ] for a formal but accessible treatment of POMDPs . TECHNICAL ISSUES/RELATED WORK : -- In the intro , the paper claims that `` partial observability [ is ] when the underlying environment is assumed to be an MDP , however , the state formulation , i.e. , the observation used by the agent is not Markovian '' . This is an extremely inaccurate statement at best . In POMDPs , observations are Markovian -- their probability depends only on the current ( hidden ) state . `` State formulation '' is also Markovian , as it is fully observable MDPs . So are the belief states . What is non-Markovian in POMDPs is the optimal policy w.r.t.the observations : an optimal policy can depend on the entire observation history . However , again , each observation history maps to a belief state , and in the belief state space the optimal policies are Markovian . -- The same goes for several other statements , e.g . `` performance degrades in environments where the delayed reward induces partial observability and hence makes the state used by the algorithm non-Markovian '' . How can delayed reward introduce partial observability ? How can partial observability make `` the state used by the algorithm '' ( an imprecise term in its own right ) non-Markovian ? -- Out of the dimensions of hardness the paper does identify , why is `` sequence length '' a distinct dimension , rather than being an instance of reward delay or reward sparsity ? -- In the related work , a notable omission is reference [ d ] , which looks at aspects that MDP hard and analyzes existing benchmarks w.r.t.those aspects . -- I was n't sure what the benefit of the `` varying reprsentations '' experiment that tries to enforce various kinds of invariances was . First of all , the experiment is about applying various types of data augmentation to images , not about varying representations . Second , the conclusion that `` this indicates that shift and other types of invariance do not come for free and that one needs to have sufficient amount of samples for the algorithm to become invariant to the transforms we desire . '' is rather obvious and well-known . -- A similar remark goes for several other experiments : varying time units , target variance , irrelevant features . Their results are completely expected , and while they might be a useful sanity check , the results description can be condensed to 1 line each and placed in the figure captions . -- A similar comment goes for the entire content of Section 4.3 as well -- it is very verbose and can be greatly condensed and structured into a short list with bullet points summraizing the findings . Typos : `` Very low cost execution '' -- > `` Very low cost of execution '' [ a ] Kolobov , Mausam , Weld , `` A Theory of Goal-Oriented MDPs with Dead Ends '' , UAI-2012 [ b ] Eysenbach , Gu , Ibarz , Levine , `` Leave no Trace : Learning to Reset for Safe and Autonomous Reinforcement Learning '' , ICLR-2018 [ c ] Kochenderfer , `` Decision Making Under Uncertainty : Theory and Application '' , MIT Press , 2015 [ d ] Maillard , Mann , Mannor `` \u201c How hard is my MDP ? \u201d The distribution-norm to the rescue '' , NIPS-2014", "rating": "5: Marginally below acceptance threshold", "reply_text": "> In ML , it is understood from results such as the no-free-lunch theorem that no learner can do well across all possible tasks , and the sensible approach is to make ML algorithms work well on meaningful data distributions that are encountered in reality . This is true for empirical research on complex environments . However , for theoretical development of algorithms , making algorithms work on meaningful data distributions that are encountered in reality may not always be a requirement . For example , Q-learning is agnostic to the structure in * P * and * R * and should work in theory for * * any * * kind of * P * and * R * . > Unfortunately , RL as a field has n't been good at sticking to this principle : many of its existing benchmarks , such as videogames , look `` interesting '' and difficult , but in a very different way that real decision-making scenarios are . > MDP Playground exacerbates this issue -- since its MDPs are randomly generated and do n't have a natural interpretation , it 's difficult to get even an intuition for a good behavior in them , and difficult to understand how to generate problems that that have a combination of hardness along different dimensions that is representative of realistic scenarios . We have made a case for how we want to help researchers identify inductive biases in 3.1 in the common response to reviewers . To do this , even random data can work as long as the data have the important features in there . Also as mentioned there , most RL algorithms that we are aware of are agnostic to the structure of * P * and * R * , so we believe it \u2019 s not wrong to test them on randomly generated * P * s and * R * s. The bsuite debugging example we have given in Appendix E.1 supports our claim . We only needed the key feature of varying reward sparsity there . > A case in point is the paper 's omission of MDP non-ergodicity ( and presence of constraints on the desired policy as a common real factor that causes non-ergodicity ) from its list of hardness dimensions . In reality , and even in some existing benchmarks , the learner can reach irrecoverable ( absorbing ) failure states , such as robots damaging themselves or objects they interact with , unless they are very careful . > MDP Playground does n't help with researching this aspect , e.g. , by having knobs for probabilities of entering such states , penalties for doing so and the cost of recovery , despite it being ubiquitous in reality . Thank you , we have a couple of responses to this . Firstly , we allow setting * terminal state density * which achieves non-ergodicity by having terminal states which only transition back to themselves . Further , we have * terminal state costs * which can also be specified , so at least in some form non-ergodicity * is * captured . Secondly , as we have said in principle ( 1 ) above , we aim to allow users to specify their own * P * and * R * and this is easily ready by the end of the rebuttal . Thirdly , we are also happy to consider adding it as a separate dimension according to our commitment ( 2 ) above . To that end , could you please point out if you \u2019 re not satisfied with the first two proposals here ? > Another major omission is the fact that in reality agent actions have durations that can make it very hard to learn a policy within a given time budget . The existing benchmark does n't allow studying the effects of action duration on learning time . We are confused by this statement . We believe from your description , that the included dimension * time unit * is exactly this feature ( please see the last paragraph of Section 2 in the paper for the relevant equation ) . It 's the time that the actions last for in the continuous environments . Could you please clarify what you believe to be missing here ? > The paper 's analysis of dimensions of hardness is quite unprincipled , contrary to the paper 's claims . We have explicitly stated our principle for selecting the dimensions here . Could you please tell us if that makes it better for you ? > In the intro , the paper claims that `` partial observability [ is ] when the underlying environment is assumed to be an MDP , however , the state formulation , i.e. , the observation used by the agent is not Markovian '' . This is an extremely inaccurate statement at best . In POMDPs , observations are Markovian -- their probability depends only on the current ( hidden ) state . `` State formulation '' is also Markovian , as it is fully observable MDPs . So are the belief states . What is non-Markovian in POMDPs is the optimal policy w.r.t.the observations : an optimal policy can depend on the entire observation history . However , again , each observation history maps to a belief state , and in the belief state space the optimal policies are Markovian . Thank you for pointing this out . We 're sorry if our statements were inaccurate . We \u2019 re happy to reframe it to be accurate for the rebuttal ."}, "1": {"review_id": "axNDkxU9-6z-1", "review_text": "The paper describes a new benchmark for evaluating reinforcement learning techniques ( MDP-playground ) . It can be seen as a toolbox allowing to generate different MDPs with different characteristics . Each MDP will then be used to probe a particular ability of learning algorithms , resulting in a comparison of methods over multiple dimensions . Proposed dimensions are reward sparsity , stochasticity , delayed reward , etc .... In addition to this toolbox , the authors also evaluate some of the classical algorithms in the domain . == Comments First of all , the idea of evaluating RL techniques over multiple dimensions is very interesting , since right now the comparison of RL techniques is very weak , and providing simple tools in that direction is crucial to make advances in the field . The MDP-playground approach is a good approach toward this goal and proposes a large number of different metrics on both continuous and discrete MDPs , making this platform the most complete in the domain . But I identify two drawbacks in the proposed toolbox : first of all , if many metrics are proposed , it is very difficult to know which of these metrics are really relevant , and which are not . Said otherwise , the methodology would gain if these metrics could be connected to real use-cases e.g what are the relevant dimensions underlying atari environments , robotics ones , etc ... Right now , imagine I evaluate my model over all the different metrics , and compare my model to other models , I still do n't know which model I have to choose for solving a concrete use-case . A second drawback is readability : the approach is somehow proposing too many metrics such that being able to understand which model is good and which model is bad is very difficult . I would propose the authors to think about organizing these metrics in a way that they can be easily presented to users ( e.g using spider plots ? by using a hierarchy ? ) At last , the paper is just comparing a few models over these metrics , while I would expect to have a more complete benchmark of existing models . To conclude , if I really like the approach proposed in this paper , and if I think that it is a nice step toward a better evaluation of RL algorithms , I find that the paper is lacking some important characteristics to make MDP-playground really usable : i ) a good way to summarize the performance of RL algorithms too many metrics allowing a good understanding of the methods ii ) a comparison of more algorithms and iii ) a link between the proposed metrics and classical benchmarks . == Considering the modifications made on the paper , I increase my score", "rating": "6: Marginally above acceptance threshold", "reply_text": "> first of all , if many metrics are proposed , it is very difficult to know which of these metrics are really relevant , and which are not . This is just the nature of RL - there are too many dimensions and that is why RL is so hard . In different application areas , different dimensions are more relevant and that is the domain of more specific benchmarks - they are meant to capture and instantiate more specific problems . In fact as our qbert example ( from 3.1 ) in common response to all reviewers ) shows , different dimensions can be relevant even within the same environment at different points in time and space and this is also true of the real world . Another example we would like to mention is that of Atari . It comes in deterministic and non-deterministic versions . While for some researchers , performing well on deterministic Atari could be a worthwhile cause , for others they might want to only really work with non-determinism . For the former , noise would not be relevant at all . So , any kind of weighting of relevances of metrics is specific to the application at hand and we have refrained from doing it . > Said otherwise , the methodology would gain if these metrics could be connected to real use-cases e.g what are the relevant dimensions underlying atari environments , robotics ones , etc . We are not really sure whether this concern is regarding textually describing underlying dimensions which we have done by motivating the dimensions how these may be relevant in different domains . We could improve and detail that description if you like . If this concern is regarding being able to measure the \u201c amounts \u201d of these dimensions in the complex environments , then we have addressed it in 3.1 and 4.2 in the common response to reviewers . > I would propose the authors to think about organizing these metrics in a way that they can be easily presented to users ( e.g using spider plots ? by using a hierarchy ? ) Thank you for the suggestion . However , one drawback of the spider plots is that it forces us to impose a subjective valuation of the different dimensions . That 's why we refrain from setting a fixed grid of values for a dimension and allow fine-grained control over the amount of R noise ( or any other dimension ) that a user can inject into the environments . The experiments we have plotted in the paper are just a sample of the experiments possible with MDP Playground . Users could easily choose a different grid for another experiment . We do , however , also see the benefit of including spider plots with user-chosen weighting of relevance of values of the dimensions and are working on adding those for the rebuttal . Would that work for you ? > At last , the paper is just comparing a few models over these metrics , while I would expect to have a more complete benchmark of existing models . We have compared 3 SOTA deep RL methods and also 3 tabular methods in the appendix . The transfer experiments to more complex environments for the deep RL methods has led to extensive resource usage on our side for the Atari and Mujoco environments and this is apart from the hyperparameter tuning which in itself is expensive . We are happy to run more agents on the toy environments though . Would that be alright for you ? > since right now the comparison of RL techniques is very weak , and providing simple tools in that direction is crucial to make advances in the field . The MDP-playground approach is a good approach toward this goal and proposes a large number of different metrics on both continuous and discrete MDPs , making this platform the most complete in the domain . Thank you for the kind words ."}, "2": {"review_id": "axNDkxU9-6z-2", "review_text": "This paper proposes a suite of benchmark tasks designed to test ( and possibly debug ) reinforcement learning algorithms . Deemed the MDP Playground , these environments are applicable to both discrete and continuous RL agents and allow tuning of various dimensions of complexity - reward delays , reward sparsity , stochasticity , etc . The authors demonstrate their framework by evaluating the performance of many well-known RL agents across a variety of these playground environments . Additionally they conduct similar experiments on Atari and Mujoco tasks and observe similar trends in agent performance when injecting noise , reward delays , and varying action max values . Finally , the MDP Playground is very quick to run and facilitates fast experimentation . It 's my view that the efficacy of a testing and debugging suite like MDP Playground is measured by the actionable insights that can be generated with it . To this end the authors describe some findings that may be applicable in the design of new environments ( such as action max needing tuning in continuous action environments ) , but little is shown about new insights gained toward understanding shortcomings of existing algorithms or routes for building better RL agents . Additionally , why do we believe that the structure of the MDPs generated by MDP Playground will resemble that of the problems that RL practitioners in the community are interested in solving ? Specifically for discrete environments having a completely connected transition function consisting of 8 states and 8 actions seems like it may not resemble more complicated environments like Atari . Similarly , moving a pointmass in a 2D plane likely has many differences from learning how to locomote a multi-jointed robot . It 's not clear that insights gained from Playground environments will transfer to more complex environments . Tangentially , it might be interesting to have a tool that could analyze a particular complex environment and automatically generate a corresponding Playground MDP that would somehow capture the same measures of difficulty , such that agents could be debugged/optimized in this low-cost environment before being transferred back to the complex environment . I have read the author response and stand by my original score of the paper .", "rating": "4: Ok but not good enough - rejection", "reply_text": "> but little is shown about new insights gained toward understanding shortcomings of existing algorithms or routes for building better RL agents . Thank you for pointing that out . We can discuss more in detail about shortcomings of existing algorithms , e.g. , how most of the algorithms we have surveyed can not identify these dimensions in environments but we as humans can give a rough measure of each of these dimensions in the situations we are faced with . And we could also discuss some new research directions ( we \u2019 ve already done a bit of this in 3.1 in common response to reviewers ) , however , all the breadth and diversity of RL is a very open research area with ( too ) many dimensions and we believe putting these together in a highly tunable platform is itself a very important tool that promotes quick development of algorithms . Identifying the dimensions and allowing fine-grained control over them is not an easy undertaking . Further , we would like to ask you to read the other reviewers \u2019 comments about wanting a more detailed description of the dimensions themselves . So , if you would like we can discuss insights in more detail but we would kindly request you to discuss with the other reviewers whether we should rather add more insights or more description of the dimensions in the main paper . > \u2026 more complicated environments like Atari . Similarly , moving a pointmass in a 2D plane likely has many differences from learning how to locomote a multi-jointed robot.It 's not clear that insights gained from Playground environments will transfer to more complex environments . We \u2019 re sorry that you feel this way , we have performed extensive experiments showing high-level transfer of trends to more complex environments in Section 4.3 . We have further discussed what kinds of trends we think will not transfer to more complex environments . For instance , we believe our environment can not capture complex dynamics interactions between different joints and have said in the text \u201c We believe this is due to correlations within the multiple degrees of freedom as opposed to a rigid object in the toy environment. \u201d . The cases you have mentioned are too specific for a toy benchmark and our platform wasn \u2019 t designed with those in mind . Like we have said in 3 in the common response to all reviewers , MDP Playground is meant as a bridge between theory and practice . We apologise if that was not clear enough . We \u2019 re not trying to claim that it will capture complex/high variance aspects like multi-jointed robots but it does capture general aspects like the significance of the time unit which we have shown transfer experiments for ."}, "3": {"review_id": "axNDkxU9-6z-3", "review_text": "This paper presents `` MDP Playground '' , a family of procedurally generated MDPs that can be used to benchmark certain dimension of difficulty considered by the authors to be challenging to current RL algorithms . The paper presents the effects of the various perturbations to the MDP on state-of-the-art learning algorithms and discusses particular dimension of interest in the paper . A full and exhaustive analysis of results is presented in the appendix . The `` MDP Playground '' is slated to be open-sourced so that the community can benchmark against it . Pros : I think that high-level difficulties of MDPs are under-studied and that the over-reliance on benchmarks such as Atari or Mujoco make people look more at per-environment/game performance instead of thinking about high-level issues with the MDPs ( exploration , delays et.c ) . Although this is done in a hand-wavy manner , all attempts at formalising this seem essential to better understand what the actual degrees of difficulty are for particular tasks and whether novel proposed approaches are really tackling the challenge they claim to be tackling . I find the use of two procedurally generated MDP , one continuous and the other discreet , in a very simple task setup to be a good design choice , and also allows for quick iteration . Cons : Although this is an opinionated position , I 'm not super happy with the choice of perturbations . Some are very general , such as delays or stochasticity , while others are very specific such as target radius , action max , or action loss weight . I feel the nomenclature around the dimensions of 'hardness ' ( nit , perhaps 'difficulty ' would be a better word here ) is not very clear . The proposed dimensions seemed to be inspired by some tasks the authors are working with , but in that case it would make more sense to ground their choice by describing the tasks and arguing why these are particularly important . For example , target radius seems to be completely arbitrary , I could define an infinite number of reward functions that describe a goal and use all sorts of topologies to window my reward and shape it as the agent nears the goal , is this really a general problem for RL though ? I remain unconvinced . With relation to bsuite I would have also like to see more discussion on why the additional dimensions of difficulty make sense . For example bsuite already proposes noise as an evaluation dimension , how does MDP Playground 's noise differ ? More generally , not all dimensions were clearly described , in particular stochasticity , it was n't clear to me how this was defined . I would have appreciated more time spent on describing the challenges rather than the analysis of the results on all sorts of environments , in the end the core contribution here is the framework and its structure , the analysis is slightly out of scope given the length of the paper . There seems to also be very similar work in this space [ https : //arxiv.org/abs/2003.11881 ] that also proposes an open-source benchmark , it would be interesting to compare the choice of hardness dimensions to the ones used here . Conclusion : Overall I think this is a good direction of work , but it is a bit too unprincipled , and the paper structure kind of confusing . I would prefer perhaps less degrees of hardness , or perhaps a couple 'families ' to make your thought process easier to understand . Then , further discussion and grounding for each family of tasks would be great , to understand where these dimensions of hardness would manifest themselves . Finally , spending more time on describing each hardness dimension clearly instead of compacting it all into the end of Section 2 would also make this an easier read . I think this will be hard to achieve for the rebuttal phase , but I encourage continued work in this domain and look forward to seeing the authors ' response .", "rating": "4: Ok but not good enough - rejection", "reply_text": "> I feel the nomenclature around the dimensions of 'hardness ' ( nit , perhaps 'difficulty ' would be a better word here ) is not very clear . It is very important for us to have a clear nomenclature since a central goal of MDP Playground is adoption by the community . We welcome any discussion to find better terminology . We previously discussed between \u201c hardness \u201d and \u201c difficulty \u201d and decided to go for 'hardness ' as it sounded intuitively better to us . But that was a subjective preference on our part . Could you please provide some reasoning as to why you think \u2018 difficulty \u2019 is a better fit here ? > Some are very general , such as delays or stochasticity , while others are very specific such as target radius , action max , or action loss weight . \u2026 The proposed dimensions seemed to be inspired by some tasks the authors are working with , but in that case it would make more sense to ground their choice by describing the tasks and arguing why these are particularly important . For example , target radius seems to be completely arbitrary , I could define an infinite number of reward functions that describe a goal and use all sorts of topologies to window my reward and shape it as the agent nears the goal , is this really a general problem for RL though ? I remain unconvinced . It is true that there is likely some bias in our selections because our research is still on a finite number of sources and we apologise for any bias that may have crept in . However , we kindly point out to the reviewer that the proposed dimensions are not inspired by any tasks we work on and we even tried to survey environments we haven \u2019 t previously worked with . * target radius * , * action max * , or * action loss weight * were introduced when we surveyed the continuous environments in the literature , most of them here ( https : //github.com/openai/gym/tree/master/gym/envs/mujoco ) , and were consistent with our principle of making the environments as parameterisable as possible . They * are * , however , specific to the continuous environments and not there for discrete environments . Regarding * target radius * , we use just one number specifying the distance from the goal to decide whether we have reached the goal or not . To the best of our knowledge , any other way to decide if we have reached a goal requires more than a single scalar . Consequently , we believe an n-dimensional sphere having a target radius is the only property which may be considered objective . We are curious about how you would define a reward function in this instance and are happy to include your suggestion . Regarding * action max * , maybe our terminology was confusing . By * action max * , we actually meant the action range * [ action_min , action_max ] * , just that in our case we decided to keep * action_min = - action_max * because the toy environment is symmetric . The action range itself is always present in continuous systems , so unless there was some confusion due to our poor terminology , we believe it to be general . If you disagree , could you be so kind as to elaborate ? Regarding * action loss weight * , we can see how this might be seen as a subjective inclusion but this parameter is so frequently present in continuous control environments , that we feel omitting it would have left a common use case unaddressed ( for instance , almost every environment in the Mujoco link above has the action loss weight ) . This can easily be turned off by setting it to 0 ( the default ) . While , just like the reviewer , we would also love to be perfectly general and as objective as possible , for practical reasons we have included some dimensions which we felt were very common but these can always be turned off ( which is usually the default ) . We welcome any constructive discussions and actionable suggestions to make our design decisions and selection of dimensions as objective and general as possible . Assuming that MDP Playground is designed to be improved based on debates like these , we hope the reviewer does not deem the proposed dimensions to be arbitrary but instead can appreciate the nature and the consequences of making design decisions and having to impose subjectivity in some instances ."}}