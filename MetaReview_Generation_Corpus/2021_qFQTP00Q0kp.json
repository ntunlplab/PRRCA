{"year": "2021", "forum": "qFQTP00Q0kp", "title": "Self-Supervised Time Series Representation Learning by Inter-Intra Relational Reasoning", "decision": "Reject", "meta_review": "This paper presents a general self-supervised time series representation learning framework. The organization is good, and the architecture is well motivated. However, the paper has limited novelty, and is a straightforward application of ideas in self-supervised learning literature.\n\nExperimental results are not entirely convincing. The used dataset is small-scale that makes the task simple. A more thorough comparison with recent related work is needed. The presentation is also sometimes hard to follow.", "reviews": [{"review_id": "qFQTP00Q0kp-0", "review_text": "==Update after rebuttal I will remain my score of weak rejection . = This paper presents to model both inter-sample and intra-temporal relations . The idea is naive but is easy to follow and reasonable . Experimental results support the effectiveness of the method . Strengths : The idea is reasonable and the application is important . Weakness : 1 . The notations are messy . The author should consider clean up some unused notations for better readability . Especially for section 3.1 , I spend quite a few time thoroughly digest the notations . The explanation in section 3.2 is even scarier . I can easily understand the concept from the intro and Figure 1 , yet I spend much more time reading sections 3.1 and 3.2 . 2.The presentation of the experimental section is also hard to follow . Specifically , what is the message conveyed in Figure 5 ? I would suggest the author highlight the region that we can pay attention to . Nonetheless , why only considering the composition of two augmentations ? Instead of providing a matrix that considers all the combinations between two different augmentations , why not providing a table summarizing the sets of different combinations of augmentations ? The sets of combinations can be the sets that the author likes us to focus on . 3.An important paper [ 1 ] is missing . The author should discuss this missing reference . Overall , I am leaning positive toward this paper , yet I feel the presentation can be greatly improved . I am giving the score 5 for now , and I may update the score after the rebuttal . [ 1 ] Temporal Relational Reasoning in Videos , Zhou et al. , ECCV 2018 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * On presentation of method parts : * * We have revised Section 3 according to the review 's suggestion . In the revised version , we clean up the unused notations and rewrite some sentences to make the description more clearer . More details are shown in the marked parts in the revised version . * * What is the message conveyed in Figure 5 ? * * Figure 5 shows the experimental results about the impact of different data augmentations and relation modules . Firstly , to evaluate the impact of different data augmentations , as similar to SimCLR [ 1 ] , we conduct linear evaluation under individual or composition of data augmentations including several common time series augmentations such as magnitude domain based transformations such as jittering ( Jit . ) , cutout ( Cut . ) , scaling ( Sca . ) , magnitude warping ( M.W . ) , and time domain based transformations such as time warping ( T.W . ) , window slicing ( W.S . ) , window warping ( W.W. ) . We observe that the composition of different data augmentations is crucial for learning useful representations . For example , inter-sample relation reasoning is more sensitive to the augmentations , and performs worse under Cut. , Sca. , and M.W . augmentations , while intra-temporal relation reasoning is less sensitive to the manner of augmentations , although it performs better under the time domain based transformation . Secondly , to evaluate the effectiveness of different relation modules for ablation study , we compare the linear evaluation results on different modules including Inter-sample Relation , Intra-temporal Relation and their combination ( SelfTime ) . We find that , by combining both the inter-sample and intra-temporal relation reasoning , the proposed SelfTime achieves better performance , which demonstrates the effectiveness of considering different levels of relation for time series representation learning . Similar experimental conclusions also hold on for other datasets . More experimental results on the other five datasets for evaluation of the impact of different relation modules and data augmentations are shown in Appendix F. * * Why only considering the composition of two augmentations ? * * As similar to SimCLR [ 1 ] , to better understand the effects of individual data augmentations and the importance of augmentation composition , we conduct linear evaluation when applying augmentations individually or in pairs . Previous studies show that those contrastive learning [ 1 ] or inter-sample relation reasoning [ 2 ] based methods are sensitive to the augmentation strategies on image data , therefore , it 's also necessary to evaluate it on time series data . Here , we use several common time series augmentations such as magnitude domain based transformations such as jittering ( Jit . ) , cutout ( Cut . ) , scaling ( Sca . ) , magnitude warping ( M.W . ) , and time domain based transformations such as time warping ( T.W . ) , window slicing ( W.S . ) , window warping ( W.W. ) . We observe that the composition of different data augmentations is crucial for learning useful time series representations . For example , inter-sample relation reasoning is more sensitive to the augmentations , and performs worse under Cut. , Sca. , and M.W . augmentations , while intra-temporal relation reasoning is less sensitive to the manner of augmentations , although it performs better under the time domain based transformation . In summary , our main purpose is to evaluate that the impact of different augmentation composition is important , not the purpose of the best composition for evaluation because there could be plenty of choices . From the experimental results , we can obviously find that inter-sample relation module is more sensitive to augmentations while intra-temporal relation module does not , while intra-temporal module performs better than the inter-sample module in most cases , which provides our more insights into pretext task designing on time series data . Therefore , we believe that conducting augmentations in pairs is enough to support our purpose . [ 1 ] Chen , Ting , et al . `` A simple framework for contrastive learning of visual representations . '' ICML'20 ( 2020 ) . [ 2 ] Self-supervised relational reasoning for representation learning . Patacchiola and Storkey . NeurIPS'20"}, {"review_id": "qFQTP00Q0kp-1", "review_text": "* * Summary * * This paper presents a general Self-supervised Time Series representation learning framework . It explores the inter-sample relation reasoning and intra-temporal relation reasoning of time series to capture the underlying structure pattern of the unlabeled time series data . The proposed method achieves new state-of-the-art results and outperforms existing methods by a significant margin on multiple real-world time-series datasets for the classification tasks . * * Contributions * * 1 . The paper is well written and easy to follow . The organization is good . 2.The architecture is well motivated . It is reasonable to use unsupervised temporal relations to learn video features . 3.The qualitative results are numerous , insightful , and convincing on multiple datasets . The authors conduct extensive experiments to demonstrate the effectiveness of the proposed method , including inter-sample relation reasoning and intra-temporal relation reasoning . * * Details * * 1 . The novelty of the proposed method . The Inter-sample relation reasoning is very similar to SimCLR , which also maximizes agreement between different views of augmentation from the same sample via a contrastive loss . Considering this , the novelty is relatively incremental . 2.Additional video recognition experiments . The used dataset is small-scale that makes the task simple . I would have wanted to see results on large-scale classification tasks , such as video action classification . The performance of action classification is closely related to the temporal feature modeling . So the effects on this task can make the proposed method more convincing . * * Conclusion * * overall , this paper proposes an interesting architecture for self-supervised temporal modeling . But the novelty is relatively limited compared to the recent SimCLR work . And it requires additional experiments on harder video classification task and datasets to show the effects and robustness of the proposed method . * * After rebuttal * * Thanks for the detailed response . This paper can be seen as an interesting attempt to use self-supervised on time series data . Although the basic idea is similar to SimCLR , It is still interesting work considering the computation complexity and new loss function . So I update my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the comments and questions ! Please find below our answers . * * On novelty of the proposed method : * * 1 . The difference between SelfTime and SimCLR : Relation reasoning is far different from the contrastive learning based methods such as SimCLR . There are two main differences between them : 1.1 . Loss function : SimCLR uses the pairwise-distance based contrastive loss function as objective to guide the inter-sample relation reasoning for time series representation learning , while in SelfTime , we use the cross-entropy based loss function as objective to guide the reasoning of different levels of entity relationship ( inter-sample relation and intra-temporal relation ) for learning of useful time series representation . The pairwise-distance based contrastive loss relies on a large number of negatives , which are difficult to obtain in mini-batch stochastic optimization , and SimCLR requires specialized optimizers ( e.g. , cosine annealing ) to stabilize the training at scale , however , the cross-entropy do not need the convoluted sample-mining heuristics , resulting in a more easy optimization process . Although cross-entropy does not explicitly involve pairwise distances , as demonstrated in [ 1 ] , it 's an upper bound on a new pairwise loss , which has a structure similar to various pairwise losses : it minimizes intra-class distances while maximizing inter-class distances . As a result , minimizing the cross-entropy can be seen as an approximate bound-optimization ( or Majorize-Minimize ) algorithm for minimizing this pairwise loss . Please refer [ 1 ] for more details about the relationship between cross-entropy loss and pairwise-distance loss . [ 1 ] Boudiaf , Malik , et al . `` A unifying mutual information view of metric learning : cross-entropy vs. pairwise losses . '' European Conference on Computer Vision . Springer , Cham , 2020 . 1.2.Computation complexity : SelfTime is a more efficient algorithm compared with the traditional contrastive learning models such as SimCLR . The complexity of SimCLR is $ O ( N^ { 2 } K^ { 2 } ) $ , while the complexity of SelfTime is $ O ( NK^ { 2 } ) +O ( NK ) $ , where $ O ( NK^ { 2 } ) $ is the complexity of inter-sample relation reasoning module , and $ O ( NK ) $ is the complexity of intra-temporal relation reasoning module . It can be seen that SimCLR scales quadratically in both training size $ N $ and augmentation number $ K $ . However , in SelfTime , inter-sample relation reasoning module scales quadratically with the number of augmentations $ K $ , and linearly with the training size $ N $ , and intra-temporal relation reasoning module scales linearly with both augmentations and training size . 2.We would like to reclaim the contributions of our work here : Our main contributions are three-fold : ( 1 ) we present a general self-supervised time series representation learning framework by investigating different levels of relations of time series data including inter-sample relation and intra-temporal relation . ( 2 ) We design a simple and effective intra-temporal relation sampling strategy to capture the underlying temporal patterns of time series . ( 3 ) We conduct extensive experiments on different categories of real-world time series data , and systematically study the impact of different data augmentation strategies and temporal relation sampling strategies on self-supervised learning of time series . By comparing with multiple state-of-the-art baselines , experimental results show that SelfTime builds new state-of-the-art on self-supervised time series representation learning ."}, {"review_id": "qFQTP00Q0kp-2", "review_text": "The paper proposes an approach for self-supervised time series representation learning by using inter-sample and intra-temporal relational reasoning . The paper builds upon the existing ideas on relational reasoning [ 3 ] and self-supervised learning to train models from unlabeled data . Self-supervised learning for time series is still under-explored and this paper attempts to bridge this gap in time series literature . The key novelty of the paper as claimed by the authors is in the design of inter-sample and intra-temporal tasks and loss functions to learn a feature extractor from unlabeled data . This idea of designing pretext tasks using global-sample structure and local-temporal structure and adapting it for time series is interesting . However , the inter-sample loss function seems to be a straightforward application of ideas in self-supervised learning literature that rely on various augmentations to create positive and negative pairs of samples . The intra-temporal task is defined where the distance between subsequences ( referred to as time pieces in the paper ) of a time series is used to create a classification task . This is a potentially novel ( albeit incremental ) aspect of the approach but the empirical evaluations ( as detailed below ) fail to highlight the impact of the same on performance clearly . Furthermore , the idea of using subsequences to define self-supervised ( intra-temporal ) tasks for time series has been explored earlier , e.g.in [ 1,2 ] .The authors seem to be unaware of papers like [ 1,2 ] , and Introduction and Related Work sections suggest that self-supervised learning for time series has not been attempted earlier . Apart from lack of clarity on novelty and contribution of the work , I have following concerns regarding empirical evaluation : 1 . The ablation studies show the effect of number of classes and the length of subsequences ( pieces ) on linear evaluation accuracy ( Linear ACC ) for the downstream classification tasks . However , the sensitivity of results to the hyperparameters of intra-temporal task is reported only on CricketX . Do the same results hold on other datasets ? Also , the results on downstream task seem to be very sensitive to the choice of parameters C and L/T . The authors state that `` In the experiment , to select a moderately difficult pretext task for different datasets , we set { class number ( C ) , piece size ( L/T ) } as { 3 , 0.2 } for CricketX , { 4 , 0.2 } for UWaveGestureLibraryAll , { 5 , 0.35 } for DodgerLoopDay , { 6 , 0.4 } for InsectWingbeatSound , { 4 , 0.2 } for MFPT , and { 4 , 0.2 } for XJTU '' - It is not clear how these choices for hyperparameters are arrived at , or what `` we set '' and `` moderately difficult pretext task '' mean . Also , the ablation study is more of a sensitivity analysis of the choice of values for C and L/T . It is not clear what happens if intra-temporal relation reasoning task or the inter-sample relation reasoning task is removed from SelfTime . Does that make SelfTime same as one of the other baselines , e.g . `` Relation '' ? Since most of the baselines used are adaptations from image domain , it is difficult to gauge where the proposed approach stands w.r.t.time-series specific methods like [ 1,2 ] . 2.For CricketX , DLD , and XJTU datasets , SelfTime performs significantly better than the supervised learning model ( Table 2 ) . How does one explain this observation as supervised methods would typically perform better or at least as good as the self-supervised methods ? 3.The authors observe that `` we find that the composition from a magnitude-based transformation ( e.g.scaling , magnitude warping ) and a time-based transformation ( e.g.time warping , window slicing ) facilitates the model to learn more useful representations . '' As per the description , it seems that this observation is based on analysis of just one dataset ( CricketX ) and on one algorithm ( SelfTime ) . I think a more thorough description and evaluation across datasets and baselines could be useful . 4.In the ablation results , it would help to see results across all datasets when using only inter-sample or intra-temporal losses . Also , the results in Fig.5 and Fig.6 are for only one dataset , and that too different ones : CricketX for Fig.5 and UGLA for Fig.6.Do the observations from Fig.5 and 6 hold across datasets ? Similarly , what motivates the particular pairing of source -- > target for the Domain Transfer Evaluation ? Given six datasets , several other combinations are possible - does this observation hold across all such combinations ? 5.The authors loosely mention that `` more augmentation results in better performance '' on the basis of references from existing literature . Though this might hold in other domains , an evaluation of the same for this setting would be useful to claim the same in the given context . Given the above , a more thorough and consistent description of the contribution in light of recent related work and empirical evaluation is needed . Given works like [ 1,2,3 ] , the contribution of the paper is limited to defining the intra-temporal task , but the same has not been evaluated carefully enough . The write-up can be improved at several places , e.g . : Eqns.1 and 2 need a minus sign to be called as losses ? Is i=j valid for the positive inter-sample relation pair ? If not , Eqn . 1 might need an update . fire -- > fair ? $ z_n $ is bold at a few places and not at other limited amount training samples Or for video data there is not enough feature we generate from its transformation counterpart and another individual sample as the positive and negative samples respectively In the experiment , we achieve new state-of-the-art results , and outperforms existing methods by a significant margin on multiple real-world time series datasets for classification task a intra-temporal Firstly , takes the original time series signals and their sampled time pieces as the inputs show that both two kinds of relation reasoning will drop the evaluation performance t-SNE visualization of the learnt feature . References : [ 1 ] Unsupervised scalable representation learning for multivariate time series . Franceschi et . al.NeurIPS'19 [ 2 ] Multi-task Self-Supervised Learning for Human Activity Detection . Saeed et.al.PACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2019 . [ 3 ] Self-supervised relational reasoning for representation learning . Patacchiola and Storkey . NeurIPS'20 * * updating the score to 5 * *", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the comments and questions ! Please find below our answers . * * On difference between relation reasoning loss and contrastive loss : * * Different from the pair-wise distance based contrastive loss [ 1 ] and cross-entropy based sample classification loss [ 2,3 ] in the traditional self-supervised models , inter-sample relation reasoning [ 4 ] leverages a sample-relation based loss for binary positive-negative relation classification . Although inter-sample loss is a binary cross-entropy based loss , its optimized object is sample relation representation , not the sample representation as in those traditional methods like [ 2,3 ] . Also , we leverage binary cross-entropy based loss for more efficient binary relation learning compared with the pair-wise distance based contrastive loss [ 1 ] . The pairwise-distance based contrastive loss relies on a large number of negatives , which are difficult to obtain in mini-batch stochastic optimization , and SimCLR requires specialized optimizers ( e.g. , cosine annealing ) to stabilize the training at scale , however , the cross-entropy do not need the convoluted sample-mining heuristics , resulting in a more easy optimization process . Although cross-entropy does not explicitly involve pairwise distances , as demonstrated in [ 5 ] , it 's an upper bound on a new pairwise loss , which has a structure similar to various pairwise losses : it minimizes intra-class distances while maximizing inter-class distances . As a result , minimizing the cross-entropy can be seen as an approximate bound-optimization ( or Majorize-Minimize ) algorithm for minimizing this pairwise loss . Please refer [ 5 ] for more details about the relationship between cross-entropy loss and pairwise-distance loss . [ 1 ] Chen , Ting , et al . `` A simple framework for contrastive learning of visual representations . '' ICML'20 ( 2020 ) . [ 2 ] Multi-task Self-Supervised Learning for Human Activity Detection . Saeed et.al.PACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2019 . [ 3 ] Sarkar , Pritam , and Ali Etemad . `` Self-supervised learning for ecg-based emotion recognition . '' ICASSP 2020-2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) . IEEE , 2020 . [ 4 ] Patacchiola and Storkey . \u201c Self-supervised relational reasoning for representation learning. \u201d NeurIPS'20 [ 5 ] Boudiaf , Malik , et al . `` A unifying mutual information view of metric learning : cross-entropy vs. pairwise losses . '' European Conference on Computer Vision . Springer , Cham , 2020 . * * The authors seem to be unaware of papers like [ 1,2 ] } , and Introduction and Related Work sections suggest that self-supervised learning for time series has not been attempted earlier . * * We have added the missing related works [ 1 , 2 ] and other related self-supervised time series representation learning works in the Introduction and Related Work section . More details are shown in the marked parts of the revised version . In [ 1 ] , the paper follows word2vec \u2019 s intuition by assuming that the context of a subsequence of a time series should probably be close to the one of the same time series , and distant from the one of randomly chosen time series , since they are probably unrelated to the original time series \u2019 s context . Then , they use triplet loss to push pairs of ( subsequence , context ) and ( subsequence , random context ) to be linearly separable for unsupervised time series representation learning . Different from [ 1 ] , in our intra-temporal module , we attempt to capture the temporal pattern by reasoning the multi-level relation among time pieces sampled from the same time series , while [ 1 ] focuses on the binary relation among time pieces sampled from the different time series . In [ 2 ] , the paper proposes a Transformation Prediction Network , where a temporal convolutional neural network is trained jointly on different signal transformations to solve a problem of transformation recognition . In [ 2 ] , there is no explicitly modeling of temporal pattern , while in our intra-temporal module , we attempt to capture the temporal pattern by reasoning the temporal relation among time pieces sampled from each time series . [ 1 ] Unsupervised scalable representation learning for multivariate time series . Franceschi et . al.NeurIPS'19 [ 2 ] Multi-task Self-Supervised Learning for Human Activity Detection . Saeed et.al.PACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2019 ."}], "0": {"review_id": "qFQTP00Q0kp-0", "review_text": "==Update after rebuttal I will remain my score of weak rejection . = This paper presents to model both inter-sample and intra-temporal relations . The idea is naive but is easy to follow and reasonable . Experimental results support the effectiveness of the method . Strengths : The idea is reasonable and the application is important . Weakness : 1 . The notations are messy . The author should consider clean up some unused notations for better readability . Especially for section 3.1 , I spend quite a few time thoroughly digest the notations . The explanation in section 3.2 is even scarier . I can easily understand the concept from the intro and Figure 1 , yet I spend much more time reading sections 3.1 and 3.2 . 2.The presentation of the experimental section is also hard to follow . Specifically , what is the message conveyed in Figure 5 ? I would suggest the author highlight the region that we can pay attention to . Nonetheless , why only considering the composition of two augmentations ? Instead of providing a matrix that considers all the combinations between two different augmentations , why not providing a table summarizing the sets of different combinations of augmentations ? The sets of combinations can be the sets that the author likes us to focus on . 3.An important paper [ 1 ] is missing . The author should discuss this missing reference . Overall , I am leaning positive toward this paper , yet I feel the presentation can be greatly improved . I am giving the score 5 for now , and I may update the score after the rebuttal . [ 1 ] Temporal Relational Reasoning in Videos , Zhou et al. , ECCV 2018 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * On presentation of method parts : * * We have revised Section 3 according to the review 's suggestion . In the revised version , we clean up the unused notations and rewrite some sentences to make the description more clearer . More details are shown in the marked parts in the revised version . * * What is the message conveyed in Figure 5 ? * * Figure 5 shows the experimental results about the impact of different data augmentations and relation modules . Firstly , to evaluate the impact of different data augmentations , as similar to SimCLR [ 1 ] , we conduct linear evaluation under individual or composition of data augmentations including several common time series augmentations such as magnitude domain based transformations such as jittering ( Jit . ) , cutout ( Cut . ) , scaling ( Sca . ) , magnitude warping ( M.W . ) , and time domain based transformations such as time warping ( T.W . ) , window slicing ( W.S . ) , window warping ( W.W. ) . We observe that the composition of different data augmentations is crucial for learning useful representations . For example , inter-sample relation reasoning is more sensitive to the augmentations , and performs worse under Cut. , Sca. , and M.W . augmentations , while intra-temporal relation reasoning is less sensitive to the manner of augmentations , although it performs better under the time domain based transformation . Secondly , to evaluate the effectiveness of different relation modules for ablation study , we compare the linear evaluation results on different modules including Inter-sample Relation , Intra-temporal Relation and their combination ( SelfTime ) . We find that , by combining both the inter-sample and intra-temporal relation reasoning , the proposed SelfTime achieves better performance , which demonstrates the effectiveness of considering different levels of relation for time series representation learning . Similar experimental conclusions also hold on for other datasets . More experimental results on the other five datasets for evaluation of the impact of different relation modules and data augmentations are shown in Appendix F. * * Why only considering the composition of two augmentations ? * * As similar to SimCLR [ 1 ] , to better understand the effects of individual data augmentations and the importance of augmentation composition , we conduct linear evaluation when applying augmentations individually or in pairs . Previous studies show that those contrastive learning [ 1 ] or inter-sample relation reasoning [ 2 ] based methods are sensitive to the augmentation strategies on image data , therefore , it 's also necessary to evaluate it on time series data . Here , we use several common time series augmentations such as magnitude domain based transformations such as jittering ( Jit . ) , cutout ( Cut . ) , scaling ( Sca . ) , magnitude warping ( M.W . ) , and time domain based transformations such as time warping ( T.W . ) , window slicing ( W.S . ) , window warping ( W.W. ) . We observe that the composition of different data augmentations is crucial for learning useful time series representations . For example , inter-sample relation reasoning is more sensitive to the augmentations , and performs worse under Cut. , Sca. , and M.W . augmentations , while intra-temporal relation reasoning is less sensitive to the manner of augmentations , although it performs better under the time domain based transformation . In summary , our main purpose is to evaluate that the impact of different augmentation composition is important , not the purpose of the best composition for evaluation because there could be plenty of choices . From the experimental results , we can obviously find that inter-sample relation module is more sensitive to augmentations while intra-temporal relation module does not , while intra-temporal module performs better than the inter-sample module in most cases , which provides our more insights into pretext task designing on time series data . Therefore , we believe that conducting augmentations in pairs is enough to support our purpose . [ 1 ] Chen , Ting , et al . `` A simple framework for contrastive learning of visual representations . '' ICML'20 ( 2020 ) . [ 2 ] Self-supervised relational reasoning for representation learning . Patacchiola and Storkey . NeurIPS'20"}, "1": {"review_id": "qFQTP00Q0kp-1", "review_text": "* * Summary * * This paper presents a general Self-supervised Time Series representation learning framework . It explores the inter-sample relation reasoning and intra-temporal relation reasoning of time series to capture the underlying structure pattern of the unlabeled time series data . The proposed method achieves new state-of-the-art results and outperforms existing methods by a significant margin on multiple real-world time-series datasets for the classification tasks . * * Contributions * * 1 . The paper is well written and easy to follow . The organization is good . 2.The architecture is well motivated . It is reasonable to use unsupervised temporal relations to learn video features . 3.The qualitative results are numerous , insightful , and convincing on multiple datasets . The authors conduct extensive experiments to demonstrate the effectiveness of the proposed method , including inter-sample relation reasoning and intra-temporal relation reasoning . * * Details * * 1 . The novelty of the proposed method . The Inter-sample relation reasoning is very similar to SimCLR , which also maximizes agreement between different views of augmentation from the same sample via a contrastive loss . Considering this , the novelty is relatively incremental . 2.Additional video recognition experiments . The used dataset is small-scale that makes the task simple . I would have wanted to see results on large-scale classification tasks , such as video action classification . The performance of action classification is closely related to the temporal feature modeling . So the effects on this task can make the proposed method more convincing . * * Conclusion * * overall , this paper proposes an interesting architecture for self-supervised temporal modeling . But the novelty is relatively limited compared to the recent SimCLR work . And it requires additional experiments on harder video classification task and datasets to show the effects and robustness of the proposed method . * * After rebuttal * * Thanks for the detailed response . This paper can be seen as an interesting attempt to use self-supervised on time series data . Although the basic idea is similar to SimCLR , It is still interesting work considering the computation complexity and new loss function . So I update my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the comments and questions ! Please find below our answers . * * On novelty of the proposed method : * * 1 . The difference between SelfTime and SimCLR : Relation reasoning is far different from the contrastive learning based methods such as SimCLR . There are two main differences between them : 1.1 . Loss function : SimCLR uses the pairwise-distance based contrastive loss function as objective to guide the inter-sample relation reasoning for time series representation learning , while in SelfTime , we use the cross-entropy based loss function as objective to guide the reasoning of different levels of entity relationship ( inter-sample relation and intra-temporal relation ) for learning of useful time series representation . The pairwise-distance based contrastive loss relies on a large number of negatives , which are difficult to obtain in mini-batch stochastic optimization , and SimCLR requires specialized optimizers ( e.g. , cosine annealing ) to stabilize the training at scale , however , the cross-entropy do not need the convoluted sample-mining heuristics , resulting in a more easy optimization process . Although cross-entropy does not explicitly involve pairwise distances , as demonstrated in [ 1 ] , it 's an upper bound on a new pairwise loss , which has a structure similar to various pairwise losses : it minimizes intra-class distances while maximizing inter-class distances . As a result , minimizing the cross-entropy can be seen as an approximate bound-optimization ( or Majorize-Minimize ) algorithm for minimizing this pairwise loss . Please refer [ 1 ] for more details about the relationship between cross-entropy loss and pairwise-distance loss . [ 1 ] Boudiaf , Malik , et al . `` A unifying mutual information view of metric learning : cross-entropy vs. pairwise losses . '' European Conference on Computer Vision . Springer , Cham , 2020 . 1.2.Computation complexity : SelfTime is a more efficient algorithm compared with the traditional contrastive learning models such as SimCLR . The complexity of SimCLR is $ O ( N^ { 2 } K^ { 2 } ) $ , while the complexity of SelfTime is $ O ( NK^ { 2 } ) +O ( NK ) $ , where $ O ( NK^ { 2 } ) $ is the complexity of inter-sample relation reasoning module , and $ O ( NK ) $ is the complexity of intra-temporal relation reasoning module . It can be seen that SimCLR scales quadratically in both training size $ N $ and augmentation number $ K $ . However , in SelfTime , inter-sample relation reasoning module scales quadratically with the number of augmentations $ K $ , and linearly with the training size $ N $ , and intra-temporal relation reasoning module scales linearly with both augmentations and training size . 2.We would like to reclaim the contributions of our work here : Our main contributions are three-fold : ( 1 ) we present a general self-supervised time series representation learning framework by investigating different levels of relations of time series data including inter-sample relation and intra-temporal relation . ( 2 ) We design a simple and effective intra-temporal relation sampling strategy to capture the underlying temporal patterns of time series . ( 3 ) We conduct extensive experiments on different categories of real-world time series data , and systematically study the impact of different data augmentation strategies and temporal relation sampling strategies on self-supervised learning of time series . By comparing with multiple state-of-the-art baselines , experimental results show that SelfTime builds new state-of-the-art on self-supervised time series representation learning ."}, "2": {"review_id": "qFQTP00Q0kp-2", "review_text": "The paper proposes an approach for self-supervised time series representation learning by using inter-sample and intra-temporal relational reasoning . The paper builds upon the existing ideas on relational reasoning [ 3 ] and self-supervised learning to train models from unlabeled data . Self-supervised learning for time series is still under-explored and this paper attempts to bridge this gap in time series literature . The key novelty of the paper as claimed by the authors is in the design of inter-sample and intra-temporal tasks and loss functions to learn a feature extractor from unlabeled data . This idea of designing pretext tasks using global-sample structure and local-temporal structure and adapting it for time series is interesting . However , the inter-sample loss function seems to be a straightforward application of ideas in self-supervised learning literature that rely on various augmentations to create positive and negative pairs of samples . The intra-temporal task is defined where the distance between subsequences ( referred to as time pieces in the paper ) of a time series is used to create a classification task . This is a potentially novel ( albeit incremental ) aspect of the approach but the empirical evaluations ( as detailed below ) fail to highlight the impact of the same on performance clearly . Furthermore , the idea of using subsequences to define self-supervised ( intra-temporal ) tasks for time series has been explored earlier , e.g.in [ 1,2 ] .The authors seem to be unaware of papers like [ 1,2 ] , and Introduction and Related Work sections suggest that self-supervised learning for time series has not been attempted earlier . Apart from lack of clarity on novelty and contribution of the work , I have following concerns regarding empirical evaluation : 1 . The ablation studies show the effect of number of classes and the length of subsequences ( pieces ) on linear evaluation accuracy ( Linear ACC ) for the downstream classification tasks . However , the sensitivity of results to the hyperparameters of intra-temporal task is reported only on CricketX . Do the same results hold on other datasets ? Also , the results on downstream task seem to be very sensitive to the choice of parameters C and L/T . The authors state that `` In the experiment , to select a moderately difficult pretext task for different datasets , we set { class number ( C ) , piece size ( L/T ) } as { 3 , 0.2 } for CricketX , { 4 , 0.2 } for UWaveGestureLibraryAll , { 5 , 0.35 } for DodgerLoopDay , { 6 , 0.4 } for InsectWingbeatSound , { 4 , 0.2 } for MFPT , and { 4 , 0.2 } for XJTU '' - It is not clear how these choices for hyperparameters are arrived at , or what `` we set '' and `` moderately difficult pretext task '' mean . Also , the ablation study is more of a sensitivity analysis of the choice of values for C and L/T . It is not clear what happens if intra-temporal relation reasoning task or the inter-sample relation reasoning task is removed from SelfTime . Does that make SelfTime same as one of the other baselines , e.g . `` Relation '' ? Since most of the baselines used are adaptations from image domain , it is difficult to gauge where the proposed approach stands w.r.t.time-series specific methods like [ 1,2 ] . 2.For CricketX , DLD , and XJTU datasets , SelfTime performs significantly better than the supervised learning model ( Table 2 ) . How does one explain this observation as supervised methods would typically perform better or at least as good as the self-supervised methods ? 3.The authors observe that `` we find that the composition from a magnitude-based transformation ( e.g.scaling , magnitude warping ) and a time-based transformation ( e.g.time warping , window slicing ) facilitates the model to learn more useful representations . '' As per the description , it seems that this observation is based on analysis of just one dataset ( CricketX ) and on one algorithm ( SelfTime ) . I think a more thorough description and evaluation across datasets and baselines could be useful . 4.In the ablation results , it would help to see results across all datasets when using only inter-sample or intra-temporal losses . Also , the results in Fig.5 and Fig.6 are for only one dataset , and that too different ones : CricketX for Fig.5 and UGLA for Fig.6.Do the observations from Fig.5 and 6 hold across datasets ? Similarly , what motivates the particular pairing of source -- > target for the Domain Transfer Evaluation ? Given six datasets , several other combinations are possible - does this observation hold across all such combinations ? 5.The authors loosely mention that `` more augmentation results in better performance '' on the basis of references from existing literature . Though this might hold in other domains , an evaluation of the same for this setting would be useful to claim the same in the given context . Given the above , a more thorough and consistent description of the contribution in light of recent related work and empirical evaluation is needed . Given works like [ 1,2,3 ] , the contribution of the paper is limited to defining the intra-temporal task , but the same has not been evaluated carefully enough . The write-up can be improved at several places , e.g . : Eqns.1 and 2 need a minus sign to be called as losses ? Is i=j valid for the positive inter-sample relation pair ? If not , Eqn . 1 might need an update . fire -- > fair ? $ z_n $ is bold at a few places and not at other limited amount training samples Or for video data there is not enough feature we generate from its transformation counterpart and another individual sample as the positive and negative samples respectively In the experiment , we achieve new state-of-the-art results , and outperforms existing methods by a significant margin on multiple real-world time series datasets for classification task a intra-temporal Firstly , takes the original time series signals and their sampled time pieces as the inputs show that both two kinds of relation reasoning will drop the evaluation performance t-SNE visualization of the learnt feature . References : [ 1 ] Unsupervised scalable representation learning for multivariate time series . Franceschi et . al.NeurIPS'19 [ 2 ] Multi-task Self-Supervised Learning for Human Activity Detection . Saeed et.al.PACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2019 . [ 3 ] Self-supervised relational reasoning for representation learning . Patacchiola and Storkey . NeurIPS'20 * * updating the score to 5 * *", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the comments and questions ! Please find below our answers . * * On difference between relation reasoning loss and contrastive loss : * * Different from the pair-wise distance based contrastive loss [ 1 ] and cross-entropy based sample classification loss [ 2,3 ] in the traditional self-supervised models , inter-sample relation reasoning [ 4 ] leverages a sample-relation based loss for binary positive-negative relation classification . Although inter-sample loss is a binary cross-entropy based loss , its optimized object is sample relation representation , not the sample representation as in those traditional methods like [ 2,3 ] . Also , we leverage binary cross-entropy based loss for more efficient binary relation learning compared with the pair-wise distance based contrastive loss [ 1 ] . The pairwise-distance based contrastive loss relies on a large number of negatives , which are difficult to obtain in mini-batch stochastic optimization , and SimCLR requires specialized optimizers ( e.g. , cosine annealing ) to stabilize the training at scale , however , the cross-entropy do not need the convoluted sample-mining heuristics , resulting in a more easy optimization process . Although cross-entropy does not explicitly involve pairwise distances , as demonstrated in [ 5 ] , it 's an upper bound on a new pairwise loss , which has a structure similar to various pairwise losses : it minimizes intra-class distances while maximizing inter-class distances . As a result , minimizing the cross-entropy can be seen as an approximate bound-optimization ( or Majorize-Minimize ) algorithm for minimizing this pairwise loss . Please refer [ 5 ] for more details about the relationship between cross-entropy loss and pairwise-distance loss . [ 1 ] Chen , Ting , et al . `` A simple framework for contrastive learning of visual representations . '' ICML'20 ( 2020 ) . [ 2 ] Multi-task Self-Supervised Learning for Human Activity Detection . Saeed et.al.PACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2019 . [ 3 ] Sarkar , Pritam , and Ali Etemad . `` Self-supervised learning for ecg-based emotion recognition . '' ICASSP 2020-2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) . IEEE , 2020 . [ 4 ] Patacchiola and Storkey . \u201c Self-supervised relational reasoning for representation learning. \u201d NeurIPS'20 [ 5 ] Boudiaf , Malik , et al . `` A unifying mutual information view of metric learning : cross-entropy vs. pairwise losses . '' European Conference on Computer Vision . Springer , Cham , 2020 . * * The authors seem to be unaware of papers like [ 1,2 ] } , and Introduction and Related Work sections suggest that self-supervised learning for time series has not been attempted earlier . * * We have added the missing related works [ 1 , 2 ] and other related self-supervised time series representation learning works in the Introduction and Related Work section . More details are shown in the marked parts of the revised version . In [ 1 ] , the paper follows word2vec \u2019 s intuition by assuming that the context of a subsequence of a time series should probably be close to the one of the same time series , and distant from the one of randomly chosen time series , since they are probably unrelated to the original time series \u2019 s context . Then , they use triplet loss to push pairs of ( subsequence , context ) and ( subsequence , random context ) to be linearly separable for unsupervised time series representation learning . Different from [ 1 ] , in our intra-temporal module , we attempt to capture the temporal pattern by reasoning the multi-level relation among time pieces sampled from the same time series , while [ 1 ] focuses on the binary relation among time pieces sampled from the different time series . In [ 2 ] , the paper proposes a Transformation Prediction Network , where a temporal convolutional neural network is trained jointly on different signal transformations to solve a problem of transformation recognition . In [ 2 ] , there is no explicitly modeling of temporal pattern , while in our intra-temporal module , we attempt to capture the temporal pattern by reasoning the temporal relation among time pieces sampled from each time series . [ 1 ] Unsupervised scalable representation learning for multivariate time series . Franceschi et . al.NeurIPS'19 [ 2 ] Multi-task Self-Supervised Learning for Human Activity Detection . Saeed et.al.PACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2019 ."}}