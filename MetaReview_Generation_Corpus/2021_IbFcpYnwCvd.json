{"year": "2021", "forum": "IbFcpYnwCvd", "title": "The Logical Options Framework", "decision": "Reject", "meta_review": "This was a borderline paper with a split recommendation from the reviewers.  The authors took great care to answer the reviewer questions in detail, and the clarity and precision of the technical exposition was strengthened.  However, substantial technical content was added to the paper during the rebuttal process, which the reviewers were not able to fully and properly assess.\n\nOverall, this is worthwhile research, but the paper is still maturing.  The contribution was perceived as incremental in light of previous work using LTL and FSAs in RL, despite the authors extensively re-explaining the significance of the work in the rebuttal.  A resubmission is more likely to resonate with reviewers and ultimately achieve higher impact.\n\nFor completeness, it would help to also briefly acknowledge and compare to hierarchical RL work that also seeks to capture composable subtask structures, such as:\n\nSohn et al. \"Hierarchical reinforcement learning for zero-shot generalization with subtask dependencies\", NeurIPS-2018\n\nSohn et al. \"Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies\", ICLR-2020", "reviews": [{"review_id": "IbFcpYnwCvd-0", "review_text": "This paper is on a new RL framework that leverages logical reasoning to improve the learning performance of RL agents . In particular , the knowledge is encoded using LTL , and includes both safety knowledge ( used for reward function definition ) and liveness knowledge ( used for constructing FSA ) . THe developed framework has been evaluated using tasks in both discrete and continuous domains , where the RL agent was realized using Q-learning and PPO respectively . The framework was compared with baselines including another LTL-based RL methods ( Reward Machines ) . Results show that LOF performed better than the baseline methods in learning rate and policy quality ( in most cases ) . The whole idea of leveraging human knowledge to improve RL agent 's learning performance makes senses . The main concern is that the developed framework looks quite incremental in comparison to the methods from the literature . For instance , there are already methods in the literature using LTL to bias the reward function , e.g. , the papers that improved the Reward Machines work . As a result , the novelty of the safety knowledge is rather incremental . At the same time , the `` liveness '' part is quite similar to those methods that plan at a symbolic level to guide RL agents , such as the following : Lyu D , Yang F , Liu B , Gustafson S. SDRL : interpretable and data-efficient deep reinforcement learning leveraging symbolic planning . 2019 It 's nice to see the discussions on satisfaction , optimality , and composability . The analysis on optimality is the most important , whereas the other two points are relatively trivial due to the LTL-based FSA . But it turned out the developed approach , LOF , does not guarantee any desired properties beyond those from the literature . In particular , the LTL-based FSA can not guarantee global optimality , where the baseline of Reward Machines does . The reviewer is curious about if Reward Machines can be considered an ablation of LOF , where the `` liveness '' component is disabled ( and `` safety '' is retained ) . If not , what are the differences between the two methods ? Knowledge-based RL can go beyond HRL methods . For instance , the following survey paper summarizes a few other ways of leveraging human knowledge in RL . Some discussions can help improve the related work section . Zhang S , Sridharan M. A Survey of Knowledge-based Sequential Decision Making under Uncertainty . 2020 In general , it 's not surprising to see human knowledge is useful in RL . However , human knowledge is not always correct . It will be nice to evaluate the situations where human knowledge in incomplete or inaccurate , or at least some discussions can be helpful . - The reviewer appreciates the response in detail . The new figures ( Figs 4-7 ) are helpful for demonstrating the differences between LOF and RM , while also demonstrating that their outputs can be similar or the same in many cases . The reconciliation between optimality and composability is a nice feature . Overall , the reviewer still feels positive on this work .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to the reviewer for the detailed review of our paper . 1 ) We want to emphasize that improving learning performance is not one of the goals of our method , although it can be a side effect . Rather , our goal was to achieve of satisfaction , optimality , and composability . We agree that in terms of \u201c satisfaction \u201d and \u201c optimality \u201d alone , our work is not novel , as there are many other algorithms that achieve satisfaction and optimality , including RM . However , we believe that our work makes a significant contribution in terms of achieving satisfaction and optimality along with composability . As AnonReviewer2 points out , \u201c This is a very good general idea , and as far as I am aware other works have not studied how one might compose previously learned sub-task policies to accomplish new tasks described by automata in this way \u201d ( although they are concerned about the generality of our work ; in the revisions we hope we have addressed those concerns ) . In order to achieve composability , we make a big shift in perspective \u2013 whereas other papers achieve satisfaction/optimality by shaping the reward function using the automaton , we instead take a planning perspective , where we find an optimal policy over the automaton . Therefore , if the automaton is changed/modified , we can find a new optimal policy by replanning over the new automaton . ( This is a similar approach to that taken in SDRL and other symbolic planning methods , but it differs in that the meta-controller of SDRL does not have the property of full composability that we describe in our work.But we are grateful for the reference and will include it in the related work ) . With regard to the \u201c liveness \u201d and \u201c safety \u201d properties , we believe that we have made a useful contribution in identifying these two properties as distinct components of a planning algorithm . Most other papers in the LTL+RL literature do not distinguish between liveness and safety properties and plan over a single , combined automaton . However , in this paper we point out that the safety property is mostly useful for defining the reward function , and the liveness property is useful for planning . Therefore , the safety property is incorporated into the reward function for training the logical options , and the liveness property is translated into an FSA so that an optimal policy can be found over the FSA . We believe that this distinction is a useful contribution for the RL+LTL literature ( although it is widely known in the formal methods community ) . 2 ) With regard to satisfaction , optimality , and composability , we believe that our most important contribution is that we have found a way to reconcile optimality and composability , which in general are opposing goals . Composability requires the policies to be flexible in a way that makes optimality difficult to achieve ( hence the existence of many optimal methods that are not composable , and many composable methods that are not optimal ) . Furthermore , we want to emphasize that our proof of optimality guarantees global optimality under the conditions that we state . Although these conditions are stricter than the conditions that RM requires , we strove to find the loosest possible assumptions for achieving optimality while also maintaining full composability . We point out in the paper that in continuous state spaces it is impossible to meet our condition that a subgoal be associated with a single state ; however , by defining the subgoal to be a small region as we do in our paper , it is possible to achieve near-optimal policies . We acknowledge that our work will not have the final say in this discussion , as future papers may be able to loosen our assumptions , but we hope that our work can kickstart the discussion ."}, {"review_id": "IbFcpYnwCvd-1", "review_text": "Summary of the work : The authors propose the Logical Options Framework ( LOF ) a framework for reasoning over high-level plans and learning low-level control policies . This framework uses Linear Temporal Logic ( LTL ) to specify properties ( high-level tasks ) in terms of propositions . The authors propose a framework in which a separate sub-task policy is learnt to accomplish each such sub-task proposition . These low-level control policies may be reused , without further training , to accomplish new high-level tasks by performing value-iteration in the proposed Hierarchical SMDP . Experimental results demonstrate the method \u2019 s effectiveness for several tasks and experimental domains . Quality and Clarity : The paper does a good job at intuitively describing the value that the proposed \u201c LOF \u201d framework provides ( satisfaction , optimality , and composability ) . In general , the language used in the paper is clear and easy to read . However , some of the technical aspects in the paper are difficult to follow concretely . There are various vague statements and definitions throughout the paper . Typos , inconsistencies , and missing explanations/discussion of seemingly important notions make some of the presented ideas imprecise . This raised several questions from me on the general applicability of the framework ( as it is presented in this paper ) beyond the presented experimental tasks , and on some of the specifics of Theorem 3.1 . Given that the focus of this paper is on the development a new framework for RL , fixing these issues is crucial . I have included a more detailed list of feedback for the authors at the end of the review . Originality and Significance : The proposed method defines \u201c logical options \u201d \u2013 sub-task policies whose goal is to trigger propositions that cause transitions in the automata-based description of the high-level task . Temporally extended tasks may then be accomplished by deploying the appropriate sub-task policy at the appropriate FSA state . This idea seems very similar to learning with Reward Machines . This similarity is acknowledged in the paper . The main conceptual difference between LOF and RMs , is that LOF proposes to learn policies that trigger transitions in an automaton , whereas learning with RMs instead learns separate policies for each state of the automaton . This difference means that in the LOF framework , previously learnt sub-task policies can be reused by composing them to complete new high-level tasks . This is not true for RMs . However , the policies learnt by LOF are not guaranteed to be optimal unless certain conditions are met . This is a very good general idea , and as far as I am aware other works have not studied how one might compose previously learned sub-task policies to accomplish new tasks described by automata in this way . However , I am concerned that the significance of the work might be limited to the specific examples of the paper . For example , by associating a cost with each safety proposition , instead of forming the safety FSA associated with the safety property , it seems that the only safety properties that can be expressed by LOF are those of the form : \u201c avoid these states \u201d . Because this is a significant limitation in comparison with all possible safety properties , an explicit discussion of the safety properties that can be represented by LOF would help the reader to understand exactly what problems LOF can solve . Furthermore , it is unclear how the composability results change in the presence of new and/or changed safety properties . I elaborate further on this point at the end of the next section of the review . Questions surrounding theorem 3.1 The main theoretical result of this paper is that under appropriate conditions , the hierarchical policy learned through the LOF framework will be equivalent to the optimal policy ( optimal when planning is allowed with respect to low-level actions in the FSA-MDP product ) . The proof follows the logic that given the appropriate reward functions , the option policies will always reach the states corresponding to their sub-goals , and that the meta-policy will select a sequence of sub-goals that always reaches the FSA goal state . The proposed method assumes that if an option corresponding to a particular sub-goal is selected by the meta-policy , then no other sub-goal proposition will be triggered before that option is complete . My concern with this assumption is as follows : what happens in scenarios in which an option policy passes through a state associated with a different sub-goal before reaching its own sub-goal ? Would it then be possible for the meta-policy to select an option corresponding to a particular sub-goal , but for a different sub-goal proposition to be triggered first , causing an unwanted transition in the FSA ? For example , suppose in the delivery domain we could complete a task by either of the following sequences of sub-goals : \u201c a then b \u201d , or \u201c b then a then c \u201d . If \u201c a then b \u201d is less costly than \u201c b then a then c \u201d , the optimal meta-policy should result in \u201c a then b \u201d . Furthermore , assume that \u201c b \u201d lies directly between \u201c a \u201d and the initial state of the agent . The optimal policy for sub-goal proposition \u201c a \u201d would be to move directly to \u201c a \u201d , which would cause it to first pass through \u201c b \u201d . This would cause the agent to be forced to follow \u201c b then a then c \u201d even if the meta-policy first chose proposition \u201c a \u201d . Meanwhile , the optimal policy allowing low-level planning in the FSA-MDP product space would be to move to \u201c a \u201d while actively avoiding \u201c b \u201d , and then to proceed to \u201c b \u201d . Also , it is unclear whether consideration of obstacles and safety propositions are included in the consideration of theorem 3.1 . It is unclear what the reward functions corresponding to the \u201c goal-conditioned policy \u201d is . If R_S is included in the reward function used to train sub-task policies , then these policies will learn to avoid obstacles . When we compose these sub-task policies for a new task with potentially different safety properties , the policies we learned previously could be sub-optimal in the new scenario . Experimental results : The experimental results demonstrate the effectiveness of the framework for the chosen experimental tasks . I like the videos provided in the supplementary materials ; they are nice visualizations of how the sub-policies are strung together to complete the overall task . From the paper \u2019 s description , it is unclear how LOF-QL is implemented . How does this algorithm make use of the liveness FSA if it does not have access to the FSA \u2019 s transition function ? Also , I am concerned that the comparison against RMs may not be fair . As stated in the paper , the RM-based method is only rewarded when the final RM state has been reached . Conversely , the reward for the LOF method is much denser . I do not see why the RM-based method could not also be rewarded for each \u201c good \u201d transition . Finally , the experiments include the \u201c can \u201d proposition which represents when one of the sub-goals has been canceled . It is unclear how the labeling function , which is defined to map specific MDP states to individual propositions , could return this proposition . If the proposition is returned randomly , the formalisms of the labeling function need to be altered to include these types of randomly occurring propositions . Pros > The general idea of the paper is good . I like the idea of pre-learning sub-task policies , and then of using automata-based task descriptions to find meta-policies that accomplish new tasks without additional training . This is the \u201c composability \u201d described in the paper . > The authors do a good job of intuitively describing the benefits that this type of method could provide over competing algorithms . > The experimental tasks ( particularly the videos of the tasks being completed ) provide a nice visualization/demonstration of the ideas of the paper . Cons > The technical aspects of the paper are hard to follow concretely . > Some of the theory seems vague and the paper has various typos and inconsistencies . This could lead to reader misinterpretations and/or author mistakes . > The method \u2019 s applicability seems to be somewhat limited to specific types of tasks , without explicit discussion of exactly what types of tasks can be solved . > The questions raised surrounding theorem 3.1 need to be clarified . Further detailed feedback : > How are the logical options learned ? In particular , how are T_ { o_p } ( s \u2019 |s ) and R_ { o_p } ( s ) learned . These are the components of the options that are subsequently used to find the metapolicy . Are they estimated by averaging values over rollouts of the learned policy ? What if the environment is highly stochastic and their values vary greatly across different rollouts ? > The assignment for T_ { o_p } ( s \u2019 |s ) on line 11 of algorithm 1 seems to assume there is a fixed k in which p will be reached . Would this always be the case if the environment is stochastic ? This also seems to be a different definition than is given in equation 2 . > In equation 1 , R_ { o } ( s ) is defined as the expected cumulative reward of options given that the option is initiated in state s at time t and ends after k time steps ; shouldn \u2019 t this make R_ { o } ( s ) a function of k as well ? > In the definition of T_ { o } ( s \u2019 |s ) , what is p ( s \u2019 , k ) ? I assume this is the probability , under the option \u2019 s policy , of reaching state s \u2019 after k timesteps from state s. If this is the case , p ( s \u2019 , k ) should be defined . > In line 9 of the Algorithm 1 , T_ { P } ( s , p ) = 1 is written . On line 11 of Algorithm 1 , T_ { P } ( s ) = p is written instead , but both have the same meaning : proposition p is returned by the labeling function from state s. > The liveness FSA \u2019 s transition function is defined and treated as a probability distribution . However , the automaton \u2019 s transitions appear to be deterministic in the presented examples . If there is a particular reason for it to be defined as a transition distribution , some discussion would be helpful for the reader . > In the definition of a Hierarchical SMDP , the transition function is defined as a cartesian product of the environment transition distribution , the labeling function , and the FSA transition function . The precise meaning of this notation is unclear . A brief description of how a transition proceeds would be very helpful for the reader to understand the order in which arguments are passed to these three functions . Does the environment transition first , and the proposition of the new environment state cause the FSA to transition ? > In section 3.1 and in appendix A.1 , the author writes that \u201c if the optimal option policy equals the goal-conditioned policy , the policy will always reach the sub-goal \u201d . This will not be the case if the environment is stochastic and it is possible for the agent , under ANY policy , to slip into a state from which the sub-goal is not reachable . > Point 2 of the contributions outlined in section 1.1 states that the options can be composed to solve new tasks with only 10-50 retraining steps . This wording is at odds with the abstract , which states that LOF \u2019 s learned policies can be composed to satisfy unseen tasks with no additional learning . > In Section 3 \u201c These may only appear in the liveness property and there may only be one instance of each subgoal. \u201d The second half of this sentence is unclear . Does this mean that the inverse image of each sub-goal proposition through the labeling function is a singleton set containing only one state ? > In section 3 \u201c Safety propositions are propositions such as \u2018 the agent received a call \u2019 that the agent has no control over or does not seek out. \u201d This sentence is unclear . It would help if you could express what it means for the agent to \u201c have control over \u201d or to \u201c seek out \u201d a proposition in terms of the MDP states , actions , and proposition labeling function .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for such a thoughtful and in-depth review . This review is really valuable to us and we have taken the feedback to heart . We believe that the reviewer has pointed out two major issues with our formulation that we have addressed in the revision . 1 ) The fact that our formulation in the paper does not cover the case where the safety property is non-trivial and the case where the LTL specification can \u2019 t be separated into independent liveness and safety specifications ( discussed in points 1 and 3 below ) . We have added a new section to the appendix ( Appendix A ) where we present a formulation that is much more general than the one presented in the paper . 2 ) The fact that our formulation of the proposition labeling function is inconsistent with randomly occurring safety propositions ( discussed in point 7 below ) . We added a new category of propositions called event propositions to address this concern . Furthermore , the reviewer has pointed out numerous other issues that we have attempted to address . We apologize for the length of this review and the fact that it has been split into so many comments , but the reviewer pointed out many really good and deep points that have taken a lot of space to address . 1 ) \u201c it seems that the only safety properties that can be expressed by LOF are those of the form : \u201c avoid these states \u201d \u201d \u2013 LOF is not limited to these types of safety properties . In the submission , we briefly mention this when we say , \u201c In our experiments , the safety property $ \\phi_ { safety } $ defines a reward function R_S that penalizes violations of safety rules . $ \\phi_ { safety } $ can be converted into an FSA $ \\mathcal { T } _S $ , and the reward function can be defined on the states of $ \\mathcal { T } _S $ . However , to limit the complexity of the formulation we assume that there is simply a reward ( cost ) associated with every safety proposition. \u201d However , we think that your concern should be addressed properly , so we have now included a more general formulation of LOF in Appendix A that can handle any safety property . We want to emphasize that LOF can handle any type of safety property , as long as the low-level policy-learning algorithm can accommodate it . In the general formulation , the safety property is translated into an automaton , and Reward Machines or other appropriate algorithms can be used to learn logical options that obey the safety property . 2 ) \u201c it is unclear how the composability results change in the presence of new and/or changed safety properties \u201d : Safety properties are not composable . In our definition of composability , we assume that the liveness property can change but not the safety property . We were not clear about this in the paper ; in our definition of composability in the introduction , we refer to \u201c tasks \u201d but should use the more specific term \u201c liveness property. \u201d We will edit the revision to be very clear about this ."}, {"review_id": "IbFcpYnwCvd-2", "review_text": "In this paper , the author proposes logic option framework for learning policies that satisfy the logic constraints . Temporal logic rules are converted into finite state machine and each of them is then converted to a learnable option , where the logic proposition is used as the reward function . Given the options , it searches for the optimal meta-policy over all the options . The LOF is tested on a gridworld and an OpenAI Gym benchmark and is compared against baselines that do not use rules . I find the idea of integrating logics into RL is an exciting direction for improving agent 's interpretability and the proposed LOF does have some novelties in applying the FOL rules . However , I 'm not familiar with the RL and option literature and it 's difficult for me to tell its original work from the existing ones on the RL side . Some questions for the author : Judging from Algorithm 1 , it seems the rules and predicates are pre-defined with the environment and their conversion into FSA and reward functions are also done with an existing method . Is there anything original in terms of rule/predicate learning or conversion for the proposed LOF ? In Algorithm 1 , each rule is converted to an option and learned independently . What does the LOF do if rules are correlated or have conflicts ? - Say , rule1 is a pre-requisite of rule2 : rule1 : `` go to dest1 '' and rule2 : '' if agent is in dest1 then go to dest2 , otherwise check rule1 '' - Or rule1 : `` go to dest1 asap '' whereas rule2 : `` do not violate the traffic rules '' - Does the LOF provide mechanism to address the above issues ? I 'm also concerned about the scalability of this method . It seems to me that the options need to be evaluated at each possible state in the environment . If the goals are fixed , probably one can store the value beforehand , but for a dynamic environment where the goal is changing , how much does it cost to update the reward function and learn the option policies ? And how well does it scale with the size of the state/action space ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to the reviewer for the questions and constructive feedback , and hope that we can clarify some of aspects of our work . 1 ) Most of the LTL+RL literature achieves satisfaction and ( sometimes ) optimality by using the automaton associated with the LTL specification to shape the reward function , including Reward Machines . During training , the environment keeps track of what state the automaton is in and rewards the agent whenever a ( favorable ) transition is achieved . This approach can guarantee satisfaction and optimality ; however , it is inherently not composable because the automaton rewards get baked into the learned value function/policy . If the automaton is changed , policy learning typically has to be done from scratch ( we have added Appendix Figures 4 , 5 , 6 , and 7 to illustrate how LOF differs from Reward Machines , the published work most similar to ours ) . Other work in symbolic planning is more similar to ours in that they learn high- and low-level controllers . However , these works do not take into account the specific tradeoffs between composability and optimality that we cover in our paper , and their high-level controllers are either not composable or only capable of limited composability . We will try to emphasize the differences between our work and the related work more in the revision . To address your question of if there is \u201c anything original in terms of rule/predicate learning or conversion for the proposed LOF \u201d : we assume that all aspects of the hierarchical semi-MDP are known , including the rules and propositions . Learning rules and propositions is a very exciting goal and definitely an avenue of future work ; in this paper , we have laid the groundwork for that type of future work by defining the formulation of the hierarchical SMDP and proposing an algorithm for finding a satisfying/optimal/composable policy over it . Our hierarchical SMDP formulation is unique in that it includes both a hierarchical action space ( as in the original options framework paper ) as well as a hierarchical state space/transition function/reward function ( which is common to many LTL+RL papers ) . 2 ) Rule conflicts are a topic that we didn \u2019 t have space to address in the initial submission , but we will talk about it in the revision . Can you elaborate more about your first example ? As stated , the rules will cause the agent to go to dest1 and then dest2 . But in general , if rules are logically incompatible , they will probably translate into a single-state automaton that is not an accepting state . For example , the formula \u201c ( ! a U b ) & ( ! b U a ) & G ! ( a & b ) & F a \u201d can be thought of as , \u201c Go to A , but you can \u2019 t go to A until you \u2019 ve gone to B , and you can \u2019 t go to B until you \u2019 ve gone to A \u201d . It results in a single-state automaton with no goal state . ( You can try inputting this formula into https : //spot.lrde.epita.fr/app/ to see for yourself ) . LOF is compatible with such an automaton ; the policy will simply terminate at the first time step . This is more an issue of user error ; hopefully the user will see that the formula does not make logical sense ( perhaps upon seeing that the automaton is trivial ) and correct the formula . We have included this discussion in Appendix D , \u201c Further Discussion \u201d . You make a good point with the second example \u2013 what if achieving the goal involves violating a safety constraint ? One of our assumptions is that the agent can achieve any subgoal from any state of the environment , so an environment that requires a violation of a hard safety constraint would violate our assumption . We believe this is a reasonable assumption to make because it is not fair to the agent to require the agent to achieve a task that is against the rules . However , under most conditions LOF will still \u201c work \u201d in such an environment . For example , in the formulation that we use in the paper , safety propositions are assigned negative but finite costs . As we discuss in the proofs of satisfaction/optimality , the cost of not satisfying the task is negative infinity , whereas the cost of violating the rules is negative but finite . Therefore , in our specific formulation , if the agent is in an environment where violating the safety rules is the only way to achieve the task , it will learn to violate the safety rules at the cost of a massive negative reward ( however , it will only learn this behavior in the limit of an infinite number of training steps ; limiting the length of a training episode to a reasonable number will prevent the agent from ever violating safety rules with large negative costs ) . We have included this discussion in Appendix D , \u201c Further Discussion \u201d . We hope that we have explained how these issues can be handled and how LOF would react to them . We are happy to clarify if you have more questions ."}, {"review_id": "IbFcpYnwCvd-3", "review_text": "Summary : The paper proposes a hiearchical RL framework augmented with linear temporal logic . Low-level policies are trained to reach a set of subgoals with penalty on violating safety constraints , allowing the high-level policy to adapt to different task by composing the low-level policies to reach a set of subgoals specified by an Linear Temporal Logic clause . My first comment is on technical novelty . Although the paper states that LOF takes a step beyond RM to enable compositionality , there is no empirical comparison or evaluation . The only experiment on evaluating compositionality ( c , d in Figure 2 ) does not compare LOF against RM . My second major comment is on technical exposition . The paper 's writing needs a lot of work , especially the method section . The method section started off by describing different concepts used by the framework , but it is unclear how these concepts constitute LOF . For example , how exactly is LTL used in LOF ? The introduction mentioned that LOF provides `` compositional '' property , but there is not a single hint of how might each piece of LOF lead to this property . It took me a few read to understand how the low-level reward might be agnostic of the high-level task , which in turn enables fast adaptation to new task because only the high-level needs to be trained , but it is better to state that clearly . I would suggest the authors to write one or two paragraphs at the beginning of the method to provide a high-level overview of ( 1 ) What LOF is , ( 2 ) what are different components of LOF ( 3 ) how these components interact with each other , intuitively ( 4 ) what are the technical challenges each component , or all components jointly , are trying to tackle ( e.g. , enabling compositionality ) . In addition , there are many mentioning of using reward function in the form of an FSA . I understand that this was already introduced in Icarte et al. , but it is worth reviewing it in the main paper . Comments on experiments : - For compositionality experiment , why might RM and Flat not be applicable here ( related to my first comment ) ? - I do not see RM curve in ( e ) Minor : - `` \\phi_ { liveness } is represented as a finite state automaton '' . The paper has been using FSA through except first introduced FSA . It 's better to be consistent .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful review of the paper . The reviewer has highlighted several shortcomings of our exposition that we will try to address . 1 ) We don \u2019 t compare LOF against RM in the composability experiments because RM is not composable . RM is not composable because it learns unique subpolicies for each state of the automaton . These state-specific subpolicies are also automaton-specific ; for example , if the automaton is \u201c go to A , then B \u201d , the subpolicy for the initial state will learn to go specifically to subgoal A . If the automaton is modified to be \u201c go to B , then A \u201d , the initial state \u2019 s subpolicy will still attempt to go to subgoal A . There is no way to adjust RM to be composable without significant changes to the algorithm . Since RM is inherently not composable , we did not include it in the composability experiments . We have added more of an explanation in the paper , and we have also added 3 new figures to the Appendix , Figures 4 , 5 , 6 , and 7 which we hope will illustrate how LOF differs from RM and why RM is not composable . The \u201c Flat Options \u201d baseline , which represents the standard options framework , is also not composable because the baseline has no knowledge of the high-level FSA . This is because the standard options framework does not consider a hierarchical state space ( in this case , the FSA ) . Therefore , changing the high-level FSA is like changing a variable that doesn \u2019 t exist in this baseline , which is why we did not include it in the composability experiments . We have added more of an explanation of \u201c Flat Options \u201d to the revision . We can also add the RM and Flat Options baselines to the composability experiments if you think it would be helpful , if only to demonstrate their inability to be composed . 2 ) We apologize for the lack of clarity in the Methods section . We will add an outline of our approach to the beginning of the Methods section which we hope will make the connections between the subsections clearer . LTL is not used directly in the LOF algorithm , but is rather used as an aid to a user to specify the rules of the system . There are automated tools that can then be used to translate the liveness property ( and safety property if applicable ) into an automaton [ 1 ] . The reason we chose to use LTL to specify the task and rules ( rather than , say , making an FSA by hand ) is because LTL corresponds closely to natural language and has proven to be a more natural way of expressing tasks and rules for engineers than designing FSAs by hand [ 2 ] . We have added this justification to the revision . LOF also uses many concepts from the formal methods community including the ideas of propositions , liveness/safety properties , and satisfaction . But LOF does not rely on LTL \u2013 in practice , the FSA could be made by hand without using LTL . The composability of LOF is its most unique attribute , as there are many existing algorithms that have the properties of satisfaction and optimality but not composability . LOF \u2019 s composability arises from 2facts \u2013 1 ) our model is hierarchical ( including the reward function ) , so the high level ( associated with the FSA ) is relatively independent from the low level ( associated with the environment ) . The low level can be learned independently from the high level as we outline in Algorithm 1 ( the low-level options are learned first ( Alg 1 line 6 ) , and only then is the metapolicy computed ( Alg 1 line 13 ) ) . If the high-level FSA is modified , a new metapolicy can be calculated without needing to re-learn the low-level options . 2 ) We associate options with subgoals rather than with automaton states ( in contrast to RM ) . Since these \u201c logical options \u201d are associated with subgoals/FSA transitions ( rather than with states as in RM ) , they can be treated as actions over the FSA . Therefore if the FSA is changed , it is easy to recompute an optimal policy over the new FSA in 10 -50 retraining steps using Logical Value Iteration ( hence , composability ) . 3 ) We agree that there are many papers that use FSA-based reward functions to learn tasks and have satisfaction and optimality . However , we are unaware of any of these methods being composable ; therefore , in our Related Works section , these methods are discussed in the \u201c Not Composable \u201d paragraph . 4 ) RM is not in curve 5e because learning was so slow that its curve would have obscured the other learning curves . Figure 14 in the Appendix includes the RM learning curve . We will mention this in the caption of the figure to make it more clear , but we do discuss it in the Results section : \u201c For the reacher domain , RM takes an order of magnitude more steps to train , so we left it out of the figure for clarity ( see Appendix Fig.14 ) . \u201d 5 ) Thanks for pointing out our inconsistency with FSA vs \u201c finite state automaton \u201d . We will fix that . [ 1 ] https : //spot.lrde.epita.fr/ ( for an interactive web app that converts LTL formulae to automata , see https : //spot.lrde.epita.fr/app/ ) [ 2 ] https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=3383958"}], "0": {"review_id": "IbFcpYnwCvd-0", "review_text": "This paper is on a new RL framework that leverages logical reasoning to improve the learning performance of RL agents . In particular , the knowledge is encoded using LTL , and includes both safety knowledge ( used for reward function definition ) and liveness knowledge ( used for constructing FSA ) . THe developed framework has been evaluated using tasks in both discrete and continuous domains , where the RL agent was realized using Q-learning and PPO respectively . The framework was compared with baselines including another LTL-based RL methods ( Reward Machines ) . Results show that LOF performed better than the baseline methods in learning rate and policy quality ( in most cases ) . The whole idea of leveraging human knowledge to improve RL agent 's learning performance makes senses . The main concern is that the developed framework looks quite incremental in comparison to the methods from the literature . For instance , there are already methods in the literature using LTL to bias the reward function , e.g. , the papers that improved the Reward Machines work . As a result , the novelty of the safety knowledge is rather incremental . At the same time , the `` liveness '' part is quite similar to those methods that plan at a symbolic level to guide RL agents , such as the following : Lyu D , Yang F , Liu B , Gustafson S. SDRL : interpretable and data-efficient deep reinforcement learning leveraging symbolic planning . 2019 It 's nice to see the discussions on satisfaction , optimality , and composability . The analysis on optimality is the most important , whereas the other two points are relatively trivial due to the LTL-based FSA . But it turned out the developed approach , LOF , does not guarantee any desired properties beyond those from the literature . In particular , the LTL-based FSA can not guarantee global optimality , where the baseline of Reward Machines does . The reviewer is curious about if Reward Machines can be considered an ablation of LOF , where the `` liveness '' component is disabled ( and `` safety '' is retained ) . If not , what are the differences between the two methods ? Knowledge-based RL can go beyond HRL methods . For instance , the following survey paper summarizes a few other ways of leveraging human knowledge in RL . Some discussions can help improve the related work section . Zhang S , Sridharan M. A Survey of Knowledge-based Sequential Decision Making under Uncertainty . 2020 In general , it 's not surprising to see human knowledge is useful in RL . However , human knowledge is not always correct . It will be nice to evaluate the situations where human knowledge in incomplete or inaccurate , or at least some discussions can be helpful . - The reviewer appreciates the response in detail . The new figures ( Figs 4-7 ) are helpful for demonstrating the differences between LOF and RM , while also demonstrating that their outputs can be similar or the same in many cases . The reconciliation between optimality and composability is a nice feature . Overall , the reviewer still feels positive on this work .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to the reviewer for the detailed review of our paper . 1 ) We want to emphasize that improving learning performance is not one of the goals of our method , although it can be a side effect . Rather , our goal was to achieve of satisfaction , optimality , and composability . We agree that in terms of \u201c satisfaction \u201d and \u201c optimality \u201d alone , our work is not novel , as there are many other algorithms that achieve satisfaction and optimality , including RM . However , we believe that our work makes a significant contribution in terms of achieving satisfaction and optimality along with composability . As AnonReviewer2 points out , \u201c This is a very good general idea , and as far as I am aware other works have not studied how one might compose previously learned sub-task policies to accomplish new tasks described by automata in this way \u201d ( although they are concerned about the generality of our work ; in the revisions we hope we have addressed those concerns ) . In order to achieve composability , we make a big shift in perspective \u2013 whereas other papers achieve satisfaction/optimality by shaping the reward function using the automaton , we instead take a planning perspective , where we find an optimal policy over the automaton . Therefore , if the automaton is changed/modified , we can find a new optimal policy by replanning over the new automaton . ( This is a similar approach to that taken in SDRL and other symbolic planning methods , but it differs in that the meta-controller of SDRL does not have the property of full composability that we describe in our work.But we are grateful for the reference and will include it in the related work ) . With regard to the \u201c liveness \u201d and \u201c safety \u201d properties , we believe that we have made a useful contribution in identifying these two properties as distinct components of a planning algorithm . Most other papers in the LTL+RL literature do not distinguish between liveness and safety properties and plan over a single , combined automaton . However , in this paper we point out that the safety property is mostly useful for defining the reward function , and the liveness property is useful for planning . Therefore , the safety property is incorporated into the reward function for training the logical options , and the liveness property is translated into an FSA so that an optimal policy can be found over the FSA . We believe that this distinction is a useful contribution for the RL+LTL literature ( although it is widely known in the formal methods community ) . 2 ) With regard to satisfaction , optimality , and composability , we believe that our most important contribution is that we have found a way to reconcile optimality and composability , which in general are opposing goals . Composability requires the policies to be flexible in a way that makes optimality difficult to achieve ( hence the existence of many optimal methods that are not composable , and many composable methods that are not optimal ) . Furthermore , we want to emphasize that our proof of optimality guarantees global optimality under the conditions that we state . Although these conditions are stricter than the conditions that RM requires , we strove to find the loosest possible assumptions for achieving optimality while also maintaining full composability . We point out in the paper that in continuous state spaces it is impossible to meet our condition that a subgoal be associated with a single state ; however , by defining the subgoal to be a small region as we do in our paper , it is possible to achieve near-optimal policies . We acknowledge that our work will not have the final say in this discussion , as future papers may be able to loosen our assumptions , but we hope that our work can kickstart the discussion ."}, "1": {"review_id": "IbFcpYnwCvd-1", "review_text": "Summary of the work : The authors propose the Logical Options Framework ( LOF ) a framework for reasoning over high-level plans and learning low-level control policies . This framework uses Linear Temporal Logic ( LTL ) to specify properties ( high-level tasks ) in terms of propositions . The authors propose a framework in which a separate sub-task policy is learnt to accomplish each such sub-task proposition . These low-level control policies may be reused , without further training , to accomplish new high-level tasks by performing value-iteration in the proposed Hierarchical SMDP . Experimental results demonstrate the method \u2019 s effectiveness for several tasks and experimental domains . Quality and Clarity : The paper does a good job at intuitively describing the value that the proposed \u201c LOF \u201d framework provides ( satisfaction , optimality , and composability ) . In general , the language used in the paper is clear and easy to read . However , some of the technical aspects in the paper are difficult to follow concretely . There are various vague statements and definitions throughout the paper . Typos , inconsistencies , and missing explanations/discussion of seemingly important notions make some of the presented ideas imprecise . This raised several questions from me on the general applicability of the framework ( as it is presented in this paper ) beyond the presented experimental tasks , and on some of the specifics of Theorem 3.1 . Given that the focus of this paper is on the development a new framework for RL , fixing these issues is crucial . I have included a more detailed list of feedback for the authors at the end of the review . Originality and Significance : The proposed method defines \u201c logical options \u201d \u2013 sub-task policies whose goal is to trigger propositions that cause transitions in the automata-based description of the high-level task . Temporally extended tasks may then be accomplished by deploying the appropriate sub-task policy at the appropriate FSA state . This idea seems very similar to learning with Reward Machines . This similarity is acknowledged in the paper . The main conceptual difference between LOF and RMs , is that LOF proposes to learn policies that trigger transitions in an automaton , whereas learning with RMs instead learns separate policies for each state of the automaton . This difference means that in the LOF framework , previously learnt sub-task policies can be reused by composing them to complete new high-level tasks . This is not true for RMs . However , the policies learnt by LOF are not guaranteed to be optimal unless certain conditions are met . This is a very good general idea , and as far as I am aware other works have not studied how one might compose previously learned sub-task policies to accomplish new tasks described by automata in this way . However , I am concerned that the significance of the work might be limited to the specific examples of the paper . For example , by associating a cost with each safety proposition , instead of forming the safety FSA associated with the safety property , it seems that the only safety properties that can be expressed by LOF are those of the form : \u201c avoid these states \u201d . Because this is a significant limitation in comparison with all possible safety properties , an explicit discussion of the safety properties that can be represented by LOF would help the reader to understand exactly what problems LOF can solve . Furthermore , it is unclear how the composability results change in the presence of new and/or changed safety properties . I elaborate further on this point at the end of the next section of the review . Questions surrounding theorem 3.1 The main theoretical result of this paper is that under appropriate conditions , the hierarchical policy learned through the LOF framework will be equivalent to the optimal policy ( optimal when planning is allowed with respect to low-level actions in the FSA-MDP product ) . The proof follows the logic that given the appropriate reward functions , the option policies will always reach the states corresponding to their sub-goals , and that the meta-policy will select a sequence of sub-goals that always reaches the FSA goal state . The proposed method assumes that if an option corresponding to a particular sub-goal is selected by the meta-policy , then no other sub-goal proposition will be triggered before that option is complete . My concern with this assumption is as follows : what happens in scenarios in which an option policy passes through a state associated with a different sub-goal before reaching its own sub-goal ? Would it then be possible for the meta-policy to select an option corresponding to a particular sub-goal , but for a different sub-goal proposition to be triggered first , causing an unwanted transition in the FSA ? For example , suppose in the delivery domain we could complete a task by either of the following sequences of sub-goals : \u201c a then b \u201d , or \u201c b then a then c \u201d . If \u201c a then b \u201d is less costly than \u201c b then a then c \u201d , the optimal meta-policy should result in \u201c a then b \u201d . Furthermore , assume that \u201c b \u201d lies directly between \u201c a \u201d and the initial state of the agent . The optimal policy for sub-goal proposition \u201c a \u201d would be to move directly to \u201c a \u201d , which would cause it to first pass through \u201c b \u201d . This would cause the agent to be forced to follow \u201c b then a then c \u201d even if the meta-policy first chose proposition \u201c a \u201d . Meanwhile , the optimal policy allowing low-level planning in the FSA-MDP product space would be to move to \u201c a \u201d while actively avoiding \u201c b \u201d , and then to proceed to \u201c b \u201d . Also , it is unclear whether consideration of obstacles and safety propositions are included in the consideration of theorem 3.1 . It is unclear what the reward functions corresponding to the \u201c goal-conditioned policy \u201d is . If R_S is included in the reward function used to train sub-task policies , then these policies will learn to avoid obstacles . When we compose these sub-task policies for a new task with potentially different safety properties , the policies we learned previously could be sub-optimal in the new scenario . Experimental results : The experimental results demonstrate the effectiveness of the framework for the chosen experimental tasks . I like the videos provided in the supplementary materials ; they are nice visualizations of how the sub-policies are strung together to complete the overall task . From the paper \u2019 s description , it is unclear how LOF-QL is implemented . How does this algorithm make use of the liveness FSA if it does not have access to the FSA \u2019 s transition function ? Also , I am concerned that the comparison against RMs may not be fair . As stated in the paper , the RM-based method is only rewarded when the final RM state has been reached . Conversely , the reward for the LOF method is much denser . I do not see why the RM-based method could not also be rewarded for each \u201c good \u201d transition . Finally , the experiments include the \u201c can \u201d proposition which represents when one of the sub-goals has been canceled . It is unclear how the labeling function , which is defined to map specific MDP states to individual propositions , could return this proposition . If the proposition is returned randomly , the formalisms of the labeling function need to be altered to include these types of randomly occurring propositions . Pros > The general idea of the paper is good . I like the idea of pre-learning sub-task policies , and then of using automata-based task descriptions to find meta-policies that accomplish new tasks without additional training . This is the \u201c composability \u201d described in the paper . > The authors do a good job of intuitively describing the benefits that this type of method could provide over competing algorithms . > The experimental tasks ( particularly the videos of the tasks being completed ) provide a nice visualization/demonstration of the ideas of the paper . Cons > The technical aspects of the paper are hard to follow concretely . > Some of the theory seems vague and the paper has various typos and inconsistencies . This could lead to reader misinterpretations and/or author mistakes . > The method \u2019 s applicability seems to be somewhat limited to specific types of tasks , without explicit discussion of exactly what types of tasks can be solved . > The questions raised surrounding theorem 3.1 need to be clarified . Further detailed feedback : > How are the logical options learned ? In particular , how are T_ { o_p } ( s \u2019 |s ) and R_ { o_p } ( s ) learned . These are the components of the options that are subsequently used to find the metapolicy . Are they estimated by averaging values over rollouts of the learned policy ? What if the environment is highly stochastic and their values vary greatly across different rollouts ? > The assignment for T_ { o_p } ( s \u2019 |s ) on line 11 of algorithm 1 seems to assume there is a fixed k in which p will be reached . Would this always be the case if the environment is stochastic ? This also seems to be a different definition than is given in equation 2 . > In equation 1 , R_ { o } ( s ) is defined as the expected cumulative reward of options given that the option is initiated in state s at time t and ends after k time steps ; shouldn \u2019 t this make R_ { o } ( s ) a function of k as well ? > In the definition of T_ { o } ( s \u2019 |s ) , what is p ( s \u2019 , k ) ? I assume this is the probability , under the option \u2019 s policy , of reaching state s \u2019 after k timesteps from state s. If this is the case , p ( s \u2019 , k ) should be defined . > In line 9 of the Algorithm 1 , T_ { P } ( s , p ) = 1 is written . On line 11 of Algorithm 1 , T_ { P } ( s ) = p is written instead , but both have the same meaning : proposition p is returned by the labeling function from state s. > The liveness FSA \u2019 s transition function is defined and treated as a probability distribution . However , the automaton \u2019 s transitions appear to be deterministic in the presented examples . If there is a particular reason for it to be defined as a transition distribution , some discussion would be helpful for the reader . > In the definition of a Hierarchical SMDP , the transition function is defined as a cartesian product of the environment transition distribution , the labeling function , and the FSA transition function . The precise meaning of this notation is unclear . A brief description of how a transition proceeds would be very helpful for the reader to understand the order in which arguments are passed to these three functions . Does the environment transition first , and the proposition of the new environment state cause the FSA to transition ? > In section 3.1 and in appendix A.1 , the author writes that \u201c if the optimal option policy equals the goal-conditioned policy , the policy will always reach the sub-goal \u201d . This will not be the case if the environment is stochastic and it is possible for the agent , under ANY policy , to slip into a state from which the sub-goal is not reachable . > Point 2 of the contributions outlined in section 1.1 states that the options can be composed to solve new tasks with only 10-50 retraining steps . This wording is at odds with the abstract , which states that LOF \u2019 s learned policies can be composed to satisfy unseen tasks with no additional learning . > In Section 3 \u201c These may only appear in the liveness property and there may only be one instance of each subgoal. \u201d The second half of this sentence is unclear . Does this mean that the inverse image of each sub-goal proposition through the labeling function is a singleton set containing only one state ? > In section 3 \u201c Safety propositions are propositions such as \u2018 the agent received a call \u2019 that the agent has no control over or does not seek out. \u201d This sentence is unclear . It would help if you could express what it means for the agent to \u201c have control over \u201d or to \u201c seek out \u201d a proposition in terms of the MDP states , actions , and proposition labeling function .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for such a thoughtful and in-depth review . This review is really valuable to us and we have taken the feedback to heart . We believe that the reviewer has pointed out two major issues with our formulation that we have addressed in the revision . 1 ) The fact that our formulation in the paper does not cover the case where the safety property is non-trivial and the case where the LTL specification can \u2019 t be separated into independent liveness and safety specifications ( discussed in points 1 and 3 below ) . We have added a new section to the appendix ( Appendix A ) where we present a formulation that is much more general than the one presented in the paper . 2 ) The fact that our formulation of the proposition labeling function is inconsistent with randomly occurring safety propositions ( discussed in point 7 below ) . We added a new category of propositions called event propositions to address this concern . Furthermore , the reviewer has pointed out numerous other issues that we have attempted to address . We apologize for the length of this review and the fact that it has been split into so many comments , but the reviewer pointed out many really good and deep points that have taken a lot of space to address . 1 ) \u201c it seems that the only safety properties that can be expressed by LOF are those of the form : \u201c avoid these states \u201d \u201d \u2013 LOF is not limited to these types of safety properties . In the submission , we briefly mention this when we say , \u201c In our experiments , the safety property $ \\phi_ { safety } $ defines a reward function R_S that penalizes violations of safety rules . $ \\phi_ { safety } $ can be converted into an FSA $ \\mathcal { T } _S $ , and the reward function can be defined on the states of $ \\mathcal { T } _S $ . However , to limit the complexity of the formulation we assume that there is simply a reward ( cost ) associated with every safety proposition. \u201d However , we think that your concern should be addressed properly , so we have now included a more general formulation of LOF in Appendix A that can handle any safety property . We want to emphasize that LOF can handle any type of safety property , as long as the low-level policy-learning algorithm can accommodate it . In the general formulation , the safety property is translated into an automaton , and Reward Machines or other appropriate algorithms can be used to learn logical options that obey the safety property . 2 ) \u201c it is unclear how the composability results change in the presence of new and/or changed safety properties \u201d : Safety properties are not composable . In our definition of composability , we assume that the liveness property can change but not the safety property . We were not clear about this in the paper ; in our definition of composability in the introduction , we refer to \u201c tasks \u201d but should use the more specific term \u201c liveness property. \u201d We will edit the revision to be very clear about this ."}, "2": {"review_id": "IbFcpYnwCvd-2", "review_text": "In this paper , the author proposes logic option framework for learning policies that satisfy the logic constraints . Temporal logic rules are converted into finite state machine and each of them is then converted to a learnable option , where the logic proposition is used as the reward function . Given the options , it searches for the optimal meta-policy over all the options . The LOF is tested on a gridworld and an OpenAI Gym benchmark and is compared against baselines that do not use rules . I find the idea of integrating logics into RL is an exciting direction for improving agent 's interpretability and the proposed LOF does have some novelties in applying the FOL rules . However , I 'm not familiar with the RL and option literature and it 's difficult for me to tell its original work from the existing ones on the RL side . Some questions for the author : Judging from Algorithm 1 , it seems the rules and predicates are pre-defined with the environment and their conversion into FSA and reward functions are also done with an existing method . Is there anything original in terms of rule/predicate learning or conversion for the proposed LOF ? In Algorithm 1 , each rule is converted to an option and learned independently . What does the LOF do if rules are correlated or have conflicts ? - Say , rule1 is a pre-requisite of rule2 : rule1 : `` go to dest1 '' and rule2 : '' if agent is in dest1 then go to dest2 , otherwise check rule1 '' - Or rule1 : `` go to dest1 asap '' whereas rule2 : `` do not violate the traffic rules '' - Does the LOF provide mechanism to address the above issues ? I 'm also concerned about the scalability of this method . It seems to me that the options need to be evaluated at each possible state in the environment . If the goals are fixed , probably one can store the value beforehand , but for a dynamic environment where the goal is changing , how much does it cost to update the reward function and learn the option policies ? And how well does it scale with the size of the state/action space ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to the reviewer for the questions and constructive feedback , and hope that we can clarify some of aspects of our work . 1 ) Most of the LTL+RL literature achieves satisfaction and ( sometimes ) optimality by using the automaton associated with the LTL specification to shape the reward function , including Reward Machines . During training , the environment keeps track of what state the automaton is in and rewards the agent whenever a ( favorable ) transition is achieved . This approach can guarantee satisfaction and optimality ; however , it is inherently not composable because the automaton rewards get baked into the learned value function/policy . If the automaton is changed , policy learning typically has to be done from scratch ( we have added Appendix Figures 4 , 5 , 6 , and 7 to illustrate how LOF differs from Reward Machines , the published work most similar to ours ) . Other work in symbolic planning is more similar to ours in that they learn high- and low-level controllers . However , these works do not take into account the specific tradeoffs between composability and optimality that we cover in our paper , and their high-level controllers are either not composable or only capable of limited composability . We will try to emphasize the differences between our work and the related work more in the revision . To address your question of if there is \u201c anything original in terms of rule/predicate learning or conversion for the proposed LOF \u201d : we assume that all aspects of the hierarchical semi-MDP are known , including the rules and propositions . Learning rules and propositions is a very exciting goal and definitely an avenue of future work ; in this paper , we have laid the groundwork for that type of future work by defining the formulation of the hierarchical SMDP and proposing an algorithm for finding a satisfying/optimal/composable policy over it . Our hierarchical SMDP formulation is unique in that it includes both a hierarchical action space ( as in the original options framework paper ) as well as a hierarchical state space/transition function/reward function ( which is common to many LTL+RL papers ) . 2 ) Rule conflicts are a topic that we didn \u2019 t have space to address in the initial submission , but we will talk about it in the revision . Can you elaborate more about your first example ? As stated , the rules will cause the agent to go to dest1 and then dest2 . But in general , if rules are logically incompatible , they will probably translate into a single-state automaton that is not an accepting state . For example , the formula \u201c ( ! a U b ) & ( ! b U a ) & G ! ( a & b ) & F a \u201d can be thought of as , \u201c Go to A , but you can \u2019 t go to A until you \u2019 ve gone to B , and you can \u2019 t go to B until you \u2019 ve gone to A \u201d . It results in a single-state automaton with no goal state . ( You can try inputting this formula into https : //spot.lrde.epita.fr/app/ to see for yourself ) . LOF is compatible with such an automaton ; the policy will simply terminate at the first time step . This is more an issue of user error ; hopefully the user will see that the formula does not make logical sense ( perhaps upon seeing that the automaton is trivial ) and correct the formula . We have included this discussion in Appendix D , \u201c Further Discussion \u201d . You make a good point with the second example \u2013 what if achieving the goal involves violating a safety constraint ? One of our assumptions is that the agent can achieve any subgoal from any state of the environment , so an environment that requires a violation of a hard safety constraint would violate our assumption . We believe this is a reasonable assumption to make because it is not fair to the agent to require the agent to achieve a task that is against the rules . However , under most conditions LOF will still \u201c work \u201d in such an environment . For example , in the formulation that we use in the paper , safety propositions are assigned negative but finite costs . As we discuss in the proofs of satisfaction/optimality , the cost of not satisfying the task is negative infinity , whereas the cost of violating the rules is negative but finite . Therefore , in our specific formulation , if the agent is in an environment where violating the safety rules is the only way to achieve the task , it will learn to violate the safety rules at the cost of a massive negative reward ( however , it will only learn this behavior in the limit of an infinite number of training steps ; limiting the length of a training episode to a reasonable number will prevent the agent from ever violating safety rules with large negative costs ) . We have included this discussion in Appendix D , \u201c Further Discussion \u201d . We hope that we have explained how these issues can be handled and how LOF would react to them . We are happy to clarify if you have more questions ."}, "3": {"review_id": "IbFcpYnwCvd-3", "review_text": "Summary : The paper proposes a hiearchical RL framework augmented with linear temporal logic . Low-level policies are trained to reach a set of subgoals with penalty on violating safety constraints , allowing the high-level policy to adapt to different task by composing the low-level policies to reach a set of subgoals specified by an Linear Temporal Logic clause . My first comment is on technical novelty . Although the paper states that LOF takes a step beyond RM to enable compositionality , there is no empirical comparison or evaluation . The only experiment on evaluating compositionality ( c , d in Figure 2 ) does not compare LOF against RM . My second major comment is on technical exposition . The paper 's writing needs a lot of work , especially the method section . The method section started off by describing different concepts used by the framework , but it is unclear how these concepts constitute LOF . For example , how exactly is LTL used in LOF ? The introduction mentioned that LOF provides `` compositional '' property , but there is not a single hint of how might each piece of LOF lead to this property . It took me a few read to understand how the low-level reward might be agnostic of the high-level task , which in turn enables fast adaptation to new task because only the high-level needs to be trained , but it is better to state that clearly . I would suggest the authors to write one or two paragraphs at the beginning of the method to provide a high-level overview of ( 1 ) What LOF is , ( 2 ) what are different components of LOF ( 3 ) how these components interact with each other , intuitively ( 4 ) what are the technical challenges each component , or all components jointly , are trying to tackle ( e.g. , enabling compositionality ) . In addition , there are many mentioning of using reward function in the form of an FSA . I understand that this was already introduced in Icarte et al. , but it is worth reviewing it in the main paper . Comments on experiments : - For compositionality experiment , why might RM and Flat not be applicable here ( related to my first comment ) ? - I do not see RM curve in ( e ) Minor : - `` \\phi_ { liveness } is represented as a finite state automaton '' . The paper has been using FSA through except first introduced FSA . It 's better to be consistent .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful review of the paper . The reviewer has highlighted several shortcomings of our exposition that we will try to address . 1 ) We don \u2019 t compare LOF against RM in the composability experiments because RM is not composable . RM is not composable because it learns unique subpolicies for each state of the automaton . These state-specific subpolicies are also automaton-specific ; for example , if the automaton is \u201c go to A , then B \u201d , the subpolicy for the initial state will learn to go specifically to subgoal A . If the automaton is modified to be \u201c go to B , then A \u201d , the initial state \u2019 s subpolicy will still attempt to go to subgoal A . There is no way to adjust RM to be composable without significant changes to the algorithm . Since RM is inherently not composable , we did not include it in the composability experiments . We have added more of an explanation in the paper , and we have also added 3 new figures to the Appendix , Figures 4 , 5 , 6 , and 7 which we hope will illustrate how LOF differs from RM and why RM is not composable . The \u201c Flat Options \u201d baseline , which represents the standard options framework , is also not composable because the baseline has no knowledge of the high-level FSA . This is because the standard options framework does not consider a hierarchical state space ( in this case , the FSA ) . Therefore , changing the high-level FSA is like changing a variable that doesn \u2019 t exist in this baseline , which is why we did not include it in the composability experiments . We have added more of an explanation of \u201c Flat Options \u201d to the revision . We can also add the RM and Flat Options baselines to the composability experiments if you think it would be helpful , if only to demonstrate their inability to be composed . 2 ) We apologize for the lack of clarity in the Methods section . We will add an outline of our approach to the beginning of the Methods section which we hope will make the connections between the subsections clearer . LTL is not used directly in the LOF algorithm , but is rather used as an aid to a user to specify the rules of the system . There are automated tools that can then be used to translate the liveness property ( and safety property if applicable ) into an automaton [ 1 ] . The reason we chose to use LTL to specify the task and rules ( rather than , say , making an FSA by hand ) is because LTL corresponds closely to natural language and has proven to be a more natural way of expressing tasks and rules for engineers than designing FSAs by hand [ 2 ] . We have added this justification to the revision . LOF also uses many concepts from the formal methods community including the ideas of propositions , liveness/safety properties , and satisfaction . But LOF does not rely on LTL \u2013 in practice , the FSA could be made by hand without using LTL . The composability of LOF is its most unique attribute , as there are many existing algorithms that have the properties of satisfaction and optimality but not composability . LOF \u2019 s composability arises from 2facts \u2013 1 ) our model is hierarchical ( including the reward function ) , so the high level ( associated with the FSA ) is relatively independent from the low level ( associated with the environment ) . The low level can be learned independently from the high level as we outline in Algorithm 1 ( the low-level options are learned first ( Alg 1 line 6 ) , and only then is the metapolicy computed ( Alg 1 line 13 ) ) . If the high-level FSA is modified , a new metapolicy can be calculated without needing to re-learn the low-level options . 2 ) We associate options with subgoals rather than with automaton states ( in contrast to RM ) . Since these \u201c logical options \u201d are associated with subgoals/FSA transitions ( rather than with states as in RM ) , they can be treated as actions over the FSA . Therefore if the FSA is changed , it is easy to recompute an optimal policy over the new FSA in 10 -50 retraining steps using Logical Value Iteration ( hence , composability ) . 3 ) We agree that there are many papers that use FSA-based reward functions to learn tasks and have satisfaction and optimality . However , we are unaware of any of these methods being composable ; therefore , in our Related Works section , these methods are discussed in the \u201c Not Composable \u201d paragraph . 4 ) RM is not in curve 5e because learning was so slow that its curve would have obscured the other learning curves . Figure 14 in the Appendix includes the RM learning curve . We will mention this in the caption of the figure to make it more clear , but we do discuss it in the Results section : \u201c For the reacher domain , RM takes an order of magnitude more steps to train , so we left it out of the figure for clarity ( see Appendix Fig.14 ) . \u201d 5 ) Thanks for pointing out our inconsistency with FSA vs \u201c finite state automaton \u201d . We will fix that . [ 1 ] https : //spot.lrde.epita.fr/ ( for an interactive web app that converts LTL formulae to automata , see https : //spot.lrde.epita.fr/app/ ) [ 2 ] https : //papers.ssrn.com/sol3/papers.cfm ? abstract_id=3383958"}}