{"year": "2020", "forum": "S1eL4kBYwr", "title": "UNITER: Learning UNiversal Image-TExt Representations", "decision": "Reject", "meta_review": "This submission proposes an approach to pre-train general-purpose image and text representations that can be effective on target tasks requiring embeddings for both modes. The authors propose several pre-training tasks beyond masked language modelling that are more suitable for the cross-modal context being addressed, and also investigate which dataset/pretraining task combinations are effective for given target tasks.\n\nAll reviewers agree that the empirical results that were achieved were impressive.\n\nShared points of concern were:\n- the novelty of the proposed pre-training schemes.\n- the lack of insight into the results that were obtained.\n\nThese concerns were insufficiently addressed after the discussion period, particularly the limited novelty. Given the remaining concerns and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.\n", "reviews": [{"review_id": "S1eL4kBYwr-0", "review_text": "# 1. Summary The authors introduce a new pre-training procedure for image-text representations. The idea is to train the model on a huge collection of different image-text datasets and the use the model for downstream tasks. The difference between the proposal wrt the concurrent work is that conditioned masking is used: (i) Masked Language Modeling (MLM) conditioned on image; (ii) Masked Region Modeling (MRM) conditioned on text; and (iii) joint Image-Text Matching (ITM). I am on the fence for this paper given the balance between strengths and weaknesses listed below. I am conservative here and decide for weak reject; but I am open for discussion, if the authors answer to my concerns detailed below. Strengths: * State-of-the-art results on several downstream vision-language tasks * Empirical work to investigate different ways to perform conditioned masking Weaknesses: * Some parts of the method needs clarification (see point 2 below) to better understand the details and practical advantages of the method. * Limited novelty: the paper is an extension of BERT to the visual domain (see point 3 below) # 2. Clarity and Motivation The paper reads quite well, although some points need to be improved: * \"Compared with LXMERT (Tan & Bansal, 2019) and ViLBERT (Lu et al., 2019) that use two streams (one Transformer for each modality), our UNITER model can learn joint contextualized ...\", why is this an advantage? Using two streams might also lead to learning context? Maybe an example can clarify my question. * End of Sec. 3.1 (and paragraph in Sec. 3.2): not clear how the model is training for ITM. What's the input and output? Why do you need a new symbol [CLS]? * Sec. 3.2 ITM: \"an additional special token [CLS] is fed into our model, which indicates the fused representation of both modalities\" - This is not clear. Why this special token is needed? Why is not needed in the MLM and MRM? * \"The scoring function is denoted as s\" -> please indicate in the main text what function you used * MRFM and MRC are clear, however the intuition of MRC-kl is missing. Why is this needed? What does it mean in practice to minimize such divergence (provide practical example)? * Combination of tasks (MLM + ITM + MRC-kl + MRFR) -> it is not clear how this is done in practice. Is the loss function composed (summed)? Within the mini-batch, the method randomly chooses which operation to do (e.g., MLM) for each sample? This should be clarified in the main text of the paper. # 3. Novelty The novelty of the paper is quite limited since it is an extension of BERT to the visual domain. The authors propose an empirical analysis of different ways to mask the visual input, however this might not be a substantial extension of previous work. In fact, recently there are many other papers (ViLBERT, VisualBERT, LXBERT, ...) working on similar topic with small differences. What it is missing in this paper is an understanding and intuition on the reasons why the conditioned masking idea should be better than the other visual masking ideas proposed in previous work. # 4. Experimentation The main advantage of this paper relies on the extensive experimental analysis done on many challenging datasets reaching the state of the art on several downstream tasks. The evaluation on both pre-training tasks and downstream tasks show that the method is working well in practice.", "rating": "6: Weak Accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- original question -- -- -- -- -- -- -- -- -- -- -- -- -- - * `` Compared with LXMERT ( Tan & Bansal , 2019 ) and ViLBERT ( Lu et al. , 2019 ) that use two streams ( one Transformer for each modality ) , our UNITER model can learn joint contextualized ... '' , why is this an advantage ? Using two streams might also lead to learning context ? Maybe an example can clarify my question . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Our argument is not about \u201c single-stream being strictly better than two-stream \u201d . In fact , we have tried a SOTA two-stream model ( MCAN , Yu et al. , 2019 ) in our preliminary experiment before the aforementioned related works are published , and found that it did not work as well as single-stream model . We therefore continued with the single-stream architecture , and focused on finding the most effective pretraining strategy , which is our main contribution . Note that both LXMERT and ViLBERT promoted two-stream models , yet we showed that single-stream model is sufficient to learn contextual representations across two modalities ."}, {"review_id": "S1eL4kBYwr-1", "review_text": "This paper presents a novel method for image-text representations called UNITER. The proposed method has been subsequently tested in many downstream tasks. A detailed ablation study helps to understand the role of each pretrained task in the proposed model. Although the empirical results are nice, performing the intensive set of experiments on many different tasks is definitely time-consuming and needs a lot of engineering efforts, the technical contribution does not seem significant to me. The paper modifies an existing pre-training procedure by conditional masking (Section 2). I agree this is well-motivated but it has little novelty and a similar idea is there in VQA (See \u201cDynamic fusion with intra and inter-modality attention flow for visual question answering\u201d). MLM and MRM are not new training procedure either, they are basically extending the BERT\u2019s training procedure with the consideration of multiple modalities. I have some questions for the authors: (1) What are the advantages of using single-stream transformer over two-stream transformer (page 2). I guess it leads to fewer parameters but I don\u2019t think this is a big problem. (2) Some visualization of attention weights would be helpful. Minor \u2022 In \u201cm \\e N^M\u201d (equation 1), what is N and M? ", "rating": "6: Weak Accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- original question -- -- -- -- -- -- -- -- -- -- -- -- -- - The paper modifies an existing pre-training procedure by conditional masking ( Section 2 ) . I agree this is well-motivated , but it has little novelty and a similar idea is there in VQA ( See \u201c Dynamic fusion with intra and inter-modality attention flow for visual question answering \u201d ) . MLM and MRM are not new training procedure either , they are basically extending the BERT \u2019 s training procedure with the consideration of multiple modalities . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Thank you for referring us a related work . After a thorough check of the paper , we agree with the reviewer this is also a relevant work . Nevertheless , Gao et al. , ( 2019 ) may focus on novel model architecture , while we proposed a generic V+L representation via pretraining . We will cite the paper and discuss it in the related work . Secondly , we argue that UNITER is not trivially derived from BERT . Even for BERT , language modeling has been around for years ( CBOW , Mikolov et al. , 2013 ) . Intuitively , mask-then-reconstruct is helpful for learning contextualization , but the key is what exact pretraining tasks are effective , especially for learning \u201c alignment \u201d across vision and language in our case . That \u2019 s why we proposed ITM , MLM , MRM ( 3 variants ) , enumerate their combinations on large-scale pretraining , and then study a diverse set of V+L downstream tasks to derive the best pretraining strategy ( Table 3 ) . To explain the superior performance of UNITER , we believe the conditioned MLM/MRM better learns the local alignment ( token/region level ) across modalities and ITM enhances the global alignment ."}, {"review_id": "S1eL4kBYwr-2", "review_text": "This is an impressive paper. LIke BERT, it proposes a tranformer based approach to derive a pre-trained network for representing images and texts. The resulting pre-trained network, used in 9 different tasks, advances the SOTA on all the tasks. The major limitation of this paper is why. Why does it happen? How this results can be achieved? What is exactly represented in this pre-trained network. Why the tasks used for pre-training build a network that is so informative? This is really the major obscure point of this impressive paper. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your insightful comments . Our assumption is that UNITER learns contextualized joint representation of both modalities . In Section 3.1 , we proposed conditional masking that allows the model to learn informative representation of one modality conditioned on the other . Note that the conditional masking happens in both modalities . Therefore , the representation is aware of both visual and textual information . The reconstruction of masked tokens/regions can be viewed as learning local alignment across modalities . Furthermore , when combined with ITM pretraining , the global alignment between both modalities is encouraged . We also show that every pretraining task contributes to the final performance gain . To better address the reviewers \u2019 concern , we are working on attention visualization , and will update the paper before the discussion period ends ."}], "0": {"review_id": "S1eL4kBYwr-0", "review_text": "# 1. Summary The authors introduce a new pre-training procedure for image-text representations. The idea is to train the model on a huge collection of different image-text datasets and the use the model for downstream tasks. The difference between the proposal wrt the concurrent work is that conditioned masking is used: (i) Masked Language Modeling (MLM) conditioned on image; (ii) Masked Region Modeling (MRM) conditioned on text; and (iii) joint Image-Text Matching (ITM). I am on the fence for this paper given the balance between strengths and weaknesses listed below. I am conservative here and decide for weak reject; but I am open for discussion, if the authors answer to my concerns detailed below. Strengths: * State-of-the-art results on several downstream vision-language tasks * Empirical work to investigate different ways to perform conditioned masking Weaknesses: * Some parts of the method needs clarification (see point 2 below) to better understand the details and practical advantages of the method. * Limited novelty: the paper is an extension of BERT to the visual domain (see point 3 below) # 2. Clarity and Motivation The paper reads quite well, although some points need to be improved: * \"Compared with LXMERT (Tan & Bansal, 2019) and ViLBERT (Lu et al., 2019) that use two streams (one Transformer for each modality), our UNITER model can learn joint contextualized ...\", why is this an advantage? Using two streams might also lead to learning context? Maybe an example can clarify my question. * End of Sec. 3.1 (and paragraph in Sec. 3.2): not clear how the model is training for ITM. What's the input and output? Why do you need a new symbol [CLS]? * Sec. 3.2 ITM: \"an additional special token [CLS] is fed into our model, which indicates the fused representation of both modalities\" - This is not clear. Why this special token is needed? Why is not needed in the MLM and MRM? * \"The scoring function is denoted as s\" -> please indicate in the main text what function you used * MRFM and MRC are clear, however the intuition of MRC-kl is missing. Why is this needed? What does it mean in practice to minimize such divergence (provide practical example)? * Combination of tasks (MLM + ITM + MRC-kl + MRFR) -> it is not clear how this is done in practice. Is the loss function composed (summed)? Within the mini-batch, the method randomly chooses which operation to do (e.g., MLM) for each sample? This should be clarified in the main text of the paper. # 3. Novelty The novelty of the paper is quite limited since it is an extension of BERT to the visual domain. The authors propose an empirical analysis of different ways to mask the visual input, however this might not be a substantial extension of previous work. In fact, recently there are many other papers (ViLBERT, VisualBERT, LXBERT, ...) working on similar topic with small differences. What it is missing in this paper is an understanding and intuition on the reasons why the conditioned masking idea should be better than the other visual masking ideas proposed in previous work. # 4. Experimentation The main advantage of this paper relies on the extensive experimental analysis done on many challenging datasets reaching the state of the art on several downstream tasks. The evaluation on both pre-training tasks and downstream tasks show that the method is working well in practice.", "rating": "6: Weak Accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- original question -- -- -- -- -- -- -- -- -- -- -- -- -- - * `` Compared with LXMERT ( Tan & Bansal , 2019 ) and ViLBERT ( Lu et al. , 2019 ) that use two streams ( one Transformer for each modality ) , our UNITER model can learn joint contextualized ... '' , why is this an advantage ? Using two streams might also lead to learning context ? Maybe an example can clarify my question . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Our argument is not about \u201c single-stream being strictly better than two-stream \u201d . In fact , we have tried a SOTA two-stream model ( MCAN , Yu et al. , 2019 ) in our preliminary experiment before the aforementioned related works are published , and found that it did not work as well as single-stream model . We therefore continued with the single-stream architecture , and focused on finding the most effective pretraining strategy , which is our main contribution . Note that both LXMERT and ViLBERT promoted two-stream models , yet we showed that single-stream model is sufficient to learn contextual representations across two modalities ."}, "1": {"review_id": "S1eL4kBYwr-1", "review_text": "This paper presents a novel method for image-text representations called UNITER. The proposed method has been subsequently tested in many downstream tasks. A detailed ablation study helps to understand the role of each pretrained task in the proposed model. Although the empirical results are nice, performing the intensive set of experiments on many different tasks is definitely time-consuming and needs a lot of engineering efforts, the technical contribution does not seem significant to me. The paper modifies an existing pre-training procedure by conditional masking (Section 2). I agree this is well-motivated but it has little novelty and a similar idea is there in VQA (See \u201cDynamic fusion with intra and inter-modality attention flow for visual question answering\u201d). MLM and MRM are not new training procedure either, they are basically extending the BERT\u2019s training procedure with the consideration of multiple modalities. I have some questions for the authors: (1) What are the advantages of using single-stream transformer over two-stream transformer (page 2). I guess it leads to fewer parameters but I don\u2019t think this is a big problem. (2) Some visualization of attention weights would be helpful. Minor \u2022 In \u201cm \\e N^M\u201d (equation 1), what is N and M? ", "rating": "6: Weak Accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- original question -- -- -- -- -- -- -- -- -- -- -- -- -- - The paper modifies an existing pre-training procedure by conditional masking ( Section 2 ) . I agree this is well-motivated , but it has little novelty and a similar idea is there in VQA ( See \u201c Dynamic fusion with intra and inter-modality attention flow for visual question answering \u201d ) . MLM and MRM are not new training procedure either , they are basically extending the BERT \u2019 s training procedure with the consideration of multiple modalities . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Thank you for referring us a related work . After a thorough check of the paper , we agree with the reviewer this is also a relevant work . Nevertheless , Gao et al. , ( 2019 ) may focus on novel model architecture , while we proposed a generic V+L representation via pretraining . We will cite the paper and discuss it in the related work . Secondly , we argue that UNITER is not trivially derived from BERT . Even for BERT , language modeling has been around for years ( CBOW , Mikolov et al. , 2013 ) . Intuitively , mask-then-reconstruct is helpful for learning contextualization , but the key is what exact pretraining tasks are effective , especially for learning \u201c alignment \u201d across vision and language in our case . That \u2019 s why we proposed ITM , MLM , MRM ( 3 variants ) , enumerate their combinations on large-scale pretraining , and then study a diverse set of V+L downstream tasks to derive the best pretraining strategy ( Table 3 ) . To explain the superior performance of UNITER , we believe the conditioned MLM/MRM better learns the local alignment ( token/region level ) across modalities and ITM enhances the global alignment ."}, "2": {"review_id": "S1eL4kBYwr-2", "review_text": "This is an impressive paper. LIke BERT, it proposes a tranformer based approach to derive a pre-trained network for representing images and texts. The resulting pre-trained network, used in 9 different tasks, advances the SOTA on all the tasks. The major limitation of this paper is why. Why does it happen? How this results can be achieved? What is exactly represented in this pre-trained network. Why the tasks used for pre-training build a network that is so informative? This is really the major obscure point of this impressive paper. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your insightful comments . Our assumption is that UNITER learns contextualized joint representation of both modalities . In Section 3.1 , we proposed conditional masking that allows the model to learn informative representation of one modality conditioned on the other . Note that the conditional masking happens in both modalities . Therefore , the representation is aware of both visual and textual information . The reconstruction of masked tokens/regions can be viewed as learning local alignment across modalities . Furthermore , when combined with ITM pretraining , the global alignment between both modalities is encouraged . We also show that every pretraining task contributes to the final performance gain . To better address the reviewers \u2019 concern , we are working on attention visualization , and will update the paper before the discussion period ends ."}}