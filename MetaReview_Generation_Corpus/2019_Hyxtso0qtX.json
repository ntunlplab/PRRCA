{"year": "2019", "forum": "Hyxtso0qtX", "title": "Adversarial Exploration Strategy for Self-Supervised Imitation Learning", "decision": "Reject", "meta_review": "This paper proposes a method for incentivizing exploration in self-supervised learning using an inverse model, and then uses the learned inverse model for imitating an expert demonstration. The approach of incentivizing the agent to visit transitions where a learned model performs poorly. This relates to prior work (e.g. [1]), but using an inverse model instead of a forward model. The results are promising on challenging problem domains, and the method is simple. The authors have addressed several of the reviewer concerns throughout the discussion period.\nHowever, three primary concerns remain:\n(A) First and foremost: There has been confusion about the problem setting and the comparisons. I think these confusions have stemmed from the writing in the paper not being sufficiently clear. First, it should be made clear in the plots that the \"Demos\" comparison is akin to an oracle. Second, the difference between self-supervised imitation learning (IL) and traditional IL needs to be spelled out more clearly in the paper. Given that self-supervised imitation learning is not a previously established term, the problem statement needs to be clearly and formally described (and without relying heavily on prior papers). Further, the term self-supervised imitation learning does not seem to be an appropriate term, since imitation learning from an expert is, by definition, not self-supervised, as it involves supervisory information from an expert. Changing this term and clearly defining the problem would likely lead to less confusion about the method and the relevant comparisons.\n(B) The \"Demos\" comparison is meant as an upper bound on the performance of this particular approach. However, it is also important to understand what the upper bound is on these problems in general, irrespective of whether or not an inverse model is used. Training a policy with behavior cloning on demonstrations with many (s,a) pairs would be able to better provide such a comparison.\n(C) Inverse models inherently model the part of the environment that is directly controllable (e.g. the robot arm), and often do not effectively model other aspects of the environment that are only indirectly controllable (e.g. the objects). If the method overcomes this issue, then that should be discussed in the paper. Otherwise, the limitation should be outlined and discussed in more detail, including text that outlines which forms of problems and environments this approach is expected to be able to handle, and which of those it cannot handle.\n\nGenerally, this paper is quite borderline, as indicated by the reviewer's scores. After going through the reviews and parts of the paper in detail, I am inclined to recommend reject as I think the above concerns do not outweigh the pros.\n\nOne more minor comment is that the paper should consider mentioning the related work by Torabi et al. [2], which considers a similar approach in a slightly different problem setting.\n\n[1] Stadie et al. https://arxiv.org/abs/1507.00814\n[2] Torabi et al. IJCAI '18 (https://arxiv.org/abs/1805.01954)", "reviews": [{"review_id": "Hyxtso0qtX-0", "review_text": "This paper presents a system for self-supervised imitation learning using a RL agent that is rewarded for finding actions that the system does not yet predict well given the current state. More precisely, an imitation learner I is trained to predict an action A given a desired observation state transition xt->xt+1; the training samples for I are generated using a RL policy that yields an action A to train given xt (a physics engine evaluates xt+1 from xt and A). The RL policy is rewarded using the loss incurred by I's prediction of A, so that moderately high loss values produce highest reward. In this way, the RL agent learns to produce effective training samples that are not too easy or hard for the learner. The method is evaluated on five block manipulation tasks, comparing to training samples generated by other recent self-supervised methods, as well as those found using a pretrained expert model for each task. Overall, this method exploration seems quite effective on the tasks evaluated. I'd be curious to know more about the limits and failures of the method, e.g. in other types of environments. Additional questions: - p.2 mentions that the environments \"are intentionally selected by us for evaluating the performance of inverse dynamics model, as each of them allows only a very limited set of chained actions\". What sort of environments would be less well fit? Are there any failure cases of this method where other baselines perform better? - sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy, but in Fig 2, the success rates do not coincide between the systems and the random baseline, at either samples=0 or samples=30k --- should they? if not, what differences caused this? - figs. 4, 5 and 6 all relate to the stabilizer value delta, and I have a couple questions here: (i) for what delta does performance start to degrade? At delta=inf, I think it should be the same as no stabilizer, while at delta=0 is the exact opposite reward (i.e. negative loss, easy samples). (ii) delta=3 is evaluated, and performance looks decent for this in fig 6 --- but fig 4 shows that the peak PDF of \"no stabilizer\" is around 3 as well, yet \"no stabilizer\" performs poorly in Fig 5. Why is this, if it tends to produce actions with loss around 3 in both cases? ", "rating": "7: Good paper, accept", "reply_text": "Here is the PDF version of our responses : https : //www.dropbox.com/s/707hka5ba9abg54/Response_2_ICLR_2019.pdf ? dl=0 ( anonymous link ) The authors appreciate the reviewer \u2019 s the time and efforts for reviewing this paper , and would like to respond to the questions in the following paragraphs . Q1 : Overall , this method exploration seems quite effective on the tasks evaluated . I 'd be curious to know more about the limits and failures of the method , e.g. , in other types of environments . Response : We would like to bring to the reviewer \u2019 s kind attention that although our method outperforms all the baseline methods in most of the tasks , all the methods ( including ours ) are unable to surpass the \u201c Demo \u201d baseline in the HandReach task . The observation implies that with regard to inverse dynamics model training , the contemporary self-supervised data-collection strategies ( including ours and the baseline methods ) are not as effective as training directly with expert demonstrations for this task . We consider that this task is a typical failure case for our method ( as well as the other baseline methods ) . The underlying rationale is presumably due to the difficulty for exploration in the high-dimensional action space of HandReach , as this is the major difference between it and the other tasks presented in our paper . We have included more analyses on the limitations and failure cases of our method in Section 4.3 of the revised version . Q2 : p.2 mentions that the environments `` are intentionally selected by us for evaluating the performance of inverse dynamics model , as each of them allows only a very limited set of chained actions '' . What sort of environments would be less well fit ? Are there any failure cases of this method where other baselines perform better ? Response : We would like to thank the reviewer for raising this question . In fact , environments that allow various valid actions for a given transition ( x_t to x_ { t+1 } ) would be less well fit for our method . As we train the inverse dynamic model by minimizing mean-square error between the predicted action a and the ground truth action \u00e2 ( Eq . ( 5 ) ) , multiple ground truth actions for the same transition would lead to high variance in the derived gradients . This is referred to as the \u201c multimodality problem \u201d and has been discussed in [ 1 ] . As the main focus of this paper is to investigate the effectiveness of the proposed adversarial exploration strategy for self-supervised imitation learning , we do not incorporate these environments and the multimodality problem in our scope of discussion to avoid confusion and potential distraction of the main subject . Q3 : Sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy , but in Fig 2 , the success rates do not coincide between the systems and the random baseline , at either samples=0 or samples=30k -- - should they ? if not , what differences caused this ? Response : We would like to sincerely apologize for the misunderstanding caused . In our experiments , pre-training with random samples is only applied to the HandReach task due to its high complexity in exploration . This is also the primary subject that we intend to discuss in Section 4.3 . As a result , the success rates of all the methods are the same in the HandReach task for the first 30K samples , as you have noticed . For the other tasks , we do not pre-train the models with random samples . Therefore , their success rates before 30K do not coincide with that of the \u201c Random \u201d baseline ."}, {"review_id": "Hyxtso0qtX-1", "review_text": "The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces. The core of the method is to train an inverse local model (a model that predicts the action that was taken from a pair of states) and its errors as an exploration bonus for a policy gradient agent. The intuition is that its a good self-regulating strategy similar to curiosity that leads the agents towards states that are less known by the inverse model. Seeing these states improves the . There are experiments run on the OpenAI gym comparing to other models of curiosity. The paper is well written and clear for the most part. pros: - the paper seems novel and results are promising - easy to implement cons: - seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body. - only accounts for the immediately controllable aspects of the environment which doesn't seem to be the hard part. Understanding the rest of the environment and its relationship to the controllable part of the state seems beyond the scope of this model. Nonetheless I can imagine it helping with initial random motions. - from (6) the bonus seems to be unbounded and (7) doesn't seem to fix that. Is that not an issue in general ? Any intuition about that ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Here is the PDF version of our responses : https : //www.dropbox.com/s/0r2hztg7af87934/Response_1_ICLR_2019.pdf ? dl=0 ( anonymous link ) The authors appreciate the reviewer \u2019 s the time and efforts for reviewing this paper , and would like to respond to the questions in the following paragraphs . Q1 : The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces . Response : We would like to sincerely apologize for the misunderstanding caused . In fact , our adversarial exploration strategy is a self-supervised data-collection strategy developed for training an inverse dynamics model , which is formally described in Sections 2.2 and 3 . Different from exploration strategies for deep reinforcement learning ( DRL ) ( which aim at learning policies for maximizing the expected returns in RL tasks ) , our work focuses on discovering a policy for collecting a useful training dataset for an inverse dynamics model . Q2 : Seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body . Response : We would like to address the reviewer \u2019 s concerns in the following two paragraphs . First , with regard to the \u201c instability \u201d issue , we are not quite sure which aspect the reviewer refers to , and would appreciate it if the reviewer could kindly share some more information with us . We assume that the reviewer could be referring to either the variance of the training losses , or the variance in the learning curves of Fig.3.For the former case , a stabilization technique is presented in Section 3.3 , and its effectiveness is analyzed and validated in Section 4.5 . For the latter case , the variance in the learning curves is mainly caused by the complexity of the high-dimensional observation spaces . As contemporary DRL techniques also suffer from the same problems when training with raw images ( i.e. , high-dimensional observation spaces ) [ 1 ] , the high variance in the learning curves can similarly be alleviated by enhancing the model architecture or the training algorithm . Please note that this issue also occurs in the learning curves of the other baseline methods . Discussion of model architectures and specific training techniques for high-dimensional observation spaces , however , is beyond the scope of this paper . Second , we agree with the reviewer that learning a policy in an environment with a large state space has been a challenging research topic . However , in the past few years , a number of DRL methods have been proposed and achieved remarkable successes in such environments , including humanoid robotic control [ 2 ] . The successes of these methods indicate that even in an environment with a large state space , it is still possible to discover an effective policy that maximizes the expected return . In the proposed adversarial exploration strategy , we train a DRL agent to maximize the expected losses of the inverse dynamics model ( Eq . ( 6 ) ) .As DRL methods have been shown effective in exploiting arbitrary reward functions in large state spaces in the literature , we consider that our method can be extended to environments with large state spaces , and exploit the losses of the inverse dynamics model to collect training samples accordingly ."}, {"review_id": "Hyxtso0qtX-2", "review_text": "The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert\u2019s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. Overall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues: - the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly. - Section 4.2.: \u201cAs opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.\u201d - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. - I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce \u201cnot too hard\u201d training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \\delta > 0. - Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy\u2019s deterioration you could report the relative change, negative when the performance goes down and positive otherwise? I do have concerns regarding the experimental evaluation: - the \u201cDemos\u201d baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000? How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform \u201cDemos\u201d, which it does pretty often. - it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results? - the results of \u201cHandReach\u201d experiments, where the proposed method works much worse than \u201cDemos\u201d are not discussed in the text at all - overall, there is no example of the proposed method making a difference between a \u201cworking\u201d and \u201cnon-working\u201d system, compared to \u201cCuriosity\u201d and \u201cRandom\u201d. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. \"Demos\" baseline doesn't perform much better, but what would happen with 10000 demonstrations? - there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like \"Demos\" does? I think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Here is the PDF version of our responses : https : //www.dropbox.com/s/mxhetdyy7m4nkp6/Response_3_ICLR_2019.pdf ? dl=0 ( anonymous link ) The authors appreciate the reviewer \u2019 s the time and efforts for reviewing this paper , and would like to respond to the questions in the following paragraphs . Q1 : The introduction has not made it crystal clear that the considered paradigm is different from e.g.DAGGER and GAIL in that expert demonstrations are used at the inference time . A much wider audience is familiar with the former methods , and this distinction should have be explained more clearly . Response : The authors appreciate the thoughtful feedbacks from the reviewer , and would like to bring to the reviewer \u2019 s kind attention that in Section 1 , the following sentence for describing this distinction had been included in the original manuscript : \u201c Self-supervised IL allows an imitator to collect training data by itself instead of using predefined extrinsic reward functions or expert supervision during training . It only needs demonstration during inference , drastically decreasing the time and effort required from human experts. \u201d Q2 : Section 4.2. : \u201c As opposed to discrete control domains , these tasks are especially challenging , as the sample complexity grows in continuous control domains. \u201d - this sentence did not make sense to me . It basically says continuous control is challenging because it is challenging . Response : Thanks for sharing your thoughts with us . We would love to clarify the meaning of this sentence as follows . It is challenging to train inverse dynamics models in continuous control domains because of its requirement of diverse sample data during the training phase . The diverse training data should cover various state transitions ( i.e. , ( x_t , x_ { t+1 } ) ) as well as their corresponding actions ( i.e. , a_t ) . To this end , an explorer ( i.e. , the self-supervised data-collection DRL agent ) is required to visit a wide range of states and extensively attempt different actions . Such a requirement can be easily achieved in discrete control domains , as their action spaces are not as large as those of the continuous domain counterparts , allowing an explorer to quickly travel through most of possible state transitions . In contrast , due to the enormous number of potential actions , the sample complexity ( https : //en.wikipedia.org/wiki/Sample_complexity ) of continuous control domains is significantly higher than that of discrete ones . This is the reason we called it challenging in the original manuscript . We sincerely hope that we have adequately addressed your concerns . Q3 : I did not understand the stabilization approach . How exactly Eq . ( 7 ) forces the policy to produce \u201c not too hard \u201d training examples for IDM ? Fig.4 shows that it is on the opposite examples with small L_I that are avoided by using \u03b4 > 0 . Response : The main objective of Eq . ( 7 ) is to shape the rewards ( i.e. , the losses of the inverse dynamics model ) to be the negative L1-distance to \u03b4 . In other words , the closer the original unshaped reward is to \u03b4 , the higher the shaped reward is . As a result , the stabilization technique presented in Eq . ( 7 ) encourages the RL agent to collect \u201c moderately difficult \u201d samples that regulates the losses of the inverse dynamics model to be around \u03b4. Q4 : Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here . Perhaps instead of policy \u2019 s deterioration you could report the relative change , negative when the performance goes down and positive otherwise ? Response : The authors appreciate the thoughtful feedbacks from the reviewer , and have rephrased the term as \u201c performance change rate \u201d and made the other necessary revisions in Section 4.4 according to the suggestions in the updated manuscript . Q5 : The \u201c Demos \u201d baseline approach should be explained in the main text ! In Appendix S.7 I see that 1000 human demonstrations were used for training . Why 1000 , and not 100 and not 10000 ? How would the results change ? This needs to be discussed . Without discussing this it is really unclear how the proposed method can outperform \u201c Demos \u201d , which it does pretty often . Response : We fully agree with your comments regarding the number of demonstrations . To address your concerns , we have incorporated an additional figure illustrating the learning curves of the Demo baseline with various number of demonstrations in the supplementary material . According to Fig.7 , we do not observe any significant difference in performance when the number of demonstrations is set to 100 , 1,000 , and 10,000 . This is the reason why we used 1,000 demonstrations for training this baseline method in our experiments ."}], "0": {"review_id": "Hyxtso0qtX-0", "review_text": "This paper presents a system for self-supervised imitation learning using a RL agent that is rewarded for finding actions that the system does not yet predict well given the current state. More precisely, an imitation learner I is trained to predict an action A given a desired observation state transition xt->xt+1; the training samples for I are generated using a RL policy that yields an action A to train given xt (a physics engine evaluates xt+1 from xt and A). The RL policy is rewarded using the loss incurred by I's prediction of A, so that moderately high loss values produce highest reward. In this way, the RL agent learns to produce effective training samples that are not too easy or hard for the learner. The method is evaluated on five block manipulation tasks, comparing to training samples generated by other recent self-supervised methods, as well as those found using a pretrained expert model for each task. Overall, this method exploration seems quite effective on the tasks evaluated. I'd be curious to know more about the limits and failures of the method, e.g. in other types of environments. Additional questions: - p.2 mentions that the environments \"are intentionally selected by us for evaluating the performance of inverse dynamics model, as each of them allows only a very limited set of chained actions\". What sort of environments would be less well fit? Are there any failure cases of this method where other baselines perform better? - sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy, but in Fig 2, the success rates do not coincide between the systems and the random baseline, at either samples=0 or samples=30k --- should they? if not, what differences caused this? - figs. 4, 5 and 6 all relate to the stabilizer value delta, and I have a couple questions here: (i) for what delta does performance start to degrade? At delta=inf, I think it should be the same as no stabilizer, while at delta=0 is the exact opposite reward (i.e. negative loss, easy samples). (ii) delta=3 is evaluated, and performance looks decent for this in fig 6 --- but fig 4 shows that the peak PDF of \"no stabilizer\" is around 3 as well, yet \"no stabilizer\" performs poorly in Fig 5. Why is this, if it tends to produce actions with loss around 3 in both cases? ", "rating": "7: Good paper, accept", "reply_text": "Here is the PDF version of our responses : https : //www.dropbox.com/s/707hka5ba9abg54/Response_2_ICLR_2019.pdf ? dl=0 ( anonymous link ) The authors appreciate the reviewer \u2019 s the time and efforts for reviewing this paper , and would like to respond to the questions in the following paragraphs . Q1 : Overall , this method exploration seems quite effective on the tasks evaluated . I 'd be curious to know more about the limits and failures of the method , e.g. , in other types of environments . Response : We would like to bring to the reviewer \u2019 s kind attention that although our method outperforms all the baseline methods in most of the tasks , all the methods ( including ours ) are unable to surpass the \u201c Demo \u201d baseline in the HandReach task . The observation implies that with regard to inverse dynamics model training , the contemporary self-supervised data-collection strategies ( including ours and the baseline methods ) are not as effective as training directly with expert demonstrations for this task . We consider that this task is a typical failure case for our method ( as well as the other baseline methods ) . The underlying rationale is presumably due to the difficulty for exploration in the high-dimensional action space of HandReach , as this is the major difference between it and the other tasks presented in our paper . We have included more analyses on the limitations and failure cases of our method in Section 4.3 of the revised version . Q2 : p.2 mentions that the environments `` are intentionally selected by us for evaluating the performance of inverse dynamics model , as each of them allows only a very limited set of chained actions '' . What sort of environments would be less well fit ? Are there any failure cases of this method where other baselines perform better ? Response : We would like to thank the reviewer for raising this question . In fact , environments that allow various valid actions for a given transition ( x_t to x_ { t+1 } ) would be less well fit for our method . As we train the inverse dynamic model by minimizing mean-square error between the predicted action a and the ground truth action \u00e2 ( Eq . ( 5 ) ) , multiple ground truth actions for the same transition would lead to high variance in the derived gradients . This is referred to as the \u201c multimodality problem \u201d and has been discussed in [ 1 ] . As the main focus of this paper is to investigate the effectiveness of the proposed adversarial exploration strategy for self-supervised imitation learning , we do not incorporate these environments and the multimodality problem in our scope of discussion to avoid confusion and potential distraction of the main subject . Q3 : Sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy , but in Fig 2 , the success rates do not coincide between the systems and the random baseline , at either samples=0 or samples=30k -- - should they ? if not , what differences caused this ? Response : We would like to sincerely apologize for the misunderstanding caused . In our experiments , pre-training with random samples is only applied to the HandReach task due to its high complexity in exploration . This is also the primary subject that we intend to discuss in Section 4.3 . As a result , the success rates of all the methods are the same in the HandReach task for the first 30K samples , as you have noticed . For the other tasks , we do not pre-train the models with random samples . Therefore , their success rates before 30K do not coincide with that of the \u201c Random \u201d baseline ."}, "1": {"review_id": "Hyxtso0qtX-1", "review_text": "The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces. The core of the method is to train an inverse local model (a model that predicts the action that was taken from a pair of states) and its errors as an exploration bonus for a policy gradient agent. The intuition is that its a good self-regulating strategy similar to curiosity that leads the agents towards states that are less known by the inverse model. Seeing these states improves the . There are experiments run on the OpenAI gym comparing to other models of curiosity. The paper is well written and clear for the most part. pros: - the paper seems novel and results are promising - easy to implement cons: - seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body. - only accounts for the immediately controllable aspects of the environment which doesn't seem to be the hard part. Understanding the rest of the environment and its relationship to the controllable part of the state seems beyond the scope of this model. Nonetheless I can imagine it helping with initial random motions. - from (6) the bonus seems to be unbounded and (7) doesn't seem to fix that. Is that not an issue in general ? Any intuition about that ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Here is the PDF version of our responses : https : //www.dropbox.com/s/0r2hztg7af87934/Response_1_ICLR_2019.pdf ? dl=0 ( anonymous link ) The authors appreciate the reviewer \u2019 s the time and efforts for reviewing this paper , and would like to respond to the questions in the following paragraphs . Q1 : The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces . Response : We would like to sincerely apologize for the misunderstanding caused . In fact , our adversarial exploration strategy is a self-supervised data-collection strategy developed for training an inverse dynamics model , which is formally described in Sections 2.2 and 3 . Different from exploration strategies for deep reinforcement learning ( DRL ) ( which aim at learning policies for maximizing the expected returns in RL tasks ) , our work focuses on discovering a policy for collecting a useful training dataset for an inverse dynamics model . Q2 : Seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body . Response : We would like to address the reviewer \u2019 s concerns in the following two paragraphs . First , with regard to the \u201c instability \u201d issue , we are not quite sure which aspect the reviewer refers to , and would appreciate it if the reviewer could kindly share some more information with us . We assume that the reviewer could be referring to either the variance of the training losses , or the variance in the learning curves of Fig.3.For the former case , a stabilization technique is presented in Section 3.3 , and its effectiveness is analyzed and validated in Section 4.5 . For the latter case , the variance in the learning curves is mainly caused by the complexity of the high-dimensional observation spaces . As contemporary DRL techniques also suffer from the same problems when training with raw images ( i.e. , high-dimensional observation spaces ) [ 1 ] , the high variance in the learning curves can similarly be alleviated by enhancing the model architecture or the training algorithm . Please note that this issue also occurs in the learning curves of the other baseline methods . Discussion of model architectures and specific training techniques for high-dimensional observation spaces , however , is beyond the scope of this paper . Second , we agree with the reviewer that learning a policy in an environment with a large state space has been a challenging research topic . However , in the past few years , a number of DRL methods have been proposed and achieved remarkable successes in such environments , including humanoid robotic control [ 2 ] . The successes of these methods indicate that even in an environment with a large state space , it is still possible to discover an effective policy that maximizes the expected return . In the proposed adversarial exploration strategy , we train a DRL agent to maximize the expected losses of the inverse dynamics model ( Eq . ( 6 ) ) .As DRL methods have been shown effective in exploiting arbitrary reward functions in large state spaces in the literature , we consider that our method can be extended to environments with large state spaces , and exploit the losses of the inverse dynamics model to collect training samples accordingly ."}, "2": {"review_id": "Hyxtso0qtX-2", "review_text": "The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert\u2019s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. Overall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues: - the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly. - Section 4.2.: \u201cAs opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.\u201d - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. - I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce \u201cnot too hard\u201d training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \\delta > 0. - Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy\u2019s deterioration you could report the relative change, negative when the performance goes down and positive otherwise? I do have concerns regarding the experimental evaluation: - the \u201cDemos\u201d baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000? How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform \u201cDemos\u201d, which it does pretty often. - it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results? - the results of \u201cHandReach\u201d experiments, where the proposed method works much worse than \u201cDemos\u201d are not discussed in the text at all - overall, there is no example of the proposed method making a difference between a \u201cworking\u201d and \u201cnon-working\u201d system, compared to \u201cCuriosity\u201d and \u201cRandom\u201d. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. \"Demos\" baseline doesn't perform much better, but what would happen with 10000 demonstrations? - there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like \"Demos\" does? I think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Here is the PDF version of our responses : https : //www.dropbox.com/s/mxhetdyy7m4nkp6/Response_3_ICLR_2019.pdf ? dl=0 ( anonymous link ) The authors appreciate the reviewer \u2019 s the time and efforts for reviewing this paper , and would like to respond to the questions in the following paragraphs . Q1 : The introduction has not made it crystal clear that the considered paradigm is different from e.g.DAGGER and GAIL in that expert demonstrations are used at the inference time . A much wider audience is familiar with the former methods , and this distinction should have be explained more clearly . Response : The authors appreciate the thoughtful feedbacks from the reviewer , and would like to bring to the reviewer \u2019 s kind attention that in Section 1 , the following sentence for describing this distinction had been included in the original manuscript : \u201c Self-supervised IL allows an imitator to collect training data by itself instead of using predefined extrinsic reward functions or expert supervision during training . It only needs demonstration during inference , drastically decreasing the time and effort required from human experts. \u201d Q2 : Section 4.2. : \u201c As opposed to discrete control domains , these tasks are especially challenging , as the sample complexity grows in continuous control domains. \u201d - this sentence did not make sense to me . It basically says continuous control is challenging because it is challenging . Response : Thanks for sharing your thoughts with us . We would love to clarify the meaning of this sentence as follows . It is challenging to train inverse dynamics models in continuous control domains because of its requirement of diverse sample data during the training phase . The diverse training data should cover various state transitions ( i.e. , ( x_t , x_ { t+1 } ) ) as well as their corresponding actions ( i.e. , a_t ) . To this end , an explorer ( i.e. , the self-supervised data-collection DRL agent ) is required to visit a wide range of states and extensively attempt different actions . Such a requirement can be easily achieved in discrete control domains , as their action spaces are not as large as those of the continuous domain counterparts , allowing an explorer to quickly travel through most of possible state transitions . In contrast , due to the enormous number of potential actions , the sample complexity ( https : //en.wikipedia.org/wiki/Sample_complexity ) of continuous control domains is significantly higher than that of discrete ones . This is the reason we called it challenging in the original manuscript . We sincerely hope that we have adequately addressed your concerns . Q3 : I did not understand the stabilization approach . How exactly Eq . ( 7 ) forces the policy to produce \u201c not too hard \u201d training examples for IDM ? Fig.4 shows that it is on the opposite examples with small L_I that are avoided by using \u03b4 > 0 . Response : The main objective of Eq . ( 7 ) is to shape the rewards ( i.e. , the losses of the inverse dynamics model ) to be the negative L1-distance to \u03b4 . In other words , the closer the original unshaped reward is to \u03b4 , the higher the shaped reward is . As a result , the stabilization technique presented in Eq . ( 7 ) encourages the RL agent to collect \u201c moderately difficult \u201d samples that regulates the losses of the inverse dynamics model to be around \u03b4. Q4 : Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here . Perhaps instead of policy \u2019 s deterioration you could report the relative change , negative when the performance goes down and positive otherwise ? Response : The authors appreciate the thoughtful feedbacks from the reviewer , and have rephrased the term as \u201c performance change rate \u201d and made the other necessary revisions in Section 4.4 according to the suggestions in the updated manuscript . Q5 : The \u201c Demos \u201d baseline approach should be explained in the main text ! In Appendix S.7 I see that 1000 human demonstrations were used for training . Why 1000 , and not 100 and not 10000 ? How would the results change ? This needs to be discussed . Without discussing this it is really unclear how the proposed method can outperform \u201c Demos \u201d , which it does pretty often . Response : We fully agree with your comments regarding the number of demonstrations . To address your concerns , we have incorporated an additional figure illustrating the learning curves of the Demo baseline with various number of demonstrations in the supplementary material . According to Fig.7 , we do not observe any significant difference in performance when the number of demonstrations is set to 100 , 1,000 , and 10,000 . This is the reason why we used 1,000 demonstrations for training this baseline method in our experiments ."}}