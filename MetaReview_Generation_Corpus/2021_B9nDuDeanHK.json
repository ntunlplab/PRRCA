{"year": "2021", "forum": "B9nDuDeanHK", "title": "Weights Having Stable Signs Are Important: Finding Primary Subnetworks and Kernels to Compress Binary Weight Networks", "decision": "Reject", "meta_review": "## Description\nThe paper discovers interesting phenomena in training neural networks with binary weights:\n- Connection between latent weight magnitude and how important its binarized version for the network performance\n-training dynamics, indicating that large latent weights are identified and stabilize early on\n- Observation that amongst learned binary kernel, several specific patterns prevail, up to the bits who's reversal has very little effect. This is so regardless of the architecture, the layer considered or the dataset. \nThe paper further demonstrates how these observations may be used to compress binary neural networks below 1 bit per weight.\n\n## Review Process and Decision\nThe reviewers welcomed the experimental investigation of new phenomena, but commented the overall technical quality of the work as somewhat substandard. The redundancy of consecutive affine transforms is known and not connected to binary weights investigation. The investigation itself lacks a more in-depth analysis. The proposed compression results appeared not convincing to reviewers since a significant drop of accuracy occurs. The AC shares these concerns and supports rejection.\n\n## General Comments\nFrom my perspective, the study undertaken is methodologically \u201ewrong\u201c. An ad-hoc training method is investigated, which is not even clearly defined in the paper (there are many \u201eSTE\u201c variants) and for which it is not known what it is doing, what are the real-valued weights for and whether they are needed at all (as empirically argued by Helwegen et al. (2019)). As such, the investigation makes impression of poking a black box (the training method in this case). At the same time, there are more clear learning formulations, applicable in the setting of the paper (binary weights), in particular considering the stochastic relaxation:\n* Shayer et al. (2017): Learning Discrete Weights Using the Local Reparameterization Trick\n* Roth et al. (2019): Training Discrete-Valued Neural Networks with Sign Activations Using Weight Distributions\n* Peters et al. (2018): Probabilistic binary neural networks\n\nThese methods are approximate, but at least the optimization is well posed and it is known what do the real-valued weights represent (e.g. logits of binary weight probabilities).\nFrom this perspective, it can be seen that latent weights close to 0 correspond to Bernoulli weights that are almost fully random (and thus only contribute noise) and are fragile to gradient steps. Therefore the model can only perform well if it learns to be robust to their state or their state becomes more deterministic (corresponding to large latent weight). So one would actually expect to see in these models phenomena similar to the observed in the paper and not bee too much surprised or astonished by them. Furthermore, there are recent works explaining STE and its latent weights as optimizing the stochastic relaxation:\n* Meng et al. (2020): Training Binary Neural Networks using the Bayesian Learning Rule\n* Yanush et al. (2020): Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks. \n\nThe authors are encouraged to make the observed phenomena more explainable by connecting to the mentioned works.\n\n## Further Details\n\n*  \u201eWe show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either hand-crafted or learnable methods brings marginal or no accuracy gain to final model.\u201c\n\nFrom theoretical perspective, this is obvious and known to me. Practically, there could be in principle some difference due to the learning dynamics, and verifying that there is none is a useful but a weak contribution. The section devoted to this issue can be given in the appendix but is not justified in the main paper.\n\n* \u201echange of weight signs is crucial in the training of BWNs\u201c\n\nThe sign determines the binary weights, so this is by definition.\n\n* \u201e Firstly, the training of BWNs demonstrates the process of seeking primary binary sub-networks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks\u201c\n\nIn the lottery ticket hypothesis paper it is shown explicitly that the identified sparse subnetwork changes during the learning the most rather than retains its initialization state or the state in the beginning of the training. It is therefore could be of a different nature.\n", "reviews": [{"review_id": "B9nDuDeanHK-0", "review_text": "Overview : The Authors show that scaling factors with hand-crafted or learnable methods are not so important when training Binary Weight Networks ( BWNs ) , while the change of weight signs is crucial . They make two observations : The weight signs of the primary binary sub-networks are determined and fixed at the early training stage . Binary kernels in the convolutional layers of final models tend to be centered on a limited number of fixed structural patterns . Based on these observations , they propose a new method called binary kernel quantization to further compress BWNs . Strength bullets : 1 . They propose a binary kernel quantization to quantize the binary kernel , which effectively reduces the number of all possible kernels , and can further compress BWNs to 2-5 times . 2.The Authors observe that the weight with the large norm , fixing their signs in the early training stage . And they compared this phenomenon with the lottery ticket hypothesis and propose the primary binary sub-networks . I think it 's an interesting idea . 3.The paper is well written and well-motivated . It contains extensive and systematic related work-study . I like it . Weakness bullets : 1 . The binary kernel quantization method is only applied to 3x3 kernels in this paper . When the size of the convolution kernel is 5x5 or 7x7\uff0chow does it work ? I hope this method can extend to another cases and it also can work well . 2.As shown in table1 , why authors report the best accuracy among different seed ? I think authors should report the averaged results with different random seeds instead of the best result . -- Post Rebuttal -- I have read all feedback and especially thanks to the authors ' efforts on the extra experiments . I think the author has addressed my first concern . For the second one , I would prefer to see the errorbar . I tend to keep my scores unchanged . I think the findings are interesting [ share similar thoughts as R4 's ] , while the experiments part need to be improved . Although I like the ideas and observations , I do n't feel especially strongly in favor of it and can not champion it .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 . * * Questions * * : \u201c The binary kernel quantization method is only applied to 3x3 kernels in this paper . When the size of the convolution kernel is 5x5 or 7x7 , how does it work ? I hope this method can extend to another cases and it also can work well. \u201d * * Answers * * : * * ( 1 ) * * On the one side , in the neural network quantization field , it is a common benchmarking protocol , retaining the first convolutional layer of a CNN model to still have full-precision values . On the other side , to the prevailing CNNs such as VGG , ResNets and MobileNets , large convolutional kernels like 5x5 or 7x7 are merely adopted in their first convolutional layer while the other convolutional layers all use 3x3 ( 2^9 binary patterns in total ) or 1x1 ( 2 binary patterns ) convolutional kernels . For a fair experimental study and comparison , in the previous version of our submission , we followed this protocol when applying the proposed method ( i.e. , Quantized Binary-kernel Networks ) to compress binary weight networks . This is why our method is only applied to 3x3 kernels in the experiments and analysis ( it was stated in Section 2.6 ) ; * * ( 2 ) * * Following your suggestion , we further applied our method to the first layer ( with 7x7 convolutional kernel ) of ResNet-20 as well as the other layers . Comparatively , for 7x7 convolutional kernel the number of possible binary patterns is 2^49 which is significantly larger than 2^9 for 3x3 convolutional kernel , and we observed most binary patterns only appear once or do not appear , leaving the flexibility to determine primary patterns . As a result , good compression results are also obtained and displayed as the followings : Network Quant Bit Acc ResNet-20 ( 64C5 ) 25-9-9-9 83.1 % ResNet-20 ( 64C5 ) 7-7-7-7 78.2 % ResNet-20 ( 64C5 ) 7-6-6-6 77.5 % ResNet-20 ( 64C7 ) 49-9-9-9 83.8 % ResNet-20 ( 64C7 ) 7-7-7-7 77.6 % ResNet-20 ( 64C7 ) 7-6-6-6 75.9 % 2 . * * Questions * * : \u201c As shown in table1 , why authors report the best accuracy among different seed ? I think authors should report the averaged results with different random seeds instead of the best result. \u201d * * Answers * * : It is a typo that we report the mean of 5 random runs \u2019 best accuracy during training . We have corrected this description to \u2018 the mean of the best accuracy during training among 5 runs with different random seeds \u2019 ."}, {"review_id": "B9nDuDeanHK-1", "review_text": "This paper provides an empirical study of binary weight networks ( BWNs ) , where they find that 1 the commonly adopted scaling factor is not critical 2 there exists a subnetwork that stabiles early in training 3 the 3x3 filters in VGG and ResNets demonstrate a sparse distribution . They combine all the observations and propose a novel quantization algorithm that achieves more aggressive compression than standard BWNs . pros : + I appreciate the careful examination of design and training details of standard BWNs . The identification of a persistent subnetwork and the analysis on the sparse distribution of kernels are particularly interesting . + The proposed quantization algorithm is interesting , which has a potential of squeezing more redundancy out of standard BWNs cons : - If I understand correctly , in the proposed algorithm the kernel distribution is only drawn from the last conv layer of the full precision network , which is then shared across all layers when retraining the BWN . This seems a strong assumption and needs to be justified . What 's the reason to believe that the selected frequent kernels are shared across different layers ? -In Algorithm 1 , W = where ( abs ( W ) > \u2206E , sign ( W ) , W ) is not motivated and explained well . What 's the reasoning of using the threshold when computing the distance to the frequency binary kernels ? -The experimental results seem to be really hard to interpret for me , and this is perhaps the weakest point of the this paper . In particular , Table 1 needs to have proper baselines . This includes the full precision , standard BWN accuracies , as well as controls which allow one to draw comparisons between the proposed algorithm and basic binarization by equating certain quantities . I suggest the authors work on the suggested improvements which will make this a much stronger contribution . * * * * * post rebuttal updates * * * * * I want to thank the authors for responding to my questions . The additional explanations are indeed helpful for clarifying my first two questions ( selection of the binary kernel and the use of \u2206E ) . However , I still have concerns about Table 1 ( and Table 2 ) . For example , I have a really hard time interpreting the significance of achieving a 3.2x CR with a loss of 3 % ( 92.3 - 89.2 from VGG-7 ) in acc with the proposed method ( although the paper argues that it 's a `` bearable '' loss ) . Considering that this is the main experiment supporting the efficacy of the proposed quantization algorithm , I think the paper needs more controlled experiments to demonstrate the practical usefulness of the proposed algorithm . As a result I 'm keeping my original score and hope the authors can work on the improvements for the next version .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 . * * Questions * * : If I understand correctly , in the proposed algorithm the kernel distribution is only drawn from the last conv layer of the full precision network , which is then shared across all layers when retraining the BWN . This seems a strong assumption and needs to be justified . What 's the reason to believe that the selected frequent kernels are shared across different layers ? \u2019 * * Answers * * : Though it contains a strong assumption that these selected binary kernels can generalize across different networks , different layers , and different datasets , we give two reasons for the justification : ( 1 ) We did an experiment to discover to what extend do the binary kernel distributions between VGG-7 BWN \u2019 s last layer \u2019 s and other different networks \u2019 different layers \u2019 in our Appendix L. In Figure 14 , we show that the selected binary kernels from one single-trail VGG-7 BWN \u2019 s last layer are highly corelated to those most frequent binary kernels in ResNet-18 \u2019 s different layers . For example , for the same k-bit kernels , the top 2^k frequent binary kernels contain a % of all binary kernels in one layer , and the selected 2^k binary kernels from VGG-7 BWN \u2019 s last layer contain b % ( b < =a because a % is already the highest percentage that 2^k binary kernels can achieve ) . So , we use b/a < = 1 to represent such correlation . b/a = 1 indicates that the top 2^k frequent binary kernels are identical to our selected 2^k binary kernels . In the figures , all are higher than 80 % , and for the deeper layers ( the last two blocks in ResNet-18 ) they are higher than 90 % . This indicates that those most frequent kernels are similar across different networks and different layers and different datasets . ( 2 ) Besides the correlation experiments , we also choose the selected kernels from the 2nd layer of VGG-7 , from 4th layer of VGG-7 , from the last conv layer of ResNet-18 , from the 9th conv layer of ResNet-18 ( the last conv of the second block of ResNet-18 ) , and merging the statistic information of these two layers of ResNet-18 . Then we apply these new selected binary kernels to VGG-7 on Cifar-10 as shown in Table.7 . 2 . * * Questions * * : In Algorithm 1 , W = where ( abs ( W ) > \u2206E , sign ( W ) , W ) is not motivated and explained well . What 's the reasoning of using the threshold when computing the distance to the frequency binary kernels ? * * Answers * * : In Section 4.2 , with the experiments we discuss about \u2018 Connection between Primary Binary Sub-Networks \u2019 . For this \u2206E , we give the explanation that this step is to make those large weights more important when calculating the L2 by assigning them with a larger number +-1 instead of their original norm ( the norm of weights are much smaller compared to 1 ) . In the paper we did further experiments on such threshold in Appendix O with Figure 16 , which indicates that \u2206=0 is worse in both VGG-7 and ResNet-56 with the worse performance compared to other cases that \u2206 > 0 . For \u2206 > 0 the optimal value for \u2206 is different case by case . 3 . * * Questions * * : The experimental results seem to be really hard to interpret for me , and this is perhaps the weakest point of the this paper . In particular , Table 1 needs to have proper baselines . This includes the full precision , standard BWN accuracies , as well as controls which allow one to draw comparisons between the proposed algorithm and basic binarization by equating certain quantities . * * Answers * * : Due to the page limit , in the paper we put the full-precision network and BWN baselines in Table 2 in Appendix . Follow your suggestion , we refer the results of baseline of full-precision network and BWN in Table 1 caption to make it easier to follow ."}, {"review_id": "B9nDuDeanHK-2", "review_text": "* * Summary : * * This paper proposes some interesting observations for training BWNs . 1 : The scaling factors can be removed with batch normalization used . 2 : The signs of the weights with large norms are determined and fixed at the early training stage . 3 : The binary weight networks can be further compressed . Moreover , the authors provide some empirical visualizations and results to demonstrate its analysis . However , the paper seems to be incomplete and needs to be further improved . * * Pros : * * The observation that the signs of weights with large norms are determined and fixed at the early training stage is interesting . The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWNs , maybe BNNs as well . * * Cons : * * 1 : Extensive descriptions are very confusing . I can only list some of them . 1 ) : The contributions of the paper include the observation that binary kernels in the learned convolutional layers tend to be centered on a limited number of fixed structural patterns . However , it is still not clear to me what the \u201c fixed structural patterns \u201d are . 2 ) : In Sec.3 , the paper claims to \u201c quantize the less frequent kernels to those high frequent kernels to save space \u201d and \u201c we sort these binary kernels according to their appearance frequency\u2026 \u201d . However , the paper fails to explain the motivation for exploring the frequency of kernels . Some theoretical explanations are needed . 3 ) : In Figure 4 \u2019 s caption , what does \u201c the certain appears in one certain Conv layer \u201d mean ? 4 ) : In Sec.4.1 , the authors propose to apply QBN on model compression . However , it is not clear to me what the points the paper intends to express . Specifically , the authors claim that \u201c we can reduce the number of parameters to represent a binary-kernel by changing the small magnitude weights \u2019 signs \u201d . Then the question comes . How can the number of parameters be reduced via changing signs ? Also , the sentence \u201c we can compress their parameters to an extremely small number by replacing the whole 3\u00d73 binary-kernel with fewer \u201d is incomplete . 2 : Extensive symbols are undefined , which makes me hard to understand the paper properly , especially in Algorithm 1 , the main algorithm of the paper . For example , definition of $ n $ in calculating $ E $ ; definition of \u201c where ( ) \u201d ; definition of $ K_m $ , are not explained . 3 : Extensive technical details are missing , which makes the algorithm difficult to understand . 1 ) : The authors should at least explain how to generate the power-of-two number of binary kernels in details in Sec.3.1.2 ) : During training , the paper generates $ k $ -bit ( i.e. , k is the bitwidth ) number of binary kernels . However , during testing , the learnt binary weights should be fixed ( $ k=1 $ ) and why the \u201c Quant Bit \u201d in Table 1 can be larger than 1 ? This makes me very confusing . 4 : This paper only binarizes weights while I am wondering whether simultaneously binarizing both weights and activations can have the similar observations . 5 : There is no comparisons with current state-of-the-arts . Also , in Sec 4.1 , the paper claims that \u201c the compressed ratio can achieve to 1.5 $ \\times $ with a bearable accuracy drop ( less than 3 % on Cifar-10 ) \u201d does not stand . The 3 % loss on Cifar-10 is significant . 6 : There is no conclusions and future works discussions . 7 : It is widely known that the scaling factors can be absorbed into BN layers . In terms of this , it should not be a contribution of this paper . 8 : The paper introduces extensive heuristic hyper-parameters . Specifically , it does not give any strategy to automatically determine the optimal quantized bitwidth and threshold $ \\Delta $ for each layer . 9 : There are lots of grammar and typo issues . For example , the words in equations should use straight format ( use \\rm in latex ) .", "rating": "3: Clear rejection", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 : Extensive descriptions are very confusing . I can only list some of them . * * Questions * * : 1 ) : The contributions of the paper include the observation that binary kernels in the learned convolutional layers tend to be centered on a limited number of fixed structural patterns . However , it is still not clear to me what the \u201c fixed structural patterns \u201d are . * * Answers * * : The fixed structural patterns are those most frequent patterns , more specifically , those 3x3 binary kernels that appear more frequently than other types of binary kernels in BWN . One example is that the binary kernels with all +1 or all -1 appear much more frequently than other kernels as we have shown in Figure 4 . To make our description clearer , we have changed the term \u2018 fixed structural patterns \u2019 to \u2018 the most frequent binary kernels \u2019 . * * Questions * * : 2 ) : In Sec.3 , the paper claims to \u201c quantize the less frequent kernels to those high frequent kernels to save space \u201d and \u201c we sort these binary kernels according to their appearance frequency\u2026 \u201d . However , the paper fails to explain the motivation for exploring the frequency of kernels . Some theoretical explanations are needed . * * Answers * * : As shown in Figure 4 about the frequency of total 512 types of binary kernels , they are highly unevenly distributed . To compress them which means using fewer types of binary kernels , we need to find cluster centers of these binary kernels . Thus , we use a straightforward method , directly regarding those binary kernels appearing most frequently as the cluster centers . * * Questions * * : 3 ) : In Figure 4 \u2019 s caption , what does \u201c the certain appears in one certain Conv layer \u201d mean ? * * Answers * * : We correct this caption to \u201c one binary kernel appears in one certain Conv layer. \u201d * * Questions * * : 4 ) : In Sec.4.1 , the authors propose to apply QBN on model compression . However , it is not clear to me what the points the paper intends to express . Specifically , the authors claim that \u201c we can reduce the number of parameters to represent a binary-kernel by changing the small magnitude weights \u2019 signs \u201d . Then the question comes . How can the number of parameters be reduced via changing signs ? * * Answers * * : ( 1 ) We illustrate how we did model compression beyond BWN in Figure 5 . By choosing four binary kernels , we assign all 512 types of binary kernels into one of these four binary kernels . Thus one original 3x3 binary kernel which requires a 9-bit binary code to represent only needs a 2-bit binary code instead since we can use such 2-bit binary code to indicate any binary kernels after applying QBN in BWN . The right figures in Figure 5 shows the binary kernel frequency distribution \u2019 s changes before and after using QBN . ( 2 ) Our findings in Section 2 that weights with smaller norm bring less influence on BWN compared to those weights with larger norm guarantee that we can cluster those binary kernels by changing those small weights \u2019 signs . * * Questions * * : Also , the sentence \u201c we can compress their parameters to an extremely small number by replacing the whole 3\u00d73 binary-kernel with fewer \u201d is incomplete . * * Answers * * : We have rewritten this sentence to \u201c we can compress their parameters to an extremely small number by replacing the whole 512 types of 3\u00d73 binary-kernel with fewer types of binary kernels from those 2^k selected binary-kernels \u201d to make it more clear . 2 : Extensive symbols are undefined , which makes me hard to understand the paper properly , especially in Algorithm 1 , the main algorithm of the paper . * * Questions * * : For example , definition of in calculating ; definition of \u201c where ( ) \u201d ; definition of , are not explained . * * Answers * * : In the algorithm we use the python-style pseudocodes to explain our method . We have replaced the definition of \u201c where ( ) \u201d by \u201c if-then \u201d . 3 : Extensive technical details are missing , which makes the algorithm difficult to understand . 1 ) : * * Questions * * : The authors should at least explain how to generate the power-of-two number of binary kernels in details in Sec.3.1 . * * Answers * * : We have answered this question in 1 . 4 ) , and in Sec.3.1 . we have explained that these power-of-two number of kernels are selected from one single VGG-7 \u2019 last Conv layer \u2019 s top 2^1 , 2^2 , \u2026 , 2^8 frequent binary kernels . 2 ) : * * Questions * * : During training , the paper generates k-bit ( i.e. , k is the bitwidth ) number of binary kernels . However , during testing , the learnt binary weights should be fixed ( k=1 ) and why the \u201c Quant Bit \u201d in Table 1 can be larger than 1 ? This makes me very confusing . * * Answers * * : In Figure 4 , we display how to represent a 3x3 binary kernel which is represented by a 9-bit binary code . And in Figure 5 , we show how a binary kernel can be assigned to fewer binary kernel which only needs a binary code with k-bit ( k < 9 ) to represent . We use k-bit to represent how many quant bits we use in each layer/block ."}, {"review_id": "B9nDuDeanHK-3", "review_text": "Summary : The authors show empirically that weight signs are more important than weight magnitudes . They also analyze the optimization process and the structure of optimized binary networks to note that i ) there are clusters of weights whose weight remain almost unchanged during the optimization ii ) optimized convolutional networks have simple sub-structures They exploit the latter to propose a new quantization method that increases the compression of binary networks . strengths : The idea of studying how binary weights update during the optimization is interesting and may help understand the optimization process of binary but also real-valued networks . The experiments include numerical results for a wide range of data sets and networks . weaknesses : The meaning of the given simple proof is not clear . What are gamma and epsilon in equations 1 and 2 ? The paper would have much more impact if , after showing that BN can absorb the redundant factors , all real-valued parameters were dropped . In some sense , the proof seems to show that the obtained result about the little influence of scaling is somehow expected . Regrading the sign flipping , observing that `` flipping weights with large full precision magnitude will cause a significant performance drop compared to those weights close to zero '' seems also something that one can expect . Finally , the presence of clusters is not really explained and shown in a very accessible way . Are there other results except from Figure 4 ( whose caption sounds a bit hermetic : `` The X-axis indicates the index of a 3 x 3 binary weight kernel while Y-axis indicates the frequency that the certain appears in one certain Conv layer '' ) questions : - what happens if all scaling factors are kept fixed ? - is alpha usually shared by all weights in the network or is layer-specific ? - the weights distributions in figure 1 are obtained by adding a regularization term ? - what happens if also the first full-precision Conv layer is quantized ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 . * * Questions * * : The meaning of the given simple proof is not clear . What are gamma and epsilon in equations 1 and 2 ? The paper would have much more impact if , after showing that BN can absorb the redundant factors , all real-valued parameters were dropped . In some sense , the proof seems to show that the obtained result about the little influence of scaling is somehow expected . * * Answers * * : ( 1 ) In equations 1 and 2 , we use the same notations of all parameters of original BN . Gamma and Beta are the affine parameters of BN while epsilon is a parameter for avoiding dividing zero when doing standardization ( PyTorch uses 5e-4 manually ) . \\bar { x } and \\sigma are the estimated moving average of mean and variance which are calculated during training and keep fixed after finishing training . We do not need to retrain one epoch to absorb the scaling factors or other operation . ( 2 ) In additional , we fixed scaling factors \\alpha = 1 rather than use the sum ( |w| ) /n which is used in XNor-BWN in Table 2 . We achieve a similar result using scaling factors \\alpha= 1 and XNor-BWN \u2019 s scaling factors . This may address your concern that all real-valued parameters were dropped . 2 . * * Questions * * : Regrading the sign flipping , observing that `` flipping weights with large full precision magnitude will cause a significant performance drop compared to those weights close to zero '' seems also something that one can expect . * * Answers * * : To best of our knowledge , this is the first work studying the influence of full precision weight magnitude on their corresponding binary weight \u2019 s sign providing with extensive experiments to prove the correctness of this observation . 3 . * * Questions * * : Finally , the presence of clusters is not really explained and shown in a very accessible way . Are there other results except from Figure 4 ( whose caption sounds a bit hermetic : `` The X-axis indicates the index of a 3 x 3 binary weight kernel while Y-axis indicates the frequency that the certain appears in one certain Conv layer '' ) * * Answers * * : ( 1 ) In Figure 4 , in order to display the frequency of all 512 types of binary kernels , we first reshape 3x3 kernels to encode each of them into a 9-bit binary code , then convert them into a integer between 0 and 511 . This helps us to visualize the frequency distribution . ( 2 ) We correct the caption to \u201c the binary kernel appears in one certain Conv layer. \u201d For questions : 1 . * * Questions * * : what happens if all scaling factors are kept fixed ? * * Answers * * : In our experiments , we use scaling factors \\alpha = 0.05 for all binarized layers and networks . We also give the results that when scaling factors \\alpha = 1 , making learning rate 10 times larger brings the similar performance in the Table 2 . 2 . * * Questions * * : is alpha usually shared by all weights in the network or is layer-specific ? * * Answers * * : In our experiments , the scaling factor alpha is the same for all networks and all layers . But for other works , cases are different . XNor-BWN use layer-specific alpha , and LQ-BWN use a learnable channel-specific alpha . 3 . * * Questions * * : the weights distributions in figure 1 are obtained by adding a regularization term ? * * Answers * * : The weights distributions are extracted from BWNs which use weight decay as the only regularization term . 4 . * * Questions * * : what happens if also the first full-precision Conv layer is quantized ? * * Answers * * : It is a common practice in BWN that we leave the first layer and the last layer as full precision weights . Here we also apply our QBN that binarizes the first Conv layer . The result is shown in Table 8 in the Appendix , please see \u201c Summary of Changes in the New Version \u201d ."}], "0": {"review_id": "B9nDuDeanHK-0", "review_text": "Overview : The Authors show that scaling factors with hand-crafted or learnable methods are not so important when training Binary Weight Networks ( BWNs ) , while the change of weight signs is crucial . They make two observations : The weight signs of the primary binary sub-networks are determined and fixed at the early training stage . Binary kernels in the convolutional layers of final models tend to be centered on a limited number of fixed structural patterns . Based on these observations , they propose a new method called binary kernel quantization to further compress BWNs . Strength bullets : 1 . They propose a binary kernel quantization to quantize the binary kernel , which effectively reduces the number of all possible kernels , and can further compress BWNs to 2-5 times . 2.The Authors observe that the weight with the large norm , fixing their signs in the early training stage . And they compared this phenomenon with the lottery ticket hypothesis and propose the primary binary sub-networks . I think it 's an interesting idea . 3.The paper is well written and well-motivated . It contains extensive and systematic related work-study . I like it . Weakness bullets : 1 . The binary kernel quantization method is only applied to 3x3 kernels in this paper . When the size of the convolution kernel is 5x5 or 7x7\uff0chow does it work ? I hope this method can extend to another cases and it also can work well . 2.As shown in table1 , why authors report the best accuracy among different seed ? I think authors should report the averaged results with different random seeds instead of the best result . -- Post Rebuttal -- I have read all feedback and especially thanks to the authors ' efforts on the extra experiments . I think the author has addressed my first concern . For the second one , I would prefer to see the errorbar . I tend to keep my scores unchanged . I think the findings are interesting [ share similar thoughts as R4 's ] , while the experiments part need to be improved . Although I like the ideas and observations , I do n't feel especially strongly in favor of it and can not champion it .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 . * * Questions * * : \u201c The binary kernel quantization method is only applied to 3x3 kernels in this paper . When the size of the convolution kernel is 5x5 or 7x7 , how does it work ? I hope this method can extend to another cases and it also can work well. \u201d * * Answers * * : * * ( 1 ) * * On the one side , in the neural network quantization field , it is a common benchmarking protocol , retaining the first convolutional layer of a CNN model to still have full-precision values . On the other side , to the prevailing CNNs such as VGG , ResNets and MobileNets , large convolutional kernels like 5x5 or 7x7 are merely adopted in their first convolutional layer while the other convolutional layers all use 3x3 ( 2^9 binary patterns in total ) or 1x1 ( 2 binary patterns ) convolutional kernels . For a fair experimental study and comparison , in the previous version of our submission , we followed this protocol when applying the proposed method ( i.e. , Quantized Binary-kernel Networks ) to compress binary weight networks . This is why our method is only applied to 3x3 kernels in the experiments and analysis ( it was stated in Section 2.6 ) ; * * ( 2 ) * * Following your suggestion , we further applied our method to the first layer ( with 7x7 convolutional kernel ) of ResNet-20 as well as the other layers . Comparatively , for 7x7 convolutional kernel the number of possible binary patterns is 2^49 which is significantly larger than 2^9 for 3x3 convolutional kernel , and we observed most binary patterns only appear once or do not appear , leaving the flexibility to determine primary patterns . As a result , good compression results are also obtained and displayed as the followings : Network Quant Bit Acc ResNet-20 ( 64C5 ) 25-9-9-9 83.1 % ResNet-20 ( 64C5 ) 7-7-7-7 78.2 % ResNet-20 ( 64C5 ) 7-6-6-6 77.5 % ResNet-20 ( 64C7 ) 49-9-9-9 83.8 % ResNet-20 ( 64C7 ) 7-7-7-7 77.6 % ResNet-20 ( 64C7 ) 7-6-6-6 75.9 % 2 . * * Questions * * : \u201c As shown in table1 , why authors report the best accuracy among different seed ? I think authors should report the averaged results with different random seeds instead of the best result. \u201d * * Answers * * : It is a typo that we report the mean of 5 random runs \u2019 best accuracy during training . We have corrected this description to \u2018 the mean of the best accuracy during training among 5 runs with different random seeds \u2019 ."}, "1": {"review_id": "B9nDuDeanHK-1", "review_text": "This paper provides an empirical study of binary weight networks ( BWNs ) , where they find that 1 the commonly adopted scaling factor is not critical 2 there exists a subnetwork that stabiles early in training 3 the 3x3 filters in VGG and ResNets demonstrate a sparse distribution . They combine all the observations and propose a novel quantization algorithm that achieves more aggressive compression than standard BWNs . pros : + I appreciate the careful examination of design and training details of standard BWNs . The identification of a persistent subnetwork and the analysis on the sparse distribution of kernels are particularly interesting . + The proposed quantization algorithm is interesting , which has a potential of squeezing more redundancy out of standard BWNs cons : - If I understand correctly , in the proposed algorithm the kernel distribution is only drawn from the last conv layer of the full precision network , which is then shared across all layers when retraining the BWN . This seems a strong assumption and needs to be justified . What 's the reason to believe that the selected frequent kernels are shared across different layers ? -In Algorithm 1 , W = where ( abs ( W ) > \u2206E , sign ( W ) , W ) is not motivated and explained well . What 's the reasoning of using the threshold when computing the distance to the frequency binary kernels ? -The experimental results seem to be really hard to interpret for me , and this is perhaps the weakest point of the this paper . In particular , Table 1 needs to have proper baselines . This includes the full precision , standard BWN accuracies , as well as controls which allow one to draw comparisons between the proposed algorithm and basic binarization by equating certain quantities . I suggest the authors work on the suggested improvements which will make this a much stronger contribution . * * * * * post rebuttal updates * * * * * I want to thank the authors for responding to my questions . The additional explanations are indeed helpful for clarifying my first two questions ( selection of the binary kernel and the use of \u2206E ) . However , I still have concerns about Table 1 ( and Table 2 ) . For example , I have a really hard time interpreting the significance of achieving a 3.2x CR with a loss of 3 % ( 92.3 - 89.2 from VGG-7 ) in acc with the proposed method ( although the paper argues that it 's a `` bearable '' loss ) . Considering that this is the main experiment supporting the efficacy of the proposed quantization algorithm , I think the paper needs more controlled experiments to demonstrate the practical usefulness of the proposed algorithm . As a result I 'm keeping my original score and hope the authors can work on the improvements for the next version .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 . * * Questions * * : If I understand correctly , in the proposed algorithm the kernel distribution is only drawn from the last conv layer of the full precision network , which is then shared across all layers when retraining the BWN . This seems a strong assumption and needs to be justified . What 's the reason to believe that the selected frequent kernels are shared across different layers ? \u2019 * * Answers * * : Though it contains a strong assumption that these selected binary kernels can generalize across different networks , different layers , and different datasets , we give two reasons for the justification : ( 1 ) We did an experiment to discover to what extend do the binary kernel distributions between VGG-7 BWN \u2019 s last layer \u2019 s and other different networks \u2019 different layers \u2019 in our Appendix L. In Figure 14 , we show that the selected binary kernels from one single-trail VGG-7 BWN \u2019 s last layer are highly corelated to those most frequent binary kernels in ResNet-18 \u2019 s different layers . For example , for the same k-bit kernels , the top 2^k frequent binary kernels contain a % of all binary kernels in one layer , and the selected 2^k binary kernels from VGG-7 BWN \u2019 s last layer contain b % ( b < =a because a % is already the highest percentage that 2^k binary kernels can achieve ) . So , we use b/a < = 1 to represent such correlation . b/a = 1 indicates that the top 2^k frequent binary kernels are identical to our selected 2^k binary kernels . In the figures , all are higher than 80 % , and for the deeper layers ( the last two blocks in ResNet-18 ) they are higher than 90 % . This indicates that those most frequent kernels are similar across different networks and different layers and different datasets . ( 2 ) Besides the correlation experiments , we also choose the selected kernels from the 2nd layer of VGG-7 , from 4th layer of VGG-7 , from the last conv layer of ResNet-18 , from the 9th conv layer of ResNet-18 ( the last conv of the second block of ResNet-18 ) , and merging the statistic information of these two layers of ResNet-18 . Then we apply these new selected binary kernels to VGG-7 on Cifar-10 as shown in Table.7 . 2 . * * Questions * * : In Algorithm 1 , W = where ( abs ( W ) > \u2206E , sign ( W ) , W ) is not motivated and explained well . What 's the reasoning of using the threshold when computing the distance to the frequency binary kernels ? * * Answers * * : In Section 4.2 , with the experiments we discuss about \u2018 Connection between Primary Binary Sub-Networks \u2019 . For this \u2206E , we give the explanation that this step is to make those large weights more important when calculating the L2 by assigning them with a larger number +-1 instead of their original norm ( the norm of weights are much smaller compared to 1 ) . In the paper we did further experiments on such threshold in Appendix O with Figure 16 , which indicates that \u2206=0 is worse in both VGG-7 and ResNet-56 with the worse performance compared to other cases that \u2206 > 0 . For \u2206 > 0 the optimal value for \u2206 is different case by case . 3 . * * Questions * * : The experimental results seem to be really hard to interpret for me , and this is perhaps the weakest point of the this paper . In particular , Table 1 needs to have proper baselines . This includes the full precision , standard BWN accuracies , as well as controls which allow one to draw comparisons between the proposed algorithm and basic binarization by equating certain quantities . * * Answers * * : Due to the page limit , in the paper we put the full-precision network and BWN baselines in Table 2 in Appendix . Follow your suggestion , we refer the results of baseline of full-precision network and BWN in Table 1 caption to make it easier to follow ."}, "2": {"review_id": "B9nDuDeanHK-2", "review_text": "* * Summary : * * This paper proposes some interesting observations for training BWNs . 1 : The scaling factors can be removed with batch normalization used . 2 : The signs of the weights with large norms are determined and fixed at the early training stage . 3 : The binary weight networks can be further compressed . Moreover , the authors provide some empirical visualizations and results to demonstrate its analysis . However , the paper seems to be incomplete and needs to be further improved . * * Pros : * * The observation that the signs of weights with large norms are determined and fixed at the early training stage is interesting . The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWNs , maybe BNNs as well . * * Cons : * * 1 : Extensive descriptions are very confusing . I can only list some of them . 1 ) : The contributions of the paper include the observation that binary kernels in the learned convolutional layers tend to be centered on a limited number of fixed structural patterns . However , it is still not clear to me what the \u201c fixed structural patterns \u201d are . 2 ) : In Sec.3 , the paper claims to \u201c quantize the less frequent kernels to those high frequent kernels to save space \u201d and \u201c we sort these binary kernels according to their appearance frequency\u2026 \u201d . However , the paper fails to explain the motivation for exploring the frequency of kernels . Some theoretical explanations are needed . 3 ) : In Figure 4 \u2019 s caption , what does \u201c the certain appears in one certain Conv layer \u201d mean ? 4 ) : In Sec.4.1 , the authors propose to apply QBN on model compression . However , it is not clear to me what the points the paper intends to express . Specifically , the authors claim that \u201c we can reduce the number of parameters to represent a binary-kernel by changing the small magnitude weights \u2019 signs \u201d . Then the question comes . How can the number of parameters be reduced via changing signs ? Also , the sentence \u201c we can compress their parameters to an extremely small number by replacing the whole 3\u00d73 binary-kernel with fewer \u201d is incomplete . 2 : Extensive symbols are undefined , which makes me hard to understand the paper properly , especially in Algorithm 1 , the main algorithm of the paper . For example , definition of $ n $ in calculating $ E $ ; definition of \u201c where ( ) \u201d ; definition of $ K_m $ , are not explained . 3 : Extensive technical details are missing , which makes the algorithm difficult to understand . 1 ) : The authors should at least explain how to generate the power-of-two number of binary kernels in details in Sec.3.1.2 ) : During training , the paper generates $ k $ -bit ( i.e. , k is the bitwidth ) number of binary kernels . However , during testing , the learnt binary weights should be fixed ( $ k=1 $ ) and why the \u201c Quant Bit \u201d in Table 1 can be larger than 1 ? This makes me very confusing . 4 : This paper only binarizes weights while I am wondering whether simultaneously binarizing both weights and activations can have the similar observations . 5 : There is no comparisons with current state-of-the-arts . Also , in Sec 4.1 , the paper claims that \u201c the compressed ratio can achieve to 1.5 $ \\times $ with a bearable accuracy drop ( less than 3 % on Cifar-10 ) \u201d does not stand . The 3 % loss on Cifar-10 is significant . 6 : There is no conclusions and future works discussions . 7 : It is widely known that the scaling factors can be absorbed into BN layers . In terms of this , it should not be a contribution of this paper . 8 : The paper introduces extensive heuristic hyper-parameters . Specifically , it does not give any strategy to automatically determine the optimal quantized bitwidth and threshold $ \\Delta $ for each layer . 9 : There are lots of grammar and typo issues . For example , the words in equations should use straight format ( use \\rm in latex ) .", "rating": "3: Clear rejection", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 : Extensive descriptions are very confusing . I can only list some of them . * * Questions * * : 1 ) : The contributions of the paper include the observation that binary kernels in the learned convolutional layers tend to be centered on a limited number of fixed structural patterns . However , it is still not clear to me what the \u201c fixed structural patterns \u201d are . * * Answers * * : The fixed structural patterns are those most frequent patterns , more specifically , those 3x3 binary kernels that appear more frequently than other types of binary kernels in BWN . One example is that the binary kernels with all +1 or all -1 appear much more frequently than other kernels as we have shown in Figure 4 . To make our description clearer , we have changed the term \u2018 fixed structural patterns \u2019 to \u2018 the most frequent binary kernels \u2019 . * * Questions * * : 2 ) : In Sec.3 , the paper claims to \u201c quantize the less frequent kernels to those high frequent kernels to save space \u201d and \u201c we sort these binary kernels according to their appearance frequency\u2026 \u201d . However , the paper fails to explain the motivation for exploring the frequency of kernels . Some theoretical explanations are needed . * * Answers * * : As shown in Figure 4 about the frequency of total 512 types of binary kernels , they are highly unevenly distributed . To compress them which means using fewer types of binary kernels , we need to find cluster centers of these binary kernels . Thus , we use a straightforward method , directly regarding those binary kernels appearing most frequently as the cluster centers . * * Questions * * : 3 ) : In Figure 4 \u2019 s caption , what does \u201c the certain appears in one certain Conv layer \u201d mean ? * * Answers * * : We correct this caption to \u201c one binary kernel appears in one certain Conv layer. \u201d * * Questions * * : 4 ) : In Sec.4.1 , the authors propose to apply QBN on model compression . However , it is not clear to me what the points the paper intends to express . Specifically , the authors claim that \u201c we can reduce the number of parameters to represent a binary-kernel by changing the small magnitude weights \u2019 signs \u201d . Then the question comes . How can the number of parameters be reduced via changing signs ? * * Answers * * : ( 1 ) We illustrate how we did model compression beyond BWN in Figure 5 . By choosing four binary kernels , we assign all 512 types of binary kernels into one of these four binary kernels . Thus one original 3x3 binary kernel which requires a 9-bit binary code to represent only needs a 2-bit binary code instead since we can use such 2-bit binary code to indicate any binary kernels after applying QBN in BWN . The right figures in Figure 5 shows the binary kernel frequency distribution \u2019 s changes before and after using QBN . ( 2 ) Our findings in Section 2 that weights with smaller norm bring less influence on BWN compared to those weights with larger norm guarantee that we can cluster those binary kernels by changing those small weights \u2019 signs . * * Questions * * : Also , the sentence \u201c we can compress their parameters to an extremely small number by replacing the whole 3\u00d73 binary-kernel with fewer \u201d is incomplete . * * Answers * * : We have rewritten this sentence to \u201c we can compress their parameters to an extremely small number by replacing the whole 512 types of 3\u00d73 binary-kernel with fewer types of binary kernels from those 2^k selected binary-kernels \u201d to make it more clear . 2 : Extensive symbols are undefined , which makes me hard to understand the paper properly , especially in Algorithm 1 , the main algorithm of the paper . * * Questions * * : For example , definition of in calculating ; definition of \u201c where ( ) \u201d ; definition of , are not explained . * * Answers * * : In the algorithm we use the python-style pseudocodes to explain our method . We have replaced the definition of \u201c where ( ) \u201d by \u201c if-then \u201d . 3 : Extensive technical details are missing , which makes the algorithm difficult to understand . 1 ) : * * Questions * * : The authors should at least explain how to generate the power-of-two number of binary kernels in details in Sec.3.1 . * * Answers * * : We have answered this question in 1 . 4 ) , and in Sec.3.1 . we have explained that these power-of-two number of kernels are selected from one single VGG-7 \u2019 last Conv layer \u2019 s top 2^1 , 2^2 , \u2026 , 2^8 frequent binary kernels . 2 ) : * * Questions * * : During training , the paper generates k-bit ( i.e. , k is the bitwidth ) number of binary kernels . However , during testing , the learnt binary weights should be fixed ( k=1 ) and why the \u201c Quant Bit \u201d in Table 1 can be larger than 1 ? This makes me very confusing . * * Answers * * : In Figure 4 , we display how to represent a 3x3 binary kernel which is represented by a 9-bit binary code . And in Figure 5 , we show how a binary kernel can be assigned to fewer binary kernel which only needs a binary code with k-bit ( k < 9 ) to represent . We use k-bit to represent how many quant bits we use in each layer/block ."}, "3": {"review_id": "B9nDuDeanHK-3", "review_text": "Summary : The authors show empirically that weight signs are more important than weight magnitudes . They also analyze the optimization process and the structure of optimized binary networks to note that i ) there are clusters of weights whose weight remain almost unchanged during the optimization ii ) optimized convolutional networks have simple sub-structures They exploit the latter to propose a new quantization method that increases the compression of binary networks . strengths : The idea of studying how binary weights update during the optimization is interesting and may help understand the optimization process of binary but also real-valued networks . The experiments include numerical results for a wide range of data sets and networks . weaknesses : The meaning of the given simple proof is not clear . What are gamma and epsilon in equations 1 and 2 ? The paper would have much more impact if , after showing that BN can absorb the redundant factors , all real-valued parameters were dropped . In some sense , the proof seems to show that the obtained result about the little influence of scaling is somehow expected . Regrading the sign flipping , observing that `` flipping weights with large full precision magnitude will cause a significant performance drop compared to those weights close to zero '' seems also something that one can expect . Finally , the presence of clusters is not really explained and shown in a very accessible way . Are there other results except from Figure 4 ( whose caption sounds a bit hermetic : `` The X-axis indicates the index of a 3 x 3 binary weight kernel while Y-axis indicates the frequency that the certain appears in one certain Conv layer '' ) questions : - what happens if all scaling factors are kept fixed ? - is alpha usually shared by all weights in the network or is layer-specific ? - the weights distributions in figure 1 are obtained by adding a regularization term ? - what happens if also the first full-precision Conv layer is quantized ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you so much for the thoughtful review and the recognition of our work . Please see our below responses to your questions . 1 . * * Questions * * : The meaning of the given simple proof is not clear . What are gamma and epsilon in equations 1 and 2 ? The paper would have much more impact if , after showing that BN can absorb the redundant factors , all real-valued parameters were dropped . In some sense , the proof seems to show that the obtained result about the little influence of scaling is somehow expected . * * Answers * * : ( 1 ) In equations 1 and 2 , we use the same notations of all parameters of original BN . Gamma and Beta are the affine parameters of BN while epsilon is a parameter for avoiding dividing zero when doing standardization ( PyTorch uses 5e-4 manually ) . \\bar { x } and \\sigma are the estimated moving average of mean and variance which are calculated during training and keep fixed after finishing training . We do not need to retrain one epoch to absorb the scaling factors or other operation . ( 2 ) In additional , we fixed scaling factors \\alpha = 1 rather than use the sum ( |w| ) /n which is used in XNor-BWN in Table 2 . We achieve a similar result using scaling factors \\alpha= 1 and XNor-BWN \u2019 s scaling factors . This may address your concern that all real-valued parameters were dropped . 2 . * * Questions * * : Regrading the sign flipping , observing that `` flipping weights with large full precision magnitude will cause a significant performance drop compared to those weights close to zero '' seems also something that one can expect . * * Answers * * : To best of our knowledge , this is the first work studying the influence of full precision weight magnitude on their corresponding binary weight \u2019 s sign providing with extensive experiments to prove the correctness of this observation . 3 . * * Questions * * : Finally , the presence of clusters is not really explained and shown in a very accessible way . Are there other results except from Figure 4 ( whose caption sounds a bit hermetic : `` The X-axis indicates the index of a 3 x 3 binary weight kernel while Y-axis indicates the frequency that the certain appears in one certain Conv layer '' ) * * Answers * * : ( 1 ) In Figure 4 , in order to display the frequency of all 512 types of binary kernels , we first reshape 3x3 kernels to encode each of them into a 9-bit binary code , then convert them into a integer between 0 and 511 . This helps us to visualize the frequency distribution . ( 2 ) We correct the caption to \u201c the binary kernel appears in one certain Conv layer. \u201d For questions : 1 . * * Questions * * : what happens if all scaling factors are kept fixed ? * * Answers * * : In our experiments , we use scaling factors \\alpha = 0.05 for all binarized layers and networks . We also give the results that when scaling factors \\alpha = 1 , making learning rate 10 times larger brings the similar performance in the Table 2 . 2 . * * Questions * * : is alpha usually shared by all weights in the network or is layer-specific ? * * Answers * * : In our experiments , the scaling factor alpha is the same for all networks and all layers . But for other works , cases are different . XNor-BWN use layer-specific alpha , and LQ-BWN use a learnable channel-specific alpha . 3 . * * Questions * * : the weights distributions in figure 1 are obtained by adding a regularization term ? * * Answers * * : The weights distributions are extracted from BWNs which use weight decay as the only regularization term . 4 . * * Questions * * : what happens if also the first full-precision Conv layer is quantized ? * * Answers * * : It is a common practice in BWN that we leave the first layer and the last layer as full precision weights . Here we also apply our QBN that binarizes the first Conv layer . The result is shown in Table 8 in the Appendix , please see \u201c Summary of Changes in the New Version \u201d ."}}