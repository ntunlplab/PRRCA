{"year": "2020", "forum": "HylxE1HKwS", "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment", "decision": "Accept (Poster)", "meta_review": "The authors propose a new method for neural architecture search, except it's not exactly that because model training is separated from architecture, which is the main point of the paper. Once this network is trained, sub-networks can be distilled from it and used for specific tasks.\n\nThe paper as submitted missed certain details, but after this was pointed out by reviewers the details were satisfactorily described by the authors. \n\nThe idea of the paper is original and interesting. The paper is correct and, after the revisions by authors, complete. In my view, this is sufficient for acceptance.", "reviews": [{"review_id": "HylxE1HKwS-0", "review_text": "In this papers, the authors learn a Once-for-all net. This starts as a big neural network which is trained normally (albeit with input images of different resolutions). It is then fine-tuned while sampling sub-networks with progressively smaller kernels, then lower depth, then width (while still sampling larger networks occasionally, as it reads). This results in a network from which one can extract sub-networks for various resource constraints (latency, memory etc.) that perform well without a need for retraining. This paper is well written, and the results are very good. However there are serious problems that need addressing. The method as described *is not reproducible*. The scheduling of sampling subnetworks is alluded to on page 4, and that's it. It is essential that the authors include their exact subnet sampling schedule e.g. as pseudocode with hyperparameters. There is no point doing good work if other researchers cannot build off it. On another reproducibility note, as far as I can tell, the original model isn't given. There would be no harm in adding this to the appendix. Figure 1 is misleading, as we don't find out until later in the paper that Once For All #25 means that each of these points was finetuned for a further 25 epochs (which on ImageNet is non-trivial). This defeats the narrative of the paper (once-for-all plus some fine-tuning isn't exactly once-for-all). Is there a reason why the progressive shrinking goes resolution->kernel->depth->width? Was this just the permutation that worked best? I would be curious as to why this is. For elastic width, I wasn't sure why the \"channel sorting operation preserves the accuracy of larger sub-networks\". Could you please elaborate? Kudos on adding CO2 emissions in Table 2, I hope this gets reported more often. In the introduction, the authors talk about iPhones and then the hardware considered is Samsung and Google. A minor note, but it seems inconsistent. Another minor note, in Table 2, (Strubell et al) should be out of the brackets, as it is part of the sentence. Given that there are 10^19 subnetworks that can be sampled, it would be nice to see more than 3-4 appear on a plot. This makes it seem like they might have been cherry-picked. Sampling a few 100/1000 subnets and producing some Pareto curves would be both interesting and insightful. Pros ------- - Good results - Well written - Neat idea Cons ------- - Training details are obfuscated. This paper should not be accepted without them. - Very few subnetworks of the vast quantity that exist are observed. In conclusion, I am giving this paper a weak reject, as it is currently impossible to reproduce, and as such, is of no use to the community. If the authors remedy this I will gladly raise my score.", "rating": "6: Weak Accept", "reply_text": "Thanks very much for your constructive and detailed comments . We will fix the typos and remove \u201c Once for All # 25 \u201d from figure 1 . 1.Training details and code release . For reproduction , we will add a detailed and clear description of our training details by Nov. 15 . We are also working on cleaning the code . The training code and pre-trained models will be posted anonymously in the OpenReview by Nov. 22 . 2.Sample more sub-networks and produce some Pareto curves . That \u2019 s a great idea . Thanks for the suggestion . We will update our figures showing the entire trade-off curves rather than a few points by Nov. 15 . 3.Adding the original model in the appendix . Thank you for the suggestion . We will add a figure showing the detailed architecture of the full ( original ) model in the appendix . 4.Why the progressive shrinking goes resolution- > kernel- > depth- > width . The order is determined based on the difficulty of each task . Intuitively , we hope the model to complete easy tasks first and then handle more difficult tasks , similar to the idea of curriculum learning . 5.Why the channel sorting operation preserves the accuracy of larger sub-networks . When performing the channel sorting operation on a specific layer , we first sort the input dimension of the layer according to their importance ( i.e. , L1 norm ) . Then the output dimension of the previous layer is reorganized accordingly to make sure the functionality of large sub-networks does not change . We have also summarized all of our planned updates in our general response above . If there are any additional comments on the paper or on the planned updates , please don \u2019 t hesitate to let us know ."}, {"review_id": "HylxE1HKwS-1", "review_text": "This paper tries to tackle the problem of searching best architectures for specialized resource constraint deployment scenarios. The authors basically take a two-step approach: First train a large network including all the small networks with weight sharing and some specially designed trick (e.g., progressive shrinking). Second, use prediction based NAS method to learn the performance/inference prediction module, from which the good sub architecture corresponding to a particular scenario is obtained. The experiments show that the proposed method is promising. Pros: 1. It is an interesting new paradigm that tries to solve AutoML for different deployment scenarios \u201conce for all\u201d. AFAIK there is no prior works thinking in this way. 2. It is useful and encouraging to see the proposed method achieves satisfactory performances, on par with the current best method specially designed for different deployment environment, while the computational cost is reduced by a large margin. 3. Paper is clearly written and easy to understand. Cons: 1. The motivation towards \u201cprogressive shrinking (PS)\u201d is not that clear. It seems natural to train a large network, and from it to train sub structures, since overparameterization helps NN training. However, it is hard to imagine that training from large to small could eliminate the \u201cinterfering\u201d of subnetworks, let alone \u201cwhile maintaining the same accuracy as independently trained networks\u201d. To me it is neither theoretically nor empirically supported (Please note the training of subnetworks definitely affect the learnt weights of the big one through weight sharing). In particular, the subnetworks with weight sharing could achieve the same, or even better performances compared with those non shared counterparts, which seems too good to be true. 1. A possible explanation might be that the overparameterization brings additional gain in the optimization process of each small network, especially with the help of knowledge distillation. If that is true, an additional ablation study should be done to separate the benefits of PS, and the disadvantage of weight sharing (i.e., interfering). 2. I see no statements about code release. If a clear, and TIMELY code (for the SEARCH phase, not only for the Eval phase) release could be done, then at least from the perspective of application, the impact of this paper could be further enhanced. ", "rating": "6: Weak Accept", "reply_text": "Thanks very much for your constructive comments . 1.Why training from large to small can prevent interference between sub-networks . Training large sub-networks can also benefit small sub-networks to learn useful features . For example , after finishing the step of elastic kernel size , the sub-network ( D=3 , W=6 , K=7 , R=224 ) can already achieve 69.1 % top-1 accuracy on ImageNet without any fine-tuning . This is consistent with previous observations in network pruning [ 1,2,3 ] . By training from large to small , both large sub-networks and small sub-networks can reuse previously learned knowledge ( or features ) . Empirically , we find that it is helpful for the optimization of the shared weights with the goal of supporting large sub-networks and small sub-networks at the same time . 2.Why subnetworks with weight sharing could achieve the same , or even better performances compared with those non shared counterparts . We first want to clarify that we are not targeting at improving the accuracy of a specific sub-network for a * * single * * scenario ; instead , we want to improve the accuracy-efficiency trade-off on * * many * * hardware platforms while reducing the total training cost . To avoid confusion about the goal of this paper , we will emphasize our main contribution and make it more clear in the revision . We conjecture the reason for this result is that smaller sub-networks can benefit from getting the knowledge transferred from well-trained large sub-networks through inheriting weights from large sub-networks and knowledge distillation . Regarding separating the benefits of PS and the disadvantage of weight sharing ( i.e. , interfering ) , we want to clarify that weight sharing is an essential component of the OFA framework since it is prohibitive to download and store so many networks independently on resource-constrained edge devices . 3.Code release . Thank you for the suggestion . We definitely hope this work can be a useful tool for application purposes . We are currently cleaning the code . The training code and pre-trained models will be released anonymously in the OpenReview by Nov. 22 . We have also summarized all of our planned updates in our general response above . If there are any additional comments on the paper or on the planned updates , please don \u2019 t hesitate to let us know . [ 1 ] Han , Song , et al . `` Deep compression : Compressing deep neural networks with pruning , trained quantization and huffman coding . '' in ICLR 2016 . [ 2 ] Liu , Zhuang , et al . `` Learning efficient convolutional networks through network slimming . '' in ICCV 2017 . [ 3 ] He , Yihui , et al . `` Channel pruning for accelerating very deep neural networks . '' in ICCV 2017 ."}, {"review_id": "HylxE1HKwS-2", "review_text": "In this manuscript, authors propose an OFA NAS framework. They train a supernet first and then finetune the elastic version of the large network. After training, the sub-networks derived from the supernet can be applied for different scenarios directly without retraining. The motivation is clear and interesting. My concerns are as follows. 1. When sampling sub-networks, a prediction model is applied to predict the accuracy of networks. It is interesting to show the accuracy of the prediction model itself and how it will influence the final selection. 2. The results compared in Table 2 are outdated. Authors should at least add the result of MobileNetV3.", "rating": "6: Weak Accept", "reply_text": "Thanks very much for your constructive comments . 1.Performance of the accuracy prediction model and how it influences the final selection . We will add a figure in the appendix by Nov. 15 , showing the relationship between the performance of the accuracy prediction model and the accuracy of selected sub-networks . 2.Comparison to MobileNetV3 in Table 2 . Thanks for the suggestion . We agree that it is essential to compare our model to MobileNetV3 which gives the current SOTA performances on mobile platforms . To have an Apple-to-Apple comparison with it , we will apply our method to the same architecture space as MobileNetV3 . The new results will be included by Nov. 15 . We have also summarized all of our planned updates in our general response above . If there are any additional comments on the paper or on the planned updates , please don \u2019 t hesitate to let us know ."}], "0": {"review_id": "HylxE1HKwS-0", "review_text": "In this papers, the authors learn a Once-for-all net. This starts as a big neural network which is trained normally (albeit with input images of different resolutions). It is then fine-tuned while sampling sub-networks with progressively smaller kernels, then lower depth, then width (while still sampling larger networks occasionally, as it reads). This results in a network from which one can extract sub-networks for various resource constraints (latency, memory etc.) that perform well without a need for retraining. This paper is well written, and the results are very good. However there are serious problems that need addressing. The method as described *is not reproducible*. The scheduling of sampling subnetworks is alluded to on page 4, and that's it. It is essential that the authors include their exact subnet sampling schedule e.g. as pseudocode with hyperparameters. There is no point doing good work if other researchers cannot build off it. On another reproducibility note, as far as I can tell, the original model isn't given. There would be no harm in adding this to the appendix. Figure 1 is misleading, as we don't find out until later in the paper that Once For All #25 means that each of these points was finetuned for a further 25 epochs (which on ImageNet is non-trivial). This defeats the narrative of the paper (once-for-all plus some fine-tuning isn't exactly once-for-all). Is there a reason why the progressive shrinking goes resolution->kernel->depth->width? Was this just the permutation that worked best? I would be curious as to why this is. For elastic width, I wasn't sure why the \"channel sorting operation preserves the accuracy of larger sub-networks\". Could you please elaborate? Kudos on adding CO2 emissions in Table 2, I hope this gets reported more often. In the introduction, the authors talk about iPhones and then the hardware considered is Samsung and Google. A minor note, but it seems inconsistent. Another minor note, in Table 2, (Strubell et al) should be out of the brackets, as it is part of the sentence. Given that there are 10^19 subnetworks that can be sampled, it would be nice to see more than 3-4 appear on a plot. This makes it seem like they might have been cherry-picked. Sampling a few 100/1000 subnets and producing some Pareto curves would be both interesting and insightful. Pros ------- - Good results - Well written - Neat idea Cons ------- - Training details are obfuscated. This paper should not be accepted without them. - Very few subnetworks of the vast quantity that exist are observed. In conclusion, I am giving this paper a weak reject, as it is currently impossible to reproduce, and as such, is of no use to the community. If the authors remedy this I will gladly raise my score.", "rating": "6: Weak Accept", "reply_text": "Thanks very much for your constructive and detailed comments . We will fix the typos and remove \u201c Once for All # 25 \u201d from figure 1 . 1.Training details and code release . For reproduction , we will add a detailed and clear description of our training details by Nov. 15 . We are also working on cleaning the code . The training code and pre-trained models will be posted anonymously in the OpenReview by Nov. 22 . 2.Sample more sub-networks and produce some Pareto curves . That \u2019 s a great idea . Thanks for the suggestion . We will update our figures showing the entire trade-off curves rather than a few points by Nov. 15 . 3.Adding the original model in the appendix . Thank you for the suggestion . We will add a figure showing the detailed architecture of the full ( original ) model in the appendix . 4.Why the progressive shrinking goes resolution- > kernel- > depth- > width . The order is determined based on the difficulty of each task . Intuitively , we hope the model to complete easy tasks first and then handle more difficult tasks , similar to the idea of curriculum learning . 5.Why the channel sorting operation preserves the accuracy of larger sub-networks . When performing the channel sorting operation on a specific layer , we first sort the input dimension of the layer according to their importance ( i.e. , L1 norm ) . Then the output dimension of the previous layer is reorganized accordingly to make sure the functionality of large sub-networks does not change . We have also summarized all of our planned updates in our general response above . If there are any additional comments on the paper or on the planned updates , please don \u2019 t hesitate to let us know ."}, "1": {"review_id": "HylxE1HKwS-1", "review_text": "This paper tries to tackle the problem of searching best architectures for specialized resource constraint deployment scenarios. The authors basically take a two-step approach: First train a large network including all the small networks with weight sharing and some specially designed trick (e.g., progressive shrinking). Second, use prediction based NAS method to learn the performance/inference prediction module, from which the good sub architecture corresponding to a particular scenario is obtained. The experiments show that the proposed method is promising. Pros: 1. It is an interesting new paradigm that tries to solve AutoML for different deployment scenarios \u201conce for all\u201d. AFAIK there is no prior works thinking in this way. 2. It is useful and encouraging to see the proposed method achieves satisfactory performances, on par with the current best method specially designed for different deployment environment, while the computational cost is reduced by a large margin. 3. Paper is clearly written and easy to understand. Cons: 1. The motivation towards \u201cprogressive shrinking (PS)\u201d is not that clear. It seems natural to train a large network, and from it to train sub structures, since overparameterization helps NN training. However, it is hard to imagine that training from large to small could eliminate the \u201cinterfering\u201d of subnetworks, let alone \u201cwhile maintaining the same accuracy as independently trained networks\u201d. To me it is neither theoretically nor empirically supported (Please note the training of subnetworks definitely affect the learnt weights of the big one through weight sharing). In particular, the subnetworks with weight sharing could achieve the same, or even better performances compared with those non shared counterparts, which seems too good to be true. 1. A possible explanation might be that the overparameterization brings additional gain in the optimization process of each small network, especially with the help of knowledge distillation. If that is true, an additional ablation study should be done to separate the benefits of PS, and the disadvantage of weight sharing (i.e., interfering). 2. I see no statements about code release. If a clear, and TIMELY code (for the SEARCH phase, not only for the Eval phase) release could be done, then at least from the perspective of application, the impact of this paper could be further enhanced. ", "rating": "6: Weak Accept", "reply_text": "Thanks very much for your constructive comments . 1.Why training from large to small can prevent interference between sub-networks . Training large sub-networks can also benefit small sub-networks to learn useful features . For example , after finishing the step of elastic kernel size , the sub-network ( D=3 , W=6 , K=7 , R=224 ) can already achieve 69.1 % top-1 accuracy on ImageNet without any fine-tuning . This is consistent with previous observations in network pruning [ 1,2,3 ] . By training from large to small , both large sub-networks and small sub-networks can reuse previously learned knowledge ( or features ) . Empirically , we find that it is helpful for the optimization of the shared weights with the goal of supporting large sub-networks and small sub-networks at the same time . 2.Why subnetworks with weight sharing could achieve the same , or even better performances compared with those non shared counterparts . We first want to clarify that we are not targeting at improving the accuracy of a specific sub-network for a * * single * * scenario ; instead , we want to improve the accuracy-efficiency trade-off on * * many * * hardware platforms while reducing the total training cost . To avoid confusion about the goal of this paper , we will emphasize our main contribution and make it more clear in the revision . We conjecture the reason for this result is that smaller sub-networks can benefit from getting the knowledge transferred from well-trained large sub-networks through inheriting weights from large sub-networks and knowledge distillation . Regarding separating the benefits of PS and the disadvantage of weight sharing ( i.e. , interfering ) , we want to clarify that weight sharing is an essential component of the OFA framework since it is prohibitive to download and store so many networks independently on resource-constrained edge devices . 3.Code release . Thank you for the suggestion . We definitely hope this work can be a useful tool for application purposes . We are currently cleaning the code . The training code and pre-trained models will be released anonymously in the OpenReview by Nov. 22 . We have also summarized all of our planned updates in our general response above . If there are any additional comments on the paper or on the planned updates , please don \u2019 t hesitate to let us know . [ 1 ] Han , Song , et al . `` Deep compression : Compressing deep neural networks with pruning , trained quantization and huffman coding . '' in ICLR 2016 . [ 2 ] Liu , Zhuang , et al . `` Learning efficient convolutional networks through network slimming . '' in ICCV 2017 . [ 3 ] He , Yihui , et al . `` Channel pruning for accelerating very deep neural networks . '' in ICCV 2017 ."}, "2": {"review_id": "HylxE1HKwS-2", "review_text": "In this manuscript, authors propose an OFA NAS framework. They train a supernet first and then finetune the elastic version of the large network. After training, the sub-networks derived from the supernet can be applied for different scenarios directly without retraining. The motivation is clear and interesting. My concerns are as follows. 1. When sampling sub-networks, a prediction model is applied to predict the accuracy of networks. It is interesting to show the accuracy of the prediction model itself and how it will influence the final selection. 2. The results compared in Table 2 are outdated. Authors should at least add the result of MobileNetV3.", "rating": "6: Weak Accept", "reply_text": "Thanks very much for your constructive comments . 1.Performance of the accuracy prediction model and how it influences the final selection . We will add a figure in the appendix by Nov. 15 , showing the relationship between the performance of the accuracy prediction model and the accuracy of selected sub-networks . 2.Comparison to MobileNetV3 in Table 2 . Thanks for the suggestion . We agree that it is essential to compare our model to MobileNetV3 which gives the current SOTA performances on mobile platforms . To have an Apple-to-Apple comparison with it , we will apply our method to the same architecture space as MobileNetV3 . The new results will be included by Nov. 15 . We have also summarized all of our planned updates in our general response above . If there are any additional comments on the paper or on the planned updates , please don \u2019 t hesitate to let us know ."}}