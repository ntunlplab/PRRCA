{"year": "2019", "forum": "H1lug3R5FX", "title": "On the Geometry of Adversarial Examples", "decision": "Reject", "meta_review": "The paper gives a theoretical analysis highlighting the role of codimension on the pervasiveness of adversarial examples. The paper demonstrates that a single decision boundary cannot be robust in different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. \n\nThe main concern with the paper is that most of the theoretical results might have a very restrictive scope and the writing is difficult to follow. \n\nThe authors expressed concerns about a review not being very constructive. In a nutshell, the review in question points out that the theory might be too restrictive, that the experimental section is not very strong, that there are other works on related topics, and that the writing of the paper could be improved. While I understand the disappointing of the authors, the main points here appear to be consistent with the other reviews, which also mention that the theoretical results in this paper are not very general, that the writing is a bit complicated or heavy in mathematics, and not easy to follow, or that it is not clear if the bounds can be useful or easily applied in other work. \n\nOne reviewer rates the paper marginally above the acceptance threshold, while two other reviewers rate the paper below the acceptance threshold. ", "reviews": [{"review_id": "H1lug3R5FX-0", "review_text": "This paper studies the geometry of adversarial examples under the assumption that dataset encountered in practice exhibit lower dimensional structure despite being embedded in very high dimensional input spaces. Under the proposed framework, the authors analyze several interesting phenomena and give theoretical results related to the necessary number of samples needed to achieves robustness. However, the theory in this paper is not very deep. Pros: The logic of this paper is very clear and easy to follow. Definitions and theories are illustrated with well-designed figures. This paper shows the tradeoff between robustness under two norm and infinity norm for the case when the manifolds of two classes of data are concentric spheres. When data are distributed on a hypercube in a k dimensional subspace, the authors show that balls with radius \\delta centered at data samples only covers a small part of the \u2018\\delta neighborhood\u2019 of the manifold. General theoretical results on robustness and minimum training set to guarantee robustness are given for nearest neighbor classifiers and other classifiers. Cons: Most of the theoretical results in this paper are not very general. The tradeoff between robustness in different norms are only shown for concentric spheres; the \u2018X^\\epsilon is a poor model of \\mathcal{M}^\\epsilon\u2019 section is only shown for hypercubes in low dimensional subspaces. Section 5 is not very convincing. As is discussed later in the paper, although $X^\\delta$ only covers a small part of \\mathcal{M}^\\delta, robustness can be achieved by using balls centered at samples with larger radius. Most of the analysis is based on the assumption that samples are perfectly distributed to achieve the best possible robustness result. A more interesting case is probably when samples are generated on the manifold following some probabilistic distributions. Theorems given in Section 6 are reasonable, but not very significant. It is not very surprising that nearest neighbor classifier is more robust than \u2018x^\\epsilon based\u2019 algorithms, especially when the samples are perfectly distributed. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Please see our new post for common comments . Below we respond to your individual concerns . Re : \u2018 X^\\epsilon is a poor model of \\mathcal { M } ^\\epsilon \u2019 is only shown for hypercubes Thank you for pointing out that this section is unclear . The primary mathematical result in Section 5 is in Equation 4 , which shows that this phenomena holds for general manifolds under additional conditions . To emphasize this , we have moved this result to the beginning of the section and under an explicit Theorem statement . We originally chose the order in the submission to first provide intuition to reader before introducing the more general result . We plan to keep that body of text but to more clearly contextualize it as an intuitive walkthrough in a special case . We invite additional suggestions for improvements . Re : Robustness tradeoff only shown for concentric spheres It is a good question to ask how often does the L2 decision axis differ from the Linf decision axis . We believe that it is the common case that the L2 decision axis differs from the Linf decision axis , and that this phenomena explains recent results on adversarial robustness under different norms [ 1 ] . The result extends easily to , e.g. , two concentric cylinders . Consider a two-dimensional axis-aligned cross section ; in this cross section , the fact that the optimal decision boundaries differ is a corollary of our result . A similar argument works for intertwined tori . We have updated the paper to include a discussion of this intuition and will update the paper with any additional formal results we develop . We also note that our proof uses the spheres in a proof by construction . We made this choice for readability . This is common in theory on adversarial examples . [ 2 ] uses theoretical results about linear classifiers and simple data distributions to provide insight into robustness for adversarial examples . Additionally , our work provides a new source of problems , motivated by adversarial examples , for the computational geometry community . The CG literature has been motivated by reconstructing manifolds , for which the decision axes under the L2 norm is all that is needed . The CG community has not explored the geometry of max-margin decision boundaries under norms other than L2 . Re : Using larger balls to achieve robustness . We are unsure of the meaning of this crique . In the statement \u201c As is discussed later in the paper \u2026 robustness can be achieved by using balls centered at samples with larger radius \u201d , can you clarify which discussion you are referencing ? We consider ball-based learners . In Section 5 we show that the ratio of the volume of the union of balls around the samples to the volume of the tubular neighborhood approaches 0 as the codimension increases . This causes problems for ball-based learners because the measure captured by the balls is 0 in high codimensions . A natural way to remedy this is to use larger balls . However there are limits to how large the balls can be made before they begin to intersect the balls from samples on a different class manifold . Theorem 1 shows that even when ball-based learners use the largest possible balls , they require many more samples to achieve the same amount of robustness . Re : Proposed extension to non-uniform distributions on data manifolds . We are very interested in the direction you suggested . In future work we intend to combine techniques from the statistics literature with our framework , including sampling according to some distribution on a manifold , to understand the more difficult setting and prove more realistic guarantees for learning algorithms . In this paper our first goal is to bridge two disparate communities , leveraging the techniques from the manifold reconstruction literature to provide a different perspective and new tools for the problem of adversarial examples . We wish to understand the simplest version of the problem , which , as shown in our paper , already makes several issues clear , such as the affect of codimension on adversarial robustness . Re : Significance of robustness of nearest neighbors versus \u2018 x^\\epsilon based \u2019 algorithms . We apologize for not making the point of the results in Section 6 clear . The importance of Theorem 1 is to show that different classification algorithms have different sampling requirements with respect to robustness . In particular nearest neighbor classifiers require fewer samples to achieve the same level of robustness for a fixed codimension . The ball-based learner is a theoretical model of the adversarial training used in state-of-the-art defenses for adversarial examples [ 3,4 ] . We have updated this section to make the importance of our results more clear . [ 1 ] Towards the first adversarially robust neural network model on MNIST . [ 2 ] Adversarially robust generalization requires more data . NIPS [ 3 ] Explaining and harnessing adversarial examples . ICLR [ 4 ] Towards deep learning models resistant to adversarial attacks . ICLR"}, {"review_id": "H1lug3R5FX-1", "review_text": "This paper gives a theoretical analysis of adversarial examples, showing that (i) there exists a tradeoff between robustness in different norms, (ii) adversarial training is sample inefficient, and (iii) the nearest neighbor classifier can be robust under certain conditions. The biggest weakness of the paper is that theoretical analysis is done on a very synthetic dataset, whereas real datasets can hardly be conceived to exhibit similar properties. Furthermore, the authors do not give a bound on the probability that the sampling conditions for the robust nearest neighbor classifier (Theorem 1) will be satisfied, leading to potentially vacuous results. While I certainly agree that theoretical analysis of the adversarial example phenomenon is challenging, there have been prior work on both analyzing the robustness of k-NN classifiers (Wang et al., 2018 - http://proceedings.mlr.press/v80/wang18c/wang18c.pdf) and on demonstrating the curse of dimensionality as a major contributing factor to adversarial examples (Shafahi et al., 2018 - https://arxiv.org/abs/1809.02104, concurrent submission to ICLR). I am very much in favor of the field moving in these directions, but I do not think this submission is demonstrating any meaningful progress. Pros: - Rigorous theoretical analysis. Cons: - Results are proven for particular settings rather than relying on realistic data distribution assumptions. - Paper is poorly written. The authors use unnecessarily complicated jargon to explain simple concepts and the proofs are written to confuse the reader. This is especially a problem since the paper exceeds the suggested page limit of 8 pages. - While it is certain that nearest neighbor classifiers are robust to adversarial examples, their application is limited to only very simple datasets. This makes the robustness result lacking in applicability. - Weak experimental validation. The authors make repeat use of synthetic datasets and only validate their claim on MNIST as a real dataset.", "rating": "3: Clear rejection", "reply_text": "Thank you for your review . Please see our new post for common comments . Below we respond to your individual concerns . Re : Dimensionality in adversarial examples The review points to a concurrent submission to ICLR [ 1 ] as an example of prior work on the relationship between high dimensional input spaces and adversarial examples . Our paper is primarily concerned with the effect of codimension on robustness to adversarial examples . In this sense our paper is not about the curse of dimensionality , but rather of codimensionality . Thus , our work is complementary to [ 1 ] . We highlight the differences below . Shafahi et al.model each class as a probability density function defined on some domain . In contrast we define each class as a low dimensional manifold . In their model , our results consider the case where data lies on a measure 0 subset of the embedding space . Their results hold under the condition that \u201c the class distribution is not overly concentrated \u201d [ 1 ] . In their discussion the suggest extending their results to measure zero densities by considering a tube around the data . In our framework a robust classifier is one that accurately classifies the tube around the data . Our results show that learning an accurate classifier on the manifold is a fundamentally easier problem than learning an accurate classifier on the tube around the manifold , i.e.a robust classifier . We also note that the results in [ 1 ] are exclusive to spheres and cubes , which exhibit concentration of measure properties . Our paper cites [ 2 ] which also considered concentration properties of spheres and how they impact adversarial robustness . Re : Analysis on synthetic datasets One of our primary contributions is to exhibit a tradeoff in robustness under different norms . We show that this tradeoff exists in general by exhibiting a setting where the decision axes are not equal . Please see our response to R1 on how often this occurs . We have updated the paper to include this response . The remainder of our theoretical results apply to general manifolds and we have updated the text to make this clear . Re : Clarity We are sorry that you found the paper unclear . We made a concerted effort to make the paper readable , but it is clear that we can do better . We also note that this is an interdisciplinary paper , and one of the contributions of our work is to make a very difficult to access field more accessible to adversarial examples researchers . High dimensional geometry is highly counterintuitive . As a result , the field of computational geometry prioritizes clear rigorous formal proof and tools . We attempted to use the simplest tools that we could and we iterated multiple times to cut down on unnecessary definitions . We also note that R1 commented positively on the clarity of our paper . We are highly committed to resolving any clarity issues with the paper and will be happy to incorporate any concrete suggestions that you have . Re : Applicability of nearest neighbor results As evidenced by the ICML paper that you provided ( Wang et al.2018 ) , the robustness of nearest neighbor classifiers is an active and interesting research question . Wang et al.even provide a modification of the standard nearest neighbors algorithm that they show is more robust in practice . Our paper is not about nearest neighbor classifiers . As we described in the rebuttal to R1 , it is important to understand why nearest neighbor classifiers are more robust in our setting . Nearest neighbors naturally handles high codimension settings because the Voronoi cells are elongated in the normal directions . We have modified the paper to make this point clear . Re : Concerns on experimental validation Our primary contribution is our theoretical results detailed in the summary above . Our experiments complement our theoretical results . Our synthetic training data is intended to explore the predictive power of our model of learners for real algorithms . Our results in Fig.2 show that our theory for changing the norm predicts when real adversarial approaches fail . The CIRCLES and PLANES datasets show that real algorithms do , in fact , show this vulnerability to codimension . Our experiment on MNIST provides an example of a dataset with non-uniform sampling where nearest neighbor classifiers have fundamentally different performance than an adversarial training approach . We will update the paper to emphasize ways our experiments complement and support our other results . We have considered additional experiments that modify co-dimension for MNIST or the big-MNIST domain from [ 1 ] and would be happy to run them if requested . We would like to highlight the fact that we made careful effort to use state-of-the-art attacks and defenses and followed best practices when running the experiments ( e.g.averaging over multiple retrainings ) . [ 1 ] Shafahi etal , Are adversarial examples inevitable ? [ 2 ] Adversarial spheres [ 3 ] Adversarially robust generalization requires more data"}, {"review_id": "H1lug3R5FX-2", "review_text": "This paper tried to analyze the high-dimensional geometry of adversarial examples from a geometric framework. The authors explained that there exists a tradeoff between being robust to different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. Moreover, this paper showed that nearest neighbor classifiers do not suffer from this insufficiency. In general, I think this paper is very interesting and enlightening. The authors analyzed the most robust boundary of norm 2 and norm infinity in different dimensions through a simple example and concluded that the single decision boundary cannot be robust in different norms. In addition, the author started from a special manifold and proposed a bound (ratio of two volumes) to prove the insufficiency of the traditional adversarial training methods and then extended to arbitrary manifold. It is good that this might provide a new way to evaluate the robustness of adversarial training method. However, I have some concerns: 1) Is it rigorous to define the bound by vol_X/vol_pi? In my opinion, the ratio of the volume of intersection (X^\\del and \\pi^\\del) and vol \\pi^\\del may be more rigorous? 2) I don't know if such bound can be useful or easily applied in other work? In my opinion, it might be difficult, since the volume itself appears difficult to calculate. I think the paper is a bit complicated or heavy in mathematics, and not easy to follow (though I believe I have well understood it). Some typos and minor issues are also listed as below. Minor concerns: 1. At the end of the introduction, 3 attacking methods, FGSM, BIM, and PGD, should be given their full names and also citations are necessary. 2. Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1. 3. In Page 7, \u201cFigure 4 (left) shows that this expression approaches 1 as the codimension (d-k) of Pi increases.\u201d I think, the subfigure shows that the ratio approaches 1 when d and k are all increased. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Please see our new post for common comments . Below we respond to your individual concerns . Re : Definition of volume ratios . You 're correct that it would not be rigorous to define the bound using \\vol X / \\vol pi , as \\vol X = 0 , since the volume of a set of points is 0 . We do not define the ratio in this way . Instead we define the ratio as \\vol X^\\del / \\vol pi^\\del . Your suggestion is indeed a rigorous way to compute this ratio . After some thought , your suggestion is equivalent to ours for this setting . In our setting , since the points are on the manifold pi , \\vol X^\\del is a subset of \\vol pi^del , and so the intersection \\vol X^\\del \\cap \\vol pi^\\del = \\vol X^\\del . It follows that ( \\vol X^\\del \\cap \\vol pi^\\del ) / \\vol pi^\\del = \\vol X^\\del / \\vol pi^\\del . We have added this argument to the paper . Re : Can volume bounds be applied to other work We had not considered applying this bound as a metric to evaluate the robustness of a specific adversarial training method . We think this is an interesting direction for future work . Toward this end , one may want to compute the intrinsic dimension of the data manifold . There has been prior work on estimating the dimension of manifolds from samples [ 1,2,3 ] . In our summary we highlight that our key contribution is to identify the role of codimension on the pervasiveness of adversarial examples . In our paper , the primary purpose of our bounds is to prove that there exist classifiers that perform differently with respect to codimension . The specific values of these bounds show that this difference can , in theory , be exponentially sized . The primary practical use of our bounds is that they suggests that the transition from robust to non-robust is rapid ; more like a phase transition , than a gradual shift . Re : Could you provide a specific example to illustrate the bound in Eq . ( 3 ) , e.g.in the case of d=3 , k=1 . This is a good idea and we are currently considering the type of figure we might create to illustrate Equation 3 . We note that we do provide an illustration in Figure 3 ( right ) to illustrate Equation 2 , which is the more difficult step in deriving Equation 3 . We updated the text to make the reference to that figure more prominent . Re : Subfigure in Figure 4 Thank you for pointing out that we should have been more clear with our explanation . We can imagine two settings , one where we hold d fixed and increase k , and another where we hold k fixed and increase d. In the first setting , Figure 4 shows that lower dimensional problems are generally easier . This aligns well with results and intuition in the machine learning community . We are trying to draw attention to the second setting , that if we hold k fixed and increase d ( and thus increase the codimension ) the problem becomes more difficult . [ 1 ] Dimension Detection by Local Homology [ 2 ] Maximum Likelihood Estimation of Intrinsic Dimension [ 3 ] Estimating Local Intrinsic Dimension with k-Nearest Neighbor Graphs"}], "0": {"review_id": "H1lug3R5FX-0", "review_text": "This paper studies the geometry of adversarial examples under the assumption that dataset encountered in practice exhibit lower dimensional structure despite being embedded in very high dimensional input spaces. Under the proposed framework, the authors analyze several interesting phenomena and give theoretical results related to the necessary number of samples needed to achieves robustness. However, the theory in this paper is not very deep. Pros: The logic of this paper is very clear and easy to follow. Definitions and theories are illustrated with well-designed figures. This paper shows the tradeoff between robustness under two norm and infinity norm for the case when the manifolds of two classes of data are concentric spheres. When data are distributed on a hypercube in a k dimensional subspace, the authors show that balls with radius \\delta centered at data samples only covers a small part of the \u2018\\delta neighborhood\u2019 of the manifold. General theoretical results on robustness and minimum training set to guarantee robustness are given for nearest neighbor classifiers and other classifiers. Cons: Most of the theoretical results in this paper are not very general. The tradeoff between robustness in different norms are only shown for concentric spheres; the \u2018X^\\epsilon is a poor model of \\mathcal{M}^\\epsilon\u2019 section is only shown for hypercubes in low dimensional subspaces. Section 5 is not very convincing. As is discussed later in the paper, although $X^\\delta$ only covers a small part of \\mathcal{M}^\\delta, robustness can be achieved by using balls centered at samples with larger radius. Most of the analysis is based on the assumption that samples are perfectly distributed to achieve the best possible robustness result. A more interesting case is probably when samples are generated on the manifold following some probabilistic distributions. Theorems given in Section 6 are reasonable, but not very significant. It is not very surprising that nearest neighbor classifier is more robust than \u2018x^\\epsilon based\u2019 algorithms, especially when the samples are perfectly distributed. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Please see our new post for common comments . Below we respond to your individual concerns . Re : \u2018 X^\\epsilon is a poor model of \\mathcal { M } ^\\epsilon \u2019 is only shown for hypercubes Thank you for pointing out that this section is unclear . The primary mathematical result in Section 5 is in Equation 4 , which shows that this phenomena holds for general manifolds under additional conditions . To emphasize this , we have moved this result to the beginning of the section and under an explicit Theorem statement . We originally chose the order in the submission to first provide intuition to reader before introducing the more general result . We plan to keep that body of text but to more clearly contextualize it as an intuitive walkthrough in a special case . We invite additional suggestions for improvements . Re : Robustness tradeoff only shown for concentric spheres It is a good question to ask how often does the L2 decision axis differ from the Linf decision axis . We believe that it is the common case that the L2 decision axis differs from the Linf decision axis , and that this phenomena explains recent results on adversarial robustness under different norms [ 1 ] . The result extends easily to , e.g. , two concentric cylinders . Consider a two-dimensional axis-aligned cross section ; in this cross section , the fact that the optimal decision boundaries differ is a corollary of our result . A similar argument works for intertwined tori . We have updated the paper to include a discussion of this intuition and will update the paper with any additional formal results we develop . We also note that our proof uses the spheres in a proof by construction . We made this choice for readability . This is common in theory on adversarial examples . [ 2 ] uses theoretical results about linear classifiers and simple data distributions to provide insight into robustness for adversarial examples . Additionally , our work provides a new source of problems , motivated by adversarial examples , for the computational geometry community . The CG literature has been motivated by reconstructing manifolds , for which the decision axes under the L2 norm is all that is needed . The CG community has not explored the geometry of max-margin decision boundaries under norms other than L2 . Re : Using larger balls to achieve robustness . We are unsure of the meaning of this crique . In the statement \u201c As is discussed later in the paper \u2026 robustness can be achieved by using balls centered at samples with larger radius \u201d , can you clarify which discussion you are referencing ? We consider ball-based learners . In Section 5 we show that the ratio of the volume of the union of balls around the samples to the volume of the tubular neighborhood approaches 0 as the codimension increases . This causes problems for ball-based learners because the measure captured by the balls is 0 in high codimensions . A natural way to remedy this is to use larger balls . However there are limits to how large the balls can be made before they begin to intersect the balls from samples on a different class manifold . Theorem 1 shows that even when ball-based learners use the largest possible balls , they require many more samples to achieve the same amount of robustness . Re : Proposed extension to non-uniform distributions on data manifolds . We are very interested in the direction you suggested . In future work we intend to combine techniques from the statistics literature with our framework , including sampling according to some distribution on a manifold , to understand the more difficult setting and prove more realistic guarantees for learning algorithms . In this paper our first goal is to bridge two disparate communities , leveraging the techniques from the manifold reconstruction literature to provide a different perspective and new tools for the problem of adversarial examples . We wish to understand the simplest version of the problem , which , as shown in our paper , already makes several issues clear , such as the affect of codimension on adversarial robustness . Re : Significance of robustness of nearest neighbors versus \u2018 x^\\epsilon based \u2019 algorithms . We apologize for not making the point of the results in Section 6 clear . The importance of Theorem 1 is to show that different classification algorithms have different sampling requirements with respect to robustness . In particular nearest neighbor classifiers require fewer samples to achieve the same level of robustness for a fixed codimension . The ball-based learner is a theoretical model of the adversarial training used in state-of-the-art defenses for adversarial examples [ 3,4 ] . We have updated this section to make the importance of our results more clear . [ 1 ] Towards the first adversarially robust neural network model on MNIST . [ 2 ] Adversarially robust generalization requires more data . NIPS [ 3 ] Explaining and harnessing adversarial examples . ICLR [ 4 ] Towards deep learning models resistant to adversarial attacks . ICLR"}, "1": {"review_id": "H1lug3R5FX-1", "review_text": "This paper gives a theoretical analysis of adversarial examples, showing that (i) there exists a tradeoff between robustness in different norms, (ii) adversarial training is sample inefficient, and (iii) the nearest neighbor classifier can be robust under certain conditions. The biggest weakness of the paper is that theoretical analysis is done on a very synthetic dataset, whereas real datasets can hardly be conceived to exhibit similar properties. Furthermore, the authors do not give a bound on the probability that the sampling conditions for the robust nearest neighbor classifier (Theorem 1) will be satisfied, leading to potentially vacuous results. While I certainly agree that theoretical analysis of the adversarial example phenomenon is challenging, there have been prior work on both analyzing the robustness of k-NN classifiers (Wang et al., 2018 - http://proceedings.mlr.press/v80/wang18c/wang18c.pdf) and on demonstrating the curse of dimensionality as a major contributing factor to adversarial examples (Shafahi et al., 2018 - https://arxiv.org/abs/1809.02104, concurrent submission to ICLR). I am very much in favor of the field moving in these directions, but I do not think this submission is demonstrating any meaningful progress. Pros: - Rigorous theoretical analysis. Cons: - Results are proven for particular settings rather than relying on realistic data distribution assumptions. - Paper is poorly written. The authors use unnecessarily complicated jargon to explain simple concepts and the proofs are written to confuse the reader. This is especially a problem since the paper exceeds the suggested page limit of 8 pages. - While it is certain that nearest neighbor classifiers are robust to adversarial examples, their application is limited to only very simple datasets. This makes the robustness result lacking in applicability. - Weak experimental validation. The authors make repeat use of synthetic datasets and only validate their claim on MNIST as a real dataset.", "rating": "3: Clear rejection", "reply_text": "Thank you for your review . Please see our new post for common comments . Below we respond to your individual concerns . Re : Dimensionality in adversarial examples The review points to a concurrent submission to ICLR [ 1 ] as an example of prior work on the relationship between high dimensional input spaces and adversarial examples . Our paper is primarily concerned with the effect of codimension on robustness to adversarial examples . In this sense our paper is not about the curse of dimensionality , but rather of codimensionality . Thus , our work is complementary to [ 1 ] . We highlight the differences below . Shafahi et al.model each class as a probability density function defined on some domain . In contrast we define each class as a low dimensional manifold . In their model , our results consider the case where data lies on a measure 0 subset of the embedding space . Their results hold under the condition that \u201c the class distribution is not overly concentrated \u201d [ 1 ] . In their discussion the suggest extending their results to measure zero densities by considering a tube around the data . In our framework a robust classifier is one that accurately classifies the tube around the data . Our results show that learning an accurate classifier on the manifold is a fundamentally easier problem than learning an accurate classifier on the tube around the manifold , i.e.a robust classifier . We also note that the results in [ 1 ] are exclusive to spheres and cubes , which exhibit concentration of measure properties . Our paper cites [ 2 ] which also considered concentration properties of spheres and how they impact adversarial robustness . Re : Analysis on synthetic datasets One of our primary contributions is to exhibit a tradeoff in robustness under different norms . We show that this tradeoff exists in general by exhibiting a setting where the decision axes are not equal . Please see our response to R1 on how often this occurs . We have updated the paper to include this response . The remainder of our theoretical results apply to general manifolds and we have updated the text to make this clear . Re : Clarity We are sorry that you found the paper unclear . We made a concerted effort to make the paper readable , but it is clear that we can do better . We also note that this is an interdisciplinary paper , and one of the contributions of our work is to make a very difficult to access field more accessible to adversarial examples researchers . High dimensional geometry is highly counterintuitive . As a result , the field of computational geometry prioritizes clear rigorous formal proof and tools . We attempted to use the simplest tools that we could and we iterated multiple times to cut down on unnecessary definitions . We also note that R1 commented positively on the clarity of our paper . We are highly committed to resolving any clarity issues with the paper and will be happy to incorporate any concrete suggestions that you have . Re : Applicability of nearest neighbor results As evidenced by the ICML paper that you provided ( Wang et al.2018 ) , the robustness of nearest neighbor classifiers is an active and interesting research question . Wang et al.even provide a modification of the standard nearest neighbors algorithm that they show is more robust in practice . Our paper is not about nearest neighbor classifiers . As we described in the rebuttal to R1 , it is important to understand why nearest neighbor classifiers are more robust in our setting . Nearest neighbors naturally handles high codimension settings because the Voronoi cells are elongated in the normal directions . We have modified the paper to make this point clear . Re : Concerns on experimental validation Our primary contribution is our theoretical results detailed in the summary above . Our experiments complement our theoretical results . Our synthetic training data is intended to explore the predictive power of our model of learners for real algorithms . Our results in Fig.2 show that our theory for changing the norm predicts when real adversarial approaches fail . The CIRCLES and PLANES datasets show that real algorithms do , in fact , show this vulnerability to codimension . Our experiment on MNIST provides an example of a dataset with non-uniform sampling where nearest neighbor classifiers have fundamentally different performance than an adversarial training approach . We will update the paper to emphasize ways our experiments complement and support our other results . We have considered additional experiments that modify co-dimension for MNIST or the big-MNIST domain from [ 1 ] and would be happy to run them if requested . We would like to highlight the fact that we made careful effort to use state-of-the-art attacks and defenses and followed best practices when running the experiments ( e.g.averaging over multiple retrainings ) . [ 1 ] Shafahi etal , Are adversarial examples inevitable ? [ 2 ] Adversarial spheres [ 3 ] Adversarially robust generalization requires more data"}, "2": {"review_id": "H1lug3R5FX-2", "review_text": "This paper tried to analyze the high-dimensional geometry of adversarial examples from a geometric framework. The authors explained that there exists a tradeoff between being robust to different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. Moreover, this paper showed that nearest neighbor classifiers do not suffer from this insufficiency. In general, I think this paper is very interesting and enlightening. The authors analyzed the most robust boundary of norm 2 and norm infinity in different dimensions through a simple example and concluded that the single decision boundary cannot be robust in different norms. In addition, the author started from a special manifold and proposed a bound (ratio of two volumes) to prove the insufficiency of the traditional adversarial training methods and then extended to arbitrary manifold. It is good that this might provide a new way to evaluate the robustness of adversarial training method. However, I have some concerns: 1) Is it rigorous to define the bound by vol_X/vol_pi? In my opinion, the ratio of the volume of intersection (X^\\del and \\pi^\\del) and vol \\pi^\\del may be more rigorous? 2) I don't know if such bound can be useful or easily applied in other work? In my opinion, it might be difficult, since the volume itself appears difficult to calculate. I think the paper is a bit complicated or heavy in mathematics, and not easy to follow (though I believe I have well understood it). Some typos and minor issues are also listed as below. Minor concerns: 1. At the end of the introduction, 3 attacking methods, FGSM, BIM, and PGD, should be given their full names and also citations are necessary. 2. Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1. 3. In Page 7, \u201cFigure 4 (left) shows that this expression approaches 1 as the codimension (d-k) of Pi increases.\u201d I think, the subfigure shows that the ratio approaches 1 when d and k are all increased. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Please see our new post for common comments . Below we respond to your individual concerns . Re : Definition of volume ratios . You 're correct that it would not be rigorous to define the bound using \\vol X / \\vol pi , as \\vol X = 0 , since the volume of a set of points is 0 . We do not define the ratio in this way . Instead we define the ratio as \\vol X^\\del / \\vol pi^\\del . Your suggestion is indeed a rigorous way to compute this ratio . After some thought , your suggestion is equivalent to ours for this setting . In our setting , since the points are on the manifold pi , \\vol X^\\del is a subset of \\vol pi^del , and so the intersection \\vol X^\\del \\cap \\vol pi^\\del = \\vol X^\\del . It follows that ( \\vol X^\\del \\cap \\vol pi^\\del ) / \\vol pi^\\del = \\vol X^\\del / \\vol pi^\\del . We have added this argument to the paper . Re : Can volume bounds be applied to other work We had not considered applying this bound as a metric to evaluate the robustness of a specific adversarial training method . We think this is an interesting direction for future work . Toward this end , one may want to compute the intrinsic dimension of the data manifold . There has been prior work on estimating the dimension of manifolds from samples [ 1,2,3 ] . In our summary we highlight that our key contribution is to identify the role of codimension on the pervasiveness of adversarial examples . In our paper , the primary purpose of our bounds is to prove that there exist classifiers that perform differently with respect to codimension . The specific values of these bounds show that this difference can , in theory , be exponentially sized . The primary practical use of our bounds is that they suggests that the transition from robust to non-robust is rapid ; more like a phase transition , than a gradual shift . Re : Could you provide a specific example to illustrate the bound in Eq . ( 3 ) , e.g.in the case of d=3 , k=1 . This is a good idea and we are currently considering the type of figure we might create to illustrate Equation 3 . We note that we do provide an illustration in Figure 3 ( right ) to illustrate Equation 2 , which is the more difficult step in deriving Equation 3 . We updated the text to make the reference to that figure more prominent . Re : Subfigure in Figure 4 Thank you for pointing out that we should have been more clear with our explanation . We can imagine two settings , one where we hold d fixed and increase k , and another where we hold k fixed and increase d. In the first setting , Figure 4 shows that lower dimensional problems are generally easier . This aligns well with results and intuition in the machine learning community . We are trying to draw attention to the second setting , that if we hold k fixed and increase d ( and thus increase the codimension ) the problem becomes more difficult . [ 1 ] Dimension Detection by Local Homology [ 2 ] Maximum Likelihood Estimation of Intrinsic Dimension [ 3 ] Estimating Local Intrinsic Dimension with k-Nearest Neighbor Graphs"}}