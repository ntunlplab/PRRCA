{"year": "2020", "forum": "HJe88xBKPr", "title": "Mixed Precision Training With 8-bit Floating Point", "decision": "Reject", "meta_review": "This paper propose a method to train DNNs using 8-bit floating point numbers, by using an enhanced loss scaling method and stochastic rounding method. However, the proposed method lacks novel and both the paper presentation and experiments need to be improved throughout. ", "reviews": [{"review_id": "HJe88xBKPr-0", "review_text": "In this paper, the authors propose a method to train deep neural networks using 8-bit floating point representation for weights, activations, errors, and gradients. They use enhanced loss scale, quantization and stochastic rounding techniques to balance the numerical accuracy and computational efficiency. Finally, they get a slightly better validation accuracy compared to full precision baseline. Overall, this paper focuses on engineering techniques about mixed precision training with 8-bit floating point, and state-of-the-art accuracy across multiple data sets shows the effectiveness of their work. However, there are some problems to be clarified. 1. The authors apply several techniques to improve the precision for training with 8-bit floating point, but they do not show the gain for each individual. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique? This should be clearly presented and more experimental comparison is expected. 2. The paper should present a bit more background knowledge and discussion on the adopted techniques. For instance, why the stochastic rounding method proposed in this article by adding a random value in probability can regulate quantization noise in the gradients? And why Resnet-50 demands a large scaling factor? 3. On Table 3, in comparison with Wang et al. (2018), the authors use layers with FP32 (not FP16 in Wang). Thus, it is hard to say the improvement comes from the proposed 8-bit training. This should be clarified. 4. How to set the hyper-parameters, such as scale, thresholds and so on, is not clear in the paper. There are no guidelines for readers to use these techniques. 5. The authors did not give a clear description of the implement for the enhanced loss scaling. They apply different loss scaling methods for different networks. This should be explained in detail. 6. In the experiment, for a single model, some layers are 8-bit, some layers are 32-bit and some layers are 16-bit. Is the 8-bit training only applicable for a part of the model? How do we know which layer is suitable for 8-bit training? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . We will attempt to answer your questions below . Q1.Our intention was to show that both enhanced loss scaling and stochastic rounding are essential for achieving full accuracy with FP8 training . For example , in section 3.1 , our experiments already use \u201c stochastic rounding \u201d on gradients ( essential for convergence ) to study the impact of loss scaling in isolation . Similarly , in section 3.2 when studying the impact of stochastic rounding , we employed the \u2018 best loss scaling strategy \u2019 derived from section 3.1 . Perhaps this is not clearly described in the paper . We will edit the text for clarity and upload the new version of the paper . In Figure 2a , we have compared multiple experimental results to demonstrated impact of using different loss scaling values on final accuracy of Resnet50 . Q2.On stochastic rounding : As we discussed in Section 3.2 , rounding plays a significant role for FP8 because the rounding errors are quite large at this precision . It is known that standard rounding methods ( up , down , towards zero , away from zero ) have a positive or a negative bias to the final distribution . The most popular rounding method used by floating point today is round to nearest even ( RNE ) \u2013 although this method is free of positive or a negative bias -- it distorts the data distribution to have more even numbers than odd . ( more info here : https : //en.wikipedia.org/wiki/Rounding # Floating-point_rounding ) . It is also known that rounding errors grow with longer accumulation chains ( like in Convolution and MatMul ) . For RNE method , the rounding errors grow proportional to the square root of number of accumulations . This is quite significant at extreme low precisions ( like FP8 ) where \u2018 episilon \u2019 value is large . Stochastic rounding is bias free because it uses random probability term for tie-breaking . It does not impact the overall data distribution of the tensor and the rounding errors are small and evenly distributed . This makes the accumulation of errors during long accumulation chains much less likely . > > On why Resnet-50 demands a large scaling factor ? : In general working with FP8 would require larger scaling factor because FP8 has smaller dynamic range compared to FP16 . The smallest number that can be represented by FP16 is 5.96e-8 whereas the smallest number that FP8 can represent is 1.52e-5 . This means that a larger percentage of smaller gradients fall \u2018 below \u2019 the FP8 range . Hence , we need to use a larger scaling factor to push them up into the FP8 range . Q3.We would like to clarify that we do not use FP32 in any of our training results . For Resnet-50 , all convolution and batchnorm layers use FP8 -- except the first conv and last FC layers which use FP16 ; we also use FP16 master copy of weight . This configuration identical to what is used by Wang et.al. -- hence the comparison is fair . The key difference between our implementations is that we use FP32 accumulator ( in the ALU ) while Wang et.al use a modified FP16 ( 1-6-9 format ) \u2013 as a result , they need to implement additional hardware in the ALU path to perform stochastic rounding on the accumulator to preserve accuracy . Given the complexity of building stochastic rounding hardware , their implementation will be more expensive to build . We discussed these design trade-offs in Section 1 . Q4.We employ the widely disseminated techniques that are used for FP16 mixed precision training , these are implemented in frameworks such as Tensorflow and PyTorch . Our loss scaling methods are modifications on top of these baseline methods.To answer your specific question : Scale ( =2 ) and threshold ( min=2 , max=2^14 ) values are hard-coded in in the current implementation of loss scaling algorithm . The dynamic loss scaling algorithm increments the loss scale value by a factor of \u2018 scale \u2019 every 2000 iteration intervals and reduced the loss scale by a factor \u2018 scale \u2019 in the case of an occurrence of \u2018 NaN \u2019 in the during gradient computation . For GNMT training , the enhanced loss scaling method updates the \u2018 min \u2019 threshold value according to the schedule shown in Figure 2b to prevent the loss scale becoming too small . We will add the description of the algorithm to the paper . Q5.We have described the loss scaling methods applied to each model in section 3.1 For Resnet50 , we use constant loss scaling of 10K , this is derived empirically through experimentation which are detailed in section 3.1 . For GNMT and Transformer , we use dynamic loss scaling implemented by Tensorflow . Q6.For now , the process of selecting which layers to run at FP8 requires human expertise and intervention . But we expect the future frameworks to automate this process of selecting multiple precision options to maximize performance . Recent work on use of AutoML [ 1 ] for mixed-precision quantization is also promising research direction . [ 1 ] HAQ : Hardware-Aware Automated Quantization with Mixed Precision , Kuan Wang et.al. , CVPR 2019 ."}, {"review_id": "HJe88xBKPr-1", "review_text": "This paper is about training deep models with 8-bit floating point numbers. The authors use an enhanced loss scaling method and stochastic rounding method to stabilize training. They do experiments on image classification and NLP tasks. The paper is clearly written. However, I don\u2019t think this paper passes the bar of ICLR. This paper lacks innovation and insightful analysis. 1.Sec. 3.1 proposes enhanced loss scaling. Loss scaling is a heuristic to train low-precision neural networks. The authors train 8-bit GNMT with a changing scaling factor. However, this looks like some manually tuned result for GNMT only. I doubt if this generalizes to other models. Besides, there is no equation or algorithm flowchart to demonstrate their method. It\u2019s not very readable. 2.The logic of Sec. 3.2 is quite confusing. The authors first empirically show that the performance of ResNet-50 significantly drops with 8-bit training. Then they show the sum of the square of the weights in ResNet-50 is high at the beginning. With this observation, they claim it demonstrates the drawback of \u2018rounding-to-nearest-even\u2019. I cannot see the connection between the norm of weights and the rounding technique. Moreover, the stochastic rounding has already been used in 8-bit training.[1] 3.The setting in the experiment section is not stated clearly. For example, what\u2019s the hyper-parameter for loss scaling? Another question is the gradient. In Sec. 3, just above Fig. 1, the authors claim the weight update is performed in full-precision. In contrast, they claim the gradient is 8-bit in table 3. If the update is full-precision, [2] is an important baseline. Small suggestions: 1.For Fig. 6, I suggest the authors to smooth the loss curves to avoid overlap of two curves. 2.There are two \u2018with\u2019s in the last paragraph of page 7. Reference: [1]Wang N, Choi J, Brand D, et al. Training deep neural networks with 8-bit floating point numbers[C]//Advances in neural information processing systems. 2018: 7675-7684. [2]Banner R, Hubara I, Hoffer E, et al. Scalable methods for 8-bit training of neural networks[C]//Advances in Neural Information Processing Systems. 2018: 5145-5153.", "rating": "1: Reject", "reply_text": "Thank you for your detailed review and comments . Q1.Please note that only GNMT required the hand tuned loss scaling schedule . We believe this method can be automated for GNMT as well . We have observed that GNMT saw wider error gradient distributions which often consisted of outliers that are much larger than the mean . When these outliers are scaled with a large scaling factor , they overflow and cause a NaN when gradients for previous layer are computed . The current automatic loss scaling algorithm is ill-equipped to handle these transient NaNs , it over-corrects ( reduces ) the loss scale value every time it encounters an outlier , resulting in divergence . Our enhanced loss scaling strategy mitigates this by adding a lower threshold to prevent loss scale value from becoming too small . We believe adding a few additional conditions to loss scaling algorithm will handle this case automatically . The current loss scaling algorithm works like this : Initial \u2018 loss_scale \u2019 value is set to \u2018 max_threshold \u2019 . When a gradient computation results in a NaN , reduce the loss_scale by a factor of \u2018 scale \u2019 ( =2 ) If there is another NaN within the \u2018 interval \u2019 , the loss scale is further reduced by a factor of 2 . If there is no NaN encountered for \u2018 interval \u2019 ( =2000 ) iterations , the \u2018 loss_scale \u2019 value is increased by a factor of 2 When the gradients have lot of outliers , we would see more of these spurious NaNs and the \u2018 loss_scale \u2019 value quickly drops . One or more of the following solutions can be applied to solve this . 1.Reduce \u2018 interval \u2019 to a smaller iteration count ( =200 ) so the \u2018 loss_scale \u2019 value can recover to quickly from a previous drop . 2.Ignore a few NaNs unless they appear in consecutive iterations . This will address the over-correction ( similar to setting a lower threshold ) 3 . A more generic solution is to derive layer-wise scaling factor which is aware of the gradient distribution at each layer [ 1 ] As per your feedback , we will update the paper with a description and/or a flow chart of this algorithm . Q2.On connection between the norm and rounding technique . As we discussed in Section 3.2 , rounding plays a significant role in FP8 training because rounding errors are quite large at this precision . It is known that round to nearest even ( RNE ) distorts the data distribution to have more even numbers than odd . As a result of this when using RNE , rounding errors grow at the rate proportional to square root of number of accumulations . ( more here : https : //en.wikipedia.org/wiki/Rounding # Floating-point_rounding ) In Figure 3c , we are showing the result of these accumulated errors on the weight distribution . The overall weight distribution is shifted towards larger numbers resulting in increasing \u201c L2_loss \u201d ( =sum of squares of the weights ) . Since l2_loss is used as a \u2018 regularization \u2019 term ( loss =cross_entropy+l2_loss ) , the loss increases as the rounding errors keep accumulating . This leads to loss of generalization , as shown in Figure 3a and 3b \u2013 the training loss keeps going down while validation loss is increasing . To avoid using l2_loss term , we tried using \u2018 drop out \u2019 method and trained without any regularization . Though the validation error improved in both these cases , there was still a significant gap in final accuracy due to ineffectiveness of these regularization methods . Then then we went back to l2 regularization \u2013 this time addressing the rounding errors in the gradients using stochastic rounding . This helped keep the accumulation of errors in check and the we achieved SOTA accuracy . Q3.The single hyper-parameter used for loss scaling indicates whether to use a \u2018 static \u2019 or a \u2018 dynamic \u2019 loss scaling method . We will add this detail to experiments section . Q3b.On the relevance of Banner et.al . [ 3 ] as an important baseline . In our case the update is not full precision . We compute weight gradients at FP8 precision and we use FP8 weight gradients and FP16 master weights for the weight update operation . In Figure 1 we are showing FP32 because the internal accumulator in ALU unit is FP32 , during weight update the weights are accumulated into FP32 accumulator and are converted to FP16 before they are written out to the master copy , we have described this in Section 3 , para 3 . In contrast Banner et.al [ 3 ] use a technique called \u2018 gradient bifurcation \u2019 where they only quantize one of the two convolutions in the backward pass . They maintain two copies of the error gradient one of which is at full precision . The full precision copy is used to compute the error gradients at FP32 precision and passed down to the previous layer . Hope that helps clarify your questions . [ 1 ] Adaptive Loss Scaling for Mixed Precision Training , Ruizhe Zhao , Brian Vogel , Tanvir Ahmed [ 2 ] Wang N , Choi J , Brand D , et al.Training deep neural networks with 8-bit floating point numbers [ 3 ] Banner R , Hubara I , Hoffer E , et al.Scalable methods for 8-bit training of neural networks ,"}, {"review_id": "HJe88xBKPr-2", "review_text": "Originality: The paper proposed a new scaling loss strategy for mixed-precision (8-bit mainly) training and verified the importance of rounding (quantization) error issue for low-precision training. Quality: The authors clearly illustrated the benefit of their proposed loss strategy and the importance of quantization error for two different tasks (image classification and NMT). The experiments are very clear and easy to follow. Clarity: The paper is clearly written with some visualizations for readers to understand the 8-bit training. Significance: 1. The enhanced loss scaling strategy is interesting but the method seems hand-tuning. Is there any automatical way or heuristic deciding way? 2. The stochastic rounding method is very intuitive. How do you choose the value of \"r\" in the equation? Is it a sensitive hyper-parameter or not? Typos: Page 7: with with roughly 200M -> with roughly 200M ", "rating": "6: Weak Accept", "reply_text": "Thank you for your helpful comments . Q1 : The enhanced loss scaling strategy is interesting but the method seems hand-tuning . Is there any automatical way or heuristic deciding way ? We believe this can be automated . We have observed that GNMT saw wider error gradient distributions which often consisted of outliers that are much larger than the mean . This is exacerbated by the additional noise induced as a result of using lower precision ( FP8 ) for error gradients . When these outliers are scaled with a large scaling factor , they overflow and cause a NaN when gradients for previous layer are computed . The current automatic loss scaling algorithm is ill-equipped to handle these transient NaNs , it over-corrects ( reduces ) the loss scale value every time it encounters an outlier , resulting in divergence . Our enhanced loss scaling strategy mitigates this by adding a 'minimum threshold ' to prevent loss scale value from becoming too small . We believe adding a few additional conditions to loss scaling algorithm will handle this case automatically . The current loss scaling algorithm works like this : Initial \u2018 loss_scale \u2019 value is set to \u2018 max_threshold \u2019 . When a gradient computation results in a NaN , reduce the loss_scale by a factor of \u2018 scale \u2019 ( =2 ) If there is another NaN within the \u2018 interval \u2019 , the loss scale is further reduced by a factor of 2 . If there is no NaN encountered for \u2018 interval \u2019 ( =2000 ) iterations , the \u2018 loss_scale \u2019 value is increased by a factor of 2 When the gradients have lot of outliers , we would see more of these spurious NaNs and the \u2018 loss_scale \u2019 value quickly drops . One or more of the following enhancements can be applied to automatic loss scaling algorithm to address this : 1 . Reduce \u2018 interval \u2019 to a smaller iteration count ( =200 ) so the \u2018 loss_scale \u2019 value can recover to quickly from a previous drop . 2.Ignore a few NaNs unless they appear in consecutive iterations . This will address the over-correction ( similar to setting a lower threshold ) 3 . A more generic solution is to derive layer-wise scaling factor which is aware of the gradient distribution at each layer [ 1 ] Q2 : The stochastic rounding method is very intuitive . How do you choose the value of `` r '' in the equation ? Is it a sensitive hyper-parameter or not ? We appreciate the positive feedback . The value of \u201c r \u201d is an 8-bit random number generated using LFSR random number generator . We also reuse these random numbers ( for about 256 times ) to save on the overheads to generate these numbers . We will fix the typos and grammatical errors you pointed out and update the paper . Hope this clarifies your questions . [ 1 ] Adaptive Loss Scaling for Mixed Precision Training , Ruizhe Zhao , Brian Vogel , Tanvir Ahmed ( https : //arxiv.org/pdf/1910.12385 )"}], "0": {"review_id": "HJe88xBKPr-0", "review_text": "In this paper, the authors propose a method to train deep neural networks using 8-bit floating point representation for weights, activations, errors, and gradients. They use enhanced loss scale, quantization and stochastic rounding techniques to balance the numerical accuracy and computational efficiency. Finally, they get a slightly better validation accuracy compared to full precision baseline. Overall, this paper focuses on engineering techniques about mixed precision training with 8-bit floating point, and state-of-the-art accuracy across multiple data sets shows the effectiveness of their work. However, there are some problems to be clarified. 1. The authors apply several techniques to improve the precision for training with 8-bit floating point, but they do not show the gain for each individual. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique? This should be clearly presented and more experimental comparison is expected. 2. The paper should present a bit more background knowledge and discussion on the adopted techniques. For instance, why the stochastic rounding method proposed in this article by adding a random value in probability can regulate quantization noise in the gradients? And why Resnet-50 demands a large scaling factor? 3. On Table 3, in comparison with Wang et al. (2018), the authors use layers with FP32 (not FP16 in Wang). Thus, it is hard to say the improvement comes from the proposed 8-bit training. This should be clarified. 4. How to set the hyper-parameters, such as scale, thresholds and so on, is not clear in the paper. There are no guidelines for readers to use these techniques. 5. The authors did not give a clear description of the implement for the enhanced loss scaling. They apply different loss scaling methods for different networks. This should be explained in detail. 6. In the experiment, for a single model, some layers are 8-bit, some layers are 32-bit and some layers are 16-bit. Is the 8-bit training only applicable for a part of the model? How do we know which layer is suitable for 8-bit training? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . We will attempt to answer your questions below . Q1.Our intention was to show that both enhanced loss scaling and stochastic rounding are essential for achieving full accuracy with FP8 training . For example , in section 3.1 , our experiments already use \u201c stochastic rounding \u201d on gradients ( essential for convergence ) to study the impact of loss scaling in isolation . Similarly , in section 3.2 when studying the impact of stochastic rounding , we employed the \u2018 best loss scaling strategy \u2019 derived from section 3.1 . Perhaps this is not clearly described in the paper . We will edit the text for clarity and upload the new version of the paper . In Figure 2a , we have compared multiple experimental results to demonstrated impact of using different loss scaling values on final accuracy of Resnet50 . Q2.On stochastic rounding : As we discussed in Section 3.2 , rounding plays a significant role for FP8 because the rounding errors are quite large at this precision . It is known that standard rounding methods ( up , down , towards zero , away from zero ) have a positive or a negative bias to the final distribution . The most popular rounding method used by floating point today is round to nearest even ( RNE ) \u2013 although this method is free of positive or a negative bias -- it distorts the data distribution to have more even numbers than odd . ( more info here : https : //en.wikipedia.org/wiki/Rounding # Floating-point_rounding ) . It is also known that rounding errors grow with longer accumulation chains ( like in Convolution and MatMul ) . For RNE method , the rounding errors grow proportional to the square root of number of accumulations . This is quite significant at extreme low precisions ( like FP8 ) where \u2018 episilon \u2019 value is large . Stochastic rounding is bias free because it uses random probability term for tie-breaking . It does not impact the overall data distribution of the tensor and the rounding errors are small and evenly distributed . This makes the accumulation of errors during long accumulation chains much less likely . > > On why Resnet-50 demands a large scaling factor ? : In general working with FP8 would require larger scaling factor because FP8 has smaller dynamic range compared to FP16 . The smallest number that can be represented by FP16 is 5.96e-8 whereas the smallest number that FP8 can represent is 1.52e-5 . This means that a larger percentage of smaller gradients fall \u2018 below \u2019 the FP8 range . Hence , we need to use a larger scaling factor to push them up into the FP8 range . Q3.We would like to clarify that we do not use FP32 in any of our training results . For Resnet-50 , all convolution and batchnorm layers use FP8 -- except the first conv and last FC layers which use FP16 ; we also use FP16 master copy of weight . This configuration identical to what is used by Wang et.al. -- hence the comparison is fair . The key difference between our implementations is that we use FP32 accumulator ( in the ALU ) while Wang et.al use a modified FP16 ( 1-6-9 format ) \u2013 as a result , they need to implement additional hardware in the ALU path to perform stochastic rounding on the accumulator to preserve accuracy . Given the complexity of building stochastic rounding hardware , their implementation will be more expensive to build . We discussed these design trade-offs in Section 1 . Q4.We employ the widely disseminated techniques that are used for FP16 mixed precision training , these are implemented in frameworks such as Tensorflow and PyTorch . Our loss scaling methods are modifications on top of these baseline methods.To answer your specific question : Scale ( =2 ) and threshold ( min=2 , max=2^14 ) values are hard-coded in in the current implementation of loss scaling algorithm . The dynamic loss scaling algorithm increments the loss scale value by a factor of \u2018 scale \u2019 every 2000 iteration intervals and reduced the loss scale by a factor \u2018 scale \u2019 in the case of an occurrence of \u2018 NaN \u2019 in the during gradient computation . For GNMT training , the enhanced loss scaling method updates the \u2018 min \u2019 threshold value according to the schedule shown in Figure 2b to prevent the loss scale becoming too small . We will add the description of the algorithm to the paper . Q5.We have described the loss scaling methods applied to each model in section 3.1 For Resnet50 , we use constant loss scaling of 10K , this is derived empirically through experimentation which are detailed in section 3.1 . For GNMT and Transformer , we use dynamic loss scaling implemented by Tensorflow . Q6.For now , the process of selecting which layers to run at FP8 requires human expertise and intervention . But we expect the future frameworks to automate this process of selecting multiple precision options to maximize performance . Recent work on use of AutoML [ 1 ] for mixed-precision quantization is also promising research direction . [ 1 ] HAQ : Hardware-Aware Automated Quantization with Mixed Precision , Kuan Wang et.al. , CVPR 2019 ."}, "1": {"review_id": "HJe88xBKPr-1", "review_text": "This paper is about training deep models with 8-bit floating point numbers. The authors use an enhanced loss scaling method and stochastic rounding method to stabilize training. They do experiments on image classification and NLP tasks. The paper is clearly written. However, I don\u2019t think this paper passes the bar of ICLR. This paper lacks innovation and insightful analysis. 1.Sec. 3.1 proposes enhanced loss scaling. Loss scaling is a heuristic to train low-precision neural networks. The authors train 8-bit GNMT with a changing scaling factor. However, this looks like some manually tuned result for GNMT only. I doubt if this generalizes to other models. Besides, there is no equation or algorithm flowchart to demonstrate their method. It\u2019s not very readable. 2.The logic of Sec. 3.2 is quite confusing. The authors first empirically show that the performance of ResNet-50 significantly drops with 8-bit training. Then they show the sum of the square of the weights in ResNet-50 is high at the beginning. With this observation, they claim it demonstrates the drawback of \u2018rounding-to-nearest-even\u2019. I cannot see the connection between the norm of weights and the rounding technique. Moreover, the stochastic rounding has already been used in 8-bit training.[1] 3.The setting in the experiment section is not stated clearly. For example, what\u2019s the hyper-parameter for loss scaling? Another question is the gradient. In Sec. 3, just above Fig. 1, the authors claim the weight update is performed in full-precision. In contrast, they claim the gradient is 8-bit in table 3. If the update is full-precision, [2] is an important baseline. Small suggestions: 1.For Fig. 6, I suggest the authors to smooth the loss curves to avoid overlap of two curves. 2.There are two \u2018with\u2019s in the last paragraph of page 7. Reference: [1]Wang N, Choi J, Brand D, et al. Training deep neural networks with 8-bit floating point numbers[C]//Advances in neural information processing systems. 2018: 7675-7684. [2]Banner R, Hubara I, Hoffer E, et al. Scalable methods for 8-bit training of neural networks[C]//Advances in Neural Information Processing Systems. 2018: 5145-5153.", "rating": "1: Reject", "reply_text": "Thank you for your detailed review and comments . Q1.Please note that only GNMT required the hand tuned loss scaling schedule . We believe this method can be automated for GNMT as well . We have observed that GNMT saw wider error gradient distributions which often consisted of outliers that are much larger than the mean . When these outliers are scaled with a large scaling factor , they overflow and cause a NaN when gradients for previous layer are computed . The current automatic loss scaling algorithm is ill-equipped to handle these transient NaNs , it over-corrects ( reduces ) the loss scale value every time it encounters an outlier , resulting in divergence . Our enhanced loss scaling strategy mitigates this by adding a lower threshold to prevent loss scale value from becoming too small . We believe adding a few additional conditions to loss scaling algorithm will handle this case automatically . The current loss scaling algorithm works like this : Initial \u2018 loss_scale \u2019 value is set to \u2018 max_threshold \u2019 . When a gradient computation results in a NaN , reduce the loss_scale by a factor of \u2018 scale \u2019 ( =2 ) If there is another NaN within the \u2018 interval \u2019 , the loss scale is further reduced by a factor of 2 . If there is no NaN encountered for \u2018 interval \u2019 ( =2000 ) iterations , the \u2018 loss_scale \u2019 value is increased by a factor of 2 When the gradients have lot of outliers , we would see more of these spurious NaNs and the \u2018 loss_scale \u2019 value quickly drops . One or more of the following solutions can be applied to solve this . 1.Reduce \u2018 interval \u2019 to a smaller iteration count ( =200 ) so the \u2018 loss_scale \u2019 value can recover to quickly from a previous drop . 2.Ignore a few NaNs unless they appear in consecutive iterations . This will address the over-correction ( similar to setting a lower threshold ) 3 . A more generic solution is to derive layer-wise scaling factor which is aware of the gradient distribution at each layer [ 1 ] As per your feedback , we will update the paper with a description and/or a flow chart of this algorithm . Q2.On connection between the norm and rounding technique . As we discussed in Section 3.2 , rounding plays a significant role in FP8 training because rounding errors are quite large at this precision . It is known that round to nearest even ( RNE ) distorts the data distribution to have more even numbers than odd . As a result of this when using RNE , rounding errors grow at the rate proportional to square root of number of accumulations . ( more here : https : //en.wikipedia.org/wiki/Rounding # Floating-point_rounding ) In Figure 3c , we are showing the result of these accumulated errors on the weight distribution . The overall weight distribution is shifted towards larger numbers resulting in increasing \u201c L2_loss \u201d ( =sum of squares of the weights ) . Since l2_loss is used as a \u2018 regularization \u2019 term ( loss =cross_entropy+l2_loss ) , the loss increases as the rounding errors keep accumulating . This leads to loss of generalization , as shown in Figure 3a and 3b \u2013 the training loss keeps going down while validation loss is increasing . To avoid using l2_loss term , we tried using \u2018 drop out \u2019 method and trained without any regularization . Though the validation error improved in both these cases , there was still a significant gap in final accuracy due to ineffectiveness of these regularization methods . Then then we went back to l2 regularization \u2013 this time addressing the rounding errors in the gradients using stochastic rounding . This helped keep the accumulation of errors in check and the we achieved SOTA accuracy . Q3.The single hyper-parameter used for loss scaling indicates whether to use a \u2018 static \u2019 or a \u2018 dynamic \u2019 loss scaling method . We will add this detail to experiments section . Q3b.On the relevance of Banner et.al . [ 3 ] as an important baseline . In our case the update is not full precision . We compute weight gradients at FP8 precision and we use FP8 weight gradients and FP16 master weights for the weight update operation . In Figure 1 we are showing FP32 because the internal accumulator in ALU unit is FP32 , during weight update the weights are accumulated into FP32 accumulator and are converted to FP16 before they are written out to the master copy , we have described this in Section 3 , para 3 . In contrast Banner et.al [ 3 ] use a technique called \u2018 gradient bifurcation \u2019 where they only quantize one of the two convolutions in the backward pass . They maintain two copies of the error gradient one of which is at full precision . The full precision copy is used to compute the error gradients at FP32 precision and passed down to the previous layer . Hope that helps clarify your questions . [ 1 ] Adaptive Loss Scaling for Mixed Precision Training , Ruizhe Zhao , Brian Vogel , Tanvir Ahmed [ 2 ] Wang N , Choi J , Brand D , et al.Training deep neural networks with 8-bit floating point numbers [ 3 ] Banner R , Hubara I , Hoffer E , et al.Scalable methods for 8-bit training of neural networks ,"}, "2": {"review_id": "HJe88xBKPr-2", "review_text": "Originality: The paper proposed a new scaling loss strategy for mixed-precision (8-bit mainly) training and verified the importance of rounding (quantization) error issue for low-precision training. Quality: The authors clearly illustrated the benefit of their proposed loss strategy and the importance of quantization error for two different tasks (image classification and NMT). The experiments are very clear and easy to follow. Clarity: The paper is clearly written with some visualizations for readers to understand the 8-bit training. Significance: 1. The enhanced loss scaling strategy is interesting but the method seems hand-tuning. Is there any automatical way or heuristic deciding way? 2. The stochastic rounding method is very intuitive. How do you choose the value of \"r\" in the equation? Is it a sensitive hyper-parameter or not? Typos: Page 7: with with roughly 200M -> with roughly 200M ", "rating": "6: Weak Accept", "reply_text": "Thank you for your helpful comments . Q1 : The enhanced loss scaling strategy is interesting but the method seems hand-tuning . Is there any automatical way or heuristic deciding way ? We believe this can be automated . We have observed that GNMT saw wider error gradient distributions which often consisted of outliers that are much larger than the mean . This is exacerbated by the additional noise induced as a result of using lower precision ( FP8 ) for error gradients . When these outliers are scaled with a large scaling factor , they overflow and cause a NaN when gradients for previous layer are computed . The current automatic loss scaling algorithm is ill-equipped to handle these transient NaNs , it over-corrects ( reduces ) the loss scale value every time it encounters an outlier , resulting in divergence . Our enhanced loss scaling strategy mitigates this by adding a 'minimum threshold ' to prevent loss scale value from becoming too small . We believe adding a few additional conditions to loss scaling algorithm will handle this case automatically . The current loss scaling algorithm works like this : Initial \u2018 loss_scale \u2019 value is set to \u2018 max_threshold \u2019 . When a gradient computation results in a NaN , reduce the loss_scale by a factor of \u2018 scale \u2019 ( =2 ) If there is another NaN within the \u2018 interval \u2019 , the loss scale is further reduced by a factor of 2 . If there is no NaN encountered for \u2018 interval \u2019 ( =2000 ) iterations , the \u2018 loss_scale \u2019 value is increased by a factor of 2 When the gradients have lot of outliers , we would see more of these spurious NaNs and the \u2018 loss_scale \u2019 value quickly drops . One or more of the following enhancements can be applied to automatic loss scaling algorithm to address this : 1 . Reduce \u2018 interval \u2019 to a smaller iteration count ( =200 ) so the \u2018 loss_scale \u2019 value can recover to quickly from a previous drop . 2.Ignore a few NaNs unless they appear in consecutive iterations . This will address the over-correction ( similar to setting a lower threshold ) 3 . A more generic solution is to derive layer-wise scaling factor which is aware of the gradient distribution at each layer [ 1 ] Q2 : The stochastic rounding method is very intuitive . How do you choose the value of `` r '' in the equation ? Is it a sensitive hyper-parameter or not ? We appreciate the positive feedback . The value of \u201c r \u201d is an 8-bit random number generated using LFSR random number generator . We also reuse these random numbers ( for about 256 times ) to save on the overheads to generate these numbers . We will fix the typos and grammatical errors you pointed out and update the paper . Hope this clarifies your questions . [ 1 ] Adaptive Loss Scaling for Mixed Precision Training , Ruizhe Zhao , Brian Vogel , Tanvir Ahmed ( https : //arxiv.org/pdf/1910.12385 )"}}