{"year": "2019", "forum": "ByetGn0cYX", "title": "Probabilistic Planning with Sequential Monte Carlo methods", "decision": "Accept (Poster)", "meta_review": "This paper presents a new approach for posing control as inference that leverages Sequential Monte Carlo and Bayesian smoothing. There is significant interest from the reviewers into this method, and also an active discussion about this paper, particularly with respect to the optimism bias issue. The paper is borderline and the authors are encouraged to address the desired clarifications and changes from the reviewers. \n", "reviews": [{"review_id": "ByetGn0cYX-0", "review_text": "The authors formulate planning as sampling from an intractable distribution motivated by control-as-inference, propose to approximately sample from the distribution using a learned model of the environment and SMC, then evaluate their approach on 3 Mujoco tasks. They claim that their method compares favorably to model-free SAC and to CEM and random shooting (RS) planning with model-based RL. This is an interesting idea and an important problem, but there appear to be several inconsistencies in the proposed algorithm and the experimental results do not provide compelling support for the algorithm. In particular, Levine 2018 explains that with stochastic transitions, computing the posterior leads to overly optimistic behavior because the transition dynamics are not enforced, whereas the variational bound explicitly enforces that. Is that an issue here? The value function estimated in SAC is V^\\pi the value function of the current policy. The value function needed in Sec 3.2 is a different value function. Can the authors clarify on this discrepancy? The SMC procedure in Alg 1 appears to be incorrect. It multiplies the weights by exp(V_{t+1}) before resampling. This needs to be accounted for by setting the weights to exp(-V_{t+1}) instead of uniform. See for example auxiliary particle filters. The experimental section could be significantly improved by addressing the following points: * How was the planning horizon h chosen? Is the method sensitive to this choice? What is the model accuracy? * Does CEM use a value function? If not, it seems like a reasonable baseline to consider CEM w/ a value function to summarize the values beyond the planning horizon. This will evaluate whether SMC or including the value function is important. * Comparing to state-of-the-art model-based RL (e.g., one of Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018). * How were the task # of steps chosen? They seem arbitrary. What is the performance at 1million and 5million steps? * Was SAC retuned for this small number of samples/steps? * Clarify where the error bars come from in Fig 5.2 in the caption. At the moment, SMCP is within the error bars of a baseline method. Comments: In the abstract, the authors claim that the major challenges in planning are: 1) model compounding errors in roll-outs and 2) the exponential search space. Their method only attempts to address 2), is that correct? If so, can the authors state that explicitly. Recent papers (Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018, Ha and Schmidhuber 2018) all show promising model-based results on continuous state/action tasks. These should be mentioned in the intro. The connection between Gu et al.'s work on SMC and SAC was unclear in the intro, can the authors clarify? For consistency, ensure that sums go to T instead of \\infty. I found the discussion of SAC at the end of Sec 2.1 confusing. As I understand SAC, it does try to approximate the gradient of the variational bound directly. Can the authors clarify what they mean? At the end of Sec 2.2, the authors claim that the tackle the particle degeneracy issue (a potentially serious issue) by \"selecting the temperature of the resampling distribution to not be too low.\" I could not find further discussion of this anywhere in the paper or appendix. Sec 3.2, mentions an action prior for the first time. Where does this come from? Sec 3.3 derives updates assuming a perfect model, but we learn a model. What are the implications of this? Please ensure the line #'s and the algorithm line #'s match. Model learning is not described in the main text though it is a key component of the algorithm. The appendix lacks details (e.g., what is the distribution used to model the next state?) and contradicts itself (e.g., one place says 3 layers and another says 2 layers). In Sec 4.1, a major difference between MCTS and SMC is that MCTS runs serially, whereas SMC runs in parallel. This should be noted and then it's unclear whether SMC-Planning should really be thought of as the maximum entropy tree search equivalent of MCTS. In Sec 4.1, the authors claim that Alpha-Go and SMCP learn proposals in similar ways. However, SMCP minimizes the KL in the reverse direction (from stated in the text). This is an important distinction. In Sec 4.3, the authors note that Gu et al. learn the proposal with the reverse KL from SMCP. VSMC (Le et al. 2018, Naesseth et al. 2017, Maddison et al. 2017) is the analogous work to Gu et al. that learn the proposal using the same KL direction as SMCP. The authors should consider citing this work as it directly relates to their algorithm. In Sec 4.3, the authors claim that their direction of minimizing KL is more appropriate for exploration. Gu et al. suggest the opposite in their work. Can the author's justify their claim? In Sec 5.1, the authors provide an example of SMCP learning a multimodal policy. This is interesting, but can the authors explain when this will be helpful? ==== 11/26 At this time, the authors have not responded to reviews. I have read the other reviews. Given the outstanding issues, I do not recommend acceptance. 12/7 After reading the author's response, I have increased my score. However, baselines that establish the claim that SMC improves planning which leads to improved control are missing (such as CEM + value function). Also, targeting the posterior introduces an optimism bias that is not dealt with or discussed.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for this very thorough review . We believe that these comments are making the paper clearer and stronger . 1 ) \u201c [ ... ] the experimental results do not provide compelling support for the algorithm. \u201c We agree the initial results were not compelling in that regard . We have updated the results and we now believe the performance of our planning method appears clearly . We used 20 seeds and also added a significance test following guidelines by Colas et al 2018 in Appendix A.7 . We furthermore added more experimental details in the Appendix A.5 and A.8 2 ) \u201c Levine 2018 explains that with stochastic transitions , computing the posterior leads to overly optimistic behavior because the transition dynamics are not enforced , whereas the variational bound explicitly enforces that . Is that an issue here ? \u201d Our model is trained by maximum likelihood as in Chua et al.2018 only from data , separately from the policy and planning . Thus , the policy has no control over the system dynamics , hence the model is not encouraged to yield over-optimistic transitions . We have added details about the model training procedure in the experiments section and have update our pseudo-code for clarity . 3 ) \u201c The value function estimated in SAC is V^\\pi the value function of the current policy . The value function needed in Sec 3.2 is a different value function . Can the authors clarify on this discrepancy ? \u201d Indeed . However as we do not have access to the optimal value function , we use the current value function of SAC as a proxy . As the SAC-policy will converge to a policy closer to optimality , so will its value function . Therefore we think this is a sensible practical choice , and this is similar to what is done in actor-critic methods for instance . 4 ) \u201c The SMC procedure in Alg . 1 appears to be incorrect . It multiplies the weights by exp ( V_ { t+1 } ) before resampling . This needs to be accounted for by setting the weights to exp ( -V_ { t+1 } ) instead of uniform . See for example auxiliary particle filters. \u201d Yes , there was indeed an issue with the weight update that we have now fixed and it does indeed align with your intuition . To be precise , we believe the weight update should be done pour multiplying with the previous weight by exp ( r - log pi + V \u2019 -log E_ { s_t | a_t-1 s_t-1 } exp V ( s_t ) ) . We thought ( wrongly ) that the log-expectation-exp was equal to the normalization constant when normalizing the weights , thus redundant . However this normalization constant takes its expectation under the states the particles are in at time t rather than the transition dynamics as it should be done . By fixing the update , we now believe we have the right formula , and this allows us to have an unweighted empirical estimate of the posterior . This is indeed similar in spirit to the auxiliary particle filter , we thank you for the reference and for pointing out the issue , it helped us derive the right update formula . 5 ) \u201c How was the planning horizon h chosen ? Is the method sensitive to this choice ? What is the model accuracy ? \u201d + \u201c At the end of Sec 2.2 , the authors claim that the tackle the particle degeneracy issue ( a potentially serious issue ) by `` selecting the temperature of the resampling distribution to not be too low . '' I could not find further discussion of this anywhere in the paper or appendix. \u201d We did not do any extensive hyperparameter search in the beginning . We tried mostly temperatures in the range [ 1-10 ] . We checked the ESS while training to make sure we did not have any weight degeneracy issue . See A.8 for a plot of the ESS during training . We have tried horizons from 5 to 50 , and while the performance is pretty stable across this range of horizons , h~20 seems a good value to work with for Walker2d and HalfCheetah . Hopper was more challenging and we found out that typically shorter horizons worked marginally better . The path degeneracy is indeed a very serious issue , and we definitely suffer from it even when tuning the temperature . While some modern smoothing algorithms like Particle Gibbs with Ancestor Sampling can alleviate it , our goal in this work is to introduce a new simple and motivated way of doing planning rather than obtaining the best performance possible ."}, {"review_id": "ByetGn0cYX-1", "review_text": "This paper proposes a sequential Monte Carlo Planning algorithm that depicts planning as an inference problem solved by SMC. The problem is interesting and the paper has a nice description of the related work. In terms of the connection between the the problem and Bayesian filtering as well as smoothing, the paper has novelty there. But it is unclear to me how the algorithm proposed is applicable in complex continuous tasks as claimed. In the introduction, the authors wrote that \"We design a new algorithm, Sequential Monte Carlo Planning (SMCP), by leveraging modern methods in Sequential Monte Carlo (SMC), Bayesian smoothing, and control as inference\". From my understanding, the SMC algorithm adopted is the bootstrap particle which is the simplest and earliest SMC algorithm adopted. The Bayesian smoothing algorithm described is also standard. I do not see the modern parts of these algorithms. The experiment section reports the return, but it is unclear to me how the SMC algorithm in this case. For example, what is the effective sample size (ESS) in these settings? The experiment described seems to be a 2-dimensional set up. How does the algorithm perform with a high-dimensional planning problem? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the correction and added additional experimental details to better understand the behaviour of the method . 1 ) \u201c [ ... ] the SMC algorithm adopted [ ... ] is the simplest and earliest SMC algorithm adopted. [ ... ] . I do not see the modern parts of these algorithms. \u201d This is fair point , we corrected the sentence . 2 ) \u201c The experiment section reports the return , but it is unclear to me how the SMC algorithm in this case . For example , what is the effective sample size ( ESS ) in these settings ? \u201d In this case , the SMC algorithm now clearly outperforms the SAC baseline as you can see in the updated version of the plot . Furthermore , we have added a new section in the Appendix A.8 describing the evolution of the ESS during training . While it is not very high , is is usually around 15 % of the sample size , which we believe is reasonable so that we do not suffer heavily from weight degeneracy . 3 ) \u201c But it is unclear to me how the algorithm proposed is applicable in complex continuous tasks as claimed. \u201d And \u201c The experiment described seems to be a 2-dimensional set up . How does the algorithm perform with a high-dimensional planning problem ? \u201d Yes , the 2d experiment is merely illustrative , the complex continuous tasks mentioned are illustrated with the experiments on Mujoco in subsection 2 of the experiments . In section 5.2 , we have updated our performance results on the 3 classic Mujoco environments . Their respective state/action dimensions are : Walker2d-v2 , state ( 17 , ) , action ( 6 , ) Hopper-v2 , state ( 11 , ) , action ( 3 , ) HalfCheetah-v2 , state ( 17 , ) , action ( 6 , ) Still , we have removed the mention of \u201c high dimensional \u201d as control tasks ( ie Mujoco ) , while complex , are maybe not what the statistical community would call \u201c high dimensional \u201d . Also , vanilla particle filters are known to suffer from the curse of dimensionality , especially if the proposal is poor . A solution we leave to future work would be to do the planning in latent space , in that case our method could scale even with very high dimensional inputs ."}, {"review_id": "ByetGn0cYX-2", "review_text": "Sequential Monte Carlo (SMC) has since its inception some 25 years ago proved to be a powerful and generally applicable tool. The authors of this paper continue this development in a very interesting and natural way by showing how SMC can be used to solve challenging planning problems. This is a enabled by reformulating the planning problem as an inference problem via the recent trend referred to as \"control as inference\". While there is unfortunately no real world experiments, the simulations clearly illustrate the potential of the approach. While the idea of viewing control as inference is far from new the idea of using SMC in this context is clearly novel as far as I can see. Well, there has been some work along the same general topic before, see e.g. Andrieu, C., Doucet, A., Singh, S.S., and Tadic, V.B. (2004). Particle methods for change detection, system identification, and contol. Proceedings of the IEEE, 92(3), 423\u2013438. However, the particular construction proposed in this paper is refreshingly novel and interesting. Hence, I view the specific idea put fourth in this paper as highly novel. The general idea of viewing control as inference goes far back and there are very nice dual relationships between LQG and the Kalman filter established and exploited long time ago. The authors interprets \"control as inference\" as viewing the planning problem as a simulation exercise where we aim to approximate the distribution of optimal future trajectories. A bit more specifically, the SMC-based planning proposed in the paper stochastically explores the most promising trajectories in the tree and randomly removes (via the resampling operation) the less promising branches. Importantly there are convergence guarantees via the use of SMC. The idea is significant in that it opens up for the use of the by now strong SMC body of methods and analysis when it comes to challenging and intractable planning problems. I foresee many interesting developments to follow in the direction layed out by this paper. When it comes to your SMC algorithm you will suffer from path degeneracy (as all SMC algorithms does, see e.g. Figure 1 in https://arxiv.org/pdf/1307.3180.pdf) and if h is large I think this can be a problem for you. However, this can easily be fixed via backward simulation. For an overview of backward simulation see Lindsten, F. and Schon, T. \"Backward simulation methods for Monte Carlo statistical inference\". Foundations and Trends in Machine Learning, 6(1):1-143, 2013. I am positive to this paper (clearly reveled by my score as well), but there are of course a few issues as well: 1. There are no theoretical results on the properties of the proposed approach. However, given the large body of literature when it comes to the analysis of SMC methods I would expect that you can provide some results via the nice bridge that you have identified. 2. Would this be possible to implement in a real-world setting with real-time requirements? 3. A very detailed question when it comes to Figure 5.2 (right-most plot), why is the performance of your method significantly degraded towards the end? It does recover indeed, but I still find this huge dip quite surprising. Minor details: * The initial references when it comes to SMC are wrong. The first papers are: N.J. Gordon, D. Salmond and A.F.M. Smith, Novel approach to nonlinear/non-Gaussian Bayesian state estimation, IEE Proc. F, 1993 L. Stewart, P. McCarty, The use of Bayesian Belief Networks to fuse continuous and discrete information for target recognition and discrete information for target recognition, tracking, and situation assessment, in Proc. SPIE Signal Processing, Sensor Fusion and Target Recognition,, vol. 1699, pp. 177-185, 1992. G. Kitagawa, Monte Carlo filter and smoother for non-Gaussian nonlinear state-space models, JCGS, 1996 * When it comes to the topic of learning a good proposal for SMC with the use of variational inference the authors provide a reference to Gu et al. (2015) which is indeed interesting and relevant in this respect. However, on this hot and interesting topic there has recently been several related papers published and I would like to mention: C. A. Naesseth, S. W. Linderman, R. Ranganath, D. M. Blei, Variational Sequential Monte Carlo. Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, Lanzarote, Spain, April 2018. C. J. Maddison, D. Lawson, G. Tucker, N. Heess, M. Norouzi, A. Mnih, A. Doucet, and Y. Whye Teh. Filtering variational objectives. In Advances in Neural Information Processing Systems, 2017. T. A. Le, M. Igl, T. Jin, T. Rainforth, and F. Wood. AutoEncoding Sequential Monte Carlo. arXiv:1705.10306, May 2017. I would like to end by saying that I really like your idea and the way in which you have developed it. I have a feeling that this will inspire quite a lot of work in this direction.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We would like to thank the reviewer for the encouraging comments and important references . \u201c When it comes to your SMC algorithm you will suffer from path degeneracy . [ ... ] However , this can easily be fixed via backward simulation [ ... ] \u201d Yes , we thank you for the suggestion . Particle Gibbs with Ancestral Sampling had been also brought to our attention to tackle this issue , but we choose to keep it simple in this work to focus more on introducing the idea rather than on getting the best results . 1 ) \u201c There are no theoretical results on the properties of the proposed approach . However , given the large body of literature when it comes to the analysis of SMC methods I would expect that you can provide some results via the nice bridge that you have identified. \u201d We believe our method is grounded when p_model = p_env and we have access to the optimal value function . However in most RL settings , both these assumptions are violated and it lessens the impact of the analysis . A very interesting theoretical analysis we wish to make is to look if we can still provide some guarantees when the model and value function are approximately optimal , but a full theoretical study is still upcoming and out of scope of this paper . 2 ) \u201c Would this be possible to implement in a real-world setting with real-time requirements ? \u201d We think it is possible if we replan every few steps instead of every step and keep a reasonable number of particles . Several methods bringing SMC methods to real-time systems exist . For instance , for embedded systems with real-time constraints , a FPGA implementation of SMC has been proposed ( Ndeved et al , 2014 , https : //www.sciencedirect.com/science/article/pii/S1474667016429812 ) . We also believe that additionally to a good search algorithm , we need to learn good representations ( eg if the input is an image ) and plan in the latent space . 3 ) \u201c A very detailed question when it comes to Figure 5.2 ( right-most plot ) , why is the performance of your method significantly degraded towards the end ? It does recover indeed , but I still find this huge dip quite surprising. \u201d Indeed . We had more time to investigate this during the review period and we realized that some of our jobs were killed around step 40k . We have since rerun all our experiments and closely monitored that no such thing happened again . We are now confident our updated results are much stronger and show with high confidence the real performance of our method ."}], "0": {"review_id": "ByetGn0cYX-0", "review_text": "The authors formulate planning as sampling from an intractable distribution motivated by control-as-inference, propose to approximately sample from the distribution using a learned model of the environment and SMC, then evaluate their approach on 3 Mujoco tasks. They claim that their method compares favorably to model-free SAC and to CEM and random shooting (RS) planning with model-based RL. This is an interesting idea and an important problem, but there appear to be several inconsistencies in the proposed algorithm and the experimental results do not provide compelling support for the algorithm. In particular, Levine 2018 explains that with stochastic transitions, computing the posterior leads to overly optimistic behavior because the transition dynamics are not enforced, whereas the variational bound explicitly enforces that. Is that an issue here? The value function estimated in SAC is V^\\pi the value function of the current policy. The value function needed in Sec 3.2 is a different value function. Can the authors clarify on this discrepancy? The SMC procedure in Alg 1 appears to be incorrect. It multiplies the weights by exp(V_{t+1}) before resampling. This needs to be accounted for by setting the weights to exp(-V_{t+1}) instead of uniform. See for example auxiliary particle filters. The experimental section could be significantly improved by addressing the following points: * How was the planning horizon h chosen? Is the method sensitive to this choice? What is the model accuracy? * Does CEM use a value function? If not, it seems like a reasonable baseline to consider CEM w/ a value function to summarize the values beyond the planning horizon. This will evaluate whether SMC or including the value function is important. * Comparing to state-of-the-art model-based RL (e.g., one of Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018). * How were the task # of steps chosen? They seem arbitrary. What is the performance at 1million and 5million steps? * Was SAC retuned for this small number of samples/steps? * Clarify where the error bars come from in Fig 5.2 in the caption. At the moment, SMCP is within the error bars of a baseline method. Comments: In the abstract, the authors claim that the major challenges in planning are: 1) model compounding errors in roll-outs and 2) the exponential search space. Their method only attempts to address 2), is that correct? If so, can the authors state that explicitly. Recent papers (Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018, Ha and Schmidhuber 2018) all show promising model-based results on continuous state/action tasks. These should be mentioned in the intro. The connection between Gu et al.'s work on SMC and SAC was unclear in the intro, can the authors clarify? For consistency, ensure that sums go to T instead of \\infty. I found the discussion of SAC at the end of Sec 2.1 confusing. As I understand SAC, it does try to approximate the gradient of the variational bound directly. Can the authors clarify what they mean? At the end of Sec 2.2, the authors claim that the tackle the particle degeneracy issue (a potentially serious issue) by \"selecting the temperature of the resampling distribution to not be too low.\" I could not find further discussion of this anywhere in the paper or appendix. Sec 3.2, mentions an action prior for the first time. Where does this come from? Sec 3.3 derives updates assuming a perfect model, but we learn a model. What are the implications of this? Please ensure the line #'s and the algorithm line #'s match. Model learning is not described in the main text though it is a key component of the algorithm. The appendix lacks details (e.g., what is the distribution used to model the next state?) and contradicts itself (e.g., one place says 3 layers and another says 2 layers). In Sec 4.1, a major difference between MCTS and SMC is that MCTS runs serially, whereas SMC runs in parallel. This should be noted and then it's unclear whether SMC-Planning should really be thought of as the maximum entropy tree search equivalent of MCTS. In Sec 4.1, the authors claim that Alpha-Go and SMCP learn proposals in similar ways. However, SMCP minimizes the KL in the reverse direction (from stated in the text). This is an important distinction. In Sec 4.3, the authors note that Gu et al. learn the proposal with the reverse KL from SMCP. VSMC (Le et al. 2018, Naesseth et al. 2017, Maddison et al. 2017) is the analogous work to Gu et al. that learn the proposal using the same KL direction as SMCP. The authors should consider citing this work as it directly relates to their algorithm. In Sec 4.3, the authors claim that their direction of minimizing KL is more appropriate for exploration. Gu et al. suggest the opposite in their work. Can the author's justify their claim? In Sec 5.1, the authors provide an example of SMCP learning a multimodal policy. This is interesting, but can the authors explain when this will be helpful? ==== 11/26 At this time, the authors have not responded to reviews. I have read the other reviews. Given the outstanding issues, I do not recommend acceptance. 12/7 After reading the author's response, I have increased my score. However, baselines that establish the claim that SMC improves planning which leads to improved control are missing (such as CEM + value function). Also, targeting the posterior introduces an optimism bias that is not dealt with or discussed.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for this very thorough review . We believe that these comments are making the paper clearer and stronger . 1 ) \u201c [ ... ] the experimental results do not provide compelling support for the algorithm. \u201c We agree the initial results were not compelling in that regard . We have updated the results and we now believe the performance of our planning method appears clearly . We used 20 seeds and also added a significance test following guidelines by Colas et al 2018 in Appendix A.7 . We furthermore added more experimental details in the Appendix A.5 and A.8 2 ) \u201c Levine 2018 explains that with stochastic transitions , computing the posterior leads to overly optimistic behavior because the transition dynamics are not enforced , whereas the variational bound explicitly enforces that . Is that an issue here ? \u201d Our model is trained by maximum likelihood as in Chua et al.2018 only from data , separately from the policy and planning . Thus , the policy has no control over the system dynamics , hence the model is not encouraged to yield over-optimistic transitions . We have added details about the model training procedure in the experiments section and have update our pseudo-code for clarity . 3 ) \u201c The value function estimated in SAC is V^\\pi the value function of the current policy . The value function needed in Sec 3.2 is a different value function . Can the authors clarify on this discrepancy ? \u201d Indeed . However as we do not have access to the optimal value function , we use the current value function of SAC as a proxy . As the SAC-policy will converge to a policy closer to optimality , so will its value function . Therefore we think this is a sensible practical choice , and this is similar to what is done in actor-critic methods for instance . 4 ) \u201c The SMC procedure in Alg . 1 appears to be incorrect . It multiplies the weights by exp ( V_ { t+1 } ) before resampling . This needs to be accounted for by setting the weights to exp ( -V_ { t+1 } ) instead of uniform . See for example auxiliary particle filters. \u201d Yes , there was indeed an issue with the weight update that we have now fixed and it does indeed align with your intuition . To be precise , we believe the weight update should be done pour multiplying with the previous weight by exp ( r - log pi + V \u2019 -log E_ { s_t | a_t-1 s_t-1 } exp V ( s_t ) ) . We thought ( wrongly ) that the log-expectation-exp was equal to the normalization constant when normalizing the weights , thus redundant . However this normalization constant takes its expectation under the states the particles are in at time t rather than the transition dynamics as it should be done . By fixing the update , we now believe we have the right formula , and this allows us to have an unweighted empirical estimate of the posterior . This is indeed similar in spirit to the auxiliary particle filter , we thank you for the reference and for pointing out the issue , it helped us derive the right update formula . 5 ) \u201c How was the planning horizon h chosen ? Is the method sensitive to this choice ? What is the model accuracy ? \u201d + \u201c At the end of Sec 2.2 , the authors claim that the tackle the particle degeneracy issue ( a potentially serious issue ) by `` selecting the temperature of the resampling distribution to not be too low . '' I could not find further discussion of this anywhere in the paper or appendix. \u201d We did not do any extensive hyperparameter search in the beginning . We tried mostly temperatures in the range [ 1-10 ] . We checked the ESS while training to make sure we did not have any weight degeneracy issue . See A.8 for a plot of the ESS during training . We have tried horizons from 5 to 50 , and while the performance is pretty stable across this range of horizons , h~20 seems a good value to work with for Walker2d and HalfCheetah . Hopper was more challenging and we found out that typically shorter horizons worked marginally better . The path degeneracy is indeed a very serious issue , and we definitely suffer from it even when tuning the temperature . While some modern smoothing algorithms like Particle Gibbs with Ancestor Sampling can alleviate it , our goal in this work is to introduce a new simple and motivated way of doing planning rather than obtaining the best performance possible ."}, "1": {"review_id": "ByetGn0cYX-1", "review_text": "This paper proposes a sequential Monte Carlo Planning algorithm that depicts planning as an inference problem solved by SMC. The problem is interesting and the paper has a nice description of the related work. In terms of the connection between the the problem and Bayesian filtering as well as smoothing, the paper has novelty there. But it is unclear to me how the algorithm proposed is applicable in complex continuous tasks as claimed. In the introduction, the authors wrote that \"We design a new algorithm, Sequential Monte Carlo Planning (SMCP), by leveraging modern methods in Sequential Monte Carlo (SMC), Bayesian smoothing, and control as inference\". From my understanding, the SMC algorithm adopted is the bootstrap particle which is the simplest and earliest SMC algorithm adopted. The Bayesian smoothing algorithm described is also standard. I do not see the modern parts of these algorithms. The experiment section reports the return, but it is unclear to me how the SMC algorithm in this case. For example, what is the effective sample size (ESS) in these settings? The experiment described seems to be a 2-dimensional set up. How does the algorithm perform with a high-dimensional planning problem? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the correction and added additional experimental details to better understand the behaviour of the method . 1 ) \u201c [ ... ] the SMC algorithm adopted [ ... ] is the simplest and earliest SMC algorithm adopted. [ ... ] . I do not see the modern parts of these algorithms. \u201d This is fair point , we corrected the sentence . 2 ) \u201c The experiment section reports the return , but it is unclear to me how the SMC algorithm in this case . For example , what is the effective sample size ( ESS ) in these settings ? \u201d In this case , the SMC algorithm now clearly outperforms the SAC baseline as you can see in the updated version of the plot . Furthermore , we have added a new section in the Appendix A.8 describing the evolution of the ESS during training . While it is not very high , is is usually around 15 % of the sample size , which we believe is reasonable so that we do not suffer heavily from weight degeneracy . 3 ) \u201c But it is unclear to me how the algorithm proposed is applicable in complex continuous tasks as claimed. \u201d And \u201c The experiment described seems to be a 2-dimensional set up . How does the algorithm perform with a high-dimensional planning problem ? \u201d Yes , the 2d experiment is merely illustrative , the complex continuous tasks mentioned are illustrated with the experiments on Mujoco in subsection 2 of the experiments . In section 5.2 , we have updated our performance results on the 3 classic Mujoco environments . Their respective state/action dimensions are : Walker2d-v2 , state ( 17 , ) , action ( 6 , ) Hopper-v2 , state ( 11 , ) , action ( 3 , ) HalfCheetah-v2 , state ( 17 , ) , action ( 6 , ) Still , we have removed the mention of \u201c high dimensional \u201d as control tasks ( ie Mujoco ) , while complex , are maybe not what the statistical community would call \u201c high dimensional \u201d . Also , vanilla particle filters are known to suffer from the curse of dimensionality , especially if the proposal is poor . A solution we leave to future work would be to do the planning in latent space , in that case our method could scale even with very high dimensional inputs ."}, "2": {"review_id": "ByetGn0cYX-2", "review_text": "Sequential Monte Carlo (SMC) has since its inception some 25 years ago proved to be a powerful and generally applicable tool. The authors of this paper continue this development in a very interesting and natural way by showing how SMC can be used to solve challenging planning problems. This is a enabled by reformulating the planning problem as an inference problem via the recent trend referred to as \"control as inference\". While there is unfortunately no real world experiments, the simulations clearly illustrate the potential of the approach. While the idea of viewing control as inference is far from new the idea of using SMC in this context is clearly novel as far as I can see. Well, there has been some work along the same general topic before, see e.g. Andrieu, C., Doucet, A., Singh, S.S., and Tadic, V.B. (2004). Particle methods for change detection, system identification, and contol. Proceedings of the IEEE, 92(3), 423\u2013438. However, the particular construction proposed in this paper is refreshingly novel and interesting. Hence, I view the specific idea put fourth in this paper as highly novel. The general idea of viewing control as inference goes far back and there are very nice dual relationships between LQG and the Kalman filter established and exploited long time ago. The authors interprets \"control as inference\" as viewing the planning problem as a simulation exercise where we aim to approximate the distribution of optimal future trajectories. A bit more specifically, the SMC-based planning proposed in the paper stochastically explores the most promising trajectories in the tree and randomly removes (via the resampling operation) the less promising branches. Importantly there are convergence guarantees via the use of SMC. The idea is significant in that it opens up for the use of the by now strong SMC body of methods and analysis when it comes to challenging and intractable planning problems. I foresee many interesting developments to follow in the direction layed out by this paper. When it comes to your SMC algorithm you will suffer from path degeneracy (as all SMC algorithms does, see e.g. Figure 1 in https://arxiv.org/pdf/1307.3180.pdf) and if h is large I think this can be a problem for you. However, this can easily be fixed via backward simulation. For an overview of backward simulation see Lindsten, F. and Schon, T. \"Backward simulation methods for Monte Carlo statistical inference\". Foundations and Trends in Machine Learning, 6(1):1-143, 2013. I am positive to this paper (clearly reveled by my score as well), but there are of course a few issues as well: 1. There are no theoretical results on the properties of the proposed approach. However, given the large body of literature when it comes to the analysis of SMC methods I would expect that you can provide some results via the nice bridge that you have identified. 2. Would this be possible to implement in a real-world setting with real-time requirements? 3. A very detailed question when it comes to Figure 5.2 (right-most plot), why is the performance of your method significantly degraded towards the end? It does recover indeed, but I still find this huge dip quite surprising. Minor details: * The initial references when it comes to SMC are wrong. The first papers are: N.J. Gordon, D. Salmond and A.F.M. Smith, Novel approach to nonlinear/non-Gaussian Bayesian state estimation, IEE Proc. F, 1993 L. Stewart, P. McCarty, The use of Bayesian Belief Networks to fuse continuous and discrete information for target recognition and discrete information for target recognition, tracking, and situation assessment, in Proc. SPIE Signal Processing, Sensor Fusion and Target Recognition,, vol. 1699, pp. 177-185, 1992. G. Kitagawa, Monte Carlo filter and smoother for non-Gaussian nonlinear state-space models, JCGS, 1996 * When it comes to the topic of learning a good proposal for SMC with the use of variational inference the authors provide a reference to Gu et al. (2015) which is indeed interesting and relevant in this respect. However, on this hot and interesting topic there has recently been several related papers published and I would like to mention: C. A. Naesseth, S. W. Linderman, R. Ranganath, D. M. Blei, Variational Sequential Monte Carlo. Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, Lanzarote, Spain, April 2018. C. J. Maddison, D. Lawson, G. Tucker, N. Heess, M. Norouzi, A. Mnih, A. Doucet, and Y. Whye Teh. Filtering variational objectives. In Advances in Neural Information Processing Systems, 2017. T. A. Le, M. Igl, T. Jin, T. Rainforth, and F. Wood. AutoEncoding Sequential Monte Carlo. arXiv:1705.10306, May 2017. I would like to end by saying that I really like your idea and the way in which you have developed it. I have a feeling that this will inspire quite a lot of work in this direction.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We would like to thank the reviewer for the encouraging comments and important references . \u201c When it comes to your SMC algorithm you will suffer from path degeneracy . [ ... ] However , this can easily be fixed via backward simulation [ ... ] \u201d Yes , we thank you for the suggestion . Particle Gibbs with Ancestral Sampling had been also brought to our attention to tackle this issue , but we choose to keep it simple in this work to focus more on introducing the idea rather than on getting the best results . 1 ) \u201c There are no theoretical results on the properties of the proposed approach . However , given the large body of literature when it comes to the analysis of SMC methods I would expect that you can provide some results via the nice bridge that you have identified. \u201d We believe our method is grounded when p_model = p_env and we have access to the optimal value function . However in most RL settings , both these assumptions are violated and it lessens the impact of the analysis . A very interesting theoretical analysis we wish to make is to look if we can still provide some guarantees when the model and value function are approximately optimal , but a full theoretical study is still upcoming and out of scope of this paper . 2 ) \u201c Would this be possible to implement in a real-world setting with real-time requirements ? \u201d We think it is possible if we replan every few steps instead of every step and keep a reasonable number of particles . Several methods bringing SMC methods to real-time systems exist . For instance , for embedded systems with real-time constraints , a FPGA implementation of SMC has been proposed ( Ndeved et al , 2014 , https : //www.sciencedirect.com/science/article/pii/S1474667016429812 ) . We also believe that additionally to a good search algorithm , we need to learn good representations ( eg if the input is an image ) and plan in the latent space . 3 ) \u201c A very detailed question when it comes to Figure 5.2 ( right-most plot ) , why is the performance of your method significantly degraded towards the end ? It does recover indeed , but I still find this huge dip quite surprising. \u201d Indeed . We had more time to investigate this during the review period and we realized that some of our jobs were killed around step 40k . We have since rerun all our experiments and closely monitored that no such thing happened again . We are now confident our updated results are much stronger and show with high confidence the real performance of our method ."}}