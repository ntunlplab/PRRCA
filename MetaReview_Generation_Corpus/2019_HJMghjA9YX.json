{"year": "2019", "forum": "HJMghjA9YX", "title": "Model Comparison for Semantic Grouping", "decision": "Reject", "meta_review": "This paper presents a novel family of probabilistic approaches to computing the similarities between two sentences using bag-of-embeddings representations, and presents evaluations on a standard benchmark to demonstrate the effectiveness of the approach. While there seem to be no substantial disputes about the soundness of the paper in its current form, the reviewers were not convinced by the broad motivation for the approach, and did not find the empirical results compelling enough to serve as a motivation on its own. Given that, no reviewer was willing to argue that this paper makes an important enough contribution to be accepted.\n\nIt is unfortunate that one of the assigned reviewers\u2014by their own admission\u2014was not well qualified to review it and that a second reviewer did not submit a review at all, necessitating a late fill-in review (thank you, anonymous emergency reviewer!). However, the paper was considered seriously: I can attest that both of the two higher-confidence reviewers are well qualified to review work on problems and methods like these.", "reviews": [{"review_id": "HJMghjA9YX-0", "review_text": "The paper proposes a Bayesian model comparison based approach for quantifying the semantic similarity between two groups of embeddings (e.g., two sentences). In particular, it proposes to use the difference between the probability that the two groups are from the same model and the probability that they are from different models. While the approach looks interesting, I have a few concerns: -- Using the Bayesian model comparison framework seems to be an interesting idea. However, what are the advantages compared to widely used learned models (say, a learned CNN that takes as input two sentences and outputs the similarity score)? The latter can fit the ground-truth labels given by humans, while it's unclear the model comparison leads to good correlation with human judgments. Some discussion should be provided. -- The von Mises-Fisher Likelihood is a very simplified model of actual text data. Have you considered using other models? In particular, more sophisticated ones may lead to better performance. -- Different information criteria can be plugged in. Are there comparisons? -- The experiments are just too simple and incomplete to make reasonable conclusions. For example, it seems compared to SIF there is not much advantage even in the online setting. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "`` The latter can fit the ground-truth labels given by humans , while it 's unclear the model comparison leads to good correlation with human judgments . Some discussion should be provided . '' STS provides a test set in order to evaluate how the methods correlate with human scores , which we have used to benchmark our proposed models . That is , performing well on the test set suggests there 's a correlation between human judgment and the model 's prediction of similarity score . We will clarify this in the new manuscript . We will discuss the relation of our approach to the one presented in Equation ( 9 ) ( Tversky 's contrast model ) of [ 6 ] - a work that analyses what a good similarity is from a cognitive science perspective . [ 6 ] J.B. Tenenbaum , and T.L . Griffiths , 2001 . Generalization , similarity , and Bayesian inference . Behavioral and brain sciences . `` Have you considered using other models ? In particular , more sophisticated ones may lead to better performance . '' The motivation for this paper is to introduce a framework in which different probabilistic models can be assessed on the STS task . This is done such that a practitioner , through specifying the likelihood function , can encode suitable assumptions and constraints that may be favourable to the application of interest . The primary goal of this work is not to find the most accurate model , however we hope that this framework could be a stepping stone in using more complex and accurate generative models of text to asses semantic similarity . In the next draft of the paper we will add two different likelihoods which allow for non-unit normed vectors , unlike the vMF distribution . `` The experiments are just too simple and incomplete to make reasonable conclusions . '' We are unsure if the reviewer has concerns about the STS task in particular , or the variety of experiments ran . To address the former , we provide our argument for why we think STS is an adequate task to assess performance on . Our focus is on the setting where one has word level embeddings that contain semantic information about individual words , but no labelled corpus with examples of similar and dissimilar pairs ( an unsupervised setting ) . Furthermore , we assume sentences arrive in an 'online ' fashion meaning that we do n't have access to the whole sentence corpus a priori . An example use-case like this is a chat-bot application . To address the latter , we will extend our experiments by considering other word vectors usually used to assess performance on STS such as fasttext and word2vec . We will also include experimental results using other likelihoods and information criteria within our framework . Below we provide a preliminary set of results using a Gaussian likelihood with a diagonal covariance matrix . + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | | Method | STS12 | STS13 | STS14 | STS15 | STS16| W . A . * | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | fasttext | Ours | 0.6193 | 0.6335 | 0.6721 | 0.7328 | 0.7518| 0.6765| | | SIF+PCA | 0.5893 | 0.7121 | 0.6790 | 0.7498 | 0.7142| 0.6810| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | glove | Ours | 0.6031 | 0.6132 | 0.6445 | 0.7171 | 0.7346| 0.6564| | | SIF+PCA | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931| 0.6552| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | word2vec| Ours | 0.5630 | 0.5799 | 0.6291 | 0.6951 | 0.6701| 0.6265| | | SIF+PCA | 0.5324 | 0.6486 | stands for weighted average Would experiments along these lines address the simplicity concern of the reviewer ?"}, {"review_id": "HJMghjA9YX-1", "review_text": "The authors propose a probabilistic model for computing the sentence similarity between two sets of representations in an online fashion (that is, they do not need to see the entire dataset at once as SIF does when using PCA). They evaluate on the STS tasks and outperform competitive baselines like WMD, averaging embeddings, and SIF (without PCA), but they have worse performance that SIF + PCA. The paper is clearly written and their model is carefully laid out along with their derivation. My concern with this paper however, is that I feel the paper lacks a motivation, was it derive an online similarity metric that outperforms SIF(without PCA)? A few experimental questions/comments: What happens to all methods when stop words are not removed? How far does performance fall? I think one reason it might fall (in addition to the reasons given in the paper) is that all vectors are set to have the same norm. For STS tasks, often the norms of these vectors are reduced during training which lessens their influence. What mechanism was used to identify the stop words and does removing these help the other methods (I know in the paper, stop words were removed in the baseline, did this unilaterally improve performance for these methods)? Overall I do like the paper, however I do find the results to be lackluster. There are many papers on combining word embeddings trained in various ways that have much stronger numbers on STS, but these methods won't be effective with this type of similarity (namely because embeddings must have unit norm in their model). Therefore, I think the paper needs some more motivation and experimental evidence of its superiority over related methods like SIF+PCA in order for it to be accepted. PROS - Probabilistic model with clear design assumptions from which a similarity metric can be derived. - Derived similarity metric doesn't require knowledge of the entire dataset (in comparison to SIF + PCA) CONS - Performance seems to be slightly better than SIF, WMD, and averaging word embeddings, but below that of SIF + PCA - Unclear motivation for the model, was it derive an online similarity metric that outperforms SIF(without PCA)? - Requires the removal of stop words, but doesn't state how these were defined. Minor point, but tuning this could be enough to cause the improvement over related methods.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for their in-depth feedback . Below , we present preliminary results , as well as clarifications on the conceptual questions that were posed . `` My concern with this paper however , is that I feel the paper lacks a motivation ... '' The main focus of this paper is the introduction of a framework that allows for clear assumptions to be made about the distribution of word vectors in a sentence via a choice of likelihood and for these assumptions to be tested on the STS benchmark - any likelihood will fit into this general framework and produce a similarity measure . This allows practitioners to design likelihoods that encode suitable properties for their application . The more practical motivation for this paper is that the online * setting is key for many real-world use-cases such as information retrieval for dialogue systems ( i.e.chatbots ) where new queries will arrive in an online fashion and methods like SIF+PCA will not be as applicable as they are in the STS task . Whilst the method is derived as an online method it can be used in applications that have offline components , and we have shown it remains competitive to offline methods such as SIF+PCA ( see the next section of this response ) . * We thank the reviewer for helpfully clarifying the definitions of an `` online '' and `` offline '' setting ; we will include this definition in the next version of the manuscript . `` ... namely because embeddings must have unit norm in their model . '' The reviewer has helpfully pointed out an implicit assumption that we made - namely , we assumed that the magnitude of word embedding was noise rather than useful information . To test this assumption , we are running experiments with a multivariate Gaussian likelihood with diagonal covariance . This does not require unit norming the vectors and a set of preliminary results are presented below . + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | | Method | STS12 | STS13 | STS14 | STS15 | STS16| W . A . * | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | fasttext | Ours | 0.6193 | 0.6335 | 0.6721 | 0.7328 | 0.7518| 0.6765| | | SIF+PCA | 0.5893 | 0.7121 | 0.6790 | 0.7498 | 0.7142| 0.6810| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | glove | Ours | 0.6031 | 0.6132 | 0.6445 | 0.7171 | 0.7346| 0.6564| | | SIF+PCA | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931| 0.6552| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | word2vec| Ours | 0.5630 | 0.5799 | 0.6291 | 0.6951 | 0.6701| 0.6265| | | SIF+PCA | 0.5324 | 0.6486 | stands for weighted average `` ... I do find the results to be lackluster . '' As we can see , the Gaussian distribution seems to be a better fit than the vMF one , matching SIF+PCA on the three word embeddings we tested on . We hope this addresses the concern of the reviewer that methods which depend on embedding magnitude wo n't be applicable with this framework . We will include a more thorough set of results in the next version of the manuscript . `` What mechanism was used to identify the stop words and does removing these help the other methods ... '' `` What happens to all methods when stop words are not removed ? '' [ EDIT ] We have decided to report only results of experiments without any stopword removal . As the reviewers suggested , stopword removal heavily benefited the vMF likelihood , while the newly introduced Gaussian likelihood proves to be more robust . This decision also heavily reduces the clutter of the paper and the amount of care that needs to be taken to reproduce our results ."}, {"review_id": "HJMghjA9YX-2", "review_text": "Main contribution: devising and evaluating a theoretically-sound algorithm for quantifying the semantic similarity between two pieces of text (e.g., two sentences), given pre-trained word embeddings (glove). Clarity: The paper is generally well-written, but I would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results. As an example, I could not understand what were the differences between the online and offline settings, with only a reference to the (Arora et al. 2016) paper that does not contain neither \"online\" nor \"offline\". The mathematical derivations are detailed, which is nice. Originality: The work looks original. It proposes a method for quantifying semantic similarity that does not rely on cosine similarity. Significance: I should start by saying I am not a great reviewer for this paper. I am not familiar with the STS dataset and don't have the mathematical background to fully understand the author's algorithm. I like to see theoretical work in a field that desperately needs some, but overall I feel the paper could do a much better job at explaining the motivation behind the work, which is limited to \"cosine similarity [...] is not backed by a solid theoretical foundation\". I am not convinced of the practicality of the algorithm either: the algorithm seems to improve slightly over the compared approaches (and it is unclear if the differences are significant), and only in some settings. The approach needs to remove stop-words, which is reminiscent of good old feature engineering. Finally, the paper claims better average time complexity than some other methods, but discussing whether the algorithm is faster for common ranges of d (the word embedding dimension) would also have been interesting. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for their comments . We have tried to address their concerns below . `` I could not understand what were the differences between the online and offline settings ... '' We apologise for the lack of definition of these terms in the paper . This will be remedied in the next version . The difference between an online and offline setting is whether one has access to the entire dataset on which the methods will be evaluated at once . Information retrieval is an example setting in which one can not perform the PCA on the query dataset as seen in ( Arora et al.2017 ) , since new queries will arrive in an online fashion . `` ... the paper could do a much better job at explaining the motivation behind the work ... '' Similarity measures are often not theoretically justified - for example , cosine similarity is preferred to dot product similarity based purely on empirical results . It is difficult for a practitioner to utilize word vectors efficiently if the underlying assumptions in the similarity measure are not well understood . Our framework addresses these issues by explicitly deriving the similarity through the likelihood of the chosen generative process , instead of empirically motivating the similarity measure . Via designing the likelihood the practitioner can encode suitable assumptions and constraints that may be favourable to the application of interest . Furthermore , this framework proposes a new research direction that could help understand semantic similarity , in which practitioners can study suitable distributions and see how these perform . We will elaborate on this further in the updated version of the manuscript . The second motivation of this work is to derive a simple but competitive similarity measure that would perform well in online settings ( as defined above ) . Online settings are both practical and key to use-cases that involve information retrieval in dialogue systems . For example , in a chat-bot application new queries will arrive in an online fashion and methods such as SIF+PCA will not perform as strongly as they do on STS . This is because the method itself ( in this case the PCA part ) was fitted on the reported test set , which will not be available a priori in online settings . `` I am not convinced of the practicality of the algorithm ... '' We were unsure of what the reviewer meant by practicality of the algorithm . The presented algorithm requires no more than 30 lines of code to implement once the derivatives for the chosen likelihood have been calculated . Furthermore , the derivatives can be computed automatically with frameworks such as autograd , TensorFlow , PyTorch , and others . Below we provide the results when using a multivariate Gaussian distribution with diagonal covariance as a likelihood . + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | | Method | STS12 | STS13 | STS14 | STS15 | STS16| W . A . * | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | fasttext | Ours | 0.6193 | 0.6335 | 0.6721 | 0.7328 | 0.7518| 0.6765| | | SIF+PCA | 0.5893 | 0.7121 | 0.6790 | 0.7498 | 0.7142| 0.6810| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | glove | Ours | 0.6031 | 0.6132 | 0.6445 | 0.7171 | 0.7346| 0.6564| | | SIF+PCA | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931| 0.6552| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | word2vec| Ours | 0.5630 | 0.5799 | 0.6291 | 0.6951 | 0.6701| 0.6265| | | SIF+PCA | 0.5324 | 0.6486 | [ EDIT ] We have decided to report only results of experiments without any stopword removal . As the reviewers suggested , stopword removal benefits some approaches more than it does others . This decision also heavily reduces the clutter of the paper and the amount of care that needs to be taken to reproduce our results . `` ... discussing whether the algorithm is faster for common ranges of d ( the word embedding dimension ) ... '' We appreciate the suggestion of grounding the complexity analysis with values for N and D in the ranges experienced in the STS dataset . We will provide an analysis with the next version of the manuscript ."}], "0": {"review_id": "HJMghjA9YX-0", "review_text": "The paper proposes a Bayesian model comparison based approach for quantifying the semantic similarity between two groups of embeddings (e.g., two sentences). In particular, it proposes to use the difference between the probability that the two groups are from the same model and the probability that they are from different models. While the approach looks interesting, I have a few concerns: -- Using the Bayesian model comparison framework seems to be an interesting idea. However, what are the advantages compared to widely used learned models (say, a learned CNN that takes as input two sentences and outputs the similarity score)? The latter can fit the ground-truth labels given by humans, while it's unclear the model comparison leads to good correlation with human judgments. Some discussion should be provided. -- The von Mises-Fisher Likelihood is a very simplified model of actual text data. Have you considered using other models? In particular, more sophisticated ones may lead to better performance. -- Different information criteria can be plugged in. Are there comparisons? -- The experiments are just too simple and incomplete to make reasonable conclusions. For example, it seems compared to SIF there is not much advantage even in the online setting. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "`` The latter can fit the ground-truth labels given by humans , while it 's unclear the model comparison leads to good correlation with human judgments . Some discussion should be provided . '' STS provides a test set in order to evaluate how the methods correlate with human scores , which we have used to benchmark our proposed models . That is , performing well on the test set suggests there 's a correlation between human judgment and the model 's prediction of similarity score . We will clarify this in the new manuscript . We will discuss the relation of our approach to the one presented in Equation ( 9 ) ( Tversky 's contrast model ) of [ 6 ] - a work that analyses what a good similarity is from a cognitive science perspective . [ 6 ] J.B. Tenenbaum , and T.L . Griffiths , 2001 . Generalization , similarity , and Bayesian inference . Behavioral and brain sciences . `` Have you considered using other models ? In particular , more sophisticated ones may lead to better performance . '' The motivation for this paper is to introduce a framework in which different probabilistic models can be assessed on the STS task . This is done such that a practitioner , through specifying the likelihood function , can encode suitable assumptions and constraints that may be favourable to the application of interest . The primary goal of this work is not to find the most accurate model , however we hope that this framework could be a stepping stone in using more complex and accurate generative models of text to asses semantic similarity . In the next draft of the paper we will add two different likelihoods which allow for non-unit normed vectors , unlike the vMF distribution . `` The experiments are just too simple and incomplete to make reasonable conclusions . '' We are unsure if the reviewer has concerns about the STS task in particular , or the variety of experiments ran . To address the former , we provide our argument for why we think STS is an adequate task to assess performance on . Our focus is on the setting where one has word level embeddings that contain semantic information about individual words , but no labelled corpus with examples of similar and dissimilar pairs ( an unsupervised setting ) . Furthermore , we assume sentences arrive in an 'online ' fashion meaning that we do n't have access to the whole sentence corpus a priori . An example use-case like this is a chat-bot application . To address the latter , we will extend our experiments by considering other word vectors usually used to assess performance on STS such as fasttext and word2vec . We will also include experimental results using other likelihoods and information criteria within our framework . Below we provide a preliminary set of results using a Gaussian likelihood with a diagonal covariance matrix . + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | | Method | STS12 | STS13 | STS14 | STS15 | STS16| W . A . * | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | fasttext | Ours | 0.6193 | 0.6335 | 0.6721 | 0.7328 | 0.7518| 0.6765| | | SIF+PCA | 0.5893 | 0.7121 | 0.6790 | 0.7498 | 0.7142| 0.6810| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | glove | Ours | 0.6031 | 0.6132 | 0.6445 | 0.7171 | 0.7346| 0.6564| | | SIF+PCA | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931| 0.6552| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | word2vec| Ours | 0.5630 | 0.5799 | 0.6291 | 0.6951 | 0.6701| 0.6265| | | SIF+PCA | 0.5324 | 0.6486 | stands for weighted average Would experiments along these lines address the simplicity concern of the reviewer ?"}, "1": {"review_id": "HJMghjA9YX-1", "review_text": "The authors propose a probabilistic model for computing the sentence similarity between two sets of representations in an online fashion (that is, they do not need to see the entire dataset at once as SIF does when using PCA). They evaluate on the STS tasks and outperform competitive baselines like WMD, averaging embeddings, and SIF (without PCA), but they have worse performance that SIF + PCA. The paper is clearly written and their model is carefully laid out along with their derivation. My concern with this paper however, is that I feel the paper lacks a motivation, was it derive an online similarity metric that outperforms SIF(without PCA)? A few experimental questions/comments: What happens to all methods when stop words are not removed? How far does performance fall? I think one reason it might fall (in addition to the reasons given in the paper) is that all vectors are set to have the same norm. For STS tasks, often the norms of these vectors are reduced during training which lessens their influence. What mechanism was used to identify the stop words and does removing these help the other methods (I know in the paper, stop words were removed in the baseline, did this unilaterally improve performance for these methods)? Overall I do like the paper, however I do find the results to be lackluster. There are many papers on combining word embeddings trained in various ways that have much stronger numbers on STS, but these methods won't be effective with this type of similarity (namely because embeddings must have unit norm in their model). Therefore, I think the paper needs some more motivation and experimental evidence of its superiority over related methods like SIF+PCA in order for it to be accepted. PROS - Probabilistic model with clear design assumptions from which a similarity metric can be derived. - Derived similarity metric doesn't require knowledge of the entire dataset (in comparison to SIF + PCA) CONS - Performance seems to be slightly better than SIF, WMD, and averaging word embeddings, but below that of SIF + PCA - Unclear motivation for the model, was it derive an online similarity metric that outperforms SIF(without PCA)? - Requires the removal of stop words, but doesn't state how these were defined. Minor point, but tuning this could be enough to cause the improvement over related methods.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for their in-depth feedback . Below , we present preliminary results , as well as clarifications on the conceptual questions that were posed . `` My concern with this paper however , is that I feel the paper lacks a motivation ... '' The main focus of this paper is the introduction of a framework that allows for clear assumptions to be made about the distribution of word vectors in a sentence via a choice of likelihood and for these assumptions to be tested on the STS benchmark - any likelihood will fit into this general framework and produce a similarity measure . This allows practitioners to design likelihoods that encode suitable properties for their application . The more practical motivation for this paper is that the online * setting is key for many real-world use-cases such as information retrieval for dialogue systems ( i.e.chatbots ) where new queries will arrive in an online fashion and methods like SIF+PCA will not be as applicable as they are in the STS task . Whilst the method is derived as an online method it can be used in applications that have offline components , and we have shown it remains competitive to offline methods such as SIF+PCA ( see the next section of this response ) . * We thank the reviewer for helpfully clarifying the definitions of an `` online '' and `` offline '' setting ; we will include this definition in the next version of the manuscript . `` ... namely because embeddings must have unit norm in their model . '' The reviewer has helpfully pointed out an implicit assumption that we made - namely , we assumed that the magnitude of word embedding was noise rather than useful information . To test this assumption , we are running experiments with a multivariate Gaussian likelihood with diagonal covariance . This does not require unit norming the vectors and a set of preliminary results are presented below . + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | | Method | STS12 | STS13 | STS14 | STS15 | STS16| W . A . * | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | fasttext | Ours | 0.6193 | 0.6335 | 0.6721 | 0.7328 | 0.7518| 0.6765| | | SIF+PCA | 0.5893 | 0.7121 | 0.6790 | 0.7498 | 0.7142| 0.6810| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | glove | Ours | 0.6031 | 0.6132 | 0.6445 | 0.7171 | 0.7346| 0.6564| | | SIF+PCA | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931| 0.6552| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | word2vec| Ours | 0.5630 | 0.5799 | 0.6291 | 0.6951 | 0.6701| 0.6265| | | SIF+PCA | 0.5324 | 0.6486 | stands for weighted average `` ... I do find the results to be lackluster . '' As we can see , the Gaussian distribution seems to be a better fit than the vMF one , matching SIF+PCA on the three word embeddings we tested on . We hope this addresses the concern of the reviewer that methods which depend on embedding magnitude wo n't be applicable with this framework . We will include a more thorough set of results in the next version of the manuscript . `` What mechanism was used to identify the stop words and does removing these help the other methods ... '' `` What happens to all methods when stop words are not removed ? '' [ EDIT ] We have decided to report only results of experiments without any stopword removal . As the reviewers suggested , stopword removal heavily benefited the vMF likelihood , while the newly introduced Gaussian likelihood proves to be more robust . This decision also heavily reduces the clutter of the paper and the amount of care that needs to be taken to reproduce our results ."}, "2": {"review_id": "HJMghjA9YX-2", "review_text": "Main contribution: devising and evaluating a theoretically-sound algorithm for quantifying the semantic similarity between two pieces of text (e.g., two sentences), given pre-trained word embeddings (glove). Clarity: The paper is generally well-written, but I would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results. As an example, I could not understand what were the differences between the online and offline settings, with only a reference to the (Arora et al. 2016) paper that does not contain neither \"online\" nor \"offline\". The mathematical derivations are detailed, which is nice. Originality: The work looks original. It proposes a method for quantifying semantic similarity that does not rely on cosine similarity. Significance: I should start by saying I am not a great reviewer for this paper. I am not familiar with the STS dataset and don't have the mathematical background to fully understand the author's algorithm. I like to see theoretical work in a field that desperately needs some, but overall I feel the paper could do a much better job at explaining the motivation behind the work, which is limited to \"cosine similarity [...] is not backed by a solid theoretical foundation\". I am not convinced of the practicality of the algorithm either: the algorithm seems to improve slightly over the compared approaches (and it is unclear if the differences are significant), and only in some settings. The approach needs to remove stop-words, which is reminiscent of good old feature engineering. Finally, the paper claims better average time complexity than some other methods, but discussing whether the algorithm is faster for common ranges of d (the word embedding dimension) would also have been interesting. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for their comments . We have tried to address their concerns below . `` I could not understand what were the differences between the online and offline settings ... '' We apologise for the lack of definition of these terms in the paper . This will be remedied in the next version . The difference between an online and offline setting is whether one has access to the entire dataset on which the methods will be evaluated at once . Information retrieval is an example setting in which one can not perform the PCA on the query dataset as seen in ( Arora et al.2017 ) , since new queries will arrive in an online fashion . `` ... the paper could do a much better job at explaining the motivation behind the work ... '' Similarity measures are often not theoretically justified - for example , cosine similarity is preferred to dot product similarity based purely on empirical results . It is difficult for a practitioner to utilize word vectors efficiently if the underlying assumptions in the similarity measure are not well understood . Our framework addresses these issues by explicitly deriving the similarity through the likelihood of the chosen generative process , instead of empirically motivating the similarity measure . Via designing the likelihood the practitioner can encode suitable assumptions and constraints that may be favourable to the application of interest . Furthermore , this framework proposes a new research direction that could help understand semantic similarity , in which practitioners can study suitable distributions and see how these perform . We will elaborate on this further in the updated version of the manuscript . The second motivation of this work is to derive a simple but competitive similarity measure that would perform well in online settings ( as defined above ) . Online settings are both practical and key to use-cases that involve information retrieval in dialogue systems . For example , in a chat-bot application new queries will arrive in an online fashion and methods such as SIF+PCA will not perform as strongly as they do on STS . This is because the method itself ( in this case the PCA part ) was fitted on the reported test set , which will not be available a priori in online settings . `` I am not convinced of the practicality of the algorithm ... '' We were unsure of what the reviewer meant by practicality of the algorithm . The presented algorithm requires no more than 30 lines of code to implement once the derivatives for the chosen likelihood have been calculated . Furthermore , the derivatives can be computed automatically with frameworks such as autograd , TensorFlow , PyTorch , and others . Below we provide the results when using a multivariate Gaussian distribution with diagonal covariance as a likelihood . + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | | Method | STS12 | STS13 | STS14 | STS15 | STS16| W . A . * | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | fasttext | Ours | 0.6193 | 0.6335 | 0.6721 | 0.7328 | 0.7518| 0.6765| | | SIF+PCA | 0.5893 | 0.7121 | 0.6790 | 0.7498 | 0.7142| 0.6810| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | glove | Ours | 0.6031 | 0.6132 | 0.6445 | 0.7171 | 0.7346| 0.6564| | | SIF+PCA | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931| 0.6552| + -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + | word2vec| Ours | 0.5630 | 0.5799 | 0.6291 | 0.6951 | 0.6701| 0.6265| | | SIF+PCA | 0.5324 | 0.6486 | [ EDIT ] We have decided to report only results of experiments without any stopword removal . As the reviewers suggested , stopword removal benefits some approaches more than it does others . This decision also heavily reduces the clutter of the paper and the amount of care that needs to be taken to reproduce our results . `` ... discussing whether the algorithm is faster for common ranges of d ( the word embedding dimension ) ... '' We appreciate the suggestion of grounding the complexity analysis with values for N and D in the ranges experienced in the STS dataset . We will provide an analysis with the next version of the manuscript ."}}