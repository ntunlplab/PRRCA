{"year": "2019", "forum": "HJxB5sRcFQ", "title": "LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators", "decision": "Accept (Poster)", "meta_review": "Reviewers agree the paper should be accepted.\nSee reviews below.", "reviews": [{"review_id": "HJxB5sRcFQ-0", "review_text": " Summary: This paper presents a novel GAN framework for generating graphic layouts which consists of a set of graphic elements which are geometrically and semantically related. The generator learns a function that maps input layout ( a random set of graphic elements denoted by their classes probabilities and and geometric parameters) and outputs the new contextually refined layout. The paper also explores two choices of discriminators: (1) relation based discriminator which directly extracts the relations among different graphic elements in the parameter space, and (2) wireframe rendering discriminator which maps graphic elements to 2D wireframe images using a differentiable layer followed by a CNN for learning the discriminator. The novel GAN framework is evaluated on several datasets such as MNIST, document layout comparison and clipart abstract scene generation Pros: - The paper is trying to solve an interesting problem of layout generation. While a large body of work has focussed on pixel generation, this paper focuses on graphic layouts which can have a wide range of practical applications. - The paper presents a novel architecture by proposing a generator that outputs a graphic layout consisting of class probabilities and polygon keypoints. They also propose a novel discriminator consisting of a differentiable layer that takes the parameters of the output layout and generates a rasterized image representing the wireframe. This is quite neat as it allows to utilize a CNN for learning a discriminator for real / fake prediction. - Qualitative results are shown on a wide variety of datasets - from MNIST to clipart scene generation and tangram graphic design generation. I found the clipart scene and tangram graphic design generation experiments quite neat. Cons: - While the paper presents a few qualitative results, the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation. - The paper also doesn\u2019t report results on simple baselines for generating graphic layouts. Why not have a simple regression based baseline for predicting polygon parameters? Or compare with the approach mentioned in [1] - Even for generating MNIST digits, the paper doesn\u2019t report numbers on previous methods used for MNIST digit generation. Interestingly, only figure 4 shows results from a traditional GAN approach (DCGAN). Why not show the output on other datasets too? Questions / Remarks: - Why is the input to the GAN not the desired graphic elements and pose the problem as just predicting the polygon keypoints for those graphic elements. I didn\u2019t quite understand the motivation of choosing a random set of graphic elements and their class probabilities as input. - How does this work for the case of clip-art generation for example? The input to the gan is a list of all graphic elements (boy, girl glasses, hat, sun and tree) or a subset of these? - It is also not clear what role the class probabilities are playing this formulation. - In section 3.3.2, it\u2019s mentioned that the target image consist of C channels assuming there are C semantic classes for each element. What do you mean by each graphic element having C semantic classes? Also in the formulation discusses in this section, there is no further mention of C. I wasn\u2019t quite clear what the purpose of C channels is then. - I found Figure 3b quite interesting - it would have been nice if you expanded on that experiments and the observations you made a little more. [1] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al ", "rating": "7: Good paper, accept", "reply_text": "Q : \u201c While the paper presents a few qualitative results , the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation \u201d A : Thank you for your suggestions . This work is the first attempt to solve layout synthesis from random input for both Clipart scene generation and tangram graphic design ( tangram data are collected and annotated by ourselves , we promise to release it upon acceptance ) . As no previous methods have focused on these problems , there is a lack of widely-accepted quantitative evaluation metrics for both tasks . To this end , we carried out a user study involving 20 respondents for a subjective evaluation of the generated Clipart abstract scenes . Please see Table 3 in the updated version . Q : \u201c The paper also doesn \u2019 t report results on simple baselines for generating graphic layouts . Why not have a simple regression based baseline for predicting polygon parameters ? Or compare with the approach mentioned in [ 1 ] \u201d [ 1 ] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al A : Thank you for your suggestions . We have supplemented experiments of generating tangram graphic design sequentially as Wang et al [ 1 ] for comparison in the updated version . Specifically , Wang et al [ 1 ] generate indoor scenes iteratively by adding objects one-by-one . The choice of such sequential paradigm is partly because the rendering process from geometric parameters ( object location ) to indoor scene images is not differentiable . Similarly , we would have faced such a problem in our layout design . However , we propose a novel wireframe rendering layer to make the layout rendering process differentiable . Benefiting from it , we can predict a set of graphic elements simultaneously in an end-to-end network . But still , we can adopt the sequential paradigm in Wang et al [ 1 ] to our layout design problem by generating graphic elements one-by-one . However , we found such sequential synthesis process suffers from accumulated error , which validates the superiority of the proposed LayoutGAN . Please see Figure 8 for comparisons in the updated version . Q : \u201c Even for generating MNIST digits , the paper doesn \u2019 t report numbers on previous methods used for MNIST digit generation . A : Our experiment on MNIST serves as sanity test . A 2D point , as the simplest geometric form , is not a desirable element representation for our approach , and we do not expect it to compete with other GANs applied to MNIST . We have reflected this in the updated version . Q : Interestingly , only figure 4 shows results from a traditional GAN approach ( DCGAN ) . Why not show the output on other datasets too ? \u201d A : Thanks for the suggestion . We added experiments to apply DCGAN to both Clipart abstract scene generation and tangram graphic design task in the updated version . Please see Figure 5 and 8 ."}, {"review_id": "HJxB5sRcFQ-1", "review_text": "Summary: The paper proposed to use GAN to synthesize graphical layouts. The generator takes a random input and generates class probabilities and geometric parameters based on a self-attention module. The discriminator is based on a differentiable wireframe rendering component (proposed by the paper) to allow back propagation through the rendering module. I found the topic very interesting and the approach seems to make sense. Quality: + The idea is very interesting and novel. Clarity: + The paper is clearly written and is easy to follow. Originality: + I believe the paper is novel. The differentiable wireframe rendering is new and very interesting. Significance: + I believe the paper has value to the community. - The evaluation of the task seems to be challenging (Inception score may not be appropriate) but since this is probably the first paper to generate layouts, I would not worry too much about the actual accuracy. Question: Why not ask the generator to generate the rendering instead of class probabilities?", "rating": "7: Good paper, accept", "reply_text": "Q : \u201c Why not ask the generator to generate the rendering instead of class probabilities ? \u201d A : The generator produces geometric layout parameters together with class probabilities . Rendering a wireframe image from the layout parameters is then trivial . Rendering an application-specific layout , e.g. , graphic design bitmap , is application-dependent and unnecessarily complex for modeling layout . Does this answer your question ?"}, {"review_id": "HJxB5sRcFQ-2", "review_text": " The authors present a GAN based framework for Graphic Layouts. Instead of considering a graphic layout as a collection of pixels, they treat it as a collection of primitive objects like polygons. The objective is to create an alignment of these objects that mimics some real data distribution. The novelty is a differentiable wireframe rendering layer allowing the discriminator to judge alignment. They compare this with a relation based discriminator based on the point net architecture by Qi et al. The experimentation is thorough and demonstrates the importance of their model architecture compared to baseline methods. Overall, this is a well written paper that proposes and solves a novel problem. My only complaint is that the most important use case of their GAN (Document Semantic Layout Generation) is tested on a synthetic dataset. It would have been nice to test it on a real life dataset.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q : \u201c My only complaint is that the most important use case of their GAN ( Document Semantic Layout Generation ) is tested on a synthetic dataset . It would have been nice to test it on a real life dataset. \u201d A : We added a new experiment of mobile app layout generation by using the RICO dataset ( http : //interactionmining.org/rico ) . We showed the results in the appendix due to page limitation . Please see Section 6.6 in the uploaded version for more details ."}], "0": {"review_id": "HJxB5sRcFQ-0", "review_text": " Summary: This paper presents a novel GAN framework for generating graphic layouts which consists of a set of graphic elements which are geometrically and semantically related. The generator learns a function that maps input layout ( a random set of graphic elements denoted by their classes probabilities and and geometric parameters) and outputs the new contextually refined layout. The paper also explores two choices of discriminators: (1) relation based discriminator which directly extracts the relations among different graphic elements in the parameter space, and (2) wireframe rendering discriminator which maps graphic elements to 2D wireframe images using a differentiable layer followed by a CNN for learning the discriminator. The novel GAN framework is evaluated on several datasets such as MNIST, document layout comparison and clipart abstract scene generation Pros: - The paper is trying to solve an interesting problem of layout generation. While a large body of work has focussed on pixel generation, this paper focuses on graphic layouts which can have a wide range of practical applications. - The paper presents a novel architecture by proposing a generator that outputs a graphic layout consisting of class probabilities and polygon keypoints. They also propose a novel discriminator consisting of a differentiable layer that takes the parameters of the output layout and generates a rasterized image representing the wireframe. This is quite neat as it allows to utilize a CNN for learning a discriminator for real / fake prediction. - Qualitative results are shown on a wide variety of datasets - from MNIST to clipart scene generation and tangram graphic design generation. I found the clipart scene and tangram graphic design generation experiments quite neat. Cons: - While the paper presents a few qualitative results, the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation. - The paper also doesn\u2019t report results on simple baselines for generating graphic layouts. Why not have a simple regression based baseline for predicting polygon parameters? Or compare with the approach mentioned in [1] - Even for generating MNIST digits, the paper doesn\u2019t report numbers on previous methods used for MNIST digit generation. Interestingly, only figure 4 shows results from a traditional GAN approach (DCGAN). Why not show the output on other datasets too? Questions / Remarks: - Why is the input to the GAN not the desired graphic elements and pose the problem as just predicting the polygon keypoints for those graphic elements. I didn\u2019t quite understand the motivation of choosing a random set of graphic elements and their class probabilities as input. - How does this work for the case of clip-art generation for example? The input to the gan is a list of all graphic elements (boy, girl glasses, hat, sun and tree) or a subset of these? - It is also not clear what role the class probabilities are playing this formulation. - In section 3.3.2, it\u2019s mentioned that the target image consist of C channels assuming there are C semantic classes for each element. What do you mean by each graphic element having C semantic classes? Also in the formulation discusses in this section, there is no further mention of C. I wasn\u2019t quite clear what the purpose of C channels is then. - I found Figure 3b quite interesting - it would have been nice if you expanded on that experiments and the observations you made a little more. [1] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al ", "rating": "7: Good paper, accept", "reply_text": "Q : \u201c While the paper presents a few qualitative results , the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation \u201d A : Thank you for your suggestions . This work is the first attempt to solve layout synthesis from random input for both Clipart scene generation and tangram graphic design ( tangram data are collected and annotated by ourselves , we promise to release it upon acceptance ) . As no previous methods have focused on these problems , there is a lack of widely-accepted quantitative evaluation metrics for both tasks . To this end , we carried out a user study involving 20 respondents for a subjective evaluation of the generated Clipart abstract scenes . Please see Table 3 in the updated version . Q : \u201c The paper also doesn \u2019 t report results on simple baselines for generating graphic layouts . Why not have a simple regression based baseline for predicting polygon parameters ? Or compare with the approach mentioned in [ 1 ] \u201d [ 1 ] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al A : Thank you for your suggestions . We have supplemented experiments of generating tangram graphic design sequentially as Wang et al [ 1 ] for comparison in the updated version . Specifically , Wang et al [ 1 ] generate indoor scenes iteratively by adding objects one-by-one . The choice of such sequential paradigm is partly because the rendering process from geometric parameters ( object location ) to indoor scene images is not differentiable . Similarly , we would have faced such a problem in our layout design . However , we propose a novel wireframe rendering layer to make the layout rendering process differentiable . Benefiting from it , we can predict a set of graphic elements simultaneously in an end-to-end network . But still , we can adopt the sequential paradigm in Wang et al [ 1 ] to our layout design problem by generating graphic elements one-by-one . However , we found such sequential synthesis process suffers from accumulated error , which validates the superiority of the proposed LayoutGAN . Please see Figure 8 for comparisons in the updated version . Q : \u201c Even for generating MNIST digits , the paper doesn \u2019 t report numbers on previous methods used for MNIST digit generation . A : Our experiment on MNIST serves as sanity test . A 2D point , as the simplest geometric form , is not a desirable element representation for our approach , and we do not expect it to compete with other GANs applied to MNIST . We have reflected this in the updated version . Q : Interestingly , only figure 4 shows results from a traditional GAN approach ( DCGAN ) . Why not show the output on other datasets too ? \u201d A : Thanks for the suggestion . We added experiments to apply DCGAN to both Clipart abstract scene generation and tangram graphic design task in the updated version . Please see Figure 5 and 8 ."}, "1": {"review_id": "HJxB5sRcFQ-1", "review_text": "Summary: The paper proposed to use GAN to synthesize graphical layouts. The generator takes a random input and generates class probabilities and geometric parameters based on a self-attention module. The discriminator is based on a differentiable wireframe rendering component (proposed by the paper) to allow back propagation through the rendering module. I found the topic very interesting and the approach seems to make sense. Quality: + The idea is very interesting and novel. Clarity: + The paper is clearly written and is easy to follow. Originality: + I believe the paper is novel. The differentiable wireframe rendering is new and very interesting. Significance: + I believe the paper has value to the community. - The evaluation of the task seems to be challenging (Inception score may not be appropriate) but since this is probably the first paper to generate layouts, I would not worry too much about the actual accuracy. Question: Why not ask the generator to generate the rendering instead of class probabilities?", "rating": "7: Good paper, accept", "reply_text": "Q : \u201c Why not ask the generator to generate the rendering instead of class probabilities ? \u201d A : The generator produces geometric layout parameters together with class probabilities . Rendering a wireframe image from the layout parameters is then trivial . Rendering an application-specific layout , e.g. , graphic design bitmap , is application-dependent and unnecessarily complex for modeling layout . Does this answer your question ?"}, "2": {"review_id": "HJxB5sRcFQ-2", "review_text": " The authors present a GAN based framework for Graphic Layouts. Instead of considering a graphic layout as a collection of pixels, they treat it as a collection of primitive objects like polygons. The objective is to create an alignment of these objects that mimics some real data distribution. The novelty is a differentiable wireframe rendering layer allowing the discriminator to judge alignment. They compare this with a relation based discriminator based on the point net architecture by Qi et al. The experimentation is thorough and demonstrates the importance of their model architecture compared to baseline methods. Overall, this is a well written paper that proposes and solves a novel problem. My only complaint is that the most important use case of their GAN (Document Semantic Layout Generation) is tested on a synthetic dataset. It would have been nice to test it on a real life dataset.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q : \u201c My only complaint is that the most important use case of their GAN ( Document Semantic Layout Generation ) is tested on a synthetic dataset . It would have been nice to test it on a real life dataset. \u201d A : We added a new experiment of mobile app layout generation by using the RICO dataset ( http : //interactionmining.org/rico ) . We showed the results in the appendix due to page limitation . Please see Section 6.6 in the uploaded version for more details ."}}