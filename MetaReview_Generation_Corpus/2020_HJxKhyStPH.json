{"year": "2020", "forum": "HJxKhyStPH", "title": "Toward Understanding The Effect of Loss Function on The Performance of Knowledge Graph Embedding", "decision": "Reject", "meta_review": "The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by choosing more appropriate loss functions.  The submission then proposes TransComplEx to further improve results.  This paper received four reviews, with three recommending rejection, and one recommending weak acceptance.  A main concern was in the clarity of motivating the different models.  Another was in the relatively low performance of RotatE compared with [1], which was raised by multiple reviewers.  The authors provided extensive responses to the concerns raised by the reviewers.  However, at least the implementation of RotatE remains of concern, with the response of the authors indicating \"Please note that we couldn\u2019t use exactly the same setting of RotatE due to limitations in our infrastructure.\"  On the balance, a majority of reviewers felt that the paper was not suitable for publication in its current form.", "reviews": [{"review_id": "HJxKhyStPH-0", "review_text": " Summary: This paper list several limitations of translational-based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. The authors propose four variants of loss function which address the limitations and propose a method, RPTransComplEx which utilizes their observations for outperforming several existing Knowledge Graph embedding methods. Overall, the proposed method is well motivated and experimental results have been found to be consistent with the theoretical analysis. Suggestions/Questions: 1. It would be great if hyperparameters listed in the \u201cExperimental Setup\u201d section could be presented in a table for better readability. 2. In Section 2, the authors have mentioned that RotatE obtains SOTA results using a very large embedding dimension (1000). However, it gives very similar performance even with smaller dimensional embedding (such as 200) with 1000 negative samples. In Section 5, RotatE results with 200 dimension and 10 negative samples are reported for a fair comparison. Wouldn\u2019t it be better to instead increase the number of negative samples in RPTransComplEx instead of decreasing negative samples in RotatE? 3. In Table 3, it is not clear why authors have not reported their performance on the WN18RR dataset for their methods. Also, the reported performance of TransE in [1] is much better than what is reported in the paper. [1] Sun, Zhiqing, Zhi-Hong Deng, Jian-Yun Nie and Jian Tang. \u201cRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\u201d ArXiv abs/1902.10197 (2019): n. pag. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your precious comments and suggestions . Comment : '' It would be great if hyperparameters listed in the \u201c Experimental Setup \u201d section could be presented in a table for better readability '' . Response : Thank you for the comments . Following the suggestion , we will put the hyper-parameters in a table in the appendix . Comment : '' In Section 2 , the authors have mentioned that RotatE obtains SOTA results using a very large embedding dimension ( 1000 ) . However , it gives very similar performance even with smaller dimensional embedding ( such as 200 ) with 1000 negative samples . In Section 5 , RotatE results with 200 dimension and 10 negative samples are reported for a fair comparison . Wouldn \u2019 t it be better to instead increase the number of negative samples in RPTransComplEx instead of decreasing negative samples in RotatE ? '' Response : Thank you for the suggestion . We decided to use a simple setting to sharply justify our theories . Additionally , we couldn \u2019 t use the same RotatE setting due to limitations in the infrastructures . However , we additionally ran our experiments using a bigger setting and will include the results in the paper . For example , using embedding dimension 300 and 256 negative samples on FB15K-237 , RotatE_Hits @ 10=51.8 , TransE5_Hits @ 10=52.1 , TransComplEx5_Hits @ 10=52.1 and RotatE_MR=195 , TransE5_MR=180 , TransComplEx5_MR=177 . Therefore , using much more negative samples with a bigger embedding dimension , the results improve . As properly mentioned in the comment of Jingpei Lei , RotatE uses a loss function approximating the condition ( c ) . Regarding the RotatE paper , table 13 ( appendix ) , TransE ( which was proposed on 2013 ) trained by the RotatE loss gets a very close performance to the RotatE model ( which was proposed on 2019 ) trained by the RotatE loss when a big setting is used ( more negative samples with a higher dimension ) . A baseline model proposed around six years ago obtains state-of-the-art performance ! It has been already reported that the model has several limitations which are not consistent with the recently reported results ( e.g. , table 13 of the RotatE paper ) . Our theories corresponding to the limitations of TransE can also explain such reported experimental results and gives a better understanding of the previous work ( especially the baselines that several models have been proposed on top of them to fix their limitations ) . it shows the importance of our work . Overall , TransE and its variants have fewer limitations than what has been reported . Our theories shed more light on this . Comment : '' In Table 3 , it is not clear why authors have not reported their performance on the WN18RR dataset for their methods '' Response : We didn \u2019 t use any relation patterns as extra knowledge to be injected into the model on WN18RR . We only reported the results of TransE5 and TransComplEx5 which are trained by using just triples . Comparing TransComplEx and RPTransComplEx on other datasets , we found that TransComplEx encodes relation patterns by training only on triples without any relation pattern injection . We will include figures of convergence of relation pattern losses to show that TransComplEx learns most of the relation patterns without any relation pattern injection . Comment : '' Also , the reported performance of TransE in [ 1 ] is much better than what is reported in the paper '' Response : [ 1 ] reported the result of TransE on WN18RR with different settings ( Table5 and 7 of [ 1 ] ) . We decided to rerun experiments on TransE with a Margin Ranking Loss using the setting which we reported in order to have 1 ) a fair comparison and 2 ) a proper justification in the theories as the theories are related to TransE with different loss . We will update the results in the paper ."}, {"review_id": "HJxKhyStPH-1", "review_text": "In this paper, the authors investigate the main limitations of TransE in the light of loss function. The authors claim that their contributions consist of two parts: 1) proving that the proper selection of loss functions is vital in KGE; 2) proposing a model called TransComplEx. The results show that the proper selection of the loss function can mitigate the limitations of TransX (X=H, D, R, etc) models. My major concerns are as follows. 1. The motivation of TransComplEx and why it works are unclear in the paper. 2. The experiments might be unconvincing. In the experiments, the authors claim that they implement RotatE [1] in their setting to make a fair comparison. However, with their setting, the performance of RotatE is much worse than that in the original paper [1]. Therefore, the experiments might be unfair to RotatE. 3. There are some typos in this paper. For example, in Line 11 of Section 4.3, the comma should be a period; in Section 5, the \"Dissuasion of Results\" should be \"Discussion of Results\". [1] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019. ", "rating": "3: Weak Reject", "reply_text": "Thank you for the valuable comments . Comment : '' The motivation of TransComplEx and why it works are unclear in the paper '' Response : According to our theories , TransComplEx has fewer limitations than the TransE model . Comment : '' The experiments might be unconvincing . In the experiments , the authors claim that they implement RotatE [ 1 ] in their setting to make a fair comparison . However , with their setting , the performance of RotatE is much worse than that in the original paper [ 1 ] . Therefore , the experiments might be unfair to RotatE . '' Response : Comparison of different models in completely different settings does not give a proper conclusion . However , following the raised point , we additionally compared the models with RotatE in a bigger setting . We saw the same patterns in the results which confirm our previous conclusions . We will report the results of the new experiments in a bigger setting in the appendix . Please note that we couldn \u2019 t use exactly the same setting of RotatE due to limitations in our infrastructure . Comment : `` There are some typos in this paper . For example , in Line 11 of Section 4.3 , the comma should be a period ; in Section 5 , the `` Dissuasion of Results '' should be `` Discussion of Results '' Response : Thank you , we fixed the typos ."}, {"review_id": "HJxKhyStPH-2", "review_text": "The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by chosing more appropriate loss functions. Furthermore, the paper proposes TransComplEx -- an adaption of ideas from ComplEx/HolE to TransE -- to mitigate issues that can not be overcome by a simply chosing a different loss. Analyzing the behavior and short-comings of commonly-used models can be an important contribution to advance the state-of-the-art. This paper focuses on the performance of TransE, which is a popular representation learning approach for knowledge graph completion and as such fits well into ICLR. Unfortunately, the current version of the paper seems to have issues regarding methodology and novelty. Regarding the experimental evaluation: The paper compares the results of TransComplEx and the different loss functions to results that have previously been published in this field (directly, without retraining). However, it seems from Section 5 (Dataset), that this paper is using a modified dataset, as the TransE models are only trained on high-confidence triples. All prior work that I checked doesn't seem to do this, and hence the numbers are not comparable. Even more serious: Following again Section 5 (Dataset), it seems that the paper imputes all missing triples in the training set for symmetric and transitive relations (\"grounding\"). Hence, the models get to see _all_ true triples for these relation types and as such the models in this paper are trained on the test set. Regarding novelty: The short-comings of TransE and improvements to the loss have been discussed quite extensively in prior work. Using complex representations in TransComplEx seems also a straightforward application of the insights of ComplEx/Hole. As such, the main novelty would lie in the experimental results which, unfortunately, seem problematic.", "rating": "1: Reject", "reply_text": "Thank you for the valuable comments . Comments : '' it seems from Section 5 ( Dataset ) , that this paper is using a modified dataset '' : Response : Actually , we used the same dataset ( WN18rr , WN18 , FB15K-237 , FB15K ) that have been extensively used for evaluation of KGEs by others and these data sets do not contain any information about the confidence of triples . Therefore , our models are not trained on high confidence triples . In Section-5 ( Dataset ) , we already mentioned that the relation patterns ( i.e. , rules ) with a lower confidence value are removed . That does not have anything to do with triples and their potential level of confidence . We used the relation patterns ( rules ) extracted by AMIE . These relation patterns ( rules ) were used ( by doing grounding ) in RUGE to be injected into the learning process . Each relation patterns ( and not triples ) used in RUGE has a confidence value . RUGE also only used relation patterns with confidence higher than 80 % . We used the same dataset . We compare our models ( trained by different losses ) with two classes of models : 1 ) the models that have not used any relation patterns ( rules ) as background knowledge ( such as RotatE and ComplEx , TransE etc ) , for injection and 2 ) the models used a set of relation patterns ( rules ) as background knowledge to inject them into the embedding models during the learning process ( such as RUGE , KALE etc ) . To have a fair comparison , we trained the TransComplEx under two conditions . First : in order to compare with the first class of models , TransComplEx is trained using only triples ( Table2,3\u2026 TransComplEx row ) and we did not use or inject any relation patterns into it . Second : in order to compare with the second class of models , RPTransComplEx used relation patterns ( rules ) as background knowledge to be injected into the learning process such as RUGE which trained ComplEx using relation patterns with confidence higher than 80 % . Therefore , we included both of the models in the Table-2,3 to have a comprehensive evaluation . Moreover , comparing TransComplEx5 and RPTransComplEx5 ( which both are trained with the same loss function ) , we see that the results of TransComplEx trained using loss ( 5 ) are very close to RPTransComplEx5 . We conclude that the model which is trained on only triples with the loss 5 ( i.e.TransComplEx5 ) is capable of properly learning the most of patterns without using additional background knowledge ( relation patterns ) to be injected . We visualized the relation patterns losses convergence for TransComplEx5 and RPTransComplEx5 ( respectively , without and with relation pattern injected ) . The convergence of the losses confirms that TransComplEx can properly learn the relation patterns without using additional knowledge to be injected . We will include the figures of relation pattern losses convergence of TransComplEx and RPTransComplEx in the paper . We did new experiments on TransComplEx and TransE as well as RotatE with different loss functions with a bigger setting . we will include them in the paper . The results are consistent with the theories corresponding to the limitations of different models . In this experiment , we didn \u2019 t use any relation patterns and the models are trained only using triples ."}, {"review_id": "HJxKhyStPH-3", "review_text": "The paper revisits limitations of relation-embedding models by taking losses into account in the derivation of these limitations. They propose and evaluate a new relation encoding (TransComplEx) and show that this encoding can address the limitations previously underlined in the literature when using the right loss. There seems to be merit in distinguishing the loss when studying relation encoding but I think the paper's analysis lacks proper rigor as-is. A loss minimization won't make equalities in (3) and (5) hold exactly, which the analysis do not account for. A rewriting of the essential elements of the different proofs could make the arguments clearer. Paper writing: * The manuscript should be improved with a thorough revision of the style and grammar. Example of mistakes include: extraneous or missing articles, incorrect verbs or tenses. * The 10-pages length is not beneficial, the recommended 8-pages could hold the same overall content. * The option list on page 8 is very difficult to read and should be put in a table, e.g. in appendix. * Parentheses are missing around many citations and equation references Theory: Equation (2) and (4) do not seem to bring much compared to the conditions in Table 1. Eq. (3) and (5) show \"a\" loss function rather than \"the\" loss function since multiple choices are possible. \\gamma_1 should be set to 0 when it is 0 rather than staying in the equations. * Minimizing the objective (3) and (5) will still not make the conditions in Table 1 hold exactly, because of slack variables. Experiments: - Can the authors provide examples of relations learned with RPTransComplEx# that go address the limitations L1...L6, validating experimentally the theoretical claims and showing that the gain with RPTransComplEx5 correspond to having learned these relations?", "rating": "3: Weak Reject", "reply_text": "Thank you for the valuable comments . Comment : `` A loss minimization wo n't make equalities in ( 3 ) and ( 5 ) hold exactly ... '' Response : In the first step , we posed the conditions ( Table-1 ) to prove theories corresponding to the limitations of the score functions . We then showed that each of these conditions can be approximated by a loss function ( which is not unique ) . The shortcomings of previous works were that they posed limitations under conditions that have not been approximated by the loss function they used . Consequently , they attempted to either address the limitations that do not really exist , or try to show the capability of their model under those conditions that are not approximated by their loss functions , resulting in some of the existing theories and conclusions that are not valid . We presented four conditions that defined the region of truth ( the region that a triple is considered positive by the model ) . Under these four conditions , we reinvestigated the main limitations of translation based class of embedding model and posed new theories in this regard . In fact , these conditions are ideal conditions to prove theorems and we highlighted that they should be enforced by a proper loss function . In practice , however , such conditions are approximated by a loss function . In other words , we aim to satisfy the condition as much as possible ( but not necessarily fully ) and as we highlighted , there is not a unique loss function to satisfy them . Our work opens a new window for future contributions in the development of new loss functions in which new losses might be proposed to satisfy the conditions , or other existing loss functions/models can be reinvestigated in the light of our proposed conditions . We will add more experimental analysis to the paper ( histogram of scores of models trained by different losses , etc ) in order to show how each of the losses approximate the corresponding conditions . Comment : `` style and grammar revision `` Response : Following your suggestion , we would revise the paper . Comment : '' The 10-pages length is not beneficial ... '' Response : We will try to reduce the length of the paper as much as possible . Comment : '' The option list on page 8 is very difficult to read ... '' Response : We include the content in a table in the appendix Comment : '' Parentheses are missing around many citations ... '' Response : We further revise the paper considering this comment Comment : `` Equation ( 2 ) and ( 4 ) do not seem to bring much compared to the conditions '' Response : We included the conditions of the Table-1 as constraints for the optimizations of ( 2 ) and ( 4 ) . For positive samples , the corresponding constraints enforce conditions to be held . However , for negative samples , we do not intend to include a hard boundary to define the region of negative samples , since some false negative samples are generated during the process of random negative sampling . Instead , we used a soft boundary for the negative samples by adding slack variables to mitigate the negative effect of noise . The goal is to approximate the conditions of Table-1 while addressing the noise in the training data . The optimal solutions ( embedding parameters ) for the optimization should be in the region defined by constraints . Therefore , the conditions are approximately satisfied . We experimentally show the value of loss function in order to approximate the conditions . Comment : '' \\gamma_1 should be set to 0 when it is 0 rather than staying in the equations . '' Response : We revise the paper following the comment . We decided to write it in a compact way , that is why we included two conditions in one formulation of loss while mentioning that setting \\gamma_1 to 0 approximates the condition ( a ) and setting \\gamma_1 to a positive ( non-zero ) value approximates the condition ( b ) for equation ( 3 ) . Comment : '' Minimizing the objective ( 3 ) and ( 5 ) will still not make the conditions in Table 1 hold exactly '' Response : As we mentioned earlier , the goal is to approximate the ideal conditions that we posed in theory , by a loss function ( not to satisfy them strictly ) . The quality of approximation of each of the conditions by each of the loss function is a new research direction that we are following as future work . Despite this fact , we will add a few results of some of our experiments to show the quality of approximation using the losses . The experiments show that the losses of ( 3 ) and ( 5 ) are properly converged to very small values , showing that the ideal conditions are properly approximated by the losses . Comment : '' Experiments : Can the authors provide examples of relations learned ... '' Response : Thank you for the important point . We are running experiments confirming our theories . We will include them in the paper . Our experiments confirm that most of the relation patterns are properly learned by the model ( and for some of them even them without injection ) , showing the value of loss function ."}], "0": {"review_id": "HJxKhyStPH-0", "review_text": " Summary: This paper list several limitations of translational-based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. The authors propose four variants of loss function which address the limitations and propose a method, RPTransComplEx which utilizes their observations for outperforming several existing Knowledge Graph embedding methods. Overall, the proposed method is well motivated and experimental results have been found to be consistent with the theoretical analysis. Suggestions/Questions: 1. It would be great if hyperparameters listed in the \u201cExperimental Setup\u201d section could be presented in a table for better readability. 2. In Section 2, the authors have mentioned that RotatE obtains SOTA results using a very large embedding dimension (1000). However, it gives very similar performance even with smaller dimensional embedding (such as 200) with 1000 negative samples. In Section 5, RotatE results with 200 dimension and 10 negative samples are reported for a fair comparison. Wouldn\u2019t it be better to instead increase the number of negative samples in RPTransComplEx instead of decreasing negative samples in RotatE? 3. In Table 3, it is not clear why authors have not reported their performance on the WN18RR dataset for their methods. Also, the reported performance of TransE in [1] is much better than what is reported in the paper. [1] Sun, Zhiqing, Zhi-Hong Deng, Jian-Yun Nie and Jian Tang. \u201cRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\u201d ArXiv abs/1902.10197 (2019): n. pag. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your precious comments and suggestions . Comment : '' It would be great if hyperparameters listed in the \u201c Experimental Setup \u201d section could be presented in a table for better readability '' . Response : Thank you for the comments . Following the suggestion , we will put the hyper-parameters in a table in the appendix . Comment : '' In Section 2 , the authors have mentioned that RotatE obtains SOTA results using a very large embedding dimension ( 1000 ) . However , it gives very similar performance even with smaller dimensional embedding ( such as 200 ) with 1000 negative samples . In Section 5 , RotatE results with 200 dimension and 10 negative samples are reported for a fair comparison . Wouldn \u2019 t it be better to instead increase the number of negative samples in RPTransComplEx instead of decreasing negative samples in RotatE ? '' Response : Thank you for the suggestion . We decided to use a simple setting to sharply justify our theories . Additionally , we couldn \u2019 t use the same RotatE setting due to limitations in the infrastructures . However , we additionally ran our experiments using a bigger setting and will include the results in the paper . For example , using embedding dimension 300 and 256 negative samples on FB15K-237 , RotatE_Hits @ 10=51.8 , TransE5_Hits @ 10=52.1 , TransComplEx5_Hits @ 10=52.1 and RotatE_MR=195 , TransE5_MR=180 , TransComplEx5_MR=177 . Therefore , using much more negative samples with a bigger embedding dimension , the results improve . As properly mentioned in the comment of Jingpei Lei , RotatE uses a loss function approximating the condition ( c ) . Regarding the RotatE paper , table 13 ( appendix ) , TransE ( which was proposed on 2013 ) trained by the RotatE loss gets a very close performance to the RotatE model ( which was proposed on 2019 ) trained by the RotatE loss when a big setting is used ( more negative samples with a higher dimension ) . A baseline model proposed around six years ago obtains state-of-the-art performance ! It has been already reported that the model has several limitations which are not consistent with the recently reported results ( e.g. , table 13 of the RotatE paper ) . Our theories corresponding to the limitations of TransE can also explain such reported experimental results and gives a better understanding of the previous work ( especially the baselines that several models have been proposed on top of them to fix their limitations ) . it shows the importance of our work . Overall , TransE and its variants have fewer limitations than what has been reported . Our theories shed more light on this . Comment : '' In Table 3 , it is not clear why authors have not reported their performance on the WN18RR dataset for their methods '' Response : We didn \u2019 t use any relation patterns as extra knowledge to be injected into the model on WN18RR . We only reported the results of TransE5 and TransComplEx5 which are trained by using just triples . Comparing TransComplEx and RPTransComplEx on other datasets , we found that TransComplEx encodes relation patterns by training only on triples without any relation pattern injection . We will include figures of convergence of relation pattern losses to show that TransComplEx learns most of the relation patterns without any relation pattern injection . Comment : '' Also , the reported performance of TransE in [ 1 ] is much better than what is reported in the paper '' Response : [ 1 ] reported the result of TransE on WN18RR with different settings ( Table5 and 7 of [ 1 ] ) . We decided to rerun experiments on TransE with a Margin Ranking Loss using the setting which we reported in order to have 1 ) a fair comparison and 2 ) a proper justification in the theories as the theories are related to TransE with different loss . We will update the results in the paper ."}, "1": {"review_id": "HJxKhyStPH-1", "review_text": "In this paper, the authors investigate the main limitations of TransE in the light of loss function. The authors claim that their contributions consist of two parts: 1) proving that the proper selection of loss functions is vital in KGE; 2) proposing a model called TransComplEx. The results show that the proper selection of the loss function can mitigate the limitations of TransX (X=H, D, R, etc) models. My major concerns are as follows. 1. The motivation of TransComplEx and why it works are unclear in the paper. 2. The experiments might be unconvincing. In the experiments, the authors claim that they implement RotatE [1] in their setting to make a fair comparison. However, with their setting, the performance of RotatE is much worse than that in the original paper [1]. Therefore, the experiments might be unfair to RotatE. 3. There are some typos in this paper. For example, in Line 11 of Section 4.3, the comma should be a period; in Section 5, the \"Dissuasion of Results\" should be \"Discussion of Results\". [1] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019. ", "rating": "3: Weak Reject", "reply_text": "Thank you for the valuable comments . Comment : '' The motivation of TransComplEx and why it works are unclear in the paper '' Response : According to our theories , TransComplEx has fewer limitations than the TransE model . Comment : '' The experiments might be unconvincing . In the experiments , the authors claim that they implement RotatE [ 1 ] in their setting to make a fair comparison . However , with their setting , the performance of RotatE is much worse than that in the original paper [ 1 ] . Therefore , the experiments might be unfair to RotatE . '' Response : Comparison of different models in completely different settings does not give a proper conclusion . However , following the raised point , we additionally compared the models with RotatE in a bigger setting . We saw the same patterns in the results which confirm our previous conclusions . We will report the results of the new experiments in a bigger setting in the appendix . Please note that we couldn \u2019 t use exactly the same setting of RotatE due to limitations in our infrastructure . Comment : `` There are some typos in this paper . For example , in Line 11 of Section 4.3 , the comma should be a period ; in Section 5 , the `` Dissuasion of Results '' should be `` Discussion of Results '' Response : Thank you , we fixed the typos ."}, "2": {"review_id": "HJxKhyStPH-2", "review_text": "The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by chosing more appropriate loss functions. Furthermore, the paper proposes TransComplEx -- an adaption of ideas from ComplEx/HolE to TransE -- to mitigate issues that can not be overcome by a simply chosing a different loss. Analyzing the behavior and short-comings of commonly-used models can be an important contribution to advance the state-of-the-art. This paper focuses on the performance of TransE, which is a popular representation learning approach for knowledge graph completion and as such fits well into ICLR. Unfortunately, the current version of the paper seems to have issues regarding methodology and novelty. Regarding the experimental evaluation: The paper compares the results of TransComplEx and the different loss functions to results that have previously been published in this field (directly, without retraining). However, it seems from Section 5 (Dataset), that this paper is using a modified dataset, as the TransE models are only trained on high-confidence triples. All prior work that I checked doesn't seem to do this, and hence the numbers are not comparable. Even more serious: Following again Section 5 (Dataset), it seems that the paper imputes all missing triples in the training set for symmetric and transitive relations (\"grounding\"). Hence, the models get to see _all_ true triples for these relation types and as such the models in this paper are trained on the test set. Regarding novelty: The short-comings of TransE and improvements to the loss have been discussed quite extensively in prior work. Using complex representations in TransComplEx seems also a straightforward application of the insights of ComplEx/Hole. As such, the main novelty would lie in the experimental results which, unfortunately, seem problematic.", "rating": "1: Reject", "reply_text": "Thank you for the valuable comments . Comments : '' it seems from Section 5 ( Dataset ) , that this paper is using a modified dataset '' : Response : Actually , we used the same dataset ( WN18rr , WN18 , FB15K-237 , FB15K ) that have been extensively used for evaluation of KGEs by others and these data sets do not contain any information about the confidence of triples . Therefore , our models are not trained on high confidence triples . In Section-5 ( Dataset ) , we already mentioned that the relation patterns ( i.e. , rules ) with a lower confidence value are removed . That does not have anything to do with triples and their potential level of confidence . We used the relation patterns ( rules ) extracted by AMIE . These relation patterns ( rules ) were used ( by doing grounding ) in RUGE to be injected into the learning process . Each relation patterns ( and not triples ) used in RUGE has a confidence value . RUGE also only used relation patterns with confidence higher than 80 % . We used the same dataset . We compare our models ( trained by different losses ) with two classes of models : 1 ) the models that have not used any relation patterns ( rules ) as background knowledge ( such as RotatE and ComplEx , TransE etc ) , for injection and 2 ) the models used a set of relation patterns ( rules ) as background knowledge to inject them into the embedding models during the learning process ( such as RUGE , KALE etc ) . To have a fair comparison , we trained the TransComplEx under two conditions . First : in order to compare with the first class of models , TransComplEx is trained using only triples ( Table2,3\u2026 TransComplEx row ) and we did not use or inject any relation patterns into it . Second : in order to compare with the second class of models , RPTransComplEx used relation patterns ( rules ) as background knowledge to be injected into the learning process such as RUGE which trained ComplEx using relation patterns with confidence higher than 80 % . Therefore , we included both of the models in the Table-2,3 to have a comprehensive evaluation . Moreover , comparing TransComplEx5 and RPTransComplEx5 ( which both are trained with the same loss function ) , we see that the results of TransComplEx trained using loss ( 5 ) are very close to RPTransComplEx5 . We conclude that the model which is trained on only triples with the loss 5 ( i.e.TransComplEx5 ) is capable of properly learning the most of patterns without using additional background knowledge ( relation patterns ) to be injected . We visualized the relation patterns losses convergence for TransComplEx5 and RPTransComplEx5 ( respectively , without and with relation pattern injected ) . The convergence of the losses confirms that TransComplEx can properly learn the relation patterns without using additional knowledge to be injected . We will include the figures of relation pattern losses convergence of TransComplEx and RPTransComplEx in the paper . We did new experiments on TransComplEx and TransE as well as RotatE with different loss functions with a bigger setting . we will include them in the paper . The results are consistent with the theories corresponding to the limitations of different models . In this experiment , we didn \u2019 t use any relation patterns and the models are trained only using triples ."}, "3": {"review_id": "HJxKhyStPH-3", "review_text": "The paper revisits limitations of relation-embedding models by taking losses into account in the derivation of these limitations. They propose and evaluate a new relation encoding (TransComplEx) and show that this encoding can address the limitations previously underlined in the literature when using the right loss. There seems to be merit in distinguishing the loss when studying relation encoding but I think the paper's analysis lacks proper rigor as-is. A loss minimization won't make equalities in (3) and (5) hold exactly, which the analysis do not account for. A rewriting of the essential elements of the different proofs could make the arguments clearer. Paper writing: * The manuscript should be improved with a thorough revision of the style and grammar. Example of mistakes include: extraneous or missing articles, incorrect verbs or tenses. * The 10-pages length is not beneficial, the recommended 8-pages could hold the same overall content. * The option list on page 8 is very difficult to read and should be put in a table, e.g. in appendix. * Parentheses are missing around many citations and equation references Theory: Equation (2) and (4) do not seem to bring much compared to the conditions in Table 1. Eq. (3) and (5) show \"a\" loss function rather than \"the\" loss function since multiple choices are possible. \\gamma_1 should be set to 0 when it is 0 rather than staying in the equations. * Minimizing the objective (3) and (5) will still not make the conditions in Table 1 hold exactly, because of slack variables. Experiments: - Can the authors provide examples of relations learned with RPTransComplEx# that go address the limitations L1...L6, validating experimentally the theoretical claims and showing that the gain with RPTransComplEx5 correspond to having learned these relations?", "rating": "3: Weak Reject", "reply_text": "Thank you for the valuable comments . Comment : `` A loss minimization wo n't make equalities in ( 3 ) and ( 5 ) hold exactly ... '' Response : In the first step , we posed the conditions ( Table-1 ) to prove theories corresponding to the limitations of the score functions . We then showed that each of these conditions can be approximated by a loss function ( which is not unique ) . The shortcomings of previous works were that they posed limitations under conditions that have not been approximated by the loss function they used . Consequently , they attempted to either address the limitations that do not really exist , or try to show the capability of their model under those conditions that are not approximated by their loss functions , resulting in some of the existing theories and conclusions that are not valid . We presented four conditions that defined the region of truth ( the region that a triple is considered positive by the model ) . Under these four conditions , we reinvestigated the main limitations of translation based class of embedding model and posed new theories in this regard . In fact , these conditions are ideal conditions to prove theorems and we highlighted that they should be enforced by a proper loss function . In practice , however , such conditions are approximated by a loss function . In other words , we aim to satisfy the condition as much as possible ( but not necessarily fully ) and as we highlighted , there is not a unique loss function to satisfy them . Our work opens a new window for future contributions in the development of new loss functions in which new losses might be proposed to satisfy the conditions , or other existing loss functions/models can be reinvestigated in the light of our proposed conditions . We will add more experimental analysis to the paper ( histogram of scores of models trained by different losses , etc ) in order to show how each of the losses approximate the corresponding conditions . Comment : `` style and grammar revision `` Response : Following your suggestion , we would revise the paper . Comment : '' The 10-pages length is not beneficial ... '' Response : We will try to reduce the length of the paper as much as possible . Comment : '' The option list on page 8 is very difficult to read ... '' Response : We include the content in a table in the appendix Comment : '' Parentheses are missing around many citations ... '' Response : We further revise the paper considering this comment Comment : `` Equation ( 2 ) and ( 4 ) do not seem to bring much compared to the conditions '' Response : We included the conditions of the Table-1 as constraints for the optimizations of ( 2 ) and ( 4 ) . For positive samples , the corresponding constraints enforce conditions to be held . However , for negative samples , we do not intend to include a hard boundary to define the region of negative samples , since some false negative samples are generated during the process of random negative sampling . Instead , we used a soft boundary for the negative samples by adding slack variables to mitigate the negative effect of noise . The goal is to approximate the conditions of Table-1 while addressing the noise in the training data . The optimal solutions ( embedding parameters ) for the optimization should be in the region defined by constraints . Therefore , the conditions are approximately satisfied . We experimentally show the value of loss function in order to approximate the conditions . Comment : '' \\gamma_1 should be set to 0 when it is 0 rather than staying in the equations . '' Response : We revise the paper following the comment . We decided to write it in a compact way , that is why we included two conditions in one formulation of loss while mentioning that setting \\gamma_1 to 0 approximates the condition ( a ) and setting \\gamma_1 to a positive ( non-zero ) value approximates the condition ( b ) for equation ( 3 ) . Comment : '' Minimizing the objective ( 3 ) and ( 5 ) will still not make the conditions in Table 1 hold exactly '' Response : As we mentioned earlier , the goal is to approximate the ideal conditions that we posed in theory , by a loss function ( not to satisfy them strictly ) . The quality of approximation of each of the conditions by each of the loss function is a new research direction that we are following as future work . Despite this fact , we will add a few results of some of our experiments to show the quality of approximation using the losses . The experiments show that the losses of ( 3 ) and ( 5 ) are properly converged to very small values , showing that the ideal conditions are properly approximated by the losses . Comment : '' Experiments : Can the authors provide examples of relations learned ... '' Response : Thank you for the important point . We are running experiments confirming our theories . We will include them in the paper . Our experiments confirm that most of the relation patterns are properly learned by the model ( and for some of them even them without injection ) , showing the value of loss function ."}}