{"year": "2020", "forum": "S1lvn0NtwH", "title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "decision": "Reject", "meta_review": "This paper presents an understudied bias known to exist in the learning patterns of children, but not present in trained NN models.  This bias is the mutual exclusivity bias: if the child already knows the word for an object, they can recognize that the object is likely not the referent when a new word is introduced.  So that is, the names of objects are mutually exclusive. \n\nThe authors and reviewers had a healthy discussion. In particular, Reviewer 3 would have liked to have seen a new algorithm or model proposed, as well as an analysis of when ME would help or hurt.  I hope these ideas can be incorporated into a future submission of this paper.", "reviews": [{"review_id": "S1lvn0NtwH-0", "review_text": "This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks. In general, the whole paper tries to tell a very interesting, and good story. The paper is very well organized and written. However, I have the following concerns. 1, the ME problem is quite similar to the concept ontology, e.g. , a \u201cDalmatian,\u201d a \u201cdog\u201d, or a \u201cmammal\u201d. So what\u2019s the key difference? Hierarchical learners can avoid this problem. 2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning. The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian. It\u2019s probably a bit unfair or misleading to claim neural networks suffering from ME bias. 3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments. Please give more explanations. 4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues? 5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN, the ME bias will be solved. Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning. 6, the experimental design of Sec. 4.2 is also a bit unfair. It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this task. Of course, this common NNs can not address it. ---- I read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.", "rating": "6: Weak Accept", "reply_text": "Thanks for your feedback . We reply to all reviewers jointly in our comments above ."}, {"review_id": "S1lvn0NtwH-1", "review_text": "Summary: This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation. Authors argue that ME bias could help the model to handle new classes and rare events better. My comments: I very much enjoyed reading this paper. I support accepting this paper. It highlights one of the missing inductive biases in ML and proposes it as a challenge. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non-NN results too. I see this is a challenge for MLE than DNNs. 1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training? 2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail. 3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain. 4. Are the authors willing to release the code and data to reproduce the results? Minor comments: 1. Page 3: second para, line 4: \u201cour aim is to study\u201d 2. Page 5: last line: estimate for -> estimated for 3. Section 4.2: 3rd line: \u201cthe class for the from\u201d ===================================================== After rebuttal: I have read the authors' response and I stand by my decision. ", "rating": "8: Accept", "reply_text": "Thank you for your supportive review . We answer the specific queries below and have also added them to the revised version of the paper . 1.We found that the entropy regularizer produces an ME score that stays constant across training , at the cost of the model being less confident about predictions made for seen classes . We added details regarding this condition to the manuscript . 2.The base rate is the probability of observing a new word in the target at that particular point in training . We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word . Thus , the base rate at time t in training is defined as : $ $ P ( \\text { new in target at t } ) = \\frac { \\text { # of unseen sentences in target with new words } } { \\text { # of unseen sentences } } $ $ 3 . In Section 4.2 , we use \u201c new \u201d to refer to the set of all the unseen classes at a particular timepoint t. For the classifier , P ( N|t ) is calculated by adding the probabilities the model assigns to all the \u201c new \u201d classes when iterating through the remaining corpus ( similar to Equation 1 in our paper ) . For the dataset , we compute P ( N|t ) by sampling all unseen images in the corpus and compute the proportion from \u201c new \u201d classes given their ground truth labels . 4.We will release our code and data with the publication of the paper . Most of our experiments are easy to replicate as they use standard datasets , models , loss functions and optimizers . We sincerely hope that our challenge and these resources will stimulate progress in this area . Please also see above where we write a general response to all reviews ."}, {"review_id": "S1lvn0NtwH-2", "review_text": "*** Increased to weak accept after discussion of merits of ME bias was improved in the paper *** This paper investigates whether neural networks exhibit a \u2018mutual exclusivity (ME) bias\u2019, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data. The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning. While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias. The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example. Comments / questions: * The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples. Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes. * For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples? * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically. It\u2019s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes). So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes? Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it. * The authors say that \u201cME can be generalized from applying to 'novel versus familiar\u2019 stimuli to instead handling \u2018rare versus frequent\u2019 stimuli\u201d and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above. It\u2019s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically. * It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not. Minor comments /questions not affecting review: * Is the acronym ME pronounced like the word \u201cme\u201d or is it spelled out \u201cM-E\u201d? If the latter, then all cases of \u201ca ME bias\u201d should be corrected to \u201can ME bias\u201d. * Section 4.2 line 3: \u201csample the class [for the] from a power law distribution\"", "rating": "6: Weak Accept", "reply_text": "We thank you for your constructive feedback . We reply to all reviews in a general response above ."}], "0": {"review_id": "S1lvn0NtwH-0", "review_text": "This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks. In general, the whole paper tries to tell a very interesting, and good story. The paper is very well organized and written. However, I have the following concerns. 1, the ME problem is quite similar to the concept ontology, e.g. , a \u201cDalmatian,\u201d a \u201cdog\u201d, or a \u201cmammal\u201d. So what\u2019s the key difference? Hierarchical learners can avoid this problem. 2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning. The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian. It\u2019s probably a bit unfair or misleading to claim neural networks suffering from ME bias. 3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments. Please give more explanations. 4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues? 5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN, the ME bias will be solved. Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning. 6, the experimental design of Sec. 4.2 is also a bit unfair. It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this task. Of course, this common NNs can not address it. ---- I read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.", "rating": "6: Weak Accept", "reply_text": "Thanks for your feedback . We reply to all reviewers jointly in our comments above ."}, "1": {"review_id": "S1lvn0NtwH-1", "review_text": "Summary: This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation. Authors argue that ME bias could help the model to handle new classes and rare events better. My comments: I very much enjoyed reading this paper. I support accepting this paper. It highlights one of the missing inductive biases in ML and proposes it as a challenge. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non-NN results too. I see this is a challenge for MLE than DNNs. 1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training? 2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail. 3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain. 4. Are the authors willing to release the code and data to reproduce the results? Minor comments: 1. Page 3: second para, line 4: \u201cour aim is to study\u201d 2. Page 5: last line: estimate for -> estimated for 3. Section 4.2: 3rd line: \u201cthe class for the from\u201d ===================================================== After rebuttal: I have read the authors' response and I stand by my decision. ", "rating": "8: Accept", "reply_text": "Thank you for your supportive review . We answer the specific queries below and have also added them to the revised version of the paper . 1.We found that the entropy regularizer produces an ME score that stays constant across training , at the cost of the model being less confident about predictions made for seen classes . We added details regarding this condition to the manuscript . 2.The base rate is the probability of observing a new word in the target at that particular point in training . We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word . Thus , the base rate at time t in training is defined as : $ $ P ( \\text { new in target at t } ) = \\frac { \\text { # of unseen sentences in target with new words } } { \\text { # of unseen sentences } } $ $ 3 . In Section 4.2 , we use \u201c new \u201d to refer to the set of all the unseen classes at a particular timepoint t. For the classifier , P ( N|t ) is calculated by adding the probabilities the model assigns to all the \u201c new \u201d classes when iterating through the remaining corpus ( similar to Equation 1 in our paper ) . For the dataset , we compute P ( N|t ) by sampling all unseen images in the corpus and compute the proportion from \u201c new \u201d classes given their ground truth labels . 4.We will release our code and data with the publication of the paper . Most of our experiments are easy to replicate as they use standard datasets , models , loss functions and optimizers . We sincerely hope that our challenge and these resources will stimulate progress in this area . Please also see above where we write a general response to all reviews ."}, "2": {"review_id": "S1lvn0NtwH-2", "review_text": "*** Increased to weak accept after discussion of merits of ME bias was improved in the paper *** This paper investigates whether neural networks exhibit a \u2018mutual exclusivity (ME) bias\u2019, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data. The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning. While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias. The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example. Comments / questions: * The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples. Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes. * For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples? * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically. It\u2019s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes). So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes? Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it. * The authors say that \u201cME can be generalized from applying to 'novel versus familiar\u2019 stimuli to instead handling \u2018rare versus frequent\u2019 stimuli\u201d and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above. It\u2019s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically. * It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not. Minor comments /questions not affecting review: * Is the acronym ME pronounced like the word \u201cme\u201d or is it spelled out \u201cM-E\u201d? If the latter, then all cases of \u201ca ME bias\u201d should be corrected to \u201can ME bias\u201d. * Section 4.2 line 3: \u201csample the class [for the] from a power law distribution\"", "rating": "6: Weak Accept", "reply_text": "We thank you for your constructive feedback . We reply to all reviews in a general response above ."}}