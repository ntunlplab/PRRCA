{"year": "2019", "forum": "r1g7y2RqYX", "title": "Label Propagation Networks", "decision": "Reject", "meta_review": "This  paper is on graph based semi-supervised learning where the goal is to develop an approach to jointly the node labeling function together with the edge weights. A natural way to formulate this problem as a bi-level optimization problem. However, the authors claim that this approach introduces two main difficulties: (a)  the \"upper\" objective function is itself the solution to the \"lower\" optimization problem (Eq. (2)), and (b) optimization is challenging (Eq. (3)). The AC disagrees. Firstly, there is a close connection between the constrained version and the regression version of the problem (e.g., Belkin, Matveeva and Niyogi) -- the former is infact a special case of the latter for a certain choice of regularization parameter. The latter reduces to an linear system. The outer problem can be optimized using standard gradient descent using the implicit function theorem trick common in bilevel optimization. Reviewers have also raised concerns about clarity, and experimental support in this paper and comparisons with related work.  ", "reviews": [{"review_id": "r1g7y2RqYX-0", "review_text": "**** After Revision *********** I thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \\tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. **** After Revision *********** This paper proposes a framework for non-linear label propagation where the weights are learned simultaneously. There are model specific and experimental setup design decisions that require justification. There also needs to be a number of ablation studies to justify the effectiveness of the different components of this framework. Finally, there seems to be an insufficient comparison (both experimentally and theoretically) to the large amount of related literature. - What is the total number of parameters in the proposed network? Please clarify how this is \"relatively few parameters\" as compared to other methods. - Please compare how your method for learning weights relates to the following papers and the references therein [1,2] [1] Online Learning of Multiple Tasks and Their Relationships. Saha et al, AISTATS, 2011. [2] Convex Learning of Multiple Tasks and their Structure, Cilberto et al, 2015. - It would be good to have an ablation study in order to discern what is the contribution of learning the weights vs propagating labels (instead of embeddings). - For clarity, please specify that \\theta are the parameters to be learned. - Please explain the intuition of using entropy and KL divergence for the attention weights. Shouldn't the attention for an edge be inversely proportional to the entropy i.e. the attention should be higher if the neighboring node's label is more certain? - Instead of the bifurcation mechanism proposed in section 3.2, isn't it possible to use a threshold to round the resulting prediction to a hard label? - In equation 13, are the hyper-parameters a, b tuned using cross-validation? Can't we learn the \\tau in the same training procedure? Please justify this design decision? - What is the performance if the loss in equation 14 is replaced by the standard empirical loss? There needs to be an ablation study on this. - If the node features are available, how are they used in this framework? - In the experimental section, why is k chosen to be equal to 1%? Please show results while varying this. - Please justify the line \"parameterizes w using a small number (~20) of informative features based on the raw features (e.g., dimensionality reduction), the graph (e.g., edge betweenness), and the labeled set (e.g., distance from labeled nodes). \" Isn't it possible to get similar performance by reducing the number of parameters so that model doesn't overfit? - Please clearly state what is the difference in the framework from the Kipf and Welling, 2016 paper? - Why isn't there a comparison to methods like Graph-Sage? - Please explain this line \"LPNnobif degrades with large T, and even \\tau slightly above 1 makes a difference\" - Finally, please explain the trend in the results in Table 1. For example, why is the performance of the proposed method poor on the Flickr dataset, but better on the DBLP dataset? - It would good to have uncertainty estimates for the results reported in Table 1. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments . Below please find details describing our modeling and experimental choices . The updated paper includes an enriched related materials section and GAT as a baseline . Fig.4 quantifies the added value of the bifurcation component . \u201c Total number of parameters , and compared to other methods ? \u201d The total number of parameters is between 12 and 44 , depending on dataset and experimental setting . Based on their published codes , GCN has 23,040 and GAT has 92,391 for CoRA . We use 38 . These include : - Weights : 30 edge features , some are per-class ( Appendix B ) - Attention : 2 parameters per class , one for entropy and one for divergence ( Sec.3.1 ) - Bifurcation : 2 parameters ( Eq . ( 13 ) ) \u201c Relation to papers by Saha et al.and Cilberto et al. \u201d : The above papers propose methods for multi-task learning ( Saha et.al for online , Ciliberto et al.for batch ) , and consider relations between tasks , which are fixed and given as input . Our paper focuses on semi-supervised learning , and considers weighted relations between examples , which are learned . \u201c Discern contribution of learning the weights vs propagating labels instead of embeddings \u201d : The LP baseline , which we generalize , propagates labels with fixed weights . Fig.4 shows the how adding bifurcation ( LPN_bif ) compares to only learning weights ( LPN_nobif ) . \u201c Please specify that \\theta are learned \u201d : We will clarify this . \u201c Entropy and divergence - inversely proportional ? \u201d : We use * negative * entropy and * negative * divergence ( see Eq . ( 11 ) and above ) . This aligns with your intuition . \u201c Use threshold for rounding instead of bifurcation \u201d : \u201c Hard \u201d rounding is non-differentiable , and can not be used efficiently with back-propagation . Bifurcation is differentiable , and much more expressive than simple rounding . It can interpolate between \u201c rounding up \u201d ( large \\tau ) and \u201c rounding down \u201d to uniform ( \\tau -- > zero ) , or result in no rounding ( \\tau=1 ) . \u201c Are a , b tuned using cross-validation ? Ca n't we learn them ? \u201d : Both a and b ( =\\theta^\\tau ) are learned , not tuned . \u201c Can \u2019 t the loss in Eq . ( 14 ) be replaced by the standard empirical loss ? \u201d : Unfortunately , no . With the standard empirical loss , Eq . ( 14 ) becomes degenerate . This is because the it compares the true and predicted labels of the labeled nodes . As in LP , predicted labels of labeled nodes are set to their true labels , so the loss is always 0 . This is also noted in Zhang & Lee ( 2007 ) . \u201c If available , how are node features used ? \u201d : Node features are used to parameterize edge weights ( Eq . ( 7 ) & appendix B ) . Sec.2.2 now includes more details . \u201c Why is k chosen to be equal to 1 % ? \u201d : We chose 1 % as it is a reasonable number in the range of those used in GCN and others ( 3.6 % , 5.2 % , and 0.3 % ) . Note that we re-train all baselines on all datasets over 10 random splits in two experimental settings , which requires considerable computational resources . \u201c Reduce features to avoid overfitting \u201d : The overall number of parameters we use is very small , especially compared to other methods . We observed that reducing the number of features only degrades performance . \u201c Differences from GCN \u201d : There are several notable differences , most of which become apparent when comparing the form of classifiers proposed by each method . The classifier of GCN is f ( x , W ; \\theta ) , while ours is f ( y ; W ( x ; \\theta ) ) . This means that : - GCN operates on features , while we propagate labels . The benefit of propagating labels is that labeled information is used not only to penalize wrong predictions ( in the loss ) , but also to * generate * predictions . This is the hallmark of the LP algorithm , which we adopt . - GCN assumes edge weights W are given as input . These are typically set heuristically . In contrast , our method learns weights by optimizing predictive accuracy . - GCN uses node features x to generate embeddings , and hence does not apply to tasks where node features are not available . For our method , when features are available , we use them to parametrize W. When they are not available , we use the information-gated attention mechanism . \u201c No comparison to GraphSage : \u201d GraphSage applies to an inductive learning setting . Our method is designed for a transductive learning setting . \u201c Explain \u2018 LPN_nobif degrades with large T \u2019 \u201d : Fig.4 shows that without bifurcation , accuracy can be sensitive to T. Adding bifurcation provides robustness . This is true even when the effects of bifurcation are subtle ( \\tau~=1 ) . \u201c Trend in Table 1 ; why is performance poor for Flickr ? \u201d : Flickr is dense compared to others ( edge/node ratio of 60:1 , vs. between 1.5:1 and 5.3:1 ) . To reduce the computational load , we sparsify the graph , which may explain the low accuracy . \u201c Uncertainty estimates \u201d : Now added ."}, {"review_id": "r1g7y2RqYX-1", "review_text": "This paper presents an interesting idea for the following task: given a graph and a subset of labelled nodes, infer the labels on the remaining nodes. Here the authors will make prediction for absent labels based on local averages on the graph of the neighbouring soft labels. The main originality is that the local average is weighted and the weights are learnt. I had trouble understanding the details of the algorithm and the authors should be more careful in their description of the algorithm. Some points to clarify: - section 3.1, I am not sure to understand the 'dynamic weights'. The main point here seems to be the use of an attention mechanism (which does not vary in time) applied to inputs varying in time? - section 3.2, I do not understand equation (13). What is \\theta^\\tau, it does not appear in the right-hand term? I think that using the term time is misleading. Time might refer to epochs in an optimization process, whereas time in Section 3 seems to refer to a number of layers as described in equation (6). Please, be more explicit on the use of raw features. How are the similarities described in appendix B incorporated in the loss? Overall, I think this paper requires a lot of clarification before being published.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments , please see our responses below . \u201c Not sure I understand dynamic weights \u201d : Yes , this is correct . The attention mechanism turns incoming soft labels ( h^t ) into edge weights ( a^ { t+1 } ) . For given parameters \\theta^\\alpha , the attention function \\alpha is indeed fixed , but since soft labels change as they pass through the layers , so do the edge weights . When viewed as a label-propagation mechanism , weights can be thought of as changing over time . \u201c What is \\theta^\\tau in Eq . ( 13 ) : \\theta^\\tau is defined just above Eq . ( 13 ) - it is simply the concatenation of a and b . The left hand side can be written as \\tau ( t ; a , b ) . In general , we use \\theta to denote parameters , and the corresponding superscript to denote what they parameterize . We will make this clearer . \u201c The term \u2018 time \u2019 is misleading \u201d : We apologize for this inclarity . Indeed , we use time , iterations , and number of layers ( or depth ) interchangeably . This is because the network \u2019 s layers simulate the iterations of LP , which we think of as applied over time . We will clarify this . \u201c How are raw features incorporated in the loss ? \u201d : We use \u201c raw \u201d features to parameterize edge weights . This means that the weight w_ { ij } of an edge ( i , j ) is a function of various edge measures \\phi_ { ij } , some of which are derived from raw node features ( see appendix B ) . The function is parameterized by \\theta^\\phi - the exact form is given in Eq . ( 7 ) .Edge weights then determine the predicted labels ( through the label propagation mechanism ) , and predictions are plugged into the loss function in Eq . ( 14 ) , where they are evaluated against the ground-truth labels ."}, {"review_id": "r1g7y2RqYX-2", "review_text": "Summary This paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps. Strong points - Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right). - Good performance. - The paper is generally well written. Concerns - Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost. As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow. - The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail. - The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used? - Too many hyperparameters to tune. Minor points - I think the line above Eq (4) should be like \\tilde w_ij = w_ij / sum_k w_ik. - Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper. - In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments , please see our responses below . \u201c Convergence issues ; the algorithm may go wrong ; bifurcation rate can be too slow/fast : \u201d Indeed , a limited number of layers might give an exact solution to the quadratic criterion in Eq . ( 2 ) .However , our results imply that using few iterations with learned weights outperforms a converged solution using heuristic weights . Because our model optimizes accuracy , each point point in Fig , 4 corresponds not only to a different T but also to different learned weights , making it difficult to compare convergence across points . While bifurcation can potentially change the rate of convergence , it does not have to . Since the bifurcation parameters ( \\theta^\\tau ) are learned , and since \\theta^\\tau = 0 implies no change in rates , bifurcation will be used ( by learning that \\theta^\\tau ! = 0 ) only if it results in better performance . \u201c Is introducing entropy always helpful ? \u201d : Since the entropy parameters ( \\theta^e ) are learned , and because \\theta^e = 0 implies uniform weights ( like in LP ) , the model will learn to use entropy only if it results in improved performance . The same applies to KL divergence . \u201c Difference in experimental setting and results from GCN : \u201d The main differences between our setup and GCN are the number of labeled nodes and how they can be used . In GCN labeled nodes are partitioned into training ( 20 nodes per class ) and validation ( 500 additional nodes ) . While training sets are kept small ( 3.6 % for Citeseer , 5.2 % for CoRA , and 0.3 % for pubmed ) , the total number of labeled nodes ( training+validation ) is rather large ( 18.6 % , 23.6 % , and 2.8 % of all nodes , respectively ) , and a huge portion of labeled nodes is pre-allocated for validation ( 81 % , 78 % , and 89 % of labeled data , respectively ) and so can only be used for tuning , not training . This puts methods that require little or no tuning ( such as LP , over which our method is built ) at an immediate disadvantage . In our view , methods should be free to choose how to best use the available labeled data , be it for training , tuning , or other . We have experimented in the GCN setting , allowing our model to use all labeled data for training . While our model outperforms GCN ( 83.4 vs. 79.6 on CoRA and 69.8 vs. 67.5 on Citeseer , averaged over 10 random splits ) , this may also seem unfair , since other baselines might also benefit from a allocation of the labeled budget . There also several other issues with the \u201c standard \u201d setting used in GCN - see the recent paper by Shchur et al . ( 2018 ) [ 1 ] for details . Due to the above , our solution was to revert to the classic SSL experimental setting used in numerous papers , where a fixed percentage of labeled nodes are drawn uniformly at random . We used 1 % as it is a reasonable number in the range of the GCN setting , and allowed us to fully train all baselines for all settings and datasets over 10 random splits in a reasonable amount of time . \u201c Number of GCN layers : \u201d We use the published GCN code which has one graph-convolution layer and is used in their paper . \u201c Too many hyperparameters to tune \u201d : Please note that we have only * one * network-related hyper-parameter that requires tuning - the number of layers ( T ) . As Fig.4 shows , choosing T can be made robust by using bifurcation . All other model parameters ( denoted by \\theta ) are learned . The regularization coefficient \\lambda is chosen by standard cross validation . \u201c Minor points \u201d : Thank you for these , we will fix them . [ 1 ] Shchur , O. , Mumme , M. , Bojchevski , A. , & G\u00fcnnemann , S. ( 2018 ) . Pitfalls of Graph Neural Network Evaluation . arXiv preprint arXiv:1811.05868 ."}], "0": {"review_id": "r1g7y2RqYX-0", "review_text": "**** After Revision *********** I thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \\tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. **** After Revision *********** This paper proposes a framework for non-linear label propagation where the weights are learned simultaneously. There are model specific and experimental setup design decisions that require justification. There also needs to be a number of ablation studies to justify the effectiveness of the different components of this framework. Finally, there seems to be an insufficient comparison (both experimentally and theoretically) to the large amount of related literature. - What is the total number of parameters in the proposed network? Please clarify how this is \"relatively few parameters\" as compared to other methods. - Please compare how your method for learning weights relates to the following papers and the references therein [1,2] [1] Online Learning of Multiple Tasks and Their Relationships. Saha et al, AISTATS, 2011. [2] Convex Learning of Multiple Tasks and their Structure, Cilberto et al, 2015. - It would be good to have an ablation study in order to discern what is the contribution of learning the weights vs propagating labels (instead of embeddings). - For clarity, please specify that \\theta are the parameters to be learned. - Please explain the intuition of using entropy and KL divergence for the attention weights. Shouldn't the attention for an edge be inversely proportional to the entropy i.e. the attention should be higher if the neighboring node's label is more certain? - Instead of the bifurcation mechanism proposed in section 3.2, isn't it possible to use a threshold to round the resulting prediction to a hard label? - In equation 13, are the hyper-parameters a, b tuned using cross-validation? Can't we learn the \\tau in the same training procedure? Please justify this design decision? - What is the performance if the loss in equation 14 is replaced by the standard empirical loss? There needs to be an ablation study on this. - If the node features are available, how are they used in this framework? - In the experimental section, why is k chosen to be equal to 1%? Please show results while varying this. - Please justify the line \"parameterizes w using a small number (~20) of informative features based on the raw features (e.g., dimensionality reduction), the graph (e.g., edge betweenness), and the labeled set (e.g., distance from labeled nodes). \" Isn't it possible to get similar performance by reducing the number of parameters so that model doesn't overfit? - Please clearly state what is the difference in the framework from the Kipf and Welling, 2016 paper? - Why isn't there a comparison to methods like Graph-Sage? - Please explain this line \"LPNnobif degrades with large T, and even \\tau slightly above 1 makes a difference\" - Finally, please explain the trend in the results in Table 1. For example, why is the performance of the proposed method poor on the Flickr dataset, but better on the DBLP dataset? - It would good to have uncertainty estimates for the results reported in Table 1. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments . Below please find details describing our modeling and experimental choices . The updated paper includes an enriched related materials section and GAT as a baseline . Fig.4 quantifies the added value of the bifurcation component . \u201c Total number of parameters , and compared to other methods ? \u201d The total number of parameters is between 12 and 44 , depending on dataset and experimental setting . Based on their published codes , GCN has 23,040 and GAT has 92,391 for CoRA . We use 38 . These include : - Weights : 30 edge features , some are per-class ( Appendix B ) - Attention : 2 parameters per class , one for entropy and one for divergence ( Sec.3.1 ) - Bifurcation : 2 parameters ( Eq . ( 13 ) ) \u201c Relation to papers by Saha et al.and Cilberto et al. \u201d : The above papers propose methods for multi-task learning ( Saha et.al for online , Ciliberto et al.for batch ) , and consider relations between tasks , which are fixed and given as input . Our paper focuses on semi-supervised learning , and considers weighted relations between examples , which are learned . \u201c Discern contribution of learning the weights vs propagating labels instead of embeddings \u201d : The LP baseline , which we generalize , propagates labels with fixed weights . Fig.4 shows the how adding bifurcation ( LPN_bif ) compares to only learning weights ( LPN_nobif ) . \u201c Please specify that \\theta are learned \u201d : We will clarify this . \u201c Entropy and divergence - inversely proportional ? \u201d : We use * negative * entropy and * negative * divergence ( see Eq . ( 11 ) and above ) . This aligns with your intuition . \u201c Use threshold for rounding instead of bifurcation \u201d : \u201c Hard \u201d rounding is non-differentiable , and can not be used efficiently with back-propagation . Bifurcation is differentiable , and much more expressive than simple rounding . It can interpolate between \u201c rounding up \u201d ( large \\tau ) and \u201c rounding down \u201d to uniform ( \\tau -- > zero ) , or result in no rounding ( \\tau=1 ) . \u201c Are a , b tuned using cross-validation ? Ca n't we learn them ? \u201d : Both a and b ( =\\theta^\\tau ) are learned , not tuned . \u201c Can \u2019 t the loss in Eq . ( 14 ) be replaced by the standard empirical loss ? \u201d : Unfortunately , no . With the standard empirical loss , Eq . ( 14 ) becomes degenerate . This is because the it compares the true and predicted labels of the labeled nodes . As in LP , predicted labels of labeled nodes are set to their true labels , so the loss is always 0 . This is also noted in Zhang & Lee ( 2007 ) . \u201c If available , how are node features used ? \u201d : Node features are used to parameterize edge weights ( Eq . ( 7 ) & appendix B ) . Sec.2.2 now includes more details . \u201c Why is k chosen to be equal to 1 % ? \u201d : We chose 1 % as it is a reasonable number in the range of those used in GCN and others ( 3.6 % , 5.2 % , and 0.3 % ) . Note that we re-train all baselines on all datasets over 10 random splits in two experimental settings , which requires considerable computational resources . \u201c Reduce features to avoid overfitting \u201d : The overall number of parameters we use is very small , especially compared to other methods . We observed that reducing the number of features only degrades performance . \u201c Differences from GCN \u201d : There are several notable differences , most of which become apparent when comparing the form of classifiers proposed by each method . The classifier of GCN is f ( x , W ; \\theta ) , while ours is f ( y ; W ( x ; \\theta ) ) . This means that : - GCN operates on features , while we propagate labels . The benefit of propagating labels is that labeled information is used not only to penalize wrong predictions ( in the loss ) , but also to * generate * predictions . This is the hallmark of the LP algorithm , which we adopt . - GCN assumes edge weights W are given as input . These are typically set heuristically . In contrast , our method learns weights by optimizing predictive accuracy . - GCN uses node features x to generate embeddings , and hence does not apply to tasks where node features are not available . For our method , when features are available , we use them to parametrize W. When they are not available , we use the information-gated attention mechanism . \u201c No comparison to GraphSage : \u201d GraphSage applies to an inductive learning setting . Our method is designed for a transductive learning setting . \u201c Explain \u2018 LPN_nobif degrades with large T \u2019 \u201d : Fig.4 shows that without bifurcation , accuracy can be sensitive to T. Adding bifurcation provides robustness . This is true even when the effects of bifurcation are subtle ( \\tau~=1 ) . \u201c Trend in Table 1 ; why is performance poor for Flickr ? \u201d : Flickr is dense compared to others ( edge/node ratio of 60:1 , vs. between 1.5:1 and 5.3:1 ) . To reduce the computational load , we sparsify the graph , which may explain the low accuracy . \u201c Uncertainty estimates \u201d : Now added ."}, "1": {"review_id": "r1g7y2RqYX-1", "review_text": "This paper presents an interesting idea for the following task: given a graph and a subset of labelled nodes, infer the labels on the remaining nodes. Here the authors will make prediction for absent labels based on local averages on the graph of the neighbouring soft labels. The main originality is that the local average is weighted and the weights are learnt. I had trouble understanding the details of the algorithm and the authors should be more careful in their description of the algorithm. Some points to clarify: - section 3.1, I am not sure to understand the 'dynamic weights'. The main point here seems to be the use of an attention mechanism (which does not vary in time) applied to inputs varying in time? - section 3.2, I do not understand equation (13). What is \\theta^\\tau, it does not appear in the right-hand term? I think that using the term time is misleading. Time might refer to epochs in an optimization process, whereas time in Section 3 seems to refer to a number of layers as described in equation (6). Please, be more explicit on the use of raw features. How are the similarities described in appendix B incorporated in the loss? Overall, I think this paper requires a lot of clarification before being published.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments , please see our responses below . \u201c Not sure I understand dynamic weights \u201d : Yes , this is correct . The attention mechanism turns incoming soft labels ( h^t ) into edge weights ( a^ { t+1 } ) . For given parameters \\theta^\\alpha , the attention function \\alpha is indeed fixed , but since soft labels change as they pass through the layers , so do the edge weights . When viewed as a label-propagation mechanism , weights can be thought of as changing over time . \u201c What is \\theta^\\tau in Eq . ( 13 ) : \\theta^\\tau is defined just above Eq . ( 13 ) - it is simply the concatenation of a and b . The left hand side can be written as \\tau ( t ; a , b ) . In general , we use \\theta to denote parameters , and the corresponding superscript to denote what they parameterize . We will make this clearer . \u201c The term \u2018 time \u2019 is misleading \u201d : We apologize for this inclarity . Indeed , we use time , iterations , and number of layers ( or depth ) interchangeably . This is because the network \u2019 s layers simulate the iterations of LP , which we think of as applied over time . We will clarify this . \u201c How are raw features incorporated in the loss ? \u201d : We use \u201c raw \u201d features to parameterize edge weights . This means that the weight w_ { ij } of an edge ( i , j ) is a function of various edge measures \\phi_ { ij } , some of which are derived from raw node features ( see appendix B ) . The function is parameterized by \\theta^\\phi - the exact form is given in Eq . ( 7 ) .Edge weights then determine the predicted labels ( through the label propagation mechanism ) , and predictions are plugged into the loss function in Eq . ( 14 ) , where they are evaluated against the ground-truth labels ."}, "2": {"review_id": "r1g7y2RqYX-2", "review_text": "Summary This paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps. Strong points - Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right). - Good performance. - The paper is generally well written. Concerns - Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost. As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow. - The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail. - The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used? - Too many hyperparameters to tune. Minor points - I think the line above Eq (4) should be like \\tilde w_ij = w_ij / sum_k w_ik. - Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper. - In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments , please see our responses below . \u201c Convergence issues ; the algorithm may go wrong ; bifurcation rate can be too slow/fast : \u201d Indeed , a limited number of layers might give an exact solution to the quadratic criterion in Eq . ( 2 ) .However , our results imply that using few iterations with learned weights outperforms a converged solution using heuristic weights . Because our model optimizes accuracy , each point point in Fig , 4 corresponds not only to a different T but also to different learned weights , making it difficult to compare convergence across points . While bifurcation can potentially change the rate of convergence , it does not have to . Since the bifurcation parameters ( \\theta^\\tau ) are learned , and since \\theta^\\tau = 0 implies no change in rates , bifurcation will be used ( by learning that \\theta^\\tau ! = 0 ) only if it results in better performance . \u201c Is introducing entropy always helpful ? \u201d : Since the entropy parameters ( \\theta^e ) are learned , and because \\theta^e = 0 implies uniform weights ( like in LP ) , the model will learn to use entropy only if it results in improved performance . The same applies to KL divergence . \u201c Difference in experimental setting and results from GCN : \u201d The main differences between our setup and GCN are the number of labeled nodes and how they can be used . In GCN labeled nodes are partitioned into training ( 20 nodes per class ) and validation ( 500 additional nodes ) . While training sets are kept small ( 3.6 % for Citeseer , 5.2 % for CoRA , and 0.3 % for pubmed ) , the total number of labeled nodes ( training+validation ) is rather large ( 18.6 % , 23.6 % , and 2.8 % of all nodes , respectively ) , and a huge portion of labeled nodes is pre-allocated for validation ( 81 % , 78 % , and 89 % of labeled data , respectively ) and so can only be used for tuning , not training . This puts methods that require little or no tuning ( such as LP , over which our method is built ) at an immediate disadvantage . In our view , methods should be free to choose how to best use the available labeled data , be it for training , tuning , or other . We have experimented in the GCN setting , allowing our model to use all labeled data for training . While our model outperforms GCN ( 83.4 vs. 79.6 on CoRA and 69.8 vs. 67.5 on Citeseer , averaged over 10 random splits ) , this may also seem unfair , since other baselines might also benefit from a allocation of the labeled budget . There also several other issues with the \u201c standard \u201d setting used in GCN - see the recent paper by Shchur et al . ( 2018 ) [ 1 ] for details . Due to the above , our solution was to revert to the classic SSL experimental setting used in numerous papers , where a fixed percentage of labeled nodes are drawn uniformly at random . We used 1 % as it is a reasonable number in the range of the GCN setting , and allowed us to fully train all baselines for all settings and datasets over 10 random splits in a reasonable amount of time . \u201c Number of GCN layers : \u201d We use the published GCN code which has one graph-convolution layer and is used in their paper . \u201c Too many hyperparameters to tune \u201d : Please note that we have only * one * network-related hyper-parameter that requires tuning - the number of layers ( T ) . As Fig.4 shows , choosing T can be made robust by using bifurcation . All other model parameters ( denoted by \\theta ) are learned . The regularization coefficient \\lambda is chosen by standard cross validation . \u201c Minor points \u201d : Thank you for these , we will fix them . [ 1 ] Shchur , O. , Mumme , M. , Bojchevski , A. , & G\u00fcnnemann , S. ( 2018 ) . Pitfalls of Graph Neural Network Evaluation . arXiv preprint arXiv:1811.05868 ."}}