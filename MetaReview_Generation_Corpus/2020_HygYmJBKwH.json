{"year": "2020", "forum": "HygYmJBKwH", "title": "YaoGAN: Learning Worst-case Competitive Algorithms from Self-generated Inputs", "decision": "Reject", "meta_review": "The authors propose an intriguing way to designing competitive online algorithms. However, the state of the paper and the provided evidence of the success of the proposed methodology is too preliminary to merit acceptance.", "reviews": [{"review_id": "HygYmJBKwH-0", "review_text": "Update to the Review after the rebuttal from the Authors: After carefully reviewing the responses by the authors especially on my concerns about the significance of solving an instance of a given problem and the improvement in the exposition of the ideas I would like to amend my earlier decision and recommend to accept. For completeness below is the original review. This paper introduces a framework to learn to generate solutions to online combinatorial optimization problems with worst case guarantees. The framework as the authors claim eliminates the need for manual hard to solve instance/data creation, which is necessary to teach the model to provide the aforementioned worst case guarantees. Therefore the main contribution of the paper can be said that this framework shows that it is possible to train a machine learning model, which can learn an algorithm to solve hard online combinatorial optimization problems and this training can be done without knowing much about the actual optimization problem domain. The only input required is the way to calculate the objective function of the actual problem. This contribution is demonstrated on two classes of problems: Ski-Rental and Fractional AdWords. The framework requires two neural networks one for solution generation agent and one for problem instance generation. These two networks are trained jointly from scratch and the underlying algorithm for the training is provided. Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems. Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach? This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem. Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making. Therefore I do not find being able to solve this problem as a supporting evidence for the contributions claimed. In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions. I find this important because for combinatorial optimization usually scale matters a lot. While a small instance of a problem can be solve by a general purpose solver quickly a small increase in the problem size can turn out to be intractable. When proposing a machine learning approach to such problems I would expect the model to scale better than pure optimization approach so that there would be demonstrable benefit. Although the paper proposes an interesting framework I would argue that it is a \u201cgreen apple\u201d in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance. Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution. In order to clarify the exposition the following are some questions: 1. Authors call the approach YaoGAN due to its structural similarity to GANs. I understand the fact that they are training two neural networks in an alternating scheme, which is similar to the GAN training. How can one evaluate the solutions generated by this framework similar to how GAN generators are evaluated? Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees? 2. The main technical contribution claim needs to be elaborated. I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation. 3. Authors claim there are two shortcomings of the previous method proposed in Kong et. al 2018. They need to elaborate how their method overcomes these issues better. 4. Authors state that fractional relaxation of combinatorial mainly integer optimization problems, which is accurate. Yet their approach is only able to solve the fractional version of the AdWords problem. In addition I agree with the fact that although continuous relaxations to integer optimization problems might provide insightful directions they usually employed to to prove bounds on the heuristic approaches. Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me. I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem. What are the shortcomings? 5.In Appendix A authors talk about no-regret dynamics, which are relevant. However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims. 6. In appendix C.2 authors provide additional plots for the Fractional AdWords problem. However, they retain from providing any intuition about them. In particular what is the conclusion to be drawn from Figure 5. This needs more elaboration. Is this way of training results expected? What is the lesson learned? 7.In Figure 8 they provide example data from experience array. What are the significance of these examples? How they help us understand the problem instance generation was actually able to find interesting instances? What kind of dynamics are under covered? These are not directly revealed by only looking at the pictures one needs more explanation to support the claims. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review . Please also see our high-level clarification above which we believe can help in better interpretation of our contribution . Some specific responses below : -- \u201c proposed method demonstrates that an instance of one class of problems , Fractional Adwords , can be learned to solve without domain expertise , however fails to prove that the approach would be beneficial for any other instances of the same problem. \u201d Please refer to our overall comments on this question ( and also a few more details in reply to Reviewer # 1 \u2019 s similar question ) . -- Comment on scale / speed for large instances of combinatorial optimization : The point of this work is only to see if ML can find optimal algorithms , and not about doing it faster than the known theoretical algorithms . Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers , since our problems are online , i.e. , the instance is not known beforehand , so there is no comparison to such \u201c general-purpose \u201d solvers . Thus we do n't compare to the running time of offline solvers , but to the worst-case competitive ratio of the optimal online algorithms . As mentioned in the comment , this approach may eventually lead to finding optimal or near-optimal algorithms for a problem ( not an instance of a problem ) for which no algorithm is known -- but this is outside the scope of this work future work . Again drawing the analogy of playing Go , the objective is mostly on training an agent that can make competitive moves rather than very fast moves , and there is no known \u201c general-purpose \u201d strategy to accomplish this . * Please also see reply to reviewer # 2 on a similar question of evaluating against other methods * -- \u201c Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent , i.e.problem instances are not generated by use of a machine learning model , which is one of the main claims the authors are making. \u201d Please see our high-level clarification on top ."}, {"review_id": "HygYmJBKwH-1", "review_text": "This papers tackles the following question. Is it possible to learn the \"most\" complex instance of a class of (combinatorial) problem while finding (or recovering) algorithms with strong minimax rate. This is very interesting and clearly a nice line of work (in theory though). The techniques used rely on GANs since it can be shown that finding the best (random) algorithm and the worst (deterministic) instance is equivalent to finding the worst random instance against the best deterministic algorithm. This is actually a direct consequence of any minmax theorem in game theory; the authors decided to credit that result to Yao (I tend to *strongly* disagree with that point as, even if he stated this fact in CS, this result was quite standard several decades before him - anyway.). Then this idea is evaluated in two examples. A toy problem (the ski rental) and a more or less concrete ones (adwords pb of Mehta). This is the major disappointment in the paper. The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract \"find algorithms with strong worst-case guarantees for online combinatorial optimization problems\". So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper). I believe that this paper is thus not in its final form and could be largely improved. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . Please also see our high-level clarification above which we believe can help in better interpretation of our contribution . Some specific responses below : Reviewer # 1 is absolutely right -- we don \u2019 t know yet how to scale this to more difficult combinatorial problems . But let \u2019 s clarify that statement a bit more : The ski-rental problem is often the first problem studied when teaching online algorithms , but it is certainly far from a \u201c toy problem \u201d when we wish to learn an algorithm from scratch . We apologize if we painted an incorrect picture by calling it a \u201c simple example \u201d and a \u201c staple introductory problem \u201d . It is easy to describe in that it has a single hidden parameter ( the length of the ski season ) and a single revealed parameter ( the cost of buying ) . It is a staple introductory problem because it is elegant and illuminates the essential difficulties in designing online algorithms : there is a nearly-trivial factor-2 competitive algorithm ( rent until you \u2019 ve spent $ B , then buy , so even if the ski season ends the next day , you \u2019 ve not spent more than twice the least possible amount ) , but the 1-1/e competitive ratio algorithm is quite creative and subtle , and serves as an introduction to the richness of the field of online algorithms . In fact , the Karlin et al . ( 1986 ) paper also introduced the notion of competitive analysis of online algorithms , and is probably the most-cited paper in this field . In some sense , this poses us the ideal challenge : can ML approaches discover creative and subtle \u201c solutions \u201d ( in our case , an algorithm ) ? On a more technical note , please note that our \u201c machinery \u201d of solving the two-player game is needed to discover an algorithm for the ski rental problem : if we don \u2019 t allow the players to alternate and reach an equilibrium , for any fixed distribution on the ski rental instances ( B , K ) , there is a deterministic algorithm that is optimal ( among all online algorithms ) , and the worst-case performance of that ( or any ) deterministic algorithm is * provably * limited by a factor of 2 ( i.e. , there exists some distribution on instances where it will fail badly ) . Also refer to our discussion on this in the high-level clarification at the top . The AdWords problem considered is actually a difficult combinatorial problem , and is an archetypal online combinatorial optimization problem that captures the class of problems solvable by one of the most powerful techniques in this area -- primal-dual algorithms , which have led to the state-of-the-art approximation algorithms for numerous hard online ( and offline ) optimization problems . In particular , it generalizes bipartite matching , historically one of the most significant combinatorial optimization problems ( led to the development of the classic Hungarian method , see https : //en.wikipedia.org/wiki/Hungarian_algorithm ) . We did water down our ambition in a few ways : Instead of producing an algorithm that works for inputs of all sizes , we focus on the case of 9x3 ( three advertisers , nine slots ) -- a fixed finite size ! This choice was arrived at based on the following criteria : what can we learn in a few hours of computation that \u2019 s still * well beyond * what can be achieved through exhaustive search ( for an algorithm ) . Think of our task roughly as learning to play a very hard game on a 9x3 board -- we would , of course , love to learn how to play the same game on arbitrary size boards , but the fact is that the game is mighty hard even at this \u201c board size \u201d ( since in each round , one player plays a 0-1 assignment to each cell in the board , and the other player picks a subset of the columns , in fact a weight vector on the columns ) . Instead of producing an algorithm that works for the 0-1 version of the problem , we produce an algorithm that works for the fractional version of the problem . This is , once again , motivated by making something work with modest amount of computation . Our explorations indicated that producing an algorithm for the 0-1 version needs reinforcement learning , and producing an algorithm that works on all 9x3 instances using this approach would still take several days of computation . On calling it Yao \u2019 s Principle : as Reviewer # 1 correctly noted , this is an application of the classic von Neumann minimax principle to the \u201c game \u201d between an \u201c algorithm player \u201d and an \u201c input player \u201d . We call it Yao \u2019 s principle primarily in accordance with tradition in theoretical CS ( see https : //en.wikipedia.org/wiki/Yao % 27s_principle and also https : //blog.computationalcomplexity.org/2006/10/favorite-theorems-yao-principle.html , where it is noted that \u201c Yao observed [ the result ] \u201d and commentators note that it \u2019 s called Yao \u2019 s principle because this observation has significant consequences for many central problems in TCS ) . We are happy to add text to reflect this ."}, {"review_id": "HygYmJBKwH-2", "review_text": "This paper introduces a new approach to solve optimization problems without relying on any human-provided data beyond the specification of the optimization problem itself. The approach is inspired by the two-player zero-sum game paradigm and follow a generative adversarial network (GAN) setting. One network is trained to output the optimal behavior for a given problem, while the other is trained to output difficult instances of the given problem. These two networks are trained simultaneously and compete against the other until some equilibrium is achieved. This approach is tested on two small problems for which the optimal behavior is known and seems to perform near theoretical optimality. I weakly reject this paper because although the approach is indeed interesting, the paper is lacking some structure, as described below: - The paper clearly mentions that no optimization of the training setup or the hyperparameters has been done because the authors are not interested in extending ML techniques. However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings. It is thus unclear if the approach is robust against different hyperparameter settings. - Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach. - Section 1.1 presents results with too many details without introducing the problem. I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved. - One task is presented in Section 2 \"Preliminaries\" while the other task is presented in Section 4 \"AdWords\". It is hard to follow the flow of ideas present in the paper when similar things are not together. I would suggest restructuring the paper into a more classical structure such as: <intro without detailed results - previous work & problematic - approach taken with more details for reproducibility - description of the two tasks - description of experiments with more details for reproducibility - results - conclusion>. - The paper mentions the MSVV algorithm twice but no reference or explanation is provided. It is very hard to understand sentences referring to this. - This work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms. It is hard to support this motivation when no experiments are done in its favor. - No comparison has been made between their approach and other previous approaches. We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio. It is thus very hard to know if this new approach brings any improvement to previous work. Below are a few things that were not considered to make a decision, but are only details that would make the paper slightly better: - typo at the beginning of section 3.1: missing 'be' in \"This can either *be* by an ...\" - typo at the beginning os section 4: missing 'be' in \"... the algorithm must irrevocably *be* allocated to ...\" - Axis' names to the different plots in the Figures would help understand them better. Also, the description of some figures could benefit more details that could be taken off from the text. ", "rating": "3: Weak Reject", "reply_text": "-- \u201c This work only considers problems for which the optimal input distribution is known , but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms . It is hard to support this motivation when no experiments are done in its favor. \u201d The long-term agenda / research program is indeed two-fold : 1 . Investigate whether known optimal worst-case algorithms can be reproduced without any domain knowledge ( i.e. , \u201c Can ML learn Algorithms \u201d ) . This is the case in which the optimal distribution of inputs is also known . 2.Discover new/better worst-case algorithms for problems with the aid of ML , when neither a good algorithms or input distribution is known . # 2 is a long-term goal , and not tackled in this paper , but we believe # 1 ( tackled in this paper ) is itself of strong interest ( and difficult ) -- would ML be able to discover the same \u201c pen-and-paper \u201d algorithms that computer scientists invented ? The problems we study ( ski-rental and Adwords ) fall into the first category of problems . Note that the algorithms in the two cases are very different in structure . Further , please note that even though the optimal distribution of input is known in these two problems , we do not use it at all in training . Indeed , this is the main point of this paper -- the previous work of Kong et al.used these distributions to train the algorithm network ( and hence that technique still needed the prior theoretical \u201c pen-and-paper \u201d work ) , while this work starts with ZERO knowledge . We follow this approach even in case # 1 when the optimal input distribution is known exactly because we have the ultimate goal # 2 in mind , that is , we want to design a framework that can eventually also work without knowledge of optimal input distribution ( but that goal is outside the scope of this paper ) . -- \u201c No comparison has been made between their approach and other previous approaches . We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio . It is thus very hard to know if this new approach brings any improvement to previous work. \u201d We believe there is some misunderstanding here as to the contribution . As such , there are no previous approaches to \u201c learn algorithms \u201d ( besides Kong et al . ) . To be more explicit ( in case we didn \u2019 t understand the comment ) , previous work for algorithmic problems could fall into a few buckets : ( 1 ) The original algorithms papers which found optimal worst case algorithms [ Karlin et al.1986 , Mehta et al.2007 ] .These give the analytical benchmarks . E.g. , [ Mehta et al.2007 ] proposes the algorithm to solve Adwords , and proves that it achieves the optimal CR of 1-1/e ~ 0.63 ( i.e. , no matter what the online input sequence is , you get > = 1-1/e of the optimal solution in hindsight if you knew the instance offline ) . Thus the difference of 0.01 CR is a direct comparison to that work . ( 2 ) One may imagine there could be some kind of optimization ( IP / LP ) technique or some ML technique to solve specific instances of the problem ( a specific instance of Adwords e.g . ) . But this is in fact not a feasible possibility , for two reasons : ( a ) Our problems are online problems where the full instance itself is not known in advance , and ( b ) we are looking for worst case competitive algorithms , i.e. , a policy that does well no matter how the instance unfolds in the future . Thus there can not be previous work to compare in such a bucket . ( 3 ) Kong et al. , 2018 is the closest previous work since it shows how to learn algorithms in the online setting . As mentioned above , the critical difference is that our paper learns the algorithms without any prior knowledge of the worst input distribution , but evolves both the distribution and the algorithm jointly ( with some parallels to GANs , AlphaZero , self-play , etc.as we have stated ) . Quantitatively , the CR results are equally good ; our main objective is to see if the learned algorithm is close in policy to the theoretical algorithm , and whether we are reasonably close to the optimal CR ."}], "0": {"review_id": "HygYmJBKwH-0", "review_text": "Update to the Review after the rebuttal from the Authors: After carefully reviewing the responses by the authors especially on my concerns about the significance of solving an instance of a given problem and the improvement in the exposition of the ideas I would like to amend my earlier decision and recommend to accept. For completeness below is the original review. This paper introduces a framework to learn to generate solutions to online combinatorial optimization problems with worst case guarantees. The framework as the authors claim eliminates the need for manual hard to solve instance/data creation, which is necessary to teach the model to provide the aforementioned worst case guarantees. Therefore the main contribution of the paper can be said that this framework shows that it is possible to train a machine learning model, which can learn an algorithm to solve hard online combinatorial optimization problems and this training can be done without knowing much about the actual optimization problem domain. The only input required is the way to calculate the objective function of the actual problem. This contribution is demonstrated on two classes of problems: Ski-Rental and Fractional AdWords. The framework requires two neural networks one for solution generation agent and one for problem instance generation. These two networks are trained jointly from scratch and the underlying algorithm for the training is provided. Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems. Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach? This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem. Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making. Therefore I do not find being able to solve this problem as a supporting evidence for the contributions claimed. In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions. I find this important because for combinatorial optimization usually scale matters a lot. While a small instance of a problem can be solve by a general purpose solver quickly a small increase in the problem size can turn out to be intractable. When proposing a machine learning approach to such problems I would expect the model to scale better than pure optimization approach so that there would be demonstrable benefit. Although the paper proposes an interesting framework I would argue that it is a \u201cgreen apple\u201d in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance. Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution. In order to clarify the exposition the following are some questions: 1. Authors call the approach YaoGAN due to its structural similarity to GANs. I understand the fact that they are training two neural networks in an alternating scheme, which is similar to the GAN training. How can one evaluate the solutions generated by this framework similar to how GAN generators are evaluated? Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees? 2. The main technical contribution claim needs to be elaborated. I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation. 3. Authors claim there are two shortcomings of the previous method proposed in Kong et. al 2018. They need to elaborate how their method overcomes these issues better. 4. Authors state that fractional relaxation of combinatorial mainly integer optimization problems, which is accurate. Yet their approach is only able to solve the fractional version of the AdWords problem. In addition I agree with the fact that although continuous relaxations to integer optimization problems might provide insightful directions they usually employed to to prove bounds on the heuristic approaches. Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me. I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem. What are the shortcomings? 5.In Appendix A authors talk about no-regret dynamics, which are relevant. However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims. 6. In appendix C.2 authors provide additional plots for the Fractional AdWords problem. However, they retain from providing any intuition about them. In particular what is the conclusion to be drawn from Figure 5. This needs more elaboration. Is this way of training results expected? What is the lesson learned? 7.In Figure 8 they provide example data from experience array. What are the significance of these examples? How they help us understand the problem instance generation was actually able to find interesting instances? What kind of dynamics are under covered? These are not directly revealed by only looking at the pictures one needs more explanation to support the claims. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review . Please also see our high-level clarification above which we believe can help in better interpretation of our contribution . Some specific responses below : -- \u201c proposed method demonstrates that an instance of one class of problems , Fractional Adwords , can be learned to solve without domain expertise , however fails to prove that the approach would be beneficial for any other instances of the same problem. \u201d Please refer to our overall comments on this question ( and also a few more details in reply to Reviewer # 1 \u2019 s similar question ) . -- Comment on scale / speed for large instances of combinatorial optimization : The point of this work is only to see if ML can find optimal algorithms , and not about doing it faster than the known theoretical algorithms . Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers , since our problems are online , i.e. , the instance is not known beforehand , so there is no comparison to such \u201c general-purpose \u201d solvers . Thus we do n't compare to the running time of offline solvers , but to the worst-case competitive ratio of the optimal online algorithms . As mentioned in the comment , this approach may eventually lead to finding optimal or near-optimal algorithms for a problem ( not an instance of a problem ) for which no algorithm is known -- but this is outside the scope of this work future work . Again drawing the analogy of playing Go , the objective is mostly on training an agent that can make competitive moves rather than very fast moves , and there is no known \u201c general-purpose \u201d strategy to accomplish this . * Please also see reply to reviewer # 2 on a similar question of evaluating against other methods * -- \u201c Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent , i.e.problem instances are not generated by use of a machine learning model , which is one of the main claims the authors are making. \u201d Please see our high-level clarification on top ."}, "1": {"review_id": "HygYmJBKwH-1", "review_text": "This papers tackles the following question. Is it possible to learn the \"most\" complex instance of a class of (combinatorial) problem while finding (or recovering) algorithms with strong minimax rate. This is very interesting and clearly a nice line of work (in theory though). The techniques used rely on GANs since it can be shown that finding the best (random) algorithm and the worst (deterministic) instance is equivalent to finding the worst random instance against the best deterministic algorithm. This is actually a direct consequence of any minmax theorem in game theory; the authors decided to credit that result to Yao (I tend to *strongly* disagree with that point as, even if he stated this fact in CS, this result was quite standard several decades before him - anyway.). Then this idea is evaluated in two examples. A toy problem (the ski rental) and a more or less concrete ones (adwords pb of Mehta). This is the major disappointment in the paper. The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract \"find algorithms with strong worst-case guarantees for online combinatorial optimization problems\". So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper). I believe that this paper is thus not in its final form and could be largely improved. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . Please also see our high-level clarification above which we believe can help in better interpretation of our contribution . Some specific responses below : Reviewer # 1 is absolutely right -- we don \u2019 t know yet how to scale this to more difficult combinatorial problems . But let \u2019 s clarify that statement a bit more : The ski-rental problem is often the first problem studied when teaching online algorithms , but it is certainly far from a \u201c toy problem \u201d when we wish to learn an algorithm from scratch . We apologize if we painted an incorrect picture by calling it a \u201c simple example \u201d and a \u201c staple introductory problem \u201d . It is easy to describe in that it has a single hidden parameter ( the length of the ski season ) and a single revealed parameter ( the cost of buying ) . It is a staple introductory problem because it is elegant and illuminates the essential difficulties in designing online algorithms : there is a nearly-trivial factor-2 competitive algorithm ( rent until you \u2019 ve spent $ B , then buy , so even if the ski season ends the next day , you \u2019 ve not spent more than twice the least possible amount ) , but the 1-1/e competitive ratio algorithm is quite creative and subtle , and serves as an introduction to the richness of the field of online algorithms . In fact , the Karlin et al . ( 1986 ) paper also introduced the notion of competitive analysis of online algorithms , and is probably the most-cited paper in this field . In some sense , this poses us the ideal challenge : can ML approaches discover creative and subtle \u201c solutions \u201d ( in our case , an algorithm ) ? On a more technical note , please note that our \u201c machinery \u201d of solving the two-player game is needed to discover an algorithm for the ski rental problem : if we don \u2019 t allow the players to alternate and reach an equilibrium , for any fixed distribution on the ski rental instances ( B , K ) , there is a deterministic algorithm that is optimal ( among all online algorithms ) , and the worst-case performance of that ( or any ) deterministic algorithm is * provably * limited by a factor of 2 ( i.e. , there exists some distribution on instances where it will fail badly ) . Also refer to our discussion on this in the high-level clarification at the top . The AdWords problem considered is actually a difficult combinatorial problem , and is an archetypal online combinatorial optimization problem that captures the class of problems solvable by one of the most powerful techniques in this area -- primal-dual algorithms , which have led to the state-of-the-art approximation algorithms for numerous hard online ( and offline ) optimization problems . In particular , it generalizes bipartite matching , historically one of the most significant combinatorial optimization problems ( led to the development of the classic Hungarian method , see https : //en.wikipedia.org/wiki/Hungarian_algorithm ) . We did water down our ambition in a few ways : Instead of producing an algorithm that works for inputs of all sizes , we focus on the case of 9x3 ( three advertisers , nine slots ) -- a fixed finite size ! This choice was arrived at based on the following criteria : what can we learn in a few hours of computation that \u2019 s still * well beyond * what can be achieved through exhaustive search ( for an algorithm ) . Think of our task roughly as learning to play a very hard game on a 9x3 board -- we would , of course , love to learn how to play the same game on arbitrary size boards , but the fact is that the game is mighty hard even at this \u201c board size \u201d ( since in each round , one player plays a 0-1 assignment to each cell in the board , and the other player picks a subset of the columns , in fact a weight vector on the columns ) . Instead of producing an algorithm that works for the 0-1 version of the problem , we produce an algorithm that works for the fractional version of the problem . This is , once again , motivated by making something work with modest amount of computation . Our explorations indicated that producing an algorithm for the 0-1 version needs reinforcement learning , and producing an algorithm that works on all 9x3 instances using this approach would still take several days of computation . On calling it Yao \u2019 s Principle : as Reviewer # 1 correctly noted , this is an application of the classic von Neumann minimax principle to the \u201c game \u201d between an \u201c algorithm player \u201d and an \u201c input player \u201d . We call it Yao \u2019 s principle primarily in accordance with tradition in theoretical CS ( see https : //en.wikipedia.org/wiki/Yao % 27s_principle and also https : //blog.computationalcomplexity.org/2006/10/favorite-theorems-yao-principle.html , where it is noted that \u201c Yao observed [ the result ] \u201d and commentators note that it \u2019 s called Yao \u2019 s principle because this observation has significant consequences for many central problems in TCS ) . We are happy to add text to reflect this ."}, "2": {"review_id": "HygYmJBKwH-2", "review_text": "This paper introduces a new approach to solve optimization problems without relying on any human-provided data beyond the specification of the optimization problem itself. The approach is inspired by the two-player zero-sum game paradigm and follow a generative adversarial network (GAN) setting. One network is trained to output the optimal behavior for a given problem, while the other is trained to output difficult instances of the given problem. These two networks are trained simultaneously and compete against the other until some equilibrium is achieved. This approach is tested on two small problems for which the optimal behavior is known and seems to perform near theoretical optimality. I weakly reject this paper because although the approach is indeed interesting, the paper is lacking some structure, as described below: - The paper clearly mentions that no optimization of the training setup or the hyperparameters has been done because the authors are not interested in extending ML techniques. However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings. It is thus unclear if the approach is robust against different hyperparameter settings. - Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach. - Section 1.1 presents results with too many details without introducing the problem. I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved. - One task is presented in Section 2 \"Preliminaries\" while the other task is presented in Section 4 \"AdWords\". It is hard to follow the flow of ideas present in the paper when similar things are not together. I would suggest restructuring the paper into a more classical structure such as: <intro without detailed results - previous work & problematic - approach taken with more details for reproducibility - description of the two tasks - description of experiments with more details for reproducibility - results - conclusion>. - The paper mentions the MSVV algorithm twice but no reference or explanation is provided. It is very hard to understand sentences referring to this. - This work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms. It is hard to support this motivation when no experiments are done in its favor. - No comparison has been made between their approach and other previous approaches. We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio. It is thus very hard to know if this new approach brings any improvement to previous work. Below are a few things that were not considered to make a decision, but are only details that would make the paper slightly better: - typo at the beginning of section 3.1: missing 'be' in \"This can either *be* by an ...\" - typo at the beginning os section 4: missing 'be' in \"... the algorithm must irrevocably *be* allocated to ...\" - Axis' names to the different plots in the Figures would help understand them better. Also, the description of some figures could benefit more details that could be taken off from the text. ", "rating": "3: Weak Reject", "reply_text": "-- \u201c This work only considers problems for which the optimal input distribution is known , but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms . It is hard to support this motivation when no experiments are done in its favor. \u201d The long-term agenda / research program is indeed two-fold : 1 . Investigate whether known optimal worst-case algorithms can be reproduced without any domain knowledge ( i.e. , \u201c Can ML learn Algorithms \u201d ) . This is the case in which the optimal distribution of inputs is also known . 2.Discover new/better worst-case algorithms for problems with the aid of ML , when neither a good algorithms or input distribution is known . # 2 is a long-term goal , and not tackled in this paper , but we believe # 1 ( tackled in this paper ) is itself of strong interest ( and difficult ) -- would ML be able to discover the same \u201c pen-and-paper \u201d algorithms that computer scientists invented ? The problems we study ( ski-rental and Adwords ) fall into the first category of problems . Note that the algorithms in the two cases are very different in structure . Further , please note that even though the optimal distribution of input is known in these two problems , we do not use it at all in training . Indeed , this is the main point of this paper -- the previous work of Kong et al.used these distributions to train the algorithm network ( and hence that technique still needed the prior theoretical \u201c pen-and-paper \u201d work ) , while this work starts with ZERO knowledge . We follow this approach even in case # 1 when the optimal input distribution is known exactly because we have the ultimate goal # 2 in mind , that is , we want to design a framework that can eventually also work without knowledge of optimal input distribution ( but that goal is outside the scope of this paper ) . -- \u201c No comparison has been made between their approach and other previous approaches . We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio . It is thus very hard to know if this new approach brings any improvement to previous work. \u201d We believe there is some misunderstanding here as to the contribution . As such , there are no previous approaches to \u201c learn algorithms \u201d ( besides Kong et al . ) . To be more explicit ( in case we didn \u2019 t understand the comment ) , previous work for algorithmic problems could fall into a few buckets : ( 1 ) The original algorithms papers which found optimal worst case algorithms [ Karlin et al.1986 , Mehta et al.2007 ] .These give the analytical benchmarks . E.g. , [ Mehta et al.2007 ] proposes the algorithm to solve Adwords , and proves that it achieves the optimal CR of 1-1/e ~ 0.63 ( i.e. , no matter what the online input sequence is , you get > = 1-1/e of the optimal solution in hindsight if you knew the instance offline ) . Thus the difference of 0.01 CR is a direct comparison to that work . ( 2 ) One may imagine there could be some kind of optimization ( IP / LP ) technique or some ML technique to solve specific instances of the problem ( a specific instance of Adwords e.g . ) . But this is in fact not a feasible possibility , for two reasons : ( a ) Our problems are online problems where the full instance itself is not known in advance , and ( b ) we are looking for worst case competitive algorithms , i.e. , a policy that does well no matter how the instance unfolds in the future . Thus there can not be previous work to compare in such a bucket . ( 3 ) Kong et al. , 2018 is the closest previous work since it shows how to learn algorithms in the online setting . As mentioned above , the critical difference is that our paper learns the algorithms without any prior knowledge of the worst input distribution , but evolves both the distribution and the algorithm jointly ( with some parallels to GANs , AlphaZero , self-play , etc.as we have stated ) . Quantitatively , the CR results are equally good ; our main objective is to see if the learned algorithm is close in policy to the theoretical algorithm , and whether we are reasonably close to the optimal CR ."}}