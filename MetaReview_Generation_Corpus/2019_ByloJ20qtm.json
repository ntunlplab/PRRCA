{"year": "2019", "forum": "ByloJ20qtm", "title": "Neural Program Repair by Jointly Learning to Localize and Repair", "decision": "Accept (Poster)", "meta_review": "This paper provides an approach to jointly localize and repair VarMisuse bugs, where a wrong variable from the context has been used. The proposed work provides an end-to-end training pipeline for jointly localizing and repairing, as opposed to independent predictions in existing work. The reviewers felt that the manuscript was very well-written and clear, with fairly strong results on a number of datasets.\n\nThe reviewers and AC note the following potential weaknesses: (1) reviewer 4 brings up related approaches from automated program repair (APR), that are much more general than the VarMisuse bugs, and the paper lacks citation and comparison to them, (2) the baselines that were compared against are fairly weak, and some recent approaches like DeepBugs and Sk_p are ignored, (3) the approach is trained and evaluated only on synthetic bugs, which look very different from the realistic ones, and (4) the contributions were found to be restricted in novelty, just uses a pointer-based LSTM for locating and fixing bugs. \n\nThe authors provided detailed comments and a revision to address and clarify these concerns. They added an evaluation on realistic bugs, along with differences from DeepBugs and Sk_p, and differences between neural and automated program repair. They also added more detail comparisons, including separating the localization vs repair aspects by comparing against enumeration. During the discussion, the reviewers disagree on the \"weakness\" of the baseline, as reviewers 1 and 4 feel it is a reasonable baseline as it builds upon the Allamanis paper. They found, to different degrees, that the results on realistic bugs are much more convincing than the synthetic bug evaluation. Finally, all reviewers agree that the novelty of this work is limited.\n\nAlthough the reviewers disagree on the strength of the baselines (a recent paper) and the evaluation benchmarks, they agreed that the results are quite strong. The paper, however, addressed many of the concerns in the response/revision, and thus, the reviewers agree that it meets the bar for acceptance.", "reviews": [{"review_id": "ByloJ20qtm-0", "review_text": "This paper considers the problem of VarMisuse, a kind of software bug where a variable has been misused. Existing approaches to the problem create a complex model, followed by enumerating all possible variable replacements at all possible positions, in order to identify where the bug may exist. This can be problematic for training which is performed using synthetic replacements; enumeration on non-buggy positions does not reflect the test case. Also, at test time, enumerating is expensive, and does not accurately capture the various dependencies of the task. This paper instead proposes a LSTM based model with pointers to break the problem down into multiple steps: (1) is the program buggy, (2) where is the bug, and (3) what is the repair. They evaluate on two datasets, and achieve substantial gains over previous approaches, showing that the idea of localizing and repairing and effective. I am quite conflicted about this paper. Overall, the paper has been strengths: - It is quite well-written, and clear. They do a good job of describing the problems with earlier approaches, and how their approach can address it. - The proposed model is straightforward, and addresses the problem quite directly. There is elegance in its simplicity. - The evaluation is quite thorough, and the resulting gains are quite impressive. However, I have some significant reservations about the novelty and the technical content. The proposed model doesn't quite bring anything new to the table. It is a straightforward combination of LSTMs with pointers, and it's likely the benefits are coming from the reformulation of the problem, not from the actual proposed model. This, along with the fact that VarMisuse is a small subset of the kinds of bugs that can appear in software, makes me feel the ideas in this paper may not lead to significant impact on the research community. As a minor aside, this paper addresses some specific aspects of VarMisuse task and the Allamanis et al 2018 model, and introduces a model just for it. I consider the Allamanis model a much more general representation of programs, and much more applicable to other kinds of debugging tasks (but yes, since they didn't demonstrate this either, I'm not penalizing this paper for it). --- Update ---- Given the author's response and the discussion, I'm going to raise the score a little. Although there are some valid concerns, it provides a clear improvement over Allamanis et al paper, and provides an interesting approach to the task. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful review and constructive feedback . Our paper proposes a joint model for localization and repair using pointers , which is novel and the main technical contribution of the paper . Even though it is applied specifically to the variable misuse problem , the idea of using pointers is fundamental and portable to other program repair problems . In particular , all program repair techniques require the bug localization step and pointers seem like an ideal mechanism for this as they can pinpoint a buggy location precisely at the token-level . Other previous works in the program repair literature either use enumerative search for localization , or perform localization at the granularity of lines or depend on external tools for localization ( such as compiler error messages for syntactic error localization ) . In contrast , our proposal to use pointers enables an end-to-end learning based solution . We use the pointer mechanism on top of sequence based encoding of programs , but pointers can be combined naturally with other representations of programs ; e.g. , trees or graphs . As we demonstrate in the paper , the previous enumerative approaches assume independence among different predictions that is problematic and leads to poor results . The end-to-end joint localization and repair is an essential step to overcome this issue , and we believe this idea of joint prediction is going to generalize to many other program repair tasks and even program completion tasks ."}, {"review_id": "ByloJ20qtm-1", "review_text": "This paper presents an LSTM-based model for bug detection and repair of a particular type of bug called VarMisuse, which occurs at a point in a program where the wrong identifier is used. This problem is introduced in the Allamanis et al. paper. The authors of the paper under review demonstrate significant improvements compared to the Allamanis et al. approach on several datasets. I have concerns with respect to the evaluation, the relation of the paper compared to the state-of-the-art in automatic program repair (APR), and the problem definition with respect to live-variable analysis. My largest concern about both this paper and the Allamanis et al. paper is how it compares to the state-of-the-art in APR in general. There is a large and growing amount of work in APR as shown in the following papers: [1] L. Gazzola, D. Micucci, and L. Mariani, \u201cAutomatic Software Repair: A Survey,\u201d IEEE Transactions on Software Engineering, pp. 1\u20131, 2017. [2] M. Monperrus, \u201cAutomatic Software Repair: A Bibliography,\u201d ACM Comput. Surv., vol. 51, no. 1, pp. 17:1\u201317:24, Jan. 2018. [3] M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun, \u201cDo automated program repair techniques repair hard and important bugs?,\u201d Empir Software Eng, pp. 1\u201347, Nov. 2017. Although the proposed LSTM-based approach for VarMisuse is interesting, it seems to be quite a small delta compared to the larger APR research space. Furthermore, the above papers on APR are not referenced. The paper under review mostly uses synthetic bugs. However, they do have a dataset from an anonymous industrial setting that they claim is realistic. In such a setting, I would simply have to trust the blinded reviewers. However, the one industrial software project tells me little about the proposed approach\u2019s effectiveness when applied to a significant number of widely-used software programs like the ones residing in state-of-the-art benchmarks for APR, of which there are at least the following two datasets: [4] C. L. Goues et al., \u201cThe ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs,\u201d IEEE Transactions on Software Engineering, vol. 41, no. 12, pp. 1236\u20131256, Dec. 2015. [5] R. Just, D. Jalali, and M. D. Ernst, \u201cDefects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs,\u201d in Proceedings of the 2014 International Symposium on Software Testing and Analysis, New York, NY, USA, 2014, pp. 437\u2013440. The above datasets are not used or referenced by the paper under review. My final concern about the paper is the formulation of live variables. A variable is live at certain program points (e.g., program statements, lines, or tokens as called in this paper). For example, from Figure 1 in the paper under review, at line 5 in (a) and (b), object_name and subject_name are live, not just sources. In the problem definition, the authors say that \"V_def^f \\subseteq V denotes the set of all live variables\", which does not account for the fact that different variables are alive (or dead) at different points of a program. The authors then say that, for the example in Figure 1, \"V_def^f contains all locations in the program where the tokens in V appear (i.e., tokens in the Blue boxes), as well as token sources from line 1\u201d. The explanation of the problem definition when applied to the example does not account for the fact that different variables are alive at different program points. I\u2019m not sure to what extent this error negatively affects the implementation of the proposed model. However, the error could be potentially quite problematic.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thoughtful review and constructive feedback . We will include a discussion about the differences between our work and the automated program repair ( APR ) techniques in the literature , as outlined below . The traditional APR approaches differ from our work in the following ways : 1 ) They require a form of specification of correctness to repair a buggy program , usually as a logical formula/assertion , a set of tests or a reference implementation . 2 ) They depend on hand-designed search techniques for localization and repair . 3 ) The techniques are applied to programs which violate the specifications ( e.g. , a program which fails some tests ) , that is , to programs which are already known to contain bugs . In contrast , a recent line of research in APR is based on end-to-end learning , of which ours is an instance . Our solution ( like some other learning based repair solutions ) has the following contrasting features : 1 ) Our solution does not require any specification of correctness . Instead it learns to fix a common class of errors directly from source code examples . 2 ) Our solution does not perform enumerative search for localization or repair . We train a neural network to perform localization and repair directly . 3 ) Our solution is capable of first classifying whether a program has the specific type of bug or not , and subsequently localizing and repairing it . ManyBugs , IntroClass , and Defects4J are benchmarks designed for test-based program repair techniques . The bugs relate to the expected specification of individual programs ( captured through test cases of the program ) and the nature of bugs vary from program to program . These benchmarks are therefore suitable to evaluate repair techniques guided by test executions . Learning based solutions like ours focus on common error types so that it is possible for a model to generalize across programs , and work directly on embeddings of source code . Thank you for your comment about the variable liveness . We misused the term of live variables , and we will update the paper accordingly . The V_def^f set contains all variables defined in a function f , including the function arguments ; in this way constructing a set of all variables that can be used within the scope . We construct one V_def^f set per function , representing a set of candidate variables for fixing bugs in that function . In this way , the V_def^f set is a ( safe ) over-approximation of the in-scope variables at each program location . The over-approximation can lead to predicting an undefined variable as a repair , however , this is not an error and model over time learns not to predict undefined variables . The V_def^f set is not constrained to only live variables ; as there are cases when solution to a bug is using a variable that is defined in the scope but not live ( not used elsewhere ) , e.g. , subject_name variable in Figure 1a . Regarding your comment about Figure 1 , in the blue boxes we show variable usages , not V_def^f set . We want to clarify that the examples in our industrial dataset ( Section 4.4 ) are not from a single industrial project . The examples do come from multiple software projects . Please let us know if this helped clarify the confusion regarding the problem definition of candidate variable set and the relationship with previous APR work and the more recent neural program repair approaches ( ours , Allamanis et.al and others ) ."}, {"review_id": "ByloJ20qtm-2", "review_text": "Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present. This paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug. Unfortunatley this leads to a number of weaknesses: - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions. - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the >80% range) - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on. - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines? Strengths of the paper are: - Well-written and easy to follow and understand. - Evaluation on several datasets. - Interesting architecture for bug-localization if the idea really works. [1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review and constructive feedback . We believe there are a few major misunderstandings in the review and we would like to take this opportunity to clarify them . We will be happy to discuss them in more detail if more clarifications might be needed or there are more questions . We would first like to point out that ours is the first model that jointly learns to perform both localization and repair of the variable misuse bugs . It exploits the property of this particular class of variable misuse bugs -- both the location and repair corresponds to variable use locations in the program . Unlike Allamanis et al.2018 that uses an enumerative approach to make a number of predictions for a program that is linear in number of variable uses , our model makes a single prediction using a two pointer based mechanism . Thanks for the pointer to the DeepBugs paper . Note that there are several differences of our work with the DeepBugs paper , which we explain below . We will add this to our revision as well . 1.DeepBugs learns a classifier over single expressions . It takes a single program expression as input ( e.g. \u201c 2 % i == 0 \u201d ) and classifies it as positive or negative . On the other hand , in addition to classifying programs , our model learns to localize and also repair the bug using a two-headed pointer network . 2.Our model uses the full program ( up to 250 number of tokens ) for learning the vector representation . DeepBugs only looks at a single expression at a time . 3.Finally , the 80 % accuracy number for DeepBugs is only for expression classification . It has no direct comparison with our model \u2019 s accuracy since it is a different problem ( classifying a single expression as correct compared to analyzing a full program to identify bug location and the corresponding repair ) . Moreover , our pointer models also get to 82.4 % classification accuracy for full programs ( Table 1 ) . Allamanis et al.only report the accuracy of repair-only model , where the model predicts a single variable at a time for each slot location in a program . Translating their 85.5 % repair accuracy number to a number that corresponds to repairing the full program would lead to a very different result . In Table 1 , we try to replicate a similar experiment and show that jointly learning the model leads to significant improvements without sacrificing true positive and classification accuracy . Moreover , Allamanis et al.2018 perform a significant amount of program preprocessing including type inference , control flow , and data flow analysis to add different types of graph edges . Without such pre-processing , they achieve an accuracy of 55.3 % on repair-only tasks ( Section 4.3 ) . In our work , we want our distributed representations to automatically learn good representations of programs without any manual feature engineering . Performance trade-off : In fact , our proposed architecture is significantly more scalable and easier to train . Since we are using sequence models to compute pointer attentions that are easier to batch over multiple examples , it is much more scalable to train compared to graph models that are difficult to batch because of different graph sizes . Our own graph implementation was significantly slower to train . In addition to that , it is also significantly faster at inference time , as it does not need to perform an O ( n ) number of model predictions , where n is the number of variable use locations in the program under test . For our model , it performs a single prediction , which is much faster . Please let us know if this helped clarify the questions and comments ."}], "0": {"review_id": "ByloJ20qtm-0", "review_text": "This paper considers the problem of VarMisuse, a kind of software bug where a variable has been misused. Existing approaches to the problem create a complex model, followed by enumerating all possible variable replacements at all possible positions, in order to identify where the bug may exist. This can be problematic for training which is performed using synthetic replacements; enumeration on non-buggy positions does not reflect the test case. Also, at test time, enumerating is expensive, and does not accurately capture the various dependencies of the task. This paper instead proposes a LSTM based model with pointers to break the problem down into multiple steps: (1) is the program buggy, (2) where is the bug, and (3) what is the repair. They evaluate on two datasets, and achieve substantial gains over previous approaches, showing that the idea of localizing and repairing and effective. I am quite conflicted about this paper. Overall, the paper has been strengths: - It is quite well-written, and clear. They do a good job of describing the problems with earlier approaches, and how their approach can address it. - The proposed model is straightforward, and addresses the problem quite directly. There is elegance in its simplicity. - The evaluation is quite thorough, and the resulting gains are quite impressive. However, I have some significant reservations about the novelty and the technical content. The proposed model doesn't quite bring anything new to the table. It is a straightforward combination of LSTMs with pointers, and it's likely the benefits are coming from the reformulation of the problem, not from the actual proposed model. This, along with the fact that VarMisuse is a small subset of the kinds of bugs that can appear in software, makes me feel the ideas in this paper may not lead to significant impact on the research community. As a minor aside, this paper addresses some specific aspects of VarMisuse task and the Allamanis et al 2018 model, and introduces a model just for it. I consider the Allamanis model a much more general representation of programs, and much more applicable to other kinds of debugging tasks (but yes, since they didn't demonstrate this either, I'm not penalizing this paper for it). --- Update ---- Given the author's response and the discussion, I'm going to raise the score a little. Although there are some valid concerns, it provides a clear improvement over Allamanis et al paper, and provides an interesting approach to the task. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful review and constructive feedback . Our paper proposes a joint model for localization and repair using pointers , which is novel and the main technical contribution of the paper . Even though it is applied specifically to the variable misuse problem , the idea of using pointers is fundamental and portable to other program repair problems . In particular , all program repair techniques require the bug localization step and pointers seem like an ideal mechanism for this as they can pinpoint a buggy location precisely at the token-level . Other previous works in the program repair literature either use enumerative search for localization , or perform localization at the granularity of lines or depend on external tools for localization ( such as compiler error messages for syntactic error localization ) . In contrast , our proposal to use pointers enables an end-to-end learning based solution . We use the pointer mechanism on top of sequence based encoding of programs , but pointers can be combined naturally with other representations of programs ; e.g. , trees or graphs . As we demonstrate in the paper , the previous enumerative approaches assume independence among different predictions that is problematic and leads to poor results . The end-to-end joint localization and repair is an essential step to overcome this issue , and we believe this idea of joint prediction is going to generalize to many other program repair tasks and even program completion tasks ."}, "1": {"review_id": "ByloJ20qtm-1", "review_text": "This paper presents an LSTM-based model for bug detection and repair of a particular type of bug called VarMisuse, which occurs at a point in a program where the wrong identifier is used. This problem is introduced in the Allamanis et al. paper. The authors of the paper under review demonstrate significant improvements compared to the Allamanis et al. approach on several datasets. I have concerns with respect to the evaluation, the relation of the paper compared to the state-of-the-art in automatic program repair (APR), and the problem definition with respect to live-variable analysis. My largest concern about both this paper and the Allamanis et al. paper is how it compares to the state-of-the-art in APR in general. There is a large and growing amount of work in APR as shown in the following papers: [1] L. Gazzola, D. Micucci, and L. Mariani, \u201cAutomatic Software Repair: A Survey,\u201d IEEE Transactions on Software Engineering, pp. 1\u20131, 2017. [2] M. Monperrus, \u201cAutomatic Software Repair: A Bibliography,\u201d ACM Comput. Surv., vol. 51, no. 1, pp. 17:1\u201317:24, Jan. 2018. [3] M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun, \u201cDo automated program repair techniques repair hard and important bugs?,\u201d Empir Software Eng, pp. 1\u201347, Nov. 2017. Although the proposed LSTM-based approach for VarMisuse is interesting, it seems to be quite a small delta compared to the larger APR research space. Furthermore, the above papers on APR are not referenced. The paper under review mostly uses synthetic bugs. However, they do have a dataset from an anonymous industrial setting that they claim is realistic. In such a setting, I would simply have to trust the blinded reviewers. However, the one industrial software project tells me little about the proposed approach\u2019s effectiveness when applied to a significant number of widely-used software programs like the ones residing in state-of-the-art benchmarks for APR, of which there are at least the following two datasets: [4] C. L. Goues et al., \u201cThe ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs,\u201d IEEE Transactions on Software Engineering, vol. 41, no. 12, pp. 1236\u20131256, Dec. 2015. [5] R. Just, D. Jalali, and M. D. Ernst, \u201cDefects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs,\u201d in Proceedings of the 2014 International Symposium on Software Testing and Analysis, New York, NY, USA, 2014, pp. 437\u2013440. The above datasets are not used or referenced by the paper under review. My final concern about the paper is the formulation of live variables. A variable is live at certain program points (e.g., program statements, lines, or tokens as called in this paper). For example, from Figure 1 in the paper under review, at line 5 in (a) and (b), object_name and subject_name are live, not just sources. In the problem definition, the authors say that \"V_def^f \\subseteq V denotes the set of all live variables\", which does not account for the fact that different variables are alive (or dead) at different points of a program. The authors then say that, for the example in Figure 1, \"V_def^f contains all locations in the program where the tokens in V appear (i.e., tokens in the Blue boxes), as well as token sources from line 1\u201d. The explanation of the problem definition when applied to the example does not account for the fact that different variables are alive at different program points. I\u2019m not sure to what extent this error negatively affects the implementation of the proposed model. However, the error could be potentially quite problematic.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thoughtful review and constructive feedback . We will include a discussion about the differences between our work and the automated program repair ( APR ) techniques in the literature , as outlined below . The traditional APR approaches differ from our work in the following ways : 1 ) They require a form of specification of correctness to repair a buggy program , usually as a logical formula/assertion , a set of tests or a reference implementation . 2 ) They depend on hand-designed search techniques for localization and repair . 3 ) The techniques are applied to programs which violate the specifications ( e.g. , a program which fails some tests ) , that is , to programs which are already known to contain bugs . In contrast , a recent line of research in APR is based on end-to-end learning , of which ours is an instance . Our solution ( like some other learning based repair solutions ) has the following contrasting features : 1 ) Our solution does not require any specification of correctness . Instead it learns to fix a common class of errors directly from source code examples . 2 ) Our solution does not perform enumerative search for localization or repair . We train a neural network to perform localization and repair directly . 3 ) Our solution is capable of first classifying whether a program has the specific type of bug or not , and subsequently localizing and repairing it . ManyBugs , IntroClass , and Defects4J are benchmarks designed for test-based program repair techniques . The bugs relate to the expected specification of individual programs ( captured through test cases of the program ) and the nature of bugs vary from program to program . These benchmarks are therefore suitable to evaluate repair techniques guided by test executions . Learning based solutions like ours focus on common error types so that it is possible for a model to generalize across programs , and work directly on embeddings of source code . Thank you for your comment about the variable liveness . We misused the term of live variables , and we will update the paper accordingly . The V_def^f set contains all variables defined in a function f , including the function arguments ; in this way constructing a set of all variables that can be used within the scope . We construct one V_def^f set per function , representing a set of candidate variables for fixing bugs in that function . In this way , the V_def^f set is a ( safe ) over-approximation of the in-scope variables at each program location . The over-approximation can lead to predicting an undefined variable as a repair , however , this is not an error and model over time learns not to predict undefined variables . The V_def^f set is not constrained to only live variables ; as there are cases when solution to a bug is using a variable that is defined in the scope but not live ( not used elsewhere ) , e.g. , subject_name variable in Figure 1a . Regarding your comment about Figure 1 , in the blue boxes we show variable usages , not V_def^f set . We want to clarify that the examples in our industrial dataset ( Section 4.4 ) are not from a single industrial project . The examples do come from multiple software projects . Please let us know if this helped clarify the confusion regarding the problem definition of candidate variable set and the relationship with previous APR work and the more recent neural program repair approaches ( ours , Allamanis et.al and others ) ."}, "2": {"review_id": "ByloJ20qtm-2", "review_text": "Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present. This paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug. Unfortunatley this leads to a number of weaknesses: - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions. - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the >80% range) - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on. - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines? Strengths of the paper are: - Well-written and easy to follow and understand. - Evaluation on several datasets. - Interesting architecture for bug-localization if the idea really works. [1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review and constructive feedback . We believe there are a few major misunderstandings in the review and we would like to take this opportunity to clarify them . We will be happy to discuss them in more detail if more clarifications might be needed or there are more questions . We would first like to point out that ours is the first model that jointly learns to perform both localization and repair of the variable misuse bugs . It exploits the property of this particular class of variable misuse bugs -- both the location and repair corresponds to variable use locations in the program . Unlike Allamanis et al.2018 that uses an enumerative approach to make a number of predictions for a program that is linear in number of variable uses , our model makes a single prediction using a two pointer based mechanism . Thanks for the pointer to the DeepBugs paper . Note that there are several differences of our work with the DeepBugs paper , which we explain below . We will add this to our revision as well . 1.DeepBugs learns a classifier over single expressions . It takes a single program expression as input ( e.g. \u201c 2 % i == 0 \u201d ) and classifies it as positive or negative . On the other hand , in addition to classifying programs , our model learns to localize and also repair the bug using a two-headed pointer network . 2.Our model uses the full program ( up to 250 number of tokens ) for learning the vector representation . DeepBugs only looks at a single expression at a time . 3.Finally , the 80 % accuracy number for DeepBugs is only for expression classification . It has no direct comparison with our model \u2019 s accuracy since it is a different problem ( classifying a single expression as correct compared to analyzing a full program to identify bug location and the corresponding repair ) . Moreover , our pointer models also get to 82.4 % classification accuracy for full programs ( Table 1 ) . Allamanis et al.only report the accuracy of repair-only model , where the model predicts a single variable at a time for each slot location in a program . Translating their 85.5 % repair accuracy number to a number that corresponds to repairing the full program would lead to a very different result . In Table 1 , we try to replicate a similar experiment and show that jointly learning the model leads to significant improvements without sacrificing true positive and classification accuracy . Moreover , Allamanis et al.2018 perform a significant amount of program preprocessing including type inference , control flow , and data flow analysis to add different types of graph edges . Without such pre-processing , they achieve an accuracy of 55.3 % on repair-only tasks ( Section 4.3 ) . In our work , we want our distributed representations to automatically learn good representations of programs without any manual feature engineering . Performance trade-off : In fact , our proposed architecture is significantly more scalable and easier to train . Since we are using sequence models to compute pointer attentions that are easier to batch over multiple examples , it is much more scalable to train compared to graph models that are difficult to batch because of different graph sizes . Our own graph implementation was significantly slower to train . In addition to that , it is also significantly faster at inference time , as it does not need to perform an O ( n ) number of model predictions , where n is the number of variable use locations in the program under test . For our model , it performs a single prediction , which is much faster . Please let us know if this helped clarify the questions and comments ."}}