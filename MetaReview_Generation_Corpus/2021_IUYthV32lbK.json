{"year": "2021", "forum": "IUYthV32lbK", "title": "On the Certified Robustness for Ensemble Models and Beyond", "decision": "Reject", "meta_review": "I thank the authors and reviewers for the lively discussions. Although reviewers agreed the work is interesting, there are some concerns about the significance of the results and experiments. None of the reviewers were strongly supportive of the paper while majority of them suggest that the paper needs a bit more work before being accepted. Also, reviewers suggest that the paper is not easy to follow and its writing should be improved. Given all, I think the paper , at the current stage, is below the accept threshold. I encourage authors to edit the paper according to the suggestions by the reviewers. ", "reviews": [{"review_id": "IUYthV32lbK-0", "review_text": "The paper offers a novel idea regarding improving robustness of ensemble models with a rigorous mathematical background . A comparison between two types of ensemble models ( Weighted Ensembles and Max-Margin Ensembles ) and to single models offers very good insight into the theoretical dynamics of certified robustness . Unfortunately , the presented methods for practical applications are two simple regularization terms ( of questionable form , see below ) , and , more importantly the authors are not able to conclusively show any benefits empirically - only MNIST and CIFAR10 show improvements vs baselines , ImageNet is evaluated as the only large-scale dataset , but with negative results . Positive : * a proof is offered why certain regularization is beneficial for robustness , while other works are only based on empirical results * the work consists of several proofs , greatly extending the theoretical background of ensemble model robustness * even if you ignore the proposed regularization term , theoretical insights of the robustness of Weighted Ensemble vs Max-Margin Ensemble vs single model are of value for further understanding the topic . ( Figure 3 in the appendix is great ) * the regularization term seems to work consistently well for CIFAR10 and MNIST Negative : * The paper has too much information in too little space - \u201c related work \u201d and \u201c conclusion \u201d seem artificially short to fit the 8 pages . * The paper reads like two papers glued together ( one would be chapter 2 + chapter 4 + appendix E about the regularization term DRT , the other is chapter 3 + appendix C + D about the theoretical certified robustness of the ensemble types with smoothness ) . The paper has two main pillars ( like mentioned above : chapter 2 and chapter 3 ) , but chapter 3 offers no results/conclusions for chapter 4 ( the experiments ) , it seems strangely misplaced between ch . 2 and ch.4 . * some notations are a bit sloppy , some assumptions are questionable * the GD regularization term ( Gradient diversity loss ) should have a different form : In the previous section the authors state that \u201c .. the magnitude of gradient sum could be efficiently reduced as long as their directions are diverse. \u201d , but the GD term is still the L2 norm instead of cosine similarity . * Experiments are extremely limited and the proposed method does not work on large-scale data Detailed comments : * In Sub-Section \u201c Key factors for the certified robustness of an ensemble \u201d ( p. 5 ) : The authors write \u201c Though reducing the gradient magnitude [ something positive ] , it may hurt the model accuracy significantly \u201d and continue showing how the L2 norm of the sum of two vectors contains the cosine similarity , which is critical for model diversity . Furthermore they argue that by reducing the L2 norm the cosine similarity is also reduced and continue using L2 as their regularization term called Gradient Diversity Loss . But if reducing the base model gradient norm is potentially so bad , then why is the regularization not exclusively the cosine similarity between the two gradients ? This should be evaluated . * based on the analysis , the authors argue the confidence score margin has also important influence on the robustness , so it is artificially increased through another regularization term called Confidence Margin Loss . This opens two follow-up questions : * naturally cross entropy already tries to reach high confidence scores , so the question is if the Confidence Margin Loss achieves anything that cross entropy implicitly does not , or if it maybe only stabilizes training convergence . I would like to see some results on that * By increasing the confidence scores further , any improvement in regarding robustness may come at the cost of an increased expected calibration error - this would be interesting to evaluate * Ablation study : an additional evaluation of how beneficial each loss term is for the robustness is missing * Theorem 4 and onwards : The random variable \u201c epsilon \u201d can not by element of R^d , since a random variable is a function . * \u201c Definition 1 \u201d has an important typo : \u201c epsilon \u201d should be \u201c r \u201d , otherwise the term \u201c r-robust \u201d makes no sense , if no r is present in the definition * section 2.1 : The whole paper is about Weighted Ensembles and Max-Margin-Ensembles , but their formal definitions are moved to the appendix , please move to main text * Proposition 1 : In \u201c min \u201d , \u201c y_i =/= y_0 \u201d is not required because if y_i == y_0 , the term is equal 0 ( this is not an error , but less clutter is preferable ) * Definition 2 : the second gradient is w.r.t.x , but the argument is y * Theorem 4 and 5 , and Corollary 2 assume the confidence scores across several base models are i.i.d.and symmetric random variables . I am not usure if this assumption holds in any case and would appreciate some discussion here : How can the output be seen as symmetrical ? Confidence scores are between 0 and 1 , so exact symmetry is almost always impossible as long as they are close to 1 ( which they are due to the training and if they are in-domain ) ... # # # # # # Post-rebuttal # # # # The authors have addressed some of my concerns and I have raised my score accordingly", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer \u2019 s appreciation for our work , and we address the questions below . \\ > Paper structure . In this paper , we aim to conduct both theoretical and empirical analysis , that means , in chapter 2 we set up the formulations for different ensemble protocols and analyze the general robustness conditions , in chapter 3 we analyze the concrete certified robustness of different ensembles with randomized smoothing , and in chapter 4 we conduct the extensive experiments . In particular , the analysis in chapter 2 motivates the GD loss and CM loss in our proposed regularizer , and chapter 3 further verifies the CM loss ( in discussion at the end of 3.1 ) . Overall , we hope our paper can provide solid theoretical justification for the proposed regularizer DRT ( chapter 2,3 ) , as well as comprehensive experimental results and comparison with baselines ( chapter 4 ) . With one more page limit for the revision , we will expand the \u201c related work \u201d and \u201c conclusion \u201d with more details as well . \\ > The form of Gradient Diversity ( GD ) regularization term . Thanks for pointing it out . Given $ \\|a+b\\|_2 = \\sqrt { a^2 + b^2 + 2\\|a\\| \\|b\\| cos ( a , b ) } $ , we can see that increasing direction diversity is equivalent to minimizing the magnitude of the joint gradient . We finally use the L2 magnitude of joint gradient here since : ( 1 ) the direction diversity itself , if measured by cosine similarity , is narrowly bounded in $ [ -1,1 ] $ , and numerically hard to regularize due to the denominator $ \\|a\\| \\|b\\| $ ; ( 2 ) the analysis shows that small gradient magnitude also helps , and joint L2 can regularize both the gradient magnitude and direction diversity . Actually , minimizing the joint L2 magnitude implies an interplay between gradient magnitude and direction diversity . A small joint L2 magnitude can be achieved by either diversified directions or small L2 magnitude from individual gradients . As we discussed above , solely regularizing gradient direction itself is hard . As discussed in the paper , solely regularizing the individual gradient hurts the model performance . However , the joint L2 combines the two goals together and circumvents these difficulties . We believe this simple but effective GD regularization term can benefit general robust ensemble training and interest the conference audience as well as the community . We will make this contribution clear in our revision . \\ > Limited Experiments and ImageNet results . We did several numerical experiments as shown in Appendix D.4 and extensive empirical certified robustness evaluation on MNIST , CIFAR , ImageNet dataset while the DRT shows its strength on MNIST , CIFAR . For ImageNet , the performance was limited due to the computation cost so we only tried one parameter set for each setting . We will follow the suggestion and tune other parameters to report the full set of results on ImageNet in our revision . Here are the updated results on ImageNet : ImageNet : | & emsp ; & emsp ; & emsp ; Radius $ r $ | & emsp ; $ 0.00 $ | & emsp ; $ 0.50 $ | & emsp ; $ 1.00 $ | & emsp ; $ 1.50 $ | & emsp ; $ 2.00 $ | & emsp ; $ 2.50 $ | & emsp ; $ 3.00 $ | | : :| : :| : :| : :| : :| : :| : :| : :| | Gaussian | & emsp ; 57 | & emsp ; 46 | & emsp ; 37 | & emsp ; 29 | & emsp ; 19 | & emsp ; 15 | & emsp ; 12 | | MME ( Gaussian ) | & emsp ; 58 | & emsp ; 47 | & emsp ; 38 | & emsp ; 31 | & emsp ; 21 | & emsp ; 16 | & emsp ; 14 | | DRT + MME ( Gaussian ) | & emsp ; 52 | & emsp ; 46 | & emsp ; 42 | & emsp ; 34 | & emsp ; 24 | & emsp ; 19 | & emsp ; 18 | | SmoothAdv | & emsp ; 54 | & emsp ; 49 | & emsp ; 43 | & emsp ; 37 | & emsp ; 27 | & emsp ; 25 | & emsp ; 20 | | MME ( SmoothAdv ) | & emsp ; 55 | & emsp ; 50 | & emsp ; 44 | & emsp ; 38 | & emsp ; 27 | & emsp ; 26 | & emsp ; 21 | | DRT + MME ( SmoothAdv ) | & emsp ; 49 | & emsp ; 44 | & emsp ; 42 | & emsp ; 37 | & emsp ; * * 29 * * | & emsp ; * * 27 * * | & emsp ; * * 22 * * | That shows , DRT can indeed improve the certified accuracy on the large radius even with limited parameter tuning ."}, {"review_id": "IUYthV32lbK-1", "review_text": "Certified robustness approaches have been studied for single models based on interval propagation as well as randomized smoothing . The use of ensembles for empirical robustness has also been studied in the literature . This paper attempts to theoretically study the certifiable defense achieved by ensembles . The paper analyzes the standard Weighted Ensemble ( WE ) and MaxMargin Ensemble ( MME ) protocols , and proves the necessary and sufficient conditions for robustness under smoothness assumptions . The key idea is to show and utilize the diversification of gradients and large confidence margins . Pros : + The paper addresses an important challenge of adversarial robustness via ensembles and attempts to develop theoretical bounds for these defenses . Cons : - Some theoretical discussion is rather straight-forward formulation of consequences of the definitions of robustness and ensemble methods . Proposition 1 and Theorem 1 follow directly from the definition of WE and MME , and r-robustness . - There are several notation lapses which make the paper difficult to read . In definition 2 , it appears the intent is to take the partial gradient of the model at two different points x and y and then bound the ration of the difference of the gradients and the distance between two points ( which would be the curvature ) . x , y are being used as values and the variable . The boldface font is not used consistently . Another example is the missing relation between r and epsilon ( which is described in the appendix ) . These make simple results difficult to parse , and negatively effect the readability of the paper and obscure the identification of novelty . - For Theorem 2 , let us set N = 1 ( that is the degenerate case when the ensemble is a single model ) , we see that the rate of change of difference between the top prediction and other predictions are now bounded by a linear spread of the difference divided by the robustness radius r and then we add or subtract ( depending on sufficiency/necessity ) the impact of curvature . Rearranging the terms so that first and third are on the same side , the theorem will read : r ( gradient term for prediction difference ) +/- r^2 ( curvature term for prediction difference ) < = prediction difference . Is n't this just a Taylor series approximation of the prediction difference using bounds on the curvature term ( assumed as part of the definition ) ? Now , if we bring back any arbitrary N , the same would be applicable by the definition of how decisions are made by WE ( the `` prediction difference '' term is now changed ) . Theorem 3 terms can be similarly rearranged to make the statement more obvious . Is the reviewer missing some non-obvious observation or challenge in proving Theorem 2/3 ? - Despite several other papers also using the general term of `` certified robustness '' , it is important to note that this robustness is against a rather benign perturbation model . Typical perturbations ( in particular adversarial attacks or other natural perturbations such as fog , rain for vision ) do not fall into this class . But the reviewer is not concerned with this aspect heavily given the prevalence of the rather generic name of `` certified robustness '' for such approaches . - The empirical enforcement of diversity in ensembles has been studied in literature such as Pang et al 2019. https : //arxiv.org/abs/1901.09981 uses cosine similarity . Experimental evaluation with these methods is crucial to understand the value of the proposed approach . The current evaluation is very limited . Questions and suggestions to the authors : 1 . It might be a good idea to bring the relationship between r and epsilon in the main text from the appendix . r-robustness is the fundamental definition and in its current presentation r does not occur anywhere . 2.Could you help understand the concern in Weakness bit 3 ? This will help the reviewer better appreciate the theoretical novelty of the paper . Currently , the theoretical statements appear to make obvious statements once we get through the rather clunky notation and presentation choices . 3.Why is cosine used for diversity ? There are several ways to enforce diversity . Was cosine selected arbitrarily or is it motivated by some theoretical insight ? The paper addresses an important challenge of mathematically well-motivated defenses against perturbations . In its current form , the paper appears to formulate rather simplistic observations as theoretical results with a rather dense presentation . The empirical evaluation is significantly incomplete . After discussion with authors - The reviewer thanks the authors for clarifications . With the confusion from the typos resolved , the paper is easier to follow . The technical correctness of the paper is not in doubt any more . But two major concerns still remain : 1 . The diversity enforcement has been reported before , and more comprehensive discussion of related work would be useful . A detailed empirical analysis would also make the paper balanced and not heavily reliant on the novelty/depth of theoretical contribution . 2.The main claimed theoretical contribution summarized in Lemma B.1 is tedious but a derivation of an obvious statement ( as sketched out in the original review ) . The reviewer is raising the score to encourage this work , but still holding to the recommendation of not accepting the paper in its current form .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your constructive comments on this work . Here are the answers to your major concerns . \\ > Some theoretical discussion is rather straight-forward formulation . Though Theorem 1 may look intuitive at the first glance , it is non-trivially followed from the definition of MME . Concretely , MME selects the base model that has the maximum margin between the top and runner-up class . This \u201c maximum \u201d is a discrete operator and poses challenges especially in the multi-class setting . We can not assert that the model predicts the true label by simply looking at the margins between only the true label and other labels unless carefully filtering out possible violated cases as we do in our Lemma B.1 . With the lemma , we derive Theorem 1 . And Theorem 1 lays the foundation for the subsequent continuous analysis of the discrete MME . On the other hand , the obvious of the final theoretical conclusion also verifies its correctness . \\ > Notation lapses . Thanks for pointing out ! We will fix the typo in Definition 2 : The $ \\ { x_0 + \\delta : \\|\\delta\\|_2 \\le \\epsilon\\ } $ is actually $ \\ { x_0 + \\delta : \\|\\delta\\|_2 \\le r\\ } $ . The boldface and gradient variable will be revised to be consistent and correct . We will also make it clear that the beta-smoothness definition ( inherited from optimization literature ) is equivalent to the curvature bound ( as recently used in certified robustness literature ) . \\ > Theorems 2 and 3 are pure Taylor series approximation . Theorems 2 and 3 combine preceding theoretical findings with Taylor expansion . For example , the ( Sufficient Condition ) of Theorem 3 is based on non-trivial Lemma B.1 . Also , from the theorems , we show that for both ensembles , especially MME , we can directly apply Taylor expansion on the vector sum of confidence scores of base models . This is a nice property that typical NNs do not have due to nonlinearity such as softmax . These theorems enable us to have a non-trivial quantitative robustness comparison between the ensembles and the base model in Corollary 1 . \\ > The use of the general term \u201c certified robustness \u201d . Though there is a growing body of certified robustness for threat models beyond Lp , since the certified robustness for Lp is still unsatisfactory , the existing work still mainly focuses on this fundamental setting [ 1 ] . Therefore , by default , the literature uses the \u201c certified robustness \u201d to refer to the Lp threat model [ 2,3 ] . The generalization to other threat models often uses specific terms [ 4,5 ] . [ 1 ] L Li et al . `` SoK : Certified Robustness for Deep Neural Networks . '' arXiv:2009.04131 . [ 2 ] S Singla and S Feizi . `` Second-Order Provable Defenses against Adversarial Attacks . '' ICML 2020 . [ 3 ] H Zhang et al . `` Towards Stable and Efficient Training of Verifiably Robust Neural Networks . '' ICLR 2019 . [ 4 ] M Jeet , et al . `` Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations . '' CVPR 2020 . [ 5 ] R , Anian , et al . `` Efficient Certification of Spatial Robustness . '' arXiv preprint arXiv:2009.09318 ."}, {"review_id": "IUYthV32lbK-2", "review_text": "This manuscript provides proofs for certified robustness for ensembles and proposes a new approach called Diversity Regularized Training ( DRT ) based on the theoretical findings . In addition to the standard loss term , DRT contains two regularization terms : ( i ) gradient diversity ( GD ) loss , and ( ii ) confidence margin loss ( CM ) . These loss terms encourage the joint gradient difference for each model pair and large margin between the true and runner-up classes for base models . The authors discuss some theoretical findings in detail and demonstrate the performance of DRT on several datasets . I find the work valuable in the sense that it nicely combines ideas of ensembling and certified robustness . However , it was difficult to follow all the theoretical results and how they motivated the main finding ( DRT ) was not clear . Below are my comments/questions : - I want to thank the authors for nice summary in Appendix B1 and B2 on certified robustness and their map to ensembles . - Theorem 1 , which serves as the foundation for the paper , assumes that either best prediction or the runner-up prediction is true class for any base model . I wonder how realistic this assumption is . - Also in theorem 1 , I am confused that both f and y have index i . The number of base models does not need to match the number of classes . - In proof for Theorem 2 , it could be helpful to mention each step . For instance , it was not clear to me where Lagrangian reminder was applied . Also , I recommend proving necessary and sufficient conditions separately . - In theorem 3 , N is assumed to be 2 . Is it possible to extend this to N > 2 ? So , discussion on such this could be helpful . - In the first paragraph on page 5 , I do not follow which equation was meant in RHS . - These statements on page 5 are not clear to me : `` ... and leads to higher certified ensemble robustness . '' and `` Thus , increasing confidence margins can lead to higher ensemble robustness . '' - For GD loss , why only pairwise summations were considered ? Is n't it easier to look at overall diversity of gradients ? - All pair-wise computation of quantities in regularizer terms should come with some computational complexity . It would be nice to include a discussion on this . - Results for Salman et al. , 2019 on Table 2 do not match the paper of Salman et al. , 2019 and imagenet results are not convincing . I understand ImageNet data can take too long but should we worry that DRT requires more hyper-parameter tuning ? Some discussion on this would be helpful .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your inspiring comments and appreciation of our work . We answer the questions below . \\ > It was difficult to follow all the theoretical results and how they motivated the main finding ( DRT ) was not clear . Thanks for the suggestions ! Below we make a brief summary and provide intuition for the theoretical results , and we will also add them to our revision . For the theoretical results in our paper , we start by analyzing the general robustness condition of two types of ensembles : WE and MME ( Section 2 , Proposition 1 , and Theorem 1 ) . Based on Taylor expansion , for either ensemble , we can see that to achieve high certified robustness , we need to reduce the L2 magnitude of the joint gradient vector ( i.e. , increase gradient diversity ) and increase the confidence margin between the true label and any other label ( Section 2 , Theorem 2 and Theorem 3 ) . Furthermore , we compare the certified robust radius between both ensemble models and the base models ( Section 2 , Corollary 1 ) . Based on these two factors : increase gradient diversity & increase the confidence margin , we propose the two regularization terms : GD loss and CM loss respectively . The DRT is composed of these two terms . Then in Section 3 , we further analyze the certified robustness under the randomized smoothing setting . Here , when randomized smoothing is applied , we can bound the smoothness of the smoothed classifier and thus derive a computable certified robust radius based on the statistical robustness bound ( Section 3 , Definition 3 ) . Note that this certified robust radius is not tight , i.e. , it is usually smaller than the actual robust radius but it is computable . Since the radius is computed from the statistical robustness bound , we analyze the statistical robustness bound for both ensembles ( Section 3 , Theorem 4 & 5 ) . The results show that whether WE or MME is better depends on the transferability among base models ( Corollary 2 ) . These theorems are mainly derived from the concentration bounds of the average of independent random variables . Then the results ( Theorem 4 & 5 ) justify the use of DRT , especially the CM loss , as shown at the end of Section 3.1 . In Section 4 , we empirically evaluate the DRT and show it achieves state-of-the-art certified robustness compared with other single-model training approaches and ensemble training approaches . \\ > Top-class and runner-up prediction assumption . We observe that when the ensemble gives the right prediction for a clean sample $ x $ , there must exist a radius $ r $ around $ x $ , such that the true label is still the runner-up or top class in $ r $ -ball as long as the confidence change is continuous . Therefore , the assumption just impedes us to reason about arbitrarily large $ r $ , but we can still reason about moderately large $ r $ . Moreover , for WE we do not have this assumption , and for the theorems for the smoothed version ( in Section 3 ) we do not need this assumption either . \\ > The number of base models does not need to match the number of classes . Thanks for pointing this out . Yes , you are right and we will make this clear in our revision . Currently , $ C $ represents # class , and $ N $ the # base models . In Theorem 1 , the superscript $ y_i $ indexes the classes other than $ y_0 $ for each base model $ i $ individually . \\ > In the proof for Theorem 2 , it could be helpful to mention each step . Thanks for the comment , and we will make the proof in Theorem 2 clear in the revision . \\ > In theorem 3 , $ N $ is assumed to be 2 . Is it possible to extend this to $ N > 2 $ ? In the smoothed version in Section 3 , we extend the results beyond $ N = 2 $ in Theorems 4 , 5 , and Corollary 2 . In this vanilla version , WE can be extended to $ N = 2 $ , and for MME , the challenge is the \u201c maximum \u201d operator in MME . When $ N = 2 $ , the sign of the sum of confidence score differences can indicate which base model the MME will choose . However , it does not hold for $ N > 2 $ , where the \u201c maximum \u201d operator can not be eliminated easily . We will add this discussion to the revision . \\ > In the first paragraph on page 5 , I do not follow which equation was meant in RHS . The RHS means $ r \\cdot \\frac { 1-\\Delta } { 1+\\Delta } ( 1-C_ { \\text { MME or ME } } ( 1-cos\\theta ) ) ^ { -1/2 } $ ."}, {"review_id": "IUYthV32lbK-3", "review_text": "This paper studies the following problem : How to train a certifiably robust classifier with ensemble methods ? The authors considered two types of ensembles : the weighted-average ensemble and large-margin ensemble . They first derived theoretically sufficient and necessary conditions for robustness under two types of ensembles , with the conclusion that large confidence margin and diversified gradients are two factors which contributes to the robustness of ensemble models . Diversity-regularized training , a method of designing loss functions for training ensemble models , is proposed motivated by their theoretical findings . They applied this methodology to randomized smoothing , performed extensive experiments and showed non-trivial improvement over single model methods . The paper is very well-written and provided extremely detailed discussion about many different aspects of the problem , both theoretically and empirically . From the reviewer 's point of view , this is the strongest part of this work . I am very impressed by the level of detail in the appendix , which covers many interesting questions like under which scenario is WE better than MME , why is ensemble before smoothing better than ensemble after smoothing , to name a few . I wish more papers in the community are written in this way . However , my current evaluation to this paper is a weak accept - it is a bit conservative , but I think it 's based on some valid concerns , detailed below . The experimental results , while showed non-trivial improvements over single model baselines , may not be very strong . In most cases , the improvements are like 3~4 % , sometimes a little over 5 % in smaller radius settings comparing to smoothadv . This improvement is much smaller than some of the earlier works like smoothadv vs gaussian . Also in certain settings , the improvement over other single model baselines becomes very small ( e.g. < 2 % over MACER in Table 12 ) . The performance is also overall very similar to recent ensemble baselines like Liu et al.2020.My understanding is that this shows a limitation of ensemble-based methods in certifiable robustness . To summarize , despite of the limitations mentioned above , I think this work is overall good enough for recommending acceptance and thank the authors for their effort . Minor comments : I do n't quite get the point of appendix D.3 , in particular , the reasoning that trying to justify the uniform distribution assumption of confidence scores . The concentration of measure in high dimensional Gaussian does not imply the uniformity of confidence scores . Although I do understand this assumption as a concrete example trying to get more interpretable results .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your thoughtful suggestions . We checked your questions carefully and answered them as follows . \\ > Limited improvement of the experiment results . First , in this paper we mainly want to emphasize our analysis framework and the methodology , and we use the experimental results as a demonstration without tuning the parameters very carefully . Following your suggestion , we actually tune our parameters a bit and we find that we can indeed get better performance as follows : MNIST : | & emsp ; & emsp ; & emsp ; & nbsp ; & nbsp ; & nbsp ; Radius $ r $ | & emsp ; ` $ 1.25 $ | & emsp ; $ 1.50 $ | & emsp ; $ 1.75 $ | & emsp ; $ 2.00 $ | & emsp ; $ 2.25 $ | & emsp ; $ 2.50 $ | | : :| : :| : :| : :| : :| : :| : :| | Gaussian | & emsp ; ` 83.0 | & emsp ; ` 68.2 | & emsp ; ` 46.6 | & emsp ; ` 33.0 | & emsp ; ` 20.5 | & emsp ; ` 11.5 | | MME ( Gaussian ) | & emsp ; ` 84.3 | & emsp ; ` 69.8 | & emsp ; ` 48.8 | & emsp ; ` 34.7 | & emsp ; ` 23.4 | & emsp ; ` 12.7 | | DRT + MME ( Gaussian ) | & emsp ; ` 86.8 | & emsp ; ` 75.2 | & emsp ; ` 55.8 | & emsp ; ` 44.8 | & emsp ; ` 38.0 | & emsp ; ` 27.0 | | SmoothAdv | & emsp ; ` 87.7 | & emsp ; ` 80.2 | & emsp ; ` 66.3 | & emsp ; ` 43.2 | & emsp ; ` 34.3 | & emsp ; ` 24.0 | | MME ( SmoothAdv ) | & emsp ; ` 88.1 | & emsp ; ` 80.7 | & emsp ; ` 67.9 | & emsp ; ` 44.8 | & emsp ; ` 35.0 | & emsp ; ` 25.2 | | DRT + MME ( SmoothAdv ) | & emsp ; ` * * 88.5 * * | & emsp ; * * ` 83.2 * * | & emsp ; * * ` 68.9 Indeed , for the small radius , the improvement is limited , and we hypothesize that \u2019 s due to the sub-models \u2019 capacity limit within the ensemble , which shows the limitation of ensemble-based methods as you mentioned and we will add related discussion in our revision . However , the significant improvement of the DRT ensemble model compared to the ensemble without DRT indicates that DRT could be a very effective and general way to improve the certified robustness of the ensemble . \\ > Questions about the uniform distribution assumption . Thanks for the question and this assumption is actually for a case study and we will make it clear in our revision . Without the uniform distribution assumption , we can prove a general case that the transferability across base models decides which ensemble is better as shown in Appendix D.2 . Here we just want to give an illustration in a specific regim and therefore we make the uniform assumption which may not hold exactly in practice . We think it would be an interesting future direction to generalize the analysis to other distributions such as the Gaussian distribution which corresponds to locally linear classifiers . We will make this discussion clear in Appendix D.3 \u2019 s remark . Thanks for your thoughtful suggestions and appreciation . We will update these new results in our revision and make the whole paper to be clearer . Hope we have answered all your questions , so that you can increase your score . We are also happy to discuss further for other concerns ."}], "0": {"review_id": "IUYthV32lbK-0", "review_text": "The paper offers a novel idea regarding improving robustness of ensemble models with a rigorous mathematical background . A comparison between two types of ensemble models ( Weighted Ensembles and Max-Margin Ensembles ) and to single models offers very good insight into the theoretical dynamics of certified robustness . Unfortunately , the presented methods for practical applications are two simple regularization terms ( of questionable form , see below ) , and , more importantly the authors are not able to conclusively show any benefits empirically - only MNIST and CIFAR10 show improvements vs baselines , ImageNet is evaluated as the only large-scale dataset , but with negative results . Positive : * a proof is offered why certain regularization is beneficial for robustness , while other works are only based on empirical results * the work consists of several proofs , greatly extending the theoretical background of ensemble model robustness * even if you ignore the proposed regularization term , theoretical insights of the robustness of Weighted Ensemble vs Max-Margin Ensemble vs single model are of value for further understanding the topic . ( Figure 3 in the appendix is great ) * the regularization term seems to work consistently well for CIFAR10 and MNIST Negative : * The paper has too much information in too little space - \u201c related work \u201d and \u201c conclusion \u201d seem artificially short to fit the 8 pages . * The paper reads like two papers glued together ( one would be chapter 2 + chapter 4 + appendix E about the regularization term DRT , the other is chapter 3 + appendix C + D about the theoretical certified robustness of the ensemble types with smoothness ) . The paper has two main pillars ( like mentioned above : chapter 2 and chapter 3 ) , but chapter 3 offers no results/conclusions for chapter 4 ( the experiments ) , it seems strangely misplaced between ch . 2 and ch.4 . * some notations are a bit sloppy , some assumptions are questionable * the GD regularization term ( Gradient diversity loss ) should have a different form : In the previous section the authors state that \u201c .. the magnitude of gradient sum could be efficiently reduced as long as their directions are diverse. \u201d , but the GD term is still the L2 norm instead of cosine similarity . * Experiments are extremely limited and the proposed method does not work on large-scale data Detailed comments : * In Sub-Section \u201c Key factors for the certified robustness of an ensemble \u201d ( p. 5 ) : The authors write \u201c Though reducing the gradient magnitude [ something positive ] , it may hurt the model accuracy significantly \u201d and continue showing how the L2 norm of the sum of two vectors contains the cosine similarity , which is critical for model diversity . Furthermore they argue that by reducing the L2 norm the cosine similarity is also reduced and continue using L2 as their regularization term called Gradient Diversity Loss . But if reducing the base model gradient norm is potentially so bad , then why is the regularization not exclusively the cosine similarity between the two gradients ? This should be evaluated . * based on the analysis , the authors argue the confidence score margin has also important influence on the robustness , so it is artificially increased through another regularization term called Confidence Margin Loss . This opens two follow-up questions : * naturally cross entropy already tries to reach high confidence scores , so the question is if the Confidence Margin Loss achieves anything that cross entropy implicitly does not , or if it maybe only stabilizes training convergence . I would like to see some results on that * By increasing the confidence scores further , any improvement in regarding robustness may come at the cost of an increased expected calibration error - this would be interesting to evaluate * Ablation study : an additional evaluation of how beneficial each loss term is for the robustness is missing * Theorem 4 and onwards : The random variable \u201c epsilon \u201d can not by element of R^d , since a random variable is a function . * \u201c Definition 1 \u201d has an important typo : \u201c epsilon \u201d should be \u201c r \u201d , otherwise the term \u201c r-robust \u201d makes no sense , if no r is present in the definition * section 2.1 : The whole paper is about Weighted Ensembles and Max-Margin-Ensembles , but their formal definitions are moved to the appendix , please move to main text * Proposition 1 : In \u201c min \u201d , \u201c y_i =/= y_0 \u201d is not required because if y_i == y_0 , the term is equal 0 ( this is not an error , but less clutter is preferable ) * Definition 2 : the second gradient is w.r.t.x , but the argument is y * Theorem 4 and 5 , and Corollary 2 assume the confidence scores across several base models are i.i.d.and symmetric random variables . I am not usure if this assumption holds in any case and would appreciate some discussion here : How can the output be seen as symmetrical ? Confidence scores are between 0 and 1 , so exact symmetry is almost always impossible as long as they are close to 1 ( which they are due to the training and if they are in-domain ) ... # # # # # # Post-rebuttal # # # # The authors have addressed some of my concerns and I have raised my score accordingly", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer \u2019 s appreciation for our work , and we address the questions below . \\ > Paper structure . In this paper , we aim to conduct both theoretical and empirical analysis , that means , in chapter 2 we set up the formulations for different ensemble protocols and analyze the general robustness conditions , in chapter 3 we analyze the concrete certified robustness of different ensembles with randomized smoothing , and in chapter 4 we conduct the extensive experiments . In particular , the analysis in chapter 2 motivates the GD loss and CM loss in our proposed regularizer , and chapter 3 further verifies the CM loss ( in discussion at the end of 3.1 ) . Overall , we hope our paper can provide solid theoretical justification for the proposed regularizer DRT ( chapter 2,3 ) , as well as comprehensive experimental results and comparison with baselines ( chapter 4 ) . With one more page limit for the revision , we will expand the \u201c related work \u201d and \u201c conclusion \u201d with more details as well . \\ > The form of Gradient Diversity ( GD ) regularization term . Thanks for pointing it out . Given $ \\|a+b\\|_2 = \\sqrt { a^2 + b^2 + 2\\|a\\| \\|b\\| cos ( a , b ) } $ , we can see that increasing direction diversity is equivalent to minimizing the magnitude of the joint gradient . We finally use the L2 magnitude of joint gradient here since : ( 1 ) the direction diversity itself , if measured by cosine similarity , is narrowly bounded in $ [ -1,1 ] $ , and numerically hard to regularize due to the denominator $ \\|a\\| \\|b\\| $ ; ( 2 ) the analysis shows that small gradient magnitude also helps , and joint L2 can regularize both the gradient magnitude and direction diversity . Actually , minimizing the joint L2 magnitude implies an interplay between gradient magnitude and direction diversity . A small joint L2 magnitude can be achieved by either diversified directions or small L2 magnitude from individual gradients . As we discussed above , solely regularizing gradient direction itself is hard . As discussed in the paper , solely regularizing the individual gradient hurts the model performance . However , the joint L2 combines the two goals together and circumvents these difficulties . We believe this simple but effective GD regularization term can benefit general robust ensemble training and interest the conference audience as well as the community . We will make this contribution clear in our revision . \\ > Limited Experiments and ImageNet results . We did several numerical experiments as shown in Appendix D.4 and extensive empirical certified robustness evaluation on MNIST , CIFAR , ImageNet dataset while the DRT shows its strength on MNIST , CIFAR . For ImageNet , the performance was limited due to the computation cost so we only tried one parameter set for each setting . We will follow the suggestion and tune other parameters to report the full set of results on ImageNet in our revision . Here are the updated results on ImageNet : ImageNet : | & emsp ; & emsp ; & emsp ; Radius $ r $ | & emsp ; $ 0.00 $ | & emsp ; $ 0.50 $ | & emsp ; $ 1.00 $ | & emsp ; $ 1.50 $ | & emsp ; $ 2.00 $ | & emsp ; $ 2.50 $ | & emsp ; $ 3.00 $ | | : :| : :| : :| : :| : :| : :| : :| : :| | Gaussian | & emsp ; 57 | & emsp ; 46 | & emsp ; 37 | & emsp ; 29 | & emsp ; 19 | & emsp ; 15 | & emsp ; 12 | | MME ( Gaussian ) | & emsp ; 58 | & emsp ; 47 | & emsp ; 38 | & emsp ; 31 | & emsp ; 21 | & emsp ; 16 | & emsp ; 14 | | DRT + MME ( Gaussian ) | & emsp ; 52 | & emsp ; 46 | & emsp ; 42 | & emsp ; 34 | & emsp ; 24 | & emsp ; 19 | & emsp ; 18 | | SmoothAdv | & emsp ; 54 | & emsp ; 49 | & emsp ; 43 | & emsp ; 37 | & emsp ; 27 | & emsp ; 25 | & emsp ; 20 | | MME ( SmoothAdv ) | & emsp ; 55 | & emsp ; 50 | & emsp ; 44 | & emsp ; 38 | & emsp ; 27 | & emsp ; 26 | & emsp ; 21 | | DRT + MME ( SmoothAdv ) | & emsp ; 49 | & emsp ; 44 | & emsp ; 42 | & emsp ; 37 | & emsp ; * * 29 * * | & emsp ; * * 27 * * | & emsp ; * * 22 * * | That shows , DRT can indeed improve the certified accuracy on the large radius even with limited parameter tuning ."}, "1": {"review_id": "IUYthV32lbK-1", "review_text": "Certified robustness approaches have been studied for single models based on interval propagation as well as randomized smoothing . The use of ensembles for empirical robustness has also been studied in the literature . This paper attempts to theoretically study the certifiable defense achieved by ensembles . The paper analyzes the standard Weighted Ensemble ( WE ) and MaxMargin Ensemble ( MME ) protocols , and proves the necessary and sufficient conditions for robustness under smoothness assumptions . The key idea is to show and utilize the diversification of gradients and large confidence margins . Pros : + The paper addresses an important challenge of adversarial robustness via ensembles and attempts to develop theoretical bounds for these defenses . Cons : - Some theoretical discussion is rather straight-forward formulation of consequences of the definitions of robustness and ensemble methods . Proposition 1 and Theorem 1 follow directly from the definition of WE and MME , and r-robustness . - There are several notation lapses which make the paper difficult to read . In definition 2 , it appears the intent is to take the partial gradient of the model at two different points x and y and then bound the ration of the difference of the gradients and the distance between two points ( which would be the curvature ) . x , y are being used as values and the variable . The boldface font is not used consistently . Another example is the missing relation between r and epsilon ( which is described in the appendix ) . These make simple results difficult to parse , and negatively effect the readability of the paper and obscure the identification of novelty . - For Theorem 2 , let us set N = 1 ( that is the degenerate case when the ensemble is a single model ) , we see that the rate of change of difference between the top prediction and other predictions are now bounded by a linear spread of the difference divided by the robustness radius r and then we add or subtract ( depending on sufficiency/necessity ) the impact of curvature . Rearranging the terms so that first and third are on the same side , the theorem will read : r ( gradient term for prediction difference ) +/- r^2 ( curvature term for prediction difference ) < = prediction difference . Is n't this just a Taylor series approximation of the prediction difference using bounds on the curvature term ( assumed as part of the definition ) ? Now , if we bring back any arbitrary N , the same would be applicable by the definition of how decisions are made by WE ( the `` prediction difference '' term is now changed ) . Theorem 3 terms can be similarly rearranged to make the statement more obvious . Is the reviewer missing some non-obvious observation or challenge in proving Theorem 2/3 ? - Despite several other papers also using the general term of `` certified robustness '' , it is important to note that this robustness is against a rather benign perturbation model . Typical perturbations ( in particular adversarial attacks or other natural perturbations such as fog , rain for vision ) do not fall into this class . But the reviewer is not concerned with this aspect heavily given the prevalence of the rather generic name of `` certified robustness '' for such approaches . - The empirical enforcement of diversity in ensembles has been studied in literature such as Pang et al 2019. https : //arxiv.org/abs/1901.09981 uses cosine similarity . Experimental evaluation with these methods is crucial to understand the value of the proposed approach . The current evaluation is very limited . Questions and suggestions to the authors : 1 . It might be a good idea to bring the relationship between r and epsilon in the main text from the appendix . r-robustness is the fundamental definition and in its current presentation r does not occur anywhere . 2.Could you help understand the concern in Weakness bit 3 ? This will help the reviewer better appreciate the theoretical novelty of the paper . Currently , the theoretical statements appear to make obvious statements once we get through the rather clunky notation and presentation choices . 3.Why is cosine used for diversity ? There are several ways to enforce diversity . Was cosine selected arbitrarily or is it motivated by some theoretical insight ? The paper addresses an important challenge of mathematically well-motivated defenses against perturbations . In its current form , the paper appears to formulate rather simplistic observations as theoretical results with a rather dense presentation . The empirical evaluation is significantly incomplete . After discussion with authors - The reviewer thanks the authors for clarifications . With the confusion from the typos resolved , the paper is easier to follow . The technical correctness of the paper is not in doubt any more . But two major concerns still remain : 1 . The diversity enforcement has been reported before , and more comprehensive discussion of related work would be useful . A detailed empirical analysis would also make the paper balanced and not heavily reliant on the novelty/depth of theoretical contribution . 2.The main claimed theoretical contribution summarized in Lemma B.1 is tedious but a derivation of an obvious statement ( as sketched out in the original review ) . The reviewer is raising the score to encourage this work , but still holding to the recommendation of not accepting the paper in its current form .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your constructive comments on this work . Here are the answers to your major concerns . \\ > Some theoretical discussion is rather straight-forward formulation . Though Theorem 1 may look intuitive at the first glance , it is non-trivially followed from the definition of MME . Concretely , MME selects the base model that has the maximum margin between the top and runner-up class . This \u201c maximum \u201d is a discrete operator and poses challenges especially in the multi-class setting . We can not assert that the model predicts the true label by simply looking at the margins between only the true label and other labels unless carefully filtering out possible violated cases as we do in our Lemma B.1 . With the lemma , we derive Theorem 1 . And Theorem 1 lays the foundation for the subsequent continuous analysis of the discrete MME . On the other hand , the obvious of the final theoretical conclusion also verifies its correctness . \\ > Notation lapses . Thanks for pointing out ! We will fix the typo in Definition 2 : The $ \\ { x_0 + \\delta : \\|\\delta\\|_2 \\le \\epsilon\\ } $ is actually $ \\ { x_0 + \\delta : \\|\\delta\\|_2 \\le r\\ } $ . The boldface and gradient variable will be revised to be consistent and correct . We will also make it clear that the beta-smoothness definition ( inherited from optimization literature ) is equivalent to the curvature bound ( as recently used in certified robustness literature ) . \\ > Theorems 2 and 3 are pure Taylor series approximation . Theorems 2 and 3 combine preceding theoretical findings with Taylor expansion . For example , the ( Sufficient Condition ) of Theorem 3 is based on non-trivial Lemma B.1 . Also , from the theorems , we show that for both ensembles , especially MME , we can directly apply Taylor expansion on the vector sum of confidence scores of base models . This is a nice property that typical NNs do not have due to nonlinearity such as softmax . These theorems enable us to have a non-trivial quantitative robustness comparison between the ensembles and the base model in Corollary 1 . \\ > The use of the general term \u201c certified robustness \u201d . Though there is a growing body of certified robustness for threat models beyond Lp , since the certified robustness for Lp is still unsatisfactory , the existing work still mainly focuses on this fundamental setting [ 1 ] . Therefore , by default , the literature uses the \u201c certified robustness \u201d to refer to the Lp threat model [ 2,3 ] . The generalization to other threat models often uses specific terms [ 4,5 ] . [ 1 ] L Li et al . `` SoK : Certified Robustness for Deep Neural Networks . '' arXiv:2009.04131 . [ 2 ] S Singla and S Feizi . `` Second-Order Provable Defenses against Adversarial Attacks . '' ICML 2020 . [ 3 ] H Zhang et al . `` Towards Stable and Efficient Training of Verifiably Robust Neural Networks . '' ICLR 2019 . [ 4 ] M Jeet , et al . `` Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations . '' CVPR 2020 . [ 5 ] R , Anian , et al . `` Efficient Certification of Spatial Robustness . '' arXiv preprint arXiv:2009.09318 ."}, "2": {"review_id": "IUYthV32lbK-2", "review_text": "This manuscript provides proofs for certified robustness for ensembles and proposes a new approach called Diversity Regularized Training ( DRT ) based on the theoretical findings . In addition to the standard loss term , DRT contains two regularization terms : ( i ) gradient diversity ( GD ) loss , and ( ii ) confidence margin loss ( CM ) . These loss terms encourage the joint gradient difference for each model pair and large margin between the true and runner-up classes for base models . The authors discuss some theoretical findings in detail and demonstrate the performance of DRT on several datasets . I find the work valuable in the sense that it nicely combines ideas of ensembling and certified robustness . However , it was difficult to follow all the theoretical results and how they motivated the main finding ( DRT ) was not clear . Below are my comments/questions : - I want to thank the authors for nice summary in Appendix B1 and B2 on certified robustness and their map to ensembles . - Theorem 1 , which serves as the foundation for the paper , assumes that either best prediction or the runner-up prediction is true class for any base model . I wonder how realistic this assumption is . - Also in theorem 1 , I am confused that both f and y have index i . The number of base models does not need to match the number of classes . - In proof for Theorem 2 , it could be helpful to mention each step . For instance , it was not clear to me where Lagrangian reminder was applied . Also , I recommend proving necessary and sufficient conditions separately . - In theorem 3 , N is assumed to be 2 . Is it possible to extend this to N > 2 ? So , discussion on such this could be helpful . - In the first paragraph on page 5 , I do not follow which equation was meant in RHS . - These statements on page 5 are not clear to me : `` ... and leads to higher certified ensemble robustness . '' and `` Thus , increasing confidence margins can lead to higher ensemble robustness . '' - For GD loss , why only pairwise summations were considered ? Is n't it easier to look at overall diversity of gradients ? - All pair-wise computation of quantities in regularizer terms should come with some computational complexity . It would be nice to include a discussion on this . - Results for Salman et al. , 2019 on Table 2 do not match the paper of Salman et al. , 2019 and imagenet results are not convincing . I understand ImageNet data can take too long but should we worry that DRT requires more hyper-parameter tuning ? Some discussion on this would be helpful .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your inspiring comments and appreciation of our work . We answer the questions below . \\ > It was difficult to follow all the theoretical results and how they motivated the main finding ( DRT ) was not clear . Thanks for the suggestions ! Below we make a brief summary and provide intuition for the theoretical results , and we will also add them to our revision . For the theoretical results in our paper , we start by analyzing the general robustness condition of two types of ensembles : WE and MME ( Section 2 , Proposition 1 , and Theorem 1 ) . Based on Taylor expansion , for either ensemble , we can see that to achieve high certified robustness , we need to reduce the L2 magnitude of the joint gradient vector ( i.e. , increase gradient diversity ) and increase the confidence margin between the true label and any other label ( Section 2 , Theorem 2 and Theorem 3 ) . Furthermore , we compare the certified robust radius between both ensemble models and the base models ( Section 2 , Corollary 1 ) . Based on these two factors : increase gradient diversity & increase the confidence margin , we propose the two regularization terms : GD loss and CM loss respectively . The DRT is composed of these two terms . Then in Section 3 , we further analyze the certified robustness under the randomized smoothing setting . Here , when randomized smoothing is applied , we can bound the smoothness of the smoothed classifier and thus derive a computable certified robust radius based on the statistical robustness bound ( Section 3 , Definition 3 ) . Note that this certified robust radius is not tight , i.e. , it is usually smaller than the actual robust radius but it is computable . Since the radius is computed from the statistical robustness bound , we analyze the statistical robustness bound for both ensembles ( Section 3 , Theorem 4 & 5 ) . The results show that whether WE or MME is better depends on the transferability among base models ( Corollary 2 ) . These theorems are mainly derived from the concentration bounds of the average of independent random variables . Then the results ( Theorem 4 & 5 ) justify the use of DRT , especially the CM loss , as shown at the end of Section 3.1 . In Section 4 , we empirically evaluate the DRT and show it achieves state-of-the-art certified robustness compared with other single-model training approaches and ensemble training approaches . \\ > Top-class and runner-up prediction assumption . We observe that when the ensemble gives the right prediction for a clean sample $ x $ , there must exist a radius $ r $ around $ x $ , such that the true label is still the runner-up or top class in $ r $ -ball as long as the confidence change is continuous . Therefore , the assumption just impedes us to reason about arbitrarily large $ r $ , but we can still reason about moderately large $ r $ . Moreover , for WE we do not have this assumption , and for the theorems for the smoothed version ( in Section 3 ) we do not need this assumption either . \\ > The number of base models does not need to match the number of classes . Thanks for pointing this out . Yes , you are right and we will make this clear in our revision . Currently , $ C $ represents # class , and $ N $ the # base models . In Theorem 1 , the superscript $ y_i $ indexes the classes other than $ y_0 $ for each base model $ i $ individually . \\ > In the proof for Theorem 2 , it could be helpful to mention each step . Thanks for the comment , and we will make the proof in Theorem 2 clear in the revision . \\ > In theorem 3 , $ N $ is assumed to be 2 . Is it possible to extend this to $ N > 2 $ ? In the smoothed version in Section 3 , we extend the results beyond $ N = 2 $ in Theorems 4 , 5 , and Corollary 2 . In this vanilla version , WE can be extended to $ N = 2 $ , and for MME , the challenge is the \u201c maximum \u201d operator in MME . When $ N = 2 $ , the sign of the sum of confidence score differences can indicate which base model the MME will choose . However , it does not hold for $ N > 2 $ , where the \u201c maximum \u201d operator can not be eliminated easily . We will add this discussion to the revision . \\ > In the first paragraph on page 5 , I do not follow which equation was meant in RHS . The RHS means $ r \\cdot \\frac { 1-\\Delta } { 1+\\Delta } ( 1-C_ { \\text { MME or ME } } ( 1-cos\\theta ) ) ^ { -1/2 } $ ."}, "3": {"review_id": "IUYthV32lbK-3", "review_text": "This paper studies the following problem : How to train a certifiably robust classifier with ensemble methods ? The authors considered two types of ensembles : the weighted-average ensemble and large-margin ensemble . They first derived theoretically sufficient and necessary conditions for robustness under two types of ensembles , with the conclusion that large confidence margin and diversified gradients are two factors which contributes to the robustness of ensemble models . Diversity-regularized training , a method of designing loss functions for training ensemble models , is proposed motivated by their theoretical findings . They applied this methodology to randomized smoothing , performed extensive experiments and showed non-trivial improvement over single model methods . The paper is very well-written and provided extremely detailed discussion about many different aspects of the problem , both theoretically and empirically . From the reviewer 's point of view , this is the strongest part of this work . I am very impressed by the level of detail in the appendix , which covers many interesting questions like under which scenario is WE better than MME , why is ensemble before smoothing better than ensemble after smoothing , to name a few . I wish more papers in the community are written in this way . However , my current evaluation to this paper is a weak accept - it is a bit conservative , but I think it 's based on some valid concerns , detailed below . The experimental results , while showed non-trivial improvements over single model baselines , may not be very strong . In most cases , the improvements are like 3~4 % , sometimes a little over 5 % in smaller radius settings comparing to smoothadv . This improvement is much smaller than some of the earlier works like smoothadv vs gaussian . Also in certain settings , the improvement over other single model baselines becomes very small ( e.g. < 2 % over MACER in Table 12 ) . The performance is also overall very similar to recent ensemble baselines like Liu et al.2020.My understanding is that this shows a limitation of ensemble-based methods in certifiable robustness . To summarize , despite of the limitations mentioned above , I think this work is overall good enough for recommending acceptance and thank the authors for their effort . Minor comments : I do n't quite get the point of appendix D.3 , in particular , the reasoning that trying to justify the uniform distribution assumption of confidence scores . The concentration of measure in high dimensional Gaussian does not imply the uniformity of confidence scores . Although I do understand this assumption as a concrete example trying to get more interpretable results .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your thoughtful suggestions . We checked your questions carefully and answered them as follows . \\ > Limited improvement of the experiment results . First , in this paper we mainly want to emphasize our analysis framework and the methodology , and we use the experimental results as a demonstration without tuning the parameters very carefully . Following your suggestion , we actually tune our parameters a bit and we find that we can indeed get better performance as follows : MNIST : | & emsp ; & emsp ; & emsp ; & nbsp ; & nbsp ; & nbsp ; Radius $ r $ | & emsp ; ` $ 1.25 $ | & emsp ; $ 1.50 $ | & emsp ; $ 1.75 $ | & emsp ; $ 2.00 $ | & emsp ; $ 2.25 $ | & emsp ; $ 2.50 $ | | : :| : :| : :| : :| : :| : :| : :| | Gaussian | & emsp ; ` 83.0 | & emsp ; ` 68.2 | & emsp ; ` 46.6 | & emsp ; ` 33.0 | & emsp ; ` 20.5 | & emsp ; ` 11.5 | | MME ( Gaussian ) | & emsp ; ` 84.3 | & emsp ; ` 69.8 | & emsp ; ` 48.8 | & emsp ; ` 34.7 | & emsp ; ` 23.4 | & emsp ; ` 12.7 | | DRT + MME ( Gaussian ) | & emsp ; ` 86.8 | & emsp ; ` 75.2 | & emsp ; ` 55.8 | & emsp ; ` 44.8 | & emsp ; ` 38.0 | & emsp ; ` 27.0 | | SmoothAdv | & emsp ; ` 87.7 | & emsp ; ` 80.2 | & emsp ; ` 66.3 | & emsp ; ` 43.2 | & emsp ; ` 34.3 | & emsp ; ` 24.0 | | MME ( SmoothAdv ) | & emsp ; ` 88.1 | & emsp ; ` 80.7 | & emsp ; ` 67.9 | & emsp ; ` 44.8 | & emsp ; ` 35.0 | & emsp ; ` 25.2 | | DRT + MME ( SmoothAdv ) | & emsp ; ` * * 88.5 * * | & emsp ; * * ` 83.2 * * | & emsp ; * * ` 68.9 Indeed , for the small radius , the improvement is limited , and we hypothesize that \u2019 s due to the sub-models \u2019 capacity limit within the ensemble , which shows the limitation of ensemble-based methods as you mentioned and we will add related discussion in our revision . However , the significant improvement of the DRT ensemble model compared to the ensemble without DRT indicates that DRT could be a very effective and general way to improve the certified robustness of the ensemble . \\ > Questions about the uniform distribution assumption . Thanks for the question and this assumption is actually for a case study and we will make it clear in our revision . Without the uniform distribution assumption , we can prove a general case that the transferability across base models decides which ensemble is better as shown in Appendix D.2 . Here we just want to give an illustration in a specific regim and therefore we make the uniform assumption which may not hold exactly in practice . We think it would be an interesting future direction to generalize the analysis to other distributions such as the Gaussian distribution which corresponds to locally linear classifiers . We will make this discussion clear in Appendix D.3 \u2019 s remark . Thanks for your thoughtful suggestions and appreciation . We will update these new results in our revision and make the whole paper to be clearer . Hope we have answered all your questions , so that you can increase your score . We are also happy to discuss further for other concerns ."}}