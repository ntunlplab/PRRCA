{"year": "2021", "forum": "dak8uQE6BOG", "title": "MVP: Multivariate polynomials for conditional generation", "decision": "Reject", "meta_review": "This paper proposes a new network architecture that implements higher order multivariate polynomials (MVP). They show that MVP generalizes well to different types of conditional variables, and can be applied to a broad range of tasks.   However, unifying discrete and continuous conditions and network without activation function are both well studied in literatures. The inappropriate discussions on the prior works and the advantages of the proposed method over prior approaches are not clearly justified.  Although outperforming SOTA is not necessary, the compared methods need to be well chosen which can provide convincing evidence on why MVP is needed under common settings.", "reviews": [{"review_id": "dak8uQE6BOG-0", "review_text": "* * Post rebuttal ( round # 3 ) * * Thanks to the authors ' effort on the rebuttal . Despite the extensive efforts , I feel the review/rebuttal iteration is not satisfactory , possibly due to some miscommunication . To be clear , I want to re-emphasize that significant parts of my concerns were about * * misleading claims * * on prior work , and comparison to them was the next step . - For example , I just wanted to clarify that the claim `` type A ignores the noise and can not learn the stochastic mapping '' is wrong . The paper could simply fix the claim instead of including a massive related work section . Maybe my review also has some responsibility : I could simply say * * fix * * the wrong claims , instead of indirectly delivering by pointing them out . Also , as I explicitly mentioned the concerns A , C , D , F , the rebuttal could address them point-by-point . - In particular , I 'm not convinced that cBN/sBN is * * not applicable * * to continuous conditions , as sBN predicts the BN parameters from the continuous latent variable . Despite the remaining concerns , I raised the score ( from 5 ) to 6 as the architectures with multiplicative interactions are an important and timely topic . However , I think the paper is on the borderline , and the rebuttal and revised paper could be much stronger . * * Summary * * This paper extends $ \\Pi $ -Net , deep polynomial neural networks , to a multivariate setting . The proposed network , MVP , is applied to various conditional GANs , including discrete , continuous , and mixed condition scenarios . * * Pros * * - Extending $ \\Pi $ -Net to multivariate setting is a natural and necessary research direction . - Good experimental results on class-conditional , image-conditional , and mixed ( class + image ) -conditional scenarios . * * Concerns/questions * * I . Novelty over $ \\Pi $ -Net is not significant - $ \\Pi $ -Net has shown that deep polynomials can be useful for an unconditional generation . Extending it to conditional generation ( using two variables ) is quite straightforward . While the paper compares with other design choices ( e.g. , SICONC and SPADE ) , it is natural that MVP performs the best since it has the strongest expressive power . - While the paper claims that `` unifying discrete and continuous conditions '' is a key property of MVP , standard conditional GANs can also handle those cases and are already discussed in the literature . For example , [ 1 ] considers image ( face ) + class ( quantized age ) conditions . - While the paper claims that `` network without activation function '' is one of the main contributions , it was originally investigated and heavily discussed in $ \\Pi $ -Net . [ 1 ] Antipov et al.Face Aging With Conditional Generative Adversarial Networks . ICIP 2017 . II.Wrong claims on the ( drawbacks of ) prior work The paper claims that prior conditional GANs : ( Type A ) `` encoder network to obtain representations that are independent of the conditional variable '' and ( Type B ) `` directly concatenate the labels in the latent space '' have several drawbacks . However , the claims are incorrect , as stated below : - Type A does not ignore the noise and can learn the stochastic mapping ( with proper training ) [ 2,3 ] - Type A can be successful for discrete conditions [ 1 ] - Type B can scale beyond 10 class , especially with nonlinear mapping such as conditional BN [ 4,5 ] The paper also claims that `` inter-class interpolations in CIFAR10 have not emerged '' , but there is no backup for the claim . To verify this , the paper should compare the generated samples of standard GANs and MVP in Figure 2 . [ 2 ] Zhu et al.Toward Multimodal Image-to-Image Translation . NeurIPS 2017.\\ [ 3 ] Huang et al.Multimodal Unsupervised Image-to-Image Translation . ECCV 2018.\\ [ 4 ] Miyato & Koyama . cGANs with Projection Discriminator . ICLR 2018.\\ [ 5 ] Brock et al.Large Scale GAN Training for High Fidelity Natural Image Synthesis . ICLR 2019 . III.Why polynomial should work better than standard GANs ? The paper claims that MVP performs better than standard GANs in various conditional generation setups . - Is there any intuition that MVP should work better than standard GANs ? - How about the number of parameters or sampling speed ? Is the comparison ( in Table 2-4 ) fair in terms of complexity ? * * Rating * * Due to the concerns above , I currently recommend a rating of 5 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The paper also claims that `` inter-class interpolations in CIFAR10 have not emerged '' , but there is no backup for the claim . To verify this , the paper should compare the generated samples of standard GANs and MVP in Figure 2 . We have not found any work in the literature on inter-class interpolations in CIFAR10 , which is what we state . The baseline , i.e. , SNGAN , uses conditional batch normalization ( CBN ) , which results in non-trivial inter-class mixing as we explain below : Our ( intuitive ) understanding is that CBN creates a mixture of distributions ( e.g. , mixture of Gaussians ) , where each distribution includes high-dimensional representations . Inter-class interpolations between these different distributions that are ( possibly ) non-overlapping can be tricky . However , we add the visualizations recommended by the reviewer in the compared methods that use one-hot representations . The resulting figure is now Fig.10 of the updated manuscript . The visualization exhibits that methods that do not capture higher-order polynomial expansions result in blurrier intermediate images . ___________________________________________________________________________________________________________________________________________ > How about the number of parameters or sampling speed ? We welcome the suggestion of the reviewer . We measure the sampling speed/parameters for both the SNGAN-based experiment and for the Pi-net-based experiment in class-conditional generation . The parameters ( indicated in millions ) along with the speed of generating images ( indicated in millisecond ) are reported . The speed is computed by running each generator in a single GPU with batch size 32 ; this is repeated 100 times and the mean is reported below . The table below compares the parameters and the speed for the SNGAN-based comparisons : | Method |Parameters ( million ) |Speed ( in ms ) | || -- | -- | |SNGAN |4.3|17.57| |SNGAN-CONC |5.3|21.69| |SNGAN-ADD |4.8|19.52| |SNGAN-SPADE |4.5|16.62| |SNGAN-MVP |4.7|19.70| The number of parameters in the MVP and ADD are very similar ( the Hadamard product is replaced with the addition ) , while the 'SNGAN-CONC ' includes additional parameters to account for the increased channels due to the concatenation . Interestingly , SNGAN-SPADE is faster than SNGAN ; this times improvement stems from conditional batch normalization ( CBN ) . SNGAN samples in every layer from CBN , while SPADE only performs a Hadamard product . The second table below measures the number of parameters and the speed in the class-conditional generation experiment of sec.4.1 on CIFAR10 . | Method |Parameters ( million ) |Speed ( in ms ) | || -- | -- | |GAN-CONC |5.6|18.79| |GAN-ADD |4.8|17.87| |SPADE |3.9|9.96| |\u03a0-net-SINCONC |4.8|12.14| |\u03a0-net |4.8|19.12| |MVP |4.1|14.85| The results are similar to the ones reported for SNGAN . Notice that the Pi-net-SINCONC and Pi-net have similar number of parameters , but substantially different speed ( over 50 % overhead in Pi-net ) . The difference is that \u03a0-net uses CBN , while \u03a0-net-SINCONC concatenates the classes in the input . Overall , the Pi-net with CBN is the most costly in terms of speed . ___________________________________________________________________________________________________________________________________________ # # # * * Summary * * All in all , we have * * updated the related work * * to reflect the recommended changes , and we have answered the concerns of the reviewer on the contributions and the significance of a unified framework . Furthermore , we * * added the requested comparisons with respect to parameters and speed * * , while we have added * * new experiments on image-to-image translation * * ( i.e. , edges-to-handbags and edges-to-shoes ) in sec.H4 ( Appendix ) ; visualizations are provided in Fig.14 . An additional experiment will be included until the weekend . The revised work is stronger , thus we would request the reviewer to adjust their rating or to pose any additional questions they might have ."}, {"review_id": "dak8uQE6BOG-1", "review_text": "Summary : This paper proposes a framework for generating conditional data using multivariate polynomials , which treats both discrete and continuous conditional variables in a unified way . From my perspective , neither the effect nor the actual use is very clear . Below are my major concerns : 1 . The author compares the proposed method with multiple methods on the class-conditional generation and image2image translation tasks , but none of the comparison methods are state-of-the-art . The choice of dataset and task is not very appropriate . Why compare with SPADE on the class conditional generation and super-resolution tasks ? SPADE is the SOTA of * * semantic image synthesis * * in 2019 . It is unfair to compare specific methods for inappropriate tasks . In addition , the selected dataset is also weird . Why not use a standard super-resolution dataset or use downsampled FFHQ and CelebAHQ images for face super-resolution tasks . 2.The author mentioned that the disadvantage of the traditional encoder/decoder architecture ( pix2pix Isola et al. , 2017 ) is that it ignores noise , but in fact , this problem was solved to a certain extent as early as 2017 e.g . ( BicycleGAN zhu al. , 2017 ) . In fact , in Figure 4 , I did not see enough of the variability caused by the noise claimed in this paper , even though the task is so simple . 3.Although the method proposed in this paper is novel , I am not sure it really makes sense in actual complex input scenarios . Besides , the results mentioned in this paper do not show any superiority over traditional architectural design .", "rating": "5: Marginally below acceptance threshold", "reply_text": "> In fact , in Figure 4 , I did not see enough of the variability caused by the noise claimed in this paper , even though the task is so simple . To better demonstrate our point , the revised paper includes a number of experiments on image translation . First of all , we include an experiment of translating MNIST to SVHN digits . This is a significant baseline in domain adaptation , e.g. , in [ 4 ] . The experiment is included in sec.H3 ( Appendix in page 25 ) and we demonstrate that without any additional loss MVP can translate one MNIST digit to an SVHN equivalent . We also conduct an experiment on the more classic edges-to-image case , i.e. , the conditional input includes the edges . The experiments with edges-to-handbags and edges-to-shoes are described in sec.H4 ( Appendix , page 26-27 ) . In contrast to standard practices in the literature that include additional losses or modules , e.g. , in pix2pix , we only include the GAN loss for the aforementioned experiments . Yet , MVP synthesizes images that are diverse ( e.g. , Fig.14 ) . We also share an additional video to demonstrate how we can interpolate z_I ( i.e. , noise ) when a given shape is provided . The video is upload in this [ link ] ( https : //anonymous.4open.science/r/b527f99e-5fe8-4377-8b87-615688fb656f/ ) and in each frame we sample one z_I , while z_II is depicted in the first row . ________________________________________________ > Although the method proposed in this paper is novel , I am not sure it really makes sense in actual complex input scenarios . Besides , the results mentioned in this paper do not show any superiority over traditional architectural design . First of all , we are thankful to the reviewer for acknowledging that our paper is novel . Once again , we repeat that according to the [ reviewer guidelines ] ( https : //iclr.cc/Conferences/2021/ReviewerGuide ) , the proposed method does not need to have sota results to add value to the ICLR community . In addition , we would gladly cite and discuss any model that can synthesize images when trained in these diverse tasks . That is actually our point , i.e. , previously separate models were built for different categories of the tasks . To sum up , we believe the tasks selected are appropriate and have significant applications , while we demonstrate consistently that the proposed model can synthesize images in a broad range of tasks . We would gladly respond to any further questions raised by the reviewer . ________________________________________________ [ 1 ] Nasrollahi , Kamal , and Thomas B. Moeslund . `` Super-resolution : a comprehensive survey . '' Machine vision and applications , 2014 . [ 2 ] Wang , Zhihao , Jian Chen , and Steven CH Hoi . `` Deep learning for image super-resolution : A survey . '' IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020 . [ 3 ] Anwar , Saeed , Salman Khan , and Nick Barnes . `` A Deep Journey into Super-resolution : A Survey . '' ACM Computing Surveys ( CSUR ) , 2020 . [ 4 ] Bousmalis , Konstantinos , et al . `` Domain Separation Networks . '' NeurIPS 2016 ."}, {"review_id": "dak8uQE6BOG-2", "review_text": "This paper proposes a conditional generation framework ( cGAN ) that bridges the gap between discrete and continuous variable used in the generation . They do so by proposing a new network architecture that implements higher order multi variate polynomials ( MVP ) . They show that MVP generalizes well to different types of conditional variables and has good expressivity even in the absence of activation functions . Pros : The figures are succinct and informative , giving a clear picture of the points the authors want to illustrate . I appreciate the multiple evaluations with mean and standard deviation reported . The methods section is well-written and the intuition provided to readers who are not familiar with the concept of polynomial networks is very useful . Cons/Comments : Section 2.1 Discrete conditional variable : the authors claim that conditional normalization might be an obstacle towards generalizing to unseen classes . They do not show their method is able to generalize . Section 2.1 Continuous conditional variable : The authors do not cite or mentioned papers that do multi modal image generation [ 1,2,3 ] that display diversity and do not require pixel-wise loss such as l1 or perceptual . They single out one-to-one translation models that are not designed for such tasks . Table 2 : Why is there a huge disparity between IS and FID scores of SNGAN-CONC compared to SNGAN ? The IS is slightly better but the FID is significantly worse . Furthermore , the FID of the original SNGAN paper [ 4 ] is 21.7 and the FID of BigGAN [ 5 ] is 14.73 . What causes the disparity the results between the papers ? In my experience , BigGAN performs much better than SNGAN yet the FID of SNGAN quoted in this paper is 14.7 , virtually the same as BigGAN . Section 4.1 Resnet-based generator : The authors claim that inter-class interpolations have been done for other datasets but not CIFAR . Is BigGAN capable of doing inter-class interpolations for CIFAR since it is able to do for ImageNet ? The experiments include conditional generation based on discrete variables and continuous variables . One of the main comparisons is with SPADE . However , SPADE was designed for image generation from semantic maps , a task not tested in this paper . Super resolution was chosen as the one of the two tasks for continuous variable . This is a task not traditionally done with multi-modal output in mind . In my opinion , this is not the most useful task to base the experiments on . A more interesting task could be image-to-image translation or semantic image synthesis . The experiments are all done at 64x64 resolution and the datasets are relatively simple compared to those used in SOTA models ( 512x512 ImageNet on BigGAN and 1024x1024 FFHQ on StyleGAN [ 6 ] ) . The quantitative improvements on the most commonly used ResNet architecture is minimal . While MVP remains expressive without activations and improves significantly using a \u03a0 net architecture , those to my knowledge are not settings widely used now in literature . In general , this paper provides a clear unified framework for conditional image generation . The method is well explained and illustrated . However , their improvements on a commonly used architecture ( ResNet ) is minimal . It is my opinion that more justification and evidence is needed on why MVP is needed under common settings . [ 1 ] Choi , Yunjey , et al . `` Stargan v2 : Diverse image synthesis for multiple domains . '' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2020 . [ 2 ] Huang , Xun , et al . `` Multimodal unsupervised image-to-image translation . '' Proceedings of the European Conference on Computer Vision ( ECCV ) . 2018 . [ 3 ] Lee , Hsin-Ying , et al . `` Drit++ : Diverse image-to-image translation via disentangled representations . '' International Journal of Computer Vision ( 2020 ) : 1-16 . [ 4 ] Miyato , Takeru , et al . `` Spectral normalization for generative adversarial networks . '' arXiv preprint arXiv:1802.05957 ( 2018 ) . [ 5 ] Brock , Andrew , Jeff Donahue , and Karen Simonyan . `` Large scale gan training for high fidelity natural image synthesis . '' arXiv preprint arXiv:1809.11096 ( 2018 ) . [ 6 ] Karras , Tero , Samuli Laine , and Timo Aila . `` A style-based generator architecture for generative adversarial networks . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "> While MVP remains expressive without activations and improves significantly using a \u03a0-net architecture , those to my knowledge are not settings widely used now in literature . This is the setting used by \u03a0-nets , hence we utilize that to provide a fair comparison . In addition , conducting theoretical work in the presence of activation functions is challenging [ 8 ] . Hence , we argue that the experiments demonstrate consistently the benefits of using MVP . ___________________________________________________________________________________________________________________________________________ > More evidence is needed to justify the importance of MVP . We appreciate the comments of the reviewer ; we have addressed all their comments . In short , we summarize that our ( stated ) goal is not to provide state-of-the-art results , which is consistent with the policy of ICLR ( please see above ) . Instead , we provide consistent results in a broad range of tasks . To our knowledge , the aforementioned tasks have not been considered before for generation from a single model . In addition , we augment the results based on the recommendation of the reviewer , i.e.we conduct experiments on image-to-image translation ( please see the updated image-to-image results above ) . Our work has become significantly stronger with the revised experimental results ( and related work ) , thus we request the reviewers to reconsider their rating . [ 1 ] Choi , Yunjey , et al . `` Stargan v2 : Diverse image synthesis for multiple domains . '' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2020 . [ 2 ] Huang , Xun , et al . `` Multimodal unsupervised image-to-image translation . '' Proceedings of the European Conference on Computer Vision ( ECCV ) . 2018 . [ 3 ] Lee , Hsin-Ying , et al . `` Drit++ : Diverse image-to-image translation via disentangled representations . '' International Journal of Computer Vision ( 2020 ) : 1-16 . [ 4 ] Miyato , Takeru , et al . `` Spectral normalization for generative adversarial networks . '' arXiv preprint arXiv:1802.05957 ( 2018 ) . [ 5 ] Brock , Andrew , Jeff Donahue , and Karen Simonyan . `` Large scale gan training for high fidelity natural image synthesis . '' arXiv preprint arXiv:1809.11096 ( 2018 ) . [ 6 ] Karras , Tero , Samuli Laine , and Timo Aila . `` A style-based generator architecture for generative adversarial networks . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2019 . [ 7 ] Bousmalis , Konstantinos , et al . `` Domain Separation Networks . '' NeurIPS 2016 . [ 8 ] Arora , Sanjeev , et al . `` A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks . '' ICLR 2019 ."}], "0": {"review_id": "dak8uQE6BOG-0", "review_text": "* * Post rebuttal ( round # 3 ) * * Thanks to the authors ' effort on the rebuttal . Despite the extensive efforts , I feel the review/rebuttal iteration is not satisfactory , possibly due to some miscommunication . To be clear , I want to re-emphasize that significant parts of my concerns were about * * misleading claims * * on prior work , and comparison to them was the next step . - For example , I just wanted to clarify that the claim `` type A ignores the noise and can not learn the stochastic mapping '' is wrong . The paper could simply fix the claim instead of including a massive related work section . Maybe my review also has some responsibility : I could simply say * * fix * * the wrong claims , instead of indirectly delivering by pointing them out . Also , as I explicitly mentioned the concerns A , C , D , F , the rebuttal could address them point-by-point . - In particular , I 'm not convinced that cBN/sBN is * * not applicable * * to continuous conditions , as sBN predicts the BN parameters from the continuous latent variable . Despite the remaining concerns , I raised the score ( from 5 ) to 6 as the architectures with multiplicative interactions are an important and timely topic . However , I think the paper is on the borderline , and the rebuttal and revised paper could be much stronger . * * Summary * * This paper extends $ \\Pi $ -Net , deep polynomial neural networks , to a multivariate setting . The proposed network , MVP , is applied to various conditional GANs , including discrete , continuous , and mixed condition scenarios . * * Pros * * - Extending $ \\Pi $ -Net to multivariate setting is a natural and necessary research direction . - Good experimental results on class-conditional , image-conditional , and mixed ( class + image ) -conditional scenarios . * * Concerns/questions * * I . Novelty over $ \\Pi $ -Net is not significant - $ \\Pi $ -Net has shown that deep polynomials can be useful for an unconditional generation . Extending it to conditional generation ( using two variables ) is quite straightforward . While the paper compares with other design choices ( e.g. , SICONC and SPADE ) , it is natural that MVP performs the best since it has the strongest expressive power . - While the paper claims that `` unifying discrete and continuous conditions '' is a key property of MVP , standard conditional GANs can also handle those cases and are already discussed in the literature . For example , [ 1 ] considers image ( face ) + class ( quantized age ) conditions . - While the paper claims that `` network without activation function '' is one of the main contributions , it was originally investigated and heavily discussed in $ \\Pi $ -Net . [ 1 ] Antipov et al.Face Aging With Conditional Generative Adversarial Networks . ICIP 2017 . II.Wrong claims on the ( drawbacks of ) prior work The paper claims that prior conditional GANs : ( Type A ) `` encoder network to obtain representations that are independent of the conditional variable '' and ( Type B ) `` directly concatenate the labels in the latent space '' have several drawbacks . However , the claims are incorrect , as stated below : - Type A does not ignore the noise and can learn the stochastic mapping ( with proper training ) [ 2,3 ] - Type A can be successful for discrete conditions [ 1 ] - Type B can scale beyond 10 class , especially with nonlinear mapping such as conditional BN [ 4,5 ] The paper also claims that `` inter-class interpolations in CIFAR10 have not emerged '' , but there is no backup for the claim . To verify this , the paper should compare the generated samples of standard GANs and MVP in Figure 2 . [ 2 ] Zhu et al.Toward Multimodal Image-to-Image Translation . NeurIPS 2017.\\ [ 3 ] Huang et al.Multimodal Unsupervised Image-to-Image Translation . ECCV 2018.\\ [ 4 ] Miyato & Koyama . cGANs with Projection Discriminator . ICLR 2018.\\ [ 5 ] Brock et al.Large Scale GAN Training for High Fidelity Natural Image Synthesis . ICLR 2019 . III.Why polynomial should work better than standard GANs ? The paper claims that MVP performs better than standard GANs in various conditional generation setups . - Is there any intuition that MVP should work better than standard GANs ? - How about the number of parameters or sampling speed ? Is the comparison ( in Table 2-4 ) fair in terms of complexity ? * * Rating * * Due to the concerns above , I currently recommend a rating of 5 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The paper also claims that `` inter-class interpolations in CIFAR10 have not emerged '' , but there is no backup for the claim . To verify this , the paper should compare the generated samples of standard GANs and MVP in Figure 2 . We have not found any work in the literature on inter-class interpolations in CIFAR10 , which is what we state . The baseline , i.e. , SNGAN , uses conditional batch normalization ( CBN ) , which results in non-trivial inter-class mixing as we explain below : Our ( intuitive ) understanding is that CBN creates a mixture of distributions ( e.g. , mixture of Gaussians ) , where each distribution includes high-dimensional representations . Inter-class interpolations between these different distributions that are ( possibly ) non-overlapping can be tricky . However , we add the visualizations recommended by the reviewer in the compared methods that use one-hot representations . The resulting figure is now Fig.10 of the updated manuscript . The visualization exhibits that methods that do not capture higher-order polynomial expansions result in blurrier intermediate images . ___________________________________________________________________________________________________________________________________________ > How about the number of parameters or sampling speed ? We welcome the suggestion of the reviewer . We measure the sampling speed/parameters for both the SNGAN-based experiment and for the Pi-net-based experiment in class-conditional generation . The parameters ( indicated in millions ) along with the speed of generating images ( indicated in millisecond ) are reported . The speed is computed by running each generator in a single GPU with batch size 32 ; this is repeated 100 times and the mean is reported below . The table below compares the parameters and the speed for the SNGAN-based comparisons : | Method |Parameters ( million ) |Speed ( in ms ) | || -- | -- | |SNGAN |4.3|17.57| |SNGAN-CONC |5.3|21.69| |SNGAN-ADD |4.8|19.52| |SNGAN-SPADE |4.5|16.62| |SNGAN-MVP |4.7|19.70| The number of parameters in the MVP and ADD are very similar ( the Hadamard product is replaced with the addition ) , while the 'SNGAN-CONC ' includes additional parameters to account for the increased channels due to the concatenation . Interestingly , SNGAN-SPADE is faster than SNGAN ; this times improvement stems from conditional batch normalization ( CBN ) . SNGAN samples in every layer from CBN , while SPADE only performs a Hadamard product . The second table below measures the number of parameters and the speed in the class-conditional generation experiment of sec.4.1 on CIFAR10 . | Method |Parameters ( million ) |Speed ( in ms ) | || -- | -- | |GAN-CONC |5.6|18.79| |GAN-ADD |4.8|17.87| |SPADE |3.9|9.96| |\u03a0-net-SINCONC |4.8|12.14| |\u03a0-net |4.8|19.12| |MVP |4.1|14.85| The results are similar to the ones reported for SNGAN . Notice that the Pi-net-SINCONC and Pi-net have similar number of parameters , but substantially different speed ( over 50 % overhead in Pi-net ) . The difference is that \u03a0-net uses CBN , while \u03a0-net-SINCONC concatenates the classes in the input . Overall , the Pi-net with CBN is the most costly in terms of speed . ___________________________________________________________________________________________________________________________________________ # # # * * Summary * * All in all , we have * * updated the related work * * to reflect the recommended changes , and we have answered the concerns of the reviewer on the contributions and the significance of a unified framework . Furthermore , we * * added the requested comparisons with respect to parameters and speed * * , while we have added * * new experiments on image-to-image translation * * ( i.e. , edges-to-handbags and edges-to-shoes ) in sec.H4 ( Appendix ) ; visualizations are provided in Fig.14 . An additional experiment will be included until the weekend . The revised work is stronger , thus we would request the reviewer to adjust their rating or to pose any additional questions they might have ."}, "1": {"review_id": "dak8uQE6BOG-1", "review_text": "Summary : This paper proposes a framework for generating conditional data using multivariate polynomials , which treats both discrete and continuous conditional variables in a unified way . From my perspective , neither the effect nor the actual use is very clear . Below are my major concerns : 1 . The author compares the proposed method with multiple methods on the class-conditional generation and image2image translation tasks , but none of the comparison methods are state-of-the-art . The choice of dataset and task is not very appropriate . Why compare with SPADE on the class conditional generation and super-resolution tasks ? SPADE is the SOTA of * * semantic image synthesis * * in 2019 . It is unfair to compare specific methods for inappropriate tasks . In addition , the selected dataset is also weird . Why not use a standard super-resolution dataset or use downsampled FFHQ and CelebAHQ images for face super-resolution tasks . 2.The author mentioned that the disadvantage of the traditional encoder/decoder architecture ( pix2pix Isola et al. , 2017 ) is that it ignores noise , but in fact , this problem was solved to a certain extent as early as 2017 e.g . ( BicycleGAN zhu al. , 2017 ) . In fact , in Figure 4 , I did not see enough of the variability caused by the noise claimed in this paper , even though the task is so simple . 3.Although the method proposed in this paper is novel , I am not sure it really makes sense in actual complex input scenarios . Besides , the results mentioned in this paper do not show any superiority over traditional architectural design .", "rating": "5: Marginally below acceptance threshold", "reply_text": "> In fact , in Figure 4 , I did not see enough of the variability caused by the noise claimed in this paper , even though the task is so simple . To better demonstrate our point , the revised paper includes a number of experiments on image translation . First of all , we include an experiment of translating MNIST to SVHN digits . This is a significant baseline in domain adaptation , e.g. , in [ 4 ] . The experiment is included in sec.H3 ( Appendix in page 25 ) and we demonstrate that without any additional loss MVP can translate one MNIST digit to an SVHN equivalent . We also conduct an experiment on the more classic edges-to-image case , i.e. , the conditional input includes the edges . The experiments with edges-to-handbags and edges-to-shoes are described in sec.H4 ( Appendix , page 26-27 ) . In contrast to standard practices in the literature that include additional losses or modules , e.g. , in pix2pix , we only include the GAN loss for the aforementioned experiments . Yet , MVP synthesizes images that are diverse ( e.g. , Fig.14 ) . We also share an additional video to demonstrate how we can interpolate z_I ( i.e. , noise ) when a given shape is provided . The video is upload in this [ link ] ( https : //anonymous.4open.science/r/b527f99e-5fe8-4377-8b87-615688fb656f/ ) and in each frame we sample one z_I , while z_II is depicted in the first row . ________________________________________________ > Although the method proposed in this paper is novel , I am not sure it really makes sense in actual complex input scenarios . Besides , the results mentioned in this paper do not show any superiority over traditional architectural design . First of all , we are thankful to the reviewer for acknowledging that our paper is novel . Once again , we repeat that according to the [ reviewer guidelines ] ( https : //iclr.cc/Conferences/2021/ReviewerGuide ) , the proposed method does not need to have sota results to add value to the ICLR community . In addition , we would gladly cite and discuss any model that can synthesize images when trained in these diverse tasks . That is actually our point , i.e. , previously separate models were built for different categories of the tasks . To sum up , we believe the tasks selected are appropriate and have significant applications , while we demonstrate consistently that the proposed model can synthesize images in a broad range of tasks . We would gladly respond to any further questions raised by the reviewer . ________________________________________________ [ 1 ] Nasrollahi , Kamal , and Thomas B. Moeslund . `` Super-resolution : a comprehensive survey . '' Machine vision and applications , 2014 . [ 2 ] Wang , Zhihao , Jian Chen , and Steven CH Hoi . `` Deep learning for image super-resolution : A survey . '' IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020 . [ 3 ] Anwar , Saeed , Salman Khan , and Nick Barnes . `` A Deep Journey into Super-resolution : A Survey . '' ACM Computing Surveys ( CSUR ) , 2020 . [ 4 ] Bousmalis , Konstantinos , et al . `` Domain Separation Networks . '' NeurIPS 2016 ."}, "2": {"review_id": "dak8uQE6BOG-2", "review_text": "This paper proposes a conditional generation framework ( cGAN ) that bridges the gap between discrete and continuous variable used in the generation . They do so by proposing a new network architecture that implements higher order multi variate polynomials ( MVP ) . They show that MVP generalizes well to different types of conditional variables and has good expressivity even in the absence of activation functions . Pros : The figures are succinct and informative , giving a clear picture of the points the authors want to illustrate . I appreciate the multiple evaluations with mean and standard deviation reported . The methods section is well-written and the intuition provided to readers who are not familiar with the concept of polynomial networks is very useful . Cons/Comments : Section 2.1 Discrete conditional variable : the authors claim that conditional normalization might be an obstacle towards generalizing to unseen classes . They do not show their method is able to generalize . Section 2.1 Continuous conditional variable : The authors do not cite or mentioned papers that do multi modal image generation [ 1,2,3 ] that display diversity and do not require pixel-wise loss such as l1 or perceptual . They single out one-to-one translation models that are not designed for such tasks . Table 2 : Why is there a huge disparity between IS and FID scores of SNGAN-CONC compared to SNGAN ? The IS is slightly better but the FID is significantly worse . Furthermore , the FID of the original SNGAN paper [ 4 ] is 21.7 and the FID of BigGAN [ 5 ] is 14.73 . What causes the disparity the results between the papers ? In my experience , BigGAN performs much better than SNGAN yet the FID of SNGAN quoted in this paper is 14.7 , virtually the same as BigGAN . Section 4.1 Resnet-based generator : The authors claim that inter-class interpolations have been done for other datasets but not CIFAR . Is BigGAN capable of doing inter-class interpolations for CIFAR since it is able to do for ImageNet ? The experiments include conditional generation based on discrete variables and continuous variables . One of the main comparisons is with SPADE . However , SPADE was designed for image generation from semantic maps , a task not tested in this paper . Super resolution was chosen as the one of the two tasks for continuous variable . This is a task not traditionally done with multi-modal output in mind . In my opinion , this is not the most useful task to base the experiments on . A more interesting task could be image-to-image translation or semantic image synthesis . The experiments are all done at 64x64 resolution and the datasets are relatively simple compared to those used in SOTA models ( 512x512 ImageNet on BigGAN and 1024x1024 FFHQ on StyleGAN [ 6 ] ) . The quantitative improvements on the most commonly used ResNet architecture is minimal . While MVP remains expressive without activations and improves significantly using a \u03a0 net architecture , those to my knowledge are not settings widely used now in literature . In general , this paper provides a clear unified framework for conditional image generation . The method is well explained and illustrated . However , their improvements on a commonly used architecture ( ResNet ) is minimal . It is my opinion that more justification and evidence is needed on why MVP is needed under common settings . [ 1 ] Choi , Yunjey , et al . `` Stargan v2 : Diverse image synthesis for multiple domains . '' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2020 . [ 2 ] Huang , Xun , et al . `` Multimodal unsupervised image-to-image translation . '' Proceedings of the European Conference on Computer Vision ( ECCV ) . 2018 . [ 3 ] Lee , Hsin-Ying , et al . `` Drit++ : Diverse image-to-image translation via disentangled representations . '' International Journal of Computer Vision ( 2020 ) : 1-16 . [ 4 ] Miyato , Takeru , et al . `` Spectral normalization for generative adversarial networks . '' arXiv preprint arXiv:1802.05957 ( 2018 ) . [ 5 ] Brock , Andrew , Jeff Donahue , and Karen Simonyan . `` Large scale gan training for high fidelity natural image synthesis . '' arXiv preprint arXiv:1809.11096 ( 2018 ) . [ 6 ] Karras , Tero , Samuli Laine , and Timo Aila . `` A style-based generator architecture for generative adversarial networks . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "> While MVP remains expressive without activations and improves significantly using a \u03a0-net architecture , those to my knowledge are not settings widely used now in literature . This is the setting used by \u03a0-nets , hence we utilize that to provide a fair comparison . In addition , conducting theoretical work in the presence of activation functions is challenging [ 8 ] . Hence , we argue that the experiments demonstrate consistently the benefits of using MVP . ___________________________________________________________________________________________________________________________________________ > More evidence is needed to justify the importance of MVP . We appreciate the comments of the reviewer ; we have addressed all their comments . In short , we summarize that our ( stated ) goal is not to provide state-of-the-art results , which is consistent with the policy of ICLR ( please see above ) . Instead , we provide consistent results in a broad range of tasks . To our knowledge , the aforementioned tasks have not been considered before for generation from a single model . In addition , we augment the results based on the recommendation of the reviewer , i.e.we conduct experiments on image-to-image translation ( please see the updated image-to-image results above ) . Our work has become significantly stronger with the revised experimental results ( and related work ) , thus we request the reviewers to reconsider their rating . [ 1 ] Choi , Yunjey , et al . `` Stargan v2 : Diverse image synthesis for multiple domains . '' Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2020 . [ 2 ] Huang , Xun , et al . `` Multimodal unsupervised image-to-image translation . '' Proceedings of the European Conference on Computer Vision ( ECCV ) . 2018 . [ 3 ] Lee , Hsin-Ying , et al . `` Drit++ : Diverse image-to-image translation via disentangled representations . '' International Journal of Computer Vision ( 2020 ) : 1-16 . [ 4 ] Miyato , Takeru , et al . `` Spectral normalization for generative adversarial networks . '' arXiv preprint arXiv:1802.05957 ( 2018 ) . [ 5 ] Brock , Andrew , Jeff Donahue , and Karen Simonyan . `` Large scale gan training for high fidelity natural image synthesis . '' arXiv preprint arXiv:1809.11096 ( 2018 ) . [ 6 ] Karras , Tero , Samuli Laine , and Timo Aila . `` A style-based generator architecture for generative adversarial networks . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2019 . [ 7 ] Bousmalis , Konstantinos , et al . `` Domain Separation Networks . '' NeurIPS 2016 . [ 8 ] Arora , Sanjeev , et al . `` A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks . '' ICLR 2019 ."}}