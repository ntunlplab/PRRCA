{"year": "2020", "forum": "H1e31AEYwB", "title": "Stiffness: A New Perspective on Generalization in Neural Networks", "decision": "Reject", "meta_review": "While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.\n\nConcerns raised include lack of sufficient motivation for the approach, and problems with clarity of the exposition.", "reviews": [{"review_id": "H1e31AEYwB-0", "review_text": "This paper introduces \u201cstiffness\u201d, a new metric to characterize generalization in neural networks. Stiffness is a pretty simple concept and is relatively straightforward to compute. The authors evaluate this metric on standard datasets using two relatively small neural networks. On the whole, the paper is written clearly and explains its methodology in simple language. I have a few observations: 1. The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided. Th equivalence is not clear so I would encourage the authors to provide a short proof. 2. Since stiffness depends on the gradients obtained on points in the input space, which in turn depends on the loss, why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses? Indeed, the authors themselves say that a network has overfitted when training and validation losses diverge. The paper fails to motivate why stiffness is better than just looking at losses during training. 3. The authors mention \u201cThe train-val stiffness is directly related to generalization, as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set. \u201d. Typically, generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement. We would expect validation error to underestimate test error so while they are related, train-val stiffness would not necessarily characterize generalization. It would be interesting to see a train-test stiffness graph to test the authors claim. 4. The paper fails to motivate the the utility of the concept of \u201cDynamical Critical distance\u201d. Since the primary goal of paper is to understand generalization, I would like the authors to clarify the motivation to study this quantity. What additional insight does this provide with respect to generalization? 5. The term \u201cdynamical critical distance\u201d is not used uniformly. For example, it is mentioned as \u201cdynamical critical scale\u201d in section 3.3 and \u201cdynamical critical length\u201d in section 4.2. 6. While the paper on the whole is written in a clear fashion, I found section 4.4 to be particularly confusing. The authors should consider rewriting that section to make it clearer. In summary, the concept of stiffness seems to closely follow training and validation losses and any problem diagnosed using stiffness would therefore be also diagnosed via examining the loss values. This along with other concerns mentioned above mean that I cannot recommend this paper for publication.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review and comments . Your review was released to us only in the middle of the rebuttal period and we therefore did n't have the expected time to prepare a reaction . We would like to address several of the points your brought up . -- -- -- -- -- -- -- -- -- -- -- -- - \u201c 1 . The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided. \u201d The connection between Equation 2 and Equation 3 is very simple and we therefore believed it did not require a detailed proof , however , we \u2019 re happy to explain it in multiple steps . It is a Taylor expansion to the first order . The steps are as follows : The derivative of the loss L at image X1 with respect to the weight vector W is the gradient vector g1 . If we look at the Taylor expansion of the change of loss due to a vector change of weights w , we obtain the dot product delta L = g1 dot w to the first order in the Taylor series . In particular , for a weight change induced by a small gradient step -epsilon * g1 , we get delta L = - epsilon g1 dot g1 . This is true as long as epsilon - > 0 , which we take in the paper . -- -- -- -- -- -- -- -- -- -- -- -- - \u201c 2 . Since stiffness depends on the gradients obtained on points in the input space , which in turn depends on the loss , why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses ? \u201c Stiffness as we defined it is related to the transfer of gains in performance from one input datapoint to another . It is defined by looking at the correlation between loss changes on different inputs , rather than the total loss at once . As such , it is a much finer metric than the total loss . However , you could look at loss changes on individual images and it would be equivalent -- that is exactly how we defined the concept of stiffness in the first place in Equations 1 - 5 . Its connection to the gradient alignment is a mathematical consequence and it is a useful way to look at it , as it makes a direct connection to other works on gradient and Hessians in neural networks . However , you can definitely think about the correlation between changes in loss alone -- we call the particular product stiffness , since it geometrically relates to how easily \u201c bendable \u201d the learned NN function is . -- -- -- -- -- -- -- -- \u201c 3 . The authors mention \u201c The train-val stiffness is directly related to generalization , as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set . \u201d .Typically , generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement. \u201d The misunderstanding here is between the validation set and the test set nomenclature . We used the term validation set to mean the held-out , never-trained-on dataset , and therefore used it to characterize generalization . You could easily call this the test set , since the important distinction wrt the train set is that not a single gradient step was ever taken wrt to a single image in it . We feel this point is only a matter of wording and we \u2019 ll try to make it clearer in the paper . -- -- -- -- -- -- -- -- \u201c 4 . The paper fails to motivate the the utility of the concept of \u201c Dynamical Critical distance \u201d . Since the primary goal of paper is to understand generalization , I would like the authors to clarify the motivation to study this quantity . What additional insight does this provide with respect to generalization ? \u201c We firmly believe that our concept of dynamical critical length is of interest and well motivated , as it captures how localized changes to the learned NN function are when a gradient step with respect to a particular example is applied . It is very related to the rich literature on the spectral bias of neural networks . In our case , we study what could be seen as the dynamical equivalent of the spectral bias -- i.e.on which length scales in the input space do changes to the learned function stop affecting the rest of the function . The additional insight you are asking for is that this allows us to measure how localized changes to the learned function are . If you apply the gradient on input X1 , the inputs X whose loss will change in a correlated manner are at a typical distance \u03be or closer . We measure how this distance changes both with training time and the learning rate used . This gives us an additional diagnostic tool which is in turn directly useful in studying how independent the effects of gradient updates are between different images . -- -- -- -- -- -- -- -- -- -- -- -- \u201c 5 . The term \u201c dynamical critical distance \u201d is not used uniformly . \u2026.The authors should consider rewriting that section to make it clearer . \u201c Thank you for having a closer look . We will correct the typos and make sure to provide greater uniformity in the relevant subsection ."}, {"review_id": "H1e31AEYwB-1", "review_text": "This paper introduces the concept of stiffness: a measure of the change in the loss of sample A due to a gradient step based on sample B. It analyses the expected dynamic for A, B samples from the same and different classes, as well as, samples from the train and test sets. To better understand the dynamics of optimization in neural networks is an open and important problem and the paper is clearly motivated in this regard. The proposed method is straight forward and I am not aware of a similar method. In addition to that, the paper also introduces \"dynamical critical length \u03be\" which is the stiffness of A, B samples based on the cosine similarity of the respective inputs (section 2.4). A linear estimator of when this length becomes 0 is also introduced. Confusingly this is also called the \"dynamical critical length \u03be\" in section 4.2. Later on the term \"dynamical scale \u03be\" and \"dynamical critical scale \u03be\" seem to be used interchangeably. Figure 6 mentions the \"critical length \u03c7\" on the y-axis which seems to be a typo as no such measure was introduced. The equivalence between eq. 2 and the two parts of eq. 3 is not obvious. We'd appreciate if the authors would provide a proof of such. Overall, the paper is written in a simple language but paragraphs remain surprisingly hard to understand. An example of such is e.g. section 4.4: What do the authors mean by \"characteristic distance\" between two input points? What is \"the typical scale of spatial variation\" of a function? etc. The paper concludes that: 1.) there is a link between generalization and stiffness 2.) stiffness decreases with the onset of overfitting 3.) \"general gradient updates with respect to a member of a class help to improve loss on data points in the same class\" 4.) \"The pattern breaks when the model starts overfitting to the training set, after which within-class stiffness eventually reaches 0\" 5.) This is observed for different models on different datasets 6.) \"we observed that the farther the datapoints and the higher the epoch of training, the less stiffness exists between them on average\" 7.) \"the higher the learning rate, the smaller the \u03be\" Verdict: Reject The conclusions are self-evident. The paper fails to demonstrate the usefulness of stiffness and most results are expected and provide little to no insights into the optimization dynamics of deep neural networks. In fact, the reasoning in this paper is almost tautological (conclusions 1-6). E.g. if the A, B samples used to compute stiffness are separately drawn from the train and test set then stiffness is a proxy for the difference between the train error and the test error after another gradient step. The authors then compute stiffness at different points of the optimization procedure and conclude that stiffness decreases when the network starts to overfit. Since overfitting is the point in training where train error and test error diverge it is obvious that this can also be observed with regards to \"stiffness\". Hence, the reasoning is circular. Conclusion 7 is slightly different in that it observes that larger learning rates result in smaller \u03be which, given the previous paragraph, we can rewrite into the statement \"larger learning rates generalise better\". This is a well known empirical observation and has been discussed thoroughly (e.g. on connection with flat and sharp minima or learning rate decay schedules). Disclaimer: This review was done on short notice. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review and comments . We appreciate that your review was done on a very short notice and thank you for it . We would like to address and dispute several of the points your brought up . -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c Overall , the paper is written in a simple language but paragraphs remain surprisingly hard to understand . An example of such is e.g.section 4.4 : What do the authors mean by `` characteristic distance '' between two input points ? \u201d \u2026 . \u201c What is `` the typical scale of spatial variation '' of a function ? \u201d We would like to clarify the points of confusion in Subsection 4.4 . The \u201c characteristic distance \u201d point you bring up is a part of a longer statement : \u201c A natural question arises as to whether the characteristic distance between two input points at which stiffness reaches zero defines the typical scale of spatial variation of the learned function. \u201d So the first \u201c characteristic distance \u201d refers to the distance between two points in the input space where the gradient step with respect to one of them will not influence the other one . This is the quantity we call `` dynamical critical length/scale \u03be '' and which we empirically measure in real networks trained on real data in Figures 5 , 6 , 9 , 10 and 11 . The `` the typical scale of spatial variation '' of a function is its dominant Fourier mode in the input space -- i.e.the length scale in the input space over which the predictions change significantly . This scale is related to the concept often called the spectral bias of neural networks and there is a large literature on the topic . Our point in Subsection 4.4 was to make clear that the scale on which input points do not influence each other under gradient updates = `` dynamical critical length/scale \u03be '' , and the scale over which the predictions change significantly in the input space = `` the typical scale of spatial variation '' , are not necessarily the same . We appreciate that this might have been harder to understand based on our description and will try to rephrase the paragraph for clarity . -- -- -- -- -- -- -- -- -- -- -- -- -- - Different terms used for \u03be and typos `` dynamical critical length \u03be '' in section 4.2 . Later on the term `` dynamical scale \u03be '' and `` dynamical critical scale \u03be '' \u2026 .. `` critical length \u03c7 '' Thank you for spotting that we use the words \u201c scale \u201d and \u201c length \u201d interchangeably . We will adopt a single one to ensure clarity . You are right that in Figure 6 \u03c7 should have been \u03be . This was a typo on our part and we will change it to \u03be . -- -- -- -- -- -- -- -- -- -- -- -- -- \u201c The equivalence between eq.2 and the two parts of eq.3 is not obvious . We 'd appreciate if the authors would provide a proof of such . \u201c The connection between Equation 2 and Equation 3 is very simple and we therefore believed it did not require a detailed proof , however , we \u2019 re happy to explain it in multiple steps . It is a Taylor expansion to the first order . The steps are as follows : The derivative of the loss L at image X1 with respect to the weight vector W is the gradient vector g1 . If we look at the Taylor expansion of the change of loss due to a vector change of weights w , we obtain the dot product delta L = g1 dot w to the first order in the Taylor series . In particular , for a weight change induced by a small gradient step -epsilon * g1 , we get delta L = - epsilon g1 dot g1 . This is true as long as epsilon - > 0 , which we take in the paper . -- -- -- -- -- -- -- -- -- -- -- -- -- \u201c The conclusions are self-evident. \u201d While the self-evidence of our conclusions is a matter of subjective judgement , we do not believe that our results are in fact self-evident and our discussions with fellow researchers support this . It is hard to rebut this point , however , if you do not provide links to specific sources in literature ."}, {"review_id": "H1e31AEYwB-2", "review_text": "This submission introduces a metric, termed stiffness, to evaluate the generalization capability of neural networks. The metric is novel and straightforward, it measures how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. The authors study several configurations on three small datasets. They demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. I give an initial rating of weak accept because (1) The paper is well motivated and well written. Studying generalization is important for neural networks. (2) It seems from experiments that stiffness is a useful metric to indicate models' generalization capability. However, I have a few concerns. First, the authors study several configurations like train-train, train-val and val-val. However, these configurations are still in-domain analysis, the data distribution is quite similar. It can not support author's claims well. Adding an experiment where domain gap is large will make the submission stronger, such as train-test, cross-dataset or challenging tasks like semantic segmentation. Second, the datasets being used are very small. I understand that for theoretically analysis, small datasets are quick to converge and easy to demonstrate. However, this submission focuses on generalization problem during transfer learning. Hence, it needs at least a bigger dataset, like ImageNet, to show it really works. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and comments . We hope that you will champion our paper . We provide a detailed response to the points you brought up below . -- -- -- -- -- -- \u201c First , the authors study several configurations like train-train , train-val and val-val . However , these configurations are still in-domain analysis , the data distribution is quite similar. \u201d We studied the stiffness between input images from the training set and the validation set precisely in order to directly quantify the transfer of performance improvement gained on the training set to the unseen validation set . We agree that those distributions are hopefully very similar , however , they are not identical . In that sense , this constitutes a weak version of the out-of-distribution performance experiment you were suggesting . We agree that adding an experiment where the domain gap is large might be interesting , however , the focus of our paper was not on transfer learning ( where this is a common regime ) , but rather on learning on a specific dataset itself . The transfer we were concerned with was from one example to another , i.e.within dataset generalization . If we , for example , looked at the stiffness between train images and random noise images , the interpretation of that metric would be very difficult , as it is not a priori known what kind of behavior would even be desirable there . It could even be the case that you do not want to transfer any performance to random out-of-distribution images , as this could limit your performance on the actual distribution . -- -- -- -- -- -- -- -- -- -- - \u201c Second , the datasets . I understand that for theoretically analysis , small datasets are quick to converge and easy to demonstrate . However , this submission focuses on generalization problem during transfer learning . Hence , it needs at least a bigger dataset , like ImageNet , to show it really works. \u201d We understand that looking at ImageNet would strengthen our case , however , the stiffness calculations are very computationally demanding and the consistent appearance of the effects on MNIST , Fashion MNIST and CIFAR-10 is , according to us , a good indication of their generality . In addition , the goal of our paper is to introduce a metric and show its usefulness , for which we believe CIFAR-10 can be sufficient . Nonetheless , we will try to show our results on larger datasets ."}], "0": {"review_id": "H1e31AEYwB-0", "review_text": "This paper introduces \u201cstiffness\u201d, a new metric to characterize generalization in neural networks. Stiffness is a pretty simple concept and is relatively straightforward to compute. The authors evaluate this metric on standard datasets using two relatively small neural networks. On the whole, the paper is written clearly and explains its methodology in simple language. I have a few observations: 1. The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided. Th equivalence is not clear so I would encourage the authors to provide a short proof. 2. Since stiffness depends on the gradients obtained on points in the input space, which in turn depends on the loss, why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses? Indeed, the authors themselves say that a network has overfitted when training and validation losses diverge. The paper fails to motivate why stiffness is better than just looking at losses during training. 3. The authors mention \u201cThe train-val stiffness is directly related to generalization, as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set. \u201d. Typically, generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement. We would expect validation error to underestimate test error so while they are related, train-val stiffness would not necessarily characterize generalization. It would be interesting to see a train-test stiffness graph to test the authors claim. 4. The paper fails to motivate the the utility of the concept of \u201cDynamical Critical distance\u201d. Since the primary goal of paper is to understand generalization, I would like the authors to clarify the motivation to study this quantity. What additional insight does this provide with respect to generalization? 5. The term \u201cdynamical critical distance\u201d is not used uniformly. For example, it is mentioned as \u201cdynamical critical scale\u201d in section 3.3 and \u201cdynamical critical length\u201d in section 4.2. 6. While the paper on the whole is written in a clear fashion, I found section 4.4 to be particularly confusing. The authors should consider rewriting that section to make it clearer. In summary, the concept of stiffness seems to closely follow training and validation losses and any problem diagnosed using stiffness would therefore be also diagnosed via examining the loss values. This along with other concerns mentioned above mean that I cannot recommend this paper for publication.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review and comments . Your review was released to us only in the middle of the rebuttal period and we therefore did n't have the expected time to prepare a reaction . We would like to address several of the points your brought up . -- -- -- -- -- -- -- -- -- -- -- -- - \u201c 1 . The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided. \u201d The connection between Equation 2 and Equation 3 is very simple and we therefore believed it did not require a detailed proof , however , we \u2019 re happy to explain it in multiple steps . It is a Taylor expansion to the first order . The steps are as follows : The derivative of the loss L at image X1 with respect to the weight vector W is the gradient vector g1 . If we look at the Taylor expansion of the change of loss due to a vector change of weights w , we obtain the dot product delta L = g1 dot w to the first order in the Taylor series . In particular , for a weight change induced by a small gradient step -epsilon * g1 , we get delta L = - epsilon g1 dot g1 . This is true as long as epsilon - > 0 , which we take in the paper . -- -- -- -- -- -- -- -- -- -- -- -- - \u201c 2 . Since stiffness depends on the gradients obtained on points in the input space , which in turn depends on the loss , why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses ? \u201c Stiffness as we defined it is related to the transfer of gains in performance from one input datapoint to another . It is defined by looking at the correlation between loss changes on different inputs , rather than the total loss at once . As such , it is a much finer metric than the total loss . However , you could look at loss changes on individual images and it would be equivalent -- that is exactly how we defined the concept of stiffness in the first place in Equations 1 - 5 . Its connection to the gradient alignment is a mathematical consequence and it is a useful way to look at it , as it makes a direct connection to other works on gradient and Hessians in neural networks . However , you can definitely think about the correlation between changes in loss alone -- we call the particular product stiffness , since it geometrically relates to how easily \u201c bendable \u201d the learned NN function is . -- -- -- -- -- -- -- -- \u201c 3 . The authors mention \u201c The train-val stiffness is directly related to generalization , as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set . \u201d .Typically , generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement. \u201d The misunderstanding here is between the validation set and the test set nomenclature . We used the term validation set to mean the held-out , never-trained-on dataset , and therefore used it to characterize generalization . You could easily call this the test set , since the important distinction wrt the train set is that not a single gradient step was ever taken wrt to a single image in it . We feel this point is only a matter of wording and we \u2019 ll try to make it clearer in the paper . -- -- -- -- -- -- -- -- \u201c 4 . The paper fails to motivate the the utility of the concept of \u201c Dynamical Critical distance \u201d . Since the primary goal of paper is to understand generalization , I would like the authors to clarify the motivation to study this quantity . What additional insight does this provide with respect to generalization ? \u201c We firmly believe that our concept of dynamical critical length is of interest and well motivated , as it captures how localized changes to the learned NN function are when a gradient step with respect to a particular example is applied . It is very related to the rich literature on the spectral bias of neural networks . In our case , we study what could be seen as the dynamical equivalent of the spectral bias -- i.e.on which length scales in the input space do changes to the learned function stop affecting the rest of the function . The additional insight you are asking for is that this allows us to measure how localized changes to the learned function are . If you apply the gradient on input X1 , the inputs X whose loss will change in a correlated manner are at a typical distance \u03be or closer . We measure how this distance changes both with training time and the learning rate used . This gives us an additional diagnostic tool which is in turn directly useful in studying how independent the effects of gradient updates are between different images . -- -- -- -- -- -- -- -- -- -- -- -- \u201c 5 . The term \u201c dynamical critical distance \u201d is not used uniformly . \u2026.The authors should consider rewriting that section to make it clearer . \u201c Thank you for having a closer look . We will correct the typos and make sure to provide greater uniformity in the relevant subsection ."}, "1": {"review_id": "H1e31AEYwB-1", "review_text": "This paper introduces the concept of stiffness: a measure of the change in the loss of sample A due to a gradient step based on sample B. It analyses the expected dynamic for A, B samples from the same and different classes, as well as, samples from the train and test sets. To better understand the dynamics of optimization in neural networks is an open and important problem and the paper is clearly motivated in this regard. The proposed method is straight forward and I am not aware of a similar method. In addition to that, the paper also introduces \"dynamical critical length \u03be\" which is the stiffness of A, B samples based on the cosine similarity of the respective inputs (section 2.4). A linear estimator of when this length becomes 0 is also introduced. Confusingly this is also called the \"dynamical critical length \u03be\" in section 4.2. Later on the term \"dynamical scale \u03be\" and \"dynamical critical scale \u03be\" seem to be used interchangeably. Figure 6 mentions the \"critical length \u03c7\" on the y-axis which seems to be a typo as no such measure was introduced. The equivalence between eq. 2 and the two parts of eq. 3 is not obvious. We'd appreciate if the authors would provide a proof of such. Overall, the paper is written in a simple language but paragraphs remain surprisingly hard to understand. An example of such is e.g. section 4.4: What do the authors mean by \"characteristic distance\" between two input points? What is \"the typical scale of spatial variation\" of a function? etc. The paper concludes that: 1.) there is a link between generalization and stiffness 2.) stiffness decreases with the onset of overfitting 3.) \"general gradient updates with respect to a member of a class help to improve loss on data points in the same class\" 4.) \"The pattern breaks when the model starts overfitting to the training set, after which within-class stiffness eventually reaches 0\" 5.) This is observed for different models on different datasets 6.) \"we observed that the farther the datapoints and the higher the epoch of training, the less stiffness exists between them on average\" 7.) \"the higher the learning rate, the smaller the \u03be\" Verdict: Reject The conclusions are self-evident. The paper fails to demonstrate the usefulness of stiffness and most results are expected and provide little to no insights into the optimization dynamics of deep neural networks. In fact, the reasoning in this paper is almost tautological (conclusions 1-6). E.g. if the A, B samples used to compute stiffness are separately drawn from the train and test set then stiffness is a proxy for the difference between the train error and the test error after another gradient step. The authors then compute stiffness at different points of the optimization procedure and conclude that stiffness decreases when the network starts to overfit. Since overfitting is the point in training where train error and test error diverge it is obvious that this can also be observed with regards to \"stiffness\". Hence, the reasoning is circular. Conclusion 7 is slightly different in that it observes that larger learning rates result in smaller \u03be which, given the previous paragraph, we can rewrite into the statement \"larger learning rates generalise better\". This is a well known empirical observation and has been discussed thoroughly (e.g. on connection with flat and sharp minima or learning rate decay schedules). Disclaimer: This review was done on short notice. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review and comments . We appreciate that your review was done on a very short notice and thank you for it . We would like to address and dispute several of the points your brought up . -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c Overall , the paper is written in a simple language but paragraphs remain surprisingly hard to understand . An example of such is e.g.section 4.4 : What do the authors mean by `` characteristic distance '' between two input points ? \u201d \u2026 . \u201c What is `` the typical scale of spatial variation '' of a function ? \u201d We would like to clarify the points of confusion in Subsection 4.4 . The \u201c characteristic distance \u201d point you bring up is a part of a longer statement : \u201c A natural question arises as to whether the characteristic distance between two input points at which stiffness reaches zero defines the typical scale of spatial variation of the learned function. \u201d So the first \u201c characteristic distance \u201d refers to the distance between two points in the input space where the gradient step with respect to one of them will not influence the other one . This is the quantity we call `` dynamical critical length/scale \u03be '' and which we empirically measure in real networks trained on real data in Figures 5 , 6 , 9 , 10 and 11 . The `` the typical scale of spatial variation '' of a function is its dominant Fourier mode in the input space -- i.e.the length scale in the input space over which the predictions change significantly . This scale is related to the concept often called the spectral bias of neural networks and there is a large literature on the topic . Our point in Subsection 4.4 was to make clear that the scale on which input points do not influence each other under gradient updates = `` dynamical critical length/scale \u03be '' , and the scale over which the predictions change significantly in the input space = `` the typical scale of spatial variation '' , are not necessarily the same . We appreciate that this might have been harder to understand based on our description and will try to rephrase the paragraph for clarity . -- -- -- -- -- -- -- -- -- -- -- -- -- - Different terms used for \u03be and typos `` dynamical critical length \u03be '' in section 4.2 . Later on the term `` dynamical scale \u03be '' and `` dynamical critical scale \u03be '' \u2026 .. `` critical length \u03c7 '' Thank you for spotting that we use the words \u201c scale \u201d and \u201c length \u201d interchangeably . We will adopt a single one to ensure clarity . You are right that in Figure 6 \u03c7 should have been \u03be . This was a typo on our part and we will change it to \u03be . -- -- -- -- -- -- -- -- -- -- -- -- -- \u201c The equivalence between eq.2 and the two parts of eq.3 is not obvious . We 'd appreciate if the authors would provide a proof of such . \u201c The connection between Equation 2 and Equation 3 is very simple and we therefore believed it did not require a detailed proof , however , we \u2019 re happy to explain it in multiple steps . It is a Taylor expansion to the first order . The steps are as follows : The derivative of the loss L at image X1 with respect to the weight vector W is the gradient vector g1 . If we look at the Taylor expansion of the change of loss due to a vector change of weights w , we obtain the dot product delta L = g1 dot w to the first order in the Taylor series . In particular , for a weight change induced by a small gradient step -epsilon * g1 , we get delta L = - epsilon g1 dot g1 . This is true as long as epsilon - > 0 , which we take in the paper . -- -- -- -- -- -- -- -- -- -- -- -- -- \u201c The conclusions are self-evident. \u201d While the self-evidence of our conclusions is a matter of subjective judgement , we do not believe that our results are in fact self-evident and our discussions with fellow researchers support this . It is hard to rebut this point , however , if you do not provide links to specific sources in literature ."}, "2": {"review_id": "H1e31AEYwB-2", "review_text": "This submission introduces a metric, termed stiffness, to evaluate the generalization capability of neural networks. The metric is novel and straightforward, it measures how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. The authors study several configurations on three small datasets. They demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. I give an initial rating of weak accept because (1) The paper is well motivated and well written. Studying generalization is important for neural networks. (2) It seems from experiments that stiffness is a useful metric to indicate models' generalization capability. However, I have a few concerns. First, the authors study several configurations like train-train, train-val and val-val. However, these configurations are still in-domain analysis, the data distribution is quite similar. It can not support author's claims well. Adding an experiment where domain gap is large will make the submission stronger, such as train-test, cross-dataset or challenging tasks like semantic segmentation. Second, the datasets being used are very small. I understand that for theoretically analysis, small datasets are quick to converge and easy to demonstrate. However, this submission focuses on generalization problem during transfer learning. Hence, it needs at least a bigger dataset, like ImageNet, to show it really works. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and comments . We hope that you will champion our paper . We provide a detailed response to the points you brought up below . -- -- -- -- -- -- \u201c First , the authors study several configurations like train-train , train-val and val-val . However , these configurations are still in-domain analysis , the data distribution is quite similar. \u201d We studied the stiffness between input images from the training set and the validation set precisely in order to directly quantify the transfer of performance improvement gained on the training set to the unseen validation set . We agree that those distributions are hopefully very similar , however , they are not identical . In that sense , this constitutes a weak version of the out-of-distribution performance experiment you were suggesting . We agree that adding an experiment where the domain gap is large might be interesting , however , the focus of our paper was not on transfer learning ( where this is a common regime ) , but rather on learning on a specific dataset itself . The transfer we were concerned with was from one example to another , i.e.within dataset generalization . If we , for example , looked at the stiffness between train images and random noise images , the interpretation of that metric would be very difficult , as it is not a priori known what kind of behavior would even be desirable there . It could even be the case that you do not want to transfer any performance to random out-of-distribution images , as this could limit your performance on the actual distribution . -- -- -- -- -- -- -- -- -- -- - \u201c Second , the datasets . I understand that for theoretically analysis , small datasets are quick to converge and easy to demonstrate . However , this submission focuses on generalization problem during transfer learning . Hence , it needs at least a bigger dataset , like ImageNet , to show it really works. \u201d We understand that looking at ImageNet would strengthen our case , however , the stiffness calculations are very computationally demanding and the consistent appearance of the effects on MNIST , Fashion MNIST and CIFAR-10 is , according to us , a good indication of their generality . In addition , the goal of our paper is to introduce a metric and show its usefulness , for which we believe CIFAR-10 can be sufficient . Nonetheless , we will try to show our results on larger datasets ."}}