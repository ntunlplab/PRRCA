{"year": "2017", "forum": "rkKCdAdgx", "title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "decision": "Reject", "meta_review": "The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.", "reviews": [{"review_id": "rkKCdAdgx-0", "review_text": "Description: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\". The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives. Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs. Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class. The paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach. Evaluation: It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test. The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step. In the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?) Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable. Minor points: As the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned. The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary. ", "rating": "3: Clear rejection", "reply_text": "Thanks for the ( somewhat delayed ) review . The main critic seems to be summarized in the phrase : `` It is hard to see a lot of novelty in this paper . The Bloom filters are an existing technique , which is applied very straightforwardly here . '' That is a surprising way to criticize the work , since many advances in deep learning come about through the application of a relatively simple method to a network or model . For instance , batch-normalization is the application of z-score normalization to each layer in the network , dropout is the application of sampling to the units of each layer in the network , embedding is adding a dense layer between the network and the input , residual learning is adding the input to the layer directly to the next layer , and much more . In that respect , we are somewhat surprised to see that the simplicity of our approach is seen as its main weakness . Our approach is simple but yields very good results and we are not aware of any `` off-the-shelf existing embedding '' that provides all the benefits of Bloom embeddings ( such as they can be applied to both inputs outputs , minimal computation overhead , increase in performance compared to alternatives or uncompressed networks in many cases , etc . ) . We would be grateful if the reviewer could point out any such `` off-the-shelf existing embedding '' with similar properties . Regarding studying more embedding dimensionalities for competing approaches , we do not do so because the majority of such approaches do not scale in terms of time and space . Consider , for instance , the time required to perform an SVD decomposition from a 70K-by-70K sparse matrix ( MSD data set ) into 30K dimensions and the space to store the resultant 30K-by-30K dense projection matrix . Moreover , considering different input and output projections would result in an unattainable number of experiments ( InputDim \\times OutputDim \\times # Datasets ( \\times # Approaches ) ) , which is somewhat prohibitive given our GPU resources . Note though that , even without optimizing for the 'right ' input/output embedding dimensionality , our method still performs very well ."}, {"review_id": "rkKCdAdgx-1", "review_text": "The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \"Hash Kernels for Structured Data\" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \"Robust Bloom Filters for Large MultiLabel Classification Tasks\". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling. The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic, that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g., 1) the model compression approach of https://arxiv.org/abs/1510.00149 2) training with integer/binary weights https://arxiv.org/abs/1511.00363 Overall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . In the next iteration of the paper we will include and comment the Shi et al . ( JMLR 2009 ) and Cisse et al . ( NIPS 2013 ) references . In addition , we will add the suggested `` Tensorflow '' and `` integer/binary weights '' references to our related work section . Regarding the `` model compression approach '' reference , we 'd like to note that we already cite it in Section 2 , together with https : //arxiv.org/abs/1511.06530 ."}, {"review_id": "rkKCdAdgx-2", "review_text": "The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox. Pros: - Can be applied to practically any model, either at the input or hte output. - Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. Cons: - The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description. - The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification. This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We 'll try to simplify the description of the method in Section 3 ."}], "0": {"review_id": "rkKCdAdgx-0", "review_text": "Description: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\". The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives. Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs. Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class. The paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach. Evaluation: It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test. The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step. In the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?) Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable. Minor points: As the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned. The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary. ", "rating": "3: Clear rejection", "reply_text": "Thanks for the ( somewhat delayed ) review . The main critic seems to be summarized in the phrase : `` It is hard to see a lot of novelty in this paper . The Bloom filters are an existing technique , which is applied very straightforwardly here . '' That is a surprising way to criticize the work , since many advances in deep learning come about through the application of a relatively simple method to a network or model . For instance , batch-normalization is the application of z-score normalization to each layer in the network , dropout is the application of sampling to the units of each layer in the network , embedding is adding a dense layer between the network and the input , residual learning is adding the input to the layer directly to the next layer , and much more . In that respect , we are somewhat surprised to see that the simplicity of our approach is seen as its main weakness . Our approach is simple but yields very good results and we are not aware of any `` off-the-shelf existing embedding '' that provides all the benefits of Bloom embeddings ( such as they can be applied to both inputs outputs , minimal computation overhead , increase in performance compared to alternatives or uncompressed networks in many cases , etc . ) . We would be grateful if the reviewer could point out any such `` off-the-shelf existing embedding '' with similar properties . Regarding studying more embedding dimensionalities for competing approaches , we do not do so because the majority of such approaches do not scale in terms of time and space . Consider , for instance , the time required to perform an SVD decomposition from a 70K-by-70K sparse matrix ( MSD data set ) into 30K dimensions and the space to store the resultant 30K-by-30K dense projection matrix . Moreover , considering different input and output projections would result in an unattainable number of experiments ( InputDim \\times OutputDim \\times # Datasets ( \\times # Approaches ) ) , which is somewhat prohibitive given our GPU resources . Note though that , even without optimizing for the 'right ' input/output embedding dimensionality , our method still performs very well ."}, "1": {"review_id": "rkKCdAdgx-1", "review_text": "The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \"Hash Kernels for Structured Data\" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \"Robust Bloom Filters for Large MultiLabel Classification Tasks\". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling. The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic, that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g., 1) the model compression approach of https://arxiv.org/abs/1510.00149 2) training with integer/binary weights https://arxiv.org/abs/1511.00363 Overall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . In the next iteration of the paper we will include and comment the Shi et al . ( JMLR 2009 ) and Cisse et al . ( NIPS 2013 ) references . In addition , we will add the suggested `` Tensorflow '' and `` integer/binary weights '' references to our related work section . Regarding the `` model compression approach '' reference , we 'd like to note that we already cite it in Section 2 , together with https : //arxiv.org/abs/1511.06530 ."}, "2": {"review_id": "rkKCdAdgx-2", "review_text": "The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox. Pros: - Can be applied to practically any model, either at the input or hte output. - Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. Cons: - The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description. - The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification. This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We 'll try to simplify the description of the method in Section 3 ."}}