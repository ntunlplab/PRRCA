{"year": "2021", "forum": "fSTD6NFIW_b", "title": "Understanding the failure modes of out-of-distribution generalization", "decision": "Accept (Poster)", "meta_review": "This paper studies the reasons for failure of trained neural network models on out of distribution tasks. While the reviewers liked the theoretical aspects of the paper, one important concern is about the applicability of these insights to real datasets. The authors added an appendix to the paper showing results on a real dataset that mitigates this concern to an extent. Further, there are interesting insights in the paper to merit acceptance.", "reviews": [{"review_id": "fSTD6NFIW_b-0", "review_text": "I stand by my initial review that this is a strong submission , and having read through the other reviews and author responses , I am raising my confidence level as well ( I think I have a solid grasp of this work 's potential import ) . I disagree with critiques of the paper 's novelty and practicality -- I think it provides new insights into OOD problems with substantive theory ( not common ) and provides actionable insights to boot . Also , the the revised manuscript is much improved . I hope this gets accepted . -- This submission presents a rigorous analysis of a subset of ways in which machine learning models can fail when encountering out-of-distribution ( OOD ) samples ( often referred to as train/test skew or as train/scoring skew in industry ) . As the paper notes , the topic has received a great deal of attention , particularly under other guises ( `` domain adaptation '' ) . However , much of that attention has aimed at pragmatic or heuristic solutions ( various tricks to design or learn `` invariant '' features ) , while our fundamental understanding of what goes wrong in OOD situations remains incomplete . This paper aims to fill those gaps in understanding by studying simplified settings , and asking the question : why does a statistical model learn to use features susceptible to shift ( `` spurious '' features ) when the task can be solved using only safe ( `` invariant '' ) features . After formulating five constraints ( guaranteed to hold true for easy-to-learn tasks ) , they go on to show that failures come in two flavors : geometric skew and statistical skew . They analyze and explain each in turn , while also providing illustrative empirical results . I like this work a lot ( though I am more lukewarm on the paper itself , see below ) , and barring discovery of a fatal flaw during the discussion , I would advocate with some enthusiasm for its inclusion in the conference . The paper 's claims are stated at the bottom of page 2 as : 1 . Careful design and articulation of `` easy-to-learn '' settings in which there are few , if any , unmeasured variables that could confound the findings ( a weakness in previous work on this topic ) . 2.Identification of two ( but not the only two ) distinct types of OOD failures that occur even in easy-to-learn settings , in the form of necessary and sufficient data `` skews . '' 3.Experimental evidence to illustrate and support the analyses from ( 2 . ) , along with enlightening discussion . I agree with the paper 's claims , though I admit that I was not previously familiar with , e.g. , Sagawa 2019 or Tsipras 2019 , and so can not confidently situate this work amongst related research . I also feel my understanding may still be somewhat superficial -- I buy its arguments but do n't have a particularly strong intuition yet for the two flavors of skew ( particularly in non-toy settings ) . This work has a very strong scientific flavor ( not always true of machine learning research ) : I would liken the restriction to carefully designed `` easy-to-learn '' settings to a well-designed laboratory experiment in which there are few , if any , unmeasured variables that could confound the findings . It is very elegant and satisfying to read and think about . I would anticipate that this paper will inspire a lot of follow-up work , in which other researchers adopt the `` easy-to-learn '' and `` skew '' framework and terminology and even utilize the specific experimental designs in this paper . After all , machine learning researchers love adopting intellectual frameworks and benchmarks that they can build upon rapidly . The `` easy-to-learn '' constraints articulated in Section 3.1 are sensible and clearly stated , and I am unable to find fault in them thus far . I agree with this statement on page 5 : `` any algorithm for solving OoD generalization should at the least hope to solve these easy-to-learn tasks well . '' The experiments were thoughtful and well-designed , and their results are presented effectively : each plot , it seems , illustrates a particular point or supports a specific argument in the paper . For example , I like how Figures 2 and 3 serve as visual summaries of the geometric and statistical skew sections , respectively . A reader ( particularly a savvy one familiar with the relevant related work ) could probably skip Sections 4 and 5 ( three pages total ! ) and still get the high level idea simply by skimming the plots and reading the captions of those two figures . The largest weakness I perceive concerns the clarity and accessibility of the writing : for example , the connection drawn in Section 4 to the work on norms in over-parameterized neural nets is very interesting , but I 'm not sure the text fully succeeds in further connecting it to OOD settings . In particular , certain details of the ongoing discussion of majority and minority groups are n't entirely clear ( to me , at least ) ... are minority group samples available during training , just in smaller number ? In that case , what is the OOD `` shift '' -- the prevalence of the minority group at test time ? Likewise , I 'm not sure I really connected with the takeaway in Section 5 -- is it that early in the optimization , the `` spurious '' weights get updated repeatedly by an amount proportional to the spurious correlation , and that it then takes a long time to undo these updates , if at all ? The statistical skew section is definitely more abstract and perhaps a little harder to connect to practical settings , vs. geometric skew where the bridge is the previous work on `` norms . '' My recommendation is to accept this submission , and at the moment , I am willing to advocate for it . However , it is entirely possible I am missing ( or misunderstanding ) key details , and so I am eager to discuss with the other reviewers .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "First , we want to address your final two questions . * * Geometric skews * * : We apologize for the clarity issues in the discussion of the majority/minority groups . We have rewritten this paragraph , but nevertheless clarify this here too . The majority and minority groups is a partition of the training data defined for the purpose of understanding how the max-margin classifier works . The max-margin classifier as such does not know which is which ! How does the \u201c increasing norms \u201d observation explain the spurious-feature-reliance ? There are four logical steps in this argument : 1 . Note the ( trivial ) fact the number of points of the minority group $ S_ { \\text { minor } } $ ( defined by points for which $ x_ { \\text { sp } } \\cdot y < 0 $ ) is ( much ) less than the size of the whole dataset $ S $ . 2.The \u201c increasing norms \u201d observation is that \u201c if we train a max-margin classifier to fit the data using only the invariant features , its norm grows with number of training points. \u201d 3 . The \u201c increasing norms \u201d observation together with step 1 tells us that if we were to use only the invariant feature , fitting just $ S_ { \\text { minor } } $ is ( much ) cheaper in $ \\ell_2 $ norm than fitting all of $ S $ . This gap in costs is what we call a geometric skew . 4.Hence , the max-margin classifier would rather just set $ w_ { \\text { sp } } > 0 $ to classify $ S_ { \\text { major } } $ and use the invariant feature to focus on classifying the remaining set , $ S_ { \\text { minor } } $ . This would be cheaper than setting $ w_ { \\text { sp } } = 0 $ and using the invariant feature to classify all of $ S $ . Thus , the max-margin uses the spurious feature because of the geometric skew . * * Statistical skews : * * What you \u2019 ve stated is precisely the takeaway i.e. , gradient descent initially updates the spurious feature proportional to the spurious correlation , but it takes a long time to \u201c correct \u201d this back to a zero . We want to make two additional points : - We have updated Theorem 2 to apply to a more general easy-to-learn setting that doesn \u2019 t just apply to a 2-dimensional task . We hope this helps in convincing you that the above intuition is indeed more generally true . - The experiments in Sec 5 are intended to bridge these insights to practice . We \u2019 re able to show that a NN trained on an MNIST/CIFAR10-based dataset with statistical skews ( but no geometric skews ) is vulnerable to shifts in the spurious feature . This shows that the NN has not converged to a \u201c max-margin-like \u201d state that doesn \u2019 t use the spurious feature . * * Regarding your ( accurate ) understanding : * * We believe that your understanding of our results , and also the background context is on point . Indeed , you make a good point that most existing algorithms in OoD in deep learning are often based on heuristics or on rough intuition . With regards to your point about related research , we think it \u2019 s useful to add that there has been very little theory understanding the exact factors behind why existing algorithms don \u2019 t work for OoD generalization . The ones we \u2019 ve cited like Sagawa et al. , are very recent . Most other work in this space has been empirical . Your understanding regarding the motivation behind trying to study easy-to-learn tasks is accurate . This motivation is a very nuanced idea , and we \u2019 re pleased that you appreciated it . Indeed , as you say , we want to \u201c switch off \u201d confounding factors to tease out the most fundamental factors of failure . Thank you for finding our discussion elegant . We think that what is particularly appealing about the result is in how these two skews are \u201c orthogonal \u201d effects . We agree with you that future work -- both theory and practice -- can hopefully build on these ideas . We \u2019 re also glad that you found the figures useful and the experiments well-justified . You \u2019 re right in that we made sure that Fig 2 conveyed the idea of geometric skews and Fig 3 the idea of statistical skews ."}, {"review_id": "fSTD6NFIW_b-1", "review_text": "The paper studies generalization under distribution shift , and tries to answer the question : why do ERM-based classifiers learn to rely on `` spurious '' features ? They present a class of distributions called `` easy-to-learn '' that rules out several explanations given in recent work and isolates the spurious correlation phenomenon in the simplest possible setting . Even on `` easy-to-learn '' distributions , linear models obtained from ERM use spurious features owing to either the dynamics of gradient descent trained on separable data ( very slow convergence to the max-margin classifier ) or a certain geometric skew in the data . Pros : - The paper address a question of great interest to the ICLR community : generalization under distribution shifts . - The `` easy-to-learn '' set of instances seems like a reasonable test-case to understand robustness to distribution shifts before moving to more complex setting . - The fact that spurious feature use can arise on the `` easy-to-learn '' instances suggests that previous explanations are underpowered in the sense that they 're ruled out by this construction , but the pattern still arises . - The `` statistical '' skew shows how the dynamics of gradient descent can introduce brittleness to distribution shift , albeit in a limited setting . Cons : - This paper is written is a way that 's confusing and frequently difficult to follow . Rather than focus on explicating the core phenomenon well and supporting it with clear theorems and well-justified experiments , the authors at times take a shotgun approach and mention numerous constructions ( often in-line ) , references to empirical results , descriptions of experiments , that seem to obfuscate rather than clarify the core phenomena they 're presenting . For instance , the `` geometric '' skew in Section 4 is never precisely explained . - In a similar vein , the paper does not justify the experiments they run in light of the phenomena they seek to `` intuitively understand . '' For instance , in Section 4/5 , the paper presents a litany of experiments , without explaining how they fit into the `` easy-to-learn '' framework or relate to a phenomena described by the theorems . It 's possible there 's a connection , but it 's not evident to this reader . - The experiments focus on a variety of synthetic distribution shifts . While this may be useful in understanding the constructions in the paper , it 's not evident to what extent these 'skews ' and the 'spurious/invariant ' distinction given here can explain the lack of robustness observed in practice on `` real '' or `` natural '' distribution shifts . Update after rebuttal : Thank you to the authors for their detailed response . The clarification and updates in the geometric skews section and the more explicit justification and connections between the framework/theoretical results and the experiments improved readability and clarity . I also appreciate putting greater weight on the theoretical contributions and `` easy-to-learn '' task definitions , which provide a simple but non-trivial test case for robustness research . I 'm increasing my score accordingly .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Justification for experiments : * * We are sorry to hear that you found the presentation of the experiments confusing . We have tried to improve this in the updated version . However , it seems like there \u2019 s disagreement among the reviews e.g. , - AnonReviewer4 : \u201c experiments are carefully designed based on the constraints\u2026 well in support of their theoretical claims. \u201d ; - AnonReviewer3 : \u201c The experiments were thoughtful and well-designed\u2026 results are presented effectively \u201d - AnonReviewer1 : \u201c a fairly rigorous empirical analysis of geometrical and statistical skews on CIFAR and MNIST \u201d , `` paper is fairly well written overall '' As such , these experiments have a precise role : they provide concrete motivation behind the abstract theoretical ideas that follow them . Here \u2019 s the justification for each inline experiment : - * * The MNIST + random ReLU features experiment in Sec 3 * * : This identifies a concrete linear task where max-margin fails even though the task is easy . Identifying this really easy task helps us in justifying our subsequent definition of the easy-to-learn tasks via the constraints . These constraints are abstract , and the reader may not appreciate why we make them without that concrete experiment beforehand . - * * The \u201c increasing norm \u201d experiments in Sec 4 * * : This empirically validates a key property of real-world invariant features that we subsequently use to explain why max-margin fails . Without demonstrating this observation upfront in Sec 4 , the reader wouldn \u2019 t be comfortable digesting the insights from Thm1 which would all be based on an unconfirmed hypothesis . - * * The GD experiment on the 2D dataset in Sec 5 * * : This empirically identifies a new failure mode that can not be explained by the previous story in Sec 4 . This then leads to the theoretical question of \u201c how can you explain this empirical observation ? \u201d and thus , the rest of that section . * * Relation between NN experiments and easy-to-learn tasks * * : We apologize for not making this connection clear enough . We clarify this below , and in more detail in the revised submission . - In Sec 4 , we present four non-linear classification tasks trained by a neural network . The first two are easy-to-learn . The last two tasks are NOT easy-to-learn -- these crucially break a constraint , which is why failure happens . But we can still argue how such a failure can be explained via a geometric argument like Thm 1 in all these tasks . The purpose of the brief main paper discussion is to only let the reader know that the geometric insights are not specific to the linear theoretical examples . We defer the exact connection between these tasks and Thm 1 to the App C. - In Sec 5 , we present two non-linear tasks trained by an NN . Both of these are easy-to-learn tasks . In both , we create a dataset with no geometric skew , but with some statistical skew . By showing that the resulting NN classifier uses the spurious feature , we demonstrate our hypothesis that failure can happen even without geometric skews , as long as the statistical skew exists . * * Synthetic spurious features * * : You \u2019 re right : empirically demonstrating these insights on datasets with realistic spurious feature shifts would be very illuminating ! To address this : - In App E , we have added experiments demonstrating the effect of these skews in an obesity estimation task . Here , instead of introducing an artificial spurious feature , we sample the data in a biased way so as to induce a spurious correlation between an existing feature and the label . - Having said that , we clarify why we think * * this is beyond the scope of our paper . * * Since we theoretically study OoD -- rather than show non-theoretical numerical improvements of a proposed algorithm -- we need datasets where it is easy to quantify and intervene on/modify the spurious feature ( e.g. , we need this to show how norms increase with training set size when we use only the invariant feature ) . * * However , in none of the existing realistic OoD benchmarks ( PACS , VLCS etc. , ) is there a way to quantify the spurious features * * . Indeed , in prior work like IRM , RiskEx etc. , all \u201c theory-verifying \u201d experiments are limited to the synthetic image dataset , Colored MNIST . In fact , we \u2019 ve even gone beyond this dataset to propose a variety of unique quantifiable , synthetic datasets . We \u2019 ve also added a cats vs. dogs datasets with a color shift in the revised version . The lack of a realistic + quantifiable dataset is an important gap , beyond the scope of this paper , that is hopefully addressed by the OoD community . * * A broad note * * : We \u2019 d also like to clarify that the experimental part of the paper forms less than quarter of our contributions . Most of our contribution lies in providing a theoretical understanding of OoD failure covered by the points you \u2019 ve mentioned in your positives . We hope that with these clarifications and our steps to address your concerns regarding the experiments and experiment discussion help you reconsider the evaluation of this paper ! Thank you !"}, {"review_id": "fSTD6NFIW_b-2", "review_text": "This paper investigates the reasons why machine learning models usually fail to generalize out-of-distribution even in easy-to-learn tasks where one would expect these models to succeed . The authors propose two kinds of skews in the data : geometric and statistical , to explain this behavior and theoretically demonstrate it in the linear and easy-to-learn settings . Pros : + The problem studied in this paper has been one of the most important in the community and this work has a meaningful attempt on the theoretical side . + The theoretical results obtained by analyzing the simplest tasks are insightful and could be seemingly drawn on in more general settings . + The experiments are carefully designed based on the constraints , which is well in support of their theoretical claims . Major concerns : I agree with the authors on most of their explanations on the failure modes of OOD generalization from the geometric and statistical perspectives in easy-to-learn tasks . However , the authors claim that these two skews are \u201c not just a sufficient but also a necessary factor for failure of these models in easy-to-learn tasks \u201c , which I think might be overclaimed , even in the easy-to-learn tasks defined in the paper . I do believe that both geometric and statistical skews play a role in the failure of OOD generalization , but they might not be the only reasons . In fact , even if there exist no spurious features , machine learning models might fail to generalize out-of-distribution as well . Here is the example . Let us consider a comparison task , where we aim to learn a linear classifier to predict which one of any given two values { x_i , x_j } is larger , i.e. , it outputs +1 if x_i > = x_j , and otherwise it outputs -1 . For simplicity , let us further assume that -1 = < x_i , x_j = < +1 and x_i^2 + x_j^2 =1 . In this case , x_inv = ( x_i , x_j ) and there is no x_sp . It is also easy to check that this task satisfies the five constraints stated in the paper . Given the training dataset in which all data points ( x_i , x_j ) are satisfying that 0 = < x_i , x_j = < +1 and x_i^2 + x_j^2 =1 , it is easy to learn a linear classifier that can be well generalized to the whole domain [ -1 , +1 ] . If we represent ( x_i , x_j ) in the polar coordinate system , then ( x_i , x_j ) is converted to ( 1 , \\theta ) . In this case , we have x_inv = \\theta and no x_sp , which also satisfies the five constraints . Although we still can learn a perfect linear classifier in the training data , it is impossible to generalize to the whole [ -\\pi , +\\pi ] corresponding to [ -1 , +1 ] . The reason is that this task is no longer linearly separable . From the example above , we can see that even without spurious features , some simple transformations only on invariant features would render OOD generalization impossible . This would be another failure mode beyond the geometric and statistical skews . Minor concerns : - The constraint on fully predictive invariant features seemingly simplifies the OOD generalization problem too much , making the theoretical results hard to apply to more general settings . Actually this constraint circumvents two key questions which play a pivotal role in the OOD generalization . One is how to justify whether or not features are fully predictive and invariant across all the domains or environments . This involves the generalization assumption , like Assumption 8 of Arjovsky et al . ( 2019 ) .The other is how to learn the partially predictive invariant features which , albeit not fully predictive , remains invariant across all the domains . If we want to solve the OOD generalization problem in practice , these two questions have to be answered . - The example given in Constraint 3.2 is not suitable . The distribution of invariant features must be identical across all domains , otherwise these features should not be treated as \u201c invariant \u201d . The exact distribution of the shapes of cows and camels vary across domains because they are not the true invariant features . - Constraint 3.5 is too strong as well . It circumvents another key question in the OOD generalization : how to identify x_inv and x_sp . If we can identify them , the OOD generalization would be reduced to a simple ERM problem . Other comments : - The last 6th line before section 3.1 says that Pr_ { D_test } [ x_sp \\cdot y > 0 ] = 0.0 . I believe this is a typo , right ? - Overall I feel too much content is placed in the appendix and it affects the fluency of reading a bit . I suggest that the authors re-organize the content and put back some stuff to the main text .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Major concern : * * We understand that your point is that there is a counter-example to our claim that \u201c geometric/statistical skews are necessary for failure in easy-to-learn tasks \u201d . However , we believe that this is not a counter-example as it is not an easy-to learn task : it breaks Constraint 2 i.e. , the invariant feature distribution must be the same in training and testing . This is not obeyed in your example since test-time points have new invariant feature values , thus making your task particularly harder than the easy-to-learn tasks ! Also , it \u2019 s worth making a minor clarification : our setting considers the realizable case where the optimal hypothesis $ h^ * $ in the learner \u2019 s hypothesis class $ H $ ( defined under the \u201c the domain generalization setting and ERM \u201d paragraph ) achieves zero error ( captured by Constraint 1 ) . In your example , the optimal hypothesis from the ERM learner \u2019 s linear hypothesis can not achieve zero error thus violating Constraint 1 too . So your task is hard-to-learn in the sense that it is in the non-realizable setting and violates Constraint 1 . * Nevertheless , the main issue with the example is that it breaks Constraint 2 . * Finally , we \u2019 d like pre-emptively clarify that our \u201c necessary and sufficient \u201d claim is intended only for ( a ) easy-to-learn tasks and ( b ) for linear , GD/max-margin . This claim is a consequence of the upper and lower bounds in Thm 1 and 2 . We had made sure to phrase this carefully in Contribution 3 . Having said all this , we want to thank you again for engaging with the material deeply . * * Minor Concern 1 : * * Your concern suggests that the set of easy-to-learn tasks doesn \u2019 t contain complicated real-world tasks where the invariant feature isn \u2019 t fully predictive . While we completely agree with this fact ( and we \u2019 ve admitted this ourselves in the conclusion ) , we must remind ourselves of a fundamental nuance here ( one which you \u2019 ve noted yourself in your paper summary ! ) . The nuance is that * * our goal here is not to prove the success of a new OoD algorithm * * -- had it been that , your concern regarding the fully predictive invariant feature is valid ! OTOH , we want to show that ERM fails . * * It \u2019 s more powerful ( and most challenging ) to show that an algorithm fails to work in a task that is easy where the invariant features are fully predictive , than one that is hard * * . Hence this is not a problematic aspect but in contrast , a highlight of our result ! Further , we think it \u2019 s natural that the insights and failure modes we elucidate here carry over to more complicated tasks where the invariant feature may not be fully predictive . For a concrete example : GD would still be slow in its convergence on more complicated tasks ( where invariant feature isn \u2019 t fully predictive ) , and hence still suffer from statistical skews ( as it \u2019 ll absorb the spurious correlation early on , and take too long to get rid of it ) . Additionally , we think our experiments on neural networks + CIFAR-like datasets are concrete evidence for the generality of our insights . * * Minor Concern 2 * * : Here \u2019 s why the example * is * suitable . Consider an example with two types of cow faces ( A & B ) and two types of camel faces ( C & D ) . In the first domain all four types are equally probable , and in the second domain , A and C are most probable . This would break Constraint 2 since the shape feature varies in distribution across domains . Yet , the \u201c shape feature \u2192 label \u201d mapping remains the same in both domains , and so the shape feature can be considered an invariant feature that fully predicts the true label . * * Minor Concern 3 : * * To clarify , this constraint did not mean to imply that the learner knows which feature is spurious and which feature is invariant . More precisely , both our Theorems hold in the case where the original feature space is orthogonally and arbitrarily rotated ( and the orthogonal rotation matrix is not known to the learner ) -- this holds because we consider GD/max-margin which are equivariant to such transformations . We wrote the constraint in this way just for ease of notation but we understand the confusion and apologize for it . We have clarified this in our updated paper . * * Your other comments : * * - We \u2019 re not sure we see a typo here could you clarify what you think it should have been ? During test time , we want $ x_ { \\text { sp } } \\cdot y < 0 $ on all datapoints in contrast to training time where $ x_ { \\text { sp } } \\cdot y > 0 $ on most datapoints . - In other words , we want to \u201c flip \u201d the correlation completely to test the robustness of the learned classifier . Regarding appendix references , we \u2019 ve tried to reorganize/add some content especially in the discussion of geometric skews , and in the empirical examples in Sec 4 . We \u2019 ll continue working on the re-organization . Thanks for the useful feedback . To summarize , we are hopeful that we have addressed your major concern and provided the required clarifications for your minor concerns too . If that is the case , we appreciate it if you consider increasing your score ."}, {"review_id": "fSTD6NFIW_b-3", "review_text": "Overview : The authors study the out of distribution generalisation problem in detail wherein a model may incorrectly use spurious correlations in the data to make predictions on data at test time . This is a fundamental problem and very crucial/difficult to tackle across various domains Quality and Clarity : The paper is fairly well written overall . Originality and Significance : OOD detection is a well-studied problem that has proven difficult to date ; it 's highly relevant across applications in high-stake domains . The approach taken in this paper is to a ) study tasks that are easy to succeed on , b ) show that OOD failure occurs even in these easy settings . The authors subsequently show that geometric and statistical skews are necessary for failure . As such the approach has limited novelty , since it does not offer a concrete solution to the OOD detection problem but rather examines necessary conditions for OOD failure . Pros : 1 ) The authors study a highly relevant problem of OOD failures 2 ) The authors present a fairly rigorous empirical analysis of geometrical and statistical skews on CIFAR and MNIST Cons : 1 ) The paper is limited in novelty ; while the authors state find that OOD failures result from statistical and geometric skews , they do not provide an adequate solution to overcoming these skews 2 ) OOD failures are only studied in the context of small theoretical examples or image tasks such as MNIST or CIFAR . However , OOD failures are widespread across various domains and become especially important to study in high risk settings such as in a clinical context etc . I would have liked to have seen some exposition of these problems in a real high-risk setting .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Finally , we think it would be helpful to place our paper in the context of current trends in ML theory . In particular , there is a large swathe of theoretical papers these days that are solely dedicated towards understanding a particular empirically observed phenomenon ( such as the success/failure of an existing deep learning algorithm ) -- * * these papers do not provide a new solution . * * We present a few examples at the end of this comment . Such papers ( including ours , we hope ) are valuable in themselves as they make novel points , provide stronger theoretical foundations , add rigor to the field , generate significant discussion and introspection on empirical progress . We feel that penalizing such work for not immediately solving the studied phenomenon would cut short a promising line of future work . We hope that keeping this in mind provides a broader perspective while approaching the question of what makes our contributions a standalone paper . Examples of works that solely understand an existing algorithm \u2019 s working/failure * * without proposing any new algorithms : * * - An investigation of why overparameterization exacerbates spurious correlations , ICML 2020 https : //arxiv.org/abs/2005.04345 ( a study of why overparameterized max-margin makes the effect of spurious correlations worse -- we cite this ) - Pitfalls of simplicity bias in neural networks , NeurIPS 2020 https : //arxiv.org/abs/2006.07710 ( a study of how standard NN training ends up learning to rely on features that separate the data only by a small margin just because they are simple to learn ) - Robustness may be at odds with accuracy , ICLR 2019 https : //arxiv.org/abs/1805.12152 ( a study of why robust training could hurt standard accuracy ) - Adversarially Robust Generalization Requires More Data , NeurIPS 2018 https : //arxiv.org/abs/1804.11285 ( a study explaining why robust training fails to generalize to unseen data ) - Identifying and understanding empirical phenomena , ICML 2019 workshop http : //deep-phenomena.org/ ( a workshop dedicated to works like this ) - The risks of invariant risk minimization , https : //arxiv.org/abs/2010.05761 ( very recent work studying the pitfalls of IRM )"}], "0": {"review_id": "fSTD6NFIW_b-0", "review_text": "I stand by my initial review that this is a strong submission , and having read through the other reviews and author responses , I am raising my confidence level as well ( I think I have a solid grasp of this work 's potential import ) . I disagree with critiques of the paper 's novelty and practicality -- I think it provides new insights into OOD problems with substantive theory ( not common ) and provides actionable insights to boot . Also , the the revised manuscript is much improved . I hope this gets accepted . -- This submission presents a rigorous analysis of a subset of ways in which machine learning models can fail when encountering out-of-distribution ( OOD ) samples ( often referred to as train/test skew or as train/scoring skew in industry ) . As the paper notes , the topic has received a great deal of attention , particularly under other guises ( `` domain adaptation '' ) . However , much of that attention has aimed at pragmatic or heuristic solutions ( various tricks to design or learn `` invariant '' features ) , while our fundamental understanding of what goes wrong in OOD situations remains incomplete . This paper aims to fill those gaps in understanding by studying simplified settings , and asking the question : why does a statistical model learn to use features susceptible to shift ( `` spurious '' features ) when the task can be solved using only safe ( `` invariant '' ) features . After formulating five constraints ( guaranteed to hold true for easy-to-learn tasks ) , they go on to show that failures come in two flavors : geometric skew and statistical skew . They analyze and explain each in turn , while also providing illustrative empirical results . I like this work a lot ( though I am more lukewarm on the paper itself , see below ) , and barring discovery of a fatal flaw during the discussion , I would advocate with some enthusiasm for its inclusion in the conference . The paper 's claims are stated at the bottom of page 2 as : 1 . Careful design and articulation of `` easy-to-learn '' settings in which there are few , if any , unmeasured variables that could confound the findings ( a weakness in previous work on this topic ) . 2.Identification of two ( but not the only two ) distinct types of OOD failures that occur even in easy-to-learn settings , in the form of necessary and sufficient data `` skews . '' 3.Experimental evidence to illustrate and support the analyses from ( 2 . ) , along with enlightening discussion . I agree with the paper 's claims , though I admit that I was not previously familiar with , e.g. , Sagawa 2019 or Tsipras 2019 , and so can not confidently situate this work amongst related research . I also feel my understanding may still be somewhat superficial -- I buy its arguments but do n't have a particularly strong intuition yet for the two flavors of skew ( particularly in non-toy settings ) . This work has a very strong scientific flavor ( not always true of machine learning research ) : I would liken the restriction to carefully designed `` easy-to-learn '' settings to a well-designed laboratory experiment in which there are few , if any , unmeasured variables that could confound the findings . It is very elegant and satisfying to read and think about . I would anticipate that this paper will inspire a lot of follow-up work , in which other researchers adopt the `` easy-to-learn '' and `` skew '' framework and terminology and even utilize the specific experimental designs in this paper . After all , machine learning researchers love adopting intellectual frameworks and benchmarks that they can build upon rapidly . The `` easy-to-learn '' constraints articulated in Section 3.1 are sensible and clearly stated , and I am unable to find fault in them thus far . I agree with this statement on page 5 : `` any algorithm for solving OoD generalization should at the least hope to solve these easy-to-learn tasks well . '' The experiments were thoughtful and well-designed , and their results are presented effectively : each plot , it seems , illustrates a particular point or supports a specific argument in the paper . For example , I like how Figures 2 and 3 serve as visual summaries of the geometric and statistical skew sections , respectively . A reader ( particularly a savvy one familiar with the relevant related work ) could probably skip Sections 4 and 5 ( three pages total ! ) and still get the high level idea simply by skimming the plots and reading the captions of those two figures . The largest weakness I perceive concerns the clarity and accessibility of the writing : for example , the connection drawn in Section 4 to the work on norms in over-parameterized neural nets is very interesting , but I 'm not sure the text fully succeeds in further connecting it to OOD settings . In particular , certain details of the ongoing discussion of majority and minority groups are n't entirely clear ( to me , at least ) ... are minority group samples available during training , just in smaller number ? In that case , what is the OOD `` shift '' -- the prevalence of the minority group at test time ? Likewise , I 'm not sure I really connected with the takeaway in Section 5 -- is it that early in the optimization , the `` spurious '' weights get updated repeatedly by an amount proportional to the spurious correlation , and that it then takes a long time to undo these updates , if at all ? The statistical skew section is definitely more abstract and perhaps a little harder to connect to practical settings , vs. geometric skew where the bridge is the previous work on `` norms . '' My recommendation is to accept this submission , and at the moment , I am willing to advocate for it . However , it is entirely possible I am missing ( or misunderstanding ) key details , and so I am eager to discuss with the other reviewers .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "First , we want to address your final two questions . * * Geometric skews * * : We apologize for the clarity issues in the discussion of the majority/minority groups . We have rewritten this paragraph , but nevertheless clarify this here too . The majority and minority groups is a partition of the training data defined for the purpose of understanding how the max-margin classifier works . The max-margin classifier as such does not know which is which ! How does the \u201c increasing norms \u201d observation explain the spurious-feature-reliance ? There are four logical steps in this argument : 1 . Note the ( trivial ) fact the number of points of the minority group $ S_ { \\text { minor } } $ ( defined by points for which $ x_ { \\text { sp } } \\cdot y < 0 $ ) is ( much ) less than the size of the whole dataset $ S $ . 2.The \u201c increasing norms \u201d observation is that \u201c if we train a max-margin classifier to fit the data using only the invariant features , its norm grows with number of training points. \u201d 3 . The \u201c increasing norms \u201d observation together with step 1 tells us that if we were to use only the invariant feature , fitting just $ S_ { \\text { minor } } $ is ( much ) cheaper in $ \\ell_2 $ norm than fitting all of $ S $ . This gap in costs is what we call a geometric skew . 4.Hence , the max-margin classifier would rather just set $ w_ { \\text { sp } } > 0 $ to classify $ S_ { \\text { major } } $ and use the invariant feature to focus on classifying the remaining set , $ S_ { \\text { minor } } $ . This would be cheaper than setting $ w_ { \\text { sp } } = 0 $ and using the invariant feature to classify all of $ S $ . Thus , the max-margin uses the spurious feature because of the geometric skew . * * Statistical skews : * * What you \u2019 ve stated is precisely the takeaway i.e. , gradient descent initially updates the spurious feature proportional to the spurious correlation , but it takes a long time to \u201c correct \u201d this back to a zero . We want to make two additional points : - We have updated Theorem 2 to apply to a more general easy-to-learn setting that doesn \u2019 t just apply to a 2-dimensional task . We hope this helps in convincing you that the above intuition is indeed more generally true . - The experiments in Sec 5 are intended to bridge these insights to practice . We \u2019 re able to show that a NN trained on an MNIST/CIFAR10-based dataset with statistical skews ( but no geometric skews ) is vulnerable to shifts in the spurious feature . This shows that the NN has not converged to a \u201c max-margin-like \u201d state that doesn \u2019 t use the spurious feature . * * Regarding your ( accurate ) understanding : * * We believe that your understanding of our results , and also the background context is on point . Indeed , you make a good point that most existing algorithms in OoD in deep learning are often based on heuristics or on rough intuition . With regards to your point about related research , we think it \u2019 s useful to add that there has been very little theory understanding the exact factors behind why existing algorithms don \u2019 t work for OoD generalization . The ones we \u2019 ve cited like Sagawa et al. , are very recent . Most other work in this space has been empirical . Your understanding regarding the motivation behind trying to study easy-to-learn tasks is accurate . This motivation is a very nuanced idea , and we \u2019 re pleased that you appreciated it . Indeed , as you say , we want to \u201c switch off \u201d confounding factors to tease out the most fundamental factors of failure . Thank you for finding our discussion elegant . We think that what is particularly appealing about the result is in how these two skews are \u201c orthogonal \u201d effects . We agree with you that future work -- both theory and practice -- can hopefully build on these ideas . We \u2019 re also glad that you found the figures useful and the experiments well-justified . You \u2019 re right in that we made sure that Fig 2 conveyed the idea of geometric skews and Fig 3 the idea of statistical skews ."}, "1": {"review_id": "fSTD6NFIW_b-1", "review_text": "The paper studies generalization under distribution shift , and tries to answer the question : why do ERM-based classifiers learn to rely on `` spurious '' features ? They present a class of distributions called `` easy-to-learn '' that rules out several explanations given in recent work and isolates the spurious correlation phenomenon in the simplest possible setting . Even on `` easy-to-learn '' distributions , linear models obtained from ERM use spurious features owing to either the dynamics of gradient descent trained on separable data ( very slow convergence to the max-margin classifier ) or a certain geometric skew in the data . Pros : - The paper address a question of great interest to the ICLR community : generalization under distribution shifts . - The `` easy-to-learn '' set of instances seems like a reasonable test-case to understand robustness to distribution shifts before moving to more complex setting . - The fact that spurious feature use can arise on the `` easy-to-learn '' instances suggests that previous explanations are underpowered in the sense that they 're ruled out by this construction , but the pattern still arises . - The `` statistical '' skew shows how the dynamics of gradient descent can introduce brittleness to distribution shift , albeit in a limited setting . Cons : - This paper is written is a way that 's confusing and frequently difficult to follow . Rather than focus on explicating the core phenomenon well and supporting it with clear theorems and well-justified experiments , the authors at times take a shotgun approach and mention numerous constructions ( often in-line ) , references to empirical results , descriptions of experiments , that seem to obfuscate rather than clarify the core phenomena they 're presenting . For instance , the `` geometric '' skew in Section 4 is never precisely explained . - In a similar vein , the paper does not justify the experiments they run in light of the phenomena they seek to `` intuitively understand . '' For instance , in Section 4/5 , the paper presents a litany of experiments , without explaining how they fit into the `` easy-to-learn '' framework or relate to a phenomena described by the theorems . It 's possible there 's a connection , but it 's not evident to this reader . - The experiments focus on a variety of synthetic distribution shifts . While this may be useful in understanding the constructions in the paper , it 's not evident to what extent these 'skews ' and the 'spurious/invariant ' distinction given here can explain the lack of robustness observed in practice on `` real '' or `` natural '' distribution shifts . Update after rebuttal : Thank you to the authors for their detailed response . The clarification and updates in the geometric skews section and the more explicit justification and connections between the framework/theoretical results and the experiments improved readability and clarity . I also appreciate putting greater weight on the theoretical contributions and `` easy-to-learn '' task definitions , which provide a simple but non-trivial test case for robustness research . I 'm increasing my score accordingly .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Justification for experiments : * * We are sorry to hear that you found the presentation of the experiments confusing . We have tried to improve this in the updated version . However , it seems like there \u2019 s disagreement among the reviews e.g. , - AnonReviewer4 : \u201c experiments are carefully designed based on the constraints\u2026 well in support of their theoretical claims. \u201d ; - AnonReviewer3 : \u201c The experiments were thoughtful and well-designed\u2026 results are presented effectively \u201d - AnonReviewer1 : \u201c a fairly rigorous empirical analysis of geometrical and statistical skews on CIFAR and MNIST \u201d , `` paper is fairly well written overall '' As such , these experiments have a precise role : they provide concrete motivation behind the abstract theoretical ideas that follow them . Here \u2019 s the justification for each inline experiment : - * * The MNIST + random ReLU features experiment in Sec 3 * * : This identifies a concrete linear task where max-margin fails even though the task is easy . Identifying this really easy task helps us in justifying our subsequent definition of the easy-to-learn tasks via the constraints . These constraints are abstract , and the reader may not appreciate why we make them without that concrete experiment beforehand . - * * The \u201c increasing norm \u201d experiments in Sec 4 * * : This empirically validates a key property of real-world invariant features that we subsequently use to explain why max-margin fails . Without demonstrating this observation upfront in Sec 4 , the reader wouldn \u2019 t be comfortable digesting the insights from Thm1 which would all be based on an unconfirmed hypothesis . - * * The GD experiment on the 2D dataset in Sec 5 * * : This empirically identifies a new failure mode that can not be explained by the previous story in Sec 4 . This then leads to the theoretical question of \u201c how can you explain this empirical observation ? \u201d and thus , the rest of that section . * * Relation between NN experiments and easy-to-learn tasks * * : We apologize for not making this connection clear enough . We clarify this below , and in more detail in the revised submission . - In Sec 4 , we present four non-linear classification tasks trained by a neural network . The first two are easy-to-learn . The last two tasks are NOT easy-to-learn -- these crucially break a constraint , which is why failure happens . But we can still argue how such a failure can be explained via a geometric argument like Thm 1 in all these tasks . The purpose of the brief main paper discussion is to only let the reader know that the geometric insights are not specific to the linear theoretical examples . We defer the exact connection between these tasks and Thm 1 to the App C. - In Sec 5 , we present two non-linear tasks trained by an NN . Both of these are easy-to-learn tasks . In both , we create a dataset with no geometric skew , but with some statistical skew . By showing that the resulting NN classifier uses the spurious feature , we demonstrate our hypothesis that failure can happen even without geometric skews , as long as the statistical skew exists . * * Synthetic spurious features * * : You \u2019 re right : empirically demonstrating these insights on datasets with realistic spurious feature shifts would be very illuminating ! To address this : - In App E , we have added experiments demonstrating the effect of these skews in an obesity estimation task . Here , instead of introducing an artificial spurious feature , we sample the data in a biased way so as to induce a spurious correlation between an existing feature and the label . - Having said that , we clarify why we think * * this is beyond the scope of our paper . * * Since we theoretically study OoD -- rather than show non-theoretical numerical improvements of a proposed algorithm -- we need datasets where it is easy to quantify and intervene on/modify the spurious feature ( e.g. , we need this to show how norms increase with training set size when we use only the invariant feature ) . * * However , in none of the existing realistic OoD benchmarks ( PACS , VLCS etc. , ) is there a way to quantify the spurious features * * . Indeed , in prior work like IRM , RiskEx etc. , all \u201c theory-verifying \u201d experiments are limited to the synthetic image dataset , Colored MNIST . In fact , we \u2019 ve even gone beyond this dataset to propose a variety of unique quantifiable , synthetic datasets . We \u2019 ve also added a cats vs. dogs datasets with a color shift in the revised version . The lack of a realistic + quantifiable dataset is an important gap , beyond the scope of this paper , that is hopefully addressed by the OoD community . * * A broad note * * : We \u2019 d also like to clarify that the experimental part of the paper forms less than quarter of our contributions . Most of our contribution lies in providing a theoretical understanding of OoD failure covered by the points you \u2019 ve mentioned in your positives . We hope that with these clarifications and our steps to address your concerns regarding the experiments and experiment discussion help you reconsider the evaluation of this paper ! Thank you !"}, "2": {"review_id": "fSTD6NFIW_b-2", "review_text": "This paper investigates the reasons why machine learning models usually fail to generalize out-of-distribution even in easy-to-learn tasks where one would expect these models to succeed . The authors propose two kinds of skews in the data : geometric and statistical , to explain this behavior and theoretically demonstrate it in the linear and easy-to-learn settings . Pros : + The problem studied in this paper has been one of the most important in the community and this work has a meaningful attempt on the theoretical side . + The theoretical results obtained by analyzing the simplest tasks are insightful and could be seemingly drawn on in more general settings . + The experiments are carefully designed based on the constraints , which is well in support of their theoretical claims . Major concerns : I agree with the authors on most of their explanations on the failure modes of OOD generalization from the geometric and statistical perspectives in easy-to-learn tasks . However , the authors claim that these two skews are \u201c not just a sufficient but also a necessary factor for failure of these models in easy-to-learn tasks \u201c , which I think might be overclaimed , even in the easy-to-learn tasks defined in the paper . I do believe that both geometric and statistical skews play a role in the failure of OOD generalization , but they might not be the only reasons . In fact , even if there exist no spurious features , machine learning models might fail to generalize out-of-distribution as well . Here is the example . Let us consider a comparison task , where we aim to learn a linear classifier to predict which one of any given two values { x_i , x_j } is larger , i.e. , it outputs +1 if x_i > = x_j , and otherwise it outputs -1 . For simplicity , let us further assume that -1 = < x_i , x_j = < +1 and x_i^2 + x_j^2 =1 . In this case , x_inv = ( x_i , x_j ) and there is no x_sp . It is also easy to check that this task satisfies the five constraints stated in the paper . Given the training dataset in which all data points ( x_i , x_j ) are satisfying that 0 = < x_i , x_j = < +1 and x_i^2 + x_j^2 =1 , it is easy to learn a linear classifier that can be well generalized to the whole domain [ -1 , +1 ] . If we represent ( x_i , x_j ) in the polar coordinate system , then ( x_i , x_j ) is converted to ( 1 , \\theta ) . In this case , we have x_inv = \\theta and no x_sp , which also satisfies the five constraints . Although we still can learn a perfect linear classifier in the training data , it is impossible to generalize to the whole [ -\\pi , +\\pi ] corresponding to [ -1 , +1 ] . The reason is that this task is no longer linearly separable . From the example above , we can see that even without spurious features , some simple transformations only on invariant features would render OOD generalization impossible . This would be another failure mode beyond the geometric and statistical skews . Minor concerns : - The constraint on fully predictive invariant features seemingly simplifies the OOD generalization problem too much , making the theoretical results hard to apply to more general settings . Actually this constraint circumvents two key questions which play a pivotal role in the OOD generalization . One is how to justify whether or not features are fully predictive and invariant across all the domains or environments . This involves the generalization assumption , like Assumption 8 of Arjovsky et al . ( 2019 ) .The other is how to learn the partially predictive invariant features which , albeit not fully predictive , remains invariant across all the domains . If we want to solve the OOD generalization problem in practice , these two questions have to be answered . - The example given in Constraint 3.2 is not suitable . The distribution of invariant features must be identical across all domains , otherwise these features should not be treated as \u201c invariant \u201d . The exact distribution of the shapes of cows and camels vary across domains because they are not the true invariant features . - Constraint 3.5 is too strong as well . It circumvents another key question in the OOD generalization : how to identify x_inv and x_sp . If we can identify them , the OOD generalization would be reduced to a simple ERM problem . Other comments : - The last 6th line before section 3.1 says that Pr_ { D_test } [ x_sp \\cdot y > 0 ] = 0.0 . I believe this is a typo , right ? - Overall I feel too much content is placed in the appendix and it affects the fluency of reading a bit . I suggest that the authors re-organize the content and put back some stuff to the main text .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Major concern : * * We understand that your point is that there is a counter-example to our claim that \u201c geometric/statistical skews are necessary for failure in easy-to-learn tasks \u201d . However , we believe that this is not a counter-example as it is not an easy-to learn task : it breaks Constraint 2 i.e. , the invariant feature distribution must be the same in training and testing . This is not obeyed in your example since test-time points have new invariant feature values , thus making your task particularly harder than the easy-to-learn tasks ! Also , it \u2019 s worth making a minor clarification : our setting considers the realizable case where the optimal hypothesis $ h^ * $ in the learner \u2019 s hypothesis class $ H $ ( defined under the \u201c the domain generalization setting and ERM \u201d paragraph ) achieves zero error ( captured by Constraint 1 ) . In your example , the optimal hypothesis from the ERM learner \u2019 s linear hypothesis can not achieve zero error thus violating Constraint 1 too . So your task is hard-to-learn in the sense that it is in the non-realizable setting and violates Constraint 1 . * Nevertheless , the main issue with the example is that it breaks Constraint 2 . * Finally , we \u2019 d like pre-emptively clarify that our \u201c necessary and sufficient \u201d claim is intended only for ( a ) easy-to-learn tasks and ( b ) for linear , GD/max-margin . This claim is a consequence of the upper and lower bounds in Thm 1 and 2 . We had made sure to phrase this carefully in Contribution 3 . Having said all this , we want to thank you again for engaging with the material deeply . * * Minor Concern 1 : * * Your concern suggests that the set of easy-to-learn tasks doesn \u2019 t contain complicated real-world tasks where the invariant feature isn \u2019 t fully predictive . While we completely agree with this fact ( and we \u2019 ve admitted this ourselves in the conclusion ) , we must remind ourselves of a fundamental nuance here ( one which you \u2019 ve noted yourself in your paper summary ! ) . The nuance is that * * our goal here is not to prove the success of a new OoD algorithm * * -- had it been that , your concern regarding the fully predictive invariant feature is valid ! OTOH , we want to show that ERM fails . * * It \u2019 s more powerful ( and most challenging ) to show that an algorithm fails to work in a task that is easy where the invariant features are fully predictive , than one that is hard * * . Hence this is not a problematic aspect but in contrast , a highlight of our result ! Further , we think it \u2019 s natural that the insights and failure modes we elucidate here carry over to more complicated tasks where the invariant feature may not be fully predictive . For a concrete example : GD would still be slow in its convergence on more complicated tasks ( where invariant feature isn \u2019 t fully predictive ) , and hence still suffer from statistical skews ( as it \u2019 ll absorb the spurious correlation early on , and take too long to get rid of it ) . Additionally , we think our experiments on neural networks + CIFAR-like datasets are concrete evidence for the generality of our insights . * * Minor Concern 2 * * : Here \u2019 s why the example * is * suitable . Consider an example with two types of cow faces ( A & B ) and two types of camel faces ( C & D ) . In the first domain all four types are equally probable , and in the second domain , A and C are most probable . This would break Constraint 2 since the shape feature varies in distribution across domains . Yet , the \u201c shape feature \u2192 label \u201d mapping remains the same in both domains , and so the shape feature can be considered an invariant feature that fully predicts the true label . * * Minor Concern 3 : * * To clarify , this constraint did not mean to imply that the learner knows which feature is spurious and which feature is invariant . More precisely , both our Theorems hold in the case where the original feature space is orthogonally and arbitrarily rotated ( and the orthogonal rotation matrix is not known to the learner ) -- this holds because we consider GD/max-margin which are equivariant to such transformations . We wrote the constraint in this way just for ease of notation but we understand the confusion and apologize for it . We have clarified this in our updated paper . * * Your other comments : * * - We \u2019 re not sure we see a typo here could you clarify what you think it should have been ? During test time , we want $ x_ { \\text { sp } } \\cdot y < 0 $ on all datapoints in contrast to training time where $ x_ { \\text { sp } } \\cdot y > 0 $ on most datapoints . - In other words , we want to \u201c flip \u201d the correlation completely to test the robustness of the learned classifier . Regarding appendix references , we \u2019 ve tried to reorganize/add some content especially in the discussion of geometric skews , and in the empirical examples in Sec 4 . We \u2019 ll continue working on the re-organization . Thanks for the useful feedback . To summarize , we are hopeful that we have addressed your major concern and provided the required clarifications for your minor concerns too . If that is the case , we appreciate it if you consider increasing your score ."}, "3": {"review_id": "fSTD6NFIW_b-3", "review_text": "Overview : The authors study the out of distribution generalisation problem in detail wherein a model may incorrectly use spurious correlations in the data to make predictions on data at test time . This is a fundamental problem and very crucial/difficult to tackle across various domains Quality and Clarity : The paper is fairly well written overall . Originality and Significance : OOD detection is a well-studied problem that has proven difficult to date ; it 's highly relevant across applications in high-stake domains . The approach taken in this paper is to a ) study tasks that are easy to succeed on , b ) show that OOD failure occurs even in these easy settings . The authors subsequently show that geometric and statistical skews are necessary for failure . As such the approach has limited novelty , since it does not offer a concrete solution to the OOD detection problem but rather examines necessary conditions for OOD failure . Pros : 1 ) The authors study a highly relevant problem of OOD failures 2 ) The authors present a fairly rigorous empirical analysis of geometrical and statistical skews on CIFAR and MNIST Cons : 1 ) The paper is limited in novelty ; while the authors state find that OOD failures result from statistical and geometric skews , they do not provide an adequate solution to overcoming these skews 2 ) OOD failures are only studied in the context of small theoretical examples or image tasks such as MNIST or CIFAR . However , OOD failures are widespread across various domains and become especially important to study in high risk settings such as in a clinical context etc . I would have liked to have seen some exposition of these problems in a real high-risk setting .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Finally , we think it would be helpful to place our paper in the context of current trends in ML theory . In particular , there is a large swathe of theoretical papers these days that are solely dedicated towards understanding a particular empirically observed phenomenon ( such as the success/failure of an existing deep learning algorithm ) -- * * these papers do not provide a new solution . * * We present a few examples at the end of this comment . Such papers ( including ours , we hope ) are valuable in themselves as they make novel points , provide stronger theoretical foundations , add rigor to the field , generate significant discussion and introspection on empirical progress . We feel that penalizing such work for not immediately solving the studied phenomenon would cut short a promising line of future work . We hope that keeping this in mind provides a broader perspective while approaching the question of what makes our contributions a standalone paper . Examples of works that solely understand an existing algorithm \u2019 s working/failure * * without proposing any new algorithms : * * - An investigation of why overparameterization exacerbates spurious correlations , ICML 2020 https : //arxiv.org/abs/2005.04345 ( a study of why overparameterized max-margin makes the effect of spurious correlations worse -- we cite this ) - Pitfalls of simplicity bias in neural networks , NeurIPS 2020 https : //arxiv.org/abs/2006.07710 ( a study of how standard NN training ends up learning to rely on features that separate the data only by a small margin just because they are simple to learn ) - Robustness may be at odds with accuracy , ICLR 2019 https : //arxiv.org/abs/1805.12152 ( a study of why robust training could hurt standard accuracy ) - Adversarially Robust Generalization Requires More Data , NeurIPS 2018 https : //arxiv.org/abs/1804.11285 ( a study explaining why robust training fails to generalize to unseen data ) - Identifying and understanding empirical phenomena , ICML 2019 workshop http : //deep-phenomena.org/ ( a workshop dedicated to works like this ) - The risks of invariant risk minimization , https : //arxiv.org/abs/2010.05761 ( very recent work studying the pitfalls of IRM )"}}