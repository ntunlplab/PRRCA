{"year": "2021", "forum": "QYjO70ACDK", "title": "Distributional Sliced-Wasserstein and Applications to Generative Modeling", "decision": "Accept (Spotlight)", "meta_review": "In this paper, the authors propose a new max-sliced Wasserstein distance. Specifically, the proposed method is a multiple sliced variants of the existing max-sliced Wasserstein distance. Compared to the subspace Robust Wasserstein distance, the proposed method can be efficiently computed.\n\nOverall, the proposed method is a good extension of the max-sliced Wasserstein and can be used in various applications. All authors agree to accept the paper, so, I also vote for acceptance.", "reviews": [{"review_id": "QYjO70ACDK-0", "review_text": "The paper presents a novel variant of the Sliced Wasserstein ( SW ) distance . Wasserstein distances have been used recently in lot of machine learning problems . One of the major problem is that , in its primal form , it is computationally expensive . In order to alleviate this problem , a class of methods , called sliced , leverage on the fact that Wasserstein has a closed form expression in 1D ( which amount to sort the samples ) . It replaces the original Wasserstein distance by an expectation of 1D sub-problems over directions drawn uniformly on the unit hypersphere ( akin to a Radon transform ) . Observing that not all the directions are meaningful , the authors propose a variant of SW where the expectation over all the directions is replaced by an expectation over a distribution of directions . The \u2018 extent \u2019 of this distribution is controlled by an extra parameter . Interestingly , the authors show that this formulation is computationally tractable if one parametrizes this distribution by a measurable function , expressed as a neural network . This result is obtained by deriving the Lagrangian dual of the original problem . Comparisons with previous works are then given in two GAN scenarii : one on MNIST to explore the importance of the different parameters , and another on larger and more complicated datasets , where the FID score is exposed . The paper is well written , very clear and easy to follow . Related works are correctly cited . There is one missing work that should be cited though : Meng , Cheng , et al . `` Large-scale optimal transport map estimation using projection pursuit . '' Advances in Neural Information Processing Systems . 2019.Where authors choose the most relevant directions in a way similar to projection pursuit . I really believe this paper could also lead to interesting comparisons . All in all , the presented method is a variant of SW that builds on several previous works exploring a similar idea ( how to better sample the directions ) , such as max-SW or subspace robust Wasserstein distances . As such , it can be considered relatively incremental , but this should not totally prevent publications if the computational benefits/performances are very good . Regarding this point , I have some questions about the experimental section : - it seems that Figures 2.a ( W_2 score ) and 3.a ( FID ) are obtained at convergence . From appendix F , it seems that the number of training epochs is fixed for all methods . I wonder if this setting is fair for comparing max-SW , as far as only one direction is contained ( the max direction ) , the gradient might gather less information . I suspect that more iterations might be needed , why not going until the full convergence of the model ? - when comparing SW and DSW , are the directions drawn randomly for every batches ? ( I guess this the common practice when using SW ) . As such , I do not understand what is the meaning of fixing the number of directions in advance . - finally it seems that DSW is computed on mini-batches of samples . While I acknowledge there is a common practice to do so , I think computing a 1D Wasserstein on a mini batch is not the same as computing the 1D W on the full dataset . As such , this mini batch version of SW is not the same as computing the true SW . In the end , i ) the size of the mini-batch might have an impact on the estimation quality , that should be discussed ii ) if computing 1D W on mini batches , why not computing and comparing with the mini batch version of the original version of W ? This has been done in several papers and has shown to give good results ( See the recent study on this theme Fatras , Kilian , et al . `` Learning with minibatch Wasserstein : asymptotic and gradient properties . '' the 23nd International Conference on Artificial Intelligence and Statistics . Vol.108.2020 . ) , plus it does not have the limitation/artefacts of the sliced Wasserstein . Finally , I would have liked to see other types of experiments than a GAN to really assess the power of this new methodology ( SW has been used for classification , domain adaptation , computing gradient flows , etc . ) . Why limiting the applications to generative modeling ? If generative modeling is the target , then I would expect to see comparisons with other types of metrics than SW ( wether it be dual version of W_1 as in WGAN , MMD , or any other divergence from the model zoo ) . For all these reasons my recommendation will only be \u2018 above acceptance threshold \u2019 . # # after author response I thank the authors for providing sensible answers to my questions , and analysis of their method compared to the minibatch version . I am still not sure about the fact that the rate of convergence of SW is indeed indepedent of the dimension , as the dimension is indeed hiddent in the scalar product . Nevertheless , I believe this question is out of the scope of the paper , and I changed my final rating accordingly to the new version of the manuscript .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time . We have revised our papers based on your comments . The changes are marked in blue color . Question 1 : \u201c Comparing with Meng , Cheng , et al . `` Large-scale optimal transport map estimation using projection pursuit . '' Advances in Neural Information Processing Systems . 2019. \u201d Answer : Thank you for your reference . Based on your suggestion , we have implemented two variants of SW , which are based on projection pursuit methods ( e.g. , directional regression ( DR ) and sliced average variance estimator ( SAVE ) ) to find the most \u201c informative \u201d projecting directions . We name these variants as drSW and saveSW . In deep generative modeling tasks , with the minibatch setting , DR and SAVE can not be applied because they require to inverse the empirical covariance matrix , which is normally low rank in the high dimensional setting . Therefore , we only apply drSW and saveSW to the color transfer tasks . The experiment results are in Figures 16 and 17 in Appendix F. Given these results , we observe that both drSW and saveSW perform worse than max-SW , SW , and DSW . Furthermore , DSW produces the most lively and realistic transferred images among these baselines . Question 2 : \u201c It seems that Figures 2.a ( W_2 score ) and 3.a ( FID ) are obtained at convergence . From appendix F , it seems that the number of training epochs is fixed for all methods . I wonder if this setting is fair for comparing max-SW , as far as only one direction is contained ( the max direction ) , the gradient might gather less information . I suspect that more iterations might be needed , why not going until the full convergence of the model ? \u201d Answer : Thank you for your comment . In the revised version , we have increased the number of epochs for Max-SW to 800 and run the generative model based on Max-SW on MNIST . The experiment results are in Figure 4 ( c ) in Appendix E.1 . Based on the results , Max-SW \u2019 s Wasserstein-2 score increases considerably with more iterations ; however , Max-SW \u2019 s result is still worse than that of DSW with a much smaller number of iterations . This result suggests that Max-SW might need several more iterations than DSW to obtain a comparable quality generative model . Question 3 : \u201c When comparing SW and DSW , are the directions drawn randomly for every batches ? ( I guess this the common practice when using SW ) . As such , I do not understand what is the meaning of fixing the number of directions in advance. \u201d Answer : Thank you for your question . We would like to clarify here that the directions are drawn randomly for every batch . By fixing the number of projections , it means that we do not change the number of drawn directions in each minibatch . Question 4 : \u201c The size of the mini-batch might have an impact on the estimation quality , that should be discussed. \u201d Answer : Thank you for your suggestion . We have already done this in Figure 5 ( b ) in the Appendix . We observed that having a bigger size of minibatch in the case of DSW can help the model distribution to converge faster to the data distribution in the sense of Wasserstein-2 distance . However , the differences are not significant when changing the minibatch size in the set { 128,256,512,1024,2048 } ."}, {"review_id": "QYjO70ACDK-1", "review_text": "Summary : The paper describes a family of sliced Wasserstein divergences that maximize the distribution over slices subject to constraints on the concentration of slices . Extremes of the family are the sliced Wasserstein and max-sliced Wasserstein distance . In between these the divergence is sensitive to informative discrepancies in multiple subspaces , while still leveraging the relatively fast computation the Wasserstein distance in one dimension for empirical distributions . A dual formulation provides a variational approximation using a ( possibly deep ) neural network to instantiate the slicing distribution through a pushforward approach . Basic theory prove the divergence is a distance metric between measures . Extensive experiments show improvement over sliced and max-sliced Wasserstein distances and related projection based approaches . Strengths : Overall the paper is a clear and original contribution to the field of sliced Wasserstein distances . The paper 's appendix shows multiple applications of the methodology where it outperforms existing divergences . Weaknesses : The specific parameterization of the mapping $ f $ is not clear ( it is not given in the main body and it is poorly described in Appendix F as a `` single multi layer perceptron ( MLP ) layer with normalized output '' ) . In the main body it is called a deep neural network . Also I am a bit wary of interpreting the wall-clock timing differences due to differences in software implementation and hyper-parameter choices for optimization and number of gradient steps ( Figure 2 ( a , c ) and Figure 3 ) . Learning curves are more fair , but even these use the best value of $ \\lambda_C $ versus default settings for other methods . As the paper recognizes the max sliced generalized Wasserstein with a deep neural network is the closest competitor and also requires solving a max-min saddle point optimization . It is shown in the appendix as Figure 6 ( a\u2013e ) ( green trace ) . Even in this case the differences in learning curve and timing could possibly be due to software implementation and hyper-parameter selection in the optimization . Overall this is a clear accept as it a meaningful improvement to sliced Wasserstein approaches . There a few points that need to be clarified , and I would argue for less emphasis on the wall-clock timing of methods . How many parameters are used in the neural network defining $ f_\\phi $ ? Would an empirical distribution optimized over a corresponding number of fixed slices ( originally randomly drawn ) perform as well ? Minor comments ( main body ) : P.1 use Author ( year ) instead of Author ( Author year ) . P.1 The case of empirical sample has not been described so what is meant by the term `` atoms '' to describe $ k $ is not apparent to the reader . Then on page 6 `` $ k $ is the number of support points '' . The terminology could be more consistent . P.2 Double parentheses `` ) ) '' after Deshpande et al. , 2019 . References . Capitalization of conference names and book titles is not consistent . Sinkhorn is a proper noun and should be capitalized . Minor comments ( appendix ) : The appendix does n't appear to be carefully proofread . Sometimes the notation uses $ . $ instead of $ \\cdot $ for the arguments of multivariate functions , this is harder to read as in $ \\mathrm { DSW } _p ( . , . ; C ) $ versus $ \\mathrm { DSW } _p ( \\cdot , \\cdot ; C ) $ There are a number of missing definite articles for example `` that length of side of right triangle '' - > `` that the length of the side of the right triangle '' . Throughout , `` close-form '' - > `` closed-form '' . P.18 `` subspcaae '' and full stop `` . '' beginning new line . P.19 `` as the al time increases considerably '' P.19 `` DSW and DGSW are slower than Max-SW , Max-GSW ( 50 gradient updates to find the max direction ) , and Max-GSW-NN ( 50 update times for the defining neural net function ) . '' Is there any meaning to the subtle variation between `` gradient updates '' and `` update times '' . P.23 `` single multi layer perceptron ( MLP ) layer '' ? This is not clear . P.23 `` as the f function in the dual empirical forms of DSW and DGSW for the dual empirical forms of these distances ) '' Redundant wording and extra parentheses .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time . We have revised our papers based on your comments . The changes are marked in blue color . Question 1 : \u201c The specific parameterization of the mapping f is not clear ( it is not given in the main body and it is poorly described in Appendix F as a `` single multilayer perceptron ( MLP ) layer with normalized output '' ) . In the main body , it is called a deep neural network. \u201d Answer : Thank you for your comment . We apologize for this confusing statement about the mapping $ f $ . In fact , we want to indicate that it is quite flexible to apply various architectures to parametrize the function $ f $ . In practice , using a powerful neural net may require harder computation while we prefer the fast computational speed of the distance . Therefore , for the sake of computation , we parametrize $ f $ as a MLP network with just one layer with $ d^2 $ parameters where d is the dimension of comparing distributions ( with the input is normalized to provide vectors from the unit sphere ) . We believe that this network is expressive enough in the general case of comparing arbitrary distributions . Finally , we would like to remark that if we have more structural information about two target probability measures , special architectures of deep learning might be helpful by introducing some inductive biases . Question 2 : \u201c Wall-clock timing differences due to differences in software implementation ... I would argue for less emphasis on the wall-clock timing of methods \u201d . Answer : Thank you for your comment . We agree that the hyperparameter settings and implementations may affect the performance of the reported methods . In the revised version , at the beginning of Section 4 , we have included an additional remark that the wall-clock timing differences of different methods may be subject to the differences in the hyperparameter settings and software implementations of different methods . Question 3 : \u201c Would an empirical distribution optimized over a corresponding number of fixed slices ( originally randomly drawn ) perform as well ? \u201d Answer : We assume that you mean that the family distribution over slices could be restricted to discrete measures that have n support points with uniform weights . Then , there is no need for the mapping $ f $ anymore , i.e. , we can optimize directly the n points with the regularization . In Appendix E.1 of the revised version , we have added the experiments to compare this version of DSW , which we refer to as discrete DSW ( dDSW ) , with the general form of DSW in generative modeling tasks on MNIST . The results are reported in Figure 4 ( d ) . We observe that dDSW performs better than sliced Wasserstein ( SW ) and max-sliced Wasserstein ( Max-SW ) , namely , dDSW ( n=10 ) converges faster than SW ( n=10 ) and Max-SW in the sense of Wasserstein-2 . However , both dDSW with n=10 and n = 1000 are worse than DSW with n = 10 ( and thus with n = 1000 ) . It suggests that DSW is better than dDSW . Question 4 : \u201c Is there any meaning to the subtle variation between `` gradient updates '' and `` update times \u201d ? \u201d Answer : Thank you for your question . There is no variation between gradient updates and update times . In the revised version , we have changed \u201c update times \u201d to \u201c gradient updates \u201d for more consistency in terminology . Question 5 : \u201c Typos , inconsistent terminologies \u201d . Thank you for your comment . We have fixed all the typos and inconsistent terminologies in the revised version ."}, {"review_id": "QYjO70ACDK-2", "review_text": "This is a well written paper with some interesting results . This paper is to propose a distributional sliced-Wasserstein distance to address the limitations of standard SW and Max-SW . The proposed method finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections . Some theoretical results are presented in both the main paper and its supplementary document . This reviewer personally enjoys reading this paper . Here are a few additional comments . 1.How to select $ \\lambda_C $ in practice ? The authors need to discuss it in details .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your comment . We have mentioned how to select $ \\lambda_ { C } $ at the beginning of Section 4 on Page 6 . In particular , on the MNIST dataset , we split the test set of MNIST into a validation set and a test set with 20 % samples and 80 % samples respectively . Then , we search for $ \\lambda_ { C } \\in $ { 1 , 10 , 100 , 1000 } , which gives the best Wasserstein-2 score on the validation set and then use that value of $ \\lambda_C $ to evaluate the score on the test set . Similarly , on CelebA , CIFAR10 , and LSUN , we also split the test set with the same ratio as that in MNIST , and also search for $ \\lambda_C \\in $ { 1 , 10 , 100 , 1000 } that gives the best FID score . In the experiments , we observed that the optimal value of $ \\lambda_C $ is always either 1 or 10 . Therefore , we would suggest choosing $ \\lambda_C $ between 1 and 10 in other practical applications of DSW . Finally , given the above observation , we would like to remark that investigating theoretically the optimal choice of the regularization parameter $ \\lambda_ { C } $ such that the DSW distance can capture all the important directions that can distinguish two target probability measures well is an important and interesting direction that we will pursue in the future ."}], "0": {"review_id": "QYjO70ACDK-0", "review_text": "The paper presents a novel variant of the Sliced Wasserstein ( SW ) distance . Wasserstein distances have been used recently in lot of machine learning problems . One of the major problem is that , in its primal form , it is computationally expensive . In order to alleviate this problem , a class of methods , called sliced , leverage on the fact that Wasserstein has a closed form expression in 1D ( which amount to sort the samples ) . It replaces the original Wasserstein distance by an expectation of 1D sub-problems over directions drawn uniformly on the unit hypersphere ( akin to a Radon transform ) . Observing that not all the directions are meaningful , the authors propose a variant of SW where the expectation over all the directions is replaced by an expectation over a distribution of directions . The \u2018 extent \u2019 of this distribution is controlled by an extra parameter . Interestingly , the authors show that this formulation is computationally tractable if one parametrizes this distribution by a measurable function , expressed as a neural network . This result is obtained by deriving the Lagrangian dual of the original problem . Comparisons with previous works are then given in two GAN scenarii : one on MNIST to explore the importance of the different parameters , and another on larger and more complicated datasets , where the FID score is exposed . The paper is well written , very clear and easy to follow . Related works are correctly cited . There is one missing work that should be cited though : Meng , Cheng , et al . `` Large-scale optimal transport map estimation using projection pursuit . '' Advances in Neural Information Processing Systems . 2019.Where authors choose the most relevant directions in a way similar to projection pursuit . I really believe this paper could also lead to interesting comparisons . All in all , the presented method is a variant of SW that builds on several previous works exploring a similar idea ( how to better sample the directions ) , such as max-SW or subspace robust Wasserstein distances . As such , it can be considered relatively incremental , but this should not totally prevent publications if the computational benefits/performances are very good . Regarding this point , I have some questions about the experimental section : - it seems that Figures 2.a ( W_2 score ) and 3.a ( FID ) are obtained at convergence . From appendix F , it seems that the number of training epochs is fixed for all methods . I wonder if this setting is fair for comparing max-SW , as far as only one direction is contained ( the max direction ) , the gradient might gather less information . I suspect that more iterations might be needed , why not going until the full convergence of the model ? - when comparing SW and DSW , are the directions drawn randomly for every batches ? ( I guess this the common practice when using SW ) . As such , I do not understand what is the meaning of fixing the number of directions in advance . - finally it seems that DSW is computed on mini-batches of samples . While I acknowledge there is a common practice to do so , I think computing a 1D Wasserstein on a mini batch is not the same as computing the 1D W on the full dataset . As such , this mini batch version of SW is not the same as computing the true SW . In the end , i ) the size of the mini-batch might have an impact on the estimation quality , that should be discussed ii ) if computing 1D W on mini batches , why not computing and comparing with the mini batch version of the original version of W ? This has been done in several papers and has shown to give good results ( See the recent study on this theme Fatras , Kilian , et al . `` Learning with minibatch Wasserstein : asymptotic and gradient properties . '' the 23nd International Conference on Artificial Intelligence and Statistics . Vol.108.2020 . ) , plus it does not have the limitation/artefacts of the sliced Wasserstein . Finally , I would have liked to see other types of experiments than a GAN to really assess the power of this new methodology ( SW has been used for classification , domain adaptation , computing gradient flows , etc . ) . Why limiting the applications to generative modeling ? If generative modeling is the target , then I would expect to see comparisons with other types of metrics than SW ( wether it be dual version of W_1 as in WGAN , MMD , or any other divergence from the model zoo ) . For all these reasons my recommendation will only be \u2018 above acceptance threshold \u2019 . # # after author response I thank the authors for providing sensible answers to my questions , and analysis of their method compared to the minibatch version . I am still not sure about the fact that the rate of convergence of SW is indeed indepedent of the dimension , as the dimension is indeed hiddent in the scalar product . Nevertheless , I believe this question is out of the scope of the paper , and I changed my final rating accordingly to the new version of the manuscript .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time . We have revised our papers based on your comments . The changes are marked in blue color . Question 1 : \u201c Comparing with Meng , Cheng , et al . `` Large-scale optimal transport map estimation using projection pursuit . '' Advances in Neural Information Processing Systems . 2019. \u201d Answer : Thank you for your reference . Based on your suggestion , we have implemented two variants of SW , which are based on projection pursuit methods ( e.g. , directional regression ( DR ) and sliced average variance estimator ( SAVE ) ) to find the most \u201c informative \u201d projecting directions . We name these variants as drSW and saveSW . In deep generative modeling tasks , with the minibatch setting , DR and SAVE can not be applied because they require to inverse the empirical covariance matrix , which is normally low rank in the high dimensional setting . Therefore , we only apply drSW and saveSW to the color transfer tasks . The experiment results are in Figures 16 and 17 in Appendix F. Given these results , we observe that both drSW and saveSW perform worse than max-SW , SW , and DSW . Furthermore , DSW produces the most lively and realistic transferred images among these baselines . Question 2 : \u201c It seems that Figures 2.a ( W_2 score ) and 3.a ( FID ) are obtained at convergence . From appendix F , it seems that the number of training epochs is fixed for all methods . I wonder if this setting is fair for comparing max-SW , as far as only one direction is contained ( the max direction ) , the gradient might gather less information . I suspect that more iterations might be needed , why not going until the full convergence of the model ? \u201d Answer : Thank you for your comment . In the revised version , we have increased the number of epochs for Max-SW to 800 and run the generative model based on Max-SW on MNIST . The experiment results are in Figure 4 ( c ) in Appendix E.1 . Based on the results , Max-SW \u2019 s Wasserstein-2 score increases considerably with more iterations ; however , Max-SW \u2019 s result is still worse than that of DSW with a much smaller number of iterations . This result suggests that Max-SW might need several more iterations than DSW to obtain a comparable quality generative model . Question 3 : \u201c When comparing SW and DSW , are the directions drawn randomly for every batches ? ( I guess this the common practice when using SW ) . As such , I do not understand what is the meaning of fixing the number of directions in advance. \u201d Answer : Thank you for your question . We would like to clarify here that the directions are drawn randomly for every batch . By fixing the number of projections , it means that we do not change the number of drawn directions in each minibatch . Question 4 : \u201c The size of the mini-batch might have an impact on the estimation quality , that should be discussed. \u201d Answer : Thank you for your suggestion . We have already done this in Figure 5 ( b ) in the Appendix . We observed that having a bigger size of minibatch in the case of DSW can help the model distribution to converge faster to the data distribution in the sense of Wasserstein-2 distance . However , the differences are not significant when changing the minibatch size in the set { 128,256,512,1024,2048 } ."}, "1": {"review_id": "QYjO70ACDK-1", "review_text": "Summary : The paper describes a family of sliced Wasserstein divergences that maximize the distribution over slices subject to constraints on the concentration of slices . Extremes of the family are the sliced Wasserstein and max-sliced Wasserstein distance . In between these the divergence is sensitive to informative discrepancies in multiple subspaces , while still leveraging the relatively fast computation the Wasserstein distance in one dimension for empirical distributions . A dual formulation provides a variational approximation using a ( possibly deep ) neural network to instantiate the slicing distribution through a pushforward approach . Basic theory prove the divergence is a distance metric between measures . Extensive experiments show improvement over sliced and max-sliced Wasserstein distances and related projection based approaches . Strengths : Overall the paper is a clear and original contribution to the field of sliced Wasserstein distances . The paper 's appendix shows multiple applications of the methodology where it outperforms existing divergences . Weaknesses : The specific parameterization of the mapping $ f $ is not clear ( it is not given in the main body and it is poorly described in Appendix F as a `` single multi layer perceptron ( MLP ) layer with normalized output '' ) . In the main body it is called a deep neural network . Also I am a bit wary of interpreting the wall-clock timing differences due to differences in software implementation and hyper-parameter choices for optimization and number of gradient steps ( Figure 2 ( a , c ) and Figure 3 ) . Learning curves are more fair , but even these use the best value of $ \\lambda_C $ versus default settings for other methods . As the paper recognizes the max sliced generalized Wasserstein with a deep neural network is the closest competitor and also requires solving a max-min saddle point optimization . It is shown in the appendix as Figure 6 ( a\u2013e ) ( green trace ) . Even in this case the differences in learning curve and timing could possibly be due to software implementation and hyper-parameter selection in the optimization . Overall this is a clear accept as it a meaningful improvement to sliced Wasserstein approaches . There a few points that need to be clarified , and I would argue for less emphasis on the wall-clock timing of methods . How many parameters are used in the neural network defining $ f_\\phi $ ? Would an empirical distribution optimized over a corresponding number of fixed slices ( originally randomly drawn ) perform as well ? Minor comments ( main body ) : P.1 use Author ( year ) instead of Author ( Author year ) . P.1 The case of empirical sample has not been described so what is meant by the term `` atoms '' to describe $ k $ is not apparent to the reader . Then on page 6 `` $ k $ is the number of support points '' . The terminology could be more consistent . P.2 Double parentheses `` ) ) '' after Deshpande et al. , 2019 . References . Capitalization of conference names and book titles is not consistent . Sinkhorn is a proper noun and should be capitalized . Minor comments ( appendix ) : The appendix does n't appear to be carefully proofread . Sometimes the notation uses $ . $ instead of $ \\cdot $ for the arguments of multivariate functions , this is harder to read as in $ \\mathrm { DSW } _p ( . , . ; C ) $ versus $ \\mathrm { DSW } _p ( \\cdot , \\cdot ; C ) $ There are a number of missing definite articles for example `` that length of side of right triangle '' - > `` that the length of the side of the right triangle '' . Throughout , `` close-form '' - > `` closed-form '' . P.18 `` subspcaae '' and full stop `` . '' beginning new line . P.19 `` as the al time increases considerably '' P.19 `` DSW and DGSW are slower than Max-SW , Max-GSW ( 50 gradient updates to find the max direction ) , and Max-GSW-NN ( 50 update times for the defining neural net function ) . '' Is there any meaning to the subtle variation between `` gradient updates '' and `` update times '' . P.23 `` single multi layer perceptron ( MLP ) layer '' ? This is not clear . P.23 `` as the f function in the dual empirical forms of DSW and DGSW for the dual empirical forms of these distances ) '' Redundant wording and extra parentheses .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time . We have revised our papers based on your comments . The changes are marked in blue color . Question 1 : \u201c The specific parameterization of the mapping f is not clear ( it is not given in the main body and it is poorly described in Appendix F as a `` single multilayer perceptron ( MLP ) layer with normalized output '' ) . In the main body , it is called a deep neural network. \u201d Answer : Thank you for your comment . We apologize for this confusing statement about the mapping $ f $ . In fact , we want to indicate that it is quite flexible to apply various architectures to parametrize the function $ f $ . In practice , using a powerful neural net may require harder computation while we prefer the fast computational speed of the distance . Therefore , for the sake of computation , we parametrize $ f $ as a MLP network with just one layer with $ d^2 $ parameters where d is the dimension of comparing distributions ( with the input is normalized to provide vectors from the unit sphere ) . We believe that this network is expressive enough in the general case of comparing arbitrary distributions . Finally , we would like to remark that if we have more structural information about two target probability measures , special architectures of deep learning might be helpful by introducing some inductive biases . Question 2 : \u201c Wall-clock timing differences due to differences in software implementation ... I would argue for less emphasis on the wall-clock timing of methods \u201d . Answer : Thank you for your comment . We agree that the hyperparameter settings and implementations may affect the performance of the reported methods . In the revised version , at the beginning of Section 4 , we have included an additional remark that the wall-clock timing differences of different methods may be subject to the differences in the hyperparameter settings and software implementations of different methods . Question 3 : \u201c Would an empirical distribution optimized over a corresponding number of fixed slices ( originally randomly drawn ) perform as well ? \u201d Answer : We assume that you mean that the family distribution over slices could be restricted to discrete measures that have n support points with uniform weights . Then , there is no need for the mapping $ f $ anymore , i.e. , we can optimize directly the n points with the regularization . In Appendix E.1 of the revised version , we have added the experiments to compare this version of DSW , which we refer to as discrete DSW ( dDSW ) , with the general form of DSW in generative modeling tasks on MNIST . The results are reported in Figure 4 ( d ) . We observe that dDSW performs better than sliced Wasserstein ( SW ) and max-sliced Wasserstein ( Max-SW ) , namely , dDSW ( n=10 ) converges faster than SW ( n=10 ) and Max-SW in the sense of Wasserstein-2 . However , both dDSW with n=10 and n = 1000 are worse than DSW with n = 10 ( and thus with n = 1000 ) . It suggests that DSW is better than dDSW . Question 4 : \u201c Is there any meaning to the subtle variation between `` gradient updates '' and `` update times \u201d ? \u201d Answer : Thank you for your question . There is no variation between gradient updates and update times . In the revised version , we have changed \u201c update times \u201d to \u201c gradient updates \u201d for more consistency in terminology . Question 5 : \u201c Typos , inconsistent terminologies \u201d . Thank you for your comment . We have fixed all the typos and inconsistent terminologies in the revised version ."}, "2": {"review_id": "QYjO70ACDK-2", "review_text": "This is a well written paper with some interesting results . This paper is to propose a distributional sliced-Wasserstein distance to address the limitations of standard SW and Max-SW . The proposed method finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections . Some theoretical results are presented in both the main paper and its supplementary document . This reviewer personally enjoys reading this paper . Here are a few additional comments . 1.How to select $ \\lambda_C $ in practice ? The authors need to discuss it in details .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your comment . We have mentioned how to select $ \\lambda_ { C } $ at the beginning of Section 4 on Page 6 . In particular , on the MNIST dataset , we split the test set of MNIST into a validation set and a test set with 20 % samples and 80 % samples respectively . Then , we search for $ \\lambda_ { C } \\in $ { 1 , 10 , 100 , 1000 } , which gives the best Wasserstein-2 score on the validation set and then use that value of $ \\lambda_C $ to evaluate the score on the test set . Similarly , on CelebA , CIFAR10 , and LSUN , we also split the test set with the same ratio as that in MNIST , and also search for $ \\lambda_C \\in $ { 1 , 10 , 100 , 1000 } that gives the best FID score . In the experiments , we observed that the optimal value of $ \\lambda_C $ is always either 1 or 10 . Therefore , we would suggest choosing $ \\lambda_C $ between 1 and 10 in other practical applications of DSW . Finally , given the above observation , we would like to remark that investigating theoretically the optimal choice of the regularization parameter $ \\lambda_ { C } $ such that the DSW distance can capture all the important directions that can distinguish two target probability measures well is an important and interesting direction that we will pursue in the future ."}}