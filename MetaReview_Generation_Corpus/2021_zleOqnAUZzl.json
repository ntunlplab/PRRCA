{"year": "2021", "forum": "zleOqnAUZzl", "title": "Are all outliers alike?  On Understanding the Diversity of Outliers for Detecting OODs", "decision": "Reject", "meta_review": "**Problem Significance**  This paper introduces an interesting taxonomy of OODs and proposed an integrated approach to detect different types of OODs. The AC agrees on the importance of a fine-grained characterization of outliers given the large OOD uncertainty space. \n\n**Technical contribution** The key idea of the paper is to combine the predictions from multiple existing OOD detection methods. While the AC recognizes the effort made by the authors to address the review comments, reviewers have several major standing concerns regarding limited contributions, insufficient analysis, and lack of clarity. The AC agrees with reviewers that the paper is not ready yet for ICLR publication, and can be further strengthened by:\n\n- (R1) reporting the computational cost for the integrated approach. The inference time for approaches such as Mahalanobis is typically a few times more expensive than the MSP baseline. The cumulative time for calculating all four scores may be non-negligible. Authors are encouraged to analyze the performance tradeoff in a future revision. \n- (R2 & R3) discussing the effect of hyper-parameters tuning, which be overly complicated in practice as it involves combinations of multiple methods that each have multiple parameters to tune. \n- (R3) comparing with more recent development on OOD detection and move the new results to the main paper. The AC also thinks it's worth discussing the connection and comparison to methods on quantifying uncertainty via Bayesian probabilistic approaches.\n- (R2 & R4) more rigorous analysis of the benefits of the proposed integrated approach, both empirically and theoretically. Based on Table 7, the performance of using Mahalanobis alone is almost competitive as the proposed approach (except for the CIFAR10-CIFAR100 pair). This may deem further careful examination to understand what value other components are adding, and in what circumstance. \n- (R2, R3 & R4) More discussion on the implication of the taxonomy and algorithm in the high-dimensional space would be valuable. The 2D toy dataset might be too simple to reflect the decision boundary as well as uncertainty space learned by NNs. Moreover, it's important to justify further how aleatoric and epistemic uncertainty manifests in the current experiments using NNs. For example, epistemic uncertainty can arise due to the use of limited samples or due to the model uncertainty associated with the model regularization. \n\nRecent work by Hsu et al. [2] also attempt to define a taxonomy of OOD inputs (based on semantic shift and domain shift), which can be relevant for the authors. \n\n**Recommendation** Three knowledgeable reviewers have indicated rejection. The AC discounted R4's rating due to the less familiarity in this area and lack of participation in the post-rebuttal discussion. \n\n[1] Richard Harang, Ethan M. Rudd. Towards Principled Uncertainty Estimation for Deep Neural Networks\n[2] Hsu et al. Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data\n", "reviews": [{"review_id": "zleOqnAUZzl-0", "review_text": "This paper introduces a taxonomy of OODs and proposed an integrated approach to detect different types of OODs . Their taxonomy classifies OOD on the nature of their uncertainty and they show that no single state-of-the-art approach detects all these OOD types . Motivated by this observation , they combine multiple existing OOD detection methods to detect various types of OODs . In general , this paper is easy to understand . But I have the following concerns : 1 . Lack of discussions about some important related work . They only compare their method to ODIN and Mahalanobis methods . But there are some other OOD detection methods which also achieve state-of-the-art results , such as [ 1 ] [ 2 ] [ 3 ] . Could the authors compare their method to these methods ? 2.In their taxonomy , they consider examples that are very close to in-distribution as OOD . I am wondering whether we should treat those examples as OOD since they are too close to the in-distribution . I think previous works like ODIN and Mahalanobis all assume that OOD inputs are far away from the in-distribution . In the experimental setup , they consider STL10 as an OOD dataset for CIFAR10 . But STL10 contains CIFAR10 alike images . It is unconvincing that we should treat those images as OOD . And I think the classifier trained on CIFAR10 may have correct predictions on some of those images . Could the authors explain why we should treat those images as OOD ? 3.I am wondering whether the analysis for the simple two-dimensional dataset could be applied to high-dimensional datasets . In the high-dimensional space , their conclusion about which method detects which type of OOD may not hold . Could the authors explain it ? 4.In Appendix A.2.1 , they mention that the best results from the twelve combinations of the aforementioned sub-categories ( one from each of the four attributions ) are reported . Could the authors explain how they select the best results ? Do they use the test OOD data to select the best results ? 5.Could the author describe how they integrate the existing state-of-the-art detection methods in detail ? It is hard for me to understand what they exactly do in their proposed method . After Reading the Updated Paper - Thanks for the update . After reading the revised paper , I still have some major concerns : 1 . The current experiments performed are not enough to demonstrate the effectiveness of the proposed method . The old experiment results ( Table 6 , 7 , 8 ) are not convincing since the authors train a binary classifier as an OOD detector using a subset of the test OOD data , which is not realizable in practice . We should assume that the test OOD data are unknown during learning the OOD detector . The new experimental results where they train the binary classifier using adversarial examples generated on in-distribution data ( follow the Mahalanobis method ) in Table 1 are limited . For example , on CIFAR10 , they only report results for ResNet50 and WideResNet , but I also want to know the results for DenseNet ( Mahalanobis method [ 4 ] performs very well on CIFAR10/SVHN using DenseNet under the same setting ) . 2.Some experimental details about their method are missing . The authors mention that they train 12 binary classifiers and then select the best one on the validation dataset . But they do n't provide the details about the validation dataset , which is critical for their results . Based on their previous response , it seems they use a subset of test OOD data to select the best classifier , which is not allowed I think . Based on the current description of experimental settings , it is hard for me to evaluate the reported results . 3.The proposed approach needs a lot of hyper-parameters ( 4 attributes , 12 combinations , the weights of the binary classifier , etc ) and it is unclear how to tune these hyper-parameters and how they would affect the results . The current ablation study is limited I think . 4.This paper does n't have rigorous analysis for why integrating different attributions would improve OOD detection . I think this is an empirical paper but the experiments provided are not sufficient to demonstrate the effectiveness of the proposed method . To clarify , I did n't agree to raise the score previously . What I said was that the previous paper needed significant revision and I could not recommend acceptance . I still have some major concerns after reading the revised paper . Thus , I keep the same rating and think the paper is not ready for publication . I hope the authors could keep improving their paper . [ 1 ] Hendrycks , Dan , Mantas Mazeika , and Thomas Dietterich . `` Deep anomaly detection with outlier exposure . '' arXiv preprint arXiv:1812.04606 ( 2018 ) . [ 2 ] Liu , Weitang , et al . `` Energy-based Out-of-distribution Detection . '' arXiv preprint arXiv:2010.03759 ( 2020 ) . [ 3 ] Lakshminarayanan , Balaji , Alexander Pritzel , and Charles Blundell . `` Simple and scalable predictive uncertainty estimation using deep ensembles . '' Advances in neural information processing systems . 2017 . [ 4 ] Lee , Kimin , et al . `` A simple unified framework for detecting out-of-distribution samples and adversarial attacks . '' Advances in Neural Information Processing Systems . 2018 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "\u201c STL10 contains CIFAR10 alike images\u2026 Could the authors explain why we should treat STL10 as OODs for CIFAR10 ? \u201d STL10 dataset is inspired byCIFAR-10 but two datasets differ in terms of the image resolution ( STL10 - 96X96 and CIFAR10 - 32X32 ) . Since STL10 is similar to the CIFAR10 dataset , it makes the OOD detection more challenging while considering STL10 as OODs for the CIFAR10 . This is the reason why we selected this pair of datasets to stress test our method . \u201c I am wondering whether the analysis for the simple two-dimensional dataset could be applied to high-dimensional datasets . In the high-dimensional space , their conclusion about which method detects which type of OOD may not hold . Could the authors explain it ? \u201d We agree that it is hard to verify whether the observations in the 2D dataset apply directly to the high-dimensional datasets . However , in fig . 5 , we show the projection of the high-dimensional features in 2D using t-SNE . Please note the highlighted examples where the OOD samples associated with both epistemic and aleatoric uncertainties are missed by the Mahalanobis [ 4 ] but were detected by our approach . Also , our ablation studies ( in appendix Table.10 , 11 , 12 ) show that individual approaches ( ODIN , PCA , Mahalanobis , etc ) are not sufficient to detect all types of OODs and an integrated approach is necessary . \u201c Could the authors explain how they select the best results ? Do they use the test OOD data to select the best results ? Could the author describe how they integrate the existing state-of-the-art detection methods in detail ? \u201d A weighted sum of the four attributes ( section A.2.1 of the appendix ) forms the signature of our OOD detector . The weights of these attributes are generated in the following manner . Following the standard experimental setup [ 4 , 6 ] , we use a small subset of both in-distribution and OOD data to train a binary classifier using a logistic loss . The trained classifier ( or OOD detector ) is then evaluated on the remaining OOD samples at the True Positive Rate of 95 % . The four attributes ( forming the signature of the OOD detector ) are 1 ) distance from the in-distribution density estimate , 2 ) reconstruction error from the principal component analysis ( PCA ) , 3 ) prediction confidence of the classifier , and 4 ) conformance measure among the nearest neighbors . These attributes can be computed in different ways . In our experiments , we consider 2 ways of generating distance from the in-distribution density estimate , 1 way of generating reconstruction error from PCA , 3 ways of generating prediction confidence of the classifier , and 2 ways of generating conformance measure among the nearest neighbors , resulting in a total of 12 ways of combining these four attributes ( 2 * 1 * 3 * 2 ) ( section A.2.1 of the appendix ) . Out of these 12 combinations , we report the best empirical result on the test OOD data . \u201c Lack of discussion on related work . Could the authors compare their method to these methods ... \u201d We are looking into the suggested papers to compare our results with these papers . We will provide an update on this once we have the results . [ 4 ] Lee , Kimin , et al . `` A simple unified framework for detecting out-of-distribution samples and adversarial attacks . '' Advances in Neural Information Processing Systems . 2018 . [ 6 ] Lee , Kimin , et al . `` Training confidence-calibrated classifiers for detecting out-of-distribution samples . '' arXiv preprint arXiv:1711.09325 ( 2017 ) ."}, {"review_id": "zleOqnAUZzl-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper introduces a novel taxonomy for OOD outliers . The authors analyze current OOD detection approaches and uncover their limitations . They propose to fuse several existing approaches into a combined one and extensively evaluate it on various data sets ( CIFAR,10 , SVNH , MNIST , STL10 , ImageNet , etc . ) . The proposed integrated OOD detection approach clearly shows superior performance . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons : Overall , I vote for accepting . The authors make several key contributions : The introduce a novel OOD taxonomy , analyse current OOD detection approaches on a toy data set , propose an integrated OOD detection approach , which shows a superior performance in their extensive evaluation . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : * Introduction of a sound and helpful OOD taxonomy * Limitation analysis of state-of-the-art OOD detection algorithms * Proposal of a new integrated approach to detect different kind of OOD inputs that unifies the advanatges of underlying algorithms . * Extensive evaluation of new approach shows clearly superior performance . On a variety of data sets ( CIFAR,10 , SVNH , MNIST , STL10 , ImageNet , etc . ) the proposed approach outperforms the baselines on all evaluation criteria ( TNR , AUROC , DTACC , AUPR IN , AUPR OUT ) for various classifier neural network architectures ( LeNet , ResNet , DenseNet ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : * The demonstration of the limitations of current OOD detection algorithms is solely empirical ( based on a toy data set ) . Theoretic motivations ( if possible ) would be a great addition . * Similarly , a sound theoretical derivation for the proposed integrated approach is lacking . * Further toy data sets beyond the two half moon data set would be helpful to better understand the implications of all algorithms . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Please address and clarify the cons above", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the invaluable comments and feedback on this paper . We address the reviewer 's comments as follows . 1.The demonstration of the limitations of current OOD detection algorithms is solely empirical ( based on a toy data set ) . Theoretic motivations ( if possible ) would be a great addition . We show the limitations of OOD detection algorithms on the toy dataset for illustration purposes . However , these limitations are applicable to real datasets as well . For example , in Fig 5 , we show the examples of the OOD sample caused by epistemic and aleatoric uncertainties on the CIFAR dataset . As mentioned in subsection `` Key observations '' , these samples are missed by the Mahalanobis approach but detected by our approach . This justifies that the definition of ODDs and limitations of the OOD detection algorithms are not limited to toy datasets only . 2.Similarly , a sound theoretical derivation for the proposed integrated approach is lacking . The proposed integrated approach is motivated by two key observations : 1 ) the OOD samples can be of various types ( Fig.1 ) and 2 ) one approach can not detect all types of OODs . ( A.2.3 ABLATION STUDY ) . Thus , an integrated approach is required to detect all types of OODs . Please note the subsection `` Key observations '' justifies our claims . We agree that a theoretical derivation will further strengthen the claims and we will consider that in the follow-up works . 3.Further toy data sets beyond the two half-moon data set would be helpful to better understand the implications of all algorithms . We consider two toy datasets : half moons ( Fig 2 ) and the mixture of Gaussian ( Fig 1 ) to show the variants of OODs . As shown in Fig 2 and 3 , after considering a deep neural network to extract features , the non-linear organization of the data points in the original half-moons tends to become linear . This is due to the non-linear projection caused by the neural transformations . We expect to see the same behavior for other 2D or 3D toy datasets as long as a sufficiently deep neural network is considered to extract the features ."}, {"review_id": "zleOqnAUZzl-2", "review_text": "-- Paper Summary : The paper presents the idea of fusion of attributes from existing sota ood detection methods to achieve higher detection performance . -- Review : - The three criteria presented in section two are questions rather than criteria . It is better to be re-worded into criteria . - Figure 1 suggests the `` tied distribution of all training data '' is different than the combination of `` class distributions '' . I wish authors could explain the difference between Type 4 and 5 in the ood sample taxonomy . - The relation between five types of OOD with three criteria for OOD categorization is not clear . - The visualization in all figures could be improved : - figure 1 : too many colors . better to use different shape or numbers directly in the figure . - figure 5 : not necessary to include , hard to see and comprehend . - the total number of figures can be reduced by eliminating some and combining others . - What was the reason to choose a subset of cifar100 as ood test set but not the whole dataset ? - Authors emphasize reporting detection TNR in the manuscript while FNR is missing from the measurements . I suggest authors either report both or use threshold agnostic metrics like area under precision recall curve ( AUPR ) or area under receiver operating curve ( AUROC ) for reporting as in the Table . - I ca n't find an explanation and/or discussion on the final detection score and it 's hyperparametere . - The results from the Mahanalobis technique [ 7 ] does not match the original paper . If authors did not use a subset of ood samples for tuning , it should be reported in the paper . -- Strengths : - interesting taxonomy of ood samples and the following conclusion for integrated detection score . -- Weaknesses : - limited on contribution - no discussion on final detection score and its hyperparameters . - comparison with more recent techniques including Outlier Exposure [ 1 ] , Self-supervised reject classifier [ 2 ] , Geometric self-superivised learning [ 3,4 ] , and contrastive learning [ 5,6 ] are missing in this paper . [ 1 ] Hendrycks , D. , Mazeika , M. , & Dietterich , T. ( 2018 , September ) . Deep Anomaly Detection with Outlier Exposure . ICLR 2019 [ 2 ] Mohseni , Sina , et al . `` Self-Supervised Learning for Generalizable Out-of-Distribution Detection . '' AAAI.2020 . [ 3 ] Hendrycks , D. , Mazeika , M. , Kadavath , S. , & Song , D. ( 2019 ) . Using self-supervised learning can improve model robustness and uncertainty . In Advances in Neural Information Processing Systems ( pp.15663-15674 ) . [ 4 ] Golan , Izhak , and Ran El-Yaniv . `` Deep anomaly detection using geometric transformations . '' Advances in Neural Information Processing Systems . 2018 . [ 5 ] Tack , J. , Mo , S. , Jeong , J. , & Shin , J . ( 2020 ) .Csi : Novelty detection via contrastive learning on distributionally shifted instances . arXiv preprint arXiv:2007.08176 . [ 6 ] Winkens , J. , Bunel , R. , Roy , A. G. , Stanforth , R. , Natarajan , V. , Ledsam , J. R. , ... & Cemgil , T. ( 2020 ) . Contrastive training for improved out-of-distribution detection . arXiv preprint arXiv:2007.05566 . [ 8 ] Liu , Hao , and Pieter Abbeel . `` Hybrid discriminative-generative training via contrastive learning . '' arXiv preprint arXiv:2007.09070 ( 2020 ) . [ 7 ] Lee , K. , Lee , K. , Lee , H. , & Shin , J . ( 2018 ) .A simple unified framework for detecting out-of-distribution samples and adversarial attacks . In Advances in Neural Information Processing Systems ( pp.7167-7177 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "\u201c Figure 1 suggests the tied distribution of all training data is different than the combination of class distributions. \u201d The tied distribution shown in figure 1 is obtained using robust covariance estimation ( https : //scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html # sklearn.covariance.EllipticEnvelope ) , representing the unimodal Gaussian distribution of the complete ( irrespective of the class ) in-distribution dataset and is only meant for illustrative purposes . This is one of the many ways to model the distribution of the complete dataset . Other potential techniques for modeling the complete dataset are kernel density estimation , multi-modal gaussian distribution , etc . \u201c I wish authors could explain the difference between Type 4 and 5 in the ood sample taxonomy. \u201d While both Type 4 and Type 5 are OODs due to high class conditional epistemic uncertainty , they vary in their principal component analysis of the class distribution ( as explained in the last paragraph of section 2 in the paper ) . Type 4 are OODs due to high deviation along the principal axis of the in-distribution class 2 . Type 5 are OODs due to relatively lower deviation along the non-principal axis ( and hence , statistically invariant ) of the in-distribution class 1 . The difference in principle axis and non-principal axis will lead to a disparate effect on the reconstruction error . \u201c The relation between five types of OOD with three criteria for OOD categorization is not clear. \u201d Criteria 1 - Is the OOD associated with higher epistemic or aleatoric uncertainty , i.e. , is the input away from in-distribution data or can it be confused between multiple classes ? OOD Types - Type 1 and Type 2 are OODs due to epistemic uncertainty as they are far from the in-distribution data . Type 3 are OODs due to aleatoric uncertainty between the in-distribution classes 0 and 1 . Criteria 2 - Is the epistemic uncertainty of an OOD sample unconditional or is it conditioned on the class predicted by the DNN model ? OOD Types - Type 1 are OODs due to the epistemic uncertainty of the tied in-distribution . In other words , they are far from the tied in-distribution ( represented by the red oval in the figure ) . Type 2 are OODs due to class conditional epistemic uncertainty . In other words , if we consider class-wise instead of a single tied in-distribution then Type 2 are far from all the class distributions . But if we consider the tied distribution , then Type 2 OODs lie within the in-distribution . So , Type 2 are OODs due to class conditional epistemic uncertainty . Criteria 3 - Is the OOD an outlier due to unusually high deviation in the principal components of the data or due to small deviation in the non-principal ( and hence , statistically invariant ) components ? OOD Types - Type 4 are OODs due to high deviation along the principal axis of the in-distribution class 2 . Type 5 are OODs due to relatively lower deviation along the non-principal axis of the in-distribution class 1 . \u201c What was the reason to choose a subset of cifar100 as ood test set but not the whole dataset ? \u201d The subset of CIFAR100 considered in the experiments consists of the following four classes - sea , road , bee , and butterfly . These classes are visually similar to the ship , automobile , and bird classes in the CIFAR10 dataset respectively ( as stated in section 4 under CIFAR10 dataset ) . Therefore , it would make the task of OOD detection with this subset of CIFAR100 for CIFAR10 as in-distribution more challenging . This is the reason for choosing a subset of CIFAR100 to stress test our method on CIFAR10 as in-distribution . `` Authors emphasize reporting detection TNR in the manuscript while FNR is missing from the measurements . I suggest authors either report both or use threshold agnostic metrics like area under precision recall curve ( AUPR ) or area under receiver operating curve ( AUROC ) for reporting as in the Table . '' AUROC has been reported in Tables 1 and 2 of the experimental section 4 of the paper ( comparison with ODIN and Mahalanobis ) and Table 3 in the Appendix ( comparison with Baseline ) . AUPR has been reported for all the experiments in Tables 4 , 5 , 6 , 7 , 8 , and 9 of the Appendix . \u201c I ca n't find an explanation and/or discussion on the final detection score and it 's hyperparameters \u201d Section A.2.1 of the Appendix , \u201c Attributes forming the signature of the OOD detector used in the experiments \u201d , explains how the final detection scores are reported . \u201c Comparison with more recent techniques including Outlier Exposure , \u2026 and contrastive learning are missing in this paper. \u201d We are looking into the suggested papers to compare our results with these papers . We will provide an update on this once we have the results ."}, {"review_id": "zleOqnAUZzl-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The authors explore the different kinds of outliers and show that the methods previously proposed detect different kinds of OOD and not a single one can detect them all . The authors propose an interesting study of the different kind of outlier on synthetic data which illustrates well the different characteristics of the outlier types . The authors then propose to combine different methods to increase the OOD detection rate . Experiments are conducted on 3 images classification datasets using different deep neural networks . For each dataset , samples from other databases are introduced as outliers and must be detected . The combination method yield better detection rates than baseline methods in almost all configurations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : The main idea of the paper is simple : combine different OOD detection metrics to increase the detection rate on different types of outliers . The proposed method indeed increases the OOD detection rate for almost all the experimental settings tested by the authors . However , the method to create the OOD samples is always the same : in-distribution samples come from a database whereas out of distribution samples are drawn from another database . It would be interesting to show that the method also increases the detection rate of outliers inside a given database . This could be done by reporting the classification rate of the DNN in an abstaining scheme : if the OOD metric is greater than a threshold , the sample is not classified ( rejected ) . If the OOD detection method is useful , the classification rate of the DNN can be freely increased by increasing the threshold and rejecting more and more samples . The author do not justify their choice of the combination method . Computing all the OOD metrics can be computationaly expensive , is it necessary to compute them all ? Are this combination of metric the best ? In which conditions ? The combination method should be described in the body of the paper , not in appendix . Guo 2017 appears twice in the bibliography .", "rating": "5: Marginally below acceptance threshold", "reply_text": "\u201c show that the method also increases the detection rate of outliers inside a given database .. by reporting the classification rate of the DNN in an abstaining scheme \u201d As per the reviewer \u2019 s suggestion , we conducted new experiments to illustrate the applicability of our approach for the detection of outliers in the same dataset . We considered a ResNet34 model trained on CIFAR10 with an accuracy of 93.67 % on the test dataset . We then trained our OOD detector using a set of randomly sampled 300 misclassified images and another set of randomly sampled 300 correctly classified images from the test dataset . This trained OOD detector was then able to correctly identify 326 out of the remaining 333 samples with incorrect predictions as outliers . Thus , using our OOD detector ( trained with the True Positive Rate of 95 % ) with abstaining on outliers improved the classification accuracy to 99.91 % from 93.67 % on non-abstaining samples . Our results demonstrate that the proposed OOD method can identify and abstain on samples on which the model is likely to produce an incorrect prediction . \u201c However , the method to create the OOD samples is always the same : in-distribution samples come from a database whereas out of distribution samples are drawn from another database. \u201d As suggested , we employ a new way to generate out-of-distribution samples by modifying in-distribution samples using adversarial attacks - fast gradient sign method ( FGSM ) [ 1 ] , DeepFool [ 2 ] , basic iterative method ( BIM ) [ 3 ] . We used CIFAR10 as the in-distribution dataset and a ResNet34 model . The attacked images were generated from the test dataset of CIFAR10 . We obtained a True Negative Rate ( i.e.the rate of detection of attacked images as OODs ) of 41.19 % , 61.66 % , 93.23 % for the attacked images generated by Deepfool , FGSM and BIM attacks , respectively at a True Positive Rate of 95 % . The context of the attacks used to generate OODs - FGSM is a single-step attack that uses the L_\\infty metric for measuring the distance between a legitimate and perturbed example . BIM was introduced to improve the performance of FGSM by running a finer iterative optimizer for multiple iterations . BIM has a much higher attacking rate than FGSM and it still causes noticeable perturbations even though fewer visual flaws occur than those crafted by FGSM . The perturbations introduced by DeepFool are unnoticeable and the attacking rate is much higher than that of FGSM and BIM . \u201c Computing all the OOD metrics can be computationally expensive , is it necessary to compute them all ? Are this combination of metric the best ? In which conditions ? \u201d As explained in the Appendix ( Section A.2.1 ) of the paper , the signature of the OOD detector is the weighted sum of the four attributes used to distinguish OODs from the in-distribution samples . These attributes are 1 ) distance from the in-distribution density estimate , 2 ) reconstruction error from the principal component analysis , 3 ) prediction confidence of the classifier , and 4 ) conformance measure among the nearest neighbors . These attributes can be computed by different metrics . Some of these metrics are mentioned as categories under these attributes in A.2.1 . Only one metric per attribute is used in the OOD detector . As illustrated in Figure 4 of section 3 in the paper , these attributes tend to capture specific types ( shown as different clusters ) of OODs but not all . Our ablation studies ( Tables 10 , 11 , and 12 of the Appendix ) evaluating each attribute show the above-mentioned limitations . Thus , we proposed an integrated approach combining these attributes to detect a diverse type of OODs . We evaluated our approach on benchmark datasets considering state-of-the-art neural network models . The proposed approach achieved a better performance as shown in Tables 1 and 2 in the experiment section justifying the importance of the integrated approach . [ 1 ] Goodfellow , Ian J. , Jonathon Shlens , and Christian Szegedy . `` Explaining and harnessing adversarial examples . '' arXiv preprint arXiv:1412.6572 ( 2014 ) . [ 2 ] Moosavi-Dezfooli , Seyed-Mohsen , Alhussein Fawzi , and Pascal Frossard . `` Deepfool : a simple and accurate method to fool deep neural networks . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2016 [ 3 ] Kurakin , Alexey , Goodfellow , Ian , and Bengio , Samy . Adversarial examples in the physical world . arXiv preprint arXiv:1607.02533 , 2016 ."}], "0": {"review_id": "zleOqnAUZzl-0", "review_text": "This paper introduces a taxonomy of OODs and proposed an integrated approach to detect different types of OODs . Their taxonomy classifies OOD on the nature of their uncertainty and they show that no single state-of-the-art approach detects all these OOD types . Motivated by this observation , they combine multiple existing OOD detection methods to detect various types of OODs . In general , this paper is easy to understand . But I have the following concerns : 1 . Lack of discussions about some important related work . They only compare their method to ODIN and Mahalanobis methods . But there are some other OOD detection methods which also achieve state-of-the-art results , such as [ 1 ] [ 2 ] [ 3 ] . Could the authors compare their method to these methods ? 2.In their taxonomy , they consider examples that are very close to in-distribution as OOD . I am wondering whether we should treat those examples as OOD since they are too close to the in-distribution . I think previous works like ODIN and Mahalanobis all assume that OOD inputs are far away from the in-distribution . In the experimental setup , they consider STL10 as an OOD dataset for CIFAR10 . But STL10 contains CIFAR10 alike images . It is unconvincing that we should treat those images as OOD . And I think the classifier trained on CIFAR10 may have correct predictions on some of those images . Could the authors explain why we should treat those images as OOD ? 3.I am wondering whether the analysis for the simple two-dimensional dataset could be applied to high-dimensional datasets . In the high-dimensional space , their conclusion about which method detects which type of OOD may not hold . Could the authors explain it ? 4.In Appendix A.2.1 , they mention that the best results from the twelve combinations of the aforementioned sub-categories ( one from each of the four attributions ) are reported . Could the authors explain how they select the best results ? Do they use the test OOD data to select the best results ? 5.Could the author describe how they integrate the existing state-of-the-art detection methods in detail ? It is hard for me to understand what they exactly do in their proposed method . After Reading the Updated Paper - Thanks for the update . After reading the revised paper , I still have some major concerns : 1 . The current experiments performed are not enough to demonstrate the effectiveness of the proposed method . The old experiment results ( Table 6 , 7 , 8 ) are not convincing since the authors train a binary classifier as an OOD detector using a subset of the test OOD data , which is not realizable in practice . We should assume that the test OOD data are unknown during learning the OOD detector . The new experimental results where they train the binary classifier using adversarial examples generated on in-distribution data ( follow the Mahalanobis method ) in Table 1 are limited . For example , on CIFAR10 , they only report results for ResNet50 and WideResNet , but I also want to know the results for DenseNet ( Mahalanobis method [ 4 ] performs very well on CIFAR10/SVHN using DenseNet under the same setting ) . 2.Some experimental details about their method are missing . The authors mention that they train 12 binary classifiers and then select the best one on the validation dataset . But they do n't provide the details about the validation dataset , which is critical for their results . Based on their previous response , it seems they use a subset of test OOD data to select the best classifier , which is not allowed I think . Based on the current description of experimental settings , it is hard for me to evaluate the reported results . 3.The proposed approach needs a lot of hyper-parameters ( 4 attributes , 12 combinations , the weights of the binary classifier , etc ) and it is unclear how to tune these hyper-parameters and how they would affect the results . The current ablation study is limited I think . 4.This paper does n't have rigorous analysis for why integrating different attributions would improve OOD detection . I think this is an empirical paper but the experiments provided are not sufficient to demonstrate the effectiveness of the proposed method . To clarify , I did n't agree to raise the score previously . What I said was that the previous paper needed significant revision and I could not recommend acceptance . I still have some major concerns after reading the revised paper . Thus , I keep the same rating and think the paper is not ready for publication . I hope the authors could keep improving their paper . [ 1 ] Hendrycks , Dan , Mantas Mazeika , and Thomas Dietterich . `` Deep anomaly detection with outlier exposure . '' arXiv preprint arXiv:1812.04606 ( 2018 ) . [ 2 ] Liu , Weitang , et al . `` Energy-based Out-of-distribution Detection . '' arXiv preprint arXiv:2010.03759 ( 2020 ) . [ 3 ] Lakshminarayanan , Balaji , Alexander Pritzel , and Charles Blundell . `` Simple and scalable predictive uncertainty estimation using deep ensembles . '' Advances in neural information processing systems . 2017 . [ 4 ] Lee , Kimin , et al . `` A simple unified framework for detecting out-of-distribution samples and adversarial attacks . '' Advances in Neural Information Processing Systems . 2018 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "\u201c STL10 contains CIFAR10 alike images\u2026 Could the authors explain why we should treat STL10 as OODs for CIFAR10 ? \u201d STL10 dataset is inspired byCIFAR-10 but two datasets differ in terms of the image resolution ( STL10 - 96X96 and CIFAR10 - 32X32 ) . Since STL10 is similar to the CIFAR10 dataset , it makes the OOD detection more challenging while considering STL10 as OODs for the CIFAR10 . This is the reason why we selected this pair of datasets to stress test our method . \u201c I am wondering whether the analysis for the simple two-dimensional dataset could be applied to high-dimensional datasets . In the high-dimensional space , their conclusion about which method detects which type of OOD may not hold . Could the authors explain it ? \u201d We agree that it is hard to verify whether the observations in the 2D dataset apply directly to the high-dimensional datasets . However , in fig . 5 , we show the projection of the high-dimensional features in 2D using t-SNE . Please note the highlighted examples where the OOD samples associated with both epistemic and aleatoric uncertainties are missed by the Mahalanobis [ 4 ] but were detected by our approach . Also , our ablation studies ( in appendix Table.10 , 11 , 12 ) show that individual approaches ( ODIN , PCA , Mahalanobis , etc ) are not sufficient to detect all types of OODs and an integrated approach is necessary . \u201c Could the authors explain how they select the best results ? Do they use the test OOD data to select the best results ? Could the author describe how they integrate the existing state-of-the-art detection methods in detail ? \u201d A weighted sum of the four attributes ( section A.2.1 of the appendix ) forms the signature of our OOD detector . The weights of these attributes are generated in the following manner . Following the standard experimental setup [ 4 , 6 ] , we use a small subset of both in-distribution and OOD data to train a binary classifier using a logistic loss . The trained classifier ( or OOD detector ) is then evaluated on the remaining OOD samples at the True Positive Rate of 95 % . The four attributes ( forming the signature of the OOD detector ) are 1 ) distance from the in-distribution density estimate , 2 ) reconstruction error from the principal component analysis ( PCA ) , 3 ) prediction confidence of the classifier , and 4 ) conformance measure among the nearest neighbors . These attributes can be computed in different ways . In our experiments , we consider 2 ways of generating distance from the in-distribution density estimate , 1 way of generating reconstruction error from PCA , 3 ways of generating prediction confidence of the classifier , and 2 ways of generating conformance measure among the nearest neighbors , resulting in a total of 12 ways of combining these four attributes ( 2 * 1 * 3 * 2 ) ( section A.2.1 of the appendix ) . Out of these 12 combinations , we report the best empirical result on the test OOD data . \u201c Lack of discussion on related work . Could the authors compare their method to these methods ... \u201d We are looking into the suggested papers to compare our results with these papers . We will provide an update on this once we have the results . [ 4 ] Lee , Kimin , et al . `` A simple unified framework for detecting out-of-distribution samples and adversarial attacks . '' Advances in Neural Information Processing Systems . 2018 . [ 6 ] Lee , Kimin , et al . `` Training confidence-calibrated classifiers for detecting out-of-distribution samples . '' arXiv preprint arXiv:1711.09325 ( 2017 ) ."}, "1": {"review_id": "zleOqnAUZzl-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper introduces a novel taxonomy for OOD outliers . The authors analyze current OOD detection approaches and uncover their limitations . They propose to fuse several existing approaches into a combined one and extensively evaluate it on various data sets ( CIFAR,10 , SVNH , MNIST , STL10 , ImageNet , etc . ) . The proposed integrated OOD detection approach clearly shows superior performance . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons : Overall , I vote for accepting . The authors make several key contributions : The introduce a novel OOD taxonomy , analyse current OOD detection approaches on a toy data set , propose an integrated OOD detection approach , which shows a superior performance in their extensive evaluation . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : * Introduction of a sound and helpful OOD taxonomy * Limitation analysis of state-of-the-art OOD detection algorithms * Proposal of a new integrated approach to detect different kind of OOD inputs that unifies the advanatges of underlying algorithms . * Extensive evaluation of new approach shows clearly superior performance . On a variety of data sets ( CIFAR,10 , SVNH , MNIST , STL10 , ImageNet , etc . ) the proposed approach outperforms the baselines on all evaluation criteria ( TNR , AUROC , DTACC , AUPR IN , AUPR OUT ) for various classifier neural network architectures ( LeNet , ResNet , DenseNet ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : * The demonstration of the limitations of current OOD detection algorithms is solely empirical ( based on a toy data set ) . Theoretic motivations ( if possible ) would be a great addition . * Similarly , a sound theoretical derivation for the proposed integrated approach is lacking . * Further toy data sets beyond the two half moon data set would be helpful to better understand the implications of all algorithms . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Please address and clarify the cons above", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the invaluable comments and feedback on this paper . We address the reviewer 's comments as follows . 1.The demonstration of the limitations of current OOD detection algorithms is solely empirical ( based on a toy data set ) . Theoretic motivations ( if possible ) would be a great addition . We show the limitations of OOD detection algorithms on the toy dataset for illustration purposes . However , these limitations are applicable to real datasets as well . For example , in Fig 5 , we show the examples of the OOD sample caused by epistemic and aleatoric uncertainties on the CIFAR dataset . As mentioned in subsection `` Key observations '' , these samples are missed by the Mahalanobis approach but detected by our approach . This justifies that the definition of ODDs and limitations of the OOD detection algorithms are not limited to toy datasets only . 2.Similarly , a sound theoretical derivation for the proposed integrated approach is lacking . The proposed integrated approach is motivated by two key observations : 1 ) the OOD samples can be of various types ( Fig.1 ) and 2 ) one approach can not detect all types of OODs . ( A.2.3 ABLATION STUDY ) . Thus , an integrated approach is required to detect all types of OODs . Please note the subsection `` Key observations '' justifies our claims . We agree that a theoretical derivation will further strengthen the claims and we will consider that in the follow-up works . 3.Further toy data sets beyond the two half-moon data set would be helpful to better understand the implications of all algorithms . We consider two toy datasets : half moons ( Fig 2 ) and the mixture of Gaussian ( Fig 1 ) to show the variants of OODs . As shown in Fig 2 and 3 , after considering a deep neural network to extract features , the non-linear organization of the data points in the original half-moons tends to become linear . This is due to the non-linear projection caused by the neural transformations . We expect to see the same behavior for other 2D or 3D toy datasets as long as a sufficiently deep neural network is considered to extract the features ."}, "2": {"review_id": "zleOqnAUZzl-2", "review_text": "-- Paper Summary : The paper presents the idea of fusion of attributes from existing sota ood detection methods to achieve higher detection performance . -- Review : - The three criteria presented in section two are questions rather than criteria . It is better to be re-worded into criteria . - Figure 1 suggests the `` tied distribution of all training data '' is different than the combination of `` class distributions '' . I wish authors could explain the difference between Type 4 and 5 in the ood sample taxonomy . - The relation between five types of OOD with three criteria for OOD categorization is not clear . - The visualization in all figures could be improved : - figure 1 : too many colors . better to use different shape or numbers directly in the figure . - figure 5 : not necessary to include , hard to see and comprehend . - the total number of figures can be reduced by eliminating some and combining others . - What was the reason to choose a subset of cifar100 as ood test set but not the whole dataset ? - Authors emphasize reporting detection TNR in the manuscript while FNR is missing from the measurements . I suggest authors either report both or use threshold agnostic metrics like area under precision recall curve ( AUPR ) or area under receiver operating curve ( AUROC ) for reporting as in the Table . - I ca n't find an explanation and/or discussion on the final detection score and it 's hyperparametere . - The results from the Mahanalobis technique [ 7 ] does not match the original paper . If authors did not use a subset of ood samples for tuning , it should be reported in the paper . -- Strengths : - interesting taxonomy of ood samples and the following conclusion for integrated detection score . -- Weaknesses : - limited on contribution - no discussion on final detection score and its hyperparameters . - comparison with more recent techniques including Outlier Exposure [ 1 ] , Self-supervised reject classifier [ 2 ] , Geometric self-superivised learning [ 3,4 ] , and contrastive learning [ 5,6 ] are missing in this paper . [ 1 ] Hendrycks , D. , Mazeika , M. , & Dietterich , T. ( 2018 , September ) . Deep Anomaly Detection with Outlier Exposure . ICLR 2019 [ 2 ] Mohseni , Sina , et al . `` Self-Supervised Learning for Generalizable Out-of-Distribution Detection . '' AAAI.2020 . [ 3 ] Hendrycks , D. , Mazeika , M. , Kadavath , S. , & Song , D. ( 2019 ) . Using self-supervised learning can improve model robustness and uncertainty . In Advances in Neural Information Processing Systems ( pp.15663-15674 ) . [ 4 ] Golan , Izhak , and Ran El-Yaniv . `` Deep anomaly detection using geometric transformations . '' Advances in Neural Information Processing Systems . 2018 . [ 5 ] Tack , J. , Mo , S. , Jeong , J. , & Shin , J . ( 2020 ) .Csi : Novelty detection via contrastive learning on distributionally shifted instances . arXiv preprint arXiv:2007.08176 . [ 6 ] Winkens , J. , Bunel , R. , Roy , A. G. , Stanforth , R. , Natarajan , V. , Ledsam , J. R. , ... & Cemgil , T. ( 2020 ) . Contrastive training for improved out-of-distribution detection . arXiv preprint arXiv:2007.05566 . [ 8 ] Liu , Hao , and Pieter Abbeel . `` Hybrid discriminative-generative training via contrastive learning . '' arXiv preprint arXiv:2007.09070 ( 2020 ) . [ 7 ] Lee , K. , Lee , K. , Lee , H. , & Shin , J . ( 2018 ) .A simple unified framework for detecting out-of-distribution samples and adversarial attacks . In Advances in Neural Information Processing Systems ( pp.7167-7177 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "\u201c Figure 1 suggests the tied distribution of all training data is different than the combination of class distributions. \u201d The tied distribution shown in figure 1 is obtained using robust covariance estimation ( https : //scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html # sklearn.covariance.EllipticEnvelope ) , representing the unimodal Gaussian distribution of the complete ( irrespective of the class ) in-distribution dataset and is only meant for illustrative purposes . This is one of the many ways to model the distribution of the complete dataset . Other potential techniques for modeling the complete dataset are kernel density estimation , multi-modal gaussian distribution , etc . \u201c I wish authors could explain the difference between Type 4 and 5 in the ood sample taxonomy. \u201d While both Type 4 and Type 5 are OODs due to high class conditional epistemic uncertainty , they vary in their principal component analysis of the class distribution ( as explained in the last paragraph of section 2 in the paper ) . Type 4 are OODs due to high deviation along the principal axis of the in-distribution class 2 . Type 5 are OODs due to relatively lower deviation along the non-principal axis ( and hence , statistically invariant ) of the in-distribution class 1 . The difference in principle axis and non-principal axis will lead to a disparate effect on the reconstruction error . \u201c The relation between five types of OOD with three criteria for OOD categorization is not clear. \u201d Criteria 1 - Is the OOD associated with higher epistemic or aleatoric uncertainty , i.e. , is the input away from in-distribution data or can it be confused between multiple classes ? OOD Types - Type 1 and Type 2 are OODs due to epistemic uncertainty as they are far from the in-distribution data . Type 3 are OODs due to aleatoric uncertainty between the in-distribution classes 0 and 1 . Criteria 2 - Is the epistemic uncertainty of an OOD sample unconditional or is it conditioned on the class predicted by the DNN model ? OOD Types - Type 1 are OODs due to the epistemic uncertainty of the tied in-distribution . In other words , they are far from the tied in-distribution ( represented by the red oval in the figure ) . Type 2 are OODs due to class conditional epistemic uncertainty . In other words , if we consider class-wise instead of a single tied in-distribution then Type 2 are far from all the class distributions . But if we consider the tied distribution , then Type 2 OODs lie within the in-distribution . So , Type 2 are OODs due to class conditional epistemic uncertainty . Criteria 3 - Is the OOD an outlier due to unusually high deviation in the principal components of the data or due to small deviation in the non-principal ( and hence , statistically invariant ) components ? OOD Types - Type 4 are OODs due to high deviation along the principal axis of the in-distribution class 2 . Type 5 are OODs due to relatively lower deviation along the non-principal axis of the in-distribution class 1 . \u201c What was the reason to choose a subset of cifar100 as ood test set but not the whole dataset ? \u201d The subset of CIFAR100 considered in the experiments consists of the following four classes - sea , road , bee , and butterfly . These classes are visually similar to the ship , automobile , and bird classes in the CIFAR10 dataset respectively ( as stated in section 4 under CIFAR10 dataset ) . Therefore , it would make the task of OOD detection with this subset of CIFAR100 for CIFAR10 as in-distribution more challenging . This is the reason for choosing a subset of CIFAR100 to stress test our method on CIFAR10 as in-distribution . `` Authors emphasize reporting detection TNR in the manuscript while FNR is missing from the measurements . I suggest authors either report both or use threshold agnostic metrics like area under precision recall curve ( AUPR ) or area under receiver operating curve ( AUROC ) for reporting as in the Table . '' AUROC has been reported in Tables 1 and 2 of the experimental section 4 of the paper ( comparison with ODIN and Mahalanobis ) and Table 3 in the Appendix ( comparison with Baseline ) . AUPR has been reported for all the experiments in Tables 4 , 5 , 6 , 7 , 8 , and 9 of the Appendix . \u201c I ca n't find an explanation and/or discussion on the final detection score and it 's hyperparameters \u201d Section A.2.1 of the Appendix , \u201c Attributes forming the signature of the OOD detector used in the experiments \u201d , explains how the final detection scores are reported . \u201c Comparison with more recent techniques including Outlier Exposure , \u2026 and contrastive learning are missing in this paper. \u201d We are looking into the suggested papers to compare our results with these papers . We will provide an update on this once we have the results ."}, "3": {"review_id": "zleOqnAUZzl-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The authors explore the different kinds of outliers and show that the methods previously proposed detect different kinds of OOD and not a single one can detect them all . The authors propose an interesting study of the different kind of outlier on synthetic data which illustrates well the different characteristics of the outlier types . The authors then propose to combine different methods to increase the OOD detection rate . Experiments are conducted on 3 images classification datasets using different deep neural networks . For each dataset , samples from other databases are introduced as outliers and must be detected . The combination method yield better detection rates than baseline methods in almost all configurations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : The main idea of the paper is simple : combine different OOD detection metrics to increase the detection rate on different types of outliers . The proposed method indeed increases the OOD detection rate for almost all the experimental settings tested by the authors . However , the method to create the OOD samples is always the same : in-distribution samples come from a database whereas out of distribution samples are drawn from another database . It would be interesting to show that the method also increases the detection rate of outliers inside a given database . This could be done by reporting the classification rate of the DNN in an abstaining scheme : if the OOD metric is greater than a threshold , the sample is not classified ( rejected ) . If the OOD detection method is useful , the classification rate of the DNN can be freely increased by increasing the threshold and rejecting more and more samples . The author do not justify their choice of the combination method . Computing all the OOD metrics can be computationaly expensive , is it necessary to compute them all ? Are this combination of metric the best ? In which conditions ? The combination method should be described in the body of the paper , not in appendix . Guo 2017 appears twice in the bibliography .", "rating": "5: Marginally below acceptance threshold", "reply_text": "\u201c show that the method also increases the detection rate of outliers inside a given database .. by reporting the classification rate of the DNN in an abstaining scheme \u201d As per the reviewer \u2019 s suggestion , we conducted new experiments to illustrate the applicability of our approach for the detection of outliers in the same dataset . We considered a ResNet34 model trained on CIFAR10 with an accuracy of 93.67 % on the test dataset . We then trained our OOD detector using a set of randomly sampled 300 misclassified images and another set of randomly sampled 300 correctly classified images from the test dataset . This trained OOD detector was then able to correctly identify 326 out of the remaining 333 samples with incorrect predictions as outliers . Thus , using our OOD detector ( trained with the True Positive Rate of 95 % ) with abstaining on outliers improved the classification accuracy to 99.91 % from 93.67 % on non-abstaining samples . Our results demonstrate that the proposed OOD method can identify and abstain on samples on which the model is likely to produce an incorrect prediction . \u201c However , the method to create the OOD samples is always the same : in-distribution samples come from a database whereas out of distribution samples are drawn from another database. \u201d As suggested , we employ a new way to generate out-of-distribution samples by modifying in-distribution samples using adversarial attacks - fast gradient sign method ( FGSM ) [ 1 ] , DeepFool [ 2 ] , basic iterative method ( BIM ) [ 3 ] . We used CIFAR10 as the in-distribution dataset and a ResNet34 model . The attacked images were generated from the test dataset of CIFAR10 . We obtained a True Negative Rate ( i.e.the rate of detection of attacked images as OODs ) of 41.19 % , 61.66 % , 93.23 % for the attacked images generated by Deepfool , FGSM and BIM attacks , respectively at a True Positive Rate of 95 % . The context of the attacks used to generate OODs - FGSM is a single-step attack that uses the L_\\infty metric for measuring the distance between a legitimate and perturbed example . BIM was introduced to improve the performance of FGSM by running a finer iterative optimizer for multiple iterations . BIM has a much higher attacking rate than FGSM and it still causes noticeable perturbations even though fewer visual flaws occur than those crafted by FGSM . The perturbations introduced by DeepFool are unnoticeable and the attacking rate is much higher than that of FGSM and BIM . \u201c Computing all the OOD metrics can be computationally expensive , is it necessary to compute them all ? Are this combination of metric the best ? In which conditions ? \u201d As explained in the Appendix ( Section A.2.1 ) of the paper , the signature of the OOD detector is the weighted sum of the four attributes used to distinguish OODs from the in-distribution samples . These attributes are 1 ) distance from the in-distribution density estimate , 2 ) reconstruction error from the principal component analysis , 3 ) prediction confidence of the classifier , and 4 ) conformance measure among the nearest neighbors . These attributes can be computed by different metrics . Some of these metrics are mentioned as categories under these attributes in A.2.1 . Only one metric per attribute is used in the OOD detector . As illustrated in Figure 4 of section 3 in the paper , these attributes tend to capture specific types ( shown as different clusters ) of OODs but not all . Our ablation studies ( Tables 10 , 11 , and 12 of the Appendix ) evaluating each attribute show the above-mentioned limitations . Thus , we proposed an integrated approach combining these attributes to detect a diverse type of OODs . We evaluated our approach on benchmark datasets considering state-of-the-art neural network models . The proposed approach achieved a better performance as shown in Tables 1 and 2 in the experiment section justifying the importance of the integrated approach . [ 1 ] Goodfellow , Ian J. , Jonathon Shlens , and Christian Szegedy . `` Explaining and harnessing adversarial examples . '' arXiv preprint arXiv:1412.6572 ( 2014 ) . [ 2 ] Moosavi-Dezfooli , Seyed-Mohsen , Alhussein Fawzi , and Pascal Frossard . `` Deepfool : a simple and accurate method to fool deep neural networks . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2016 [ 3 ] Kurakin , Alexey , Goodfellow , Ian , and Bengio , Samy . Adversarial examples in the physical world . arXiv preprint arXiv:1607.02533 , 2016 ."}}