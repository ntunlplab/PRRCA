{"year": "2018", "forum": "B1e5ef-C-", "title": "A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs", "decision": "Accept (Poster)", "meta_review": "sadly, none of the reviewers seem to have been able to fully appreciate and check the proofs.\n\nbut in the words of even the least positive reviewer:\nIn general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.\n\ni think we can all gain from fresh perspectives of LSTMs and DL for NLP :)\n", "reviews": [{"review_id": "B1e5ef-C--0", "review_text": "The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams. This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition. Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings. I didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive review ! We are currently preparing a revision incorporating these comments . We would also like to clarify that our paper concerns LSTM document embeddings , not word embeddings ."}, {"review_id": "B1e5ef-C--1", "review_text": "The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized. -In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive review ! We are currently preparing a revision incorporating these comments . Comment : \u201c the embedding dimension $ d $ is depending on $ T^2 $ , and it may scale poorly with respect to $ T $ . \u201d Yes the bound may scale poorly with document length . At the moment many tasks in this area use short sentences ( e.g.SST has avg . length < 20 ) , and Fig.4 indicates convergence of DisC to BonC performance even on the IMDB task ( avg.length > 250 ) so perhaps our bound is too pessimistic . Note that in the unigram ( BoW ) case the scaling is ( provably ) linear in T because then the design matrix is an i.i.d.Rademacher ensemble ."}, {"review_id": "B1e5ef-C--2", "review_text": "My review reflects more from the compressive sensing perspective, instead that of deep learners. In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective. The paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow. The second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory. Partial explanations are provided, again using results in compressive sensing theory. In my personal opinion, the explanations are opaque and unsatisfactory. An alternative route is suggested in my detailed review. Finally, extensive experiments are conducted and they are in accordance with the theory. My most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5. Specifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not. This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery. In particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]. There, a simple deterministic condition (the null space property) for successful recovery is proved. It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs. Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all. While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC. Exactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise. Several minor comments: 1. Please avoid the use of \u201cinformation theory\u201d, especially \u201cclassical information theory\u201d, in the current context. These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon. I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there\u2019s no need to mention information theory here. 2. In Theorem 4.1, please be specific about how the l2-regularization is chosen. 3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case. I understood the necessity only through reading proofs. 4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical? 5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017]. 6. Page 2, first paragraph of related work, the sentence \u201cOur method also closely related to ...\u201d is incomplete. 7. Page 2, second paragraph of related work, \u201cPagliardini also introduceD a linear ...\u201d 8. Page 9, conclusion, the beginning sentence of the second paragraph is erroneous. [1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, \u201cThe Convex Geometry of Linear Inverse Problems\u201d, Foundations of Computational Mathematics, 2012.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thorough review ! We \u2019 ll revise incorporating your comments . Main Responses : 1 ) \u201c instead of using [ Donoho & Tanner 2005 ] it should be better to use [ Chandrasekaran et al.2012 ] \u2019 s deterministic condition , the null space property or NSP \u201d ( paraphrase ) We knew of NSP but turned to Donoho & Tanner ( 2005 ) because NSP is difficult to work with ( no obvious method to check if local NSP holds ; checking global NSP is NP-hard ( Tillmann & Pfetsch , 2014 ) ) . Since NSP is equivalent to exact recovery , our experiments ( Fig.1-2 ) strongly suggest that local NSP holds , but we did not find a way to use it to gain intuition or proofs . While closely related to NSP , the polytope condition of Donoho & Tanner ( 2005 ) implies Corollary 5.1 , which suggests both a nice property of word embeddings and an efficient method to check recovery of nonnegative signals . 2 ) : \u201c [ the claim that ] certain LSTMs with random initialization are at least as good as the linear classifiers\u2026 ... is almost a direction application of the RIP of random Rademacher matrices \u201d This is true for the unigram ( BoW ) case . The proof for the n-gram case necessitated constructing a design matrix with correlated entries for which RIP is not as obvious . We agree that the bigger technical contribution is in connecting these ideas to text embeddings . Other Responses : Restricted Strong Convexity ( RSC ) : \u201c it is proved in [ Chandrasekran et al.2012 ] that Restricted Strong Convexity ( RSC ) alone is enough to guarantee successful recovery. \u201d To our knowledge RSC is used mostly for the case of signal/measurement noise ( Negahban et al. , 2010 ; Chandrasekaran et al. , 2012 ) , whereas we are in the noiseless setting . We know of work by Elenberg et al . ( 2016 ) using RSC to guarantee recovery with Orthogonal Matching Pursuit , but we have found that such greedy methods do not work well for pretrained embeddings ( Section 5.1 paragraph 2 ) , indicating that a sufficient RSC condition does not hold . LASSO/Dantzig Selector : \u201c the same comments above apply to many other common estimators ( lasso , Dantzig selector , etc . ) in compressive sensing which might be more tolerant to noise. \u201d LASSO was in fact the first approach we tried , with similar results as Basis Pursuit ( we refer to it in Section 5.1 paragraph 2 as an \u201c l_0-surrogate method \u201d ) . However , as we are in the noiseless setting we do not need the robustness provided by LASSO ; indeed , experiments show it performs somewhat worse for both pretrained and random vectors . Furthermore , to our knowledge guarantees for LASSO often have analogous results for Basis Pursuit , so the theoretical benefit to studying it is unclear . Although we did not try the Dantzig Selector , it can also be seen as a robust extension of Basis Pursuit and so similarly does not provide a clear advantage in our case . Minor Points : 1 . We use the phrase \u201c classical information theory \u201d only in connection with the scheme in Paskov et al. , ( 2013 ) which is inspired by the Lempel-Ziv compression algorithm ( Ziv & Lempel , 1977 ) ; 40 years old and directly inspired by Shannon \u2019 s works ! 2.In theory the regularization constant C is chosen to minimize the error bound ; in practice it is chosen by cross-validation . 3.We extend the analysis in order to handle logistic loss as it is commonly used in the NLP community and by supervised LSTMs . We do not need Theorem 4.2 to hold for all Lipschitz functions to get Theorem 4.1 , but the function does need to be Lipschitz to control the error . 4.1 This assumption is without loss of generality and is made to remove a spurious dependence on T in the error bound . 4.2.There will sometimes be n-cooccurrences that contain a word more than once , e.g . ( as , long , as ) , but they occur infrequently and can be removed by merging words as a preprocessing step . In the SST training corpus only 0.019 % of bigrams and 0.75 % of trigrams have this issue , the latter often due to words between two commas in a list . 5-8.Will be addressed in revision . V. Chandrasekaran , B. Recht , P. A. Parrilo , and A. S. Willsky . \u201c The Convex Geometry of Linear Inverse Problems. \u201d Found . of Comp.Mathematics 2012 . D. L. Donoho and J. Tanner . \u201c Sparse nonnegative solution of underdetermined linear equations by linear programming. \u201d PNAS 2005 . E. R. Elenberg , R. Khanna , A. G. Dimakis , and S. Negahban . \u201c Restricted strong convexity implies weak submodularity. \u201d arXiv 2016 . S. Negahban , B. Yu , M. J. Wainwright , and P. K. Ravikumar . \u201c A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. \u201d NIPS 2009 . H. S. Paskov , R. West , J. C. Mitchell , and T. J. Hastie . \u201c Compressive feature learning. \u201d NIPS 2013 . A. M. Tillmann and M. E. Pfetsch . \u201c The computational complexity of the restricted isometry property , the nullspace property , and related concepts in compressed sensing. \u201d IEEE Trans . on Info.Theory 2014 . J. Ziv and A. Lempel . \u201c A Universal Algorithm for Sequential Data Compression. \u201d IEEE Trans . on Info.Theory 1977 ."}], "0": {"review_id": "B1e5ef-C--0", "review_text": "The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams. This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition. Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings. I didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive review ! We are currently preparing a revision incorporating these comments . We would also like to clarify that our paper concerns LSTM document embeddings , not word embeddings ."}, "1": {"review_id": "B1e5ef-C--1", "review_text": "The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized. -In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive review ! We are currently preparing a revision incorporating these comments . Comment : \u201c the embedding dimension $ d $ is depending on $ T^2 $ , and it may scale poorly with respect to $ T $ . \u201d Yes the bound may scale poorly with document length . At the moment many tasks in this area use short sentences ( e.g.SST has avg . length < 20 ) , and Fig.4 indicates convergence of DisC to BonC performance even on the IMDB task ( avg.length > 250 ) so perhaps our bound is too pessimistic . Note that in the unigram ( BoW ) case the scaling is ( provably ) linear in T because then the design matrix is an i.i.d.Rademacher ensemble ."}, "2": {"review_id": "B1e5ef-C--2", "review_text": "My review reflects more from the compressive sensing perspective, instead that of deep learners. In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective. The paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow. The second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory. Partial explanations are provided, again using results in compressive sensing theory. In my personal opinion, the explanations are opaque and unsatisfactory. An alternative route is suggested in my detailed review. Finally, extensive experiments are conducted and they are in accordance with the theory. My most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5. Specifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not. This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery. In particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]. There, a simple deterministic condition (the null space property) for successful recovery is proved. It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs. Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all. While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC. Exactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise. Several minor comments: 1. Please avoid the use of \u201cinformation theory\u201d, especially \u201cclassical information theory\u201d, in the current context. These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon. I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there\u2019s no need to mention information theory here. 2. In Theorem 4.1, please be specific about how the l2-regularization is chosen. 3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case. I understood the necessity only through reading proofs. 4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical? 5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017]. 6. Page 2, first paragraph of related work, the sentence \u201cOur method also closely related to ...\u201d is incomplete. 7. Page 2, second paragraph of related work, \u201cPagliardini also introduceD a linear ...\u201d 8. Page 9, conclusion, the beginning sentence of the second paragraph is erroneous. [1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, \u201cThe Convex Geometry of Linear Inverse Problems\u201d, Foundations of Computational Mathematics, 2012.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thorough review ! We \u2019 ll revise incorporating your comments . Main Responses : 1 ) \u201c instead of using [ Donoho & Tanner 2005 ] it should be better to use [ Chandrasekaran et al.2012 ] \u2019 s deterministic condition , the null space property or NSP \u201d ( paraphrase ) We knew of NSP but turned to Donoho & Tanner ( 2005 ) because NSP is difficult to work with ( no obvious method to check if local NSP holds ; checking global NSP is NP-hard ( Tillmann & Pfetsch , 2014 ) ) . Since NSP is equivalent to exact recovery , our experiments ( Fig.1-2 ) strongly suggest that local NSP holds , but we did not find a way to use it to gain intuition or proofs . While closely related to NSP , the polytope condition of Donoho & Tanner ( 2005 ) implies Corollary 5.1 , which suggests both a nice property of word embeddings and an efficient method to check recovery of nonnegative signals . 2 ) : \u201c [ the claim that ] certain LSTMs with random initialization are at least as good as the linear classifiers\u2026 ... is almost a direction application of the RIP of random Rademacher matrices \u201d This is true for the unigram ( BoW ) case . The proof for the n-gram case necessitated constructing a design matrix with correlated entries for which RIP is not as obvious . We agree that the bigger technical contribution is in connecting these ideas to text embeddings . Other Responses : Restricted Strong Convexity ( RSC ) : \u201c it is proved in [ Chandrasekran et al.2012 ] that Restricted Strong Convexity ( RSC ) alone is enough to guarantee successful recovery. \u201d To our knowledge RSC is used mostly for the case of signal/measurement noise ( Negahban et al. , 2010 ; Chandrasekaran et al. , 2012 ) , whereas we are in the noiseless setting . We know of work by Elenberg et al . ( 2016 ) using RSC to guarantee recovery with Orthogonal Matching Pursuit , but we have found that such greedy methods do not work well for pretrained embeddings ( Section 5.1 paragraph 2 ) , indicating that a sufficient RSC condition does not hold . LASSO/Dantzig Selector : \u201c the same comments above apply to many other common estimators ( lasso , Dantzig selector , etc . ) in compressive sensing which might be more tolerant to noise. \u201d LASSO was in fact the first approach we tried , with similar results as Basis Pursuit ( we refer to it in Section 5.1 paragraph 2 as an \u201c l_0-surrogate method \u201d ) . However , as we are in the noiseless setting we do not need the robustness provided by LASSO ; indeed , experiments show it performs somewhat worse for both pretrained and random vectors . Furthermore , to our knowledge guarantees for LASSO often have analogous results for Basis Pursuit , so the theoretical benefit to studying it is unclear . Although we did not try the Dantzig Selector , it can also be seen as a robust extension of Basis Pursuit and so similarly does not provide a clear advantage in our case . Minor Points : 1 . We use the phrase \u201c classical information theory \u201d only in connection with the scheme in Paskov et al. , ( 2013 ) which is inspired by the Lempel-Ziv compression algorithm ( Ziv & Lempel , 1977 ) ; 40 years old and directly inspired by Shannon \u2019 s works ! 2.In theory the regularization constant C is chosen to minimize the error bound ; in practice it is chosen by cross-validation . 3.We extend the analysis in order to handle logistic loss as it is commonly used in the NLP community and by supervised LSTMs . We do not need Theorem 4.2 to hold for all Lipschitz functions to get Theorem 4.1 , but the function does need to be Lipschitz to control the error . 4.1 This assumption is without loss of generality and is made to remove a spurious dependence on T in the error bound . 4.2.There will sometimes be n-cooccurrences that contain a word more than once , e.g . ( as , long , as ) , but they occur infrequently and can be removed by merging words as a preprocessing step . In the SST training corpus only 0.019 % of bigrams and 0.75 % of trigrams have this issue , the latter often due to words between two commas in a list . 5-8.Will be addressed in revision . V. Chandrasekaran , B. Recht , P. A. Parrilo , and A. S. Willsky . \u201c The Convex Geometry of Linear Inverse Problems. \u201d Found . of Comp.Mathematics 2012 . D. L. Donoho and J. Tanner . \u201c Sparse nonnegative solution of underdetermined linear equations by linear programming. \u201d PNAS 2005 . E. R. Elenberg , R. Khanna , A. G. Dimakis , and S. Negahban . \u201c Restricted strong convexity implies weak submodularity. \u201d arXiv 2016 . S. Negahban , B. Yu , M. J. Wainwright , and P. K. Ravikumar . \u201c A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. \u201d NIPS 2009 . H. S. Paskov , R. West , J. C. Mitchell , and T. J. Hastie . \u201c Compressive feature learning. \u201d NIPS 2013 . A. M. Tillmann and M. E. Pfetsch . \u201c The computational complexity of the restricted isometry property , the nullspace property , and related concepts in compressed sensing. \u201d IEEE Trans . on Info.Theory 2014 . J. Ziv and A. Lempel . \u201c A Universal Algorithm for Sequential Data Compression. \u201d IEEE Trans . on Info.Theory 1977 ."}}