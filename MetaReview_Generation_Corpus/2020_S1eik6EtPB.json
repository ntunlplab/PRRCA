{"year": "2020", "forum": "S1eik6EtPB", "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "decision": "Reject", "meta_review": "This submission studies an interesting problem. However, as some of the reviewers point out, the novelty of the proposed contributions is fairly limited.", "reviews": [{"review_id": "S1eik6EtPB-0", "review_text": "The paper studies how a min-max framework can incorporate different tasks related to adversarial robustness. Specifically, the authors study adversarial attacks against model ensembles, universal perturbations, and attacks constrained by the union of Lp norms. They propose optimizing a probability distribution over \"domains\" (models in an ensemble, inputs, Lp balls; respectively per task) and regularizing it to be close to uniform. They perform experiments to evaluate their method. From a conceptual point of view, I did not find the contribution of the paper significant. All of the tasks discussed are direct application of the min-max framework and have been studied to a certain extent in prior work (https://arxiv.org/abs/1811.11304, https://arxiv.org/abs/1706.04701, https://arxiv.org/abs/1904.13000). The novel tools introduced are the regularizer on simplex probability and the attack diversity regularizer. However, the theoretical justification for these tools is rather weak and their utility would need to be demonstrated empirically. From an experimental point of view, the baselines considered are very weak. At a high level, the authors compare their version of min-max optimization to a very simple average-case optimization. In order to demonstrate the utility of the tools introduced the authors would need to at least compare to a reasonable min-max baseline. For instance, a very simple heuristic capping the loss of each domain in the finite-sum formulation (https://arxiv.org/abs/1811.11304) would be the bare minimum. In their current state, the experiments only demonstrate that a min-max approach outperforms an average case approach, which is fully expected. At the same time, the diversity regularizer does seem to offer some empirical gains and I would encourage the authors to investigate further. Overall, the conceptual and experimental contributions of the paper are rather weak and I thus recommend rejection. ========================= UPDATE: I appreciate the authors' response and additional experimental results. I am still quite concerned about the universal perturbation baseline. I suspect that the clipping factor used might be too large since clipping barely has any impact (the attack is still focusing too much on B ignoring C). Conceptually, clipping should be quite similar to a min-max formulation. I do see that the proposed method outperforms the one proposed in Shafahi et al in terms of universal adversarial training. I feel like this is a more reasonable baseline and I am increasing my score to a 3.", "rating": "3: Weak Reject", "reply_text": "Thanks for your valuable comments and suggestions . Q1 : Conceptual novelty ? A1 : Thanks for your comments . In the revision , we will clarify our conceptual contributions , and elaborate on our differences with the existing works ( Shafahi et al. , 2018 ; Tram\u00e8r et al. , 2019 ; He et al. , 2017 ) . We are sorry to hear that \u201c All of the tasks discussed are direct applications of the min-max framework and have been studied to a certain extent in prior work \u201d . We strongly believe that our work contains substantial differences to the existing works , and is not a direct application of the min-max framework . Differences to ( Shafahi et al. , 2018 ; Tram\u00e8r et al. , 2019 ; He et al. , 2017 ) . The prior work ( Shafahi et al. , 2018 ) proposed a min-max based AT by leveraging universal perturbations ( rather than per-image perturbation ) . We highlight two main differences even just at the defense side . First , different from Shafahi et al. , we generalize AT subject to mixed types of $ \\ell_p $ adversarial perturbations . Figure A2 motivates on why the consideration of multiple perturbation types could matter in AT . Second , our min-max formulation ( 11 ) stems from the min-max-max formulation ( 18 ) , where the last max step is conducted on the importance weights $ \\mathbf w $ . In Lemma 1 , it is our contribution to show the equivalence between ( 18 ) and ( 11 ) . Although the prior work ( Tram\u00e8r et al. , 2019 ) considered a modified AT method subject to multiple types of adversarial perturbations , our work is still quite different from it . Note that some differences had been highlighted in Sec.2 of the original manuscript . We make further clarification as below . First , the incorporation of domain weights $ \\mathbf w $ , the corresponding regularization on $ \\mathbf w $ and the diversity-promoting regularization are new to AT under multiple types of adversarial perturbations . Second , our proposed framework is general , which applies to both attack generation and AT . Even at the defense side , our approach covers the formulations in Tram\u00e8r et al. , as $ \\gamma $ approaches $ 0 $ and $ \\infty $ . Third , please note that ours and Tram\u00e8r et al. , 2019 are actually independent works . The prior work ( He et al. , 2019 ) considered an ensemble-based defensive method . We feel that this is less relevant to the min-max AT framework . Our differences to the previous work ( Shafahi et al. , 2018 ) hold for He et al. , 2019 . A summary of our conceptual contributions . ( Attack ) Even if the concept of min-max optimization was used in other works , our formulation on the min-max attack and its specification to ensemble attack , universal perturbation , and attack under physical transformations are new to the field . In Proposition 1 , we derived the analytical solution to the projection operator subject to the intersection of $ \\ell_p $ norm ( $ p = 0,1,2 , \\infty $ ) inequality constraint and the box constraint . This is different from the conventional attack design , where the $ \\ell_p $ norm was regularized in the objective function . The derived solution to the projection operator subject to the hard constraints also facilitates the implementation of AT under multiple types of adversarial perturbations ( see Sec.3.2 ) . ( Defense ) The proposed min-max formulation ( 11 ) on AT under mixed types of $ \\ell_p $ perturbations is not trivial . It actually stems from the min-max-max formulation ( 18 ) , where the last max step is conducted on the importance weights $ \\mathbf w $ . In Lemma 1 , we show that problem ( 18 ) can equivalently be transformed into the proposed min-max problem ( 11 ) . Moreover , the diversity regularization on multiple perturbation directions is new to AT . In both a ) and b ) , the introduction of self-adjusted weights $ \\mathbf w $ and the strongly concave regularization are not trivial . First , the learnable $ \\mathbf w $ can adjust the model robustness or attack power automatically during the training . Second , the introduction of strongly concave regularization is useful , which ensures $ O ( 1/T ) $ convergence rate for APGD ( Theorem 1 ) and helps the training process in AT under multiple types of $ \\ell_p $ perturbations ( Figure A3 ) . Reference : Universal Adversarial Training . Shafahi et al. , 2018 . Adversarial Training and Robustness for Multiple Perturbations . Tram\u00e8r et al. , NeurIPS 2019 . Adversarial Example Defenses : Ensembles of Weak Defenses are not Strong . He et al. , WOOT 2017"}, {"review_id": "S1eik6EtPB-1", "review_text": "The paper presents a unified framework for adversarial training & robustness. The problem is important and interesting. The proposed framework has solid theory and is well conceived. A generic method is proposed with O(1/T) convergence rate, which is also empirically demonstrated with good performance on often-used MNIST and CIFAR-10 benchmarks. An alternating multi-step PGD is also proposed. Empirical experiments are thorough and well organized. Overall I feel it is a well written paper with sufficient contributions and is of interest to a range of ICLR audience. ", "rating": "8: Accept", "reply_text": "We thank Reviewer # 2 very much to recognize our contributions and have positive comments on our paper ! We will further update our paper by adding new experiment results ( suggested by other reviewers ) as well as improving our presentation ."}, {"review_id": "S1eik6EtPB-2", "review_text": "Note: I applied a higher standard to this paper given that it significantly exceeds the recommended page limit. Furthermore, important details are left out in the appendices, which make it difficult to read the main body of the paper in a self-contained fashion. Given that the main body was already over the recommended page limit, I did not read the appendices. This paper generalizes the min max formulation of adversarial training, and proposes a formulation that encompasses adversarial training of an ensemble, robustness to universal adversarial examples, and robustness to non-adversarial transformations. This formulation is used to derive an adversarial training procedure that trains against the worst-case adversarial example among adversarial examples generated by a set of attacks. Experiments seek to demonstrate applicability of this framework to both attacks and defenses. As far as experiments are concerned, Section 4.1 presents results on MNIST, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705]. If models C and D are more difficult to attack, could better baselines be employed than attacking the ensemble A+B+C+D? For instance, would an adversary evading models C+D only perform better? It is difficult to draw insights that are generally applicable from a single ensemble. How was the ensemble chosen? Why would a defender add models which are known to be significantly less robust to the ensemble? When discussing universal perturbations, how are they generated? Given that the performance of the proposed approach significantly degrades average evasion across all images from all groups, what is the threat model for an adversary being interested in group-level success rather than average evasion across all images from all groups? How were the values of K chosen? This comment also applies to experiments over data transformations. For these experiments, what was the value of K? As far as the defensive perspective is concerned, it is not clear whether the improvements observed are statistically significant. Were multiple runs averaged to produce Table 4? Given that without DPAR, the improvement is negligible, this is important to interpret results. It appears that most of the robustness gains in both the average and max settings stem from DPAR. This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness. In particular, it is not clear whether DPAR is \u201ca beneficial supplement to adversarial training\u201d or a required supplement to adversarial training - per the formulation in this paper. There are issues with grammar throughout the document, which make it difficult to read. Some specific issues: 1 - Adversarial attack is a tautology (an attack is always adversarial) 7 - What does \u201crobust\u201d adversarial attack mean? 7 - What is CAAD-18? 7 - Define ASR_all: what does evade mean here? Is the attack targeted or untargeted? 7 - What is an \u201cadvanced\u201d DNN?", "rating": "3: Weak Reject", "reply_text": "Thanks for your valuable comments and suggestions . Q1 : The paper exceeds the recommended page limit ( so apply higher standard ) . It is not self-contained in the main body and leaves some important details in the appendix . A1 : Thank you . The reviewer \u2019 s suggestion reminds us to better organize the paper and to make our presentation clearer . For ease of understanding , we will add a table of contents in the Appendix for ease of associating our main sections with supplementary details in Appendix . Moreover , in the main paper ( especially in the experiment section ) , we will make a better organization when the figures/tables/sections in Appendix are cited . Q2 : Sec 4.1 presents results on MNIST , which is known to be a poor dataset to study adversarial examples ( https : //arxiv.org/abs/1902.06705 ) A2 : First , we conducted the same experiments on CIFAR-10 and tested more models ( e.g. , VGG16 , Wide-ResNet , GoogLeNet ) in the appendix ( Table 3 , 4 ) . To make it more clearly , we will move the CIFAR-10 experiment to the main body in the updated paper . Moreover , we obtained consistent attack results on both MNIST and CIFAR-10 . Second , MNIST is a dataset , which provides images of easy visualization . For example , we use MNIST to visualize the effect of self-adjusted weights $ \\mathbf w $ on the attack performance ( Table A8 and A9 ) . It is clear to see that the larger domain weights correspond to the MNIST letters with clearer appearance , implying that the learnable weights $ \\mathbf w $ could offer visual interpretability of \u201c image robustness \u201d . Third , we hesitate to call MNIST a poor dataset for studying adversarial examples . To the best of our knowledge , many works ( Madry et al , 2017 ; Athalye et al. , 2019 ; Tram\u00e8r et al. , 2019 ) considered MNIST as a standard dataset . Even for the seminal work ( Carlini et al. , 2019 ) , we did not see a clear objection on MNIST to study adversarial examples . A possible relevant point in the provided paper is that one should consider different perturbation radiuses for different datasets ; for example , $ \\epsilon=0.2 $ ( $ \\ell_\\infty $ attack ) used for MNIST becomes too large for CIFAR-10 . In our work , we follow the commonly-used setting of the perturbation radius , 0.2 for MNIST , 0.05 or 0.03 for CIFAR-10 . Reference : Towards Deep Learning Models Resistant to Adversarial Attacks . Madry et al. , ICLR 2017 . Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples . Athalye et al. , ICLR 2019 . Adversarial Training and Robustness for Multiple Perturbations . Tram\u00e8r et al. , NeurIPS 2019 . On Evaluating Adversarial Robustness . Carlini et al. , 2019 . Q3 : Questions about ensemble models ( better baselines , insights of attacking model ensembles ) . A3 : Thanks for the suggestion . First , we would like to highlight that the weights $ \\mathbf w $ to combine ensemble models are learnable , which avoids supervised manual adjustment . Thus , our approach does not need prior knowledge on the robustness level of different models from both attacker and defender \u2019 s perspectives . Second , following the reviewer 's suggestion , we conduct additional experiments ( Table 1 in https : //tinyurl.com/t6hax2m ) to show that it is nontrivial to set the heuristic weights beforehand . For example , when an adversary evades models C+D only , the generated adversarial examples have a poor transferability to even less robust models A and B . This implies that having the ensemble attack to learn the adjusted weights by itself not only avoids the issue of heuristic weight selection but also boosts the transferability of attacks to different models . Note that it is actually a common practice to attack ensemble models to promote the transferability to unknown black-box models . For instance , in NIPS 2017 , CAAD-2018 competitions , the winner solutions usually integrate multiple adversarially trained models ( usually > 5 ) to enhance the transferability with equal or manually fine-tuned weights . However , we have shown in Table1 that our min-max solution outperforms this averaging strategy . Third , our approach does not rely on specific choices of the model ensemble . For available public models , the importance weights $ \\mathbf w $ are jointly learned , and the resulting results of $ \\mathbf w $ could be used as metrics to reveal the robustness of individual models in the ensemble ( Figure 1c ) ."}], "0": {"review_id": "S1eik6EtPB-0", "review_text": "The paper studies how a min-max framework can incorporate different tasks related to adversarial robustness. Specifically, the authors study adversarial attacks against model ensembles, universal perturbations, and attacks constrained by the union of Lp norms. They propose optimizing a probability distribution over \"domains\" (models in an ensemble, inputs, Lp balls; respectively per task) and regularizing it to be close to uniform. They perform experiments to evaluate their method. From a conceptual point of view, I did not find the contribution of the paper significant. All of the tasks discussed are direct application of the min-max framework and have been studied to a certain extent in prior work (https://arxiv.org/abs/1811.11304, https://arxiv.org/abs/1706.04701, https://arxiv.org/abs/1904.13000). The novel tools introduced are the regularizer on simplex probability and the attack diversity regularizer. However, the theoretical justification for these tools is rather weak and their utility would need to be demonstrated empirically. From an experimental point of view, the baselines considered are very weak. At a high level, the authors compare their version of min-max optimization to a very simple average-case optimization. In order to demonstrate the utility of the tools introduced the authors would need to at least compare to a reasonable min-max baseline. For instance, a very simple heuristic capping the loss of each domain in the finite-sum formulation (https://arxiv.org/abs/1811.11304) would be the bare minimum. In their current state, the experiments only demonstrate that a min-max approach outperforms an average case approach, which is fully expected. At the same time, the diversity regularizer does seem to offer some empirical gains and I would encourage the authors to investigate further. Overall, the conceptual and experimental contributions of the paper are rather weak and I thus recommend rejection. ========================= UPDATE: I appreciate the authors' response and additional experimental results. I am still quite concerned about the universal perturbation baseline. I suspect that the clipping factor used might be too large since clipping barely has any impact (the attack is still focusing too much on B ignoring C). Conceptually, clipping should be quite similar to a min-max formulation. I do see that the proposed method outperforms the one proposed in Shafahi et al in terms of universal adversarial training. I feel like this is a more reasonable baseline and I am increasing my score to a 3.", "rating": "3: Weak Reject", "reply_text": "Thanks for your valuable comments and suggestions . Q1 : Conceptual novelty ? A1 : Thanks for your comments . In the revision , we will clarify our conceptual contributions , and elaborate on our differences with the existing works ( Shafahi et al. , 2018 ; Tram\u00e8r et al. , 2019 ; He et al. , 2017 ) . We are sorry to hear that \u201c All of the tasks discussed are direct applications of the min-max framework and have been studied to a certain extent in prior work \u201d . We strongly believe that our work contains substantial differences to the existing works , and is not a direct application of the min-max framework . Differences to ( Shafahi et al. , 2018 ; Tram\u00e8r et al. , 2019 ; He et al. , 2017 ) . The prior work ( Shafahi et al. , 2018 ) proposed a min-max based AT by leveraging universal perturbations ( rather than per-image perturbation ) . We highlight two main differences even just at the defense side . First , different from Shafahi et al. , we generalize AT subject to mixed types of $ \\ell_p $ adversarial perturbations . Figure A2 motivates on why the consideration of multiple perturbation types could matter in AT . Second , our min-max formulation ( 11 ) stems from the min-max-max formulation ( 18 ) , where the last max step is conducted on the importance weights $ \\mathbf w $ . In Lemma 1 , it is our contribution to show the equivalence between ( 18 ) and ( 11 ) . Although the prior work ( Tram\u00e8r et al. , 2019 ) considered a modified AT method subject to multiple types of adversarial perturbations , our work is still quite different from it . Note that some differences had been highlighted in Sec.2 of the original manuscript . We make further clarification as below . First , the incorporation of domain weights $ \\mathbf w $ , the corresponding regularization on $ \\mathbf w $ and the diversity-promoting regularization are new to AT under multiple types of adversarial perturbations . Second , our proposed framework is general , which applies to both attack generation and AT . Even at the defense side , our approach covers the formulations in Tram\u00e8r et al. , as $ \\gamma $ approaches $ 0 $ and $ \\infty $ . Third , please note that ours and Tram\u00e8r et al. , 2019 are actually independent works . The prior work ( He et al. , 2019 ) considered an ensemble-based defensive method . We feel that this is less relevant to the min-max AT framework . Our differences to the previous work ( Shafahi et al. , 2018 ) hold for He et al. , 2019 . A summary of our conceptual contributions . ( Attack ) Even if the concept of min-max optimization was used in other works , our formulation on the min-max attack and its specification to ensemble attack , universal perturbation , and attack under physical transformations are new to the field . In Proposition 1 , we derived the analytical solution to the projection operator subject to the intersection of $ \\ell_p $ norm ( $ p = 0,1,2 , \\infty $ ) inequality constraint and the box constraint . This is different from the conventional attack design , where the $ \\ell_p $ norm was regularized in the objective function . The derived solution to the projection operator subject to the hard constraints also facilitates the implementation of AT under multiple types of adversarial perturbations ( see Sec.3.2 ) . ( Defense ) The proposed min-max formulation ( 11 ) on AT under mixed types of $ \\ell_p $ perturbations is not trivial . It actually stems from the min-max-max formulation ( 18 ) , where the last max step is conducted on the importance weights $ \\mathbf w $ . In Lemma 1 , we show that problem ( 18 ) can equivalently be transformed into the proposed min-max problem ( 11 ) . Moreover , the diversity regularization on multiple perturbation directions is new to AT . In both a ) and b ) , the introduction of self-adjusted weights $ \\mathbf w $ and the strongly concave regularization are not trivial . First , the learnable $ \\mathbf w $ can adjust the model robustness or attack power automatically during the training . Second , the introduction of strongly concave regularization is useful , which ensures $ O ( 1/T ) $ convergence rate for APGD ( Theorem 1 ) and helps the training process in AT under multiple types of $ \\ell_p $ perturbations ( Figure A3 ) . Reference : Universal Adversarial Training . Shafahi et al. , 2018 . Adversarial Training and Robustness for Multiple Perturbations . Tram\u00e8r et al. , NeurIPS 2019 . Adversarial Example Defenses : Ensembles of Weak Defenses are not Strong . He et al. , WOOT 2017"}, "1": {"review_id": "S1eik6EtPB-1", "review_text": "The paper presents a unified framework for adversarial training & robustness. The problem is important and interesting. The proposed framework has solid theory and is well conceived. A generic method is proposed with O(1/T) convergence rate, which is also empirically demonstrated with good performance on often-used MNIST and CIFAR-10 benchmarks. An alternating multi-step PGD is also proposed. Empirical experiments are thorough and well organized. Overall I feel it is a well written paper with sufficient contributions and is of interest to a range of ICLR audience. ", "rating": "8: Accept", "reply_text": "We thank Reviewer # 2 very much to recognize our contributions and have positive comments on our paper ! We will further update our paper by adding new experiment results ( suggested by other reviewers ) as well as improving our presentation ."}, "2": {"review_id": "S1eik6EtPB-2", "review_text": "Note: I applied a higher standard to this paper given that it significantly exceeds the recommended page limit. Furthermore, important details are left out in the appendices, which make it difficult to read the main body of the paper in a self-contained fashion. Given that the main body was already over the recommended page limit, I did not read the appendices. This paper generalizes the min max formulation of adversarial training, and proposes a formulation that encompasses adversarial training of an ensemble, robustness to universal adversarial examples, and robustness to non-adversarial transformations. This formulation is used to derive an adversarial training procedure that trains against the worst-case adversarial example among adversarial examples generated by a set of attacks. Experiments seek to demonstrate applicability of this framework to both attacks and defenses. As far as experiments are concerned, Section 4.1 presents results on MNIST, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705]. If models C and D are more difficult to attack, could better baselines be employed than attacking the ensemble A+B+C+D? For instance, would an adversary evading models C+D only perform better? It is difficult to draw insights that are generally applicable from a single ensemble. How was the ensemble chosen? Why would a defender add models which are known to be significantly less robust to the ensemble? When discussing universal perturbations, how are they generated? Given that the performance of the proposed approach significantly degrades average evasion across all images from all groups, what is the threat model for an adversary being interested in group-level success rather than average evasion across all images from all groups? How were the values of K chosen? This comment also applies to experiments over data transformations. For these experiments, what was the value of K? As far as the defensive perspective is concerned, it is not clear whether the improvements observed are statistically significant. Were multiple runs averaged to produce Table 4? Given that without DPAR, the improvement is negligible, this is important to interpret results. It appears that most of the robustness gains in both the average and max settings stem from DPAR. This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness. In particular, it is not clear whether DPAR is \u201ca beneficial supplement to adversarial training\u201d or a required supplement to adversarial training - per the formulation in this paper. There are issues with grammar throughout the document, which make it difficult to read. Some specific issues: 1 - Adversarial attack is a tautology (an attack is always adversarial) 7 - What does \u201crobust\u201d adversarial attack mean? 7 - What is CAAD-18? 7 - Define ASR_all: what does evade mean here? Is the attack targeted or untargeted? 7 - What is an \u201cadvanced\u201d DNN?", "rating": "3: Weak Reject", "reply_text": "Thanks for your valuable comments and suggestions . Q1 : The paper exceeds the recommended page limit ( so apply higher standard ) . It is not self-contained in the main body and leaves some important details in the appendix . A1 : Thank you . The reviewer \u2019 s suggestion reminds us to better organize the paper and to make our presentation clearer . For ease of understanding , we will add a table of contents in the Appendix for ease of associating our main sections with supplementary details in Appendix . Moreover , in the main paper ( especially in the experiment section ) , we will make a better organization when the figures/tables/sections in Appendix are cited . Q2 : Sec 4.1 presents results on MNIST , which is known to be a poor dataset to study adversarial examples ( https : //arxiv.org/abs/1902.06705 ) A2 : First , we conducted the same experiments on CIFAR-10 and tested more models ( e.g. , VGG16 , Wide-ResNet , GoogLeNet ) in the appendix ( Table 3 , 4 ) . To make it more clearly , we will move the CIFAR-10 experiment to the main body in the updated paper . Moreover , we obtained consistent attack results on both MNIST and CIFAR-10 . Second , MNIST is a dataset , which provides images of easy visualization . For example , we use MNIST to visualize the effect of self-adjusted weights $ \\mathbf w $ on the attack performance ( Table A8 and A9 ) . It is clear to see that the larger domain weights correspond to the MNIST letters with clearer appearance , implying that the learnable weights $ \\mathbf w $ could offer visual interpretability of \u201c image robustness \u201d . Third , we hesitate to call MNIST a poor dataset for studying adversarial examples . To the best of our knowledge , many works ( Madry et al , 2017 ; Athalye et al. , 2019 ; Tram\u00e8r et al. , 2019 ) considered MNIST as a standard dataset . Even for the seminal work ( Carlini et al. , 2019 ) , we did not see a clear objection on MNIST to study adversarial examples . A possible relevant point in the provided paper is that one should consider different perturbation radiuses for different datasets ; for example , $ \\epsilon=0.2 $ ( $ \\ell_\\infty $ attack ) used for MNIST becomes too large for CIFAR-10 . In our work , we follow the commonly-used setting of the perturbation radius , 0.2 for MNIST , 0.05 or 0.03 for CIFAR-10 . Reference : Towards Deep Learning Models Resistant to Adversarial Attacks . Madry et al. , ICLR 2017 . Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples . Athalye et al. , ICLR 2019 . Adversarial Training and Robustness for Multiple Perturbations . Tram\u00e8r et al. , NeurIPS 2019 . On Evaluating Adversarial Robustness . Carlini et al. , 2019 . Q3 : Questions about ensemble models ( better baselines , insights of attacking model ensembles ) . A3 : Thanks for the suggestion . First , we would like to highlight that the weights $ \\mathbf w $ to combine ensemble models are learnable , which avoids supervised manual adjustment . Thus , our approach does not need prior knowledge on the robustness level of different models from both attacker and defender \u2019 s perspectives . Second , following the reviewer 's suggestion , we conduct additional experiments ( Table 1 in https : //tinyurl.com/t6hax2m ) to show that it is nontrivial to set the heuristic weights beforehand . For example , when an adversary evades models C+D only , the generated adversarial examples have a poor transferability to even less robust models A and B . This implies that having the ensemble attack to learn the adjusted weights by itself not only avoids the issue of heuristic weight selection but also boosts the transferability of attacks to different models . Note that it is actually a common practice to attack ensemble models to promote the transferability to unknown black-box models . For instance , in NIPS 2017 , CAAD-2018 competitions , the winner solutions usually integrate multiple adversarially trained models ( usually > 5 ) to enhance the transferability with equal or manually fine-tuned weights . However , we have shown in Table1 that our min-max solution outperforms this averaging strategy . Third , our approach does not rely on specific choices of the model ensemble . For available public models , the importance weights $ \\mathbf w $ are jointly learned , and the resulting results of $ \\mathbf w $ could be used as metrics to reveal the robustness of individual models in the ensemble ( Figure 1c ) ."}}