{"year": "2018", "forum": "HkCnm-bAb", "title": "Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?", "decision": "Invite to Workshop Track", "meta_review": "The paper introduces an interesting family of two-player zero-sum games with tunable complexity, called Erdos-Selfridge-Spencer games, as a new domain for RL.  The authors report on extensive empirical results using a wide variety of training methods, including supervised learning and several flavors of RL (PPO, A2C, DQN) as well as single-agent vs. multi-agent training.  The reviewers also appear to agree that the method appears to be technically correct, clearly written, and easy to read.\n\nA drawback of the paper is that it does not make a *significant* contribution to the field.  In combing through the reviewer comments, none of them identify a significant contribution.  Even in the text of the paper, the authors do not anywhere claim to have made a significant contribution. As the paper is still interesting, the committee would like to recommend this for the workshop track.\n\nPros:\n        Interesting domain with tunable complexity\n        High-quality extensive empirical results\n        Writing is clear\n\nCons:\n        Lacks a significant contribution\n        Appears to overlook self-play, the dominant RL training paradigm for decades (multiagent training appears to be related but different)\n        Per Reviewer3, \"I remain unconvinced that these games are good general tests for Deep RL\"", "reviews": [{"review_id": "HkCnm-bAb-0", "review_text": "The paper presents Erdos-Selfridge-Spencer games as environments for investigating deep reinforcement learning algorithms. The proposed games are interesting and clearly challenging, but I am not sure what they tell us about the algorithms chosen to test them. There are some clarity issues with the justification and evaluation which undermine the message the authors are trying to make. In particular, I have the following concerns: \u2022 these games have optimal policies that are expressible as a linear model, meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space, then they will perform better than more general algorithms. What does this tell us about the relative merits of each approach? The authors could do more to formally motivate these games as \"difficult\" for any deep learning architecture if possible. \u2022 the authors compare linear models with non-linear models at some point for attacker policies, but it is unclear whether these linear models are able to express the optimal policy. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). \u2022 As the authors state, this paper is an empirical evaluation, and the theorems presented are derived from earlier work. There is possibly too much focus on the proofs of these theorems. \u2022 There are a number of ambiguities and errors which places difficulties on the interpretation (and potential replication) of the experiments. As this is an empirical study, this is the yardstick by which the paper should be judged. In particular, this relates to: \u25e6 The architecture of each of the tested Deep RL methods. \u25e6 What is done to select appropriate tuning parameters of the tested Deep RL methods, if anything. \u25e6 It is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to non-optimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5 \u25e6 Fig 4. right looks like a reward signal, but is labelled Proportion correct. The text is not clear enough to be sure which it is. \u25e6 Fig 4. left and right has 4 methods: rl rewards, rl correct actions, sup rewards, and sup correct actions. The specifics of how these methods are constructed is unclear from the paper. \u25e6 What parts of the evaluation explores how well these methods are able to represent the states (feature/representation learning) and what parts are evaluating the propagation of sparse rewards (the reinforcment learning core)? The authors could be clearer and more targetted with respect to this question. There is value in this work, but in its current state I do not think it is ready for publicaiton. # Detailed notes [p4, end of sec 3] The authors say that the difficulty of the games can be varied with \"continuous changes in potential\", but the potential is derived from the discrete initial game state, so these values are not continuously varying (even though it is possible to adjust them by non-integer amounts). [p4, sec 4.1] \"strategy unevenly partitions the occupied levels...with the proportional difference between the two sets being sampled randomly\" What is meant by this? The proportional difference between the two sets is discussed as if it is a continuous property, but must be chosen from the discrete set of all available partitions. If one partition one is chosen uniformly randomly from all possibly sets A, B (and the potential proportion calculated) then I don't know why it would be written in this way. That suggests that proportions that are closer to 1:1 are chosen more often than \"extreme\" partitions, but how? This feels a little under-justified. \"very different states A, B (uneven potential, disjoint occupied levels)\" Are these states really \"very different\", or at least for the reasons indicated. Later on (Theorem 3) we see how an optimal partition is generated. This chooses a partition where one part contains all pieces in layer (l+1) and above and one part with all pieces in layer (l-1) and below, with layer l being distributed between the two parts. The first part will typically have a slightly lower potential than the other and all layers other than layer l will be disjoint. [p6, Fig 4] The right plot y-limits vary between -1 and 1 so it cannot represent a proportion of correct actions. Also, in the text the authors say: >> The results, shown in Figure 4 are surprising. Reinforcement learning >> is better at playing the game, but does worse at predicting optimal moves. I am not sure which plot shows the playing of the game. Is this the right hand plot? In which case are we looking at rewards? In fact, I am a little confused as to what is being shown here. Is \"sup rewards\" a supervised learning method trained on rewards, or evaluated on rewards, or both? And how is this done. The text is just not clear enough. [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. This relates to the \"surprising\" fact that \"Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.\". I think an important point here is how many training/test examples there are in each bin. If there are more in the range 3-7 moves from the end of the game, than there are outside this range, then the supervised learner will [p8 proof of theorem 3] \"\u03c6(A l+1 ) < 0.5 and \u03c6(A l ) > 0.5.\" Is it true that both these inequalities are strict? \"Since A l only contains pieces from levels K to l + 1\" In fact this should read from levels K to l. \"we can move k < m \u2212 n pieces from A l+1 to A l\" Do the authors mean that we can define a partition A, B where A = A_{l+1} plus some (but not all) elements in level l (A_{l}\\setminus A_{l+1})? \"...such that the potential of the new set equals 0.5\" It will equal exactly 0.5 as suggested, but the authors could make it more precise as to why (there is a value n+k < l (maybe <=l) such that (n+k)*2^{-(K-l+1)}=0.5 (guaranteed). They should also indicate why this then justifies their proof (namely that phi(S0)-0.5 >= 0.5). [p8 paramterising action space] A comment: this doesn't give as much control as the authors suggest. Perhaps the agent should also chose the proportion of elements in layer l to set A. For instance, if there are a large number of elements in l, and or phi(A_{l+1}) is very close to 0.5 (or phi(A_l) is very close to 0.5) then this doesn't give the attacker the opportunity to fine tune the policy to select very good partitions. It is unclear expected level of control that agents have under various conditions (K and starting states). [p9 Fig 8] As the defender's score is functionally determined by the attackers score, it doesn't help to include this on the plot. It just distracts from the signal. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your time in reviewing the paper and your comments ! We \u2019 ve uploaded a new version of the paper based on the feedback , and have addressed specific points below . # # # # # \u201c Optimal policies expressible as a linear model \u201d # # # # # We have added a linear baseline ( Figure 2 and Section 4.1.1 ) showing that these games are hard to learn well with what is theoretically an expressive enough model . This is additional motivation for studying deeper architectures . # # # # \u201c 'incorrect actions ' in the supervised learning evaluations \u201d # # # # The natural formulation of the optimal policy is for the agent to keep the potential in the next state reached as small as possible ; this ensures that the agent preserves the minimax value of the game from all states . This policy corresponds to choosing the set of larger potential to delete ; correspondingly , an incorrect action is one where the agent does not choose the set of larger potential . In the supervised learning setting , we have a starting potential < 1 , so the defender , if playing according to the optimal policy , is guaranteed a win and one of A , B will always have potential < 0.5 . Even if , due to suboptimal play , there was a state where A , B both have potential > 0.5 , the policy that minimizes the potential in the next state would still have a well-defined move , which is to choose the set of larger potential to delete ; we would view this as the correct move under optimal play . # # # # # Figure 4 ( in old version ) now Figure 5 # # # # Setup for Figure 4 : we first train a RL defender agent to play the game , and store all the game trajectories that it sees . We then take each state in the game , and label it with the correct action ( as described in the comment above , i.e.the correct action is picking the ` larger \u2019 set to destroy , which is what the optimal policy does . ) We train a model in a supervised fashion on this labelled dataset We now have edited Figure 4 based on your feedback to make it clearer ( it is Figure 5 in the new version . ) The left pane shows the proportion of correct actions for different K achieved by RL and Supervised Learning . Unsurprisingly , we see that supervised learning is better than RL in terms of number of correct actions . However , RL is better at playing the game : we take a model trained in ( 1 ) supervised fashion ( 2 ) with RL and test it on the environment , and find that RL achieves significantly higher reward , particularly for larger K. We investigate this further in Figure 6 with a new plot ( left pane ) , through studying \u201c fatal mistakes \u201d -- errors made that take the agent from a winning state to a losing state . We find that Supervised Learning is much more prone to fatal mistakes than RL , suggesting a natural basis for the worse performance . # # # # # Hyperparameter Choice for Deep RL architectures # # # # # Aside from experiments to determine the effect of depth and width of architectures , we used minimal hyperparameter tuning . While additional hyperparameter tuning would have likely helped improve performance , the overall conclusions drawn from the paper ( better generalization in multiagent vs single agent , ability of RL to avoid \u201c fatal mistakes \u201d made by supervised learning , decrease in rewards as game difficulty increased ) would not have been affected by hyperparameter changes . # # # # # \u201c As the authors state , this paper is an empirical evaluation , and the theorems presented are derived from earlier work \u201d # # # # # This is not completely the case : Theorem 3 is a theoretical contribution that is original to this paper ; and it is an important component of the paper since without it , training an attacker agent would be intractable . The earlier theorems are explained in detail since the central approach of the paper is based on the linearly expressible potential function and its connection to the optimal policy , and one needs the proofs of these earlier theorems -- not simply their statements -- in order to understand this structure . # # # # # \u201c The authors compare linear models with non-linear models for attacker policies \u201d # # # # # This is incorrect -- we don \u2019 t use linear models for attacker policies . # # # # [ p4 , end of sec 3 ] Continuous changes of potential # # # # The potential changes are indeed due to the discrete initial game state , but for a game with K levels , we can adjust the potential in increments of 2^-K ( e.g.for K=20 , we can adjust the potential in increments of ~0.0000009 ) which seems to be a reasonable approximation to continuous ."}, {"review_id": "HkCnm-bAb-1", "review_text": "This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game. The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. This is used as a sort of ground truth that enables study of the behavior of certain reinforcement learning algorithms (for just one or to both players). An empirical study is performed, measuring the performance of both agents, tuning the difficulty of the game for each agent by changing the starting position of the game. - The comparison of supervised learning vs RL performance is interesting. Is the supervised algorithm only able to implement Markovian policies? Is the RL agent able to find policies with longer-term dependence that it can follow? Is that what is meant by the sentence on page 6 \"We conjecture that reinforcement learning is learning to focus most on moves that matter for winning\"? - Why do you think the defender trained as part of a multiagent setting generalizes better than the single agent defender? Is there something different about the distribution of policies seen by each defender? Quality: The method appears to be technically correct, clearly written, and easy to read. Originality: I believe this is the first use of ESS games to study RL algorithms. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. Impact: I think this is an interesting and creative contribution to studying RL, particularly the use of an easy-to-analyze game in an RL setting. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time in reviewing the paper and your comments ! We \u2019 ve uploaded a new version of the paper based on the feedback , and have addressed specific points below . # # # # # \u201d Supervised Learning vs RL \u201d # # # # # In this setting , both Supervised Learning and RL learn markovian policies because there is no additional dependence on previous states . However , supervised learning is less able to associate important moves with their * * time delayed * * reward . Motivated by your comments , we ran another experiment to explore this ( Figure 6 in the new version ) , where we looked at the number of \u201c fatal mistakes \u201d made by supervised learning vs RL : a fatal mistake being one where the agent makes an irrecoverable error . We found that supervised learning is * much * more prone to fatal mistakes , explaining the worse performance , and validating our conjecture that \u201c reinforcement learning is learning to focus most on moves that matter for winning \u201d # # # # # \u201c Why does multiagent generalize better than single agent defender \u201d # # # # # Training in the multiagent setting likely means the defender sees a greater diversity in the data , resulting in a more robust learned policy . Exploring this further could be interesting future work ! # # # # # Summary # # # # # Thank you for the kind comments ! We also believe that it is valuable and unique contribution to have a challenging game but with linearly expressible optimal policy to study RL , make comparisons to Supervised Learning and explore Generalization ."}, {"review_id": "HkCnm-bAb-2", "review_text": "This paper presents an adversarial combinatorial game: Erdos-Selfridge-Spencer attacker-defender game, with the goal to use it as a benchmark for reinforcement learning. It first compares PPO, A2C, DQN on the task of defending vs. an epsilon-sub-optimal attacker, with varying levels of difficulty. Secondly it compared RL and supervised learning (as they know the optimal actiona at all times). Then it trains (RL) the attacker, and finally trains the attacker and the defender (each a separate model) jointly/concurrently. Various points: - The explanation of the Erdos-Selfridge-Spencer attacker-defender game is clear. - As noted by the authors in section 5, with this featurization, the network only has to learn the weight \"to multiply\" (the multiplication is already the inner product) the feature x_i to be 2^{-(K-i)}, K is fixed for an experiment, and i is the index of the feature, thus can be matched by the index of the weight (vector or diagonal matrix). The defender network has to do this to the features of A and of B, and compare the values; the attacker (with the action space following theorem 3) has to do this for (at most) K progressive partitions. All of this leads me to think that a linear baseline is a must-have in most of the plots, not just Figure 15 in the appendix on one task, moreso as the environment (game) is new. A linear baseline also allows for easy interpretation of what is learned (is it the exact formula of phi(S)?), and can be parametrized to work with varying values of K. - In the experimental section, it seems (due transparent coloring in plots, that I understand to be the minimum and maximum values as said in the text in section 4.1, or is that a confidence interval or standard deviation(s)? In ny case:) that 3 random seeds are sometimes not enough to derive strong conclusions, in particular in Figure 9. - Everything leads me to believe that, up to 6.2, the game is only dealt with as a fixed MDP to be overfit by the model through RL: - there is no generalization from K=k (train) to K > k (test). - sections 6.2, 6.3 and the appendix are more promising but there is only one experiment with potential=1.0 (which is the most interesting operating point for multiagent training) in Figure 8, and potential=0.999 in the appendix. There is no study of the dynamics of attacks/defenses (best responses or Nash equilibrium). Nits: - in Figure 8, there is no need to plot both the attacker and defender rewards. - Figure 3 overwrites x axis of top figures. - Figure 4 right y-axis should be \"average rewards\". It seems the game is easy from a reinforcement learning standpoint, and this is not necessarily a bad thing, but then the experimental study should be more rigorous in term of convergences, error bars, and baselines.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time in reviewing the paper and your comments ! We \u2019 ve uploaded a new version of the paper based on the feedback , and have addressed specific points below . # # # # # \u201c A linear baseline is a must have \u201d # # # # # We \u2019 ve added a new subsection and results ( Figure 2 , section 4.1.1 . ) where we show the performance of linear models that are trained with PPO , A2C and DQN . While theoretically , a linear model is expressive enough to learn the optimal policy , in practice , we see a large improvement in using a deeper model . ( We had also observed this in initial experiments with the environment , but omitted it due to the better performance with the deeper models . ) # # # # # \u201c Three random seeds not enough in particular in Figure 9 \u201d # # # # # # We reran the experiment for the paper v1 figure 9 , now figure 10 , with 8 random seeds . Due to the larger number of seeds , we plotted the mean and show shaded the standard deviation . The figure shows even more clearly the better generalization of multiagent training over single agent training . # # # # # Figure 4 ( in original paper ) now Figure 5 # # # # # Thanks for your comments , we \u2019 ve edited Figure 4 ( old version ) , now figure 5 to make the main message clearer : supervised learning does better on a per move basis , but does worse at playing the game . # # # # # Fatal Mistakes ( Figure 6 ) # # # # # We \u2019 ve also interpreted this result further , and show that this performance difference is likely due to supervised learning making many more fatal mistakes ( Figure 6 ) -- errors in play that can not be recovered from . # # # # # Figure 8 ( original paper ) now Figure 9 # # # # # Thanks for the comment , we \u2019 ve removed the dashed lines to make the performance clearer . # # # # # Multiagent training at potential 1.0 # # # # # There aren \u2019 t many plots in the paper with potential=1.0 for multiagent training because training an attacker agent successfully is much harder than training the defender agent ( larger action space ) , and for larger K , the attacker performs poorly at potential~1.0 , with the defender typically dominating . # # # # # \u201c There is no generalization from K=k train to K > k ( test ) \u201d # # # # # As we understand it , this would involve picking a K_0 for training , and then testing on K > K_0 during test time . But we do n't see a way for this to produce useful insights , since if the model has never seen pieces at levels other than the K_0 levels shown at train time , we can not expect that it will learn the correct weighting for the levels it hasn \u2019 t seen at all . We therefore try the converse of this , where we train on K_0 , and test on K , K < K_0 , and find that decreasing K does not improve play . While this does suggest that the model is overfitting to K_0 , we believe this is an interesting phenomena , highlighting some of the weaknesses of the current methodology . Determining how to adapt our methods to enable this generalization across different K would be exciting to explore in the future . # # # # Summary # # # # We believe we have addressed the main points of your response ( linear baselines , more seeds , additional interpretation plots ) as well as clarified certain points of confusion ( multiagent training at potential 1 , generalization at different levels . ) Our results present a environment that has variable difficulty and is challenging to learn , but also a known , simple optimal policy to compare to . The environment demonstrates many of the typical phenomena observed with RL and provides insights into Supervised Learning vs RL , the effects of multiagent play , and also generalization and catastrophic forgetting . We strongly believe that further work on this environment will help develop more robust RL methods ."}], "0": {"review_id": "HkCnm-bAb-0", "review_text": "The paper presents Erdos-Selfridge-Spencer games as environments for investigating deep reinforcement learning algorithms. The proposed games are interesting and clearly challenging, but I am not sure what they tell us about the algorithms chosen to test them. There are some clarity issues with the justification and evaluation which undermine the message the authors are trying to make. In particular, I have the following concerns: \u2022 these games have optimal policies that are expressible as a linear model, meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space, then they will perform better than more general algorithms. What does this tell us about the relative merits of each approach? The authors could do more to formally motivate these games as \"difficult\" for any deep learning architecture if possible. \u2022 the authors compare linear models with non-linear models at some point for attacker policies, but it is unclear whether these linear models are able to express the optimal policy. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). \u2022 As the authors state, this paper is an empirical evaluation, and the theorems presented are derived from earlier work. There is possibly too much focus on the proofs of these theorems. \u2022 There are a number of ambiguities and errors which places difficulties on the interpretation (and potential replication) of the experiments. As this is an empirical study, this is the yardstick by which the paper should be judged. In particular, this relates to: \u25e6 The architecture of each of the tested Deep RL methods. \u25e6 What is done to select appropriate tuning parameters of the tested Deep RL methods, if anything. \u25e6 It is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to non-optimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5 \u25e6 Fig 4. right looks like a reward signal, but is labelled Proportion correct. The text is not clear enough to be sure which it is. \u25e6 Fig 4. left and right has 4 methods: rl rewards, rl correct actions, sup rewards, and sup correct actions. The specifics of how these methods are constructed is unclear from the paper. \u25e6 What parts of the evaluation explores how well these methods are able to represent the states (feature/representation learning) and what parts are evaluating the propagation of sparse rewards (the reinforcment learning core)? The authors could be clearer and more targetted with respect to this question. There is value in this work, but in its current state I do not think it is ready for publicaiton. # Detailed notes [p4, end of sec 3] The authors say that the difficulty of the games can be varied with \"continuous changes in potential\", but the potential is derived from the discrete initial game state, so these values are not continuously varying (even though it is possible to adjust them by non-integer amounts). [p4, sec 4.1] \"strategy unevenly partitions the occupied levels...with the proportional difference between the two sets being sampled randomly\" What is meant by this? The proportional difference between the two sets is discussed as if it is a continuous property, but must be chosen from the discrete set of all available partitions. If one partition one is chosen uniformly randomly from all possibly sets A, B (and the potential proportion calculated) then I don't know why it would be written in this way. That suggests that proportions that are closer to 1:1 are chosen more often than \"extreme\" partitions, but how? This feels a little under-justified. \"very different states A, B (uneven potential, disjoint occupied levels)\" Are these states really \"very different\", or at least for the reasons indicated. Later on (Theorem 3) we see how an optimal partition is generated. This chooses a partition where one part contains all pieces in layer (l+1) and above and one part with all pieces in layer (l-1) and below, with layer l being distributed between the two parts. The first part will typically have a slightly lower potential than the other and all layers other than layer l will be disjoint. [p6, Fig 4] The right plot y-limits vary between -1 and 1 so it cannot represent a proportion of correct actions. Also, in the text the authors say: >> The results, shown in Figure 4 are surprising. Reinforcement learning >> is better at playing the game, but does worse at predicting optimal moves. I am not sure which plot shows the playing of the game. Is this the right hand plot? In which case are we looking at rewards? In fact, I am a little confused as to what is being shown here. Is \"sup rewards\" a supervised learning method trained on rewards, or evaluated on rewards, or both? And how is this done. The text is just not clear enough. [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. This relates to the \"surprising\" fact that \"Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.\". I think an important point here is how many training/test examples there are in each bin. If there are more in the range 3-7 moves from the end of the game, than there are outside this range, then the supervised learner will [p8 proof of theorem 3] \"\u03c6(A l+1 ) < 0.5 and \u03c6(A l ) > 0.5.\" Is it true that both these inequalities are strict? \"Since A l only contains pieces from levels K to l + 1\" In fact this should read from levels K to l. \"we can move k < m \u2212 n pieces from A l+1 to A l\" Do the authors mean that we can define a partition A, B where A = A_{l+1} plus some (but not all) elements in level l (A_{l}\\setminus A_{l+1})? \"...such that the potential of the new set equals 0.5\" It will equal exactly 0.5 as suggested, but the authors could make it more precise as to why (there is a value n+k < l (maybe <=l) such that (n+k)*2^{-(K-l+1)}=0.5 (guaranteed). They should also indicate why this then justifies their proof (namely that phi(S0)-0.5 >= 0.5). [p8 paramterising action space] A comment: this doesn't give as much control as the authors suggest. Perhaps the agent should also chose the proportion of elements in layer l to set A. For instance, if there are a large number of elements in l, and or phi(A_{l+1}) is very close to 0.5 (or phi(A_l) is very close to 0.5) then this doesn't give the attacker the opportunity to fine tune the policy to select very good partitions. It is unclear expected level of control that agents have under various conditions (K and starting states). [p9 Fig 8] As the defender's score is functionally determined by the attackers score, it doesn't help to include this on the plot. It just distracts from the signal. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your time in reviewing the paper and your comments ! We \u2019 ve uploaded a new version of the paper based on the feedback , and have addressed specific points below . # # # # # \u201c Optimal policies expressible as a linear model \u201d # # # # # We have added a linear baseline ( Figure 2 and Section 4.1.1 ) showing that these games are hard to learn well with what is theoretically an expressive enough model . This is additional motivation for studying deeper architectures . # # # # \u201c 'incorrect actions ' in the supervised learning evaluations \u201d # # # # The natural formulation of the optimal policy is for the agent to keep the potential in the next state reached as small as possible ; this ensures that the agent preserves the minimax value of the game from all states . This policy corresponds to choosing the set of larger potential to delete ; correspondingly , an incorrect action is one where the agent does not choose the set of larger potential . In the supervised learning setting , we have a starting potential < 1 , so the defender , if playing according to the optimal policy , is guaranteed a win and one of A , B will always have potential < 0.5 . Even if , due to suboptimal play , there was a state where A , B both have potential > 0.5 , the policy that minimizes the potential in the next state would still have a well-defined move , which is to choose the set of larger potential to delete ; we would view this as the correct move under optimal play . # # # # # Figure 4 ( in old version ) now Figure 5 # # # # Setup for Figure 4 : we first train a RL defender agent to play the game , and store all the game trajectories that it sees . We then take each state in the game , and label it with the correct action ( as described in the comment above , i.e.the correct action is picking the ` larger \u2019 set to destroy , which is what the optimal policy does . ) We train a model in a supervised fashion on this labelled dataset We now have edited Figure 4 based on your feedback to make it clearer ( it is Figure 5 in the new version . ) The left pane shows the proportion of correct actions for different K achieved by RL and Supervised Learning . Unsurprisingly , we see that supervised learning is better than RL in terms of number of correct actions . However , RL is better at playing the game : we take a model trained in ( 1 ) supervised fashion ( 2 ) with RL and test it on the environment , and find that RL achieves significantly higher reward , particularly for larger K. We investigate this further in Figure 6 with a new plot ( left pane ) , through studying \u201c fatal mistakes \u201d -- errors made that take the agent from a winning state to a losing state . We find that Supervised Learning is much more prone to fatal mistakes than RL , suggesting a natural basis for the worse performance . # # # # # Hyperparameter Choice for Deep RL architectures # # # # # Aside from experiments to determine the effect of depth and width of architectures , we used minimal hyperparameter tuning . While additional hyperparameter tuning would have likely helped improve performance , the overall conclusions drawn from the paper ( better generalization in multiagent vs single agent , ability of RL to avoid \u201c fatal mistakes \u201d made by supervised learning , decrease in rewards as game difficulty increased ) would not have been affected by hyperparameter changes . # # # # # \u201c As the authors state , this paper is an empirical evaluation , and the theorems presented are derived from earlier work \u201d # # # # # This is not completely the case : Theorem 3 is a theoretical contribution that is original to this paper ; and it is an important component of the paper since without it , training an attacker agent would be intractable . The earlier theorems are explained in detail since the central approach of the paper is based on the linearly expressible potential function and its connection to the optimal policy , and one needs the proofs of these earlier theorems -- not simply their statements -- in order to understand this structure . # # # # # \u201c The authors compare linear models with non-linear models for attacker policies \u201d # # # # # This is incorrect -- we don \u2019 t use linear models for attacker policies . # # # # [ p4 , end of sec 3 ] Continuous changes of potential # # # # The potential changes are indeed due to the discrete initial game state , but for a game with K levels , we can adjust the potential in increments of 2^-K ( e.g.for K=20 , we can adjust the potential in increments of ~0.0000009 ) which seems to be a reasonable approximation to continuous ."}, "1": {"review_id": "HkCnm-bAb-1", "review_text": "This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game. The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. This is used as a sort of ground truth that enables study of the behavior of certain reinforcement learning algorithms (for just one or to both players). An empirical study is performed, measuring the performance of both agents, tuning the difficulty of the game for each agent by changing the starting position of the game. - The comparison of supervised learning vs RL performance is interesting. Is the supervised algorithm only able to implement Markovian policies? Is the RL agent able to find policies with longer-term dependence that it can follow? Is that what is meant by the sentence on page 6 \"We conjecture that reinforcement learning is learning to focus most on moves that matter for winning\"? - Why do you think the defender trained as part of a multiagent setting generalizes better than the single agent defender? Is there something different about the distribution of policies seen by each defender? Quality: The method appears to be technically correct, clearly written, and easy to read. Originality: I believe this is the first use of ESS games to study RL algorithms. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. Impact: I think this is an interesting and creative contribution to studying RL, particularly the use of an easy-to-analyze game in an RL setting. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time in reviewing the paper and your comments ! We \u2019 ve uploaded a new version of the paper based on the feedback , and have addressed specific points below . # # # # # \u201d Supervised Learning vs RL \u201d # # # # # In this setting , both Supervised Learning and RL learn markovian policies because there is no additional dependence on previous states . However , supervised learning is less able to associate important moves with their * * time delayed * * reward . Motivated by your comments , we ran another experiment to explore this ( Figure 6 in the new version ) , where we looked at the number of \u201c fatal mistakes \u201d made by supervised learning vs RL : a fatal mistake being one where the agent makes an irrecoverable error . We found that supervised learning is * much * more prone to fatal mistakes , explaining the worse performance , and validating our conjecture that \u201c reinforcement learning is learning to focus most on moves that matter for winning \u201d # # # # # \u201c Why does multiagent generalize better than single agent defender \u201d # # # # # Training in the multiagent setting likely means the defender sees a greater diversity in the data , resulting in a more robust learned policy . Exploring this further could be interesting future work ! # # # # # Summary # # # # # Thank you for the kind comments ! We also believe that it is valuable and unique contribution to have a challenging game but with linearly expressible optimal policy to study RL , make comparisons to Supervised Learning and explore Generalization ."}, "2": {"review_id": "HkCnm-bAb-2", "review_text": "This paper presents an adversarial combinatorial game: Erdos-Selfridge-Spencer attacker-defender game, with the goal to use it as a benchmark for reinforcement learning. It first compares PPO, A2C, DQN on the task of defending vs. an epsilon-sub-optimal attacker, with varying levels of difficulty. Secondly it compared RL and supervised learning (as they know the optimal actiona at all times). Then it trains (RL) the attacker, and finally trains the attacker and the defender (each a separate model) jointly/concurrently. Various points: - The explanation of the Erdos-Selfridge-Spencer attacker-defender game is clear. - As noted by the authors in section 5, with this featurization, the network only has to learn the weight \"to multiply\" (the multiplication is already the inner product) the feature x_i to be 2^{-(K-i)}, K is fixed for an experiment, and i is the index of the feature, thus can be matched by the index of the weight (vector or diagonal matrix). The defender network has to do this to the features of A and of B, and compare the values; the attacker (with the action space following theorem 3) has to do this for (at most) K progressive partitions. All of this leads me to think that a linear baseline is a must-have in most of the plots, not just Figure 15 in the appendix on one task, moreso as the environment (game) is new. A linear baseline also allows for easy interpretation of what is learned (is it the exact formula of phi(S)?), and can be parametrized to work with varying values of K. - In the experimental section, it seems (due transparent coloring in plots, that I understand to be the minimum and maximum values as said in the text in section 4.1, or is that a confidence interval or standard deviation(s)? In ny case:) that 3 random seeds are sometimes not enough to derive strong conclusions, in particular in Figure 9. - Everything leads me to believe that, up to 6.2, the game is only dealt with as a fixed MDP to be overfit by the model through RL: - there is no generalization from K=k (train) to K > k (test). - sections 6.2, 6.3 and the appendix are more promising but there is only one experiment with potential=1.0 (which is the most interesting operating point for multiagent training) in Figure 8, and potential=0.999 in the appendix. There is no study of the dynamics of attacks/defenses (best responses or Nash equilibrium). Nits: - in Figure 8, there is no need to plot both the attacker and defender rewards. - Figure 3 overwrites x axis of top figures. - Figure 4 right y-axis should be \"average rewards\". It seems the game is easy from a reinforcement learning standpoint, and this is not necessarily a bad thing, but then the experimental study should be more rigorous in term of convergences, error bars, and baselines.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time in reviewing the paper and your comments ! We \u2019 ve uploaded a new version of the paper based on the feedback , and have addressed specific points below . # # # # # \u201c A linear baseline is a must have \u201d # # # # # We \u2019 ve added a new subsection and results ( Figure 2 , section 4.1.1 . ) where we show the performance of linear models that are trained with PPO , A2C and DQN . While theoretically , a linear model is expressive enough to learn the optimal policy , in practice , we see a large improvement in using a deeper model . ( We had also observed this in initial experiments with the environment , but omitted it due to the better performance with the deeper models . ) # # # # # \u201c Three random seeds not enough in particular in Figure 9 \u201d # # # # # # We reran the experiment for the paper v1 figure 9 , now figure 10 , with 8 random seeds . Due to the larger number of seeds , we plotted the mean and show shaded the standard deviation . The figure shows even more clearly the better generalization of multiagent training over single agent training . # # # # # Figure 4 ( in original paper ) now Figure 5 # # # # # Thanks for your comments , we \u2019 ve edited Figure 4 ( old version ) , now figure 5 to make the main message clearer : supervised learning does better on a per move basis , but does worse at playing the game . # # # # # Fatal Mistakes ( Figure 6 ) # # # # # We \u2019 ve also interpreted this result further , and show that this performance difference is likely due to supervised learning making many more fatal mistakes ( Figure 6 ) -- errors in play that can not be recovered from . # # # # # Figure 8 ( original paper ) now Figure 9 # # # # # Thanks for the comment , we \u2019 ve removed the dashed lines to make the performance clearer . # # # # # Multiagent training at potential 1.0 # # # # # There aren \u2019 t many plots in the paper with potential=1.0 for multiagent training because training an attacker agent successfully is much harder than training the defender agent ( larger action space ) , and for larger K , the attacker performs poorly at potential~1.0 , with the defender typically dominating . # # # # # \u201c There is no generalization from K=k train to K > k ( test ) \u201d # # # # # As we understand it , this would involve picking a K_0 for training , and then testing on K > K_0 during test time . But we do n't see a way for this to produce useful insights , since if the model has never seen pieces at levels other than the K_0 levels shown at train time , we can not expect that it will learn the correct weighting for the levels it hasn \u2019 t seen at all . We therefore try the converse of this , where we train on K_0 , and test on K , K < K_0 , and find that decreasing K does not improve play . While this does suggest that the model is overfitting to K_0 , we believe this is an interesting phenomena , highlighting some of the weaknesses of the current methodology . Determining how to adapt our methods to enable this generalization across different K would be exciting to explore in the future . # # # # Summary # # # # We believe we have addressed the main points of your response ( linear baselines , more seeds , additional interpretation plots ) as well as clarified certain points of confusion ( multiagent training at potential 1 , generalization at different levels . ) Our results present a environment that has variable difficulty and is challenging to learn , but also a known , simple optimal policy to compare to . The environment demonstrates many of the typical phenomena observed with RL and provides insights into Supervised Learning vs RL , the effects of multiagent play , and also generalization and catastrophic forgetting . We strongly believe that further work on this environment will help develop more robust RL methods ."}}