{"year": "2020", "forum": "HJx7uJStPH", "title": "Music Source Separation in the Waveform Domain", "decision": "Reject", "meta_review": "The paper proposed a waveform-to-waveform music source separation system. Experimental justification shows the proposed model achieved the best SDR among all the existing waveform-to-waveform models, and obtained similar performance to spectrogram based ones. The paper is clearly written and the experimental evaluation and ablation study are thorough. But the main concern is the limited novelty, it is an improvement over the existing Wave-U-Net, it added some changes to the existing model architecture for better modeling the waveform data and compared masking vs. synthesis for music source separation.  ", "reviews": [{"review_id": "HJx7uJStPH-0", "review_text": "The authors present modifications to the state of the art (waveform based) source separation model (Wave-U-Net) and improve the state of the art to be comparable to spectral masking based methods. They clearly outline all the architectural changes they make (GLU nonlinearities, strided upsampling, bidirectional RNN), perform thorough evaluations against strong baselines, and a complete ablation study to demonstrate the value of each component. The paper reads well and quickly informs newcomers to the field with proper motivation and context. For all these reasons I think it should be accepted. One weakness of the paper is that the relative incremental nature of the study, but given the thoroughness and clarity of the experiments, and the non-triviality of the architecture search, I think this is a valuable contribution for the ICLR community. One small suggestion, the language difference between \"upsampled\" convolution and \"transposed\" convolution is a bit confusing. I might suggest focusing on the striding of the convolution vs. bilinear upsampling, as a non-strided convolution can still be \"transposed\" from a standard API point of view in pytorch or tensorflow.", "rating": "8: Accept", "reply_text": "We thank the reviewer for the constructive and positive review . For the discussion of `` upsamplied convolution '' vs `` transposed convolution '' , We clarify that what we mean by `` upsample-convolution '' is a ( e.g. , bilinear ) upsampling method followed by a non-strrided convolution vs a strides . We also made significant updates of the paper ( please see the general comments for a summary of the main changes ) ."}, {"review_id": "HJx7uJStPH-1", "review_text": "This paper suggests using a Unet type architecture to perform end-to-end source separation. They are reporting performance improvement over another architecture which uses Unets architecture in conjunction with Wavenet decoders. They also report a very marginal performance improvement over an STFT based model (open unmix) The improment is 0.02 dB (table 1), and I am not sure if it is statistically significant. Also, there is no mention of algorithms which adaptively learn the basis and then do masking similar to what we do in the STFT domain. A very popular example for this is tasnet, which performs well on speech source separation tasks. I would like to see comparisons with this model. If you think there is a specific reason why not to use adaptive basis approaches such as TASNET, please do let me know. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for pointing out the Tasnet paper . We made experiments with Tasnet and updated the paper accordingly . Please , see the general comments for more details . We also improved the model performance ( mostly the inference procedure that takes an average over random initial shifts of the input ) . The results of Demucs ( and Tasnet ) are now very significantly better than the best models operating on spectrograms ( +0.4 SDR points for Tasnet and +0.3 SDR for Demucs ) ."}, {"review_id": "HJx7uJStPH-2", "review_text": "In this work, the authors consider the task of supervised music source separation, i.e., separating out the components (bass, drums, voice, other) out of a mixed music track. In particular, the work considers the task of supervised source separation where the individual target tracks are available during training time. The main contribution of this work is the improvement of an end-to-end waveform-to-waveform separation models through a number of architectural changes that allow such waveform-to-waveform models to perform comparably with other current state-of-the-art methods that instead operate in the spectrogram domain. Overall, the paper is generally well written and the method is easy to follow. However, I have a couple of concerns about this work, based on which I would rate the work as \u201cweak reject\u201d: 1. The specific architecture proposed in this task does appear to improve performance for this particular task, but it is not clear to me that the conclusions drawn based on the study in this work will be generally applicable to other related tasks. Also, the final model is still only comparable to the spectrogram-based methods overall, and appears to do a slightly worse job in separating the \u2018vocal\u2019 and \u2018other\u2019 tracks than these baseline methods. As such, I\u2019m unsure about the impact of this work. 2. The section describing the evaluation metric SDR wasn\u2019t very clear to me. Perhaps it would be better to just refer back to (Vincent et al., 06) and just describe what SDR captures instead? Or alternatively, more details could be added to explain the computation more clearly. 3. Section 4.2: The authors mention that they multiply each source by +/- 1. I didn\u2019t follow why this is being done. Could the authors please clarify. Minor comments: 1. The notation in Equation 1 is slightly confusing because \u2018s\u2019 is used to index both the mixture and the sources. I think x \\in \\mathcal{D} would be clearer. Similarly I wonder if x_s \\in \\mathbb{R}^C \\times \\mathbb{R}^T would be clearer than x_s \\in \\mathbb{R}^{C,T} 2. The reference (Oord et al., 2017) should be rendered as (van den Oord et al., 2017) 3. The Figure uses K, and S to denote Kernel Width and stride, but this isn\u2019t explicitly mentioned in the text. Perhaps it would be useful, in Section 3.1 to write: \u201c... convolution with kernel width, K=8, and stride, S=4, ...\u201d or something similar, and updating the caption to explain the notation. 4. Section 3.2: \u201cThe decoder is almost the symmetric of the encoder\u201d --> \u201cThe decoder is almost the inverse of the encoder\u201d would perhaps be better? 5. Section 3.2: \u201cThe final layer \u2026 S.C_0 ...\u201d --> \u201cThe final layer \u2026 S * C_0 ...\u201d 6. The notation in Equation 1 and Equation 2 is slightly inconsistent (lower-case vs. upper-case L for the loss function) 7. Section 4.3: \u201cadditionnaly\u201d --> \u201cadditionally\u201d; \u201c32 GB or RAM\u201d --> \u201c32 GB of RAM\u201d ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the constructive comments . The reviewer 's main points were : 1- `` it is not clear to me that the conclusions drawn based on the study in this work will be generally applicable to other related tasks '' We argue that there are two contributions in that paper : a ) showing that now the state-of-the-art is on multi-instrument source separation is obtained by wave-form-to-waveform models ( see general comments for updated results ) b ) showing that approaches based on convolutional encoders/decoders are effective for source separation tasks . Of course , experiments on other source sepraration problems with similar architectures would be a big plus , but it is out of the scope of the current paper and we leave such studies as future work . 2- '' The section describing the evaluation metric SDR wasn \u2019 t very clear to me '' We rephrased it and followed the reviewer suggestion to refer to Vincent et al.and only explain what the metric is supposed to capture 3- multiplying by +1 or -1 corresponds to a global shift of phase . we use that as a form of data augmentation , since the model should be equivalent to such a shift We also updated the paper to correct the reviewer 's minor comments ."}], "0": {"review_id": "HJx7uJStPH-0", "review_text": "The authors present modifications to the state of the art (waveform based) source separation model (Wave-U-Net) and improve the state of the art to be comparable to spectral masking based methods. They clearly outline all the architectural changes they make (GLU nonlinearities, strided upsampling, bidirectional RNN), perform thorough evaluations against strong baselines, and a complete ablation study to demonstrate the value of each component. The paper reads well and quickly informs newcomers to the field with proper motivation and context. For all these reasons I think it should be accepted. One weakness of the paper is that the relative incremental nature of the study, but given the thoroughness and clarity of the experiments, and the non-triviality of the architecture search, I think this is a valuable contribution for the ICLR community. One small suggestion, the language difference between \"upsampled\" convolution and \"transposed\" convolution is a bit confusing. I might suggest focusing on the striding of the convolution vs. bilinear upsampling, as a non-strided convolution can still be \"transposed\" from a standard API point of view in pytorch or tensorflow.", "rating": "8: Accept", "reply_text": "We thank the reviewer for the constructive and positive review . For the discussion of `` upsamplied convolution '' vs `` transposed convolution '' , We clarify that what we mean by `` upsample-convolution '' is a ( e.g. , bilinear ) upsampling method followed by a non-strrided convolution vs a strides . We also made significant updates of the paper ( please see the general comments for a summary of the main changes ) ."}, "1": {"review_id": "HJx7uJStPH-1", "review_text": "This paper suggests using a Unet type architecture to perform end-to-end source separation. They are reporting performance improvement over another architecture which uses Unets architecture in conjunction with Wavenet decoders. They also report a very marginal performance improvement over an STFT based model (open unmix) The improment is 0.02 dB (table 1), and I am not sure if it is statistically significant. Also, there is no mention of algorithms which adaptively learn the basis and then do masking similar to what we do in the STFT domain. A very popular example for this is tasnet, which performs well on speech source separation tasks. I would like to see comparisons with this model. If you think there is a specific reason why not to use adaptive basis approaches such as TASNET, please do let me know. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for pointing out the Tasnet paper . We made experiments with Tasnet and updated the paper accordingly . Please , see the general comments for more details . We also improved the model performance ( mostly the inference procedure that takes an average over random initial shifts of the input ) . The results of Demucs ( and Tasnet ) are now very significantly better than the best models operating on spectrograms ( +0.4 SDR points for Tasnet and +0.3 SDR for Demucs ) ."}, "2": {"review_id": "HJx7uJStPH-2", "review_text": "In this work, the authors consider the task of supervised music source separation, i.e., separating out the components (bass, drums, voice, other) out of a mixed music track. In particular, the work considers the task of supervised source separation where the individual target tracks are available during training time. The main contribution of this work is the improvement of an end-to-end waveform-to-waveform separation models through a number of architectural changes that allow such waveform-to-waveform models to perform comparably with other current state-of-the-art methods that instead operate in the spectrogram domain. Overall, the paper is generally well written and the method is easy to follow. However, I have a couple of concerns about this work, based on which I would rate the work as \u201cweak reject\u201d: 1. The specific architecture proposed in this task does appear to improve performance for this particular task, but it is not clear to me that the conclusions drawn based on the study in this work will be generally applicable to other related tasks. Also, the final model is still only comparable to the spectrogram-based methods overall, and appears to do a slightly worse job in separating the \u2018vocal\u2019 and \u2018other\u2019 tracks than these baseline methods. As such, I\u2019m unsure about the impact of this work. 2. The section describing the evaluation metric SDR wasn\u2019t very clear to me. Perhaps it would be better to just refer back to (Vincent et al., 06) and just describe what SDR captures instead? Or alternatively, more details could be added to explain the computation more clearly. 3. Section 4.2: The authors mention that they multiply each source by +/- 1. I didn\u2019t follow why this is being done. Could the authors please clarify. Minor comments: 1. The notation in Equation 1 is slightly confusing because \u2018s\u2019 is used to index both the mixture and the sources. I think x \\in \\mathcal{D} would be clearer. Similarly I wonder if x_s \\in \\mathbb{R}^C \\times \\mathbb{R}^T would be clearer than x_s \\in \\mathbb{R}^{C,T} 2. The reference (Oord et al., 2017) should be rendered as (van den Oord et al., 2017) 3. The Figure uses K, and S to denote Kernel Width and stride, but this isn\u2019t explicitly mentioned in the text. Perhaps it would be useful, in Section 3.1 to write: \u201c... convolution with kernel width, K=8, and stride, S=4, ...\u201d or something similar, and updating the caption to explain the notation. 4. Section 3.2: \u201cThe decoder is almost the symmetric of the encoder\u201d --> \u201cThe decoder is almost the inverse of the encoder\u201d would perhaps be better? 5. Section 3.2: \u201cThe final layer \u2026 S.C_0 ...\u201d --> \u201cThe final layer \u2026 S * C_0 ...\u201d 6. The notation in Equation 1 and Equation 2 is slightly inconsistent (lower-case vs. upper-case L for the loss function) 7. Section 4.3: \u201cadditionnaly\u201d --> \u201cadditionally\u201d; \u201c32 GB or RAM\u201d --> \u201c32 GB of RAM\u201d ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the constructive comments . The reviewer 's main points were : 1- `` it is not clear to me that the conclusions drawn based on the study in this work will be generally applicable to other related tasks '' We argue that there are two contributions in that paper : a ) showing that now the state-of-the-art is on multi-instrument source separation is obtained by wave-form-to-waveform models ( see general comments for updated results ) b ) showing that approaches based on convolutional encoders/decoders are effective for source separation tasks . Of course , experiments on other source sepraration problems with similar architectures would be a big plus , but it is out of the scope of the current paper and we leave such studies as future work . 2- '' The section describing the evaluation metric SDR wasn \u2019 t very clear to me '' We rephrased it and followed the reviewer suggestion to refer to Vincent et al.and only explain what the metric is supposed to capture 3- multiplying by +1 or -1 corresponds to a global shift of phase . we use that as a form of data augmentation , since the model should be equivalent to such a shift We also updated the paper to correct the reviewer 's minor comments ."}}