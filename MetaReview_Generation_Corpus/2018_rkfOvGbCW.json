{"year": "2018", "forum": "rkfOvGbCW", "title": "Memory-based Parameter Adaptation", "decision": "Accept (Poster)", "meta_review": "the proposed approach nicely incorporates various ideas from recent work into a single meta-learning (or domain adaptation or incremental learning or ...) framework. although better empirical comparison to existing (however recent they are) approaches would have made it stronger, the reviewers all found this submission to be worth publication, with which i agree.", "reviews": [{"review_id": "rkfOvGbCW-0", "review_text": "Overall, the idea of this paper is simple but interesting. Via weighted mean NLL over retrieved neighbors, one can update parameters of output network for a given query input. The MAP interpretation provides a flexible Bayesian explanation about this MbPA. The paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.) Here are some comments: 1 MbPA is built upon memory. How large should it be? Is it efficient to retrieve neighbors for a given query? 2 For each test, how many steps of MbPA do we need in general? Furthermore, it is a bit unfair for me to retrain deep model, based on test inputs. It seems that, you are implicitly using test data to fit model. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Please find below our response and clarifications . 1 ) MbPA is built upon memory . How large should it be ? * The optimal memory size is task dependent , but in general the larger the memory the better . However , performance saturates at a given point . * A nice property of the model ( as shown in the continual and incremental learning setups ) is that performance degrades gracefully as memory size decreases . For continual learning , even storing 1 % of data seen on a task boosts performance significantly . * One important aspect to note is that the smaller the memory the more important it becomes to add regularization to prevent overfitting to the local context , as explained in Section 2.1 . This is the case of the language modeling experiments . * For the ImageNet experiments we show how performance varies with memory size in Fig 6 ( Appendix ) . We will include a similar evaluation for continual learning and language modeling tasks . 2 ) Is it efficient to retrieve neighbors for a given query ? * In this work it is the cost of an exact nearest neighbour search . Which is linear in memory size . We see that the cost of retrieving neighbours is negligible compared to the rest of the model ( eg.the inner optimisation ) . For eg.on PTB language modeling with a cache size of 5000 , the content based lookup is about ~20us , and each step of optimization is ~1ms on one GPU . * Fast approximate knn search can be used , but performance could degrade depending on the recall of the approximate search . This would be a nice direction for future work . * One of the advantages of not querying the memory at training time , is that we avoid this cost . 3 ) For each test , how many steps of MbPA do we need in general ? * This is a hyper-parameter of the model . Across all tasks , we observed that a small number of iteration is sufficient , between 5 and 20 . However , we see noticeable gains with even 1 step . 4 ) Furthermore , it is a bit unfair for me to retrain deep model , based on test inputs . It seems that , you are implicitly using test data to fit model . * Many algorithms have a clean split between train and test . They are unable to adapt to shifts in distribution . We are interested specifically in studying algorithms that are capable of adapting to domain shift . Or , to leverage the temporal correlation during an evaluation episode . * We only do this in the language model example , which deals with quickly adapting to a change in the data distribution at test time . The effect of online adaptation during test time has been long studied in this task , solutions dating back to Dynamic Evaluation ( A. Graves \u2019 thesis ) . Naturally , all these approaches use the test data in a causal way ( as in online learning ) , meaning , only the examples that have been processed are available for training . * Note that we \u2019 re comparing with many models that also use the observed test samples to adapt their predictions . The data seen at each test example is thus consistent across all baselines . We will update the text to take into account all clarifications above ."}, {"review_id": "rkfOvGbCW-1", "review_text": "This paper proposes a non-parametric episodic memory that can be used for the rapid acquisition of new knowledge while preserving the old ones. More specially, it locally adapts the parameters of a network using the episodic memory structure. Strength: + The paper works on a relevant and interesting problem. + The experiment sections are very thorough and I like the fact that the authors selected different tasks to compare their models with. + The paper is well-written except for sections 2 and 3. Weakness and Questions: - Even though the paper addresses the interesting and challenging problem of slow adaption when distribution shifts, their episodic memory is quite similar (if not same as) to the Pritzel et al., 2017. - In addition, as the author mentioned in the text, their model is also similar to the Kirkpatrick et al., 2017, Finn et al., 2017, Krause et al., 2017. That would be great if the author can list \"explicitly\" the contribution of the paper with comparing with those. Right now, the text mentioned some of the similarity but it spreads across different sections and parts. - The proposed model does adaption during the test time, but other papers such as Li & Hoiem, 2016 handles the shift across domain in the train time. Can authors say sth about the motivation behind adaptation during test time vs. training time? - There are some inconsistencies in the text about the parameters and formulations: -- what is second subscript in {v_i}_i? (page 2, 3rd paragraph) -- in Equation 4, what is the difference between x_c and x? -- What happened to $x$ in Eq 5? -- The \"\u2212\" in Eq. 7 doesn't make sense. - Section 2.2, after equation 7, the text is not that clear. - Paper is well beyond the 8-page limit and should be fitted to be 8 pages. - In order to make the experiments reproducible, the paper needs to contain full details (in the appendix) about the setup and hyperparameters of the experiments. Others: Do the authors plan to release the codes? ------------------------------------ ------------------------------------ Update after rebuttal: Thanks for the revised version and answering my concerns. In the revised version, the writing has been improved and the contribution of the paper is more obvious. Given the authors' responses and the changes, I have increased my review score. A couple of comments and questions: 1. Can you explain how/why $x_c$ is replaced by $h_k$ in eq_7? 2. In the same equation (7), how $\\log p(v_k| h_k,\\theta_x, x)$ will be calculated? I have some intuition but not sure. Can you please explain? 3. in equation(8), what happened to $x$ in log p(..)? 4. How figure 2 is plotted? based on a real experiment? if yes, what was the setting? if not, how? 5. It'd be very useful to the community if the authors decide to release their codes. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Please find below our response and clarifications . The comment has been split into two to ensure we are under the comment character limit . 1 ) Even though the paper addresses the interesting and challenging problem of slow adaptation when distribution shifts , their episodic memory is quite similar ( if not same as ) to the Pritzel et al. , 2017 . * Our memory module is indeed essentially the same as that of Pritzel et al , 2017 , differing only on how the keys are obtained . The keys are embeddings computed from our parametric model ( embedding + output networks ) trained directly on the target task instead of relying on gradients through the memory . Note that several other works ( cited in the manuscript ) use very similar memory architectures . We do not claim the memory as one of our contributions , instead , the novelty lies in the use of the memory as a way of enhancing powerful parametric models . We will further clarify this in the text . 2 ) In addition , as the author mentioned in the text , their model is also similar to the Kirkpatrick et al. , 2017 , Finn et al. , 2017 , Krause et al. , 2017 . That would be great if the author can list `` explicitly '' the contribution of the paper with comparing with those . Right now , the text mentioned some of the similarity but it spreads across different sections and parts . * We will include a detailed description of our contributions , and concentrate ( and expand ) the relation with previous work in Section 3 . * The contributions of our work are : ( i ) proposing an architecture for enhancing powerful parametric models to include a fast adaptation mechanism to cope with changes in the task at hand ( ii ) we establish connections of our method with attention mechanisms frequently used for querying memories ( iii ) we present a bayesian interpretation of the method allowing a principled form regularization ( iv ) we evaluate the method in a range of different tasks : continual learning ( pmnist ) , incremental learning ( imagenet ) and data distribution shifts ( language ) , obtaining promising results . * The only similarity with Krause et al.is that we too use a memory buffer in the context of language modelling . Their method of using the memory is via a mixture of experts system to deal with recent words for language models . We do compare to this baseline for our LM experiments , however their method does not deal with the problem of distributional shifts and can not be applied to continual or incremental learning set ups . * Finn et al.devises MAML - a way of doing meta-learning over a distribution of tasks . Both of our methods extend the classic fine-tuning technique used in domain adaptation type of ideas ( e.g.fit a given neural network to a small set of new data ) . Their algorithm aims at learning an easily adaptable set of weights , such that given a small amount of training data for a given task following the training distribution , the fine-tuning procedure would effectively adapt the weights to this particular task . Their work does not use any memory or per-example adaptation and is not based on a continual ( life-long ) learning setting . In contrast , our work , aims at augmenting a powerful neural network with a fine-tuning procedure that is used at inference only . The idea is to enhance the performance of the parametric model while maintaining it 's full training . * EWC , developed in Kirkpatrick et al.2017 is powerful method of doing continual learning across tasks . The algorithm works by learning a new task with an additional loss forcing the model to stay close to the solution found on the previous task . This method makes no use of memory or local adaptation , requiring instead the storing of weights and fisher matrices for each task seen . We compare to this method for our continual learning tasks as a very competitive baseline . MbPA does not rely on storing past weights or fisher matrices . We show comparable performance with even 100 examples stored per task and show how these methods are orthogonal and can be combined . One similarity we do note is that adding a regularization term to the local loss of MbPA can be seen as a local version or approximation of the EWC loss term - i.e.forcing the model to stay close to the solution found at training time ."}, {"review_id": "rkfOvGbCW-2", "review_text": "This article introduces a new method to improve neural network performances on tasks ranging from continual learning (non-stationary target distribution, appearance of new classes, adaptation to new tasks, etc) to better handling of class imbalance, via a hybrid architecture between nearest neighbours and neural net. After an introduction summarizing their goal, the authors introduce their Model-based parameter adaptation: this hybrid architecture enriches classical deep architectures with a non-parametric \u201cepisodic\u201d memory, which is filled at training time with (possibly learned) encodings of training examples and then polled at inference time to refine the neural network parameters with a few steps of gradient in a direction determined by the closest neighbours in memory to the input being processed. The authors justify this inference-time SGD update with three different interpretations: one linked in Maximum A Posteriori optimization, another to Elastic Weight Regularisation (the current state of the art in continual learning), and one generalising attention mechanisms (although to be honest that later was more elusive to this reviewer). The mandatory literature review on the abundant recent uses of memory in neural networks is then followed by experiments on continual learning tasks involving permuted MNIST tasks, ImageNET incremental inclusion of classes, ImageNet unbalanced, and two language modeling tasks. This is an overall very interesting idea, which has the merit of being rather simple in its execution and can be combined with many other methods: it is fully compatible with any optimiser (e.g. ADAM) and can be tacked on top of EWC (which the authors do). The justification is clear, the examples reasonably thorough. It is a very solid paper, which this reviewer believes to be of real interest to the ICLR community. The following important clarifications from the authors could make it even better: * Algorithm 1 in its current form seems to imply an infinite memory, which the experiments make clear is not the case. Therefore: how does the algorithm decide what entries to discard when the memory fills up? * In most non-trivial settings, the parameter $gamma$ of the encoding is learned, and therefore older entries in the memory lose any ability to be compared to more recent encodings. How do the authors handle this obsolescence of the memory, other than the trivial scheme of relying on KNN to only match recent entries? * Because gamma needs to be \u201crecent\u201d, this means \u201ctheta\u201d is also recent: could the authors give a good intuition on how the two sets of parameters can evolve at different enough timescales to really make the episodic memory relevant? Is it anything else than relying on the fact that the lower levels of a neural net converge before the upper levels? * Table 1: could the authors explain why the pre-trained Parametric (and then Mixture) models have the best AUC in the low-data regime, whereas MbPA was designed very much to be superior in such regimes? * Paragraph below equation (5), page 3: why not including the regularisation term, whereas the authors just went to great pain to explain it? Rationale? Not including it is also akin to using an improper non-information prior on theta^x independent of theta, which is quite a strong choice to be made \u201cby default\u201d. * The extra complexity of choosing the learning rate alpha_M and the number of MpAB steps is worrying this reviewer somewhat. In practice, in Section 4.1the authors explain using grid search to tune the parameters. Is this reviewer correct in understanding that this search is done across all tasks, as opposed to only the first task? And if so, doesn\u2019t this grid search introduce an information leak by bringing information from the whole pre-determined set of task, therefore undermining the very \u201ccontinuous learning\u201d aim? How do the algorithm performs if the grid search is done only on the first task? * Figure 3: the text could clarify that the accuracy is measured across all tasks seen so far. It would be interesting to add a figure (in the Appendix) showing the evolution of the accuracy *per task*, not just the aggregated accuracy. * In the related works linking neural networks to encoded episodic memory, the authors might want to include the stream of research on HMAX of Anselmi et al 2014 (https://arxiv.org/pdf/1311.4158.pdf) , Leibo et al 2015 (https://arxiv.org/abs/1512.08457), and Blundell et al 2016 (https://arxiv.org/pdf/1606.04460.pdf ). Minor typos: * Figure 4: the title of the key says \u201cNew/Old\u201d but then the lines read, in order, \u201cOld\u201d then \u201cNew\u201d -- it would be nicer to have them in the same order. * Section 5: missing period between \"ephemeral gradient modifications\" and \"Further\". * Section 4.2, parenthesis should be \"perform well across all 1000 classes\", not \"all 100 classes\". With the above clarifications, this article could become a very remarked contribution. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . Please find below our response and clarifications . The responses have been split to ensure we are under the ICLR comment character limit . 1 ) Algorithm 1 in its current form seems to imply an infinite memory , which the experiments make clear is not the case . Therefore : how does the algorithm decide what entries to discard when the memory fills up ? * In the current implementation we simply treat the memory as a circular buffer , in which we overwrite the oldest data as the memory gets full . We will clarify this on the text . * Deciding what to store ( or overwrite ) is indeed a very interesting question that we did not explore and will address in future work . We evaluated a few heuristics ( e.g.storing only examples with high training loss ) that did not perform better than the circular buffer described above . 2 ) In most non-trivial settings , the parameter $ gamma $ of the encoding is learned , and therefore older entries in the memory lose any ability to be compared to more recent encodings . How do the authors handle this obsolescence of the memory , other than the trivial scheme of relying on KNN to only match recent entries ? * Having a stable ( or slow changing ) network is important for being able to have long term recall . This could be justified ( as the reviewer mentions ) by the fact that lower level parameters converge faster than those in the higher part of the network . Hence , it is inevitable some memory obsolescence in the beginning of training . This is also the case on humans as infant amnesia could be explained as memories stored with an old ( not consolidated ) network that can not be recovered later in life . We will include a short comment further clarifying this important point . * An alternative approach would be to rely on replay of raw data ( e.g.store the input images from pixels ) . A downside is that , unlike internal activations ( embeddings ) , replaying raw data requires a large amount of storage . However many artificial systems do it ( e.g.DQN for RL ) . If we store raw data , we could still base our look-ups on a distance in the embedding space in order to obtain a semantic ( more relevant ) metric . We would replay the memories to prevent catastrophic forgetting and periodically recompute the embeddings to keep them up to date . We did not implement this variant . 3 ) Table 1 : could the authors explain why the pre-trained Parametric ( and then Mixture ) models have the best AUC in the low-data regime , whereas MbPA was designed very much to be superior in such regimes ? * Note that this happens only for the classes that were used during pre-training . The result makes sense : the initial parametric model performs very well on the classes that was pre-trained on . The memory is initially empty , so adapting the predictions of parametric model ( via MbPA or the mixture model ) using few examples slightly degrades its performance in the beginning . This quickly changes as more examples are collected . * On the other hand , for the new classes , relying on the memories massively improves performance even when few examples have been stored . 4 ) Paragraph below equation ( 5 ) , page 3 : why not including the regularisation term , whereas the authors just went to great pain to explain it ? Rationale ? Not including it is also akin to using an improper non-information prior on theta^x independent of theta , which is quite a strong choice to be made \u201c by default \u201d . * We wrote it in this way for ease of explanation and developed later in Section 2.1 , as we only talk about the bayesian interpretation then . We will change the text accordingly ."}], "0": {"review_id": "rkfOvGbCW-0", "review_text": "Overall, the idea of this paper is simple but interesting. Via weighted mean NLL over retrieved neighbors, one can update parameters of output network for a given query input. The MAP interpretation provides a flexible Bayesian explanation about this MbPA. The paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.) Here are some comments: 1 MbPA is built upon memory. How large should it be? Is it efficient to retrieve neighbors for a given query? 2 For each test, how many steps of MbPA do we need in general? Furthermore, it is a bit unfair for me to retrain deep model, based on test inputs. It seems that, you are implicitly using test data to fit model. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Please find below our response and clarifications . 1 ) MbPA is built upon memory . How large should it be ? * The optimal memory size is task dependent , but in general the larger the memory the better . However , performance saturates at a given point . * A nice property of the model ( as shown in the continual and incremental learning setups ) is that performance degrades gracefully as memory size decreases . For continual learning , even storing 1 % of data seen on a task boosts performance significantly . * One important aspect to note is that the smaller the memory the more important it becomes to add regularization to prevent overfitting to the local context , as explained in Section 2.1 . This is the case of the language modeling experiments . * For the ImageNet experiments we show how performance varies with memory size in Fig 6 ( Appendix ) . We will include a similar evaluation for continual learning and language modeling tasks . 2 ) Is it efficient to retrieve neighbors for a given query ? * In this work it is the cost of an exact nearest neighbour search . Which is linear in memory size . We see that the cost of retrieving neighbours is negligible compared to the rest of the model ( eg.the inner optimisation ) . For eg.on PTB language modeling with a cache size of 5000 , the content based lookup is about ~20us , and each step of optimization is ~1ms on one GPU . * Fast approximate knn search can be used , but performance could degrade depending on the recall of the approximate search . This would be a nice direction for future work . * One of the advantages of not querying the memory at training time , is that we avoid this cost . 3 ) For each test , how many steps of MbPA do we need in general ? * This is a hyper-parameter of the model . Across all tasks , we observed that a small number of iteration is sufficient , between 5 and 20 . However , we see noticeable gains with even 1 step . 4 ) Furthermore , it is a bit unfair for me to retrain deep model , based on test inputs . It seems that , you are implicitly using test data to fit model . * Many algorithms have a clean split between train and test . They are unable to adapt to shifts in distribution . We are interested specifically in studying algorithms that are capable of adapting to domain shift . Or , to leverage the temporal correlation during an evaluation episode . * We only do this in the language model example , which deals with quickly adapting to a change in the data distribution at test time . The effect of online adaptation during test time has been long studied in this task , solutions dating back to Dynamic Evaluation ( A. Graves \u2019 thesis ) . Naturally , all these approaches use the test data in a causal way ( as in online learning ) , meaning , only the examples that have been processed are available for training . * Note that we \u2019 re comparing with many models that also use the observed test samples to adapt their predictions . The data seen at each test example is thus consistent across all baselines . We will update the text to take into account all clarifications above ."}, "1": {"review_id": "rkfOvGbCW-1", "review_text": "This paper proposes a non-parametric episodic memory that can be used for the rapid acquisition of new knowledge while preserving the old ones. More specially, it locally adapts the parameters of a network using the episodic memory structure. Strength: + The paper works on a relevant and interesting problem. + The experiment sections are very thorough and I like the fact that the authors selected different tasks to compare their models with. + The paper is well-written except for sections 2 and 3. Weakness and Questions: - Even though the paper addresses the interesting and challenging problem of slow adaption when distribution shifts, their episodic memory is quite similar (if not same as) to the Pritzel et al., 2017. - In addition, as the author mentioned in the text, their model is also similar to the Kirkpatrick et al., 2017, Finn et al., 2017, Krause et al., 2017. That would be great if the author can list \"explicitly\" the contribution of the paper with comparing with those. Right now, the text mentioned some of the similarity but it spreads across different sections and parts. - The proposed model does adaption during the test time, but other papers such as Li & Hoiem, 2016 handles the shift across domain in the train time. Can authors say sth about the motivation behind adaptation during test time vs. training time? - There are some inconsistencies in the text about the parameters and formulations: -- what is second subscript in {v_i}_i? (page 2, 3rd paragraph) -- in Equation 4, what is the difference between x_c and x? -- What happened to $x$ in Eq 5? -- The \"\u2212\" in Eq. 7 doesn't make sense. - Section 2.2, after equation 7, the text is not that clear. - Paper is well beyond the 8-page limit and should be fitted to be 8 pages. - In order to make the experiments reproducible, the paper needs to contain full details (in the appendix) about the setup and hyperparameters of the experiments. Others: Do the authors plan to release the codes? ------------------------------------ ------------------------------------ Update after rebuttal: Thanks for the revised version and answering my concerns. In the revised version, the writing has been improved and the contribution of the paper is more obvious. Given the authors' responses and the changes, I have increased my review score. A couple of comments and questions: 1. Can you explain how/why $x_c$ is replaced by $h_k$ in eq_7? 2. In the same equation (7), how $\\log p(v_k| h_k,\\theta_x, x)$ will be calculated? I have some intuition but not sure. Can you please explain? 3. in equation(8), what happened to $x$ in log p(..)? 4. How figure 2 is plotted? based on a real experiment? if yes, what was the setting? if not, how? 5. It'd be very useful to the community if the authors decide to release their codes. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Please find below our response and clarifications . The comment has been split into two to ensure we are under the comment character limit . 1 ) Even though the paper addresses the interesting and challenging problem of slow adaptation when distribution shifts , their episodic memory is quite similar ( if not same as ) to the Pritzel et al. , 2017 . * Our memory module is indeed essentially the same as that of Pritzel et al , 2017 , differing only on how the keys are obtained . The keys are embeddings computed from our parametric model ( embedding + output networks ) trained directly on the target task instead of relying on gradients through the memory . Note that several other works ( cited in the manuscript ) use very similar memory architectures . We do not claim the memory as one of our contributions , instead , the novelty lies in the use of the memory as a way of enhancing powerful parametric models . We will further clarify this in the text . 2 ) In addition , as the author mentioned in the text , their model is also similar to the Kirkpatrick et al. , 2017 , Finn et al. , 2017 , Krause et al. , 2017 . That would be great if the author can list `` explicitly '' the contribution of the paper with comparing with those . Right now , the text mentioned some of the similarity but it spreads across different sections and parts . * We will include a detailed description of our contributions , and concentrate ( and expand ) the relation with previous work in Section 3 . * The contributions of our work are : ( i ) proposing an architecture for enhancing powerful parametric models to include a fast adaptation mechanism to cope with changes in the task at hand ( ii ) we establish connections of our method with attention mechanisms frequently used for querying memories ( iii ) we present a bayesian interpretation of the method allowing a principled form regularization ( iv ) we evaluate the method in a range of different tasks : continual learning ( pmnist ) , incremental learning ( imagenet ) and data distribution shifts ( language ) , obtaining promising results . * The only similarity with Krause et al.is that we too use a memory buffer in the context of language modelling . Their method of using the memory is via a mixture of experts system to deal with recent words for language models . We do compare to this baseline for our LM experiments , however their method does not deal with the problem of distributional shifts and can not be applied to continual or incremental learning set ups . * Finn et al.devises MAML - a way of doing meta-learning over a distribution of tasks . Both of our methods extend the classic fine-tuning technique used in domain adaptation type of ideas ( e.g.fit a given neural network to a small set of new data ) . Their algorithm aims at learning an easily adaptable set of weights , such that given a small amount of training data for a given task following the training distribution , the fine-tuning procedure would effectively adapt the weights to this particular task . Their work does not use any memory or per-example adaptation and is not based on a continual ( life-long ) learning setting . In contrast , our work , aims at augmenting a powerful neural network with a fine-tuning procedure that is used at inference only . The idea is to enhance the performance of the parametric model while maintaining it 's full training . * EWC , developed in Kirkpatrick et al.2017 is powerful method of doing continual learning across tasks . The algorithm works by learning a new task with an additional loss forcing the model to stay close to the solution found on the previous task . This method makes no use of memory or local adaptation , requiring instead the storing of weights and fisher matrices for each task seen . We compare to this method for our continual learning tasks as a very competitive baseline . MbPA does not rely on storing past weights or fisher matrices . We show comparable performance with even 100 examples stored per task and show how these methods are orthogonal and can be combined . One similarity we do note is that adding a regularization term to the local loss of MbPA can be seen as a local version or approximation of the EWC loss term - i.e.forcing the model to stay close to the solution found at training time ."}, "2": {"review_id": "rkfOvGbCW-2", "review_text": "This article introduces a new method to improve neural network performances on tasks ranging from continual learning (non-stationary target distribution, appearance of new classes, adaptation to new tasks, etc) to better handling of class imbalance, via a hybrid architecture between nearest neighbours and neural net. After an introduction summarizing their goal, the authors introduce their Model-based parameter adaptation: this hybrid architecture enriches classical deep architectures with a non-parametric \u201cepisodic\u201d memory, which is filled at training time with (possibly learned) encodings of training examples and then polled at inference time to refine the neural network parameters with a few steps of gradient in a direction determined by the closest neighbours in memory to the input being processed. The authors justify this inference-time SGD update with three different interpretations: one linked in Maximum A Posteriori optimization, another to Elastic Weight Regularisation (the current state of the art in continual learning), and one generalising attention mechanisms (although to be honest that later was more elusive to this reviewer). The mandatory literature review on the abundant recent uses of memory in neural networks is then followed by experiments on continual learning tasks involving permuted MNIST tasks, ImageNET incremental inclusion of classes, ImageNet unbalanced, and two language modeling tasks. This is an overall very interesting idea, which has the merit of being rather simple in its execution and can be combined with many other methods: it is fully compatible with any optimiser (e.g. ADAM) and can be tacked on top of EWC (which the authors do). The justification is clear, the examples reasonably thorough. It is a very solid paper, which this reviewer believes to be of real interest to the ICLR community. The following important clarifications from the authors could make it even better: * Algorithm 1 in its current form seems to imply an infinite memory, which the experiments make clear is not the case. Therefore: how does the algorithm decide what entries to discard when the memory fills up? * In most non-trivial settings, the parameter $gamma$ of the encoding is learned, and therefore older entries in the memory lose any ability to be compared to more recent encodings. How do the authors handle this obsolescence of the memory, other than the trivial scheme of relying on KNN to only match recent entries? * Because gamma needs to be \u201crecent\u201d, this means \u201ctheta\u201d is also recent: could the authors give a good intuition on how the two sets of parameters can evolve at different enough timescales to really make the episodic memory relevant? Is it anything else than relying on the fact that the lower levels of a neural net converge before the upper levels? * Table 1: could the authors explain why the pre-trained Parametric (and then Mixture) models have the best AUC in the low-data regime, whereas MbPA was designed very much to be superior in such regimes? * Paragraph below equation (5), page 3: why not including the regularisation term, whereas the authors just went to great pain to explain it? Rationale? Not including it is also akin to using an improper non-information prior on theta^x independent of theta, which is quite a strong choice to be made \u201cby default\u201d. * The extra complexity of choosing the learning rate alpha_M and the number of MpAB steps is worrying this reviewer somewhat. In practice, in Section 4.1the authors explain using grid search to tune the parameters. Is this reviewer correct in understanding that this search is done across all tasks, as opposed to only the first task? And if so, doesn\u2019t this grid search introduce an information leak by bringing information from the whole pre-determined set of task, therefore undermining the very \u201ccontinuous learning\u201d aim? How do the algorithm performs if the grid search is done only on the first task? * Figure 3: the text could clarify that the accuracy is measured across all tasks seen so far. It would be interesting to add a figure (in the Appendix) showing the evolution of the accuracy *per task*, not just the aggregated accuracy. * In the related works linking neural networks to encoded episodic memory, the authors might want to include the stream of research on HMAX of Anselmi et al 2014 (https://arxiv.org/pdf/1311.4158.pdf) , Leibo et al 2015 (https://arxiv.org/abs/1512.08457), and Blundell et al 2016 (https://arxiv.org/pdf/1606.04460.pdf ). Minor typos: * Figure 4: the title of the key says \u201cNew/Old\u201d but then the lines read, in order, \u201cOld\u201d then \u201cNew\u201d -- it would be nicer to have them in the same order. * Section 5: missing period between \"ephemeral gradient modifications\" and \"Further\". * Section 4.2, parenthesis should be \"perform well across all 1000 classes\", not \"all 100 classes\". With the above clarifications, this article could become a very remarked contribution. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . Please find below our response and clarifications . The responses have been split to ensure we are under the ICLR comment character limit . 1 ) Algorithm 1 in its current form seems to imply an infinite memory , which the experiments make clear is not the case . Therefore : how does the algorithm decide what entries to discard when the memory fills up ? * In the current implementation we simply treat the memory as a circular buffer , in which we overwrite the oldest data as the memory gets full . We will clarify this on the text . * Deciding what to store ( or overwrite ) is indeed a very interesting question that we did not explore and will address in future work . We evaluated a few heuristics ( e.g.storing only examples with high training loss ) that did not perform better than the circular buffer described above . 2 ) In most non-trivial settings , the parameter $ gamma $ of the encoding is learned , and therefore older entries in the memory lose any ability to be compared to more recent encodings . How do the authors handle this obsolescence of the memory , other than the trivial scheme of relying on KNN to only match recent entries ? * Having a stable ( or slow changing ) network is important for being able to have long term recall . This could be justified ( as the reviewer mentions ) by the fact that lower level parameters converge faster than those in the higher part of the network . Hence , it is inevitable some memory obsolescence in the beginning of training . This is also the case on humans as infant amnesia could be explained as memories stored with an old ( not consolidated ) network that can not be recovered later in life . We will include a short comment further clarifying this important point . * An alternative approach would be to rely on replay of raw data ( e.g.store the input images from pixels ) . A downside is that , unlike internal activations ( embeddings ) , replaying raw data requires a large amount of storage . However many artificial systems do it ( e.g.DQN for RL ) . If we store raw data , we could still base our look-ups on a distance in the embedding space in order to obtain a semantic ( more relevant ) metric . We would replay the memories to prevent catastrophic forgetting and periodically recompute the embeddings to keep them up to date . We did not implement this variant . 3 ) Table 1 : could the authors explain why the pre-trained Parametric ( and then Mixture ) models have the best AUC in the low-data regime , whereas MbPA was designed very much to be superior in such regimes ? * Note that this happens only for the classes that were used during pre-training . The result makes sense : the initial parametric model performs very well on the classes that was pre-trained on . The memory is initially empty , so adapting the predictions of parametric model ( via MbPA or the mixture model ) using few examples slightly degrades its performance in the beginning . This quickly changes as more examples are collected . * On the other hand , for the new classes , relying on the memories massively improves performance even when few examples have been stored . 4 ) Paragraph below equation ( 5 ) , page 3 : why not including the regularisation term , whereas the authors just went to great pain to explain it ? Rationale ? Not including it is also akin to using an improper non-information prior on theta^x independent of theta , which is quite a strong choice to be made \u201c by default \u201d . * We wrote it in this way for ease of explanation and developed later in Section 2.1 , as we only talk about the bayesian interpretation then . We will change the text accordingly ."}}