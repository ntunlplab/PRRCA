{"year": "2017", "forum": "SkxKPDv5xl", "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model", "decision": "Accept (Poster)", "meta_review": "The reviewers were unanimous in their agreement about accepting this paper.\n Pros \n - novel formulation that don't require sample by sample prediction\n - interesting results\n \n Cons\n - lack of details / explanation in the mathematical formulation / motivations for the model.", "reviews": [{"review_id": "SkxKPDv5xl-0", "review_text": "The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version. The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for reviewing our paper . It is much appreciated . Please find the top latest comment for the changelog of the recent revision ."}, {"review_id": "SkxKPDv5xl-1", "review_text": "The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines. It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion. The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable. The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of \"r\" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative. Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome. Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening. Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain. Other remarks: - upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise? - The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well? - Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear AnonReviewer3 , Thank you for the detailed review and comments . I will try to answer your points one by one in the following sections : > `` a subsequence length of 512 samples ( ~32 ms ) is sufficient to get good results '' . This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion . - To make sure that the model can properly model larger context it is necessary to carry the information from the past . In this model this is mostly due to passing the relevant information ( e.g.phoneme-like sounds ) from one 32ms chunk to the next one ( stateful RNN ) during Truncated BPTT . It is that some part of the memory of each tier is responsible to keep track of the broader context . > Lacking details e.g.which value of `` r '' is used for the different tiers , how many units per layer , ... - Added to the recent version . Please see the recent comment for changelog . Specifically , frame size values are FS_1 = FS_2 = 2 and FS_3 = 8 . 1024 GRU units per layer were showed the best performance and each tier has one layer RNN . > `` I think a comparison in terms of computational cost , training time and number of parameters would also be very informative . '' - Comparison : ( Our re-implementation of WaveNet ) vs. ( SampleRNN ) Number of parameters ( 1M ) vs. ( 21M ) * Approximated training time memory footprint ( 10GB ) vs. ( 5GB ) Mini-batch size ( 8 ) vs ( 128 ) Training time ( 7 days ) vs. ( 3 days ) * We made sure to make the WaveNet model as big as possible while still fitting in one ( large ) GPU per model . > `` Surprisingly , Table 1 shows a vanilla RNN ( LSTM ) substantially outperforming this model [ WaveNet ] in terms of likelihood , which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best . One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent . Similarly , Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset . This raises questions about the implementation of the latter . Some discussion about this result and whether the authors expected it or not would be very welcome . '' - When we were replicating their results , although we were trying our best to replicate them , we had no frame of reference to compare to except for the unconditional samples . The best speculation as why LSTM is getting better numbers is similar to what I have stated in response to your first question : it is a combination of TBPTT and stateful RNN when training . This seems to be a great approximation of training on really long sequences in audio data . > Why 2-tier is outperforming the 3-tier model for music ? - We did not expect that , but for any dataset and architecture structure , there is an optimal depth . Considering that this is a deep RNN ( which introduces a form of recurrent depth , here very large ) and the hypothesis that it is difficult to train such architectures in the first place , it is possible that alternative training procedures could yield better results with a deeper model . > `` This choice of upsampling method is not motivated . Why not just use linear interpolation or nearest neighbour upsampling ? What is the advantage of learning this operation ? Do n't the r linear projections end up learning largely the same thing , give or take some noise ? '' - The same argument is applicable to upsampling in CNNs [ 1 ] but it works better there . So we decided to borrow the same technique . There should be no problem using other methods . However if this module helps the final model , even slightly , and takes away the burden of upsampling from each individual tier , there is no reason not to use it . In addition , for small upsampling ratio this will learn the same thing give or take some noise but as the ratio grows there will be more structure which can be captured by a learned operation . > The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used . This is in contrast to Wavenet , for which an 8-bit mu-law encoding was used , and this supposedly improves the audio fidelity of the samples . Did you try this as well ? - Mu-law was recently tested . This is a harder representation to model and takes more time to train on . We can get good results that you can find in ( https : //soundcloud.com/samplernn/sets/mu-law ) . By the time WaveNet came out we were done by most of the experiments so due to time constraints we did not attempt to re-do everything . > Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input , without any reference to prior work that made the same observation . A reference is given in 2.1.1 , but it should probably be moved up a bit to avoid giving the impression that this is a novel observation . - Citation was moved up . Thanks for noting . [ 1 ] Dosovitskiy , Alexey , Jost Springenberg , Maxim Tatarchenko , and Thomas Brox . `` Learning to Generate Chairs , Tables and Cars with Convolutional Networks . '' ( 2016 ) ."}, {"review_id": "SkxKPDv5xl-2", "review_text": "Pros: The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time. RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples. Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work. Cons: The paper is lacking equations that detail the model. This can be remedied in the camera-ready version. The paper is lacking detailed explanations of the modeling choices: - It's not clear why an MLP is used in the bottom layer instead of (another) RNN. - It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation. As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable. Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear AnonReviewer2 , Thanks for reading the paper . I would like to respond to the cons section of the review . - Re.lacking equations : we are in the process of updating the paper . - Re.MLP vs RNN : Modelling nearby samples is relatively easy and adding another RNN is making the model more complex and slower to train ; there is no need to run an RNN if it is getting input from couple of past samples , there is no long-term dependency there . Besides that , if a memoryless module can cope with that , there is no reason theoretically to think that a RNN can not accomplish the same . - Re.upsampling : > Why ca n't the high-frequency module just read the same ( repeated ) state of low frequency module ? The high-frequency module can certainly do that ; we would consider this still a sort of upsampling ( nearest-neighbor ) . > And if you want to have a transformation there , why make it r linear projections ? This seems both a redundant and weak modeling choice . One argument for r separate projections ( c_1=W_1 * h_t , ... , c_r=W_r * h_t ) is that the RNN , roughly , should be modeling p ( x_ { t+1 } ... x_ { t+r } |x0 ... xt ) . Considering the case where the lower module is an MLP which is run r times with the same state vector as input , there 's no easy way for it to know which part of that state vector to pay attention to ( i.e.what the current timestep is ) otherwise . Also , our approach of upsampling with r linear projections is exactly equivalent to upsampling by adding zeros and then applying a linear convolution . This is sometimes called `` perforated '' upsampling in the context of CNNs . It was first demonstrated to work well in [ 1 ] , and is a fairly common upsampling technique . [ 1 ] Dosovitskiy , Alexey , Jost Springenberg , Maxim Tatarchenko , and Thomas Brox . `` Learning to Generate Chairs , Tables and Cars with Convolutional Networks . '' ( 2016 ) ."}], "0": {"review_id": "SkxKPDv5xl-0", "review_text": "The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version. The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for reviewing our paper . It is much appreciated . Please find the top latest comment for the changelog of the recent revision ."}, "1": {"review_id": "SkxKPDv5xl-1", "review_text": "The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines. It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion. The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable. The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of \"r\" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative. Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome. Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening. Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain. Other remarks: - upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise? - The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well? - Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear AnonReviewer3 , Thank you for the detailed review and comments . I will try to answer your points one by one in the following sections : > `` a subsequence length of 512 samples ( ~32 ms ) is sufficient to get good results '' . This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion . - To make sure that the model can properly model larger context it is necessary to carry the information from the past . In this model this is mostly due to passing the relevant information ( e.g.phoneme-like sounds ) from one 32ms chunk to the next one ( stateful RNN ) during Truncated BPTT . It is that some part of the memory of each tier is responsible to keep track of the broader context . > Lacking details e.g.which value of `` r '' is used for the different tiers , how many units per layer , ... - Added to the recent version . Please see the recent comment for changelog . Specifically , frame size values are FS_1 = FS_2 = 2 and FS_3 = 8 . 1024 GRU units per layer were showed the best performance and each tier has one layer RNN . > `` I think a comparison in terms of computational cost , training time and number of parameters would also be very informative . '' - Comparison : ( Our re-implementation of WaveNet ) vs. ( SampleRNN ) Number of parameters ( 1M ) vs. ( 21M ) * Approximated training time memory footprint ( 10GB ) vs. ( 5GB ) Mini-batch size ( 8 ) vs ( 128 ) Training time ( 7 days ) vs. ( 3 days ) * We made sure to make the WaveNet model as big as possible while still fitting in one ( large ) GPU per model . > `` Surprisingly , Table 1 shows a vanilla RNN ( LSTM ) substantially outperforming this model [ WaveNet ] in terms of likelihood , which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best . One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent . Similarly , Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset . This raises questions about the implementation of the latter . Some discussion about this result and whether the authors expected it or not would be very welcome . '' - When we were replicating their results , although we were trying our best to replicate them , we had no frame of reference to compare to except for the unconditional samples . The best speculation as why LSTM is getting better numbers is similar to what I have stated in response to your first question : it is a combination of TBPTT and stateful RNN when training . This seems to be a great approximation of training on really long sequences in audio data . > Why 2-tier is outperforming the 3-tier model for music ? - We did not expect that , but for any dataset and architecture structure , there is an optimal depth . Considering that this is a deep RNN ( which introduces a form of recurrent depth , here very large ) and the hypothesis that it is difficult to train such architectures in the first place , it is possible that alternative training procedures could yield better results with a deeper model . > `` This choice of upsampling method is not motivated . Why not just use linear interpolation or nearest neighbour upsampling ? What is the advantage of learning this operation ? Do n't the r linear projections end up learning largely the same thing , give or take some noise ? '' - The same argument is applicable to upsampling in CNNs [ 1 ] but it works better there . So we decided to borrow the same technique . There should be no problem using other methods . However if this module helps the final model , even slightly , and takes away the burden of upsampling from each individual tier , there is no reason not to use it . In addition , for small upsampling ratio this will learn the same thing give or take some noise but as the ratio grows there will be more structure which can be captured by a learned operation . > The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used . This is in contrast to Wavenet , for which an 8-bit mu-law encoding was used , and this supposedly improves the audio fidelity of the samples . Did you try this as well ? - Mu-law was recently tested . This is a harder representation to model and takes more time to train on . We can get good results that you can find in ( https : //soundcloud.com/samplernn/sets/mu-law ) . By the time WaveNet came out we were done by most of the experiments so due to time constraints we did not attempt to re-do everything . > Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input , without any reference to prior work that made the same observation . A reference is given in 2.1.1 , but it should probably be moved up a bit to avoid giving the impression that this is a novel observation . - Citation was moved up . Thanks for noting . [ 1 ] Dosovitskiy , Alexey , Jost Springenberg , Maxim Tatarchenko , and Thomas Brox . `` Learning to Generate Chairs , Tables and Cars with Convolutional Networks . '' ( 2016 ) ."}, "2": {"review_id": "SkxKPDv5xl-2", "review_text": "Pros: The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time. RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples. Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work. Cons: The paper is lacking equations that detail the model. This can be remedied in the camera-ready version. The paper is lacking detailed explanations of the modeling choices: - It's not clear why an MLP is used in the bottom layer instead of (another) RNN. - It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation. As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable. Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear AnonReviewer2 , Thanks for reading the paper . I would like to respond to the cons section of the review . - Re.lacking equations : we are in the process of updating the paper . - Re.MLP vs RNN : Modelling nearby samples is relatively easy and adding another RNN is making the model more complex and slower to train ; there is no need to run an RNN if it is getting input from couple of past samples , there is no long-term dependency there . Besides that , if a memoryless module can cope with that , there is no reason theoretically to think that a RNN can not accomplish the same . - Re.upsampling : > Why ca n't the high-frequency module just read the same ( repeated ) state of low frequency module ? The high-frequency module can certainly do that ; we would consider this still a sort of upsampling ( nearest-neighbor ) . > And if you want to have a transformation there , why make it r linear projections ? This seems both a redundant and weak modeling choice . One argument for r separate projections ( c_1=W_1 * h_t , ... , c_r=W_r * h_t ) is that the RNN , roughly , should be modeling p ( x_ { t+1 } ... x_ { t+r } |x0 ... xt ) . Considering the case where the lower module is an MLP which is run r times with the same state vector as input , there 's no easy way for it to know which part of that state vector to pay attention to ( i.e.what the current timestep is ) otherwise . Also , our approach of upsampling with r linear projections is exactly equivalent to upsampling by adding zeros and then applying a linear convolution . This is sometimes called `` perforated '' upsampling in the context of CNNs . It was first demonstrated to work well in [ 1 ] , and is a fairly common upsampling technique . [ 1 ] Dosovitskiy , Alexey , Jost Springenberg , Maxim Tatarchenko , and Thomas Brox . `` Learning to Generate Chairs , Tables and Cars with Convolutional Networks . '' ( 2016 ) ."}}