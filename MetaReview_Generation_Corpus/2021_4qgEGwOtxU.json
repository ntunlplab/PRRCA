{"year": "2021", "forum": "4qgEGwOtxU", "title": "Importance and Coherence: Methods for Evaluating Modularity in Neural Networks", "decision": "Reject", "meta_review": "The paper present an approach for defending for, and search for, 'modularity' in neural networks, as a step to better interpretations of their functional structure. This is an interesting, and highly original approach, as recognised by the reviewers. However,  there was also some discussion about what exactly can be learned from the derived clusters/modules, and if and how they will lead to a better understanding of neural networks, or provide concrete ways of improving them.  While the authors addressed some issues during the review process, and provided additional results, the consensus (of all three reviewers) was finally that the paper did not reach the quality standards required by ICLR. I share this view-- the paper provides a refreshing perspective, but I still am not convinced that I see a clear, compelling 'use case' for their approach.  ", "reviews": [{"review_id": "4qgEGwOtxU-0", "review_text": "The manuscript introduces an approach , based on importance and coherence , for evaluation whether a partitioning of a network exhibits modular characteristics . Importance refers to how crucial is a neuron , or set of neurons , to the performance of a network on a given task , e.g.classification . Coherence refers to how consistently the neuron ( s ) in question are related to specific features . Experiments are conducted by considering sets of neurons identified via a spectral clustering algorithm . The manuscript proposes a method to verify to what extent a partitioning of a network follows modular characteristics . To a good extent the proposed method is grounded on proper theoretical foundations which is highly desirable . The only part where this can not be fully verified is its dependence on the method from [ Anonymous , 2021 ] which can not be verified . My main concerns with the manuscripts are the following : - When conducting spectral clustering , the number of clusters is set to 12 . Is there a procedure to set this value in a principled manner ? is there an indication on the effect of this parameter ? the manuscript would benefit from analyzing the effect of this parameter in the observation made on the reported experiments ? - Visualizations discussed in Sec.3.1.1 ( Fig.1 ) are quite subjective . While in some cases some patterns are indeed visible , in other cases it is hard to make sense of what is being presented . Is it possible to evaluate the produced visualizations in a more objective manner ? In recent years , several methods ( Bau et al , 2016 , Oramas et al.ICLR'19 , Yang and Kim , arXiv:1907.09701 ) for quantitative evaluation of methods for visual interpretation and explanation have been processed . Perhaps one of these could be adopted in the manuscript with the goal of objectively evaluating the visualizations/explanations presented in Fig.1.- In some cases design decision are made that seem to favor observations expected in some experiments . For instance , in order to favor clusterability small MLPs are pruned , to improve visualization MLPs are trained with dropout ; and other factors relevant to the proposed method . Therefore my question by ensuring that some of these properties , e.g.cleaner cluster , clearer visualizations , do n't you favor the measurement capabilities of the proposed importance/coherence metrics ? - When analyzing the `` importance '' metric on the lesion tests ( Sec.3.2 ) there are new conditions that are applied to the clusters being considered in the analysis , e.g.the size of the cluster , minimum effect of the cluster on accuracy , etc . Keeping this present , my questions are : i ) Were these conditions also applied when analyzing `` coherence '' , and ii ) why these type of condition were not applied in the experiments of Sec.3.1 ? Ideally , some level of consistency is expected among the experiments . Otherwise it is hard to assess properly the origin of observations made on the results of the experiments . - At the end of Sec.4 , it is stated that the conducted experiments , combining spectral clustering with feature visualization , highlight the usefulness of combining multiple interpretability methods in order to build an improved set of tools for rigorously understanding systems . However , from the observations made on the experiments I do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models ( networks ) . - Very related , one paragraph later , it is stated that having modular networks is useful both for interpretability and for building better models . However , from the content of the manuscript it is not clear how having a modular network/representation does contribute with the two listed aspects . - Significant parts of the manuscript are delegated to the supplementary material . In addition , the third part of the proposed method , i.e.intrinsic partition evaluation , is part of another manuscript [ Anonymous,2021 ] that does not seem to be published . For these reasons , to a good extent , the manuscript is not self-contained .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank reviewer 1 for the constructive feedback . ( 1 ) Re : \u201c The only part where this can not be fully verified is its dependence on the method from [ Anonymous , 2021 ] which can not be verified ... the third part of the proposed method , i.e.intrinsic partition evaluation , is part of another manuscript [ Anonymous,2021 ] that does not seem to be published \u201d While the other paper investigates spectral clustering in depth , this paper uses spectral clustering as an example of a partition-generating procedure . In that sense , this paper can stand alone . Regarding intrinsic partition evaluation , we mention this for completeness , but it is not relevant to the experiments we conduct in this paper . However , we are hoping to improve clarity here . Do you have any improvements in mind ? For example , is it confusing to mention Intrinsic Partition Evaluation in section 2 ? ( 2 ) Re : \u201c When conducting spectral clustering , the number of clusters is set to 12. \u201d We will change section 2 to clarify this . We wanted a number greater than 10 ( the number of classes in MNIST/CIFAR ) , but small relative to the number of filters in the networks ' layers . Note that finding results with k=12 in networks ranging from the MNIST to ImageNet scale suggests that results are robust to different choices of $ k $ . We have explored using higher $ k $ , but much larger values lead to slow runtimes for ImageNet experiments . We are currently running lesion experiments for k=8 and k=18 which we will report here soon . ( 3 ) Re : \u201c Visualizations discussed in Sec.3.1.1 ( Fig.1 ) are quite subjective ... Is it possible to evaluate the produced visualizations in a more objective manner ? \u201d We intended section 3.1.1. to provide simple visual examples before the more rigorous quantitative results in the rest of section 3 . Note that in 3.1.2 and 3.2 , we also perform experiments on the same types of networks which are visualized in 3.1.1 . ( 4 ) Re : \u201c In some cases design decision are made that seem to favor observations expected in some experiments ... do n't you favor the measurement capabilities of the proposed importance/coherence metrics \u201d Would you be able to further clarify if/how this is an issue ? We do not see an inherent issue with tuning our training process for clearer results . We will clarify in section 2 our approach with pruning and dropout . Put more simply , we use pruning in MLPs and no other networks . We use dropout in the regularized CIFAR-10 VGGs and also in the MLPs used for correlation-based visualization ( with the exception of the network trained on halves-diff data in figure 4d ) . Also bear in mind that the baselines we compare all results to are random sub-clusters , not subclusters of unregularized networks . ( 5 ) Re : \u201c When analyzing the `` importance '' metric on the lesion tests ( Sec.3.2 ) there are new conditions that are applied to the clusters being considered in the analysis ... i ) Were these conditions also applied when analyzing `` coherence '' , and ii ) why these type of condition were not applied in the experiments of Sec.3.1 ? \u201d We will rewrite some of these explanations for clarity . In all experiments with importance and coherence from which Table 1 is produced , we omit sub-clusters that are extremely small or large ( details in supplement ) , but we do not otherwise select for clusters . We ONLY introduce new conditions for classifying sub-clusters for the sake of producing Figure 2 and providing an example taxonomy that demonstrates the diversity among sub-clusters in size , importance , and importance relative to random sub-clusters . ( 6 ) Re : \u201c it is not clear how having a modular network/representation does contribute with the two listed aspects \u201d Our argument is that this work motivates building networks that better lend themselves to modular deconstructions because this level of abstraction is useful for interpretability . The argument that modularity leads to better models made via discussion and citation of works in the final paragraph of related works . ( 7 ) Re : \u201c I do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models. \u201d Our argument is that the advantage of combining clustering with other interpretability methods is that it provides a method of generating partitions based on weight connectivity which does n't involve activations or gradients . The key thing that weight-based clustering brings to the table is non-redundancy with methods based on runtime analysis ."}, {"review_id": "4qgEGwOtxU-1", "review_text": "This paper explores the application of spectral clustering methods to assess modular organization in the emergent architecture of deep networks . In particular , sub-modules identified by spectral clustering are evaluated in terms of `` importance '' and `` coherence '' , two metrics defined by the authors with the goal of capturing how crucial the neurons in the sub-module are to the classification accuracy , and how consistent their activation is across input and output patterns . Overall , the paper addresses important questions related to the way structural properties in a deep network might support the emergence of functional properties , which is a key issue given the relatively poor theoretical understanding we have about these self-organizing systems . The paper is comprehensible , though the general structure and the writing could be improved to improve readability . For example , in Section 3 it is not immediately clear how the importance and coherence metrics relate to the specific technique adopted for feature visualization , or to the lesioning method applied . The \u201c Related Work \u201d section should be moved at the beginning of the paper , and the contribution should be better framed in the context of other existing approaches . Although I appreciate the wide range of networks tested by the authors , I think that the way results are presented does not easily allow to establish whether we can identify a series of robust findings that are valid across all architectures / tasks , or whether specific cases entail peculiar findings . It would also be valuable to better assess the relationship between modularity and regularization techniques , such as dropout , L2 and pruning : I think that this is a very important point that should deserve further investigation , since it could give important insights about the role of regularizers in shaping the final network architecture . Finally , it would strengthen the paper if the proposed method is put in relation to other approaches that have proven effective to address this kind of question . This would greatly improve the robustness of the results , since the current baseline is basically constituted by a comparison with random sub-modules . For example , it would be nice to see if clustering coefficient and average path length ( as defined in [ 1 ] ) can provide useful information also for the analyses proposed by the authors . Note that in [ 1 ] the authors investigated deep networks with similar architectures ( MLPs , CNNs , ResNets ) trained on similar tasks ( CIFAR-10 , ImageNet ) . Other comments : - Pg . 3 : the technique used to visualize a sub-cluster by creating an aggregate measure of the learned features can be discussed in relation to the method based on Earth-mover distance proposed by [ 2 ] , where the authors also discuss other graph-based metrics that might be useful in the present setting . - Could the \u201c intersection information \u201d approach presented in [ 3 ] can be exploited also in the analyses of the sub-modules detected by the spectral clustering ? Note that in [ 3 ] the authors also investigate \u201c lesion tests \u201d by means of interventional techniques , which would make that approach very interesting as a further benchmark . - What is the rationale for setting the number of clusters to 12 ? If this value is not theoretically motivated , further analyses should show that the results are robust to variations in this value . - When comparing \u201c true sub-clusters \u201d with \u201c random sub-clusters \u201d , a useful control analysis would be to create random sub-clusters by matching some connectivity property ( e.g. , same average strength , clustering coefficient and/or average path length ) . - Regarding the gradient-based method discussed in Olah et al . ( 2017 ) , it would be useful to have some dispersion measure over the final optimization score , in order to better assess whether all neurons in the sub-cluster where in fact similarly activated by the optimized image . - It would be interesting to include as baseline some analysis on randomly connected networks , since it has been shown that subgraphs in large random networks can in fact support accurate task performance even without ever training the weight values [ 4 ] . - Where the p-values corrected for multiple comparisons ? - The authors consider \u201c modularity as an organizing principle to achieve mechanistic transparency \u201d . Though I sympathize with this statement , I guess there are several cases where modular systems ( or in general systems with localized representations ) can develop complex emergent dynamics that still prevent interpretability . References [ 1 ] J . You , J. Leskovec , K. He , and S. Xie , \u201c Graph Structure of Neural Networks , \u201d in International Conference on Machine Learning , 2020 . [ 2 ] A. Testolin , M. Piccolini , and S. Suweis , \u201c Deep learning systems as complex networks , \u201d J . Complex Networks , vol . 0000 , no.1 , pp.1\u201321 , Jun . 2019 . [ 3 ] S. Panzeri , C. D. Harvey , E. Piasini , P. E. Latham , and T. Fellin , \u201c Cracking the Neural Code for Sensory Perception by Combining Statistics , Intervention , and Behavior , \u201d Neuron , vol . 93 , no.3 , pp.491\u2013507 , 2017 . [ 4 ] V. Ramanujan , M. Wortsman , A. Kembhavi , A. Farhadi , and M. Rastegari , \u201c What \u2019 s Hidden in a Randomly Weighted Neural Network ? , \u201d In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 11893-11902 . 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank reviewer 4 for the constructive feedback . ( 1 ) Re : \u201c the way results are presented does not easily allow to establish whether we can identify a series of robust findings that are valid across all architectures / tasks. \u201d It is true that we find evidence of modular sub-clusters in some networks but not in others . However ( a ) we don \u2019 t believe that finding different trends across different networks should be disqualifying , and ( b ) while we show that spectral clustering often identifies modular sub-clusters , the the most central purpose of this paper is to develop ways to measure and use importance and coherence for modularity evaluation . ( 2 ) Re : \u201c It would also be valuable to better assess the relationship between modularity and regularization techniques , such as dropout , L2 and pruning. \u201d While we do not study this thoroughly , note that our comparisons between the regularized and unregularized CIFAR-10 VGGs relate to this . Nonetheless , we believe that further work focusing on this question and other ways to promote modularity architecturally or via training procedure will be valuable . ( 3 ) Re : \u201c it would strengthen the paper if the proposed method is put in relation to other approaches that have proven effective to address this kind of question\u2026.For example , it would be nice to see if clustering coefficient and average path length ... \u201d and \u201c When comparing \u201c true sub-clusters \u201d with \u201c random sub-clusters \u201d , a useful control analysis would be to create random sub-clusters by matching some connectivity property \u201d Overall , we agree that additional controls would be interesting , but we do argue that controlling for location ( layer ) and size are sufficient to evaluate importance and coherence . In this paper , we do not aim to compare partition generation methods . As a side note , spectral clustering has a random-walk interpretation and is very closely related with commute-time/average path length . See von Luxburg ( 2007 ) . ( 4 ) Re : Earth-mover distance and intersection information . We agree that earth mover distance could be used as a measure of coherence in 3.1.1 . Other methods from 3.1.2 and 3.2 could as well . But we intended for 3.1.1. to give visual examples preceding more comprehensive results in 3.1.2 and 3.2 . We also agree that intersection information could be used as a measure of coherence . However , to our understanding , because intersection information measures the association between a stimulus and output , this is closely related to our experiments with lesions . Is this consistent with your understanding ? ( 5 ) Re : \u201c What is the rationale for setting the number of clusters to 12 ? \u201d We will change section 2 to clarify this . We wanted a number greater than 10 ( the number of classes in MNIST/CIFAR ) , but small relative to the number of filters in the networks ' layers . Note that finding results with k=12 in networks ranging from the MNIST to ImageNet scale suggests that these phenomena are robust to different choices of the ratio between $ k $ and the size of the network . We have explored using higher $ k $ , but much larger values lead to slow runtimes for ImageNet experiments . We are currently running lesion experiments for k=8 and k=18 which we will report here soon . ( 6 ) Re : \u201c Regarding the gradient-based method discussed in Olah et al . ( 2017 ) , it would be useful to have some dispersion measure over the final optimization score \u201d We agree this may be interesting . We are running some experiments to get a sense of dispersion and will hopefully be able to report this soon . ( 7 ) Re : \u201c It would be interesting to include as baseline some analysis on randomly connected networks. \u201d We agree that this would be interesting for understanding how much sub-cluster importance and coherence result from training , but we do not see a strong motivation for this involving the development of interpretability methods . ( 8 ) Re : Where the p-values corrected for multiple comparisons ? No.But we agree that also reporting multiple testing corrections in the Supplement would be helpful . We are working on this now and will report results soon . ( 9 ) Re : \u201c I guess there are several cases where modular systems ( or in general systems with localized representations ) can develop complex emergent dynamics that still prevent interpretability. \u201d We agree . We believe that future work will be needed for clarifying the association between interpretability and modularity . Note that this is a _motivation_ for our work though because developing methods for identifying modules and validating their boundaries is a prerequisite to answering this question . [ 1 ] Ulrike von Luxburg . A tutorial on spectral clustering . Statistics and computing , 17 ( 4 ) :395\u2013416 , 2007 ."}, {"review_id": "4qgEGwOtxU-2", "review_text": "The authors identify putative clusters of units/neurons in deep networks using spectral clustering on a graph defined by synaptic weights . The authors then argue that these structurally defined clusters of neurons have similar * functional representations * . Finding interpretable relationships between weight matrices and functional modules is challenging , and the authors should be applauded for attempting to tackle this challenging problem that few research groups are devoting energy to . Despite these positive notes , I have reservations about the presentation and results of the paper . My main concerns are : ( 1 ) The results are largely qualitative and anecdotal . In figure 1 , for example , the authors show slightly higher contrast in their identified clusters than random clusters . The results are limited to black and white images ( MNIST and fashion-MNIST ) , and not all examples look great . Only 2 figures are shown in the main paper , with a lot of other details shoved into the supplement . Thus , the writing and presentation could be improved to highlight the most exciting and surprising findings . ( 2 ) The results crucially rely on a second paper which was concurrently submitted and ca n't be reviewed because it is anonymized . The results shown in this paper are thin and qualitative ( see point 1 ) , so in my view these two paper should be combined into a single paper which overall might tell a more comprehensive and compelling story . ( 3 ) The paper does not generate testable predictions or practical insights that could be used by used by practitioners . The only takeaway point for me was that some neurons / units show correlated representations , which is arguably already known ( e.g.Csordas et al 2020 ) . How to exploit this modularity to develop human-interpretable explanations of network function remains unclear to me .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank reviewer 3 for the constructive feedback . ( 1 ) Re : \u201c Largely qualitative and anecdotal ... The results shown in this paper are thin and qualitative \u201d Section 3.1.1 and Figure 1 are indeed qualitative . Our goal for them was to provide visual examples with a simple dataset like MNIST . More importantly though , 3.1.2 and 3.2 are built entirely around quantitative results . We would like to emphasize that having quantitative results based on statistical hypothesis testing at all is actually something which sets this paper apart from related work using similar methods such as Bau et al . ( 2017 ) , Watanabe ( 2019 ) , and Carter et al. ( 2019 ) . ( 2 ) Re : \u201c The results [ in Figure 1 ] are limited to black and white images ( MNIST and fashion-MNIST ) , and not all examples look great. \u201d What we want to emphasize in Figure 1is that these sub-clusters , despite being based only on weights , systematically exhibit coherence w.r.t.testing data . We find this interesting because it shows that we can uncover runtime properties of a network without making queries to it . And we believe this is useful because interpreting units at the sub-cluster level gives us a flexible level of abstraction and pairs well with data-sensitive interpretability methods . ( 3 ) Re : \u201c Only 2 figures are shown in the main paper , with a lot of other details shoved into the supplement . Thus , the writing and presentation could be improved ... \u201d Would it be possible to provide more detailed suggestions about what figures and details from the supplement should be emphasized more and included in the main paper ? For example , should we incorporate hypothesis testing details from the supplement into the main paper ? ( 4 ) Re : \u201c The results crucially rely on a second paper which was concurrently submitted and ca n't be reviewed because it is anonymized. \u201d While the other paper investigates spectral clustering in depth , this paper only relies on spectral clustering as an example of a partition-generating procedure for neurons . In that sense , this paper stands alone . However , we are hoping to improve clarity here . What kind of details would you like to see ? For example , is it confusing to mention Intrinsic Partition Evaluation in section 2 ? ( 5 ) Re : \u201c The paper does not generate testable predictions or practical insights ... \u201d We agree that our main contribution is not a set of methods that will be immediately useful to practitioners , but this was not our goal . Our approach focused on understanding phenomena and introducing tools which evaluate whether a given neuron partitioning reflects the network functionality , and thus supports the existence of modularity . ( 6 ) Re : \u201c The only takeaway point for me was that some neurons / units show correlated representations \u201d This is not how we would characterize our contributions . While a cluster of units being highly correlated with one another w.r.t.a data distribution would be sufficient to show input coherence , it would not be necessary , nor would it establish importance or output coherence . Note that it is also novel that spectral clustering can find sets of neurons which exhibit these properties at all . We argue that the main takeaways from this paper are the concepts of importance and coherence , statistical methods for measuring them , and demonstrating that spectral clustering , which takes only the network \u2019 s weights as an input , often reveals important and input coherent sub-clusters . [ 1 ] David Bau , Bolei Zhou , Aditya Khosla , Aude Oliva , and Antonio Torralba . Network dissection : Quantifying interpretability of deep visual representations . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 6541\u20136549 , 2017 . [ 2 ] Chihiro Watanabe . Interpreting layered neural networks via hierarchical modular representation . In International Conference on Neural Information Processing , pp . 376\u2013388.Springer , 2019 . [ 3 ] Shan Carter , Zan Armstrong , Ludwig Schubert , Ian Johnson , and Chris Olah . Activation atlas . Distill , 4 ( 3 ) : e15 , 2019 ."}], "0": {"review_id": "4qgEGwOtxU-0", "review_text": "The manuscript introduces an approach , based on importance and coherence , for evaluation whether a partitioning of a network exhibits modular characteristics . Importance refers to how crucial is a neuron , or set of neurons , to the performance of a network on a given task , e.g.classification . Coherence refers to how consistently the neuron ( s ) in question are related to specific features . Experiments are conducted by considering sets of neurons identified via a spectral clustering algorithm . The manuscript proposes a method to verify to what extent a partitioning of a network follows modular characteristics . To a good extent the proposed method is grounded on proper theoretical foundations which is highly desirable . The only part where this can not be fully verified is its dependence on the method from [ Anonymous , 2021 ] which can not be verified . My main concerns with the manuscripts are the following : - When conducting spectral clustering , the number of clusters is set to 12 . Is there a procedure to set this value in a principled manner ? is there an indication on the effect of this parameter ? the manuscript would benefit from analyzing the effect of this parameter in the observation made on the reported experiments ? - Visualizations discussed in Sec.3.1.1 ( Fig.1 ) are quite subjective . While in some cases some patterns are indeed visible , in other cases it is hard to make sense of what is being presented . Is it possible to evaluate the produced visualizations in a more objective manner ? In recent years , several methods ( Bau et al , 2016 , Oramas et al.ICLR'19 , Yang and Kim , arXiv:1907.09701 ) for quantitative evaluation of methods for visual interpretation and explanation have been processed . Perhaps one of these could be adopted in the manuscript with the goal of objectively evaluating the visualizations/explanations presented in Fig.1.- In some cases design decision are made that seem to favor observations expected in some experiments . For instance , in order to favor clusterability small MLPs are pruned , to improve visualization MLPs are trained with dropout ; and other factors relevant to the proposed method . Therefore my question by ensuring that some of these properties , e.g.cleaner cluster , clearer visualizations , do n't you favor the measurement capabilities of the proposed importance/coherence metrics ? - When analyzing the `` importance '' metric on the lesion tests ( Sec.3.2 ) there are new conditions that are applied to the clusters being considered in the analysis , e.g.the size of the cluster , minimum effect of the cluster on accuracy , etc . Keeping this present , my questions are : i ) Were these conditions also applied when analyzing `` coherence '' , and ii ) why these type of condition were not applied in the experiments of Sec.3.1 ? Ideally , some level of consistency is expected among the experiments . Otherwise it is hard to assess properly the origin of observations made on the results of the experiments . - At the end of Sec.4 , it is stated that the conducted experiments , combining spectral clustering with feature visualization , highlight the usefulness of combining multiple interpretability methods in order to build an improved set of tools for rigorously understanding systems . However , from the observations made on the experiments I do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models ( networks ) . - Very related , one paragraph later , it is stated that having modular networks is useful both for interpretability and for building better models . However , from the content of the manuscript it is not clear how having a modular network/representation does contribute with the two listed aspects . - Significant parts of the manuscript are delegated to the supplementary material . In addition , the third part of the proposed method , i.e.intrinsic partition evaluation , is part of another manuscript [ Anonymous,2021 ] that does not seem to be published . For these reasons , to a good extent , the manuscript is not self-contained .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank reviewer 1 for the constructive feedback . ( 1 ) Re : \u201c The only part where this can not be fully verified is its dependence on the method from [ Anonymous , 2021 ] which can not be verified ... the third part of the proposed method , i.e.intrinsic partition evaluation , is part of another manuscript [ Anonymous,2021 ] that does not seem to be published \u201d While the other paper investigates spectral clustering in depth , this paper uses spectral clustering as an example of a partition-generating procedure . In that sense , this paper can stand alone . Regarding intrinsic partition evaluation , we mention this for completeness , but it is not relevant to the experiments we conduct in this paper . However , we are hoping to improve clarity here . Do you have any improvements in mind ? For example , is it confusing to mention Intrinsic Partition Evaluation in section 2 ? ( 2 ) Re : \u201c When conducting spectral clustering , the number of clusters is set to 12. \u201d We will change section 2 to clarify this . We wanted a number greater than 10 ( the number of classes in MNIST/CIFAR ) , but small relative to the number of filters in the networks ' layers . Note that finding results with k=12 in networks ranging from the MNIST to ImageNet scale suggests that results are robust to different choices of $ k $ . We have explored using higher $ k $ , but much larger values lead to slow runtimes for ImageNet experiments . We are currently running lesion experiments for k=8 and k=18 which we will report here soon . ( 3 ) Re : \u201c Visualizations discussed in Sec.3.1.1 ( Fig.1 ) are quite subjective ... Is it possible to evaluate the produced visualizations in a more objective manner ? \u201d We intended section 3.1.1. to provide simple visual examples before the more rigorous quantitative results in the rest of section 3 . Note that in 3.1.2 and 3.2 , we also perform experiments on the same types of networks which are visualized in 3.1.1 . ( 4 ) Re : \u201c In some cases design decision are made that seem to favor observations expected in some experiments ... do n't you favor the measurement capabilities of the proposed importance/coherence metrics \u201d Would you be able to further clarify if/how this is an issue ? We do not see an inherent issue with tuning our training process for clearer results . We will clarify in section 2 our approach with pruning and dropout . Put more simply , we use pruning in MLPs and no other networks . We use dropout in the regularized CIFAR-10 VGGs and also in the MLPs used for correlation-based visualization ( with the exception of the network trained on halves-diff data in figure 4d ) . Also bear in mind that the baselines we compare all results to are random sub-clusters , not subclusters of unregularized networks . ( 5 ) Re : \u201c When analyzing the `` importance '' metric on the lesion tests ( Sec.3.2 ) there are new conditions that are applied to the clusters being considered in the analysis ... i ) Were these conditions also applied when analyzing `` coherence '' , and ii ) why these type of condition were not applied in the experiments of Sec.3.1 ? \u201d We will rewrite some of these explanations for clarity . In all experiments with importance and coherence from which Table 1 is produced , we omit sub-clusters that are extremely small or large ( details in supplement ) , but we do not otherwise select for clusters . We ONLY introduce new conditions for classifying sub-clusters for the sake of producing Figure 2 and providing an example taxonomy that demonstrates the diversity among sub-clusters in size , importance , and importance relative to random sub-clusters . ( 6 ) Re : \u201c it is not clear how having a modular network/representation does contribute with the two listed aspects \u201d Our argument is that this work motivates building networks that better lend themselves to modular deconstructions because this level of abstraction is useful for interpretability . The argument that modularity leads to better models made via discussion and citation of works in the final paragraph of related works . ( 7 ) Re : \u201c I do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models. \u201d Our argument is that the advantage of combining clustering with other interpretability methods is that it provides a method of generating partitions based on weight connectivity which does n't involve activations or gradients . The key thing that weight-based clustering brings to the table is non-redundancy with methods based on runtime analysis ."}, "1": {"review_id": "4qgEGwOtxU-1", "review_text": "This paper explores the application of spectral clustering methods to assess modular organization in the emergent architecture of deep networks . In particular , sub-modules identified by spectral clustering are evaluated in terms of `` importance '' and `` coherence '' , two metrics defined by the authors with the goal of capturing how crucial the neurons in the sub-module are to the classification accuracy , and how consistent their activation is across input and output patterns . Overall , the paper addresses important questions related to the way structural properties in a deep network might support the emergence of functional properties , which is a key issue given the relatively poor theoretical understanding we have about these self-organizing systems . The paper is comprehensible , though the general structure and the writing could be improved to improve readability . For example , in Section 3 it is not immediately clear how the importance and coherence metrics relate to the specific technique adopted for feature visualization , or to the lesioning method applied . The \u201c Related Work \u201d section should be moved at the beginning of the paper , and the contribution should be better framed in the context of other existing approaches . Although I appreciate the wide range of networks tested by the authors , I think that the way results are presented does not easily allow to establish whether we can identify a series of robust findings that are valid across all architectures / tasks , or whether specific cases entail peculiar findings . It would also be valuable to better assess the relationship between modularity and regularization techniques , such as dropout , L2 and pruning : I think that this is a very important point that should deserve further investigation , since it could give important insights about the role of regularizers in shaping the final network architecture . Finally , it would strengthen the paper if the proposed method is put in relation to other approaches that have proven effective to address this kind of question . This would greatly improve the robustness of the results , since the current baseline is basically constituted by a comparison with random sub-modules . For example , it would be nice to see if clustering coefficient and average path length ( as defined in [ 1 ] ) can provide useful information also for the analyses proposed by the authors . Note that in [ 1 ] the authors investigated deep networks with similar architectures ( MLPs , CNNs , ResNets ) trained on similar tasks ( CIFAR-10 , ImageNet ) . Other comments : - Pg . 3 : the technique used to visualize a sub-cluster by creating an aggregate measure of the learned features can be discussed in relation to the method based on Earth-mover distance proposed by [ 2 ] , where the authors also discuss other graph-based metrics that might be useful in the present setting . - Could the \u201c intersection information \u201d approach presented in [ 3 ] can be exploited also in the analyses of the sub-modules detected by the spectral clustering ? Note that in [ 3 ] the authors also investigate \u201c lesion tests \u201d by means of interventional techniques , which would make that approach very interesting as a further benchmark . - What is the rationale for setting the number of clusters to 12 ? If this value is not theoretically motivated , further analyses should show that the results are robust to variations in this value . - When comparing \u201c true sub-clusters \u201d with \u201c random sub-clusters \u201d , a useful control analysis would be to create random sub-clusters by matching some connectivity property ( e.g. , same average strength , clustering coefficient and/or average path length ) . - Regarding the gradient-based method discussed in Olah et al . ( 2017 ) , it would be useful to have some dispersion measure over the final optimization score , in order to better assess whether all neurons in the sub-cluster where in fact similarly activated by the optimized image . - It would be interesting to include as baseline some analysis on randomly connected networks , since it has been shown that subgraphs in large random networks can in fact support accurate task performance even without ever training the weight values [ 4 ] . - Where the p-values corrected for multiple comparisons ? - The authors consider \u201c modularity as an organizing principle to achieve mechanistic transparency \u201d . Though I sympathize with this statement , I guess there are several cases where modular systems ( or in general systems with localized representations ) can develop complex emergent dynamics that still prevent interpretability . References [ 1 ] J . You , J. Leskovec , K. He , and S. Xie , \u201c Graph Structure of Neural Networks , \u201d in International Conference on Machine Learning , 2020 . [ 2 ] A. Testolin , M. Piccolini , and S. Suweis , \u201c Deep learning systems as complex networks , \u201d J . Complex Networks , vol . 0000 , no.1 , pp.1\u201321 , Jun . 2019 . [ 3 ] S. Panzeri , C. D. Harvey , E. Piasini , P. E. Latham , and T. Fellin , \u201c Cracking the Neural Code for Sensory Perception by Combining Statistics , Intervention , and Behavior , \u201d Neuron , vol . 93 , no.3 , pp.491\u2013507 , 2017 . [ 4 ] V. Ramanujan , M. Wortsman , A. Kembhavi , A. Farhadi , and M. Rastegari , \u201c What \u2019 s Hidden in a Randomly Weighted Neural Network ? , \u201d In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 11893-11902 . 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank reviewer 4 for the constructive feedback . ( 1 ) Re : \u201c the way results are presented does not easily allow to establish whether we can identify a series of robust findings that are valid across all architectures / tasks. \u201d It is true that we find evidence of modular sub-clusters in some networks but not in others . However ( a ) we don \u2019 t believe that finding different trends across different networks should be disqualifying , and ( b ) while we show that spectral clustering often identifies modular sub-clusters , the the most central purpose of this paper is to develop ways to measure and use importance and coherence for modularity evaluation . ( 2 ) Re : \u201c It would also be valuable to better assess the relationship between modularity and regularization techniques , such as dropout , L2 and pruning. \u201d While we do not study this thoroughly , note that our comparisons between the regularized and unregularized CIFAR-10 VGGs relate to this . Nonetheless , we believe that further work focusing on this question and other ways to promote modularity architecturally or via training procedure will be valuable . ( 3 ) Re : \u201c it would strengthen the paper if the proposed method is put in relation to other approaches that have proven effective to address this kind of question\u2026.For example , it would be nice to see if clustering coefficient and average path length ... \u201d and \u201c When comparing \u201c true sub-clusters \u201d with \u201c random sub-clusters \u201d , a useful control analysis would be to create random sub-clusters by matching some connectivity property \u201d Overall , we agree that additional controls would be interesting , but we do argue that controlling for location ( layer ) and size are sufficient to evaluate importance and coherence . In this paper , we do not aim to compare partition generation methods . As a side note , spectral clustering has a random-walk interpretation and is very closely related with commute-time/average path length . See von Luxburg ( 2007 ) . ( 4 ) Re : Earth-mover distance and intersection information . We agree that earth mover distance could be used as a measure of coherence in 3.1.1 . Other methods from 3.1.2 and 3.2 could as well . But we intended for 3.1.1. to give visual examples preceding more comprehensive results in 3.1.2 and 3.2 . We also agree that intersection information could be used as a measure of coherence . However , to our understanding , because intersection information measures the association between a stimulus and output , this is closely related to our experiments with lesions . Is this consistent with your understanding ? ( 5 ) Re : \u201c What is the rationale for setting the number of clusters to 12 ? \u201d We will change section 2 to clarify this . We wanted a number greater than 10 ( the number of classes in MNIST/CIFAR ) , but small relative to the number of filters in the networks ' layers . Note that finding results with k=12 in networks ranging from the MNIST to ImageNet scale suggests that these phenomena are robust to different choices of the ratio between $ k $ and the size of the network . We have explored using higher $ k $ , but much larger values lead to slow runtimes for ImageNet experiments . We are currently running lesion experiments for k=8 and k=18 which we will report here soon . ( 6 ) Re : \u201c Regarding the gradient-based method discussed in Olah et al . ( 2017 ) , it would be useful to have some dispersion measure over the final optimization score \u201d We agree this may be interesting . We are running some experiments to get a sense of dispersion and will hopefully be able to report this soon . ( 7 ) Re : \u201c It would be interesting to include as baseline some analysis on randomly connected networks. \u201d We agree that this would be interesting for understanding how much sub-cluster importance and coherence result from training , but we do not see a strong motivation for this involving the development of interpretability methods . ( 8 ) Re : Where the p-values corrected for multiple comparisons ? No.But we agree that also reporting multiple testing corrections in the Supplement would be helpful . We are working on this now and will report results soon . ( 9 ) Re : \u201c I guess there are several cases where modular systems ( or in general systems with localized representations ) can develop complex emergent dynamics that still prevent interpretability. \u201d We agree . We believe that future work will be needed for clarifying the association between interpretability and modularity . Note that this is a _motivation_ for our work though because developing methods for identifying modules and validating their boundaries is a prerequisite to answering this question . [ 1 ] Ulrike von Luxburg . A tutorial on spectral clustering . Statistics and computing , 17 ( 4 ) :395\u2013416 , 2007 ."}, "2": {"review_id": "4qgEGwOtxU-2", "review_text": "The authors identify putative clusters of units/neurons in deep networks using spectral clustering on a graph defined by synaptic weights . The authors then argue that these structurally defined clusters of neurons have similar * functional representations * . Finding interpretable relationships between weight matrices and functional modules is challenging , and the authors should be applauded for attempting to tackle this challenging problem that few research groups are devoting energy to . Despite these positive notes , I have reservations about the presentation and results of the paper . My main concerns are : ( 1 ) The results are largely qualitative and anecdotal . In figure 1 , for example , the authors show slightly higher contrast in their identified clusters than random clusters . The results are limited to black and white images ( MNIST and fashion-MNIST ) , and not all examples look great . Only 2 figures are shown in the main paper , with a lot of other details shoved into the supplement . Thus , the writing and presentation could be improved to highlight the most exciting and surprising findings . ( 2 ) The results crucially rely on a second paper which was concurrently submitted and ca n't be reviewed because it is anonymized . The results shown in this paper are thin and qualitative ( see point 1 ) , so in my view these two paper should be combined into a single paper which overall might tell a more comprehensive and compelling story . ( 3 ) The paper does not generate testable predictions or practical insights that could be used by used by practitioners . The only takeaway point for me was that some neurons / units show correlated representations , which is arguably already known ( e.g.Csordas et al 2020 ) . How to exploit this modularity to develop human-interpretable explanations of network function remains unclear to me .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank reviewer 3 for the constructive feedback . ( 1 ) Re : \u201c Largely qualitative and anecdotal ... The results shown in this paper are thin and qualitative \u201d Section 3.1.1 and Figure 1 are indeed qualitative . Our goal for them was to provide visual examples with a simple dataset like MNIST . More importantly though , 3.1.2 and 3.2 are built entirely around quantitative results . We would like to emphasize that having quantitative results based on statistical hypothesis testing at all is actually something which sets this paper apart from related work using similar methods such as Bau et al . ( 2017 ) , Watanabe ( 2019 ) , and Carter et al. ( 2019 ) . ( 2 ) Re : \u201c The results [ in Figure 1 ] are limited to black and white images ( MNIST and fashion-MNIST ) , and not all examples look great. \u201d What we want to emphasize in Figure 1is that these sub-clusters , despite being based only on weights , systematically exhibit coherence w.r.t.testing data . We find this interesting because it shows that we can uncover runtime properties of a network without making queries to it . And we believe this is useful because interpreting units at the sub-cluster level gives us a flexible level of abstraction and pairs well with data-sensitive interpretability methods . ( 3 ) Re : \u201c Only 2 figures are shown in the main paper , with a lot of other details shoved into the supplement . Thus , the writing and presentation could be improved ... \u201d Would it be possible to provide more detailed suggestions about what figures and details from the supplement should be emphasized more and included in the main paper ? For example , should we incorporate hypothesis testing details from the supplement into the main paper ? ( 4 ) Re : \u201c The results crucially rely on a second paper which was concurrently submitted and ca n't be reviewed because it is anonymized. \u201d While the other paper investigates spectral clustering in depth , this paper only relies on spectral clustering as an example of a partition-generating procedure for neurons . In that sense , this paper stands alone . However , we are hoping to improve clarity here . What kind of details would you like to see ? For example , is it confusing to mention Intrinsic Partition Evaluation in section 2 ? ( 5 ) Re : \u201c The paper does not generate testable predictions or practical insights ... \u201d We agree that our main contribution is not a set of methods that will be immediately useful to practitioners , but this was not our goal . Our approach focused on understanding phenomena and introducing tools which evaluate whether a given neuron partitioning reflects the network functionality , and thus supports the existence of modularity . ( 6 ) Re : \u201c The only takeaway point for me was that some neurons / units show correlated representations \u201d This is not how we would characterize our contributions . While a cluster of units being highly correlated with one another w.r.t.a data distribution would be sufficient to show input coherence , it would not be necessary , nor would it establish importance or output coherence . Note that it is also novel that spectral clustering can find sets of neurons which exhibit these properties at all . We argue that the main takeaways from this paper are the concepts of importance and coherence , statistical methods for measuring them , and demonstrating that spectral clustering , which takes only the network \u2019 s weights as an input , often reveals important and input coherent sub-clusters . [ 1 ] David Bau , Bolei Zhou , Aditya Khosla , Aude Oliva , and Antonio Torralba . Network dissection : Quantifying interpretability of deep visual representations . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp . 6541\u20136549 , 2017 . [ 2 ] Chihiro Watanabe . Interpreting layered neural networks via hierarchical modular representation . In International Conference on Neural Information Processing , pp . 376\u2013388.Springer , 2019 . [ 3 ] Shan Carter , Zan Armstrong , Ludwig Schubert , Ian Johnson , and Chris Olah . Activation atlas . Distill , 4 ( 3 ) : e15 , 2019 ."}}