{"year": "2021", "forum": "7IDIy7Jb00l", "title": "Offline Meta Learning of Exploration", "decision": "Reject", "meta_review": "The paper studies offline meta reinforcement learning. Overall the scope of this contribution seems limited. Reviewers have raised concerns about the significance of the presented results given the assumptions, and that the experimental environments are not extensive and do not fully support the claimed advances. ", "reviews": [{"review_id": "7IDIy7Jb00l-0", "review_text": "Summary : This submission studies the meta-learning problem in RL under offline settings . A new algorithm is proposed to address this problem by extending the recent VariBAD algorithm designed for online meta-RL . The key modifications to adapt the original VariBAD to offline settings are the state re-labelling and reward re-labelling tricks , which aim at addressing the MDP ambiguity specifically showing up in offline meta-RL . Experiments are conducted on four sparse reward tasks under both offline and online settings , comparing with PEARL ( a recent online approach ) and the original VariBAD respectively . Pros : 1.The problem studied is interesting and important in RL , and largely open . 2.The proposed tricks for adapting VariBAD to offline settings are simple and easy to implement . 3.The ablation study on the Ant-Semi-circle task is helpful . Cons : 1.Regarding the reward re-labelling trick , the assumption that reward functions for all training environments are known limits the applicability of the proposed algorithm for real-world problems . 2.Regarding the experiments comparing with online PEARL , the training curves of PEARL in Fig.3 do not seem correct to me , it is strange that PEARL does not make any progress with training , which is very different from the behavior reported in the original PEARL paper . 3.Regarding the soundness of comparison experiments , it would be much more convincing if the following things are taken care of : ( a ) All four tasks reported in the original VariBAD paper are included , rather than just picking one from them . ( b ) Averaging performance is reported over 5 runs rather than just 3 , especially considering that SAC could perform quite differently between different random seeds . ( c ) Hyper parameters are set to be more consistent for the same RL backbone ( say SAC ) across different tasks . It is understandable that two sets of parameters might be needed for online and offline settings . Based on my main concerns regarding the assumptions on reward function and soundness/correctness of experiments , I lean towards a rejection . Other questions : 1 . Regarding the state re-labelling trick , I have a question regarding the proof of Proposition 1 : the first equality of the proof in Appendix A does not seem straightforward to me . It would be helpful if more derivation details are provided . 2.I feel that the claim of approximating Bayes-optimal policy in the offline setting is overly stated . Is there any explicit arguments showing that this is `` approximately '' true ? = Post rebuttal : My concerns regarding the experiments are mostly addressed , though as pointed out by other reviewers , more convincing experiments under changing transition dynamics would be very helpful . I also stand by the authors explanation regarding limited resource in running RL experiments , especially for novel research directions . That said , for the same reason ( pioneering research vs large-scale application ) , I do not agree with the authors explanation regarding the limitation of the proposed approach in assuming a known ground truth reward function . The main contribution of this submission is not in solving a specific real-world RL application problem as the cited references . As one of the first efforts in addressing the meta-RL under offline setting , I feel that setting this constraint is a limitation and should be relaxed by means of estimating the reward function . This should be addressed in future work . I raise my rating to a weak acceptance conditioned on the wording regarding `` Bayes optimal '' being more precisely presented , I think it is currently over-stated throughout the paper , which could be misleading to the community if published as it is . It is important to carefully reword in which sense the proposed algorithm `` approximate Bayes-optimal policy '' , as explained in the authors ' response , the algorithm is shown to qualitatively behave in a way that a Bayes optimal policy would do under this particular setup , this is far from sufficient to claim any approximation to the Bayes-optimality in a principled sense . I would like to also point out that while Bayes-optimality is generally intractable , it is possible and not uncommon for a method to start from an explicit pursue of Bayes optimal solution and specify where and how approximations are performed to overcome specific intractibilities , and further show quantitatively that such Bayes optimality is indeed achievable under well-controlled toy examples where the true Bayes optimal solution is known . The concept of Bayes optimality is in essence quantitative , rather than qualitative .", "rating": "6: Marginally above acceptance threshold", "reply_text": "PEARL performance : In Figure 3 we do not present the training curves of PEARL , but only its * best performance * during online training . We added the training curves of PEARL to Appendix G.2 . The rationale for this presentation is that comparing offline and online methods in terms of reward vs. training iterations is rather meaningless . Known reward function : our method requires knowing the reward function only during training . We are not aware of any real-world RL application in which the reward function was not known ( [ 1 , 2 , 3 ] ) . On the contrary , the reward is typically the practitioner 's method of specifying the task ! SAC hyperparameters : some hyperparameters for SAC should be chosen proportional to the action dimensions ( Appendix D in [ 4 ] ) , which are different in our domains . Further , it is common in RL to choose different hyperparamters for different domains ( see , e.g. , PEARL , VariBAD , and SAC ( Appendix D in [ 4 ] ) ) . From the trajectory visualizations , it is clear that the superior performance of our method with respect to PEARL is not due to hyperparameter tweaking , but due to the Bayesian RL formulation . Random seeds : we will add more seeds to the evaluation . Evaluation tasks : please see our general comment on this issue above . In particular , we added sparse reward tasks that are much more difficult than the tasks in the VariBAD work . Proposition 1 : we marginalize over possible reward and transition functions and use conditional probability followed by the definition of conditional expectation ( see full derivation in new version , page 11 ) . Bayes-optimality : finding the Bayes-optimal is generally intractable . However , in domains with sparse rewards and uniform prior over MDPs , the Bayes-optimal policy amounts to visiting as many states as possible , without revisiting states , and in case the goal state is found - staying at the goal . Although we do not provide any theoretical guarantees regarding the optimality of our learned policy , we qualitatively show that our trained agent produces trajectories that ( approximately ) match such Bayes-optimal behavior . [ 1 ] Abbeel et al.An application of reinforcement learning to aerobatic helicopter flight . Advances in neural information processing systems , 19:1\u20138 , 2006 . \\ [ 2 ] Gu et al.Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates . In 2017 IEEE international conference on robotics and automation ( ICRA ) , pp . 3389\u20133396 . IEEE , 2017 . \\ [ 3 ] Schoettler et al.Meta-reinforcement learning for robotic industrial insertion tasks . arXiv preprint arXiv:2004.14404,2020 . \\ [ 4 ] Haarnoja et al.Soft actor-critic algorithms and applications . arXiv preprint arXiv:1812.05905 , 2018 ."}, {"review_id": "7IDIy7Jb00l-1", "review_text": "Summary after discussion period : - The authors have done a good job in toning down their claims to match what the evidence supports . After reading the other review 's comments and the updated version of the paper , I feel both my comments and most of the reivewer 's comments have been addressed , allowing me to recommend this paper for acceptance . Summary - In this paper , the authors tackle the problem of offline meta RL , where one observes trajectories from environments drawn from a distribution of environments , then using these past observations finds a policy which will work well on new environments drawn from the same distribution , optimizing both in the face of uncertainty about the environment and about the random transitions . The authors propose a method based on VariBAD which achieves good experimental results . Pros : - I find the paper to be novel , relevant and well written , a good contribution to the scientific community . I liked it \u2019 s introduction to the idea of offline Meta-RL , and thought the experimental results where an agent learns to walk the semi-circle were impressive . Further , I greatly enjoyed the discussion of MDP ambiguity , it \u2019 s a genuine problem that I \u2019 d never seen before and I think addressing exactly this issue will be important in advancing offline meta-RL . Major Concern : -- My major concern is that the evidence does not fully support the author \u2019 s central claim : \u201c quickly maximize reward in a new , unseen task from the same distribution \u201d where this distribution is described as varying over both the reward and transition functions . Although I do believe that the author \u2019 s method addresses varying reward structures , they have provided no evidence that the method addresses varying transition dynamics . In their experiments , they sample different MDPs , but each has the same transition dynamics , only the reward dynamics vary . Please do not misunderstand me , I believe that science is incremental , small steps are valuable , and having a method which addresses only the smaller problem of fixed-transition dynamics with sampled reward dynamics is already an interesting and valuable contribution . Yet I don \u2019 t feel anyone is served by a paper which claims it solves the larger problem of offline Meta-RL , but doesn \u2019 t provide evidence for this larger claim . Therefore I would request the authors either provide evidence for the larger claim , or ( it \u2019 s probably the easier route ) simply make a smaller claim that is supported by the evidence , and save the real offline Meta-RL problem for a future submission . This would be that one is interested in the setting where MDPs with varying reward structure are sampled . I believe that the problem of MDP ambiguity can \u2019 t be so easily solved for ambiguous state transitions . In section 3.3 , the authors nicely address the issue of MDP ambiguity , ( it was a very insightful section , a valuable contribution ) and that it \u2019 s impossible to know if two trajectories come from different MDPs with different rewards , or one MDP with rewards at both locations . It \u2019 s an inherent counterfactual problem , one doesn \u2019 t know what would have happened if one had done something else . The authors fill in the gaps in this counterfactual knowledge by assuming knowledge of the true reward structure from each past MDP . In this way , for all past trajectories the authors can say what would have happened if the same actions had been taken in a setting with a different rewards structure . Yet the problem of MDP ambiguity isn \u2019 t restricted to unknown reward structures . Assume a meta MDP setting where each MDP is a gridworld with a key and a door and the agent collects reward after getting the key then going to the door , thus the state is the agent \u2019 s position and a binary variable indicating whether the agent has collected the key . For different MDPs in the distribution , the key is placed in different places . In this way , the reward structure is the same for different MDPs in the distribution , but the transition structure ( where the agent collects the key ) changes . I don \u2019 t see how the author \u2019 s proposed MDP ambiguity fix would address ambiguity in the transition dynamics , or even provide a roadmap for addressing this issue . Indeed , it is telling that the experiments only deal with Meta-RL where the reward distribution changes , and never address changing transition dynamics . Therefore I would ask the authors to clarify the scope of their contribution , and perhaps address what challenges stand in the way of offline Meta-RL where the state transitions vary too . After that , I 'd be happy to update my review . Minor Concerns : - In Prop . 1 , why do we have $ E_ { \\mathcal R , \\mathcal P\\sim b_t } $ , it seems one must only sample $ \\mathcal P $ in the first line , and $ \\mathcal R $ in the second . \u201c Recall that the VariBAD VAE encoder\u2026. \u201d perhaps drop the word \u2018 encoder \u2019 , encoder is already in the \u2018 E \u2019 of VAE . In section 3.3 , when describing figure 2 ( also in the caption of figure two ) you say the reward is in the yellow circle , but the circle looks reddish on my PDF viewer .", "rating": "7: Good paper, accept", "reply_text": "We completely agree with the reviewer 's evaluation of our work -- both the merits and the limitations . Regarding MDP ambiguity - please see our general comment above . Our method should successfully handle MDPs with different transitions , but only if the identifiable states are dense enough . Indeed , when ambiguity is due to transitions , our method is not applicable , and we do not see how this problem can be solved . We will clarify this issue in the text . We are running experiments with a 'dense ' uncertainty in transitions . If the results will not be ready before the rebuttal period ends , we will remove our claims of handling varying transition functions ."}, {"review_id": "7IDIy7Jb00l-2", "review_text": "The paper studies the problem of offline Meta-Reinforcement Learning ( RL ) . In this problem , N separate RL environments are considered which are drawn from a specific underlying distribution . For each such environment , M trajectories each of horizon H are provided beforehand . The task is to train an RL agent that performs well in expectation on a new RL environment drawn from the same distribution . The authors adapt the online VariBAD algorithm ( Zintgraf et al. , 2020 ) by the use of a techniques called state relabeling and reward relabeling . The VariBAD algorithm formulates online Meta-RL as a Bayesian Adaptive MDP ( BAMDP ) and solves it approximately using a variational approach . The authors , due to lack of online data , use partial offline trajectories to create belief over RL environment . Next , this belief is used to map offline Meta-RL to the BAMDP setting -- a.k.a . state relabelling . Once mapped to BAMDP off-the-shelf offline RL techniques are used to solve the problem approximately . The authors further introduce reward relabelling that adds new trajectories by mixing rewards among the RL environments conditioned on the state and action . This is claimed to be useful if the reward distribution is independent of the transition matrix distribution of the RL . The authors provide multiple experiments with sparse rewards to show the VariBAD with relabelling works in some established benchmark RL tasks ( albeit in the new offline Meta-RL setting ) . Pros : * The offline Meta-RL is a very relevant problem with a lot of applications in the coming years . Designing a methodology to solve this problem using off-the-shelf techniques is a novel pursuit , which is carried out in the paper . * The idea of state and reward relabeling is novel in the meta-RL setup , as per my understanding ( as an outsider to the Meta-RL community but with a background in RL ) . Cons : 1.Methodology : * The paper does not explore properly the ( in ) -famous `` deadly triad '' which is known to make offline RL difficult . Given the state-space of BAMDP captures beliefs over ( possibly ) complicated RL environments , the effect of `` deadly triad '' is conceivably even more prominent in the current formulation . * I am not sure why the reward relabeling is able to solve the MDP ( a.k.a.aliasing in the offline RL community ) . In particular , `` Note that our relabelling effectively samples data from an MDP with transitions Pi and reward Ri , which has non-zero prior probability mass under the decoupled prior assumption '' -- the above statement is very vague given finite trajectories . The MDP aliasing ( a part of the deadly triad ) is closely related to this issue . * The state relabelling is heavily reliant on the existing VariBAD algorithm , and off-the-shelf RL techniques . Therefore , the algorithmic contribution seems somewhat lacking in my opinion . 2.Experiments : * The provided experiments lack any conclusive evidence of the effectiveness of reward-labeling . Why in the Half-Cheetah-Vel experiment reward relabeling performs worse -- is the reward independence assumption not true here ? Will reward relabeling always perform worse ? * The effectiveness of the Thompson sampling baseline used is not clear to me . Does no other simple baseline exist ? I am alright with not comparing it with the concurrent works . However , such a comparison would have been much more convincing .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Methodology * * : \\ Deadly triad : the deadly triad concerns theoretical convergence issues when bootstrapping and function approximation are used in off-policy RL . In recent years , many successful applications of off-policy RL with deep neural networks have been reported . Thus , it seems that with some implementation tricks ( e.g. , experience replay , target networks ) , it is not a practical limitation . The off-policy algorithms we use ( DQN , SAC ) are well documented to solve challenging RL problems with deep neural networks and high dimensional state spaces . The belief in our case is represented by the means and covariances from the VAE , which can be seen as simply additional continuous state variables . As our results indicate , the RL algorithms we used were capable of handling this scenario . Aliasing and MDP ambiguity : state aliasing occurs when two states have similar representation under the RL function approximator and , as a result , when we update the Q-function in one of them , we also change the Q-function of the other . The MDP ambiguity problem , on the other hand , affects the training of the VAE , and * is not a concern for the off-policy RL solver * . The goal of the VAE is to infer an MDP from experience ( represented by Gaussian approximate posterior ) . When MDP ambiguity occurs , inferring the correct MDP becomes impossible since , based on offline data , we can not tell from which MDP the data came from ( no overlapping ` identifying states ' -- see general comment above ) . Therefore , this problem prevents us from obtaining a good estimate of the belief ( which is part of the hyper-state we form ) . Reward relabelling : When we relabel the rewards of a state-action trajectory collected from MDP i , by querying the reward function of MDP i ' , we effectively add a state-action-reward trajectory that is collected from an MDP with transitions P_i ( s^ ( i ) -a^ ( i ) -s^ ' ( i ) ) and rewards R_i ' ( s^ ( i ) -a^ ( i ) -r^ ( i ' ) ) . Such MDP ( with parameters P_i , R_i ' ) has non-zero probability under the prior ( it lies within the prior support ) . Algorithmic contribution : as we acknowledge in the paper , the state-relabelling is indeed similar to that used in VariBAD , and we could have just run VariBAD blindly with off-policy RL and gotten the same results . However , understanding why the algorithm works is important . Our contribution is providing the theory for VariBAD in the off-policy setting , which is conceptually different from standard VariBAD due to the fact that off-policy methods are trained using tuples of ( s , a , r , s ' ) rather than trajectories . See Section 3.2 for more details . * * Experiments * * : \\ Half-cheetah domain : please see our general comment about MDP ambiguity and sparse rewards , and the explanation why in dense rewards domains ( as with Cheetah ) , reward relabelling is not needed . The performance with reward relabelling is only slightly worse than without it , and we believe the differences are due to noise ( random seed ) and are not systematic . We emphasize that both with and without reward relabelling , our method outperforms the PEARL baseline . Baselines : to the best of our knowledge , there are no studies that meta learn exploration strategies in the offline setting that are not based on Thompson sampling ( TS ) , * including concurrent works * . Thus , by comparing with a state-of-the-art TS algorithm that is trained * online * ( or computed exactly in the discrete setting ) , we effectively compare with an * upper bound to the performance of all other works in the offline setting to date * ."}, {"review_id": "7IDIy7Jb00l-3", "review_text": "Summary : The paper proposes an extension of VariBAD to the offline setting . The main difficulty of such an application in an offline setting is what the paper termed `` MDP ambiguity '' while proposing reward relabeling as a solution . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper is clear and well-written . 2.It proposes a simple but effective solution to adapt VariBAD to the offline setting for a type of environments where transition probabilities and reward probabilities are independent . 3.The paper demonstrates very interesting learned exploration behavior and shows its superior empirical performance compared to baselines ( VariBAD and PEARL ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The test environments are not extensive : all 4 test environments used in the paper are quite simple and 2 out of 4 are even similar . What about other openai gym control tasks ( or Atari games ) ? Given the strong independence assumption between transition and reward probabilities , it would be nice to demonstrate how resilient this method is on environments that violate this assumption . 2.There are few baselines being compared against in this paper so it 's unclear given some offline data from an unknown environment this method is the sota . For example , one potential area of comparison is model-based RL agents where the prediction models ( or even transition models if the environment is simple enough ) are trained with supervised learning using offline data . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : 1 . Why is the top right slot of Table 1 blank ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "To the best of our knowledge , the Ant-Semi-circle domain is the most complex domain studied in the meta-RL setting to date [ 1 , 2 ] . Independence assumption : when the assumption does not hold , applying reward relabelling amounts to adding tasks to the data that have zero probability under the prior . This should not hurt the general performance of our method , as the VAE would simply learn to identify more possible MDPs . It can , however , make the agent search for the correct MDP less effectively , as it now has more MDP possibilities in the prior . If we were to add such an experiment , it is not clear what the comparison would be , as we do not have a better method for handling MDP ambiguity when the assumption does not hold . Baselines : we are not aware of stronger baselines for the setting we consider . This is the first study of deep Bayesian RL in the offline setting , and the relevant prior works are all based on Thompson sampling ( TS ) . Therefore , we compared against the strongest TS baselines that we can think of : exact computation in the discrete Gridworld , and online PEARL in continuous domains . If the reviewer has a stronger specific baseline in mind , we will happily add comparisons . Model-based approach : we emphasize that our method uses offline data * only during training * . At test time , it does not require any data from the new task prior to running the learned policy ! The model-based RL approach that the reviewer suggests does not answer the question how to explore effectively , which is exactly what our agent learns . To further clarify , observe for example the Ant-Semi-circle task -- here our learned agent finds the goal after only 2 episodes in a new test MDP . The agent 's strategy for collecting information is approximately optimal , and any other method , model-based or model-free , can not discover the reward with less than 2 episodes of interaction data . Table 1 is now full in the new , updated version . [ 1 ] Rakelly et al.Efficient off-policy meta-reinforcement learning via probabilistic context variables . arXiv preprint arXiv:1903.08254 , 2019 . [ 2 ] Mitchell et al.Offline meta reinforcement learning with advantage weighting . arXiv preprint arXiv:2008.06043 , 2020 ."}], "0": {"review_id": "7IDIy7Jb00l-0", "review_text": "Summary : This submission studies the meta-learning problem in RL under offline settings . A new algorithm is proposed to address this problem by extending the recent VariBAD algorithm designed for online meta-RL . The key modifications to adapt the original VariBAD to offline settings are the state re-labelling and reward re-labelling tricks , which aim at addressing the MDP ambiguity specifically showing up in offline meta-RL . Experiments are conducted on four sparse reward tasks under both offline and online settings , comparing with PEARL ( a recent online approach ) and the original VariBAD respectively . Pros : 1.The problem studied is interesting and important in RL , and largely open . 2.The proposed tricks for adapting VariBAD to offline settings are simple and easy to implement . 3.The ablation study on the Ant-Semi-circle task is helpful . Cons : 1.Regarding the reward re-labelling trick , the assumption that reward functions for all training environments are known limits the applicability of the proposed algorithm for real-world problems . 2.Regarding the experiments comparing with online PEARL , the training curves of PEARL in Fig.3 do not seem correct to me , it is strange that PEARL does not make any progress with training , which is very different from the behavior reported in the original PEARL paper . 3.Regarding the soundness of comparison experiments , it would be much more convincing if the following things are taken care of : ( a ) All four tasks reported in the original VariBAD paper are included , rather than just picking one from them . ( b ) Averaging performance is reported over 5 runs rather than just 3 , especially considering that SAC could perform quite differently between different random seeds . ( c ) Hyper parameters are set to be more consistent for the same RL backbone ( say SAC ) across different tasks . It is understandable that two sets of parameters might be needed for online and offline settings . Based on my main concerns regarding the assumptions on reward function and soundness/correctness of experiments , I lean towards a rejection . Other questions : 1 . Regarding the state re-labelling trick , I have a question regarding the proof of Proposition 1 : the first equality of the proof in Appendix A does not seem straightforward to me . It would be helpful if more derivation details are provided . 2.I feel that the claim of approximating Bayes-optimal policy in the offline setting is overly stated . Is there any explicit arguments showing that this is `` approximately '' true ? = Post rebuttal : My concerns regarding the experiments are mostly addressed , though as pointed out by other reviewers , more convincing experiments under changing transition dynamics would be very helpful . I also stand by the authors explanation regarding limited resource in running RL experiments , especially for novel research directions . That said , for the same reason ( pioneering research vs large-scale application ) , I do not agree with the authors explanation regarding the limitation of the proposed approach in assuming a known ground truth reward function . The main contribution of this submission is not in solving a specific real-world RL application problem as the cited references . As one of the first efforts in addressing the meta-RL under offline setting , I feel that setting this constraint is a limitation and should be relaxed by means of estimating the reward function . This should be addressed in future work . I raise my rating to a weak acceptance conditioned on the wording regarding `` Bayes optimal '' being more precisely presented , I think it is currently over-stated throughout the paper , which could be misleading to the community if published as it is . It is important to carefully reword in which sense the proposed algorithm `` approximate Bayes-optimal policy '' , as explained in the authors ' response , the algorithm is shown to qualitatively behave in a way that a Bayes optimal policy would do under this particular setup , this is far from sufficient to claim any approximation to the Bayes-optimality in a principled sense . I would like to also point out that while Bayes-optimality is generally intractable , it is possible and not uncommon for a method to start from an explicit pursue of Bayes optimal solution and specify where and how approximations are performed to overcome specific intractibilities , and further show quantitatively that such Bayes optimality is indeed achievable under well-controlled toy examples where the true Bayes optimal solution is known . The concept of Bayes optimality is in essence quantitative , rather than qualitative .", "rating": "6: Marginally above acceptance threshold", "reply_text": "PEARL performance : In Figure 3 we do not present the training curves of PEARL , but only its * best performance * during online training . We added the training curves of PEARL to Appendix G.2 . The rationale for this presentation is that comparing offline and online methods in terms of reward vs. training iterations is rather meaningless . Known reward function : our method requires knowing the reward function only during training . We are not aware of any real-world RL application in which the reward function was not known ( [ 1 , 2 , 3 ] ) . On the contrary , the reward is typically the practitioner 's method of specifying the task ! SAC hyperparameters : some hyperparameters for SAC should be chosen proportional to the action dimensions ( Appendix D in [ 4 ] ) , which are different in our domains . Further , it is common in RL to choose different hyperparamters for different domains ( see , e.g. , PEARL , VariBAD , and SAC ( Appendix D in [ 4 ] ) ) . From the trajectory visualizations , it is clear that the superior performance of our method with respect to PEARL is not due to hyperparameter tweaking , but due to the Bayesian RL formulation . Random seeds : we will add more seeds to the evaluation . Evaluation tasks : please see our general comment on this issue above . In particular , we added sparse reward tasks that are much more difficult than the tasks in the VariBAD work . Proposition 1 : we marginalize over possible reward and transition functions and use conditional probability followed by the definition of conditional expectation ( see full derivation in new version , page 11 ) . Bayes-optimality : finding the Bayes-optimal is generally intractable . However , in domains with sparse rewards and uniform prior over MDPs , the Bayes-optimal policy amounts to visiting as many states as possible , without revisiting states , and in case the goal state is found - staying at the goal . Although we do not provide any theoretical guarantees regarding the optimality of our learned policy , we qualitatively show that our trained agent produces trajectories that ( approximately ) match such Bayes-optimal behavior . [ 1 ] Abbeel et al.An application of reinforcement learning to aerobatic helicopter flight . Advances in neural information processing systems , 19:1\u20138 , 2006 . \\ [ 2 ] Gu et al.Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates . In 2017 IEEE international conference on robotics and automation ( ICRA ) , pp . 3389\u20133396 . IEEE , 2017 . \\ [ 3 ] Schoettler et al.Meta-reinforcement learning for robotic industrial insertion tasks . arXiv preprint arXiv:2004.14404,2020 . \\ [ 4 ] Haarnoja et al.Soft actor-critic algorithms and applications . arXiv preprint arXiv:1812.05905 , 2018 ."}, "1": {"review_id": "7IDIy7Jb00l-1", "review_text": "Summary after discussion period : - The authors have done a good job in toning down their claims to match what the evidence supports . After reading the other review 's comments and the updated version of the paper , I feel both my comments and most of the reivewer 's comments have been addressed , allowing me to recommend this paper for acceptance . Summary - In this paper , the authors tackle the problem of offline meta RL , where one observes trajectories from environments drawn from a distribution of environments , then using these past observations finds a policy which will work well on new environments drawn from the same distribution , optimizing both in the face of uncertainty about the environment and about the random transitions . The authors propose a method based on VariBAD which achieves good experimental results . Pros : - I find the paper to be novel , relevant and well written , a good contribution to the scientific community . I liked it \u2019 s introduction to the idea of offline Meta-RL , and thought the experimental results where an agent learns to walk the semi-circle were impressive . Further , I greatly enjoyed the discussion of MDP ambiguity , it \u2019 s a genuine problem that I \u2019 d never seen before and I think addressing exactly this issue will be important in advancing offline meta-RL . Major Concern : -- My major concern is that the evidence does not fully support the author \u2019 s central claim : \u201c quickly maximize reward in a new , unseen task from the same distribution \u201d where this distribution is described as varying over both the reward and transition functions . Although I do believe that the author \u2019 s method addresses varying reward structures , they have provided no evidence that the method addresses varying transition dynamics . In their experiments , they sample different MDPs , but each has the same transition dynamics , only the reward dynamics vary . Please do not misunderstand me , I believe that science is incremental , small steps are valuable , and having a method which addresses only the smaller problem of fixed-transition dynamics with sampled reward dynamics is already an interesting and valuable contribution . Yet I don \u2019 t feel anyone is served by a paper which claims it solves the larger problem of offline Meta-RL , but doesn \u2019 t provide evidence for this larger claim . Therefore I would request the authors either provide evidence for the larger claim , or ( it \u2019 s probably the easier route ) simply make a smaller claim that is supported by the evidence , and save the real offline Meta-RL problem for a future submission . This would be that one is interested in the setting where MDPs with varying reward structure are sampled . I believe that the problem of MDP ambiguity can \u2019 t be so easily solved for ambiguous state transitions . In section 3.3 , the authors nicely address the issue of MDP ambiguity , ( it was a very insightful section , a valuable contribution ) and that it \u2019 s impossible to know if two trajectories come from different MDPs with different rewards , or one MDP with rewards at both locations . It \u2019 s an inherent counterfactual problem , one doesn \u2019 t know what would have happened if one had done something else . The authors fill in the gaps in this counterfactual knowledge by assuming knowledge of the true reward structure from each past MDP . In this way , for all past trajectories the authors can say what would have happened if the same actions had been taken in a setting with a different rewards structure . Yet the problem of MDP ambiguity isn \u2019 t restricted to unknown reward structures . Assume a meta MDP setting where each MDP is a gridworld with a key and a door and the agent collects reward after getting the key then going to the door , thus the state is the agent \u2019 s position and a binary variable indicating whether the agent has collected the key . For different MDPs in the distribution , the key is placed in different places . In this way , the reward structure is the same for different MDPs in the distribution , but the transition structure ( where the agent collects the key ) changes . I don \u2019 t see how the author \u2019 s proposed MDP ambiguity fix would address ambiguity in the transition dynamics , or even provide a roadmap for addressing this issue . Indeed , it is telling that the experiments only deal with Meta-RL where the reward distribution changes , and never address changing transition dynamics . Therefore I would ask the authors to clarify the scope of their contribution , and perhaps address what challenges stand in the way of offline Meta-RL where the state transitions vary too . After that , I 'd be happy to update my review . Minor Concerns : - In Prop . 1 , why do we have $ E_ { \\mathcal R , \\mathcal P\\sim b_t } $ , it seems one must only sample $ \\mathcal P $ in the first line , and $ \\mathcal R $ in the second . \u201c Recall that the VariBAD VAE encoder\u2026. \u201d perhaps drop the word \u2018 encoder \u2019 , encoder is already in the \u2018 E \u2019 of VAE . In section 3.3 , when describing figure 2 ( also in the caption of figure two ) you say the reward is in the yellow circle , but the circle looks reddish on my PDF viewer .", "rating": "7: Good paper, accept", "reply_text": "We completely agree with the reviewer 's evaluation of our work -- both the merits and the limitations . Regarding MDP ambiguity - please see our general comment above . Our method should successfully handle MDPs with different transitions , but only if the identifiable states are dense enough . Indeed , when ambiguity is due to transitions , our method is not applicable , and we do not see how this problem can be solved . We will clarify this issue in the text . We are running experiments with a 'dense ' uncertainty in transitions . If the results will not be ready before the rebuttal period ends , we will remove our claims of handling varying transition functions ."}, "2": {"review_id": "7IDIy7Jb00l-2", "review_text": "The paper studies the problem of offline Meta-Reinforcement Learning ( RL ) . In this problem , N separate RL environments are considered which are drawn from a specific underlying distribution . For each such environment , M trajectories each of horizon H are provided beforehand . The task is to train an RL agent that performs well in expectation on a new RL environment drawn from the same distribution . The authors adapt the online VariBAD algorithm ( Zintgraf et al. , 2020 ) by the use of a techniques called state relabeling and reward relabeling . The VariBAD algorithm formulates online Meta-RL as a Bayesian Adaptive MDP ( BAMDP ) and solves it approximately using a variational approach . The authors , due to lack of online data , use partial offline trajectories to create belief over RL environment . Next , this belief is used to map offline Meta-RL to the BAMDP setting -- a.k.a . state relabelling . Once mapped to BAMDP off-the-shelf offline RL techniques are used to solve the problem approximately . The authors further introduce reward relabelling that adds new trajectories by mixing rewards among the RL environments conditioned on the state and action . This is claimed to be useful if the reward distribution is independent of the transition matrix distribution of the RL . The authors provide multiple experiments with sparse rewards to show the VariBAD with relabelling works in some established benchmark RL tasks ( albeit in the new offline Meta-RL setting ) . Pros : * The offline Meta-RL is a very relevant problem with a lot of applications in the coming years . Designing a methodology to solve this problem using off-the-shelf techniques is a novel pursuit , which is carried out in the paper . * The idea of state and reward relabeling is novel in the meta-RL setup , as per my understanding ( as an outsider to the Meta-RL community but with a background in RL ) . Cons : 1.Methodology : * The paper does not explore properly the ( in ) -famous `` deadly triad '' which is known to make offline RL difficult . Given the state-space of BAMDP captures beliefs over ( possibly ) complicated RL environments , the effect of `` deadly triad '' is conceivably even more prominent in the current formulation . * I am not sure why the reward relabeling is able to solve the MDP ( a.k.a.aliasing in the offline RL community ) . In particular , `` Note that our relabelling effectively samples data from an MDP with transitions Pi and reward Ri , which has non-zero prior probability mass under the decoupled prior assumption '' -- the above statement is very vague given finite trajectories . The MDP aliasing ( a part of the deadly triad ) is closely related to this issue . * The state relabelling is heavily reliant on the existing VariBAD algorithm , and off-the-shelf RL techniques . Therefore , the algorithmic contribution seems somewhat lacking in my opinion . 2.Experiments : * The provided experiments lack any conclusive evidence of the effectiveness of reward-labeling . Why in the Half-Cheetah-Vel experiment reward relabeling performs worse -- is the reward independence assumption not true here ? Will reward relabeling always perform worse ? * The effectiveness of the Thompson sampling baseline used is not clear to me . Does no other simple baseline exist ? I am alright with not comparing it with the concurrent works . However , such a comparison would have been much more convincing .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Methodology * * : \\ Deadly triad : the deadly triad concerns theoretical convergence issues when bootstrapping and function approximation are used in off-policy RL . In recent years , many successful applications of off-policy RL with deep neural networks have been reported . Thus , it seems that with some implementation tricks ( e.g. , experience replay , target networks ) , it is not a practical limitation . The off-policy algorithms we use ( DQN , SAC ) are well documented to solve challenging RL problems with deep neural networks and high dimensional state spaces . The belief in our case is represented by the means and covariances from the VAE , which can be seen as simply additional continuous state variables . As our results indicate , the RL algorithms we used were capable of handling this scenario . Aliasing and MDP ambiguity : state aliasing occurs when two states have similar representation under the RL function approximator and , as a result , when we update the Q-function in one of them , we also change the Q-function of the other . The MDP ambiguity problem , on the other hand , affects the training of the VAE , and * is not a concern for the off-policy RL solver * . The goal of the VAE is to infer an MDP from experience ( represented by Gaussian approximate posterior ) . When MDP ambiguity occurs , inferring the correct MDP becomes impossible since , based on offline data , we can not tell from which MDP the data came from ( no overlapping ` identifying states ' -- see general comment above ) . Therefore , this problem prevents us from obtaining a good estimate of the belief ( which is part of the hyper-state we form ) . Reward relabelling : When we relabel the rewards of a state-action trajectory collected from MDP i , by querying the reward function of MDP i ' , we effectively add a state-action-reward trajectory that is collected from an MDP with transitions P_i ( s^ ( i ) -a^ ( i ) -s^ ' ( i ) ) and rewards R_i ' ( s^ ( i ) -a^ ( i ) -r^ ( i ' ) ) . Such MDP ( with parameters P_i , R_i ' ) has non-zero probability under the prior ( it lies within the prior support ) . Algorithmic contribution : as we acknowledge in the paper , the state-relabelling is indeed similar to that used in VariBAD , and we could have just run VariBAD blindly with off-policy RL and gotten the same results . However , understanding why the algorithm works is important . Our contribution is providing the theory for VariBAD in the off-policy setting , which is conceptually different from standard VariBAD due to the fact that off-policy methods are trained using tuples of ( s , a , r , s ' ) rather than trajectories . See Section 3.2 for more details . * * Experiments * * : \\ Half-cheetah domain : please see our general comment about MDP ambiguity and sparse rewards , and the explanation why in dense rewards domains ( as with Cheetah ) , reward relabelling is not needed . The performance with reward relabelling is only slightly worse than without it , and we believe the differences are due to noise ( random seed ) and are not systematic . We emphasize that both with and without reward relabelling , our method outperforms the PEARL baseline . Baselines : to the best of our knowledge , there are no studies that meta learn exploration strategies in the offline setting that are not based on Thompson sampling ( TS ) , * including concurrent works * . Thus , by comparing with a state-of-the-art TS algorithm that is trained * online * ( or computed exactly in the discrete setting ) , we effectively compare with an * upper bound to the performance of all other works in the offline setting to date * ."}, "3": {"review_id": "7IDIy7Jb00l-3", "review_text": "Summary : The paper proposes an extension of VariBAD to the offline setting . The main difficulty of such an application in an offline setting is what the paper termed `` MDP ambiguity '' while proposing reward relabeling as a solution . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper is clear and well-written . 2.It proposes a simple but effective solution to adapt VariBAD to the offline setting for a type of environments where transition probabilities and reward probabilities are independent . 3.The paper demonstrates very interesting learned exploration behavior and shows its superior empirical performance compared to baselines ( VariBAD and PEARL ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The test environments are not extensive : all 4 test environments used in the paper are quite simple and 2 out of 4 are even similar . What about other openai gym control tasks ( or Atari games ) ? Given the strong independence assumption between transition and reward probabilities , it would be nice to demonstrate how resilient this method is on environments that violate this assumption . 2.There are few baselines being compared against in this paper so it 's unclear given some offline data from an unknown environment this method is the sota . For example , one potential area of comparison is model-based RL agents where the prediction models ( or even transition models if the environment is simple enough ) are trained with supervised learning using offline data . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : 1 . Why is the top right slot of Table 1 blank ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "To the best of our knowledge , the Ant-Semi-circle domain is the most complex domain studied in the meta-RL setting to date [ 1 , 2 ] . Independence assumption : when the assumption does not hold , applying reward relabelling amounts to adding tasks to the data that have zero probability under the prior . This should not hurt the general performance of our method , as the VAE would simply learn to identify more possible MDPs . It can , however , make the agent search for the correct MDP less effectively , as it now has more MDP possibilities in the prior . If we were to add such an experiment , it is not clear what the comparison would be , as we do not have a better method for handling MDP ambiguity when the assumption does not hold . Baselines : we are not aware of stronger baselines for the setting we consider . This is the first study of deep Bayesian RL in the offline setting , and the relevant prior works are all based on Thompson sampling ( TS ) . Therefore , we compared against the strongest TS baselines that we can think of : exact computation in the discrete Gridworld , and online PEARL in continuous domains . If the reviewer has a stronger specific baseline in mind , we will happily add comparisons . Model-based approach : we emphasize that our method uses offline data * only during training * . At test time , it does not require any data from the new task prior to running the learned policy ! The model-based RL approach that the reviewer suggests does not answer the question how to explore effectively , which is exactly what our agent learns . To further clarify , observe for example the Ant-Semi-circle task -- here our learned agent finds the goal after only 2 episodes in a new test MDP . The agent 's strategy for collecting information is approximately optimal , and any other method , model-based or model-free , can not discover the reward with less than 2 episodes of interaction data . Table 1 is now full in the new , updated version . [ 1 ] Rakelly et al.Efficient off-policy meta-reinforcement learning via probabilistic context variables . arXiv preprint arXiv:1903.08254 , 2019 . [ 2 ] Mitchell et al.Offline meta reinforcement learning with advantage weighting . arXiv preprint arXiv:2008.06043 , 2020 ."}}