{"year": "2019", "forum": "r1NDBsAqY7", "title": "Unsupervised Word Discovery with Segmental Neural Language Models", "decision": "Reject", "meta_review": "a major issue or complaint from the reviewers seems to come from perhaps a wrong framing of this submission. i believe the framing of this work should have been a better language model (or translation model) with word discovery as an awesome side effect, which i carefully guess would've been a perfectly good story assuming that the perplexity result in Table 4 translates to text with blank spaces left in (it is not possible tell whether this is the case from the text alone.) even discounting R1, who i disagree with on quite a few points, the other reviewers also did not see much of the merit of this work, again probably due to the framing issue above. \n\ni highly encourage the authors to change the framing, evaluate it as a usual sequence model on various benchmarks and resubmit it to another venue.", "reviews": [{"review_id": "r1NDBsAqY7-0", "review_text": "This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. On the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. I think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed. The validation and test dataset have been modified to remove \"samples\" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets. The use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment \"doyou\" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison. Some minor comments: - A typo in the introduction \"... semi-Markov model. The the characters inside ...\". - Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented. - Is it possible to efficiently generate a sequence? [Updated after reconsidering other reviews] Although this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments . Regarding the question about the regularizer . Every model we know of in the literature that contains a nonparametric lexicon has had to have some explicit mechanism for preventing degenerate solutions ( variously based on MDL criteria , nonparametric priors , traditional regularization criteria ) ; while it \u2019 s possible that the regularization due to drop-out or early stopping would be sufficient to prevent overfitting , those methods are tailored to the neural net aspect of the model ( and used here ) , but we do not find it surprising that a model containing a new kind of memory would also need a new kind of capacity control . We see this not as a surprise or disappointment , but rather as another weak confirmation that the model is behaving as we expect . Regarding changes to the valid/test data , we removed a single utterance from the validation data ( since it contained the rare phoneme \u2018 zh \u2019 in the word pleasure , which was not present in the training data ) . Regarding the performance of models that discover \u201c bad \u201d segments but have good predictive distributions . There are indeed many such models , and traditional statistical criteria like mutual information discover many \u201c non-words \u201d . We will clarify this in the paper , but we wish to refer the reviewers and area chairs to our introduction which articulates that there is intrinsic scientific value in developing models that can chunk unsegmented input into actual words ( as humans do ) ."}, {"review_id": "r1NDBsAqY7-1", "review_text": "[Note to the authors: I was assigned this paper after the reviewing deadline.] The authors train language models on unsegmented text, simultaneously discovering word boundaries without direct supervision. Given the past history, but ignoring past segmentation decisions to keep computations tractable, the model predicts the next character segment (word-like unit) by combining a character-level LSTM with a lexical memory. To prevent overusing the lexical memory, which would lead to poor generalization, the authors propose a segment length penalty. Strengths: The model architecture is interesting, combining the benefits of a character-level model (open vocabulary) with those of a lexical model (effective for frequent character sequences). Despite the exponential number of possible segmentations, inference remains tractable using dynamic programming (with some simplifying assumptions). The ablation study clearly shows that both the lexical memory and the length penalty contribute significantly. Weaknesses: The writing quality is somewhat weak. Many errors should have been caught when proofreading the paper (e.g. \"The segmentation decisions and decisions\" and \"The the characters\" on page 1). I am confused by the key-value pairs of the lexical memory. Shouldn't character sequences be keys, and their trainable vector representations be values? It is hard to evaluate how good the language models are, as the strength of the baselines is unclear. How well-tuned is the LSTM? Comparison to some other segmentation approaches (not necessarily with language modeling) is limited. In particular, adaptor grammars perform very well on the Brent corpus [1]. However, [2] is mentioned briefly. As these other approaches work better for segmentation, the authors should carefully justify why having a single model that does both language modeling and word segmentation well matters. Many neural approaches have also been suggested for Chinese word segmentation (among others [3]). In these papers, results on the PKU dataset are much better. Are these directly comparable with yours? I would have liked a finer analysis of the impact of the length penalty. A plot showing how validation likelihood and segmentation performance vary as \\lambda is increased could potentially be interesting. [1] Johnson and Goldwater. \"Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars\", HLT, 2009 [2] Berg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\", NAACL, 2010 [3] Yang et al. Yang, Jie, Yue Zhang, and Fei Dong. \"Neural Word Segmentation with Rich Pretraining\", ACL, 2017", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you very much for the review . The paper about Chinese segmentation ( [ 3 ] ) is a supervised segmentation model . Which is not comparable with our paper . As we discuss in the paper and in the reply to a comment posted below , the best performing segmentation models [ 2 ] rely on hyper-parameters tuned by looking at segmentation performance . However , we argue this is not purely unsupervised learning as it requires gold segmentations at training time . Those models will not work on zero-resource languages or multiple languages without manually tuning the parameters for each language . Nor does supervised heldout tuning provide a plausible model for human language acquisition . That \u2019 s why we think it \u2019 s important that our model , which is tuned to maximize unsupervised held-out likelihood , achieves competitive segmentation performance . The keys are vectors and values are string as we use vectors to query what strings to be generated ."}, {"review_id": "r1NDBsAqY7-2", "review_text": "This paper proposes a neural architecture for segmental language modeling that enables unsupervised word discoveries. The architecture employes a two-stage architecture that a word might be a type, or a sequence of characters of its spellings. This idea is basically similar to Nested Pitman-Yor language models (Mochihashi et al. 2009) and two-stage language models (Goldwater et al. 2011), but the authors seem not to notice these previous work. Experimental results show some improvements on naive baselines, but clearly below the state-of-the-art in unsupervised word segmentation. As noted above, the crucial drawback of this paper is that the authors are completely unaware of latest achievements on unsupervised word segmentation and discovery, rather than old, simplistic baselines such as Goldwater+ (2009, idea is based on Goldwater+ ACL 2006) or Berg-Kirkpatrick (2010). The idea of using characters and words is already exploited in Mochihashi+ (ACL 2009) in a nonparametric Bayesian framework; it has a better F1 than this work by a large margin. Moreover, it is recently extended (Uchiumi+ TACL 2015) to also include latent word categories as well as segmentations to yield the state-of-the-art accuracies on F1=81.6 on PKU corpus, as compared to 73.1 in this paper. Note that they employ a prior distribution on segment lengths as a (mixture of) Poisson distributions or negative binomials whose parameters are automatically learned during inference, as compared to a post-hoc regularization used in this paper. In a Bayesian framework, interpolations between words and characters are theoretically derived and quite carefully learned, and regularizations are automatically adjusted. While neural architectures have some potentials to improve over them, current heuristic architectures that have lower performance does not have any advantage over these methods, both theoretically and empicially. ", "rating": "3: Clear rejection", "reply_text": "This work presents a recurrent neural network language model that obtains better predictive distributions ( perplexity ) than an LSTM while also discovering the words that exist in language . The review above misconstrues the aim of this paper as simply producing the best segmentation accuracy , which the papers cited achieve by tuning for segmentation performance on held out data and sacrificing predictive accuracy . While we agree with the reviewer that there has been much excellent work done on nonparametric Bayesian segmentation ( and two key ideas from that modeling tradition directly inspired this work ! ) , no such model has been shown to achieve perplexities close to those of an RNN . However no previous RNN has been shown to discover plausible word segmentations . Our model achieves both . In doing so we argue that word segmentation is not a task that should be studied in isolation from the rest of the language learning but that the flexibility of neural models means they can approach other aspects ( e.g. , grounding in different modalities or tasks , learning large scale syntactic regularities ) more naturally than would be practical with current Bayesian techniques . Below we elaborate on several more technical objections to this review : First , our decision to focus on DP/HDP models rather than the extensions referred to in the review ( specifically PYP/HPYPs , nested PYPs , integrating out hyperparameters , etc . ) was not due to ignorance , but rather that we were incorporating a two core ideas from Bayesian nonparametric word segmentation/language modeling into neural networks and we chose the simplest possible Bayesian model that made our points . These are : ( 1 ) that a lexicon that memorizes word chunks is useful for inducing good segmentations ; ( 2 ) that capacity control is important when you have a lexicon like this . We do agree that nested PYPs , which learn to model character sequences ( although not across word boundaries ) , deserve discussion and we will update the paper accordingly ( again we emphasize that this is an oversight , not something that changes the meaningfulness of our results ) . Thus , the DP/HDP models otherwise perfectly illustrates the points they needed to illustrate , and the newer variations do not offer any additional insight into how to fix the problems that RNNs have with discovering words . Second , our results are not precisely comparable since Bayesian unsupervised learning has traditionally been evaluated in a setup which does not distinguish between a train and test set , or which uses observations from both when performing posterior inference . As we demonstrated , in Bayesian models , selecting hyperparameters empirically ( ie , based on held-out likelihood ) results in less effective structure discovery than setting the hyperparameters subjectively ( however , since some standard datasets did not have a train/test split until this paper , we expect that in many cases , these models were chosen based on reported segmentation accuracies ! ) . We certainly appreciate the insights that have been enabled by using both methodologies , but our perspective is that relying on held-out likelihood for model selection is eminently defensible methodology . However , held-out likelihood is indeed a radically different development/training/evaluation methodology for working on segmentation that is a better fit for neural models ( and , we think , Bayesian models as well ) than what came before , and it does make the results incomparable . Finally , another source of incompatibility is that the Uchiumi et al ( 2015 ) length distribution correction relies on hand-engineered features . We expected these were selected to improve reported segmentation accuracy ( rather than validation likelihood ) , and as an ICLR paper , we are exploring how well we can do with learning representations , rather than engineering them . Third , the goal of this paper was to show that it is possible to align the goals of good segment discovery with good held-out models of language ( after all , humans are good at both ! ) . In their zeal to argue that our segmentation accuracy lags behind that of the best Bayesian models ( which we questioned in the previous paragraph ) , the reviewers ignore the crucial fact that the most basic RNNs outperform the best hierarchical Bayesian language models by far in terms of predicting held-out data . Surprisingly , although posterior predictive checking is a standard tool for assessing Bayesian models , none of the existing Bayesian segmentation papers seem to have used this methodology , and so we had to include our own likelihood experiments ( Table 4 ) to demonstrate this disparity . These results show clearly that while Bayesian models are perhaps slightly better than our models in terms of segmentation accuracy , they are far less good than RNNs in terms of predictive accuracy . On the other hand , our RNNs are good at both ."}], "0": {"review_id": "r1NDBsAqY7-0", "review_text": "This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. On the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. I think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed. The validation and test dataset have been modified to remove \"samples\" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets. The use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment \"doyou\" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison. Some minor comments: - A typo in the introduction \"... semi-Markov model. The the characters inside ...\". - Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented. - Is it possible to efficiently generate a sequence? [Updated after reconsidering other reviews] Although this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments . Regarding the question about the regularizer . Every model we know of in the literature that contains a nonparametric lexicon has had to have some explicit mechanism for preventing degenerate solutions ( variously based on MDL criteria , nonparametric priors , traditional regularization criteria ) ; while it \u2019 s possible that the regularization due to drop-out or early stopping would be sufficient to prevent overfitting , those methods are tailored to the neural net aspect of the model ( and used here ) , but we do not find it surprising that a model containing a new kind of memory would also need a new kind of capacity control . We see this not as a surprise or disappointment , but rather as another weak confirmation that the model is behaving as we expect . Regarding changes to the valid/test data , we removed a single utterance from the validation data ( since it contained the rare phoneme \u2018 zh \u2019 in the word pleasure , which was not present in the training data ) . Regarding the performance of models that discover \u201c bad \u201d segments but have good predictive distributions . There are indeed many such models , and traditional statistical criteria like mutual information discover many \u201c non-words \u201d . We will clarify this in the paper , but we wish to refer the reviewers and area chairs to our introduction which articulates that there is intrinsic scientific value in developing models that can chunk unsegmented input into actual words ( as humans do ) ."}, "1": {"review_id": "r1NDBsAqY7-1", "review_text": "[Note to the authors: I was assigned this paper after the reviewing deadline.] The authors train language models on unsegmented text, simultaneously discovering word boundaries without direct supervision. Given the past history, but ignoring past segmentation decisions to keep computations tractable, the model predicts the next character segment (word-like unit) by combining a character-level LSTM with a lexical memory. To prevent overusing the lexical memory, which would lead to poor generalization, the authors propose a segment length penalty. Strengths: The model architecture is interesting, combining the benefits of a character-level model (open vocabulary) with those of a lexical model (effective for frequent character sequences). Despite the exponential number of possible segmentations, inference remains tractable using dynamic programming (with some simplifying assumptions). The ablation study clearly shows that both the lexical memory and the length penalty contribute significantly. Weaknesses: The writing quality is somewhat weak. Many errors should have been caught when proofreading the paper (e.g. \"The segmentation decisions and decisions\" and \"The the characters\" on page 1). I am confused by the key-value pairs of the lexical memory. Shouldn't character sequences be keys, and their trainable vector representations be values? It is hard to evaluate how good the language models are, as the strength of the baselines is unclear. How well-tuned is the LSTM? Comparison to some other segmentation approaches (not necessarily with language modeling) is limited. In particular, adaptor grammars perform very well on the Brent corpus [1]. However, [2] is mentioned briefly. As these other approaches work better for segmentation, the authors should carefully justify why having a single model that does both language modeling and word segmentation well matters. Many neural approaches have also been suggested for Chinese word segmentation (among others [3]). In these papers, results on the PKU dataset are much better. Are these directly comparable with yours? I would have liked a finer analysis of the impact of the length penalty. A plot showing how validation likelihood and segmentation performance vary as \\lambda is increased could potentially be interesting. [1] Johnson and Goldwater. \"Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars\", HLT, 2009 [2] Berg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\", NAACL, 2010 [3] Yang et al. Yang, Jie, Yue Zhang, and Fei Dong. \"Neural Word Segmentation with Rich Pretraining\", ACL, 2017", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you very much for the review . The paper about Chinese segmentation ( [ 3 ] ) is a supervised segmentation model . Which is not comparable with our paper . As we discuss in the paper and in the reply to a comment posted below , the best performing segmentation models [ 2 ] rely on hyper-parameters tuned by looking at segmentation performance . However , we argue this is not purely unsupervised learning as it requires gold segmentations at training time . Those models will not work on zero-resource languages or multiple languages without manually tuning the parameters for each language . Nor does supervised heldout tuning provide a plausible model for human language acquisition . That \u2019 s why we think it \u2019 s important that our model , which is tuned to maximize unsupervised held-out likelihood , achieves competitive segmentation performance . The keys are vectors and values are string as we use vectors to query what strings to be generated ."}, "2": {"review_id": "r1NDBsAqY7-2", "review_text": "This paper proposes a neural architecture for segmental language modeling that enables unsupervised word discoveries. The architecture employes a two-stage architecture that a word might be a type, or a sequence of characters of its spellings. This idea is basically similar to Nested Pitman-Yor language models (Mochihashi et al. 2009) and two-stage language models (Goldwater et al. 2011), but the authors seem not to notice these previous work. Experimental results show some improvements on naive baselines, but clearly below the state-of-the-art in unsupervised word segmentation. As noted above, the crucial drawback of this paper is that the authors are completely unaware of latest achievements on unsupervised word segmentation and discovery, rather than old, simplistic baselines such as Goldwater+ (2009, idea is based on Goldwater+ ACL 2006) or Berg-Kirkpatrick (2010). The idea of using characters and words is already exploited in Mochihashi+ (ACL 2009) in a nonparametric Bayesian framework; it has a better F1 than this work by a large margin. Moreover, it is recently extended (Uchiumi+ TACL 2015) to also include latent word categories as well as segmentations to yield the state-of-the-art accuracies on F1=81.6 on PKU corpus, as compared to 73.1 in this paper. Note that they employ a prior distribution on segment lengths as a (mixture of) Poisson distributions or negative binomials whose parameters are automatically learned during inference, as compared to a post-hoc regularization used in this paper. In a Bayesian framework, interpolations between words and characters are theoretically derived and quite carefully learned, and regularizations are automatically adjusted. While neural architectures have some potentials to improve over them, current heuristic architectures that have lower performance does not have any advantage over these methods, both theoretically and empicially. ", "rating": "3: Clear rejection", "reply_text": "This work presents a recurrent neural network language model that obtains better predictive distributions ( perplexity ) than an LSTM while also discovering the words that exist in language . The review above misconstrues the aim of this paper as simply producing the best segmentation accuracy , which the papers cited achieve by tuning for segmentation performance on held out data and sacrificing predictive accuracy . While we agree with the reviewer that there has been much excellent work done on nonparametric Bayesian segmentation ( and two key ideas from that modeling tradition directly inspired this work ! ) , no such model has been shown to achieve perplexities close to those of an RNN . However no previous RNN has been shown to discover plausible word segmentations . Our model achieves both . In doing so we argue that word segmentation is not a task that should be studied in isolation from the rest of the language learning but that the flexibility of neural models means they can approach other aspects ( e.g. , grounding in different modalities or tasks , learning large scale syntactic regularities ) more naturally than would be practical with current Bayesian techniques . Below we elaborate on several more technical objections to this review : First , our decision to focus on DP/HDP models rather than the extensions referred to in the review ( specifically PYP/HPYPs , nested PYPs , integrating out hyperparameters , etc . ) was not due to ignorance , but rather that we were incorporating a two core ideas from Bayesian nonparametric word segmentation/language modeling into neural networks and we chose the simplest possible Bayesian model that made our points . These are : ( 1 ) that a lexicon that memorizes word chunks is useful for inducing good segmentations ; ( 2 ) that capacity control is important when you have a lexicon like this . We do agree that nested PYPs , which learn to model character sequences ( although not across word boundaries ) , deserve discussion and we will update the paper accordingly ( again we emphasize that this is an oversight , not something that changes the meaningfulness of our results ) . Thus , the DP/HDP models otherwise perfectly illustrates the points they needed to illustrate , and the newer variations do not offer any additional insight into how to fix the problems that RNNs have with discovering words . Second , our results are not precisely comparable since Bayesian unsupervised learning has traditionally been evaluated in a setup which does not distinguish between a train and test set , or which uses observations from both when performing posterior inference . As we demonstrated , in Bayesian models , selecting hyperparameters empirically ( ie , based on held-out likelihood ) results in less effective structure discovery than setting the hyperparameters subjectively ( however , since some standard datasets did not have a train/test split until this paper , we expect that in many cases , these models were chosen based on reported segmentation accuracies ! ) . We certainly appreciate the insights that have been enabled by using both methodologies , but our perspective is that relying on held-out likelihood for model selection is eminently defensible methodology . However , held-out likelihood is indeed a radically different development/training/evaluation methodology for working on segmentation that is a better fit for neural models ( and , we think , Bayesian models as well ) than what came before , and it does make the results incomparable . Finally , another source of incompatibility is that the Uchiumi et al ( 2015 ) length distribution correction relies on hand-engineered features . We expected these were selected to improve reported segmentation accuracy ( rather than validation likelihood ) , and as an ICLR paper , we are exploring how well we can do with learning representations , rather than engineering them . Third , the goal of this paper was to show that it is possible to align the goals of good segment discovery with good held-out models of language ( after all , humans are good at both ! ) . In their zeal to argue that our segmentation accuracy lags behind that of the best Bayesian models ( which we questioned in the previous paragraph ) , the reviewers ignore the crucial fact that the most basic RNNs outperform the best hierarchical Bayesian language models by far in terms of predicting held-out data . Surprisingly , although posterior predictive checking is a standard tool for assessing Bayesian models , none of the existing Bayesian segmentation papers seem to have used this methodology , and so we had to include our own likelihood experiments ( Table 4 ) to demonstrate this disparity . These results show clearly that while Bayesian models are perhaps slightly better than our models in terms of segmentation accuracy , they are far less good than RNNs in terms of predictive accuracy . On the other hand , our RNNs are good at both ."}}