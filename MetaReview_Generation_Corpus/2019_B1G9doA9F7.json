{"year": "2019", "forum": "B1G9doA9F7", "title": "Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation", "decision": "Accept (Poster)", "meta_review": "The authors propose a method for low-resource domain adaptation where the number of examples available in the target domain are limited. The proposed method modifies the basic approach in a CycleGAN by augmenting it with a \u201ccontent\u201d (task-specific) loss, instead of the standard reconstruction error. The authors also demonstrate experimentally that it is important to enforce the loss in both directions (target \u2192 source and source --> target). Experiments are conducted on both supervised as well as unsupervised settings.\nThe main concern expressed by the reviewers relates to the novelty of the approach since it is a relatively straightforward extension of CycleGAN/CyCADA, but in the view of a majority of reviewers the work serves a useful contribution as a practical method for developing systems in low-resource conditions where it is feasible to label a few new instances. Although the reviewers were not unanimous in their recommendations, on balance in the view of the AC the work is a useful contribution with clear and detailed experiments in the revised version.\n", "reviews": [{"review_id": "B1G9doA9F7-0", "review_text": "The authors propose an extension of cycle-consistent adversarial adaptation methods in order to tackle domain adaptation in settings where a limited amount of supervised target data is available (though they also validate their model in the standard unsupervised setting as well). The method appears to be a natural generalization/extension of CycleGAN/CyCADA. It uses the ideas of the semantic consistency loss and training on adapted data from CyCADA, but \"fills out\" the model by applying these techniques in both directions (whereas CyCADA only applied them in the source-to-target direction). The writing in this paper is a little awkward at times (many omitted articles such as \"the\" or \"a'), but, with a few exceptions, it is generally easy to understand what the authors are saying. They provide experiments in a variety of settings in order to validate their model, including both visual domain adaptation and speech domain adaptation. The experiments show that their model is effective both in low-resource supervised adaptation settings as well as high-resource unsupervised adaptation settings. An ablation study, provided in Section 4.1, helps to understand how well the various instantiations of the authors' model perform, indicating that enforcing consistency in both methods is crucial to achieving performance beyond the simple baselines. It's a little hard to understand how this method stands in comparison to existing work. Table 3 helps to show that the model can scale up to the high-resource setting, but it would also be nice to see the reverse: comparisons against existing work run in the limited data setting, to better understand how much limited data negatively impacts the performance of models that weren't designed with this setting in mind. I would've also liked to see more comparisons against the simple baseline of a classifier trained exclusively on the available supervised target data, or with the source and target data together\u2014in my experience, these baselines can prove to be surprisingly strong, and would give a better sense of how effective this paper's contributions are. This corresponds to rows 2 and 3 of Table 1, and inspection of the numbers in that table shows that the baseline performance is quite strong even relative to the proposed method, so it would be nice to see these numbers in Table 2 as well, since that table is intended to demonstrate the model's effectiveness across a variety of different domain shifts. While it's nice that the model is experimentally validated on the speech domain, the experiment itself is not explained well. The speech experiments are hard to understand\u2014it's unclear what the various training sets are, such as \"Adapted Male\" or \"All Data,\" making it hard to understand exactly what numbers should be compared. Why is there no CycleGAN result for \"Female + Adapted Male,\" or \"All Data + Adapted Male,\" for example? The paper would greatly benefit from a more careful explanation and analysis of this experimental setting. Ultimately, I think the idea is a nice generalization of previous work, and the experiments seem to indicate that the model is effective, but the limited scope of the experiments prevent me from being entirely convinced. The inclusion of additional baselines and a great deal of clarification on the speech experiments would improve the quality of this paper enormously. --- Update: After looking over the additional revisions and experiments, I'm bumping this to a weak accept. I agree with reviewer 3 that novelty is not the greatest, but there is a useful contribution here, and the demonstration of its effectiveness on low resource settings is valuable, since in a practical setting it is usually feasible to manually label a few examples. I'm still not convinced by the TIMIT experiments, now that I better understand them, since the F+M baseline is quite strong and very simple to run. It simply doesn't seem worthwhile to introduce all of this extra machinery for such a marginal improvement, but the experiment does serve the job of at least demonstrating an improvement over existing methods.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Comment on \u201c low-resource supervised adaptation , Table 2 \u201d : To provide more baseline results on low-resource supervised adaptation , we ran additional experiments and replaced table 2 with bar plots in Figure 3 . Baselines include classifier trained on low-resource target data , and including source data , with no adaptation . As shown in Figure 3 , Augmented-Cyc algorithm outperforms FADA model and the two baselines . Comment on \u201c comparing with existing works on low-resource unsupervised adaptation \u201d : We added experiments on low-resource unsupervised adaptation to compare with CyCADA , and the results are shown in Figure 2 . This experiment investigates the effectiveness and robustness of using two cycle with semantic consistency enforced by auxiliary task loss , compared to CyCADA , where semantic consistency is enforced by reconstruction loss . As shown in Figure 2 , CyCADA model fails to learn a good adaptation , where target domain contains few unsupervised data . Additionally , CyCADA model shows high instability in low-resource situation . Our model achieves more robust and better performance . We think this is attributed to proper use of source classifier to enforce consistency and robustness that we get by using two cycles ( also shown in ablation study in Table 1 ) . Comment on speech domain experiments : We have edited the speech experiment section for more clarification . To mention some , \u201c Adapted Male \u201d is changed to \u201c Male- > Female \u201d to preserve consistency in notation . \u201c All Data \u201d refers to \u201c Male+Female \u201d with no adaption . CycleGAN results are added for '' Female + Adapted Male , '' or `` All Data + Adapted Male , \u201d"}, {"review_id": "B1G9doA9F7-1", "review_text": "This paper introduces a domain adaptation approach based on the idea of Cyclic GAN. Two different algorithms are proposed. The first one incorporates a semantic consistency loss based on domain-specific classifiers acting on full cycles of the of the generators. The second one also makes use of domain-specific classifiers, but acting either directly on the training samples or on the data mapped from one domain to the other. Strengths: - The different terms in the proposed loss functions are well justified. - The results on low-resources supervised domain adaptation indicate that the method works better than the that of Motiian et al. 2017. Weaknesses: - Novelty is limited: The two algorithms are essentially small modification of the semantic consistency term used in Hoffman et al. 2018. They involve making use of both the source and target classifiers, instead of only the source one, and, for the relaxed version, making use of complete cycles instead of just one mapping from one domain to the other. While the modifications are justified, I find this a bit weak for ICLR. - It is not clear to me why it is worth presenting the relaxed cycle-consistency object, since it always yields worse results than the augmented one. In fact, at first, I though both objectives would be combined in a single loss, and was thus surprised not to see Eq. 5 appear in Algorithm 1. It only became clear when reading the experiments that the authors were treating the two objectives as two different algorithms. Note that, in addition to not performing as well as the augmented version, it is also unclear how the relaxed one could work in the unsupervised scenario. - Experiments: * In 4.1, the authors mention that 10 samples per class are available in the target domain. Are they labeled or unlabeled? If labeled, are additional unlabeled samples also used? * In Table 1, and in Table 3, is there a method that corresponds to CyCADA? I feel that this comparison would be useful considering the similarity. That said, I also understand that CyCADA uses both a reconstruction term (as in Eq. 4) and a semantic consistency one, whereas here only a semantic reconstruction term is used. I therefore suggest the authors to also compare with a baseline that replaces their objective with the semantic consistency one of CyCADA, i.e., CyCADA without reconstruction term. * In 4.2, it is again not entirely clear if the authors use only the few labeled samples, or if this is complemented with additional unlabeled samples. In any event, does this reproduce the setting used by Motiian et al. 2017? * As the argument is that the proposed loss is better than the reconstruction one and that of Hoffman et al. 2018 for low-resource supervised adaptation, it would be worth demonstrating this empirically in Table 2. Summary: The proposed objective functions are well motivated, but I feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at ICLR. After Response: After the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Comment on Weakness , and similarity to CyCADA model : To differentiate between our model and CyCADA , below is the detail of two models and how they perform semantic consistency , and enforcing style adaptation CyCADA : semantic ( content ) consistency is enforced by two loss ; reconstruction loss ( CycleGAN ) ; and additionally using reconstruction at feature level . Style adaptation is enforced using adversarial learning on pixel ( observation ) and feature ( hidden ) space . Therefore , it need to learn additional model for representing data in feature space . Augmented-Cyc : semantic consistency is shown to be achieved by only using auxiliary task loss for each cycle . Style adaptation is achieved by using adversarial learning on pixel ( observation ) space only . We use cycles in both direction to achieve robust performance in low resource ( either supervised or unsupervised ) setting . Therefor , CyCADA requires an additional adversarial learning at feature space , while our model achieve this by only adaptation at observation space . Moreover , to compare the performance of the two model on variable-size target domain , we added more experiments for low resource unsupervised adaptation ( see Figure 2 ) . It is evident that CyCADA model fails to provide suitable adaptation , while our model outperforms by large margin , when target domain data is small Note : Both our ablation ( see Table 1 ) and additional experiments ( see Figure 2 ) suggest the benefit of using two cycles for low resource situation , whether supervised or unsupervised . Therefore , we think this is an important aspect for robust domain adaptation under resource constraint . Comment on relaxed cycle consistency : The main purpose of presenting relaxed-consistency results in ablation study is to demonstrate the effectiveness of using auxiliary task loss in any or both cycles , rather than L1 reconstruction loss . We have only evaluated relaxed-consistency in low-resource supervised setting , and it is not evaluated for unsupervised adaptation . In unsupervised setting , we are using source classifier M_ { S } as pseudo-labeler of target samples . Note : In this setting , if we turn off using task model M_ { T } to be trained using source data , this is similar to using relaxed version in unsupervised adaptation Comments on Experiments : For all low resource target domain experiment , only the denoted number of samples are used , irrespective if they are labeled or not . For example , in supervised case , 10 labeled sample per class means we only use 10 labeled samples per class in the target domain is used and no other data is used in the target domain . Similarly for unsupervised case , 5 samples per class means only used 5 unsupervised samples from target domain . - Section 4.1 : we only used 10 labeled sample per class . In this experiment , NO unlabeled data is used . - Table 1 : this table is intended for ablation of our model . - Table 3 : we have added CyCADA results in this table for comparison . To directly compare our model with CyCADA , we added new experiments on variable-size target domain which is presented in Figure 2 . - Section 4.2 : Table 2 is replaced with Figure 3 , for low-resource supervised adaptation . In this experiment , no unlabeled data is used , and it is a direct comparison between our model and FADA ( Motiian et al.2017 ) In Figure 2 , we have shown the benefit of the proposed auxiliary task-specific loss to reconstruction loss ( CyCADA ) on low-resource unsupervised domain adaptation ."}, {"review_id": "B1G9doA9F7-2", "review_text": " I am putting \"weak accept\" because I think the paper addresses an important problem (domain adaptation) and has an interesting approach. As the other reviewers pointed out, it's maybe not *super* novel. But it's still interesting, and pretty readable for the most part. I do question the statistical significance of the TIMIT experiments: TIMIT has a very tiny test set to start with, and by focusing on the female portion only you are further reducing the amount. Small point: I don't think GANs are technically nonparametric, as the neural nets do have parameters. I am a little skeptical that this method would have as general applicability or usefulness as the authors seem to think. The reason is that, since the cycle constraint no longer exists, there is nothing to stop the network from just figuring out the class label of the input (say) image, and treating all the rest of the information in that image as noise the same way a regular non-cyclic GAN would treat it. Of course, one wouldn't expect a convolutional network to behave like this, but in theory it could happen in general cases. This is just speculation though. Personally I would have tended to accept the paper, but I'm not going to argue with the other reviewers, who are probably more familiar with GAN literature than me. -- I am changing from \"marginally above acceptance threshold\" to \"clear accept\" after reading the response and thinking about the paper a bit more. I acknowledge that the difference from previously published methods is not that large, but I still think it has value as it's getting quite close to being a practical method for generating fake training data for speech recognition. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "- Comment on \u201c statistical significance on TIMIT experiments \u201d : We have chosen TIMIT dataset because of its inherent low-resource domain for different genders . As shown in Table 3 , when using only Male speech for training the network , testing on female genders results in a large margin ( 11 % on phoneme error recognition ) , compared to baseline . However by using only \u201c Male- > Female \u201d data in training of proposed model , this gap can be reduced by ~10 % for 124 voices in validation and 64 voices in test set for female domain . - Comment on \u201c Whether GAN \u2019 s are parametric or non-parametrics \u201d : Here we refer to the classical parametric models for modeling data distribution . In this sense , the generator in GAN implicitly models the true distribution . Therefore , we categorize GAN as a non-parametric density estimation model since it does not assume any form of distribution . - Comment on general applicability of the proposed domain adaptation model : Since for any sample , whether target or source , there are two classifier in the cycle to preserve the class label information during transformation across domains , we believe that this implicit enforcement of content preservation will hold in broader applications . If the model is able to figure out which part is important for a certain class and ignore other parts , that is a desired behavior , since only those parts are important for the task in mind ."}], "0": {"review_id": "B1G9doA9F7-0", "review_text": "The authors propose an extension of cycle-consistent adversarial adaptation methods in order to tackle domain adaptation in settings where a limited amount of supervised target data is available (though they also validate their model in the standard unsupervised setting as well). The method appears to be a natural generalization/extension of CycleGAN/CyCADA. It uses the ideas of the semantic consistency loss and training on adapted data from CyCADA, but \"fills out\" the model by applying these techniques in both directions (whereas CyCADA only applied them in the source-to-target direction). The writing in this paper is a little awkward at times (many omitted articles such as \"the\" or \"a'), but, with a few exceptions, it is generally easy to understand what the authors are saying. They provide experiments in a variety of settings in order to validate their model, including both visual domain adaptation and speech domain adaptation. The experiments show that their model is effective both in low-resource supervised adaptation settings as well as high-resource unsupervised adaptation settings. An ablation study, provided in Section 4.1, helps to understand how well the various instantiations of the authors' model perform, indicating that enforcing consistency in both methods is crucial to achieving performance beyond the simple baselines. It's a little hard to understand how this method stands in comparison to existing work. Table 3 helps to show that the model can scale up to the high-resource setting, but it would also be nice to see the reverse: comparisons against existing work run in the limited data setting, to better understand how much limited data negatively impacts the performance of models that weren't designed with this setting in mind. I would've also liked to see more comparisons against the simple baseline of a classifier trained exclusively on the available supervised target data, or with the source and target data together\u2014in my experience, these baselines can prove to be surprisingly strong, and would give a better sense of how effective this paper's contributions are. This corresponds to rows 2 and 3 of Table 1, and inspection of the numbers in that table shows that the baseline performance is quite strong even relative to the proposed method, so it would be nice to see these numbers in Table 2 as well, since that table is intended to demonstrate the model's effectiveness across a variety of different domain shifts. While it's nice that the model is experimentally validated on the speech domain, the experiment itself is not explained well. The speech experiments are hard to understand\u2014it's unclear what the various training sets are, such as \"Adapted Male\" or \"All Data,\" making it hard to understand exactly what numbers should be compared. Why is there no CycleGAN result for \"Female + Adapted Male,\" or \"All Data + Adapted Male,\" for example? The paper would greatly benefit from a more careful explanation and analysis of this experimental setting. Ultimately, I think the idea is a nice generalization of previous work, and the experiments seem to indicate that the model is effective, but the limited scope of the experiments prevent me from being entirely convinced. The inclusion of additional baselines and a great deal of clarification on the speech experiments would improve the quality of this paper enormously. --- Update: After looking over the additional revisions and experiments, I'm bumping this to a weak accept. I agree with reviewer 3 that novelty is not the greatest, but there is a useful contribution here, and the demonstration of its effectiveness on low resource settings is valuable, since in a practical setting it is usually feasible to manually label a few examples. I'm still not convinced by the TIMIT experiments, now that I better understand them, since the F+M baseline is quite strong and very simple to run. It simply doesn't seem worthwhile to introduce all of this extra machinery for such a marginal improvement, but the experiment does serve the job of at least demonstrating an improvement over existing methods.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Comment on \u201c low-resource supervised adaptation , Table 2 \u201d : To provide more baseline results on low-resource supervised adaptation , we ran additional experiments and replaced table 2 with bar plots in Figure 3 . Baselines include classifier trained on low-resource target data , and including source data , with no adaptation . As shown in Figure 3 , Augmented-Cyc algorithm outperforms FADA model and the two baselines . Comment on \u201c comparing with existing works on low-resource unsupervised adaptation \u201d : We added experiments on low-resource unsupervised adaptation to compare with CyCADA , and the results are shown in Figure 2 . This experiment investigates the effectiveness and robustness of using two cycle with semantic consistency enforced by auxiliary task loss , compared to CyCADA , where semantic consistency is enforced by reconstruction loss . As shown in Figure 2 , CyCADA model fails to learn a good adaptation , where target domain contains few unsupervised data . Additionally , CyCADA model shows high instability in low-resource situation . Our model achieves more robust and better performance . We think this is attributed to proper use of source classifier to enforce consistency and robustness that we get by using two cycles ( also shown in ablation study in Table 1 ) . Comment on speech domain experiments : We have edited the speech experiment section for more clarification . To mention some , \u201c Adapted Male \u201d is changed to \u201c Male- > Female \u201d to preserve consistency in notation . \u201c All Data \u201d refers to \u201c Male+Female \u201d with no adaption . CycleGAN results are added for '' Female + Adapted Male , '' or `` All Data + Adapted Male , \u201d"}, "1": {"review_id": "B1G9doA9F7-1", "review_text": "This paper introduces a domain adaptation approach based on the idea of Cyclic GAN. Two different algorithms are proposed. The first one incorporates a semantic consistency loss based on domain-specific classifiers acting on full cycles of the of the generators. The second one also makes use of domain-specific classifiers, but acting either directly on the training samples or on the data mapped from one domain to the other. Strengths: - The different terms in the proposed loss functions are well justified. - The results on low-resources supervised domain adaptation indicate that the method works better than the that of Motiian et al. 2017. Weaknesses: - Novelty is limited: The two algorithms are essentially small modification of the semantic consistency term used in Hoffman et al. 2018. They involve making use of both the source and target classifiers, instead of only the source one, and, for the relaxed version, making use of complete cycles instead of just one mapping from one domain to the other. While the modifications are justified, I find this a bit weak for ICLR. - It is not clear to me why it is worth presenting the relaxed cycle-consistency object, since it always yields worse results than the augmented one. In fact, at first, I though both objectives would be combined in a single loss, and was thus surprised not to see Eq. 5 appear in Algorithm 1. It only became clear when reading the experiments that the authors were treating the two objectives as two different algorithms. Note that, in addition to not performing as well as the augmented version, it is also unclear how the relaxed one could work in the unsupervised scenario. - Experiments: * In 4.1, the authors mention that 10 samples per class are available in the target domain. Are they labeled or unlabeled? If labeled, are additional unlabeled samples also used? * In Table 1, and in Table 3, is there a method that corresponds to CyCADA? I feel that this comparison would be useful considering the similarity. That said, I also understand that CyCADA uses both a reconstruction term (as in Eq. 4) and a semantic consistency one, whereas here only a semantic reconstruction term is used. I therefore suggest the authors to also compare with a baseline that replaces their objective with the semantic consistency one of CyCADA, i.e., CyCADA without reconstruction term. * In 4.2, it is again not entirely clear if the authors use only the few labeled samples, or if this is complemented with additional unlabeled samples. In any event, does this reproduce the setting used by Motiian et al. 2017? * As the argument is that the proposed loss is better than the reconstruction one and that of Hoffman et al. 2018 for low-resource supervised adaptation, it would be worth demonstrating this empirically in Table 2. Summary: The proposed objective functions are well motivated, but I feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at ICLR. After Response: After the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Comment on Weakness , and similarity to CyCADA model : To differentiate between our model and CyCADA , below is the detail of two models and how they perform semantic consistency , and enforcing style adaptation CyCADA : semantic ( content ) consistency is enforced by two loss ; reconstruction loss ( CycleGAN ) ; and additionally using reconstruction at feature level . Style adaptation is enforced using adversarial learning on pixel ( observation ) and feature ( hidden ) space . Therefore , it need to learn additional model for representing data in feature space . Augmented-Cyc : semantic consistency is shown to be achieved by only using auxiliary task loss for each cycle . Style adaptation is achieved by using adversarial learning on pixel ( observation ) space only . We use cycles in both direction to achieve robust performance in low resource ( either supervised or unsupervised ) setting . Therefor , CyCADA requires an additional adversarial learning at feature space , while our model achieve this by only adaptation at observation space . Moreover , to compare the performance of the two model on variable-size target domain , we added more experiments for low resource unsupervised adaptation ( see Figure 2 ) . It is evident that CyCADA model fails to provide suitable adaptation , while our model outperforms by large margin , when target domain data is small Note : Both our ablation ( see Table 1 ) and additional experiments ( see Figure 2 ) suggest the benefit of using two cycles for low resource situation , whether supervised or unsupervised . Therefore , we think this is an important aspect for robust domain adaptation under resource constraint . Comment on relaxed cycle consistency : The main purpose of presenting relaxed-consistency results in ablation study is to demonstrate the effectiveness of using auxiliary task loss in any or both cycles , rather than L1 reconstruction loss . We have only evaluated relaxed-consistency in low-resource supervised setting , and it is not evaluated for unsupervised adaptation . In unsupervised setting , we are using source classifier M_ { S } as pseudo-labeler of target samples . Note : In this setting , if we turn off using task model M_ { T } to be trained using source data , this is similar to using relaxed version in unsupervised adaptation Comments on Experiments : For all low resource target domain experiment , only the denoted number of samples are used , irrespective if they are labeled or not . For example , in supervised case , 10 labeled sample per class means we only use 10 labeled samples per class in the target domain is used and no other data is used in the target domain . Similarly for unsupervised case , 5 samples per class means only used 5 unsupervised samples from target domain . - Section 4.1 : we only used 10 labeled sample per class . In this experiment , NO unlabeled data is used . - Table 1 : this table is intended for ablation of our model . - Table 3 : we have added CyCADA results in this table for comparison . To directly compare our model with CyCADA , we added new experiments on variable-size target domain which is presented in Figure 2 . - Section 4.2 : Table 2 is replaced with Figure 3 , for low-resource supervised adaptation . In this experiment , no unlabeled data is used , and it is a direct comparison between our model and FADA ( Motiian et al.2017 ) In Figure 2 , we have shown the benefit of the proposed auxiliary task-specific loss to reconstruction loss ( CyCADA ) on low-resource unsupervised domain adaptation ."}, "2": {"review_id": "B1G9doA9F7-2", "review_text": " I am putting \"weak accept\" because I think the paper addresses an important problem (domain adaptation) and has an interesting approach. As the other reviewers pointed out, it's maybe not *super* novel. But it's still interesting, and pretty readable for the most part. I do question the statistical significance of the TIMIT experiments: TIMIT has a very tiny test set to start with, and by focusing on the female portion only you are further reducing the amount. Small point: I don't think GANs are technically nonparametric, as the neural nets do have parameters. I am a little skeptical that this method would have as general applicability or usefulness as the authors seem to think. The reason is that, since the cycle constraint no longer exists, there is nothing to stop the network from just figuring out the class label of the input (say) image, and treating all the rest of the information in that image as noise the same way a regular non-cyclic GAN would treat it. Of course, one wouldn't expect a convolutional network to behave like this, but in theory it could happen in general cases. This is just speculation though. Personally I would have tended to accept the paper, but I'm not going to argue with the other reviewers, who are probably more familiar with GAN literature than me. -- I am changing from \"marginally above acceptance threshold\" to \"clear accept\" after reading the response and thinking about the paper a bit more. I acknowledge that the difference from previously published methods is not that large, but I still think it has value as it's getting quite close to being a practical method for generating fake training data for speech recognition. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "- Comment on \u201c statistical significance on TIMIT experiments \u201d : We have chosen TIMIT dataset because of its inherent low-resource domain for different genders . As shown in Table 3 , when using only Male speech for training the network , testing on female genders results in a large margin ( 11 % on phoneme error recognition ) , compared to baseline . However by using only \u201c Male- > Female \u201d data in training of proposed model , this gap can be reduced by ~10 % for 124 voices in validation and 64 voices in test set for female domain . - Comment on \u201c Whether GAN \u2019 s are parametric or non-parametrics \u201d : Here we refer to the classical parametric models for modeling data distribution . In this sense , the generator in GAN implicitly models the true distribution . Therefore , we categorize GAN as a non-parametric density estimation model since it does not assume any form of distribution . - Comment on general applicability of the proposed domain adaptation model : Since for any sample , whether target or source , there are two classifier in the cycle to preserve the class label information during transformation across domains , we believe that this implicit enforcement of content preservation will hold in broader applications . If the model is able to figure out which part is important for a certain class and ignore other parts , that is a desired behavior , since only those parts are important for the task in mind ."}}