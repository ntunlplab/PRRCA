{"year": "2021", "forum": "qZzy5urZw9", "title": "Robust Overfitting may be mitigated by properly learned smoothening", "decision": "Accept (Poster)", "meta_review": "This paper focuses on the problem of robust overfitting. The philosophy behind sounds quite interesting to me, namely, injecting \nmore learned smoothening during adversarial training. This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self-training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights.\n\nThe clarity and novelty are above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all \ncomments in the final version.", "reviews": [{"review_id": "qZzy5urZw9-0", "review_text": "The paper studies a method for mitigating robust overfitting . Rice et al. , and others have observed that when training a neural network robustly on say CIFAR10 , then the robust test error often overfits , i.e. , it has a U-shaped curve as a function of training epochs . Rice et al.demonstrated that early stopping the robust training enables state-of-the-art robust performance . However , to realize this performance , it is necessary to find a good early stopping point , which can be difficult ( but can be found with testing on a validation set ) . The paper proposes an alternative to early stopping : smoothing the logits and smoothing the weights , by using two existing techniques , namely self-training and stochastic weight averaging . The paper finds that smoothing mitigates robust overfitting , and reports even a slight improvement over early stopping at the optimal point . Strength : - The paper proposes two existing techniques to prevent robust overfitting : self-training and stochastic weight averaging , and shows that combining those two approaches is very effective in preventing robust overfitting for ResNet-18 . - The paper is well written and easy to follow . Weaknesses : - All experiments are carried out with one network only : ResNet-10 . To validate the claim that learned label smoothing can mitigate robust training , is important to test the label smoothing method on a variety of setups and models . - The improvement are tiny relativ to early stopping , and another submission to this workshop ( https : //openreview.net/pdf ? id=Xb8xvrtB8Ce ) has shown that the baseline the paper under review is comparing to ( the setting of Rice et al . ) is quite brittle relative to choices of hyperparameters such as slight differences in weight decay . Therefore , the gains obtained by the paper under review could very well be subsumed by slightly tuning early stopping . Summary : It is an important problem to study methods that mitigate robust overfitting , and the paper proposes a combination of two smoothing techniques and demonstrates its effectiveness through extensive experiments . I 'm therefore leaning to recommend acceptance of this paper , however , as mentioned , the paper 's results might not generalize to other models and the slight gains over early stopping might be void by slightly tuning ES better . Therefore , I would not be upset if this paper were rejected . In any case , the paper 's results would be more convincing if it would contain results for different models ( e.g. , VGG and other deep nets but also simple baseline models such as random feature models ) , and if it would contain a simple theoretical statement to provide intuition why label smoothing should help . - UPDATE : Thanks ; I have read the response , kept my score , and responded below .", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ Cons1 : More Setups and Models ] Thanks for the helpful suggestions . We have conducted new experiments using VGG16 , Wide-ResNet-34-4 , and Wide-ResNet-34-10 on the CIFAR-10/100 dataset and report the results in Table S1 . And if the reviewer could kindly provide more details or some references about the random feature models , we are very willing to conduct new experiments and report the results . As we can see , our approaches largely mitigate robust overfitting across multiple models . For example , in VGG16 , our methods reduce the gap between the RA best checkpoint and the final epochs from 5.83 % to 0.06 % on CIFAR-10 and 4.21 % to 0.06 % on CIFAR-100 . Meanwhile , our methods gain an extra robustness improvement ( 2.57 % on CIFAR-10 and 3.15 % on CIFAR-100 ) compared with early stopping . Consistent improvements can also be observed with Wide-ResNets . In addition , we conduct more experiments to verify our approaches under Auto-Attack and CW attack , which is shown in Table S2 . Evaluated under the more rigorous attack method , i.e.Auto-Attack , our methods can still be effective . Compared with the results reported in the Auto-Attack leaderboard from [ 2 ] , our approaches reach 1.19 % and 6.47 % improvement on CIFAR-10 and CIFAR-100 under ResNet18 , respectively . Besides , there is no significant drop of robust accuracy ( RA ) between the RA best checkpoint and the final epoch under both Auto-Attack and CW attack . [ 1 ] Bag of Tricks for adversarial training [ 2 ] Overfitting in adversarially robust deep learning [ 3 ] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses Table S1 Performance showing the occurrence of robust overfitting across different architectures and the effectiveness of our proposed remedies under $ \\ell_\\infty $ PGD-20 adversary . We pick the checkpoint which has the best robust accuracy on the validation set . |Architecture|Dataset|Settings|Robust Accuray ( RA ) ( Best- > Final ) | Standard Accuracy ( SA ) ( Best- > Final ) | | : - : | : - : | : - : | : - : | : - : | |VGG-16|CIFAR-10|Baseline| ( 46.42\u219240.59 ) ( \u21935.83 ) | ( 75.29\u219279.54 ) ( \u21914.25 ) | |VGG-16|CIFAR-10|Our Methods| ( 48.99\u219248.93 ) ( \u21930.06 ) | ( 79.00\u219279.69 ) ( \u21910.69 ) | |VGG-16|CIFAR-100|Baseline| ( 21.64\u219217.43 ) ( \u21934.21 ) | ( 39.26\u219245.84 ) ( \u21916.58 ) | |VGG-16|CIFAR-100|Our Methods| ( 24.79\u219224.73 ) ( \u21930.06 ) | ( 48.20\u219249.00 ) ( \u21910.80 ) | |WideResNet-34-4|CIFAR-10|Baseline| ( 52.59\u219243.06 ) ( \u21939.53 ) | ( 81.53\u219283.28 ) ( \u21911.75 ) | |WideResNet-34-4|CIFAR-10|Our Methods| ( 54.28\u219253.90 ) ( \u21930.38 ) | ( 85.17\u219285.50 ) ( \u21910.33 ) | |WideResNet-34-4|CIFAR-100|Baseline| ( 28.02\u219220.61 ) ( \u21937.41 ) | ( 53.19\u219253.63 ) ( \u21910.44 ) | |WideResNet-34-4|CIFAR-100|Our Methods| ( 30.10\u219229.80 ) ( \u21930.30 ) | ( 57.23\u219258.05 ) ( \u21910.82 ) | |WideResNet-34-10|CIFAR-10|Baseline| ( 54.27\u219247.12 ) ( \u21937.15 ) | ( 84.16\u219285.72 ) ( \u21911.56 ) The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set . We follow the same setting as [ 3 ] for CW Attack : 1 search step on C with an initial constant of 0.1 , with 100 iterations for each search step and learning rate is 0.01 . |Dataset|Norm|Settings|Auto-Attack ( Best- > Final ) |CW Attack ( Best- > Final ) | | : - : | : - : | : - : | : - : | : - : | |CIFAR-10| $ \\ell_\\infty $ |Baseline| ( 47.00\u219239.96 ) ( \u21937.04 ) | ( 75.48\u219260.52 ) ( \u219314.96 ) | |CIFAR-10| $ \\ell_\\infty $ |Our Methods| ( 49.35\u219249.44 ) ( \u21910.09 ) | ( 77.83\u219277.04 ) ( \u21930.79 ) | |CIFAR-10| $ \\ell_2 $ |Baseline| ( 67.18\u219264.29 ) ( \u21932.89 ) | ( 73.80\u219253.77 ) ( \u219320.03 ) | |CIFAR-10| $ \\ell_2 $ |Our Methods| ( 68.87\u219268.90 ) ( \u21910.03 ) | ( 73.89\u219273.79 ) ( \u21930.10 ) | |CIFAR-100| $ \\ell_\\infty $ |Baseline| ( 22.73\u219218.11 ) ( \u21934.62 ) | ( 45.89\u219237.76 ) ( \u21938.13 ) | |CIFAR-100| $ \\ell_\\infty $ |Our Methods| ( 25.42\u219225.35 ) ( \u21930.07 ) | ( 49.46\u219249.07 ) ( \u21930.39 ) | |CIFAR-100| $ \\ell_2 $ |Baseline| ( 37.16\u219233.43 ) ( \u21933.73 ) | ( 48.43\u219237.73 ) ( \u219310.70 ) | |CIFAR-100| $ \\ell_2 $ |Our Methods| ( 40.56\u219240.61 ) ( \u21910.05 ) | ( 51.02\u219250.90 ) ( \u21930.12 ) |"}, {"review_id": "qZzy5urZw9-1", "review_text": "Summary = The paper leverages two methods for improving generalization in standard training , logit smoothing and stochastic weight averaging , and show that these results can mitigate robust overfitting and improve generalization for adversarial training methods . Overall , the paper was clear and easy to follow . There are a number of ablation studies showing the marginal effects of the two methods , as well as experiments demonstrating how the approaches vary with certain choices in methodology . My initial impression is positive , though there are certain changes described below which would help solidify the paper and its claims . Comments for discussion == By improving upon the results in Rice et al.2020 , the authors purport to have state of the art results . However , there 's be a plethora of new work since then which have improved these numbers even further . Fortunately , since the submitted work handles the standard CIFAR10 setting , there are a number of public benchmarks that can be used here . It would be great if the authors could train a comparable model and evaluate it using one of these benchmarks ( e.g.the autoattack framework at https : //github.com/fra31/auto-attack ) . To be clear , since a number of these approaches on the benchmark are quite recent , I am not requesting that the authors directly compare to these new methods in their work . However , the baseline that they do compare to ( e.g.Rice et al.2020 ) is evaluated in this framework ( and had a not-insignifcant drop in robust accuracy ) , so it would be of significant utility to also evaluate the approach using the improved attack . Performing this evaluation would serve two purposes : 1 . This should alleviate most concerns on the validity of the result 2 . This makes the work easily comparable for future work Note that reaching the top of the benchmark is not a requirement for publication . As long as it is consistent with the claims of the paper , that the approach reduces robust overfitting for PGD training and improves upon the PGD baseline within this benchmark , then this is fine . If the authors can report how their approach performs under this improved evaluation or a comparable alternative , then I am happy to adjust my score accordingly . However , the authors probably should n't claim state-of-the-art performance without doing this evaluation first . Minor comments == + In section 3.2 , it is mentioned that Table 2 supposedly shows differences when a robust self-teacher is pretrained , but this does not seem to be the case . Update I have looked through the response and edited version . The updated evaluation looks solid and provides a potential solution to a robust overfitting problem . Although the work is primarily empirical in nature , it may inspire directions for future work to look into more theoretical explanations of robust overfitting .", "rating": "7: Good paper, accept", "reply_text": "We \u2019 re very glad you had a positive initial impression , and likewise , we found the set of perceptive questions you raised in your feedback very insightful , pushing us to think of a tighter experiment design . We provide pointwise responses below . [ Cons1 : Evaluation under the Improved Attack ] Thanks for the helpful suggestion . We understood your concerns on the evaluation of our approach under improved attacks , e.g. , Auto-Attack and CW attack . We have included the reference and experiment results of the Auto-Attack and CW attack in our modified draft . The Auto-attack and CW attack implementations that we used follow ( https : //github.com/fra31/auto-attack ) and the advertorch ( https : //github.com/BorealisAI/advertorch ) , respectively . As shown in Table S1 , after applying the combination of KD and SWA , the overfitting problem is largely mitigated under Auto-Attack and CW attack . Take CIFAR-10 $ \\ell_\\infty $ adversary as an example . As we can see , our approach reduces the drop of robust accuracy from 7.04 % to -0.09 % under Auto-Attack , and 14.96 % to 0.79 % under CW attack , when comparing the best model to the eventually converged model . Although the Auto-Attack Leaderboard is a rising benchmark , it is hard to conduct a fair comparison since there are diverse settings , including extra data , network architecture , training epochs , and other implementation configurations . We try our best to unify the setting and compare it with the ResNet-18 models from Rice et al . [ 1 ] in the Auto-Attack Leaderboard ( https : //github.com/fra31/auto-attack ) . We observe the ResNet-18 models from [ 1 ] achieve 18.95 % robust accuracy with $ \\ell_\\infty $ adversary auto-attacks on CIFAR-100 and 67.68 % robust accuracy with $ \\ell_2 $ adversary on CIFAR-10 , while our approaches reach 25.42 % and 68.87 % robust accuracy , respectively . Under the exact same settings , our proposal achieves substantial robustness improvement ( 6.47 % and 1.19 % robust accuracy ) under Auto-Attacks . In addition , we have also provided additional experiment results on Wide-ResNet in the modified draft and submit all our models to the Auto-Attack leaderboard in the future . [ Cons2 : Typo ] Thanks for the careful reading . Table 2 was a typo . It should be Table 4 . [ 1 ] Overfitting in adversarially robust deep learning [ 2 ] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses Table S1 Robust accuracy under Auto-Attack and CW Attack on CIFAR-10/100 with ResNet18 . The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set . We follow the same setting as [ 2 ] for CW Attack : 1 search step on C with an initial constant of 0.1 , with 100 iterations for each search step and learning rate is 0.01 . |Dataset|Norm|Settings|Auto-Attack ( Best- > Final ) |CW Attack ( Best- > Final ) | | : - : | : - : | : - : | : - : | : - : | |CIFAR-10| $ \\ell_\\infty $ |Baseline| ( 47.00\u219239.96 ) ( \u21937.04 ) | ( 75.48\u219260.52 ) ( \u219314.96 ) | |CIFAR-10| $ \\ell_\\infty $ |Our Methods| ( 49.35\u219249.44 ) ( \u21910.09 ) | ( 77.83\u219277.04 ) ( \u21930.79 ) | |CIFAR-10| $ \\ell_2 $ |Baseline| ( 67.18\u219264.29 ) ( \u21932.89 ) | ( 73.80\u219253.77 ) ( \u219320.03 ) | |CIFAR-10| $ \\ell_2 $ |Our Methods| ( 68.87\u219268.90 ) ( \u21910.03 ) | ( 73.89\u219273.79 ) ( \u21930.10 ) | |CIFAR-100| $ \\ell_\\infty $ |Baseline| ( 22.73\u219218.11 ) ( \u21934.62 ) | ( 45.89\u219237.76 ) ( \u21938.13 ) | |CIFAR-100| $ \\ell_\\infty $ |Our Methods| ( 25.42\u219225.35 ) ( \u21930.07 ) | ( 49.46\u219249.07 ) ( \u21930.39 ) | |CIFAR-100| $ \\ell_2 $ |Baseline| ( 37.16\u219233.43 ) ( \u21933.73 ) | ( 48.43\u219237.73 ) ( \u219310.70 ) | |CIFAR-100| $ \\ell_2 $ |Our Methods| ( 40.56\u219240.61 ) ( \u21910.05 ) | ( 51.02\u219250.90 ) ( \u21930.12 ) |"}, {"review_id": "qZzy5urZw9-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper uses the existing tricks that can enhance the standard training , to show that combining some of those tricks ( in this paper , labels/logits smoothing and weight averaging ) can improve adversarial training . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 Compared with existing weight manipulation AT methods , this paper first utilizes stochastic weight averaging ( SWA ) ( averaging multiple checkpoints along the training trajectory ) without incurring computational overhead . 2 This paper conducted experiments across four different datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 The paper \u2019 s novelty is marginal . Specifically , first , label/logit smoothing has been demonstrated effective in adversarial training due to the better separation of different classes . For example , to my knowledge , three papers got accepted with the shared philosophy but slightly different techniques/decorations [ 1 , 2 , 3 ] Second , as the authors mentioned , manipulating model weights is also shown effective [ 4 ] . Therefore , this paper 's conceptual improvements are marginal . [ 1 ] Metric Learning for Adversarial Robustness , NeurIPS 2019\\ [ 2 ] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness , ICLR 2020\\ [ 3 ] Boosting Adversarial Training with Hypersphere Embedding , NeurIPS 2020\\ [ 4 ] Revisiting loss landscape for adversarial robustness , NeurIPS 2020 2 This paper hypothesizes \u201c one source of robust overfitting might lie in that the model \u2018 overfits \u2019 the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late stage. \u201d It is not clear to me why this hypothesis is valid . Would you explain more justifications ? 3.In Figure 3 , are there any experimental results for SWA SA and SWA RA ? Besides , more robustness evaluations are needed , e.g. , CW attack , AA attack , Guided Adversarial Margin Attack . More adversarial training on different network structures are needed , e.g. , Wide ResNet .", "rating": "7: Good paper, accept", "reply_text": "[ Cons 1 : Marginal Contribution . ] We disagree that the contribution is marginal . As recognized by Reviewer # 2 , our work for the first time provides principal and effective solutions to mitigate robust overfitting [ 5 ] beyond the early stopping . As shown in Table 3 , KD or SWA alone helps alleviate robust overfitting , and the combination of KD and SWA further improves the performance , indicating their supplementary benefits . Technically , we leverage knowledge distillation and self-training to smooth the logits rather than proposing metric learning regularizers [ 1,2,3 ] . For the model weights manipulation , we utilize a much simpler yet effective technique , stochastic weight averaging , while [ 4 ] resorts to adversarial weight perturbation via a complex min-max optimization . To the best of our knowledge , none of [ 1-4 ] was proposed for solving the problem of robust overfitting . Thus , we believe that our differences with [ 1-4 ] are significant . However , we also would like to thank the reviewer for pointing out [ 1-4 ] and we have included them in the updated paper . Thanks for the suggestions ! [ Cons 2 : Hypothesize Explanation . ] Thanks for the question . Our explanation on \u201c one source of robust overfitting might lie in that the model \u2018 overfits \u2019 the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late-stage \u201d can be verified by experiment results in Figure 4 ( Left ) . To be more specific , we evaluate the transferability of attacks generated by models at different epoch checkpoints of PGD-AT Baseline , Baseline + KD , and Baseline + KD + SWA against an unseen victim model given by robustified ResNet-50 with PGD-10 on CIFAR-100 . As we can see , the attacks generated by later checkpoints ( > 50 epochs ) of PGD-AT Baseline lack transferability to the unseen ( test ) model . This is an insightful result as attacks generated by later checkpoints ( corresponding to more robust source models ) are supposed to have better transferability ( namely , generalization over unseen models ) . The violation of the above intuition suggests that the defender ( outer minimizer in PGD-AT Baseline ) starts to overfit the attacker ( inner maximizer in PGD-AT Baseline ) at later epochs and in turn , is unable to generate attacks with generalization-ability . In AT , the two-layer game makes the defender and the attacker co-evolved and becomes overfitting to each other at later epochs . Indeed , as we use the proposed Baseline + KD + SWA in Figure 4 ( Left ) , the overfitting issue , characterized by the degradation of attack transferability at later epochs , can largely be mitigated . [ 1 ] Metric Learning for Adversarial Robustness [ 2 ] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness [ 3 ] Boosting Adversarial Training with Hypersphere Embedding [ 4 ] Revisiting loss landscape for adversarial robustness [ 5 ] Overfitting in adversarially robust deep learning [ 6 ] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses"}], "0": {"review_id": "qZzy5urZw9-0", "review_text": "The paper studies a method for mitigating robust overfitting . Rice et al. , and others have observed that when training a neural network robustly on say CIFAR10 , then the robust test error often overfits , i.e. , it has a U-shaped curve as a function of training epochs . Rice et al.demonstrated that early stopping the robust training enables state-of-the-art robust performance . However , to realize this performance , it is necessary to find a good early stopping point , which can be difficult ( but can be found with testing on a validation set ) . The paper proposes an alternative to early stopping : smoothing the logits and smoothing the weights , by using two existing techniques , namely self-training and stochastic weight averaging . The paper finds that smoothing mitigates robust overfitting , and reports even a slight improvement over early stopping at the optimal point . Strength : - The paper proposes two existing techniques to prevent robust overfitting : self-training and stochastic weight averaging , and shows that combining those two approaches is very effective in preventing robust overfitting for ResNet-18 . - The paper is well written and easy to follow . Weaknesses : - All experiments are carried out with one network only : ResNet-10 . To validate the claim that learned label smoothing can mitigate robust training , is important to test the label smoothing method on a variety of setups and models . - The improvement are tiny relativ to early stopping , and another submission to this workshop ( https : //openreview.net/pdf ? id=Xb8xvrtB8Ce ) has shown that the baseline the paper under review is comparing to ( the setting of Rice et al . ) is quite brittle relative to choices of hyperparameters such as slight differences in weight decay . Therefore , the gains obtained by the paper under review could very well be subsumed by slightly tuning early stopping . Summary : It is an important problem to study methods that mitigate robust overfitting , and the paper proposes a combination of two smoothing techniques and demonstrates its effectiveness through extensive experiments . I 'm therefore leaning to recommend acceptance of this paper , however , as mentioned , the paper 's results might not generalize to other models and the slight gains over early stopping might be void by slightly tuning ES better . Therefore , I would not be upset if this paper were rejected . In any case , the paper 's results would be more convincing if it would contain results for different models ( e.g. , VGG and other deep nets but also simple baseline models such as random feature models ) , and if it would contain a simple theoretical statement to provide intuition why label smoothing should help . - UPDATE : Thanks ; I have read the response , kept my score , and responded below .", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ Cons1 : More Setups and Models ] Thanks for the helpful suggestions . We have conducted new experiments using VGG16 , Wide-ResNet-34-4 , and Wide-ResNet-34-10 on the CIFAR-10/100 dataset and report the results in Table S1 . And if the reviewer could kindly provide more details or some references about the random feature models , we are very willing to conduct new experiments and report the results . As we can see , our approaches largely mitigate robust overfitting across multiple models . For example , in VGG16 , our methods reduce the gap between the RA best checkpoint and the final epochs from 5.83 % to 0.06 % on CIFAR-10 and 4.21 % to 0.06 % on CIFAR-100 . Meanwhile , our methods gain an extra robustness improvement ( 2.57 % on CIFAR-10 and 3.15 % on CIFAR-100 ) compared with early stopping . Consistent improvements can also be observed with Wide-ResNets . In addition , we conduct more experiments to verify our approaches under Auto-Attack and CW attack , which is shown in Table S2 . Evaluated under the more rigorous attack method , i.e.Auto-Attack , our methods can still be effective . Compared with the results reported in the Auto-Attack leaderboard from [ 2 ] , our approaches reach 1.19 % and 6.47 % improvement on CIFAR-10 and CIFAR-100 under ResNet18 , respectively . Besides , there is no significant drop of robust accuracy ( RA ) between the RA best checkpoint and the final epoch under both Auto-Attack and CW attack . [ 1 ] Bag of Tricks for adversarial training [ 2 ] Overfitting in adversarially robust deep learning [ 3 ] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses Table S1 Performance showing the occurrence of robust overfitting across different architectures and the effectiveness of our proposed remedies under $ \\ell_\\infty $ PGD-20 adversary . We pick the checkpoint which has the best robust accuracy on the validation set . |Architecture|Dataset|Settings|Robust Accuray ( RA ) ( Best- > Final ) | Standard Accuracy ( SA ) ( Best- > Final ) | | : - : | : - : | : - : | : - : | : - : | |VGG-16|CIFAR-10|Baseline| ( 46.42\u219240.59 ) ( \u21935.83 ) | ( 75.29\u219279.54 ) ( \u21914.25 ) | |VGG-16|CIFAR-10|Our Methods| ( 48.99\u219248.93 ) ( \u21930.06 ) | ( 79.00\u219279.69 ) ( \u21910.69 ) | |VGG-16|CIFAR-100|Baseline| ( 21.64\u219217.43 ) ( \u21934.21 ) | ( 39.26\u219245.84 ) ( \u21916.58 ) | |VGG-16|CIFAR-100|Our Methods| ( 24.79\u219224.73 ) ( \u21930.06 ) | ( 48.20\u219249.00 ) ( \u21910.80 ) | |WideResNet-34-4|CIFAR-10|Baseline| ( 52.59\u219243.06 ) ( \u21939.53 ) | ( 81.53\u219283.28 ) ( \u21911.75 ) | |WideResNet-34-4|CIFAR-10|Our Methods| ( 54.28\u219253.90 ) ( \u21930.38 ) | ( 85.17\u219285.50 ) ( \u21910.33 ) | |WideResNet-34-4|CIFAR-100|Baseline| ( 28.02\u219220.61 ) ( \u21937.41 ) | ( 53.19\u219253.63 ) ( \u21910.44 ) | |WideResNet-34-4|CIFAR-100|Our Methods| ( 30.10\u219229.80 ) ( \u21930.30 ) | ( 57.23\u219258.05 ) ( \u21910.82 ) | |WideResNet-34-10|CIFAR-10|Baseline| ( 54.27\u219247.12 ) ( \u21937.15 ) | ( 84.16\u219285.72 ) ( \u21911.56 ) The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set . We follow the same setting as [ 3 ] for CW Attack : 1 search step on C with an initial constant of 0.1 , with 100 iterations for each search step and learning rate is 0.01 . |Dataset|Norm|Settings|Auto-Attack ( Best- > Final ) |CW Attack ( Best- > Final ) | | : - : | : - : | : - : | : - : | : - : | |CIFAR-10| $ \\ell_\\infty $ |Baseline| ( 47.00\u219239.96 ) ( \u21937.04 ) | ( 75.48\u219260.52 ) ( \u219314.96 ) | |CIFAR-10| $ \\ell_\\infty $ |Our Methods| ( 49.35\u219249.44 ) ( \u21910.09 ) | ( 77.83\u219277.04 ) ( \u21930.79 ) | |CIFAR-10| $ \\ell_2 $ |Baseline| ( 67.18\u219264.29 ) ( \u21932.89 ) | ( 73.80\u219253.77 ) ( \u219320.03 ) | |CIFAR-10| $ \\ell_2 $ |Our Methods| ( 68.87\u219268.90 ) ( \u21910.03 ) | ( 73.89\u219273.79 ) ( \u21930.10 ) | |CIFAR-100| $ \\ell_\\infty $ |Baseline| ( 22.73\u219218.11 ) ( \u21934.62 ) | ( 45.89\u219237.76 ) ( \u21938.13 ) | |CIFAR-100| $ \\ell_\\infty $ |Our Methods| ( 25.42\u219225.35 ) ( \u21930.07 ) | ( 49.46\u219249.07 ) ( \u21930.39 ) | |CIFAR-100| $ \\ell_2 $ |Baseline| ( 37.16\u219233.43 ) ( \u21933.73 ) | ( 48.43\u219237.73 ) ( \u219310.70 ) | |CIFAR-100| $ \\ell_2 $ |Our Methods| ( 40.56\u219240.61 ) ( \u21910.05 ) | ( 51.02\u219250.90 ) ( \u21930.12 ) |"}, "1": {"review_id": "qZzy5urZw9-1", "review_text": "Summary = The paper leverages two methods for improving generalization in standard training , logit smoothing and stochastic weight averaging , and show that these results can mitigate robust overfitting and improve generalization for adversarial training methods . Overall , the paper was clear and easy to follow . There are a number of ablation studies showing the marginal effects of the two methods , as well as experiments demonstrating how the approaches vary with certain choices in methodology . My initial impression is positive , though there are certain changes described below which would help solidify the paper and its claims . Comments for discussion == By improving upon the results in Rice et al.2020 , the authors purport to have state of the art results . However , there 's be a plethora of new work since then which have improved these numbers even further . Fortunately , since the submitted work handles the standard CIFAR10 setting , there are a number of public benchmarks that can be used here . It would be great if the authors could train a comparable model and evaluate it using one of these benchmarks ( e.g.the autoattack framework at https : //github.com/fra31/auto-attack ) . To be clear , since a number of these approaches on the benchmark are quite recent , I am not requesting that the authors directly compare to these new methods in their work . However , the baseline that they do compare to ( e.g.Rice et al.2020 ) is evaluated in this framework ( and had a not-insignifcant drop in robust accuracy ) , so it would be of significant utility to also evaluate the approach using the improved attack . Performing this evaluation would serve two purposes : 1 . This should alleviate most concerns on the validity of the result 2 . This makes the work easily comparable for future work Note that reaching the top of the benchmark is not a requirement for publication . As long as it is consistent with the claims of the paper , that the approach reduces robust overfitting for PGD training and improves upon the PGD baseline within this benchmark , then this is fine . If the authors can report how their approach performs under this improved evaluation or a comparable alternative , then I am happy to adjust my score accordingly . However , the authors probably should n't claim state-of-the-art performance without doing this evaluation first . Minor comments == + In section 3.2 , it is mentioned that Table 2 supposedly shows differences when a robust self-teacher is pretrained , but this does not seem to be the case . Update I have looked through the response and edited version . The updated evaluation looks solid and provides a potential solution to a robust overfitting problem . Although the work is primarily empirical in nature , it may inspire directions for future work to look into more theoretical explanations of robust overfitting .", "rating": "7: Good paper, accept", "reply_text": "We \u2019 re very glad you had a positive initial impression , and likewise , we found the set of perceptive questions you raised in your feedback very insightful , pushing us to think of a tighter experiment design . We provide pointwise responses below . [ Cons1 : Evaluation under the Improved Attack ] Thanks for the helpful suggestion . We understood your concerns on the evaluation of our approach under improved attacks , e.g. , Auto-Attack and CW attack . We have included the reference and experiment results of the Auto-Attack and CW attack in our modified draft . The Auto-attack and CW attack implementations that we used follow ( https : //github.com/fra31/auto-attack ) and the advertorch ( https : //github.com/BorealisAI/advertorch ) , respectively . As shown in Table S1 , after applying the combination of KD and SWA , the overfitting problem is largely mitigated under Auto-Attack and CW attack . Take CIFAR-10 $ \\ell_\\infty $ adversary as an example . As we can see , our approach reduces the drop of robust accuracy from 7.04 % to -0.09 % under Auto-Attack , and 14.96 % to 0.79 % under CW attack , when comparing the best model to the eventually converged model . Although the Auto-Attack Leaderboard is a rising benchmark , it is hard to conduct a fair comparison since there are diverse settings , including extra data , network architecture , training epochs , and other implementation configurations . We try our best to unify the setting and compare it with the ResNet-18 models from Rice et al . [ 1 ] in the Auto-Attack Leaderboard ( https : //github.com/fra31/auto-attack ) . We observe the ResNet-18 models from [ 1 ] achieve 18.95 % robust accuracy with $ \\ell_\\infty $ adversary auto-attacks on CIFAR-100 and 67.68 % robust accuracy with $ \\ell_2 $ adversary on CIFAR-10 , while our approaches reach 25.42 % and 68.87 % robust accuracy , respectively . Under the exact same settings , our proposal achieves substantial robustness improvement ( 6.47 % and 1.19 % robust accuracy ) under Auto-Attacks . In addition , we have also provided additional experiment results on Wide-ResNet in the modified draft and submit all our models to the Auto-Attack leaderboard in the future . [ Cons2 : Typo ] Thanks for the careful reading . Table 2 was a typo . It should be Table 4 . [ 1 ] Overfitting in adversarially robust deep learning [ 2 ] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses Table S1 Robust accuracy under Auto-Attack and CW Attack on CIFAR-10/100 with ResNet18 . The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set . We follow the same setting as [ 2 ] for CW Attack : 1 search step on C with an initial constant of 0.1 , with 100 iterations for each search step and learning rate is 0.01 . |Dataset|Norm|Settings|Auto-Attack ( Best- > Final ) |CW Attack ( Best- > Final ) | | : - : | : - : | : - : | : - : | : - : | |CIFAR-10| $ \\ell_\\infty $ |Baseline| ( 47.00\u219239.96 ) ( \u21937.04 ) | ( 75.48\u219260.52 ) ( \u219314.96 ) | |CIFAR-10| $ \\ell_\\infty $ |Our Methods| ( 49.35\u219249.44 ) ( \u21910.09 ) | ( 77.83\u219277.04 ) ( \u21930.79 ) | |CIFAR-10| $ \\ell_2 $ |Baseline| ( 67.18\u219264.29 ) ( \u21932.89 ) | ( 73.80\u219253.77 ) ( \u219320.03 ) | |CIFAR-10| $ \\ell_2 $ |Our Methods| ( 68.87\u219268.90 ) ( \u21910.03 ) | ( 73.89\u219273.79 ) ( \u21930.10 ) | |CIFAR-100| $ \\ell_\\infty $ |Baseline| ( 22.73\u219218.11 ) ( \u21934.62 ) | ( 45.89\u219237.76 ) ( \u21938.13 ) | |CIFAR-100| $ \\ell_\\infty $ |Our Methods| ( 25.42\u219225.35 ) ( \u21930.07 ) | ( 49.46\u219249.07 ) ( \u21930.39 ) | |CIFAR-100| $ \\ell_2 $ |Baseline| ( 37.16\u219233.43 ) ( \u21933.73 ) | ( 48.43\u219237.73 ) ( \u219310.70 ) | |CIFAR-100| $ \\ell_2 $ |Our Methods| ( 40.56\u219240.61 ) ( \u21910.05 ) | ( 51.02\u219250.90 ) ( \u21930.12 ) |"}, "2": {"review_id": "qZzy5urZw9-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper uses the existing tricks that can enhance the standard training , to show that combining some of those tricks ( in this paper , labels/logits smoothing and weight averaging ) can improve adversarial training . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 Compared with existing weight manipulation AT methods , this paper first utilizes stochastic weight averaging ( SWA ) ( averaging multiple checkpoints along the training trajectory ) without incurring computational overhead . 2 This paper conducted experiments across four different datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 The paper \u2019 s novelty is marginal . Specifically , first , label/logit smoothing has been demonstrated effective in adversarial training due to the better separation of different classes . For example , to my knowledge , three papers got accepted with the shared philosophy but slightly different techniques/decorations [ 1 , 2 , 3 ] Second , as the authors mentioned , manipulating model weights is also shown effective [ 4 ] . Therefore , this paper 's conceptual improvements are marginal . [ 1 ] Metric Learning for Adversarial Robustness , NeurIPS 2019\\ [ 2 ] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness , ICLR 2020\\ [ 3 ] Boosting Adversarial Training with Hypersphere Embedding , NeurIPS 2020\\ [ 4 ] Revisiting loss landscape for adversarial robustness , NeurIPS 2020 2 This paper hypothesizes \u201c one source of robust overfitting might lie in that the model \u2018 overfits \u2019 the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late stage. \u201d It is not clear to me why this hypothesis is valid . Would you explain more justifications ? 3.In Figure 3 , are there any experimental results for SWA SA and SWA RA ? Besides , more robustness evaluations are needed , e.g. , CW attack , AA attack , Guided Adversarial Margin Attack . More adversarial training on different network structures are needed , e.g. , Wide ResNet .", "rating": "7: Good paper, accept", "reply_text": "[ Cons 1 : Marginal Contribution . ] We disagree that the contribution is marginal . As recognized by Reviewer # 2 , our work for the first time provides principal and effective solutions to mitigate robust overfitting [ 5 ] beyond the early stopping . As shown in Table 3 , KD or SWA alone helps alleviate robust overfitting , and the combination of KD and SWA further improves the performance , indicating their supplementary benefits . Technically , we leverage knowledge distillation and self-training to smooth the logits rather than proposing metric learning regularizers [ 1,2,3 ] . For the model weights manipulation , we utilize a much simpler yet effective technique , stochastic weight averaging , while [ 4 ] resorts to adversarial weight perturbation via a complex min-max optimization . To the best of our knowledge , none of [ 1-4 ] was proposed for solving the problem of robust overfitting . Thus , we believe that our differences with [ 1-4 ] are significant . However , we also would like to thank the reviewer for pointing out [ 1-4 ] and we have included them in the updated paper . Thanks for the suggestions ! [ Cons 2 : Hypothesize Explanation . ] Thanks for the question . Our explanation on \u201c one source of robust overfitting might lie in that the model \u2018 overfits \u2019 the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late-stage \u201d can be verified by experiment results in Figure 4 ( Left ) . To be more specific , we evaluate the transferability of attacks generated by models at different epoch checkpoints of PGD-AT Baseline , Baseline + KD , and Baseline + KD + SWA against an unseen victim model given by robustified ResNet-50 with PGD-10 on CIFAR-100 . As we can see , the attacks generated by later checkpoints ( > 50 epochs ) of PGD-AT Baseline lack transferability to the unseen ( test ) model . This is an insightful result as attacks generated by later checkpoints ( corresponding to more robust source models ) are supposed to have better transferability ( namely , generalization over unseen models ) . The violation of the above intuition suggests that the defender ( outer minimizer in PGD-AT Baseline ) starts to overfit the attacker ( inner maximizer in PGD-AT Baseline ) at later epochs and in turn , is unable to generate attacks with generalization-ability . In AT , the two-layer game makes the defender and the attacker co-evolved and becomes overfitting to each other at later epochs . Indeed , as we use the proposed Baseline + KD + SWA in Figure 4 ( Left ) , the overfitting issue , characterized by the degradation of attack transferability at later epochs , can largely be mitigated . [ 1 ] Metric Learning for Adversarial Robustness [ 2 ] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness [ 3 ] Boosting Adversarial Training with Hypersphere Embedding [ 4 ] Revisiting loss landscape for adversarial robustness [ 5 ] Overfitting in adversarially robust deep learning [ 6 ] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses"}}