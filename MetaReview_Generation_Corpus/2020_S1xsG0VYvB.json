{"year": "2020", "forum": "S1xsG0VYvB", "title": "Understanding the functional and structural differences across excitatory and inhibitory neurons", "decision": "Reject", "meta_review": "This paper explores the role of excitatory and inhibitory neurons, and how their properties might differ based on simulations.  A few issues were raised during the review period, and I commend the authors for stepping up to address these comments and run additional experiments.  It seems, though, that the reviewer's worries were born out in the results of the additional experiments: \"1. The object classification task is not really relevant to elicit the observed behavior and 2. Inhibitory neurons are not essential (at least when training with batch norm).\"  I hope the authors can make improvements in light of these observations, and discuss their implications in a future version of this paper. ", "reviews": [{"review_id": "S1xsG0VYvB-0", "review_text": "[Update after rebuttal period] I would like to thank the authors for the detailed response and for addressing the concerns on such a tight schedule. Overall my two concerns appear to have been right: 1. The object classification task is not really relevant to elicit the observed behavior and 2. Inhibitory neurons are not essential (at least when training with batch norm). Do the conclusions of the paper about understanding of E/I cells in the brain still hold? I am not really sure. I would like to urge the authors to carefully assess their conclusions again in light of the new evidence and perhaps rephrase abstract, intro and discussion where necessary. Having said that, I still like the paper and its approach. I think it provides valuable insights to the field (e.g. neuro-inspired architecture design, concerns regarding training objectives, etc.) and important steps in the right direction and could be a valuable contribution even if the original claims are not entirely substantiated. [Original review] This paper seeks to understand why excitatory neurons in cortex are more stimulus selective than inhibitory cells and why excitatory cells are more sparsely connected to each other. The authors train a recurrent neural network model with a number of biological constraints on CIFAR-10. These constraints include Dale's law, an unequal number of E vs. I cells and only excitatory connections between layers (areas) of the network. I would summarize the main findings as follows: - Excitatory principal neurons are more selective and more sparsely connected to each other than local inhibitory neurons, consistent with biology - Stimulus selectivity and performance depend only on the number of excitatory cells, but neither on the E/I ratio nor the number of I cells - High selectivity appears to be a general property of principal cells, but does not depend on the sign of local connections, while sparse connectivity is mainly linked to the excitatory local connections Strengths: + Architectural decisions are well motivated from a neuroscientific perspective + Provides a hypothesis why certain connectivity and selectivity patterns emerge in the brain by incorporating biological knowledge into neural network models trained on a specific task + Well written and clear, logic development of the arguments Weaknesses: - Unclear whether classification task is necessary to elicit the authors' observations - Interneurons seem unnecessary, raising concerns about relevance of results - Also trains on ImageNet, but only some results are shown; most from CIFAR Overall I like the paper from the perspective of a neuroscientist, as it provides a kind of normative account of why things in the brain might look the way they do. My enthusiasm is somewhat limited, however, because I am not convinced the classification task is actually what drives the authors' observations. I am willing to increase my score and argue more strongly for the paper if the authors can address this concern, detailed in the following: A few observations lead me to believe the classification task the networks are trained may not be important to elicit the observed behavior. 1.) The emergent properties in stimulus selectivity show up almost immediately after training starts according to Fig. 5A+B. Performance, on the other hand, takes a few more epochs to pick up according to Fig. 10. 2.) The connectivity preferences that emerge in Figure 5C appear at already 50 epochs where CIFAR-10 performance is at 50%. To put that into perspective, a linear SVM already achieves 40% on this task, and a one-layer multilayer perceptron with only 100 hidden neurons reaches that performance. 3.) The results of Fig. 11 suggest that the number of inhibitory channels is not important. Did the authors try a stripped down version with only excitatory cells? I suggest the authors train on CIFAR-10 with shuffled labels on the training set [Zhang et al. 2016; https://arxiv.org/abs/1611.03530]. Although performance on the test set would be at chance (no object recognition capabilities), it would be interesting to see whether the selectivity and connectivity properties still emerge. Finally, (related to 3.) I wonder whether it makes sense to draw conclusions about differences between E and I cells from a model trained on a task where the number of inhibitory cells seems irrelevant. Wouldn't one need to have at least both types of neurons to be required for the task in order to draw such conclusions? Minor Comments: - I sometimes found the nomenclature of the multiple models you tried a bit confusing and hard to follow \u2014 especially in parts of the Appendix - The type of operations (convolution, element-wise) in equation 1, together with the meaning of nonlinearities like \\sigma_c are only defined one page after, making that section a bit hard to follow. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the very thoughtful observations and feedback . We have run multiple preliminary experiments that hopefully address the reviewer \u2019 s concerns . We included these preliminary results in two new supplementary figures ( Figure 16 , 17 ) and sections ( Appendix J , K ) . Given the short amount of time available for rebuttal , these figures are far from publication quality yet . We will continue to improve them . 1.On whether the classification task is important . The reviewer made an astute observation about the earlier rise of orientation and image skewness compared to classification performance . To draw attention to this phenomenon , we have edited the manuscript text . This phenomenon can have at least two causes . One possibility is that the rise of skewness is unrelated to classification per se . Another possibility is that the rise of skewness is necessary for subsequent improvement in classification accuracy . To test these hypotheses , as suggested by the reviewer , we trained networks on CIFAR-10 with shuffled labels closely following the method of Zhang et al.2016 ( Appendix J , Figure 16 ) . Our Standard network for CIFAR10 contains 107K trainable parameters , compared to ~1.6M parameters in networks used by Zhang et al.2016 , therefore we used shuffled training sets 10X smaller than the original training set . Unlike Zhang et al.2016 , networks trained to fit random labels did not have regularizations turned off . If we turn off L2 regularization on weights , then connection probability of excitatory and inhibitory neurons approach 1 , similar to observed before ( see Appendix A ) . After training , networks can achieve high accuracy on the training set ( but not on the validation set , as expected ) . Higher selectivity and sparser connectivity among excitatory neurons still emerge in such networks ( Figure 16 ) , although less consistently across areas or random seeds . Understanding the minimal task necessary for the emergence of E-I differences would be interesting for future work . We have added a Results section to show this finding . 2.On whether inhibitory neurons are necessary . The reviewer made another excellent observation that the classification accuracy appears to depend very weakly on the number of inhibitory channels . As suggested by the reviewer , we trained the Standard and StandardEI networks with no inhibitory channels ( Appendix K , Figure 17 ) . To our surprise , the accuracy dropped , but rather moderately ( Standard , 0.855 \u2014 > 0.817 ; StandardEI , 0.842 \u2014 > 0.728 ) . We sought to understand what compensated for the loss of inhibitory neurons . We found that without Batch Normalization ( BN ) on excitatory neurons , the loss of inhibitory neurons becomes devastating , reducing the performance to chance level ( Standard , 0.837 \u2014 > 0.107 ; StandardEI , 0.711 \u2014 > 0.122 ) . Therefore , inhibitory neurons are particularly necessary in networks trained without BN . To ensure that the main findings are independent of whether batch normalization is applied , we repeated our experiments of Figs . 6 and 7 , but for Standard and StandardEI architectures with no BN . We were able to reproduce all major findings summarized by the reviewer . We did notice that even though having 0 inhibitory neurons is devastating , the accuracy can be rescued by having a single inhibitory channel ( Figure 17c ) , which in a convolutional network corresponds to many inhibitory neurons with the same tuning . This is consistent with experimental findings that inhibitory neurons are weakly or non-selective . We would also like to point out that , even with BN applied , inhibitory neurons are active and strongly connected with excitatory neurons , meaning that they participate in the proper functioning of the network . To summarize , inhibitory neurons are always used , but only indispensable when BN is not applied . The main conclusions of the paper hold with or without BN . These new results are included in the updated manuscript and Figures . 3.On the results from ImageNet We did not run the same numbers of experiments as we did for CIFAR10 , but we will include a summary figure in the same style as Figure 7 for networks trained on ImageNet . 4.On confusing nomenclature and explanation of Eq.1 We will update the figures with hopefully clearer names , including the ones in the Appendix . We will also update the manuscript to clarify the section of Eq.1 ."}, {"review_id": "S1xsG0VYvB-1", "review_text": "The idea behind the paper is quite interesting and enticing. The authors attempt to emulate real-life human brain neural network structure with artificial neural networks similar in architecture to the visual cortex. The authors claim that they observe similar patterns emerge in the Neural Networks when it comes to the interaction and topological properties of excitatory and inhibitory neurons. Unfortunately, the paper's claims seems to be undermined by several factors. First, the properties of the artificial neural networks that the authors build into the model (excitatory/inhibitory neurons relative abundance, inter-layer projection) are likely to be sufficient to explain the properties authors claim are informative in their model. In addition to that, some architectural choices by the authors are highly disputable and counter-intuitive. For instance ReLU activation for Neurons would be contradicting any biological reality. Besides, to be biologically innovative, the authors' approach seems to miss a number of crucial factors - notably interaction between neurotransmitters, timing of discharges and synapses offsets, allowing single neurons to perform complex logical computations requiring artificial neural networks with simple scalar non-linear activation functions multi-layer sub-networks to learn. The case the authors cite - similarity of architectures of convolutional neural networks and the pinwheel architecture of visual cortex V1/V2 areas, is one of the few examples of convergent architectures of real-world and artificial neural networks specializing in a task. Finally, the role of Inhibory neurons in the visual cortex seems to be well understood, both biologically and mathematically (see for instance https://www.sciencedirect.com/science/article/pii/S0896627303003325 or https://www.sciencedirect.com/science/article/pii/S0896627303003325 or https://www.sciencedirect.com/science/article/pii/S092842570300072X) Overall, the quality of the article at least in its current state, does not seem to be ready for acceptance to ICLR, but I'm willing to adjust my opinion after reading the opinion of more qualified reviewers' in this area and the authors response.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the feedback . We have edited the manuscript to address the reviewer \u2019 s concerns . 1.On the motivation of this work We thank the reviewer for observing that \u201c the properties ... build into the model ... are likely to be sufficient to explain the properties authors claim are informative in their model. \u201d This is indeed exactly what we are attempting to demonstrate . Our results show that the built-in properties ( abundance of E neurons , excitatory long-range projections ) are sufficient to explain the emergence of other biologically-realistic properties ( sparser E-E connections , more selective E neurons ) not present at initialization . Training is required for the built-in properties to foster the development of these emergent properties . This can be seen in Figure 5 , where there is no difference between selectivity and connectivity of E and I neurons at the beginning of training . One of our goals is to find precisely what the reviewer references : the minimal set of network characteristics that are required to produce biologically-realistic connectivity and selectivity through training . This is why we systematically trained different variants of the original model ( Fig.6 , 7 ) .We edited our Introduction section to clarify these points ."}, {"review_id": "S1xsG0VYvB-2", "review_text": "This paper explores why the brain might have separate excitatory (E) and inhibitory (I) cells and how their different properties affect their network connectivity. The paper is novel and addresses an interesting question. The paper follows Dale's law and has separate populations of E and I cells. The cells differ in proportion (4 times more E cells) and in projection patterns (E cells project to the next layer). From these constraints they found that some of the other observed differences between E and I cells developed, specifically greater selectivity of E cells and greater connectivity of I cells. Both subtractive and divisive inhibition are modeled in the Standard model. Through careful controls, they show that increased selectivity of E neurons depends on having a lot of E neurons (but there was not a corresponding effect for I neurons). Likewise the sparse connectivity of E neurons was due to a large number of E neurons. They also showed that being projection neurons was more important than being E cells for strong selectivity. Finally networks without Dale's law imposed perform slightly better. These are all interesting findings that will benefit the field of neuroscience but may also lead to insights for the next architecture boost to improve deep learning. I am in favor of acceptance. ", "rating": "8: Accept", "reply_text": "We thank the reviewer for the feedback . Our work assesses what properties are necessary and sufficient for the emergence of sparse connectivity and high selectivity of excitatory neurons . It would be very important to understand why excitatory and inhibitory neurons segregate in the first place . Although this topic is not quite addressed in our work , it would be of tremendous interests for the future . We have clarified this point in the Discussion ."}], "0": {"review_id": "S1xsG0VYvB-0", "review_text": "[Update after rebuttal period] I would like to thank the authors for the detailed response and for addressing the concerns on such a tight schedule. Overall my two concerns appear to have been right: 1. The object classification task is not really relevant to elicit the observed behavior and 2. Inhibitory neurons are not essential (at least when training with batch norm). Do the conclusions of the paper about understanding of E/I cells in the brain still hold? I am not really sure. I would like to urge the authors to carefully assess their conclusions again in light of the new evidence and perhaps rephrase abstract, intro and discussion where necessary. Having said that, I still like the paper and its approach. I think it provides valuable insights to the field (e.g. neuro-inspired architecture design, concerns regarding training objectives, etc.) and important steps in the right direction and could be a valuable contribution even if the original claims are not entirely substantiated. [Original review] This paper seeks to understand why excitatory neurons in cortex are more stimulus selective than inhibitory cells and why excitatory cells are more sparsely connected to each other. The authors train a recurrent neural network model with a number of biological constraints on CIFAR-10. These constraints include Dale's law, an unequal number of E vs. I cells and only excitatory connections between layers (areas) of the network. I would summarize the main findings as follows: - Excitatory principal neurons are more selective and more sparsely connected to each other than local inhibitory neurons, consistent with biology - Stimulus selectivity and performance depend only on the number of excitatory cells, but neither on the E/I ratio nor the number of I cells - High selectivity appears to be a general property of principal cells, but does not depend on the sign of local connections, while sparse connectivity is mainly linked to the excitatory local connections Strengths: + Architectural decisions are well motivated from a neuroscientific perspective + Provides a hypothesis why certain connectivity and selectivity patterns emerge in the brain by incorporating biological knowledge into neural network models trained on a specific task + Well written and clear, logic development of the arguments Weaknesses: - Unclear whether classification task is necessary to elicit the authors' observations - Interneurons seem unnecessary, raising concerns about relevance of results - Also trains on ImageNet, but only some results are shown; most from CIFAR Overall I like the paper from the perspective of a neuroscientist, as it provides a kind of normative account of why things in the brain might look the way they do. My enthusiasm is somewhat limited, however, because I am not convinced the classification task is actually what drives the authors' observations. I am willing to increase my score and argue more strongly for the paper if the authors can address this concern, detailed in the following: A few observations lead me to believe the classification task the networks are trained may not be important to elicit the observed behavior. 1.) The emergent properties in stimulus selectivity show up almost immediately after training starts according to Fig. 5A+B. Performance, on the other hand, takes a few more epochs to pick up according to Fig. 10. 2.) The connectivity preferences that emerge in Figure 5C appear at already 50 epochs where CIFAR-10 performance is at 50%. To put that into perspective, a linear SVM already achieves 40% on this task, and a one-layer multilayer perceptron with only 100 hidden neurons reaches that performance. 3.) The results of Fig. 11 suggest that the number of inhibitory channels is not important. Did the authors try a stripped down version with only excitatory cells? I suggest the authors train on CIFAR-10 with shuffled labels on the training set [Zhang et al. 2016; https://arxiv.org/abs/1611.03530]. Although performance on the test set would be at chance (no object recognition capabilities), it would be interesting to see whether the selectivity and connectivity properties still emerge. Finally, (related to 3.) I wonder whether it makes sense to draw conclusions about differences between E and I cells from a model trained on a task where the number of inhibitory cells seems irrelevant. Wouldn't one need to have at least both types of neurons to be required for the task in order to draw such conclusions? Minor Comments: - I sometimes found the nomenclature of the multiple models you tried a bit confusing and hard to follow \u2014 especially in parts of the Appendix - The type of operations (convolution, element-wise) in equation 1, together with the meaning of nonlinearities like \\sigma_c are only defined one page after, making that section a bit hard to follow. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the very thoughtful observations and feedback . We have run multiple preliminary experiments that hopefully address the reviewer \u2019 s concerns . We included these preliminary results in two new supplementary figures ( Figure 16 , 17 ) and sections ( Appendix J , K ) . Given the short amount of time available for rebuttal , these figures are far from publication quality yet . We will continue to improve them . 1.On whether the classification task is important . The reviewer made an astute observation about the earlier rise of orientation and image skewness compared to classification performance . To draw attention to this phenomenon , we have edited the manuscript text . This phenomenon can have at least two causes . One possibility is that the rise of skewness is unrelated to classification per se . Another possibility is that the rise of skewness is necessary for subsequent improvement in classification accuracy . To test these hypotheses , as suggested by the reviewer , we trained networks on CIFAR-10 with shuffled labels closely following the method of Zhang et al.2016 ( Appendix J , Figure 16 ) . Our Standard network for CIFAR10 contains 107K trainable parameters , compared to ~1.6M parameters in networks used by Zhang et al.2016 , therefore we used shuffled training sets 10X smaller than the original training set . Unlike Zhang et al.2016 , networks trained to fit random labels did not have regularizations turned off . If we turn off L2 regularization on weights , then connection probability of excitatory and inhibitory neurons approach 1 , similar to observed before ( see Appendix A ) . After training , networks can achieve high accuracy on the training set ( but not on the validation set , as expected ) . Higher selectivity and sparser connectivity among excitatory neurons still emerge in such networks ( Figure 16 ) , although less consistently across areas or random seeds . Understanding the minimal task necessary for the emergence of E-I differences would be interesting for future work . We have added a Results section to show this finding . 2.On whether inhibitory neurons are necessary . The reviewer made another excellent observation that the classification accuracy appears to depend very weakly on the number of inhibitory channels . As suggested by the reviewer , we trained the Standard and StandardEI networks with no inhibitory channels ( Appendix K , Figure 17 ) . To our surprise , the accuracy dropped , but rather moderately ( Standard , 0.855 \u2014 > 0.817 ; StandardEI , 0.842 \u2014 > 0.728 ) . We sought to understand what compensated for the loss of inhibitory neurons . We found that without Batch Normalization ( BN ) on excitatory neurons , the loss of inhibitory neurons becomes devastating , reducing the performance to chance level ( Standard , 0.837 \u2014 > 0.107 ; StandardEI , 0.711 \u2014 > 0.122 ) . Therefore , inhibitory neurons are particularly necessary in networks trained without BN . To ensure that the main findings are independent of whether batch normalization is applied , we repeated our experiments of Figs . 6 and 7 , but for Standard and StandardEI architectures with no BN . We were able to reproduce all major findings summarized by the reviewer . We did notice that even though having 0 inhibitory neurons is devastating , the accuracy can be rescued by having a single inhibitory channel ( Figure 17c ) , which in a convolutional network corresponds to many inhibitory neurons with the same tuning . This is consistent with experimental findings that inhibitory neurons are weakly or non-selective . We would also like to point out that , even with BN applied , inhibitory neurons are active and strongly connected with excitatory neurons , meaning that they participate in the proper functioning of the network . To summarize , inhibitory neurons are always used , but only indispensable when BN is not applied . The main conclusions of the paper hold with or without BN . These new results are included in the updated manuscript and Figures . 3.On the results from ImageNet We did not run the same numbers of experiments as we did for CIFAR10 , but we will include a summary figure in the same style as Figure 7 for networks trained on ImageNet . 4.On confusing nomenclature and explanation of Eq.1 We will update the figures with hopefully clearer names , including the ones in the Appendix . We will also update the manuscript to clarify the section of Eq.1 ."}, "1": {"review_id": "S1xsG0VYvB-1", "review_text": "The idea behind the paper is quite interesting and enticing. The authors attempt to emulate real-life human brain neural network structure with artificial neural networks similar in architecture to the visual cortex. The authors claim that they observe similar patterns emerge in the Neural Networks when it comes to the interaction and topological properties of excitatory and inhibitory neurons. Unfortunately, the paper's claims seems to be undermined by several factors. First, the properties of the artificial neural networks that the authors build into the model (excitatory/inhibitory neurons relative abundance, inter-layer projection) are likely to be sufficient to explain the properties authors claim are informative in their model. In addition to that, some architectural choices by the authors are highly disputable and counter-intuitive. For instance ReLU activation for Neurons would be contradicting any biological reality. Besides, to be biologically innovative, the authors' approach seems to miss a number of crucial factors - notably interaction between neurotransmitters, timing of discharges and synapses offsets, allowing single neurons to perform complex logical computations requiring artificial neural networks with simple scalar non-linear activation functions multi-layer sub-networks to learn. The case the authors cite - similarity of architectures of convolutional neural networks and the pinwheel architecture of visual cortex V1/V2 areas, is one of the few examples of convergent architectures of real-world and artificial neural networks specializing in a task. Finally, the role of Inhibory neurons in the visual cortex seems to be well understood, both biologically and mathematically (see for instance https://www.sciencedirect.com/science/article/pii/S0896627303003325 or https://www.sciencedirect.com/science/article/pii/S0896627303003325 or https://www.sciencedirect.com/science/article/pii/S092842570300072X) Overall, the quality of the article at least in its current state, does not seem to be ready for acceptance to ICLR, but I'm willing to adjust my opinion after reading the opinion of more qualified reviewers' in this area and the authors response.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the feedback . We have edited the manuscript to address the reviewer \u2019 s concerns . 1.On the motivation of this work We thank the reviewer for observing that \u201c the properties ... build into the model ... are likely to be sufficient to explain the properties authors claim are informative in their model. \u201d This is indeed exactly what we are attempting to demonstrate . Our results show that the built-in properties ( abundance of E neurons , excitatory long-range projections ) are sufficient to explain the emergence of other biologically-realistic properties ( sparser E-E connections , more selective E neurons ) not present at initialization . Training is required for the built-in properties to foster the development of these emergent properties . This can be seen in Figure 5 , where there is no difference between selectivity and connectivity of E and I neurons at the beginning of training . One of our goals is to find precisely what the reviewer references : the minimal set of network characteristics that are required to produce biologically-realistic connectivity and selectivity through training . This is why we systematically trained different variants of the original model ( Fig.6 , 7 ) .We edited our Introduction section to clarify these points ."}, "2": {"review_id": "S1xsG0VYvB-2", "review_text": "This paper explores why the brain might have separate excitatory (E) and inhibitory (I) cells and how their different properties affect their network connectivity. The paper is novel and addresses an interesting question. The paper follows Dale's law and has separate populations of E and I cells. The cells differ in proportion (4 times more E cells) and in projection patterns (E cells project to the next layer). From these constraints they found that some of the other observed differences between E and I cells developed, specifically greater selectivity of E cells and greater connectivity of I cells. Both subtractive and divisive inhibition are modeled in the Standard model. Through careful controls, they show that increased selectivity of E neurons depends on having a lot of E neurons (but there was not a corresponding effect for I neurons). Likewise the sparse connectivity of E neurons was due to a large number of E neurons. They also showed that being projection neurons was more important than being E cells for strong selectivity. Finally networks without Dale's law imposed perform slightly better. These are all interesting findings that will benefit the field of neuroscience but may also lead to insights for the next architecture boost to improve deep learning. I am in favor of acceptance. ", "rating": "8: Accept", "reply_text": "We thank the reviewer for the feedback . Our work assesses what properties are necessary and sufficient for the emergence of sparse connectivity and high selectivity of excitatory neurons . It would be very important to understand why excitatory and inhibitory neurons segregate in the first place . Although this topic is not quite addressed in our work , it would be of tremendous interests for the future . We have clarified this point in the Discussion ."}}