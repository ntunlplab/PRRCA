{"year": "2021", "forum": "r7L91opmsr", "title": "Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models", "decision": "Reject", "meta_review": "This paper explores the use of partial rejection control (PRC) for improved SMC-based variational bounds. While an unbiased SMC variant with PRC has been previously introduced by Kudlicka et al. (2020), this work introduces innovations that can help apply such ideas to variational inference. These bounds result in improvements in empirical performance. \n\nThis paper was heavily discussed, with significant engagement by both the authors and the reviewers. Most reviewers recommended acceptance of this paper, with one reviewer (R4) recommending against acceptance. R4's central concerns regard the novelty of the proposed approach and its positioning relative to the existing SMC literature. The authors argued vigorously in the comments that this paper should be judged as a contribution to the VI literature and not the SMC literature.  Unfortunately, I will recommend that this paper is rejected. It is my opinion that R4's concerns were not fully addressed.\n\nOn the one hand, I agree with the authors that there is significant value to be had in exploring variants of SMC for VI. Indeed, some prior art, like FIVO and IWAE, contributed little to the Monte Carlo literature. I believe that these were good contributions.\n\nOn the other hand, I am concerned that the current draft does not clearly circumscribe its contributions. I read the sections that disuss the works of Schmon et al. (2019) and Kudlicka et al. (2020), and the writing did not leave me with a clear enough sense of the differences. I also read the abstract and introduction of the paper. The introduction of the paper positions this work clearly within the VI literature, but does not clearly discuss prior SMC art, e.g., it does not cite Kudlicka et al. (2020). Despite citing rejection control for SMC, the writing of the abstract and introduction left me with the impression that this work was the first to introduce *unbiased, partial* rejection control for SMC. I believe that impressions matter and that the machine learning community should be generous to adjacent communities when assigning credit.\n\nI realize that my decision is a matter of taste. I also want to say that I am confident that the authors have a clear sense of where their contribution sits, and I suspect that it is a valuable contribution. However, I cannot recommend the draft in its current form. If this is a contribution to the VI literature, as the authors argue, then the authors should not hesitate to give full credit to prior SMC art. My reading of the current draft still leaves me confused about which aspects of the SMC estimator are actual contributions.", "reviews": [{"review_id": "r7L91opmsr-0", "review_text": "# # # Review update following author discussion I 've read the author responses as well as some of the discussion with the other reviewers . Overall , this is valuable work and I 've considered raising my score , but I think a weak accept is appropriate , all things considered . I 've raised the confidence score for my review , as I understand the technical details better now . I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN . However , the significance of the paper in terms of the novelty of the ideas , both conceptual and technical are overstated in my opinion , hence the weak accept . One other point of feedback for the final version in case of an accept : I also share R3 's concern about the paper 's positioning in relation to the extremely general dice enterprise framework ( or , even the bernoulli factory , for that matter ) as somewhat misleading for the particular use case in Section 2.2 . The exact same multinomial sampling scheme is in fact more succinctly presented ( proposed ? ) in BRPF [ Schmon et . al , 2019 ] as the `` Bernoulli Race '' ( which the authors have cited ) . I would think that the current scheme is a special case of the `` Bernoulli race '' that uses a particular form of the acceptance probability parameterization similar to prior work ( e.g.VRS [ Grover et . al.2018 ] ) .See Section 3 , specifically e.g.Eq ( 10 ) and Proposition 2 from BRPF [ Schmon et . al. , 2019 ] , which can be compared to Eq ( 3 ) and Proposition 1 in the current submission , respectively . Minor nit : In step 3 of the algorithm ( right below Eq ( 8 ) ) , you use the notation $ z_t $ for sampling a new variable from $ q_\\phi $ . However , this $ z_t $ has nothing to do with the latent variables that are used in computing the constants $ c^i_t $ . The way it 's written makes it appear as if there 's a circular depdendence of the $ c_t $ on $ z_t $ and then the $ z_t $ is again resampled , which changes the $ c_t $ . For this reason , it maybe better to use a completely unrelated variable for the sample from $ q_\\phi $ in step 3 . # # # Key Strengths The paper puts together several ideas from prior works ( partial rejection control/SMC , variational inference , dice-enterprise ) and also evaluates these ideas for latent variable sequential state space model benchmarks . The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks . # # # Key Weaknesses It seems like a key contribution in terms of novelty/theory is in fixing the bias in prior works using Partial Rejection Control in SMC ( e.g.Peters et . al.2012 ) .More information to support this claim in terms of why the prior estimate is biased would be helpful in assessing the strengths of the paper 's contributions ( Peters et.al do n't seem to explicitly focus on the exact bias for PRC ) . Having said that , the experiments seem to show that unbiased gradients are worse than a biased version ( Figure 2 , left bottom ) , so it seems like focusing on the exact bias is not that important . # # # Additional Comments * The proof of unbiasedness ( Prop 2 ) says `` it is easy to show that Eq ( 15 ) is an unbiased estimator '' and refers to prior work by [ Naesseth et . al ] for details . More clarity here would be helpful ( especially around the term $ Q_ { VSMC-PRC } $ ) for assessing this claim , given that this is one of the main contributions claimed in the paper ( besides also commenting on where the bias in prior work is coming from ) . * The $ Z ( ) $ function first appears in Equation ( 9 ) , without a prior reference/definition . You might want to introduce this around Eq ( 4 ) , where the integral appears . There 's also a reference to what will later be $ Z $ as $ p^i_t $ in Eq ( 8 ) . * Using partial forms of rejection to proposal distribution samples specifically for variational inference ( rather than SMC more generally ) has also been considered in prior work [ R. Gummadi , `` Resampled Belief Networks for Variational Inference '' , Advances in Approximate Bayesian Inference Workshop , 2014 ] .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the helpful comments . Our response to your comments is provided below ( for ease of reference , we have highlighted the text from your original comments , while responding to them ) . Please also note that we have also revised the manuscript incorporating the various comments from the reviewers . \\textbf { More information to support this claim in terms of why the prior estimate is biased would be helpful in assessing the strengths of the paper 's contributions ( Peters et.al do n't seem to explicitly focus on the exact bias for PRC ) } The ancestor variables will be biased if we use the Monte-Carlo estimate of $ \\alpha_ { t } ( . ) $ ( please see equation 7 of the main paper ) ; this was done in Peters et al.To avoid that , we use a dice-enterprise to fix the bias issue . Aside from improving the bias issue , the main contribution of our work is developing a superior VI bound compared to existing bounds like FIVO and IWAE . We can further see the proposed bound VSMC-PRC as a nice generalization of existing approaches like FIVO , IWAE , and VRS ( please see figure 1 and paragraph 3 of related work ) . \\textbf { Having said that , the experiments seem to show that unbiased gradients are worse than a biased version ( Figure 2 , left bottom ) , so it seems like focusing on the exact bias is not that important . } Yes , we have used biased gradients due to high variance ( this problem is not unique to our method ; FIVO also suffers from the same issue ) . However , the bound values presented in the toy example and VRNN are still unbiased . To make things more concrete , let 's denote Z to be an unbiased estimator for marginal likelihood p ( x ) ; through Jensen 's inequality , we can show that E [ log Z ] < log p ( x ) . The quantity E [ log Z ] is maximized wrt some parameters . If $ Z $ is biased then we ca n't say that E [ log Z ] is a lower bound on marginal likelihood and therefore maximizing it does n't make much sense . Although unbiasedness is satisfied easily for ELBO and FIVO bound , it is not straightforward to construct unbiased estimators for SMC-PRC . Therefore , fixing the bias issue is essential for combining VI with SMC-PRC . \\textbf { The proof of unbiasednes .... given that this is one of the main contributions claimed in the paper } Yes , we will include more details about the proof in the supplementary material . We have explained the bias issue above . \\textbf { The function first appears in Equation ( 9 ) , without a prior reference/definition } Thanks for pointing out . We will fix this . \\textbf { Using partial forms of rejection ... } Thank you for pointing out the work by Gummadi ( 2014 ) . We have cited it now in the revised manuscript ."}, {"review_id": "r7L91opmsr-1", "review_text": "The paper considers SMC to construct variational approximations . SMC methods can be improved with partial rejection control ( PRC ) . Then it is not obvious that one can obtain unbiased estimators of the normalizing constant , as in plain SMC . The authors consider a way of obtaining unbiased estimators in that setting . Experiments include a linear Gaussian state-space model and recurrent neural networks on polyphonic music datasets . The problem is interesting . The notation is a bit cumbersome at times , but it is the case for most papers on SMC . The writing is mostly clear . The experiments include a mix of toy and more realistic examples . The fact that SMC with PRC can still produce unbiased estimators of the marginal likelihood was described by Kudlicka , Murray , Schoen , Lindsten , `` Particle filter with rejection control and unbiased estimator of the marginal likelihood '' . The authors do cite that paper , but I did not understand exactly why that paper does not completely solve the problem that the authors consider . At first glance it seems that the Kudlicka et al paper would apply `` out of the box '' when replacing the prior by an arbitrary proposal in SMC , i.e.using a proposal `` $ q_t $ '' instead of `` $ f_t $ '' and the ratio `` $ g_t f_t/q_t $ '' for the weights . My understanding is that Kudlicka et al focus on the bootstrap particle filter because PRC is a remarkably generic approach to improve it , applicable even when sampling from the model prior is the only option ; not because the same reasoning would not apply for generic proposals . Thus it was not clear to me that there 's a need for another paper showing that SMC with PRC can still provide unbiased estimators of the marginal likelihood . If the authors made a convincing case that the extension to general proposals within SMC with PRC requires significant work , their contribution would be more convincing . Furthermore , the manuscript makes references to Bernoulli factories and dice enterprise . In fact , the manuscript addresses the problem of categorical sampling with unbiased estimators of the underlying probabilities . The problem was addressed in `` Bernoulli Race Particle Filters '' by Schmon , Doucet , Deligiannidis . That paper is cited by the authors , but the authors do not make it clear that their algorithm ( in Section 2.2 ) is exactly the same as `` Algorithm 2 '' of that paper , their Proposition 1 is exactly `` equation ( 12 ) '' of that paper , etc . Furthermore , I do n't think their algorithm is indeed a `` dice enterprise '' as in the terminology of Morina et al ; I believe the references to the Bernoulli factory literature would be sending most readers in the wrong direction . Based on these flaws I do not think that the manuscript is suitable for publication . Perhaps a clearer explanation of the specific shortcomings of Kudlicka et al 's work would make the paper more convincing . Small comments : - page 2 : `` We further assume that the joint density [ ... ] '' in fact that decomposition always holds , it is not an assumption . It is just p ( a , b ) = p ( a ) p ( b|a ) . - page 2 : A SMC sampler - > An SMC sampler - page 2 and later : there is some inconsistency between the notation M ( i , t-1 ) and M ( t-1 , i ) . - The latex command \\eqref seems to have been used instead of \\ref , in various places . - page 6 `` utlizing the best of both worlds ''", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the helpful comments . Our response to your comments is provided below : \\textbf { Concerns regarding novelty } : Firstly , note that the goal of our paper was to propose a family of bounds that could improve Variational inference ( VI ) , whereas Kudlicka et al ( 2020 ) does not consider VI . We agree with the reviewer that the work of Kudlicka et al.is also viable for SMC-PRC , just like our method . However , our work aims to leverage the robust nature of SMC-PRC within a variational inference framework ( resulting in our framework VSMC-PRC ) whereas Kudlicka et al ( 2020 ) do not consider VI in the first place but only focus on SMC-PRC . Therefore , the scope and positioning of our work is closer to recent work on VI based bounds like FIVO , IWAE , unlike SMC based methods like Kudlicka et al.We have clarified this fact further in the revised manuscript as well . To elaborate further , we would like to highlight that our proposed bound is a nice generalization of several existing approaches like IWAE , FIVO , and VRS ( see paragraph 3 of related work ) , i.e. , it generalizes several existing VI algorithms in a single framework . In addition to unbiasedness , we also explained how to maximize the bound , the gradient term and its variance reduction , tuning the hyper-parameter $ M ( . ) $ , and discussing the role of $ K $ and $ \\gamma $ . The above summarizes novel and important contributions of our work , in addition to the alternative unbiased estimation of the marginal likelihood proposed in our paper . Thus , we respectfully disagree that our paper is solving something that is `` already tackled , '' since the above problems are not addressed in either Kudlicka et al.or Schmon et al. , and we do not believe these details are straightforward/trivial . Our experimental results confirm that the proposed bound indeed improves VI based bounds like FIVO , IWAE . We again sincerely hope that these aspect will alleviate the concerns from the reviewer . \\textbf { Thus it was not clear to me ... likelihood . } We agree that Kudlicka et al . ( 2020 ) also presented an unbiased estimator for SMC-PRC . However , there can be multiple ways to construct unbiased estimators for SMC-PRC . We believe our estimator is itself a valuable contribution to the literature in providing users a toolbox of multiple unbiased estimation techniques in SMC-PRC . We stress here again that the main contribution of our work is to obtain an improved VI bound , and the unbiased estimation of SMC-PRC is only one of tools used in the pursuit of this goal . \\textbf { That paper is cited by the authors , but the authors ... `` equation ( 12 ) '' of that paper } We respectfully disagree with the reviewer that our work is a trivial modification of `` Bernoulli Race Particle Filters '' by Schmon , Doucet , Deligiannidis . Please see the clarification below : ( 1 ) `` Equation 12 and algorithm 2 are exactly same '' . In their paper , Schmon et al , implement their `` Bernoulli Race '' algorithm , for which they have to calculate an upper bound $ c $ for every experiment to bound their intractable weights ; there is no closed-form expression . The efficiency of this algorithm is then critically dependent on the tightness of this upper bound , carefully obtained for niche individual problem . In our work , the intractable normalization constant $ Z ( . ) $ is naturally bounded , and in the event that this bound is too loose , our tuning parameters allow for fast implementation of the dice-enterprise . ( 2 ) In our case , we can control Bernoulli factory 's efficiency through a user parameter $ \\gamma $ , i.e. , our algorithm will take around $ \\gamma^ { -1 } $ ( independent of the problem under consideration ) steps in general . On the other hand , the approach of Schmon et al.will terminate in around $ \\frac { 1 } { \\overline { b } } $ steps ( please see page 4 of Schmon et al . ) on which they have no control and no guarantees , unlike our method . Schmon et al.have to not only find a proper $ c $ value ( which is known to be challenging outside of toy problems ) ; a loose upper bound can make their method impractical even for 1-dimension . We will request the reviewer to see our discussion on Proposition 1 ( page 4 ) and Section 3.3 explaining the role of $ \\gamma $ . ( 3 ) Most of Bernoulli factory 's existing work is limited to 1-2 dimensions ( even `` Bernoulli Race Particle Filters '' ) . However , our approach applies to high dimensional sequences often seen in machine learning . We are not aware of any method that has used the Bernoulli factory for such general settings ; therefore , we do n't think it is straightforward . To put things in perspective , the maximum sequence length in \\textbf { pianomid } data-set is around 36000 with each latent variable $ \\ { z_ { t } \\ } _ { t=1 } ^ { 36000 } \\in \\mathbb { R } ^ { 64 } $ , we are quite sure that approach of Schmon et al.is not applicable here . \\textbf { Furthermore , I do n't ... of Morina et al } Regarding the name `` Dice-Enterprise '' , please see our comments in `` Common Response to all reviewers '' ."}, {"review_id": "r7L91opmsr-2", "review_text": "Summary : The submission suggests a new variational bound for sequential latent variable models . Unlike previous work that optimize this bound using \u2018 standard \u2019 particle filters with unbiased resampling , the new bound is constructed based on a partial rejection control step and uses a dice enterprise for sampling the ancestor variables . Positives : The combination of partial rejection control and dice enterprise for variational inference is new and interesting . Particle filters with partial rejection control have been used before for constructing ( biased ) bounds based on the marginal likelihood . However , using a dice-enterprise step allows for a new unbiased bound which makes it possible to consider a lower bound on the log-likelihood via variational ideas that can be optimized with standard techniques . Empirical experiments suggest that the method outperforms previous work . Negatives : Does the complexity of the new bound not scale linearly with K ( while K=1 for FIVO ) ? This seems to be not accounted for in the experiments . Choosing larger N=16 also has a better performance in the FIVO paper . Recommendation : I vote for acceptance of the paper . However , I think that the experimental section should be improved . Comments : Variational bounds can also be constructed by targeting a smoothing distribution ( Lawson et al , 2019 ) and particle filters with complexity N^2 based on a marginal Fisher identity have been suggested ( POYIADJIS et al , 2011 ) for parameter estimation that avoid estimator variances scaling quadratically in time . I was wondering if there is a connection between such filters and the method suggested here , particularly for K=N ? Can you explain the connection between the variance of the estimator for the normalizing constant obtained from particle filters and the tightness of the variational bound in more details ? Are the signal-to-noise gradient issues for large N or K ? How do the methods in the experiments compare for a larger number of particles ? Is there some useful practical advice on choosing the ratio N/K and gamma ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the helpful comments . Our response to your comments is provided below ( for ease of reference , we have highlighted the text from your original comments , while responding to them ) . Please also note that we have also revised the manuscript incorporating the various comments from the reviewers . \\textbf { Does the complexity of the new bound not scale linearly with K ( while K=1 for FIVO ) ? This seems to be not accounted for in the experiments . Choosing larger N=16 also has a better performance in the FIVO paper . I think that the experimental section should be improved . } Yes , the time complexity of bound indeed scales linearly with $ K $ . However , we wanted to highlight that increasing $ K $ does not significantly improve the bound value despite increasing the time consumption . Therefore , $ K > 1 $ seems unnecessary to us . If we look at the experiments closely , $ K=1 , \\gamma > 0.8 $ appears to be a good enough configuration for VI practitioners . We will include this conclusion in the experimental section as well to avoid confusion . Choosing larger $ N $ will improve FIVO but will also enhance VSMC-PRC since it is an SMC bound . We deliberately avoided many particles as it may compromise the gradient quality , as explained by `` Tighter Variational Bounds are Not Necessarily Better '' by Rainforth et al.and also discussed by AE-SMC ( Le et al. , 2017 ) . \\textbf { I was wondering if there is a connection between such filters ( Lawson et al , 2019 , POYIADJIS et al , 2011 ) and the method suggested here , particularly for $ K=N $ ? } We think that our method is different from the work of Lawson et al , 2019 because we assume that the latent variable $ z_ { 1 : t } $ is conditionally independent of $ x_ { t+1 : T } $ given $ x_ { 1 : t } $ . It would be interesting to consider PRC for such models , though . \\textbf { Can you explain the connection between the variance of the estimator for the normalizing constant obtained from particle filters and the tightness of the variational bound in more details ? } Let the estimator for normalization constant be $ Z $ . As the variance of $ Z $ decreases ( through more particles or PRC ) , we expect $ E [ \\log ( Z ) ] $ ( variational bound ) to get tighter . However , we would like to highlight that maximizing $ E [ \\log ( Z ) ] $ is not the ideal choice to get a low variance estimator of the normalization constant . The best option is to minimize chi-square divergence `` Variational Inference via $ \\chi $ -Upper Bound Minimization '' by Dieng et al.Unfortunately , minimizing chi-square divergence is not feasible for large-scale problems . \\textbf { Are the signal-to-noise gradient issues for large N or K ? How do the methods in the experiments compare for a larger number of particles ? } Increasing $ K $ should not cause gradient issues because , even in the limit , the normalization constant $ Z ( . ) $ still depends quite strongly on variational parameters . Although the same can not be said for large $ N $ because the bound will approach the marginal likelihood reducing the gradient quality . `` Tighter Variational Bounds are Not Necessarily Better '' Rainforth et al.explains this phenomenon in far more detail . Although comparing our method with FIVO for a large number of particles would be interesting , it could be problematic in a VI context because it may compromise the gradient quality . We would also like to add that PRC is a greedy strategy ; therefore , we can construct some specific scenarios in which FIVO may outperform VSMC-PRC , though we did not find this in practice . \\textbf { Is there some useful practical advice on choosing the ratio N/K and gamma ? } We believe that choosing $ K=1 $ and $ \\gamma > 0.8 $ is a reasonable choice for any $ N $ . Although we can use large $ K $ , the gains are not that much ( see our experimental results on VRNN ) ; therefore , $ K=1 $ seems appropriate . Similarly , choosing low $ \\gamma $ could be detrimental to the algorithm as it increases the time taken by both the PRC step as well as the resampling step via dice-enterprise . Although we are not sure about this , low $ \\gamma $ can also reduce the gradient quality as the PRC 's improved density may become independent ( slightly dependent ) of the variational proposal . We have added this recommendation ( $ K=1 $ and $ \\gamma > 0.8 $ ) in the experiment section to help the readers readily implement our method . Choosing $ N $ can be tricky due to the gradient issue , but we believe $ N \\leq16 $ ( as done in FIVO ) should not be that problematic ."}, {"review_id": "r7L91opmsr-3", "review_text": "This paper describes an SMC algorithm to sample the posterior distribution of latent states $ p_\\theta ( z_ { 1 : T } |x_ { 1 : T } ) $ in a latent variable models $ p_\\theta ( x_ { 1 : T } , z_ { 1 : T } ) $ . The authors consider a completely general setting ( the authors assume Eq . ( 1 ) but clearly there is nothing to assume here , this the standard Bayes rule ) . It is well known that the vanilla SMC sampler is a good candidate for ELBO because it provides an unbiased estimator of the likelihood . But the authors prefer here to use a more sophisticated version of the SMC algorithm , which features a partial rejection algorithm , which amounts to eliminate proposed particles which are `` large enough '' likelihood . It is difficult for an expert in SMC algorithms to understand the algorithm as it is described ( one must even guess the meaning of the notations ) . Equation 3 ) is rather misleading because it is not understood that one continues the acceptance/rejection sampling step until the sample is accepted [ Peters ' paper , a little less ambitious in its generality , is much more readable ] . Also , the form of the rejection probability does not help to understand the action beyond the scene . The level of generality here is a killer : what the hyperparameter $ M ( i , t-1 ) $ means , what is the meaning of this form of rejection probability ( we see something like a Barker ratio ) , this is very puzzling The rejection probability modifies the mutation kernel and should be taken into account when computing the importance weights ( Eqs . ( 4 ) and ( 5 ) ) . This implies to estimate a quantity $ \\alpha_t ( z_ { 1 : t } ^i ) $ which is not tractable . The authors suggest ( similar to Peters ( 2012 ) ) to use a Monte Carlo estimator of these quantities . To sample the ancestors variables from Eq . ( 7 ) with the $ \\alpha_t ( z_ { 1 : t } ^i ) $ defined in Eq . ( 6 ) , the authors use the `` Dice-Enterprise '' algorithm . It does not seem necessary to appeal to such beautiful algorithm to understand the validity of the algorithm described page 3 . Here again , everything is done to frighten the reader and not much is done to explain what is being done ... The results presented are encouraging and shows that the proposed approach outperforms IWAE and FIVO for a given calculation time . This result is clearly interesting and shows that partial rejection helps despite the additional difficulties linked with the intractability which requires an additional layer of complexity . I like the paper even if I have found it extremely unfriendly to read !", "rating": "7: Good paper, accept", "reply_text": "Thank you for the helpful comments . We apologize if the manuscript was a bit dense at a few places ; we will strive to further improve the readability more in the final revision . Our response to your comments is provided below ( for ease of reference , we have highlighted the text from your original comments , while responding to them ) . Please also note that we have also revised the manuscript incorporating the various comments from the reviewers . \\textbf { Equation ( 3 ) is rather misleading because it is not understood that one continues the acceptance/rejection sampling step until the sample is accepted [ Peters ' paper , a little less ambitious in its generality , is much more readable ] . Also , the form of the rejection probability does not help to understand the action beyond the scene . } Thank you for pointing it out ; we will fix the mistake . We chose Barker 's acceptance probability because we wanted to relate our method with existing work from the literature , i.e. , Variational Rejection Sampling ( VRS ) ( as we discussed in the 3rd paragraph of related work ) . Note that , with Barker 's acceptance probability and $ N=T=1 $ , our method essentially reduces to VRS . Differentiable versions of acceptance probability are desirable because we will optimize the proposed lower bound later on . \\textbf { Authors use the `` Dice-Enterprise '' algorithm . It does not seem necessary to appeal to such beautiful algorithm to understand the validity of the algorithm described page 3 . Here again , everything is done to frighten the reader and not much is done to explain what is being done ... } Regarding the name `` Dice-Enterprise , '' please see our comments in `` Common Response to all reviewers . '' The name `` dice-enterprise '' is given to any categorical sampling problem , where the success probabilities are intractable . We want to point out that we do not use the exact dice-enterprise mentioned in Morina et . al . ; rather we only use the nomenclature established there . We just wanted to make sure that all relevant work is appropriately discussed . Further , we have tried to analyze the effect of every hyper-parameter ( $ K , \\gamma $ , and $ M $ ) so that the reader could get more insights into the VSMC-PRC bound . Also , figure 1 of the main paper and 3rd paragraph of related work was added to make the reader more comfortable with the overall algorithm ; for example , the connection with VRS , FIVO , and IWAE . We will add more details about the notation so that the article is more readable . We thank you again for your constructive remarks ."}], "0": {"review_id": "r7L91opmsr-0", "review_text": "# # # Review update following author discussion I 've read the author responses as well as some of the discussion with the other reviewers . Overall , this is valuable work and I 've considered raising my score , but I think a weak accept is appropriate , all things considered . I 've raised the confidence score for my review , as I understand the technical details better now . I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN . However , the significance of the paper in terms of the novelty of the ideas , both conceptual and technical are overstated in my opinion , hence the weak accept . One other point of feedback for the final version in case of an accept : I also share R3 's concern about the paper 's positioning in relation to the extremely general dice enterprise framework ( or , even the bernoulli factory , for that matter ) as somewhat misleading for the particular use case in Section 2.2 . The exact same multinomial sampling scheme is in fact more succinctly presented ( proposed ? ) in BRPF [ Schmon et . al , 2019 ] as the `` Bernoulli Race '' ( which the authors have cited ) . I would think that the current scheme is a special case of the `` Bernoulli race '' that uses a particular form of the acceptance probability parameterization similar to prior work ( e.g.VRS [ Grover et . al.2018 ] ) .See Section 3 , specifically e.g.Eq ( 10 ) and Proposition 2 from BRPF [ Schmon et . al. , 2019 ] , which can be compared to Eq ( 3 ) and Proposition 1 in the current submission , respectively . Minor nit : In step 3 of the algorithm ( right below Eq ( 8 ) ) , you use the notation $ z_t $ for sampling a new variable from $ q_\\phi $ . However , this $ z_t $ has nothing to do with the latent variables that are used in computing the constants $ c^i_t $ . The way it 's written makes it appear as if there 's a circular depdendence of the $ c_t $ on $ z_t $ and then the $ z_t $ is again resampled , which changes the $ c_t $ . For this reason , it maybe better to use a completely unrelated variable for the sample from $ q_\\phi $ in step 3 . # # # Key Strengths The paper puts together several ideas from prior works ( partial rejection control/SMC , variational inference , dice-enterprise ) and also evaluates these ideas for latent variable sequential state space model benchmarks . The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks . # # # Key Weaknesses It seems like a key contribution in terms of novelty/theory is in fixing the bias in prior works using Partial Rejection Control in SMC ( e.g.Peters et . al.2012 ) .More information to support this claim in terms of why the prior estimate is biased would be helpful in assessing the strengths of the paper 's contributions ( Peters et.al do n't seem to explicitly focus on the exact bias for PRC ) . Having said that , the experiments seem to show that unbiased gradients are worse than a biased version ( Figure 2 , left bottom ) , so it seems like focusing on the exact bias is not that important . # # # Additional Comments * The proof of unbiasedness ( Prop 2 ) says `` it is easy to show that Eq ( 15 ) is an unbiased estimator '' and refers to prior work by [ Naesseth et . al ] for details . More clarity here would be helpful ( especially around the term $ Q_ { VSMC-PRC } $ ) for assessing this claim , given that this is one of the main contributions claimed in the paper ( besides also commenting on where the bias in prior work is coming from ) . * The $ Z ( ) $ function first appears in Equation ( 9 ) , without a prior reference/definition . You might want to introduce this around Eq ( 4 ) , where the integral appears . There 's also a reference to what will later be $ Z $ as $ p^i_t $ in Eq ( 8 ) . * Using partial forms of rejection to proposal distribution samples specifically for variational inference ( rather than SMC more generally ) has also been considered in prior work [ R. Gummadi , `` Resampled Belief Networks for Variational Inference '' , Advances in Approximate Bayesian Inference Workshop , 2014 ] .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the helpful comments . Our response to your comments is provided below ( for ease of reference , we have highlighted the text from your original comments , while responding to them ) . Please also note that we have also revised the manuscript incorporating the various comments from the reviewers . \\textbf { More information to support this claim in terms of why the prior estimate is biased would be helpful in assessing the strengths of the paper 's contributions ( Peters et.al do n't seem to explicitly focus on the exact bias for PRC ) } The ancestor variables will be biased if we use the Monte-Carlo estimate of $ \\alpha_ { t } ( . ) $ ( please see equation 7 of the main paper ) ; this was done in Peters et al.To avoid that , we use a dice-enterprise to fix the bias issue . Aside from improving the bias issue , the main contribution of our work is developing a superior VI bound compared to existing bounds like FIVO and IWAE . We can further see the proposed bound VSMC-PRC as a nice generalization of existing approaches like FIVO , IWAE , and VRS ( please see figure 1 and paragraph 3 of related work ) . \\textbf { Having said that , the experiments seem to show that unbiased gradients are worse than a biased version ( Figure 2 , left bottom ) , so it seems like focusing on the exact bias is not that important . } Yes , we have used biased gradients due to high variance ( this problem is not unique to our method ; FIVO also suffers from the same issue ) . However , the bound values presented in the toy example and VRNN are still unbiased . To make things more concrete , let 's denote Z to be an unbiased estimator for marginal likelihood p ( x ) ; through Jensen 's inequality , we can show that E [ log Z ] < log p ( x ) . The quantity E [ log Z ] is maximized wrt some parameters . If $ Z $ is biased then we ca n't say that E [ log Z ] is a lower bound on marginal likelihood and therefore maximizing it does n't make much sense . Although unbiasedness is satisfied easily for ELBO and FIVO bound , it is not straightforward to construct unbiased estimators for SMC-PRC . Therefore , fixing the bias issue is essential for combining VI with SMC-PRC . \\textbf { The proof of unbiasednes .... given that this is one of the main contributions claimed in the paper } Yes , we will include more details about the proof in the supplementary material . We have explained the bias issue above . \\textbf { The function first appears in Equation ( 9 ) , without a prior reference/definition } Thanks for pointing out . We will fix this . \\textbf { Using partial forms of rejection ... } Thank you for pointing out the work by Gummadi ( 2014 ) . We have cited it now in the revised manuscript ."}, "1": {"review_id": "r7L91opmsr-1", "review_text": "The paper considers SMC to construct variational approximations . SMC methods can be improved with partial rejection control ( PRC ) . Then it is not obvious that one can obtain unbiased estimators of the normalizing constant , as in plain SMC . The authors consider a way of obtaining unbiased estimators in that setting . Experiments include a linear Gaussian state-space model and recurrent neural networks on polyphonic music datasets . The problem is interesting . The notation is a bit cumbersome at times , but it is the case for most papers on SMC . The writing is mostly clear . The experiments include a mix of toy and more realistic examples . The fact that SMC with PRC can still produce unbiased estimators of the marginal likelihood was described by Kudlicka , Murray , Schoen , Lindsten , `` Particle filter with rejection control and unbiased estimator of the marginal likelihood '' . The authors do cite that paper , but I did not understand exactly why that paper does not completely solve the problem that the authors consider . At first glance it seems that the Kudlicka et al paper would apply `` out of the box '' when replacing the prior by an arbitrary proposal in SMC , i.e.using a proposal `` $ q_t $ '' instead of `` $ f_t $ '' and the ratio `` $ g_t f_t/q_t $ '' for the weights . My understanding is that Kudlicka et al focus on the bootstrap particle filter because PRC is a remarkably generic approach to improve it , applicable even when sampling from the model prior is the only option ; not because the same reasoning would not apply for generic proposals . Thus it was not clear to me that there 's a need for another paper showing that SMC with PRC can still provide unbiased estimators of the marginal likelihood . If the authors made a convincing case that the extension to general proposals within SMC with PRC requires significant work , their contribution would be more convincing . Furthermore , the manuscript makes references to Bernoulli factories and dice enterprise . In fact , the manuscript addresses the problem of categorical sampling with unbiased estimators of the underlying probabilities . The problem was addressed in `` Bernoulli Race Particle Filters '' by Schmon , Doucet , Deligiannidis . That paper is cited by the authors , but the authors do not make it clear that their algorithm ( in Section 2.2 ) is exactly the same as `` Algorithm 2 '' of that paper , their Proposition 1 is exactly `` equation ( 12 ) '' of that paper , etc . Furthermore , I do n't think their algorithm is indeed a `` dice enterprise '' as in the terminology of Morina et al ; I believe the references to the Bernoulli factory literature would be sending most readers in the wrong direction . Based on these flaws I do not think that the manuscript is suitable for publication . Perhaps a clearer explanation of the specific shortcomings of Kudlicka et al 's work would make the paper more convincing . Small comments : - page 2 : `` We further assume that the joint density [ ... ] '' in fact that decomposition always holds , it is not an assumption . It is just p ( a , b ) = p ( a ) p ( b|a ) . - page 2 : A SMC sampler - > An SMC sampler - page 2 and later : there is some inconsistency between the notation M ( i , t-1 ) and M ( t-1 , i ) . - The latex command \\eqref seems to have been used instead of \\ref , in various places . - page 6 `` utlizing the best of both worlds ''", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the helpful comments . Our response to your comments is provided below : \\textbf { Concerns regarding novelty } : Firstly , note that the goal of our paper was to propose a family of bounds that could improve Variational inference ( VI ) , whereas Kudlicka et al ( 2020 ) does not consider VI . We agree with the reviewer that the work of Kudlicka et al.is also viable for SMC-PRC , just like our method . However , our work aims to leverage the robust nature of SMC-PRC within a variational inference framework ( resulting in our framework VSMC-PRC ) whereas Kudlicka et al ( 2020 ) do not consider VI in the first place but only focus on SMC-PRC . Therefore , the scope and positioning of our work is closer to recent work on VI based bounds like FIVO , IWAE , unlike SMC based methods like Kudlicka et al.We have clarified this fact further in the revised manuscript as well . To elaborate further , we would like to highlight that our proposed bound is a nice generalization of several existing approaches like IWAE , FIVO , and VRS ( see paragraph 3 of related work ) , i.e. , it generalizes several existing VI algorithms in a single framework . In addition to unbiasedness , we also explained how to maximize the bound , the gradient term and its variance reduction , tuning the hyper-parameter $ M ( . ) $ , and discussing the role of $ K $ and $ \\gamma $ . The above summarizes novel and important contributions of our work , in addition to the alternative unbiased estimation of the marginal likelihood proposed in our paper . Thus , we respectfully disagree that our paper is solving something that is `` already tackled , '' since the above problems are not addressed in either Kudlicka et al.or Schmon et al. , and we do not believe these details are straightforward/trivial . Our experimental results confirm that the proposed bound indeed improves VI based bounds like FIVO , IWAE . We again sincerely hope that these aspect will alleviate the concerns from the reviewer . \\textbf { Thus it was not clear to me ... likelihood . } We agree that Kudlicka et al . ( 2020 ) also presented an unbiased estimator for SMC-PRC . However , there can be multiple ways to construct unbiased estimators for SMC-PRC . We believe our estimator is itself a valuable contribution to the literature in providing users a toolbox of multiple unbiased estimation techniques in SMC-PRC . We stress here again that the main contribution of our work is to obtain an improved VI bound , and the unbiased estimation of SMC-PRC is only one of tools used in the pursuit of this goal . \\textbf { That paper is cited by the authors , but the authors ... `` equation ( 12 ) '' of that paper } We respectfully disagree with the reviewer that our work is a trivial modification of `` Bernoulli Race Particle Filters '' by Schmon , Doucet , Deligiannidis . Please see the clarification below : ( 1 ) `` Equation 12 and algorithm 2 are exactly same '' . In their paper , Schmon et al , implement their `` Bernoulli Race '' algorithm , for which they have to calculate an upper bound $ c $ for every experiment to bound their intractable weights ; there is no closed-form expression . The efficiency of this algorithm is then critically dependent on the tightness of this upper bound , carefully obtained for niche individual problem . In our work , the intractable normalization constant $ Z ( . ) $ is naturally bounded , and in the event that this bound is too loose , our tuning parameters allow for fast implementation of the dice-enterprise . ( 2 ) In our case , we can control Bernoulli factory 's efficiency through a user parameter $ \\gamma $ , i.e. , our algorithm will take around $ \\gamma^ { -1 } $ ( independent of the problem under consideration ) steps in general . On the other hand , the approach of Schmon et al.will terminate in around $ \\frac { 1 } { \\overline { b } } $ steps ( please see page 4 of Schmon et al . ) on which they have no control and no guarantees , unlike our method . Schmon et al.have to not only find a proper $ c $ value ( which is known to be challenging outside of toy problems ) ; a loose upper bound can make their method impractical even for 1-dimension . We will request the reviewer to see our discussion on Proposition 1 ( page 4 ) and Section 3.3 explaining the role of $ \\gamma $ . ( 3 ) Most of Bernoulli factory 's existing work is limited to 1-2 dimensions ( even `` Bernoulli Race Particle Filters '' ) . However , our approach applies to high dimensional sequences often seen in machine learning . We are not aware of any method that has used the Bernoulli factory for such general settings ; therefore , we do n't think it is straightforward . To put things in perspective , the maximum sequence length in \\textbf { pianomid } data-set is around 36000 with each latent variable $ \\ { z_ { t } \\ } _ { t=1 } ^ { 36000 } \\in \\mathbb { R } ^ { 64 } $ , we are quite sure that approach of Schmon et al.is not applicable here . \\textbf { Furthermore , I do n't ... of Morina et al } Regarding the name `` Dice-Enterprise '' , please see our comments in `` Common Response to all reviewers '' ."}, "2": {"review_id": "r7L91opmsr-2", "review_text": "Summary : The submission suggests a new variational bound for sequential latent variable models . Unlike previous work that optimize this bound using \u2018 standard \u2019 particle filters with unbiased resampling , the new bound is constructed based on a partial rejection control step and uses a dice enterprise for sampling the ancestor variables . Positives : The combination of partial rejection control and dice enterprise for variational inference is new and interesting . Particle filters with partial rejection control have been used before for constructing ( biased ) bounds based on the marginal likelihood . However , using a dice-enterprise step allows for a new unbiased bound which makes it possible to consider a lower bound on the log-likelihood via variational ideas that can be optimized with standard techniques . Empirical experiments suggest that the method outperforms previous work . Negatives : Does the complexity of the new bound not scale linearly with K ( while K=1 for FIVO ) ? This seems to be not accounted for in the experiments . Choosing larger N=16 also has a better performance in the FIVO paper . Recommendation : I vote for acceptance of the paper . However , I think that the experimental section should be improved . Comments : Variational bounds can also be constructed by targeting a smoothing distribution ( Lawson et al , 2019 ) and particle filters with complexity N^2 based on a marginal Fisher identity have been suggested ( POYIADJIS et al , 2011 ) for parameter estimation that avoid estimator variances scaling quadratically in time . I was wondering if there is a connection between such filters and the method suggested here , particularly for K=N ? Can you explain the connection between the variance of the estimator for the normalizing constant obtained from particle filters and the tightness of the variational bound in more details ? Are the signal-to-noise gradient issues for large N or K ? How do the methods in the experiments compare for a larger number of particles ? Is there some useful practical advice on choosing the ratio N/K and gamma ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the helpful comments . Our response to your comments is provided below ( for ease of reference , we have highlighted the text from your original comments , while responding to them ) . Please also note that we have also revised the manuscript incorporating the various comments from the reviewers . \\textbf { Does the complexity of the new bound not scale linearly with K ( while K=1 for FIVO ) ? This seems to be not accounted for in the experiments . Choosing larger N=16 also has a better performance in the FIVO paper . I think that the experimental section should be improved . } Yes , the time complexity of bound indeed scales linearly with $ K $ . However , we wanted to highlight that increasing $ K $ does not significantly improve the bound value despite increasing the time consumption . Therefore , $ K > 1 $ seems unnecessary to us . If we look at the experiments closely , $ K=1 , \\gamma > 0.8 $ appears to be a good enough configuration for VI practitioners . We will include this conclusion in the experimental section as well to avoid confusion . Choosing larger $ N $ will improve FIVO but will also enhance VSMC-PRC since it is an SMC bound . We deliberately avoided many particles as it may compromise the gradient quality , as explained by `` Tighter Variational Bounds are Not Necessarily Better '' by Rainforth et al.and also discussed by AE-SMC ( Le et al. , 2017 ) . \\textbf { I was wondering if there is a connection between such filters ( Lawson et al , 2019 , POYIADJIS et al , 2011 ) and the method suggested here , particularly for $ K=N $ ? } We think that our method is different from the work of Lawson et al , 2019 because we assume that the latent variable $ z_ { 1 : t } $ is conditionally independent of $ x_ { t+1 : T } $ given $ x_ { 1 : t } $ . It would be interesting to consider PRC for such models , though . \\textbf { Can you explain the connection between the variance of the estimator for the normalizing constant obtained from particle filters and the tightness of the variational bound in more details ? } Let the estimator for normalization constant be $ Z $ . As the variance of $ Z $ decreases ( through more particles or PRC ) , we expect $ E [ \\log ( Z ) ] $ ( variational bound ) to get tighter . However , we would like to highlight that maximizing $ E [ \\log ( Z ) ] $ is not the ideal choice to get a low variance estimator of the normalization constant . The best option is to minimize chi-square divergence `` Variational Inference via $ \\chi $ -Upper Bound Minimization '' by Dieng et al.Unfortunately , minimizing chi-square divergence is not feasible for large-scale problems . \\textbf { Are the signal-to-noise gradient issues for large N or K ? How do the methods in the experiments compare for a larger number of particles ? } Increasing $ K $ should not cause gradient issues because , even in the limit , the normalization constant $ Z ( . ) $ still depends quite strongly on variational parameters . Although the same can not be said for large $ N $ because the bound will approach the marginal likelihood reducing the gradient quality . `` Tighter Variational Bounds are Not Necessarily Better '' Rainforth et al.explains this phenomenon in far more detail . Although comparing our method with FIVO for a large number of particles would be interesting , it could be problematic in a VI context because it may compromise the gradient quality . We would also like to add that PRC is a greedy strategy ; therefore , we can construct some specific scenarios in which FIVO may outperform VSMC-PRC , though we did not find this in practice . \\textbf { Is there some useful practical advice on choosing the ratio N/K and gamma ? } We believe that choosing $ K=1 $ and $ \\gamma > 0.8 $ is a reasonable choice for any $ N $ . Although we can use large $ K $ , the gains are not that much ( see our experimental results on VRNN ) ; therefore , $ K=1 $ seems appropriate . Similarly , choosing low $ \\gamma $ could be detrimental to the algorithm as it increases the time taken by both the PRC step as well as the resampling step via dice-enterprise . Although we are not sure about this , low $ \\gamma $ can also reduce the gradient quality as the PRC 's improved density may become independent ( slightly dependent ) of the variational proposal . We have added this recommendation ( $ K=1 $ and $ \\gamma > 0.8 $ ) in the experiment section to help the readers readily implement our method . Choosing $ N $ can be tricky due to the gradient issue , but we believe $ N \\leq16 $ ( as done in FIVO ) should not be that problematic ."}, "3": {"review_id": "r7L91opmsr-3", "review_text": "This paper describes an SMC algorithm to sample the posterior distribution of latent states $ p_\\theta ( z_ { 1 : T } |x_ { 1 : T } ) $ in a latent variable models $ p_\\theta ( x_ { 1 : T } , z_ { 1 : T } ) $ . The authors consider a completely general setting ( the authors assume Eq . ( 1 ) but clearly there is nothing to assume here , this the standard Bayes rule ) . It is well known that the vanilla SMC sampler is a good candidate for ELBO because it provides an unbiased estimator of the likelihood . But the authors prefer here to use a more sophisticated version of the SMC algorithm , which features a partial rejection algorithm , which amounts to eliminate proposed particles which are `` large enough '' likelihood . It is difficult for an expert in SMC algorithms to understand the algorithm as it is described ( one must even guess the meaning of the notations ) . Equation 3 ) is rather misleading because it is not understood that one continues the acceptance/rejection sampling step until the sample is accepted [ Peters ' paper , a little less ambitious in its generality , is much more readable ] . Also , the form of the rejection probability does not help to understand the action beyond the scene . The level of generality here is a killer : what the hyperparameter $ M ( i , t-1 ) $ means , what is the meaning of this form of rejection probability ( we see something like a Barker ratio ) , this is very puzzling The rejection probability modifies the mutation kernel and should be taken into account when computing the importance weights ( Eqs . ( 4 ) and ( 5 ) ) . This implies to estimate a quantity $ \\alpha_t ( z_ { 1 : t } ^i ) $ which is not tractable . The authors suggest ( similar to Peters ( 2012 ) ) to use a Monte Carlo estimator of these quantities . To sample the ancestors variables from Eq . ( 7 ) with the $ \\alpha_t ( z_ { 1 : t } ^i ) $ defined in Eq . ( 6 ) , the authors use the `` Dice-Enterprise '' algorithm . It does not seem necessary to appeal to such beautiful algorithm to understand the validity of the algorithm described page 3 . Here again , everything is done to frighten the reader and not much is done to explain what is being done ... The results presented are encouraging and shows that the proposed approach outperforms IWAE and FIVO for a given calculation time . This result is clearly interesting and shows that partial rejection helps despite the additional difficulties linked with the intractability which requires an additional layer of complexity . I like the paper even if I have found it extremely unfriendly to read !", "rating": "7: Good paper, accept", "reply_text": "Thank you for the helpful comments . We apologize if the manuscript was a bit dense at a few places ; we will strive to further improve the readability more in the final revision . Our response to your comments is provided below ( for ease of reference , we have highlighted the text from your original comments , while responding to them ) . Please also note that we have also revised the manuscript incorporating the various comments from the reviewers . \\textbf { Equation ( 3 ) is rather misleading because it is not understood that one continues the acceptance/rejection sampling step until the sample is accepted [ Peters ' paper , a little less ambitious in its generality , is much more readable ] . Also , the form of the rejection probability does not help to understand the action beyond the scene . } Thank you for pointing it out ; we will fix the mistake . We chose Barker 's acceptance probability because we wanted to relate our method with existing work from the literature , i.e. , Variational Rejection Sampling ( VRS ) ( as we discussed in the 3rd paragraph of related work ) . Note that , with Barker 's acceptance probability and $ N=T=1 $ , our method essentially reduces to VRS . Differentiable versions of acceptance probability are desirable because we will optimize the proposed lower bound later on . \\textbf { Authors use the `` Dice-Enterprise '' algorithm . It does not seem necessary to appeal to such beautiful algorithm to understand the validity of the algorithm described page 3 . Here again , everything is done to frighten the reader and not much is done to explain what is being done ... } Regarding the name `` Dice-Enterprise , '' please see our comments in `` Common Response to all reviewers . '' The name `` dice-enterprise '' is given to any categorical sampling problem , where the success probabilities are intractable . We want to point out that we do not use the exact dice-enterprise mentioned in Morina et . al . ; rather we only use the nomenclature established there . We just wanted to make sure that all relevant work is appropriately discussed . Further , we have tried to analyze the effect of every hyper-parameter ( $ K , \\gamma $ , and $ M $ ) so that the reader could get more insights into the VSMC-PRC bound . Also , figure 1 of the main paper and 3rd paragraph of related work was added to make the reader more comfortable with the overall algorithm ; for example , the connection with VRS , FIVO , and IWAE . We will add more details about the notation so that the article is more readable . We thank you again for your constructive remarks ."}}