{"year": "2017", "forum": "ry2YOrcge", "title": "Learning a Natural Language Interface with Neural Programmer", "decision": "Accept (Poster)", "meta_review": "The paper applies a previously introduced method (from ICLR '16) to the challenging question answering dataset (wikitables). The results are strong and quite close to the performance obtained by a semantic parser. There reviewers generally agree that this is an interesting and promising direction / results. The application of the neural programmer to this dataset required model modifications which are reasonable though quite straightforward, so, in that respect, the work is incremental. Still, achieving strong results on this moderately sized dataset with an expressive \n model is far from trivial. Though the approach, as has been discussed, does not directly generalize to QA with large knowledge bases (as well as other end-to-end differentiable methods for the QA task proposed so far), it is an important step forward and the task is already realistic and important.\n \n Pros\n \n + interesting direction\n + strong results on a interesting dataset\n \n Cons\n - incremental, the model is largely the same as in the previous paper", "reviews": [{"review_id": "ry2YOrcge-0", "review_text": "The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). I do have a few objections though. -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the \"row selector\", the \"scalar answer\" and the \"lookup answer\" etc. -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive feedback . 1 ) We open-sourced the implementation of our model : https : //github.com/tensorflow/models/tree/master/neural_programmer . The results reported in the paper can be reproduced using the provided code . 2 ) We added an appendix section discussing how the operations are exactly defined and the role of the variables . 3 ) We agree that since we do full attention the model does not scale easily to huge databases . We have few ideas to scale up the model and leave that for future work . We would also like to point out the fact that public datasets requiring rich semantic parsing on large databases are not currently available . For example , current semantic parsing datasets on Freebase require much simpler programs to be induced than those considered in this work ."}, {"review_id": "ry2YOrcge-1", "review_text": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback . 1 ) As mentioned in the paper , we have open sourced our code and the code is here : https : //github.com/tensorflow/models/tree/master/neural_programmer . The results presented in the paper can be reproduced using the code . 2 ) We have included an ablation study under the title `` contribution of different operations '' , analyzing the contribution of different operations . We revised our submission and request the reviewer to check the latest version of the paper . The paper also discusses the effect of adding regularization to the model . We are currently running experiments to study the effect of anonymizing matched phrases and the addition of boolean features . Apart from these changes , to the best of our knowledge , the work presented in the paper does not make any significant new model design choices . For example , the newly proposed training objective is a requirement to apply Neural Programmer on this dataset and not a choice . If the reviewer has more model ablation studies in mind , we would be happy to include them ."}, {"review_id": "ry2YOrcge-2", "review_text": "This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role. It'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What's the headroom and oracle number with the current approach? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback . If the reviewer has more suggestions for error analysis , we would be happy to perform them . 1 ) The oracle score is 50.5 % which indicates that there is still a lot of room for improvement . 2 ) We think there are three reasons why the accuracy is lower compared to other NLP tasks : a ) Weak Supervision : most of the standard NLP tasks like parsing , part-of-speech tagging and machine translation have full supervision . Whereas in this task , the program that needs to be induced is not annotated and the model learns from a weak supervision signal consisting of only the final answer . b ) Small Training set : The dataset considered in this work has only 10k training examples . The datasets for other related language understanding tasks like textual entailment ( http : //nlp.stanford.edu/projects/snli/ ) and reading comprehension ( https : //rajpurkar.github.io/SQuAD-explorer/ ) have at least an order of magnitude more examples . The accuracy of our model in the training set is 53 % while it is only 34 % in the development and test set indicating that there is significant overfitting even after employing strong regularization . We think that with more training data , this gap can be reduced . c ) 21 % questions not answerable : The paper that introduced this dataset ( http : //cs.stanford.edu/~ppasupat/resource/ACL2015-paper.pdf ) reports that 21 % of questions ( on a random sample of 200 examples ) can not be answered because of various issues like annotation errors , tables requiring advanced normalization etc . We have revised our submission incorporating the above analysis ."}], "0": {"review_id": "ry2YOrcge-0", "review_text": "The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). I do have a few objections though. -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the \"row selector\", the \"scalar answer\" and the \"lookup answer\" etc. -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive feedback . 1 ) We open-sourced the implementation of our model : https : //github.com/tensorflow/models/tree/master/neural_programmer . The results reported in the paper can be reproduced using the provided code . 2 ) We added an appendix section discussing how the operations are exactly defined and the role of the variables . 3 ) We agree that since we do full attention the model does not scale easily to huge databases . We have few ideas to scale up the model and leave that for future work . We would also like to point out the fact that public datasets requiring rich semantic parsing on large databases are not currently available . For example , current semantic parsing datasets on Freebase require much simpler programs to be induced than those considered in this work ."}, "1": {"review_id": "ry2YOrcge-1", "review_text": "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback . 1 ) As mentioned in the paper , we have open sourced our code and the code is here : https : //github.com/tensorflow/models/tree/master/neural_programmer . The results presented in the paper can be reproduced using the code . 2 ) We have included an ablation study under the title `` contribution of different operations '' , analyzing the contribution of different operations . We revised our submission and request the reviewer to check the latest version of the paper . The paper also discusses the effect of adding regularization to the model . We are currently running experiments to study the effect of anonymizing matched phrases and the addition of boolean features . Apart from these changes , to the best of our knowledge , the work presented in the paper does not make any significant new model design choices . For example , the newly proposed training objective is a requirement to apply Neural Programmer on this dataset and not a choice . If the reviewer has more model ablation studies in mind , we would be happy to include them ."}, "2": {"review_id": "ry2YOrcge-2", "review_text": "This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role. It'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What's the headroom and oracle number with the current approach? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback . If the reviewer has more suggestions for error analysis , we would be happy to perform them . 1 ) The oracle score is 50.5 % which indicates that there is still a lot of room for improvement . 2 ) We think there are three reasons why the accuracy is lower compared to other NLP tasks : a ) Weak Supervision : most of the standard NLP tasks like parsing , part-of-speech tagging and machine translation have full supervision . Whereas in this task , the program that needs to be induced is not annotated and the model learns from a weak supervision signal consisting of only the final answer . b ) Small Training set : The dataset considered in this work has only 10k training examples . The datasets for other related language understanding tasks like textual entailment ( http : //nlp.stanford.edu/projects/snli/ ) and reading comprehension ( https : //rajpurkar.github.io/SQuAD-explorer/ ) have at least an order of magnitude more examples . The accuracy of our model in the training set is 53 % while it is only 34 % in the development and test set indicating that there is significant overfitting even after employing strong regularization . We think that with more training data , this gap can be reduced . c ) 21 % questions not answerable : The paper that introduced this dataset ( http : //cs.stanford.edu/~ppasupat/resource/ACL2015-paper.pdf ) reports that 21 % of questions ( on a random sample of 200 examples ) can not be answered because of various issues like annotation errors , tables requiring advanced normalization etc . We have revised our submission incorporating the above analysis ."}}