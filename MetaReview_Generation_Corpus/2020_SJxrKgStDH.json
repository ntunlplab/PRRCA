{"year": "2020", "forum": "SJxrKgStDH", "title": "SCALOR: Generative World Models with Scalable Object Representations", "decision": "Accept (Poster)", "meta_review": "After the author response and paper revision, the reviewers all came to appreciate this paper and unanimously recommended it be accepted.  The paper makes a nice contribution to generative modelling of object-oriented representations with large numbers of objects.  The authors adequately addressed the main reviewer concerns with their detailed rebuttal and revision.", "reviews": [{"review_id": "SJxrKgStDH-0", "review_text": "UPDATE: My original main concern was the lack of baseline, but during the rebuttal period the authors have conducted the request comparison and addressed my questions satisfactorily. Therefore, I would recommend the paper be accepted. --- Summary: This paper proposes a generative model and inference algorithm for discovering and propagating object latents in a way that scales to hundreds of objects. The key components of their approach is the parallel discovery and propagation of object latents as well as the explicit modeling of the background. The authors show that the model is able to model and generalize to scenes with hundreds of objects, including a real-world scene of a train station. Research Problem: This paper tackles the problem of scaling object-oriented generative modeling of scenes to scenes with a large number of objects. The main weakness of the submission is the lack of a baseline, and mainly for this result I would recommend rejecting the submission at its current state. However, if the authors are able to revise the submission to include such comparisons (with Kosiorek et al. (2018), van Steenkiste et al. (2018), and Alahi et al. (2016), detailed below), then I would highly consider accepting the paper, as the paper makes a novel contribution to modeling scenes with many more objects than previous work, as far as I am aware. Strengths: - The authors show that the method can model various synthetic and real-world datasets that show the efficacy of the method - The method can also generalize to more objects and longer timesteps than trained on. Weaknesses: - The main weakness of the submission is the lack of a baseline. It would be important to understand the differences between Kosiorek et al. (2018), which the authors claim is the closest work to theirs, and van Steenkiste et al (2018), which also models objects in a parallel fashion. Alah et al. (2016) also takes an approach of dividing the scene into grid cells and also demonstrate results on modeling human trajectories. - Motivation: Whereas the authors motivate the benefits for modeling objects, the motivation for specifically scaling to model hundreds of objects is less clear. It would be helpful for the authors to provide examples or arguments for the benefits of modeling so many objects at once. One argument against such a need is that humans only pay attention to a few number of objects at a time and do not explicitly model every possible object in parallel. One argument in favor of such a need is the ability to gain superhuman performance on tasks that could benefit from modeling multiple entities, such as playing Starcraft, detecting anamolies in medical scans, or modeling large scale weather patterns. - A possible limitation of the method may be in modeling interactions between entities. What mechanism in the propagation step allows for modeling such interactions, and if not, how could such a mechanism be incorporated? - How would SCALOR behave if the grid cells were smaller than the objects? In this case an object may occupy multiple grid cells. Would the authors provide an experiment analyzing this case? Would SCALOR model a object as multiple entities in this case (because the object spans multiple grid cells), or would SCALOR model the object with a single latent variable? Question: - What is the fundamental reason for why the structure of such a generative model would cause the latents to model objects, rather than something else, such as the image patches that show the empty space between objects? Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., & Savarese, S. (2016). Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 961-971). Van Steenkiste, S., Chang, M., Greff, K., & Schmidhuber, J. (2018). Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353. Kosiorek, A., Bewley, A., & Posner, I. (2017). Hierarchical attentive recurrent tracking. In Advances in Neural Information Processing Systems (pp. 3053-3061).", "rating": "6: Weak Accept", "reply_text": "* * Motivation Thanks for pointing this out . We agree with the suggestion , and will provide a clearer presentation of the motivation in our revision . We believe that the reviewer agrees with our perspective that it is not unusual to encounter natural scenes with many ( tens or hundreds of ) objects . The question is whether we need to explicitly attend to all of these or not . Regarding this , our perspective aligns with the reviewer \u2019 s argument that it is required for superhuman performance . Although we also agree with the limitation of the human attention capacity , we do not see a reason why we ought to artificially impose such a limitation in an AI system . For example , contemporary self-driving technology heavily takes advantage of this superhuman level of attention/detection capacity , although those approaches are based on expensive supervised learning systems . As another example , due to the limited capacity of human attention , we as humans need to resort to ( sequential ) search or scanning to find something among many objects , which is clearly more time-consuming than a parallel search . Also , considering that the architecture of modern computers is optimized for parallel processing , we believe that a contemporary AI system should maximize the utility of parallel processing . The parallel attention on the possible strategies used in the Monte-Carlo Search of the AlphaGo system is another strong evidence supporting this ( human Go-players are limited in this ability . ) We are grateful for the provided arguments on human attention and other examples . This is an interesting point and we will include it in our revision . * * Interactions between entities We agree that modeling interaction would indeed lead to a more comprehensive model . The main focus of this paper , however , is to make the current state-of-the-art ( SQAIR ) more scalable beyond just 3-4 objects . This focus on scalability , while not considering interaction , seems fairly reasonable , given that SQAIR also does not possess any interaction modeling ( note that SQAIR has a relation module in its architecture but it is not for modeling interactions but rather for more accurate tracking , and their experiments do not show interaction . ) . Indeed , adding interaction modeling , e.g , . using a graph network of the trackers , will be a key point for future work following the present work . * * What if the grid cells were smaller than the objects Because the encoder is a deep CNN , the input receptive field corresponding to a grid cell is actually a quite large area of the input image . The encoder learns which part of the input image a grid cell should represent , and thus , it is actually not a problem . The proposed method will still model an object entirely when the object can occupy more than one cell . In fact , in our Low Density ( LD ) experiment setting and the additional Very Low Density ( VLD ) setting that will be added in the revised version , the object size ( 24 * 24 ) is larger than the cell size ( 8 * 8 ) . * * About the fundamental reason the proposed method models the object and not the empty spaces One of the most fundamental reasons is that the model is inclined to predict as few bounding boxes as possible in the image . As we can see in the paper , in both the generation process and the inference process , we use \ud835\udc33\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60 to control whether we will generate the appearance latent variable \ud835\udc33\ud835\udc64\u210e\ud835\udc4e\ud835\udc61 and the location latent variable $ \\mathbf { \ud835\udc33 } ^\\text { \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 } $ . If $ \\mathbf { \ud835\udc33 } ^\\text { pres } = 1 $ , we introduce new KL terms for $ \\mathbf { \ud835\udc33 } ^\\text { what } $ and $ \\mathbf { \ud835\udc33 } ^\\text { \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 } $ in the ELBO . This is equivalent to increasing the loss in the objective function . Moreover , to penalize unnecessary bounding boxes , we also assume a low probability of object existence in the prior distinction in the ELBO to further encourage fewer boxes . Therefore , to reduce the KL loss while maintaining a good reconstruction , the model converges to a behavior that learns the object representations properly ."}, {"review_id": "SJxrKgStDH-1", "review_text": "I thank the authors for the detailed rebuttal, as well as for the updates to the text and several new experiments in the revised version of the paper. Most of my comments are addressed well. I am happy to improve my rating and recommend to accept the paper. --- The paper proposes an approach for unsupervised detection and tracking of objects in videos. The method continues the \"Attend, Infer, Repear\" (AIR) and Sequential AIR (SQAIR) line of work, but improves on these previous approaches in terms of scalability and can thus be applied to scenes with tens of objects. The scalability is obtained by replacing, wherever possible, sequential processing of objects by parallel processing. Experiments are performed on three datasets: moving DSprites, moving MNIST, and the real-world \"Crowded Grand Central Station\" dataset. The method seems to work in all cases, as confirmed by quantitative evaluation on the first two datasets and a qualitative evaluation on all three. I recommend rejecting the paper in its current state. On one hand, the results look quite good, and the method seems to indeed scale well to many objects. On the other hand, novelty is limited and the experiments are limited in that there are no comparisons with relevant baselines and no clear experiments showing the specific impact of the architectural modifications proposed in this paper. Moreover, the paper is over 9 pages, which, as far as I remember, requires the reviewers apply \"higher standards\". Overall, I would like the authors to comment on my concerns (below) and may reconsider my rating after that. Pros: 1) Relatively clear presentation. 2) Judging from the extensive qualitative results and the (limited) quantitative evaluation, the method seems to work. 3) I appreciate the additoinal results on generalization and parallel disovery in the appendix. Cons: 1) Novelty of the work seems quite limited. The main contribution is in improving the efficiency of object detection/tracking by parallelizing the computation, without much conceptual innovation. This might be a sufficient contribution (in the end, efficiency is very important for actually applying methods in practice), but then a thorough evaluation of the method would be expected (see further comments about it further). Moreover, the previously published SPAIR method by Crawford and Pineau seems very relevant and related, but is not compared against and is only briefly commented upon, despite the code for that method seems to be available online. I would like the authors to clarify the relation to SPAIR and preferably provide an experimental comparison. 2) The experiments are restricted. While there are quite many qualitative results, several issues remain: 2a) No baseline results are reported. It is thus impossible to judge if the method indeed improves upon related prior works. In particular, comparisons to SQAIR and SPAIR would be very useful. If possible, it would be useful to provide even more baselines, for instance some traditional tracking methods. Comparisons can be both in terms of tracking/reconstruction performanc, as well as in terms of computational efficiency. Both can be measured as functions of the number of objects in the scene. 2b) There are few quantitative resutls. In experiments 2-4 of section 5.1 it seems it would be fairly easy to introduce some, in particular, one could compare how do these more difficult settings compare to the \"default\" one. In section 5.2 given that the ground truth tracks are not available, evaluating tracking is challenging, but one could still compare NLL/reconstruction with appropriate baselines. The provided comparison to a vanilla VAE in therms of NLL is actually somewhat confusing - not sure what it tells the reader; moreover, I would actually expect the NLL of the proposed structured model to be better. Why is it not? 2c) Since the paper is largely about tracking objects through videos, it would be very usefuly to include a supplementary video with qualitative results. 3) (minor) Some issues with the presentation: 3a) I found the method description at times confusing and incomplete. For instance, it is quite unclear which exactly architectures are used for different components. The details can perhaps bee looked up in the SQAIR paper, but it would still be useful to summarize most crucial points in the supplementary material to make the paper more self-contained. 3b) The use of \\citet vs \\citep is often incorrect. \\citet should be used when the author names are a part of the text, while \\ciptp if the paper is cited in passing, for instance: \"Smith et al. (2010) have shown that snow is white.\" vs \"It is known that snow is white (Smith et al. 2010).\" 3c) Calling AIR a \"seminal work in the field of object detection\" is not quite correct - object detection is a well-established task in computer vision, and AIR is not really considered a seminal work in that field. It is a great paper, but not really in object detection.", "rating": "6: Weak Accept", "reply_text": "* * Comparison to traditional tracking methods and is SCALOR ( and SQAIR ) a tracking model ? Other similar works such as SQAIR do not conduct such a comparison because existing tracking methods are either supervised , not probabilistic , or can not learn to render . Our contribution is not in the space of supervised tracking or non-generative modeling . For these reasons , we believe that SQAIR and VRNN ( for generation quality ) should be the proper baselines to compare to . However , we will make sure to better acknowledge previous work on tracking . * * How do these more difficult settings ( in experiments 2-4 of section 5.1 ) compare to the \u201c default \u201d one . This is a good point . Thanks for pointing this out . We will provide a comparison to our default settings . * * In sec 5.2. , given that the ground truth tracks are not available , evaluating tracking is challenging , but one could still compare NLL/recon with appropriate baselines . We will provide a comparison to VRNN in our revision . Note that SQAIR can not be used because it does not work for that number of objects and is unable to cope with the background rendering . Hence , it seems that the best one can do is to compare NLL/recon to VRNN . We hope that the reviewer understands the difficulty of this research due to the total failure of the baseline in high-density settings . * * Why compare to VAE & why our NLL is not better than VAE . The goal of the comparison is to show that our model achieves its main goal ( learning object-oriented representations ) without losing the generation quality . Thus , VAE , which does not need to model temporal information and object-level representations is the appropriate baseline to evaluate the generation quality . Regarding the fact that the NLL of SCALOR is not better than that of VAE , it is actually a common misconception to expect a better generation quality for a discrete representation learning model like ours . Although a discrete structure in neural networks provides many advantages such as interpretability and compositionality , it generally comes with some performance degradation because it significantly limits the model space and optimization performance , compared to continuous representations . So , it is noteworthy to devise a model that uses the power of discrete latent representations while still having generation quality comparable to continuous models . * * Supplementary Video > We have made a project website where you can find videos . Link : https : //sites.google.com/view/scalor/home * * Minor Comments > Thanks for the comments . All of these comments make sense , and we will incorporate all of them in our revision ."}, {"review_id": "SJxrKgStDH-2", "review_text": "This paper proposes a generative model for scalable sequential object-oriented representation. The paper proposes several improvements based on the method SQAIR (Kosiorek et al. 2018b), (1) modeling the background and foreground dynamics separately; (2) parallelizing the propagation-discovery process by introducing the propose-reject model which reducing the time complexity. Finally, the proposed model can deal with orders of magnitude more objects than previous methods, and can model more complex scenes with complex background. Accept. The paper is clearly written and the experimental results are well organized. The results in the paper may be useful for unsupervised multi-objects tracking .I have one concern here, \uff081\uff09As argued in the paper, previous methods are difficult to deal with the nearly a hundred objects situation and there is no direct comparison for these methods. So has the author compared the method SCALOR with previous methods in few objects setting? Does the technical improvements of the method benefit in the few objects setting?", "rating": "6: Weak Accept", "reply_text": "Thank you for the suggestion . Yes , in the revision , we will add two additional experiments in a \u201c Very Low Density ( VLD ) \u201d setting , containing up to 4 objects , in which SQAIR works properly , and several different metrics to compare our method to SQAIR quantitatively . As we show quantitatively , our method can outperform SQAIR even in those settings , leading to more accurate and consistent bounding boxes ."}], "0": {"review_id": "SJxrKgStDH-0", "review_text": "UPDATE: My original main concern was the lack of baseline, but during the rebuttal period the authors have conducted the request comparison and addressed my questions satisfactorily. Therefore, I would recommend the paper be accepted. --- Summary: This paper proposes a generative model and inference algorithm for discovering and propagating object latents in a way that scales to hundreds of objects. The key components of their approach is the parallel discovery and propagation of object latents as well as the explicit modeling of the background. The authors show that the model is able to model and generalize to scenes with hundreds of objects, including a real-world scene of a train station. Research Problem: This paper tackles the problem of scaling object-oriented generative modeling of scenes to scenes with a large number of objects. The main weakness of the submission is the lack of a baseline, and mainly for this result I would recommend rejecting the submission at its current state. However, if the authors are able to revise the submission to include such comparisons (with Kosiorek et al. (2018), van Steenkiste et al. (2018), and Alahi et al. (2016), detailed below), then I would highly consider accepting the paper, as the paper makes a novel contribution to modeling scenes with many more objects than previous work, as far as I am aware. Strengths: - The authors show that the method can model various synthetic and real-world datasets that show the efficacy of the method - The method can also generalize to more objects and longer timesteps than trained on. Weaknesses: - The main weakness of the submission is the lack of a baseline. It would be important to understand the differences between Kosiorek et al. (2018), which the authors claim is the closest work to theirs, and van Steenkiste et al (2018), which also models objects in a parallel fashion. Alah et al. (2016) also takes an approach of dividing the scene into grid cells and also demonstrate results on modeling human trajectories. - Motivation: Whereas the authors motivate the benefits for modeling objects, the motivation for specifically scaling to model hundreds of objects is less clear. It would be helpful for the authors to provide examples or arguments for the benefits of modeling so many objects at once. One argument against such a need is that humans only pay attention to a few number of objects at a time and do not explicitly model every possible object in parallel. One argument in favor of such a need is the ability to gain superhuman performance on tasks that could benefit from modeling multiple entities, such as playing Starcraft, detecting anamolies in medical scans, or modeling large scale weather patterns. - A possible limitation of the method may be in modeling interactions between entities. What mechanism in the propagation step allows for modeling such interactions, and if not, how could such a mechanism be incorporated? - How would SCALOR behave if the grid cells were smaller than the objects? In this case an object may occupy multiple grid cells. Would the authors provide an experiment analyzing this case? Would SCALOR model a object as multiple entities in this case (because the object spans multiple grid cells), or would SCALOR model the object with a single latent variable? Question: - What is the fundamental reason for why the structure of such a generative model would cause the latents to model objects, rather than something else, such as the image patches that show the empty space between objects? Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., & Savarese, S. (2016). Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 961-971). Van Steenkiste, S., Chang, M., Greff, K., & Schmidhuber, J. (2018). Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353. Kosiorek, A., Bewley, A., & Posner, I. (2017). Hierarchical attentive recurrent tracking. In Advances in Neural Information Processing Systems (pp. 3053-3061).", "rating": "6: Weak Accept", "reply_text": "* * Motivation Thanks for pointing this out . We agree with the suggestion , and will provide a clearer presentation of the motivation in our revision . We believe that the reviewer agrees with our perspective that it is not unusual to encounter natural scenes with many ( tens or hundreds of ) objects . The question is whether we need to explicitly attend to all of these or not . Regarding this , our perspective aligns with the reviewer \u2019 s argument that it is required for superhuman performance . Although we also agree with the limitation of the human attention capacity , we do not see a reason why we ought to artificially impose such a limitation in an AI system . For example , contemporary self-driving technology heavily takes advantage of this superhuman level of attention/detection capacity , although those approaches are based on expensive supervised learning systems . As another example , due to the limited capacity of human attention , we as humans need to resort to ( sequential ) search or scanning to find something among many objects , which is clearly more time-consuming than a parallel search . Also , considering that the architecture of modern computers is optimized for parallel processing , we believe that a contemporary AI system should maximize the utility of parallel processing . The parallel attention on the possible strategies used in the Monte-Carlo Search of the AlphaGo system is another strong evidence supporting this ( human Go-players are limited in this ability . ) We are grateful for the provided arguments on human attention and other examples . This is an interesting point and we will include it in our revision . * * Interactions between entities We agree that modeling interaction would indeed lead to a more comprehensive model . The main focus of this paper , however , is to make the current state-of-the-art ( SQAIR ) more scalable beyond just 3-4 objects . This focus on scalability , while not considering interaction , seems fairly reasonable , given that SQAIR also does not possess any interaction modeling ( note that SQAIR has a relation module in its architecture but it is not for modeling interactions but rather for more accurate tracking , and their experiments do not show interaction . ) . Indeed , adding interaction modeling , e.g , . using a graph network of the trackers , will be a key point for future work following the present work . * * What if the grid cells were smaller than the objects Because the encoder is a deep CNN , the input receptive field corresponding to a grid cell is actually a quite large area of the input image . The encoder learns which part of the input image a grid cell should represent , and thus , it is actually not a problem . The proposed method will still model an object entirely when the object can occupy more than one cell . In fact , in our Low Density ( LD ) experiment setting and the additional Very Low Density ( VLD ) setting that will be added in the revised version , the object size ( 24 * 24 ) is larger than the cell size ( 8 * 8 ) . * * About the fundamental reason the proposed method models the object and not the empty spaces One of the most fundamental reasons is that the model is inclined to predict as few bounding boxes as possible in the image . As we can see in the paper , in both the generation process and the inference process , we use \ud835\udc33\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60 to control whether we will generate the appearance latent variable \ud835\udc33\ud835\udc64\u210e\ud835\udc4e\ud835\udc61 and the location latent variable $ \\mathbf { \ud835\udc33 } ^\\text { \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 } $ . If $ \\mathbf { \ud835\udc33 } ^\\text { pres } = 1 $ , we introduce new KL terms for $ \\mathbf { \ud835\udc33 } ^\\text { what } $ and $ \\mathbf { \ud835\udc33 } ^\\text { \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 } $ in the ELBO . This is equivalent to increasing the loss in the objective function . Moreover , to penalize unnecessary bounding boxes , we also assume a low probability of object existence in the prior distinction in the ELBO to further encourage fewer boxes . Therefore , to reduce the KL loss while maintaining a good reconstruction , the model converges to a behavior that learns the object representations properly ."}, "1": {"review_id": "SJxrKgStDH-1", "review_text": "I thank the authors for the detailed rebuttal, as well as for the updates to the text and several new experiments in the revised version of the paper. Most of my comments are addressed well. I am happy to improve my rating and recommend to accept the paper. --- The paper proposes an approach for unsupervised detection and tracking of objects in videos. The method continues the \"Attend, Infer, Repear\" (AIR) and Sequential AIR (SQAIR) line of work, but improves on these previous approaches in terms of scalability and can thus be applied to scenes with tens of objects. The scalability is obtained by replacing, wherever possible, sequential processing of objects by parallel processing. Experiments are performed on three datasets: moving DSprites, moving MNIST, and the real-world \"Crowded Grand Central Station\" dataset. The method seems to work in all cases, as confirmed by quantitative evaluation on the first two datasets and a qualitative evaluation on all three. I recommend rejecting the paper in its current state. On one hand, the results look quite good, and the method seems to indeed scale well to many objects. On the other hand, novelty is limited and the experiments are limited in that there are no comparisons with relevant baselines and no clear experiments showing the specific impact of the architectural modifications proposed in this paper. Moreover, the paper is over 9 pages, which, as far as I remember, requires the reviewers apply \"higher standards\". Overall, I would like the authors to comment on my concerns (below) and may reconsider my rating after that. Pros: 1) Relatively clear presentation. 2) Judging from the extensive qualitative results and the (limited) quantitative evaluation, the method seems to work. 3) I appreciate the additoinal results on generalization and parallel disovery in the appendix. Cons: 1) Novelty of the work seems quite limited. The main contribution is in improving the efficiency of object detection/tracking by parallelizing the computation, without much conceptual innovation. This might be a sufficient contribution (in the end, efficiency is very important for actually applying methods in practice), but then a thorough evaluation of the method would be expected (see further comments about it further). Moreover, the previously published SPAIR method by Crawford and Pineau seems very relevant and related, but is not compared against and is only briefly commented upon, despite the code for that method seems to be available online. I would like the authors to clarify the relation to SPAIR and preferably provide an experimental comparison. 2) The experiments are restricted. While there are quite many qualitative results, several issues remain: 2a) No baseline results are reported. It is thus impossible to judge if the method indeed improves upon related prior works. In particular, comparisons to SQAIR and SPAIR would be very useful. If possible, it would be useful to provide even more baselines, for instance some traditional tracking methods. Comparisons can be both in terms of tracking/reconstruction performanc, as well as in terms of computational efficiency. Both can be measured as functions of the number of objects in the scene. 2b) There are few quantitative resutls. In experiments 2-4 of section 5.1 it seems it would be fairly easy to introduce some, in particular, one could compare how do these more difficult settings compare to the \"default\" one. In section 5.2 given that the ground truth tracks are not available, evaluating tracking is challenging, but one could still compare NLL/reconstruction with appropriate baselines. The provided comparison to a vanilla VAE in therms of NLL is actually somewhat confusing - not sure what it tells the reader; moreover, I would actually expect the NLL of the proposed structured model to be better. Why is it not? 2c) Since the paper is largely about tracking objects through videos, it would be very usefuly to include a supplementary video with qualitative results. 3) (minor) Some issues with the presentation: 3a) I found the method description at times confusing and incomplete. For instance, it is quite unclear which exactly architectures are used for different components. The details can perhaps bee looked up in the SQAIR paper, but it would still be useful to summarize most crucial points in the supplementary material to make the paper more self-contained. 3b) The use of \\citet vs \\citep is often incorrect. \\citet should be used when the author names are a part of the text, while \\ciptp if the paper is cited in passing, for instance: \"Smith et al. (2010) have shown that snow is white.\" vs \"It is known that snow is white (Smith et al. 2010).\" 3c) Calling AIR a \"seminal work in the field of object detection\" is not quite correct - object detection is a well-established task in computer vision, and AIR is not really considered a seminal work in that field. It is a great paper, but not really in object detection.", "rating": "6: Weak Accept", "reply_text": "* * Comparison to traditional tracking methods and is SCALOR ( and SQAIR ) a tracking model ? Other similar works such as SQAIR do not conduct such a comparison because existing tracking methods are either supervised , not probabilistic , or can not learn to render . Our contribution is not in the space of supervised tracking or non-generative modeling . For these reasons , we believe that SQAIR and VRNN ( for generation quality ) should be the proper baselines to compare to . However , we will make sure to better acknowledge previous work on tracking . * * How do these more difficult settings ( in experiments 2-4 of section 5.1 ) compare to the \u201c default \u201d one . This is a good point . Thanks for pointing this out . We will provide a comparison to our default settings . * * In sec 5.2. , given that the ground truth tracks are not available , evaluating tracking is challenging , but one could still compare NLL/recon with appropriate baselines . We will provide a comparison to VRNN in our revision . Note that SQAIR can not be used because it does not work for that number of objects and is unable to cope with the background rendering . Hence , it seems that the best one can do is to compare NLL/recon to VRNN . We hope that the reviewer understands the difficulty of this research due to the total failure of the baseline in high-density settings . * * Why compare to VAE & why our NLL is not better than VAE . The goal of the comparison is to show that our model achieves its main goal ( learning object-oriented representations ) without losing the generation quality . Thus , VAE , which does not need to model temporal information and object-level representations is the appropriate baseline to evaluate the generation quality . Regarding the fact that the NLL of SCALOR is not better than that of VAE , it is actually a common misconception to expect a better generation quality for a discrete representation learning model like ours . Although a discrete structure in neural networks provides many advantages such as interpretability and compositionality , it generally comes with some performance degradation because it significantly limits the model space and optimization performance , compared to continuous representations . So , it is noteworthy to devise a model that uses the power of discrete latent representations while still having generation quality comparable to continuous models . * * Supplementary Video > We have made a project website where you can find videos . Link : https : //sites.google.com/view/scalor/home * * Minor Comments > Thanks for the comments . All of these comments make sense , and we will incorporate all of them in our revision ."}, "2": {"review_id": "SJxrKgStDH-2", "review_text": "This paper proposes a generative model for scalable sequential object-oriented representation. The paper proposes several improvements based on the method SQAIR (Kosiorek et al. 2018b), (1) modeling the background and foreground dynamics separately; (2) parallelizing the propagation-discovery process by introducing the propose-reject model which reducing the time complexity. Finally, the proposed model can deal with orders of magnitude more objects than previous methods, and can model more complex scenes with complex background. Accept. The paper is clearly written and the experimental results are well organized. The results in the paper may be useful for unsupervised multi-objects tracking .I have one concern here, \uff081\uff09As argued in the paper, previous methods are difficult to deal with the nearly a hundred objects situation and there is no direct comparison for these methods. So has the author compared the method SCALOR with previous methods in few objects setting? Does the technical improvements of the method benefit in the few objects setting?", "rating": "6: Weak Accept", "reply_text": "Thank you for the suggestion . Yes , in the revision , we will add two additional experiments in a \u201c Very Low Density ( VLD ) \u201d setting , containing up to 4 objects , in which SQAIR works properly , and several different metrics to compare our method to SQAIR quantitatively . As we show quantitatively , our method can outperform SQAIR even in those settings , leading to more accurate and consistent bounding boxes ."}}