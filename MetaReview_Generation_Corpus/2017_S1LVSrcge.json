{"year": "2017", "forum": "S1LVSrcge", "title": "Variable Computation in Recurrent Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.\n \n Pros:\n - new idea\n - convincing results in a head to head comparison between different set ups.\n \n Cons:\n - results are not nearly as good as the state of the art on the reported tasks.\n \n The reviewers and I had several rounds of discussion on whether or not to accept the paper. One reviewer had significant reservations about the paper since the results were far from SOTA. However, since getting SOTA often requires a combination of several tricks, I felt that perhaps it would not be fair to require this, and gave them the benefit of doubt (especially because the other two reviewers did not think this was a dealbreaker). In my opinion, the authors did a fair enough job on the head to head comparison between their proposed VCRNN models and the underlying LSTMs and GRUs, which showed that the model did well enough.", "reviews": [{"review_id": "S1LVSrcge-0", "review_text": "This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account. The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It\u2019s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I\u2019m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I\u2019ll come back to this point in the next paragraph because it\u2019s mainly a clarity issue. The abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it\u2019s not clear from the text how the hyperparameter \\bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don\u2019t consider this a very serious flaw because I\u2019m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value. To my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper\u2019s strongest points. It\u2019s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it\u2019s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. Pros: Original clever idea. Nice interesting visualizations. Interesting experiments. Cons: Some experimental details are not clear. I\u2019m not convinced of the strength of the baseline. The paper shouldn\u2019t claim actual computational savings without reporting wall-clock times. Edit: I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. Edit: Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out. ", "rating": "7: Good paper, accept", "reply_text": "We hope that the general answer above will have clarified the choice of \\bar { m } : the PTB result corresponds to the best validation perplexity , and Figure 5 illustrates the influence of the choice on the model performance and number of operations . Regardless of the question of the number of hyper-parameter ( would the RNN dimension count as one ? ) , whether it has a regularizing effect is certainly worth investigating : it seems that VCRNN overfits a bit less than the RNN with the same number of operations , but the VCGRU / GRU comparison gives the opposite result . We will be sure to investigate this further . We will alter the language to clarify the fact that the efficiency gain is indeed currently conceptual : a reduced number of operations , which will require a lower-level programming effort to produce savings in wall-clock times . We mostly hope that these results will inspire other work in variable computation , to the point where the advantage of such adaptive low-level implementation will make it unequivocally worthwhile . We will also add the LSTM and GRU baselines to our results ."}, {"review_id": "S1LVSrcge-1", "review_text": "This is high novelty work, and an enjoyable read. My concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.) In pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation. --- 2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment.", "rating": "7: Good paper, accept", "reply_text": "The general answer above provides results on PTB for LSTMs and GRUs , both with the same dimension and with comparable numbers of operations as the RNN or VCRNN ( or VCGRU ) . We also discuss our gating mechanism as it relates to others . We will add and expand on both in the next version of the paper ."}, {"review_id": "S1LVSrcge-2", "review_text": "TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., \"variable computation\") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline. === Gating Mechanism === At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism. Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \\bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are. This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated. A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well. === Variable Computation === One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell. === Evaluation === This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments. The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions. One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar. === Minor === * Please add Equations numbers to the paper, hard to refer to in a review and discussion! References Chung et al., \"Hierarchical Multiscale Recurrent Neural Networks,\" in 2016. Graves et al., \"Adaptive Computation Time for Recurrent Neural Networks,\" in 2016. Wu et al., \"On Multiplicative Integration with Recurrent Neural Networks,\" in 2016.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We hope that our general answer will have addressed most of the issues raised in the above review , and that the following will answer the rest . === Gating Mechanism === Epsilon was introduced to ensure the soundness of the model description : if the mask value is smaller than epsilon for dimension i , then h_i can be carried over from the previous time step without any computation . In practice however , it was given a fixed value to match the GPU floating point sensitivity . The reason for the choice of this specific \u201c first-D \u201d shape is that we wanted to be able to control the computational complexity efficiently . Thus , we only have one parameter ; a more general solution using any subset of the dimensions would be as expensive to compute as the full new hidden state , which would limit the usefulness of the mechanism . === Evaluation === Indeed , the magnitude of the change for the GRU hidden state at each time step does follow a similar pattern , in that it changes slightly more at the beginning of each word ( the average gate value goes down monotonically from 0.8 at the beginning of a word to 0.6 at the end ) . There are two main differences however . First , the GRU gate controller seems to only rely on the information provided by encountering a space character , and does not exhibit any of the morphological phenomena that can be observed with the VCRNN ( as illustrated in Figure 4 ) . Secondly , and more importantly , that measure is given a posteriori : computing the gate is as expensive as computing the full new state . One of the purposes of our work was to show that it is possible to obtain that information for much cheaper . We will add this discussion to the paper , as these are indeed important distinctions ."}], "0": {"review_id": "S1LVSrcge-0", "review_text": "This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account. The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It\u2019s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I\u2019m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I\u2019ll come back to this point in the next paragraph because it\u2019s mainly a clarity issue. The abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it\u2019s not clear from the text how the hyperparameter \\bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don\u2019t consider this a very serious flaw because I\u2019m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value. To my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper\u2019s strongest points. It\u2019s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it\u2019s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. Pros: Original clever idea. Nice interesting visualizations. Interesting experiments. Cons: Some experimental details are not clear. I\u2019m not convinced of the strength of the baseline. The paper shouldn\u2019t claim actual computational savings without reporting wall-clock times. Edit: I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. Edit: Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out. ", "rating": "7: Good paper, accept", "reply_text": "We hope that the general answer above will have clarified the choice of \\bar { m } : the PTB result corresponds to the best validation perplexity , and Figure 5 illustrates the influence of the choice on the model performance and number of operations . Regardless of the question of the number of hyper-parameter ( would the RNN dimension count as one ? ) , whether it has a regularizing effect is certainly worth investigating : it seems that VCRNN overfits a bit less than the RNN with the same number of operations , but the VCGRU / GRU comparison gives the opposite result . We will be sure to investigate this further . We will alter the language to clarify the fact that the efficiency gain is indeed currently conceptual : a reduced number of operations , which will require a lower-level programming effort to produce savings in wall-clock times . We mostly hope that these results will inspire other work in variable computation , to the point where the advantage of such adaptive low-level implementation will make it unequivocally worthwhile . We will also add the LSTM and GRU baselines to our results ."}, "1": {"review_id": "S1LVSrcge-1", "review_text": "This is high novelty work, and an enjoyable read. My concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.) In pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation. --- 2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment.", "rating": "7: Good paper, accept", "reply_text": "The general answer above provides results on PTB for LSTMs and GRUs , both with the same dimension and with comparable numbers of operations as the RNN or VCRNN ( or VCGRU ) . We also discuss our gating mechanism as it relates to others . We will add and expand on both in the next version of the paper ."}, "2": {"review_id": "S1LVSrcge-2", "review_text": "TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., \"variable computation\") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline. === Gating Mechanism === At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism. Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \\bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are. This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated. A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well. === Variable Computation === One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell. === Evaluation === This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments. The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions. One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar. === Minor === * Please add Equations numbers to the paper, hard to refer to in a review and discussion! References Chung et al., \"Hierarchical Multiscale Recurrent Neural Networks,\" in 2016. Graves et al., \"Adaptive Computation Time for Recurrent Neural Networks,\" in 2016. Wu et al., \"On Multiplicative Integration with Recurrent Neural Networks,\" in 2016.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We hope that our general answer will have addressed most of the issues raised in the above review , and that the following will answer the rest . === Gating Mechanism === Epsilon was introduced to ensure the soundness of the model description : if the mask value is smaller than epsilon for dimension i , then h_i can be carried over from the previous time step without any computation . In practice however , it was given a fixed value to match the GPU floating point sensitivity . The reason for the choice of this specific \u201c first-D \u201d shape is that we wanted to be able to control the computational complexity efficiently . Thus , we only have one parameter ; a more general solution using any subset of the dimensions would be as expensive to compute as the full new hidden state , which would limit the usefulness of the mechanism . === Evaluation === Indeed , the magnitude of the change for the GRU hidden state at each time step does follow a similar pattern , in that it changes slightly more at the beginning of each word ( the average gate value goes down monotonically from 0.8 at the beginning of a word to 0.6 at the end ) . There are two main differences however . First , the GRU gate controller seems to only rely on the information provided by encountering a space character , and does not exhibit any of the morphological phenomena that can be observed with the VCRNN ( as illustrated in Figure 4 ) . Secondly , and more importantly , that measure is given a posteriori : computing the gate is as expensive as computing the full new state . One of the purposes of our work was to show that it is possible to obtain that information for much cheaper . We will add this discussion to the paper , as these are indeed important distinctions ."}}