{"year": "2021", "forum": "yT7-k6Q6gda", "title": "Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization", "decision": "Reject", "meta_review": "This is a tricky one, hence my low confidence rating.\n\nThe reviewers seem to agree that the paper is well written, easy to follow, and that it tests a relevant hypothesis that is of interest to the community. There was some disagreement as to whether the experiments are comprehensive, complete and/or conclusive enough, although on balance it seems reviewers were overall satisfied barring a few additional requests which the authors addressed in their feedback.\n\nHowever, no reviewers support the paper strongly (borderline accepts) while R5 remains unconvinced and has raised a technical point in their review about the estimator of the Trace of the Fisher information matrix. The question R5 has raised is central to the paper's methods, arguments and conclusions. In a message to ACs and PCs the authors raised concerns about R5. I personally thought that while R5 could have worded their review more carefully and respectfully (as I pointed out in my respose) the concerns raised were otherwise motivated, the reviewer engaged in a discussion, and the arguments were laid out clearly. I side with R5 and I think that the paper should be rewritten with more clarity on this question - the problem R5 found is likely to trip up others who read or build on the paper.\n\nThe authors have raised that there are two parallel submissions closely related to this one, complicating the decision making somewhat:\n[1] https://openreview.net/forum?id=rq_Qr0c1Hyo [2] https://openreview.net/forum?id=3q5IqUrkcF\n", "reviews": [{"review_id": "yT7-k6Q6gda-0", "review_text": "After the rebuttal , the authors added a section on large-batch training , which shows that the catastrophic Fisher explosion also occurs in large batch training . This makes the paper more convincing . However , the main concern that this paper lacks theoretical contribution still exists . Therefore , I keep the weak acceptance recommendation . Summary : In this paper , the authors made a study on Fisher Information Matrix ( FIM ) in the training dynamics . By tracking the trace of FIM in the initial training process , the authors show that it can be strongly connected to a model \u2019 s generalization performance . This successfully explains why some hyper-parameter choices lead to significantly better generalization . Finally , empirical evaluations show that penalizing the trace FIM can reduce memorization and encourage wide minima solution , leading to better generalization . Pros : This paper takes one step forward to the question of why some hyper-parameter choices ( small batch size or large learning rate ) lead to better generalization . In many previous works , the implicit regularization effect of stochastic gradient descent ( SGD ) accounts for this observation ( small batch size generalizes better ) . Most of the above works focused on the anisotropic noise in SGD . However , there is still a gap between the anisotropic noise and implicit regularization , i.e.why does anisotropic noise lead to implicit regularization hence better generalization . [ 1 ] explains this in a very general way : it helps to escape local minima . This paper tries to fill in the gap by leveraging FIM . FIM is also studied in the context of SGD regularization [ 2 ] . However , it only shows that the trace of FIM affects optimization performance . How it relates to the generalization performance still remains unknown . Section 4 and section 5 shed light on this question by drawing connection to memorization and final curvatures . Larger scale dataset such as Tiny-Imagenet is included in the experiments . This shows that the conclusion also stands on a more practical and noisy dataset . Cons : Most of the empirical verifications in this paper are comparing large and small learning rates . To be more general , the authors can also include the study of batch size . The central claim in this paper would be more convincing if catastrophic Fisher explosion also occurs in large batch training . More importantly , reducing the trace can improve large batch training generalization . Additionally , the connection between FIM and noisy gradient descent [ 3 ] is worth mentioning . For example , how does the trace of FIM in noisy gradient descent compared to other methods ? One of the postulations is regularizing the trace of FIM reduces memorization . The paper verifies this by introducing noisy labels . This requires modification to the training dataset . Another way to verify this postulation is to make comparisons on out-of-distribution dataset ( CIFAR10-C and CIFAR100-C ) . The method which reduces memorization should be more robust on out-of-distribution datasets . A majority of the analysis comes from empirical evaluations . The claim is more convincing if the conclusion can also be analytically proved in the convex quadratic scenario . The experiments section of this paper is based on the empirical Fisher matrix ( true Fisher matrix is expensive to compute ) . Although empirical Fisher is commonly used in practice to approximate true Fisher , the approximation is only accurate in the local minima [ 4 ] . The paper should have a discussion on the difference between true Fisher and empirical Fisher . Legends are missing in Figure 4 and Figure 6 . [ 1 ] : Zhu , Zhanxing et al. \u201c The Anisotropic Noise in Stochastic Gradient Descent : Its Behavior of Escaping from Sharp Minima and Regularization Effects. \u201d ICML ( 2019 ) . [ 2 ] : Wen , Yeming et al. \u201c An Empirical Study of Stochastic Gradient Descent with Structured Covariance Noise. \u201d AISTATS ( 2020 ) . [ 3 ] : Wu , Jingfeng et al. \u201c On the Noisy Gradient Descent that Generalizes as SGD. \u201d ICML ( 2020 ) [ 4 ] : Kunstner , Frederik et al. \u201c Limitations of the empirical Fisher approximation for natural gradient descent. \u201d NeurIPS ( 2019 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201d One of the postulations is regularizing the trace of FIM reduces memorization . The paper verifies this by introducing noisy labels . This requires modification to the training dataset . Another way to verify this postulation is to make comparisons on the out-of-distribution dataset ( CIFAR10-C and CIFAR100-C ) . The method which reduces memorization should be more robust on out-of-distribution datasets. \u201d * * & nbsp ; & nbsp ; & nbsp ; This is a great idea - we will certainly add this comparison in the final version ( or in this rebuttal phase if we manage to run it before the coming Tuesday ) . * * \u201d The experiments section of this paper is based on the empirical Fisher matrix ( true Fisher matrix is expensive to compute ) . Although empirical Fisher is commonly used in practice to approximate true Fisher , the approximation is only accurate in the local minima [ 4 ] . The paper should have a discussion on the difference between true Fisher and empirical Fisher. \u201d * * & nbsp ; & nbsp ; & nbsp ; We apologize for the confusion ! We do compute the trace of the actual Fisher matrix . To approximate the Fisher Information Matrix , we sample y from the model prediction p ( y|x ) , without using the training labels . Hence it is not the `` empirical FIM '' mentioned in [ 1 ] , but rather a Monte Carlo estimate of the actual FIM . We will clarify this in the paper . * * \u201d A majority of the analysis comes from empirical evaluations . The claim is more convincing if the conclusion can also be analytically proved in the convex quadratic scenario \u201d * * & nbsp ; & nbsp ; & nbsp ; We fully agree . While a theoretical argument would be challenging for us to add at this stage , we will make a more explicit connection to research on generalization bounds . In particular , Jiang et al.in \u201c Fantastic Generalization Measures and Where to Find Them \u201d ( https : //arxiv.org/abs/1912.02178 ) argue that sharpness based bounds are best correlates of generalization across multiple experiments . To the extent to which sharpness-based bounds are sensible bounds on generalization , Tr ( F ) can be argued to lead to tighter generalization bounds and hence better generalization . * * '' Legends are missing in Figure 4 and Figure 6 . `` * * Thank you . We will fix it . * * '' Additionally , the connection between FIM and noisy gradient descent [ 3 ] is worth mentioning . `` * * Thank you . We will add a discussion of [ 1-3 ] to the paper ."}, {"review_id": "yT7-k6Q6gda-1", "review_text": "# Summary This work claims SGD regularizes the model through penalizing the trace of Fisher in the early phase . This claim is supported by the similar generalization behavior of SGD ( with optimal learning rate ) and Fisher penalization ( for SGD with small learning rate ) . A series experiments are conducted to verify the understanding . I find the paper writing is rather misleading . On the one hand , no mathematical justifications , and the logical chain is weak hard to claim which factor is the cause and which one is the effect . On the other hand , though written as `` trace of Fisher '' in the paper , in experiments they compute a different regularizer norm of expected gradient . Detailed questions come in the following . # Questions 1 . Abstract , `` We highlight that in the absence of implicit or explicit regularization ... '' . How do you get rid of the implicit regularization ? ? ? 2.Eq . ( 2 ) is super misleading . Trace of Fisher is the expected gradient norm , but in Eq . ( 2 ) you compute norm of expected gradient . Statistically the latter has nothing to do with the former : one is the second moment , and the other is the squared first moment . Please justify . 3.I have a very simple explanation to your observed phenomenon . In the beginning , gradient is large , thus your version of `` trace of fisher '' is large , as the gradient decreases , the `` trace of fisher '' decreases . Now we look at large learning rate and small learning rate . With large learning rate , SGD converges faster , thus its gradients decrease faster , which causes the `` trace of fisher '' decreases faster . But with small learning rate , SGD converges slower , thus its gradient decrease slower , and the `` trace of fisher '' appears to be large in the beginning epochs . Therefore I am not at all convinced by your arguments . 4.From the above discussion , at least you need to normalize the `` trace of fisher '' by gradient norm which then becomes a measurement of gradient confusion . See following for references . 5.FP/GP.If I understand correctly , you need to compute gradient of gradient , which expands your computation graph at least twice . Can you report the GPU memory consumption ? 6.Finally , let us take about the practical role of FP . According to Table 1 , the improvement of FP over GP is marginal . I can not see a potential of FP . Not only in theory , but also in practice this paper is not satisfactory . # Missing Refs Tons of theory paper should be discussed . A few of them come to my brain are listed in below . Please do a more complete literature investigation . Fisher - Liang , Tengyuan , et al . `` Fisher-rao metric , geometry , and complexity of neural networks . '' The 22nd International Conference on Artificial Intelligence and Statistics . 2019.- Karakida , Ryo , Shotaro Akaho , and Shun-ichi Amari . `` Universal statistics of Fisher information in deep neural networks : Mean field approach . '' The 22nd International Conference on Artificial Intelligence and Statistics . PMLR , 2019 . SGD regularization mechanism - Daneshmand , H. , Kohler , J. , Lucchi , A. , and Hofmann , T. Escaping saddles with stochastic gradients . arXiv preprint arXiv:1803.05999 , 2018 . - Zhu , Zhanxing , et al . `` The Anisotropic Noise in Stochastic Gradient Descent : Its Behavior of Escaping from Sharp Minima and Regularization Effects . '' arXiv preprint arXiv:1803.00195 ( 2018 ) . - Wu , Jingfeng , et al . `` On the Noisy Gradient Descent that Generalizes as SGD . '' arXiv preprint arXiv:1906.07405 ( 2019 ) . Gradient confusion is highly related to the trace Fisher . See this one and its follow-ups . - Sankararaman , Karthik A. , et al . `` The impact of neural network overparameterization on gradient confusion and stochastic gradient descent . '' arXiv preprint arXiv:1904.06963 ( 2019 ) . Adversarial regularization is also related to GP/FP : - Miyato , Takeru , et al . `` Virtual adversarial training : a regularization method for supervised and semi-supervised learning . '' IEEE transactions on pattern analysis and machine intelligence 41.8 ( 2018 ) : 1979-1993 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * \u201c How do you get rid of the implicit regularization ? ? ? \u201d * * & nbsp ; & nbsp ; & nbsp ; In our opinion , one of ( many ) key outstanding questions in deep learning theory is why and how high learning rate or small batch size in SGD impacts generalization . By implicit regularization we were referring to the generalization effect of a reasonably large learning rate . We suppress its regularization effect by using lower values of learning rate ( which we refer to as a sub-optimal learning rate in the paper ) . We apologize if it were not clear , and we will improve background and motivation in the paper . * * '' Finally , let us take about the practical role of FP . According to Table 1 , the improvement of FP over GP is marginal . I can not see a potential of FP . Not only in theory , but also in practice this paper is not satisfactory . `` * * & nbsp ; & nbsp ; & nbsp ; The difference is approximately 2 % for DenseNet/C100 and VGG11/C100 and 4 % on SimpleCNN/C10 . In these cases we would like to argue it is not marginal by the standards of the computer vision community . In the other two ( out of 5 ) cases you are right - they are similar . However , even if they are similar , that \u2019 s actually still exciting . We are primarily excited about the theoretical importance of the finding that we can explain away an important aspect of implicit regularization in SGD . Please note that two other works were concurrently submitted to ICLR that propose only gradient norm regularization as the implicit regularizer [ 1,2 ] . On a practical role of FP , let us also draw your attention to experiments involving noisy labels . In these experiments ( Table 3 ) FP was substantially better than GP_x , and comparable to mixup . Please note that gradient norm regularization hasn \u2019 t been shown to be effective in this setting before . [ 1 ] https : //openreview.net/forum ? id=rq_Qr0c1Hyo [ 2 ] https : //openreview.net/forum ? id=3q5IqUrkcF * * '' From the above discussion , at least you need to normalize the `` trace of fisher '' by gradient norm which then becomes a measurement of gradient confusion . See following for references '' * * & nbsp ; & nbsp ; & nbsp ; Given that we measure the trace of the actual Fisher , not \u201c trace of Fisher \u201d , we assume that this answers this question , but please let us know if not . * * Can you report the GPU memory consumption ? * * & nbsp ; & nbsp ; & nbsp ; That \u2019 s a good point . It is approximately 2-3x larger . We will add an exact measurement to the final version of the paper . * * Tons of theory paper should be discussed * * & nbsp ; & nbsp ; & nbsp ; We agree with the references provided and that we missed some of important work about Fisher . We will update the related work to include them ."}, {"review_id": "yT7-k6Q6gda-2", "review_text": "This paper empirically investigates the effect of the trace of the Fisher Information Matrix ( FIM ) early in training has on the generalization of SGD . Authors demonstrate that the effect of optimally chosen learning rate and batch size for SGD can be modeled as an implicit penalty on the trace of FIM . They argue that explicitly penalizing the trace of FIM discourages memorizing noisy labels , thus leading to better generalization . Furthermore , they experimentally show that the early low value of the trace of FIM may bias the optimization towards a flat optimum which has been observed to correlate well with good generalization . * Positives : * + The paper is well-written and easy to follow . Authors convey their message clearly . + The paper 's motivation is definitely relevant . Understanding the connection between the early stage loss landscape of neural networks and final generalization is crucial and has drawn significant attention recently . + The introduced Fisher-penalty is interesting and the authors provide insights into the mechanism how regularizing the trace of FIM might improve generalization . * Negatives : * - The exact mechanism how FP influences learning with noisy labels could be investigated a bit more rigorously . The discussion provided in the paper is not completely clear . - The experiments are somewhat inconclusive in some cases ( see below ) and would require some extra comments/explanation . Overall , the paper has some interesting contribution by demonstrating the positive effect of various gradient norm penalties on the generalization of SGD and it is tied well into existing literature . Furthermore , the experiments are comprehensive and in-depth . Therefore I would recommend acceptance if a couple of issues are addressed . First , as seen in Fig.4 generalization peaks if FP is applied starting after some positive number of epochs . The difference in validation accuracy between turning on FP from the very beginning and only turning it on somewhat later is not always negligible . This observation somewhat violates the 'earlier we regularize the better ' idea . Can the authors comment on this discrepancy ? Second , the benefit of high early regularization is somewhat inconclusive based on Fig.10.It appears that the best generalization belongs to low early regularization , whereas high early regularization leads to better average test accuracy . I would be interested to know how much FP might hinder optimization by discouraging the exploration of certain directions in the loss landscape . Can it even contribute to trapping the network in local optima by penalizing large gradients needed to escape such 'bad ' optima ? Minor comment : there is a typo in the caption of Fig.6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201d I would be interested to know how much FP might hinder optimization by discouraging the exploration of certain directions in the loss landscape . Can it even contribute to trapping the network in local optima by penalizing large gradients needed to escape such 'bad ' optima ? \u201d * * & nbsp ; & nbsp ; & nbsp ; In short , it depends on which examples have large gradient norms if one would resample their labels . These examples will learn slower when Fisher Penalty is applied . There is one specific case in which we could imagine Fisher Penalty ( as well as using a large learning rate ) to trap the model in a bad solution . If the training examples with noisy ( incorrect ) labels are easier to learn than the correctly labeled samples . This can happen for instance when ( say ) all the corrupted samples have the same label . However , the above scenario generally should not happen in practice , at least not in the synthetic noise scenario . In particular , the authors of [ 1 ] show explicitly that examples with incorrect labels on CIFAR-10 tend to have lower gradient norms . As the authors discuss further , it is also supported by the fact that deep neural networks first learn \u2018 clean \u2019 examples , which makes sense if they have larger gradient norms . We will include the discussion above in the paper . Please let us know if you have any further questions about this point . [ 1 ] https : //openreview.net/forum ? id=ryeFY0EFwS * * \u201d The exact mechanism of how FP influences learning with noisy labels could be investigated a bit more rigorously . The discussion provided in the paper is not completely clear. \u201d * * Sorry , it was a bit shortened due to space requirements . We will include a similar discussion as in the answer to your previous question . Please let us know if you have any specific questions . * * \u201c Minor comment : there is a typo in the caption of Fig.6 \u201d * * & nbsp ; & nbsp ; & nbsp ; Thank you ."}, {"review_id": "yT7-k6Q6gda-3", "review_text": "Summary : This paper explores the relationship between the trace of the Fisher Information Matrix ( FIM ) and generalization performance of deep learning models . There are a few core insights which are verified empirically across a number of different models and datasets : ( 1 ) large learning rate/small batch sizes ( which tends to lead to improved generalization ) can be realized as implicitly penalizing the trace of Fisher and ( 2 ) how regularizing the trace of Fisher discourages memorization and guides optimization to `` flatter minima '' . My overall assessment of this paper leans towards positive . While it is probably not surprising to deep learning optimization/generalization researchers that the trace of FIM is closely related to generalization performance , this paper does a good job from the empirical standpoint , the experiments are done in a clean and systematic way to verify the hypotheses . Some comments/remarks/questions : - The manuscript is overall well-written ; I have no problems following the logic and the experimental setups . Related works for the most part are discussed throughly and well-explained . A downside is that there is completely no theoretical arguments given in the paper , I am wondering if it 's possible even in the case of a convex quadratic/extremely simple networks , the authors can show how a small Tr ( F ) leads to a tighter/better generalization bound ? - Just to verify- the Fisher matrix which is computed throughout is the empirical estimate of the true Fisher and not the `` empirical Fisher '' - as the labels are sampled from the predictive distribution ? This often results in a confusion ; see [ 1 ] for more details . - In Section 3 , the authors compared FP with GP which is a fair comparison . I am wondering if the authors compared their approach to even more closely-related methods such as input-output Jacobian regularization [ 2 ] , regularizing by the Frobenius norm of Fisher ; Tr ( F^2 ) instead of Tr ( F ) , and the Fisher-Rao norm which has a tight relationship with capacity/generalization of neural networks as shown in [ 3 ] . References : [ 1 ] Kunstner , Frederik , Philipp Hennig , and Lukas Balles . `` Limitations of the empirical Fisher approximation for natural gradient descent . '' Advances in Neural Information Processing Systems . 2019 . [ 2 ] Novak , Roman , et al . `` Sensitivity and generalization in neural networks : an empirical study . '' arXiv preprint arXiv:1802.08760 ( 2018 ) . [ 3 ] Liang , Tengyuan , et al . `` Fisher-rao metric , geometry , and complexity of neural networks . '' The 22nd International Conference on Artificial Intelligence and Statistics . 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time and the review ! We are particularly happy to hear that you found the presentation and testing of the hypothesis clear . Please let us know if you have any other questions or would like us to run any additional experiments . * * I am wondering if it 's possible even in the case of a convex quadratic/extremely simple networks , the authors can show how a small Tr ( F ) leads to a tighter/better generalization bound ? * * & nbsp ; & nbsp ; & nbsp ; That is a great idea - we will add a discussion about this and connect Tr ( F ) better to existing literature on generalization bounds . In particular , Jiang et al.in in \u201c Fantastic Generalization Measures and Where to Find Them \u201d ( https : //arxiv.org/abs/1912.02178 ) argues that sharpness-based bounds are best correlates of generalization across multiple experiments . To the extent to which sharpness based bounds are sensible bounds on generalization , Tr ( F ) can be argued to lead to tighter generalization bounds . * * \u201d Just to verify- the Fisher matrix which is computed throughout is the empirical estimate of the true Fisher and not the `` empirical Fisher '' - as the labels are sampled from the predictive distribution ? \u201d * * & nbsp ; & nbsp ; & nbsp ; Yes . To approximate the FIM , we sample y from the model prediction p ( y|x ) , and not use the training label . Hence it is not the `` empirical FIM '' mentioned in [ 1 ] , but is rather a Monte Carlo estimate of the actual FIM . This is mentioned below Equation 2 , and more details on the penalty are provided in Appendix C. [ 1 ] Kunstner , Frederik , Philipp Hennig , and Lukas Balles . `` Limitations of the empirical Fisher approximation for natural gradient descent . '' Advances in Neural Information Processing Systems . 2019 . * * \u201d I am wondering if the authors compared their approach to even more closely-related methods such as input-output Jacobian regularization [ 2 ] , regularizing by the Frobenius norm of Fisher ; Tr ( F^2 ) instead of Tr ( F ) , and the Fisher-Rao norm. \u201d * * & nbsp ; & nbsp ; & nbsp ; We indeed included the results for input-output Jacobian regularization along with Fisher penalty ( FP ) in the paper . We call this regularization $ GP_x $ in the paper and the results are shown in Figure 3 and Table 3 . In Figure 3 , we trained on the clean CIFAR-100 dataset with these regularizations using ResNet and VGG architectures . We found that for ResNet both regularizations perform similarly , while for VGG , FP is substantially better than FP . In Table 3 , we experimented with the effect of these regularizations on preventing memorization when training on a corrupted version of the CIFAR-100 training set with different levels of label noise . We found that FP performs substantially better than GP_x in all cases . The same experiments for CIFAR-10 are shown in Figure 8 and Table 4 in the appendix . Overall , we found that FP performs best under the various settings while GP_x performs similarly under some settings but worse under others . Due to the increasing amount of resources needed , we did not try the other variants of Fisher norm , but we will definitely include references to them ."}], "0": {"review_id": "yT7-k6Q6gda-0", "review_text": "After the rebuttal , the authors added a section on large-batch training , which shows that the catastrophic Fisher explosion also occurs in large batch training . This makes the paper more convincing . However , the main concern that this paper lacks theoretical contribution still exists . Therefore , I keep the weak acceptance recommendation . Summary : In this paper , the authors made a study on Fisher Information Matrix ( FIM ) in the training dynamics . By tracking the trace of FIM in the initial training process , the authors show that it can be strongly connected to a model \u2019 s generalization performance . This successfully explains why some hyper-parameter choices lead to significantly better generalization . Finally , empirical evaluations show that penalizing the trace FIM can reduce memorization and encourage wide minima solution , leading to better generalization . Pros : This paper takes one step forward to the question of why some hyper-parameter choices ( small batch size or large learning rate ) lead to better generalization . In many previous works , the implicit regularization effect of stochastic gradient descent ( SGD ) accounts for this observation ( small batch size generalizes better ) . Most of the above works focused on the anisotropic noise in SGD . However , there is still a gap between the anisotropic noise and implicit regularization , i.e.why does anisotropic noise lead to implicit regularization hence better generalization . [ 1 ] explains this in a very general way : it helps to escape local minima . This paper tries to fill in the gap by leveraging FIM . FIM is also studied in the context of SGD regularization [ 2 ] . However , it only shows that the trace of FIM affects optimization performance . How it relates to the generalization performance still remains unknown . Section 4 and section 5 shed light on this question by drawing connection to memorization and final curvatures . Larger scale dataset such as Tiny-Imagenet is included in the experiments . This shows that the conclusion also stands on a more practical and noisy dataset . Cons : Most of the empirical verifications in this paper are comparing large and small learning rates . To be more general , the authors can also include the study of batch size . The central claim in this paper would be more convincing if catastrophic Fisher explosion also occurs in large batch training . More importantly , reducing the trace can improve large batch training generalization . Additionally , the connection between FIM and noisy gradient descent [ 3 ] is worth mentioning . For example , how does the trace of FIM in noisy gradient descent compared to other methods ? One of the postulations is regularizing the trace of FIM reduces memorization . The paper verifies this by introducing noisy labels . This requires modification to the training dataset . Another way to verify this postulation is to make comparisons on out-of-distribution dataset ( CIFAR10-C and CIFAR100-C ) . The method which reduces memorization should be more robust on out-of-distribution datasets . A majority of the analysis comes from empirical evaluations . The claim is more convincing if the conclusion can also be analytically proved in the convex quadratic scenario . The experiments section of this paper is based on the empirical Fisher matrix ( true Fisher matrix is expensive to compute ) . Although empirical Fisher is commonly used in practice to approximate true Fisher , the approximation is only accurate in the local minima [ 4 ] . The paper should have a discussion on the difference between true Fisher and empirical Fisher . Legends are missing in Figure 4 and Figure 6 . [ 1 ] : Zhu , Zhanxing et al. \u201c The Anisotropic Noise in Stochastic Gradient Descent : Its Behavior of Escaping from Sharp Minima and Regularization Effects. \u201d ICML ( 2019 ) . [ 2 ] : Wen , Yeming et al. \u201c An Empirical Study of Stochastic Gradient Descent with Structured Covariance Noise. \u201d AISTATS ( 2020 ) . [ 3 ] : Wu , Jingfeng et al. \u201c On the Noisy Gradient Descent that Generalizes as SGD. \u201d ICML ( 2020 ) [ 4 ] : Kunstner , Frederik et al. \u201c Limitations of the empirical Fisher approximation for natural gradient descent. \u201d NeurIPS ( 2019 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201d One of the postulations is regularizing the trace of FIM reduces memorization . The paper verifies this by introducing noisy labels . This requires modification to the training dataset . Another way to verify this postulation is to make comparisons on the out-of-distribution dataset ( CIFAR10-C and CIFAR100-C ) . The method which reduces memorization should be more robust on out-of-distribution datasets. \u201d * * & nbsp ; & nbsp ; & nbsp ; This is a great idea - we will certainly add this comparison in the final version ( or in this rebuttal phase if we manage to run it before the coming Tuesday ) . * * \u201d The experiments section of this paper is based on the empirical Fisher matrix ( true Fisher matrix is expensive to compute ) . Although empirical Fisher is commonly used in practice to approximate true Fisher , the approximation is only accurate in the local minima [ 4 ] . The paper should have a discussion on the difference between true Fisher and empirical Fisher. \u201d * * & nbsp ; & nbsp ; & nbsp ; We apologize for the confusion ! We do compute the trace of the actual Fisher matrix . To approximate the Fisher Information Matrix , we sample y from the model prediction p ( y|x ) , without using the training labels . Hence it is not the `` empirical FIM '' mentioned in [ 1 ] , but rather a Monte Carlo estimate of the actual FIM . We will clarify this in the paper . * * \u201d A majority of the analysis comes from empirical evaluations . The claim is more convincing if the conclusion can also be analytically proved in the convex quadratic scenario \u201d * * & nbsp ; & nbsp ; & nbsp ; We fully agree . While a theoretical argument would be challenging for us to add at this stage , we will make a more explicit connection to research on generalization bounds . In particular , Jiang et al.in \u201c Fantastic Generalization Measures and Where to Find Them \u201d ( https : //arxiv.org/abs/1912.02178 ) argue that sharpness based bounds are best correlates of generalization across multiple experiments . To the extent to which sharpness-based bounds are sensible bounds on generalization , Tr ( F ) can be argued to lead to tighter generalization bounds and hence better generalization . * * '' Legends are missing in Figure 4 and Figure 6 . `` * * Thank you . We will fix it . * * '' Additionally , the connection between FIM and noisy gradient descent [ 3 ] is worth mentioning . `` * * Thank you . We will add a discussion of [ 1-3 ] to the paper ."}, "1": {"review_id": "yT7-k6Q6gda-1", "review_text": "# Summary This work claims SGD regularizes the model through penalizing the trace of Fisher in the early phase . This claim is supported by the similar generalization behavior of SGD ( with optimal learning rate ) and Fisher penalization ( for SGD with small learning rate ) . A series experiments are conducted to verify the understanding . I find the paper writing is rather misleading . On the one hand , no mathematical justifications , and the logical chain is weak hard to claim which factor is the cause and which one is the effect . On the other hand , though written as `` trace of Fisher '' in the paper , in experiments they compute a different regularizer norm of expected gradient . Detailed questions come in the following . # Questions 1 . Abstract , `` We highlight that in the absence of implicit or explicit regularization ... '' . How do you get rid of the implicit regularization ? ? ? 2.Eq . ( 2 ) is super misleading . Trace of Fisher is the expected gradient norm , but in Eq . ( 2 ) you compute norm of expected gradient . Statistically the latter has nothing to do with the former : one is the second moment , and the other is the squared first moment . Please justify . 3.I have a very simple explanation to your observed phenomenon . In the beginning , gradient is large , thus your version of `` trace of fisher '' is large , as the gradient decreases , the `` trace of fisher '' decreases . Now we look at large learning rate and small learning rate . With large learning rate , SGD converges faster , thus its gradients decrease faster , which causes the `` trace of fisher '' decreases faster . But with small learning rate , SGD converges slower , thus its gradient decrease slower , and the `` trace of fisher '' appears to be large in the beginning epochs . Therefore I am not at all convinced by your arguments . 4.From the above discussion , at least you need to normalize the `` trace of fisher '' by gradient norm which then becomes a measurement of gradient confusion . See following for references . 5.FP/GP.If I understand correctly , you need to compute gradient of gradient , which expands your computation graph at least twice . Can you report the GPU memory consumption ? 6.Finally , let us take about the practical role of FP . According to Table 1 , the improvement of FP over GP is marginal . I can not see a potential of FP . Not only in theory , but also in practice this paper is not satisfactory . # Missing Refs Tons of theory paper should be discussed . A few of them come to my brain are listed in below . Please do a more complete literature investigation . Fisher - Liang , Tengyuan , et al . `` Fisher-rao metric , geometry , and complexity of neural networks . '' The 22nd International Conference on Artificial Intelligence and Statistics . 2019.- Karakida , Ryo , Shotaro Akaho , and Shun-ichi Amari . `` Universal statistics of Fisher information in deep neural networks : Mean field approach . '' The 22nd International Conference on Artificial Intelligence and Statistics . PMLR , 2019 . SGD regularization mechanism - Daneshmand , H. , Kohler , J. , Lucchi , A. , and Hofmann , T. Escaping saddles with stochastic gradients . arXiv preprint arXiv:1803.05999 , 2018 . - Zhu , Zhanxing , et al . `` The Anisotropic Noise in Stochastic Gradient Descent : Its Behavior of Escaping from Sharp Minima and Regularization Effects . '' arXiv preprint arXiv:1803.00195 ( 2018 ) . - Wu , Jingfeng , et al . `` On the Noisy Gradient Descent that Generalizes as SGD . '' arXiv preprint arXiv:1906.07405 ( 2019 ) . Gradient confusion is highly related to the trace Fisher . See this one and its follow-ups . - Sankararaman , Karthik A. , et al . `` The impact of neural network overparameterization on gradient confusion and stochastic gradient descent . '' arXiv preprint arXiv:1904.06963 ( 2019 ) . Adversarial regularization is also related to GP/FP : - Miyato , Takeru , et al . `` Virtual adversarial training : a regularization method for supervised and semi-supervised learning . '' IEEE transactions on pattern analysis and machine intelligence 41.8 ( 2018 ) : 1979-1993 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * \u201c How do you get rid of the implicit regularization ? ? ? \u201d * * & nbsp ; & nbsp ; & nbsp ; In our opinion , one of ( many ) key outstanding questions in deep learning theory is why and how high learning rate or small batch size in SGD impacts generalization . By implicit regularization we were referring to the generalization effect of a reasonably large learning rate . We suppress its regularization effect by using lower values of learning rate ( which we refer to as a sub-optimal learning rate in the paper ) . We apologize if it were not clear , and we will improve background and motivation in the paper . * * '' Finally , let us take about the practical role of FP . According to Table 1 , the improvement of FP over GP is marginal . I can not see a potential of FP . Not only in theory , but also in practice this paper is not satisfactory . `` * * & nbsp ; & nbsp ; & nbsp ; The difference is approximately 2 % for DenseNet/C100 and VGG11/C100 and 4 % on SimpleCNN/C10 . In these cases we would like to argue it is not marginal by the standards of the computer vision community . In the other two ( out of 5 ) cases you are right - they are similar . However , even if they are similar , that \u2019 s actually still exciting . We are primarily excited about the theoretical importance of the finding that we can explain away an important aspect of implicit regularization in SGD . Please note that two other works were concurrently submitted to ICLR that propose only gradient norm regularization as the implicit regularizer [ 1,2 ] . On a practical role of FP , let us also draw your attention to experiments involving noisy labels . In these experiments ( Table 3 ) FP was substantially better than GP_x , and comparable to mixup . Please note that gradient norm regularization hasn \u2019 t been shown to be effective in this setting before . [ 1 ] https : //openreview.net/forum ? id=rq_Qr0c1Hyo [ 2 ] https : //openreview.net/forum ? id=3q5IqUrkcF * * '' From the above discussion , at least you need to normalize the `` trace of fisher '' by gradient norm which then becomes a measurement of gradient confusion . See following for references '' * * & nbsp ; & nbsp ; & nbsp ; Given that we measure the trace of the actual Fisher , not \u201c trace of Fisher \u201d , we assume that this answers this question , but please let us know if not . * * Can you report the GPU memory consumption ? * * & nbsp ; & nbsp ; & nbsp ; That \u2019 s a good point . It is approximately 2-3x larger . We will add an exact measurement to the final version of the paper . * * Tons of theory paper should be discussed * * & nbsp ; & nbsp ; & nbsp ; We agree with the references provided and that we missed some of important work about Fisher . We will update the related work to include them ."}, "2": {"review_id": "yT7-k6Q6gda-2", "review_text": "This paper empirically investigates the effect of the trace of the Fisher Information Matrix ( FIM ) early in training has on the generalization of SGD . Authors demonstrate that the effect of optimally chosen learning rate and batch size for SGD can be modeled as an implicit penalty on the trace of FIM . They argue that explicitly penalizing the trace of FIM discourages memorizing noisy labels , thus leading to better generalization . Furthermore , they experimentally show that the early low value of the trace of FIM may bias the optimization towards a flat optimum which has been observed to correlate well with good generalization . * Positives : * + The paper is well-written and easy to follow . Authors convey their message clearly . + The paper 's motivation is definitely relevant . Understanding the connection between the early stage loss landscape of neural networks and final generalization is crucial and has drawn significant attention recently . + The introduced Fisher-penalty is interesting and the authors provide insights into the mechanism how regularizing the trace of FIM might improve generalization . * Negatives : * - The exact mechanism how FP influences learning with noisy labels could be investigated a bit more rigorously . The discussion provided in the paper is not completely clear . - The experiments are somewhat inconclusive in some cases ( see below ) and would require some extra comments/explanation . Overall , the paper has some interesting contribution by demonstrating the positive effect of various gradient norm penalties on the generalization of SGD and it is tied well into existing literature . Furthermore , the experiments are comprehensive and in-depth . Therefore I would recommend acceptance if a couple of issues are addressed . First , as seen in Fig.4 generalization peaks if FP is applied starting after some positive number of epochs . The difference in validation accuracy between turning on FP from the very beginning and only turning it on somewhat later is not always negligible . This observation somewhat violates the 'earlier we regularize the better ' idea . Can the authors comment on this discrepancy ? Second , the benefit of high early regularization is somewhat inconclusive based on Fig.10.It appears that the best generalization belongs to low early regularization , whereas high early regularization leads to better average test accuracy . I would be interested to know how much FP might hinder optimization by discouraging the exploration of certain directions in the loss landscape . Can it even contribute to trapping the network in local optima by penalizing large gradients needed to escape such 'bad ' optima ? Minor comment : there is a typo in the caption of Fig.6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201d I would be interested to know how much FP might hinder optimization by discouraging the exploration of certain directions in the loss landscape . Can it even contribute to trapping the network in local optima by penalizing large gradients needed to escape such 'bad ' optima ? \u201d * * & nbsp ; & nbsp ; & nbsp ; In short , it depends on which examples have large gradient norms if one would resample their labels . These examples will learn slower when Fisher Penalty is applied . There is one specific case in which we could imagine Fisher Penalty ( as well as using a large learning rate ) to trap the model in a bad solution . If the training examples with noisy ( incorrect ) labels are easier to learn than the correctly labeled samples . This can happen for instance when ( say ) all the corrupted samples have the same label . However , the above scenario generally should not happen in practice , at least not in the synthetic noise scenario . In particular , the authors of [ 1 ] show explicitly that examples with incorrect labels on CIFAR-10 tend to have lower gradient norms . As the authors discuss further , it is also supported by the fact that deep neural networks first learn \u2018 clean \u2019 examples , which makes sense if they have larger gradient norms . We will include the discussion above in the paper . Please let us know if you have any further questions about this point . [ 1 ] https : //openreview.net/forum ? id=ryeFY0EFwS * * \u201d The exact mechanism of how FP influences learning with noisy labels could be investigated a bit more rigorously . The discussion provided in the paper is not completely clear. \u201d * * Sorry , it was a bit shortened due to space requirements . We will include a similar discussion as in the answer to your previous question . Please let us know if you have any specific questions . * * \u201c Minor comment : there is a typo in the caption of Fig.6 \u201d * * & nbsp ; & nbsp ; & nbsp ; Thank you ."}, "3": {"review_id": "yT7-k6Q6gda-3", "review_text": "Summary : This paper explores the relationship between the trace of the Fisher Information Matrix ( FIM ) and generalization performance of deep learning models . There are a few core insights which are verified empirically across a number of different models and datasets : ( 1 ) large learning rate/small batch sizes ( which tends to lead to improved generalization ) can be realized as implicitly penalizing the trace of Fisher and ( 2 ) how regularizing the trace of Fisher discourages memorization and guides optimization to `` flatter minima '' . My overall assessment of this paper leans towards positive . While it is probably not surprising to deep learning optimization/generalization researchers that the trace of FIM is closely related to generalization performance , this paper does a good job from the empirical standpoint , the experiments are done in a clean and systematic way to verify the hypotheses . Some comments/remarks/questions : - The manuscript is overall well-written ; I have no problems following the logic and the experimental setups . Related works for the most part are discussed throughly and well-explained . A downside is that there is completely no theoretical arguments given in the paper , I am wondering if it 's possible even in the case of a convex quadratic/extremely simple networks , the authors can show how a small Tr ( F ) leads to a tighter/better generalization bound ? - Just to verify- the Fisher matrix which is computed throughout is the empirical estimate of the true Fisher and not the `` empirical Fisher '' - as the labels are sampled from the predictive distribution ? This often results in a confusion ; see [ 1 ] for more details . - In Section 3 , the authors compared FP with GP which is a fair comparison . I am wondering if the authors compared their approach to even more closely-related methods such as input-output Jacobian regularization [ 2 ] , regularizing by the Frobenius norm of Fisher ; Tr ( F^2 ) instead of Tr ( F ) , and the Fisher-Rao norm which has a tight relationship with capacity/generalization of neural networks as shown in [ 3 ] . References : [ 1 ] Kunstner , Frederik , Philipp Hennig , and Lukas Balles . `` Limitations of the empirical Fisher approximation for natural gradient descent . '' Advances in Neural Information Processing Systems . 2019 . [ 2 ] Novak , Roman , et al . `` Sensitivity and generalization in neural networks : an empirical study . '' arXiv preprint arXiv:1802.08760 ( 2018 ) . [ 3 ] Liang , Tengyuan , et al . `` Fisher-rao metric , geometry , and complexity of neural networks . '' The 22nd International Conference on Artificial Intelligence and Statistics . 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time and the review ! We are particularly happy to hear that you found the presentation and testing of the hypothesis clear . Please let us know if you have any other questions or would like us to run any additional experiments . * * I am wondering if it 's possible even in the case of a convex quadratic/extremely simple networks , the authors can show how a small Tr ( F ) leads to a tighter/better generalization bound ? * * & nbsp ; & nbsp ; & nbsp ; That is a great idea - we will add a discussion about this and connect Tr ( F ) better to existing literature on generalization bounds . In particular , Jiang et al.in in \u201c Fantastic Generalization Measures and Where to Find Them \u201d ( https : //arxiv.org/abs/1912.02178 ) argues that sharpness-based bounds are best correlates of generalization across multiple experiments . To the extent to which sharpness based bounds are sensible bounds on generalization , Tr ( F ) can be argued to lead to tighter generalization bounds . * * \u201d Just to verify- the Fisher matrix which is computed throughout is the empirical estimate of the true Fisher and not the `` empirical Fisher '' - as the labels are sampled from the predictive distribution ? \u201d * * & nbsp ; & nbsp ; & nbsp ; Yes . To approximate the FIM , we sample y from the model prediction p ( y|x ) , and not use the training label . Hence it is not the `` empirical FIM '' mentioned in [ 1 ] , but is rather a Monte Carlo estimate of the actual FIM . This is mentioned below Equation 2 , and more details on the penalty are provided in Appendix C. [ 1 ] Kunstner , Frederik , Philipp Hennig , and Lukas Balles . `` Limitations of the empirical Fisher approximation for natural gradient descent . '' Advances in Neural Information Processing Systems . 2019 . * * \u201d I am wondering if the authors compared their approach to even more closely-related methods such as input-output Jacobian regularization [ 2 ] , regularizing by the Frobenius norm of Fisher ; Tr ( F^2 ) instead of Tr ( F ) , and the Fisher-Rao norm. \u201d * * & nbsp ; & nbsp ; & nbsp ; We indeed included the results for input-output Jacobian regularization along with Fisher penalty ( FP ) in the paper . We call this regularization $ GP_x $ in the paper and the results are shown in Figure 3 and Table 3 . In Figure 3 , we trained on the clean CIFAR-100 dataset with these regularizations using ResNet and VGG architectures . We found that for ResNet both regularizations perform similarly , while for VGG , FP is substantially better than FP . In Table 3 , we experimented with the effect of these regularizations on preventing memorization when training on a corrupted version of the CIFAR-100 training set with different levels of label noise . We found that FP performs substantially better than GP_x in all cases . The same experiments for CIFAR-10 are shown in Figure 8 and Table 4 in the appendix . Overall , we found that FP performs best under the various settings while GP_x performs similarly under some settings but worse under others . Due to the increasing amount of resources needed , we did not try the other variants of Fisher norm , but we will definitely include references to them ."}}