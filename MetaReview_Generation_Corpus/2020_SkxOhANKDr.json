{"year": "2020", "forum": "SkxOhANKDr", "title": "Generative Cleaning Networks with Quantized Nonlinear Transform  for  Deep Neural Network Defense", "decision": "Reject", "meta_review": "This paper presents a method to defend neural networks from adversarial attack. The proposed generative cleaning network has a trainable quantization module which is claimed to be able to eliminate adversarial noise and recover the original image. \nAfter the intensive interaction with authors and discussion, one expert reviewer (R3) admitted that the experimental procedure basically makes sense and increased the score to Weak Reject. Yet, R3 is still not satisfied with some details such as the number of BPDA iterations, and more importantly, concludes that the meaningful numbers reported in the paper show only small gains, making the claim of the paper less convincing. As authors seem to have less interest in providing theoretical analysis and support, this issue is critical for decision, and there was no objection from other reviewers. After carefully reading the paper myself, I decided to support the opinion and therefore would like to recommend rejection. \n", "reviews": [{"review_id": "SkxOhANKDr-0", "review_text": "This paper proposes a method for adversarial defense based on generative cleaning. The paper does not follow any of the best practices for evaluating adversarial robustness, e.g. in these two papers: \"On Evaluating Adversarial Robustness\" https://arxiv.org/abs/1902.06705 \"Obfuscated Gradients Give a False Sense of Security\" https://arxiv.org/abs/1802.00420 For instance the paper does not use a large number of PGD iterations (10 is too small) and does not check that accuracies go to zero for large epsilon (an important sanity check to reveal gradient masking). In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes. In the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected.", "rating": "1: Reject", "reply_text": "As we can see from our paper , we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer . Following existing papers recently published in ICLR/CVPR/ICCV/ECCV 2017-2019 , we used the advTorch standard package to generate all attacks on our method . Figure 3 shows the large number of BPDA iterations . Due to the page limitation , we did not include the figure for larger number of PGD iterations , since BPDA is a more powerful attack method than PGD . We could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations . Following recently published paper , we have included the most comprehensive performance comparison results in the paper . We have demonstrated that our method significantly outperforms existing state-of-the-art methods ."}, {"review_id": "SkxOhANKDr-1", "review_text": "This paper proposes a new method to defend a neural network agains adversarial attacks (both white-box and black-box attacks). By jointly training a Generative Cleaning Network with quantized nonlinear transform, and a Detector Network, the proposed cleans the incoming attacked image and correctly classifies its true label. The authors use state-of-the-art attack methods on various models, and the proposed model consistently outperforms all baseline models, even dramatically outperforming them for some specific attack methods. Comment: Is there a reason the authors did not test the same set of attack methods for SVHN as they did for CIFAR-10?", "rating": "8: Accept", "reply_text": "We sincerely thank the Reviewer for the positive feedback and high recommendation of our paper ! Thanks for pointing out this ! In this paper , we use the benchmark datasets and compare our results against the results published in existing papers . Unfortunately , not all published papers provided complete results for both of these datasets . For example , some papers did not have results on SVHN . In this case , we left them empty . We chose not to re-implement existing methods since it will be very hard to re-produce their exact results , which might lead to unfair performance comparisons . During our experiments , we tried our best to compare with as many methods as possible if they have published results on the dataset ."}, {"review_id": "SkxOhANKDr-2", "review_text": "This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. The experimental results demonstrated that the proposed approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. A few comments: 1. It does not provide theoretical reasons why the prosed method can defend against those attacks. 2. The experiments are a bit messy and the attacks' setup need to improve. 3. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees. ", "rating": "3: Weak Reject", "reply_text": "We sincerely thank Reviewer 1 for the positive and encouraging comments ! The reviewer suggests that we need to provide theoretical analysis or proof why the proposed deep neural network defense method is working . We must admit that this is really hard . The deep learning research community is still working very hard to establish theoretical analysis or proof for deep neural networks , which is however is very challenging . However , our proposed method is based on data-driven deep learning . All the defense networks are trained based on the loss functions . So , if the loss function converges , the proposed method is achieving the target performance . Following existing state-of-the-art methods published in recent ICLR/CVPR/ICCV/ECCV papers , we use the benchmark their datasets and standard evaluation protocols to demonstrate that our proposed method is significantly outperforming existing methods . For the experiments , we have results on two datasets , CIFAR-10 and SVHN with two different attack modes : white-box attack and black-box attacks . Sorry for the confusion . We will better organize these experimental results . For the attacks , we follow the standard procedure used in all existing methods . Specifically , we use advTorch evaluation package to generate all attacks ."}], "0": {"review_id": "SkxOhANKDr-0", "review_text": "This paper proposes a method for adversarial defense based on generative cleaning. The paper does not follow any of the best practices for evaluating adversarial robustness, e.g. in these two papers: \"On Evaluating Adversarial Robustness\" https://arxiv.org/abs/1902.06705 \"Obfuscated Gradients Give a False Sense of Security\" https://arxiv.org/abs/1802.00420 For instance the paper does not use a large number of PGD iterations (10 is too small) and does not check that accuracies go to zero for large epsilon (an important sanity check to reveal gradient masking). In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes. In the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected.", "rating": "1: Reject", "reply_text": "As we can see from our paper , we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer . Following existing papers recently published in ICLR/CVPR/ICCV/ECCV 2017-2019 , we used the advTorch standard package to generate all attacks on our method . Figure 3 shows the large number of BPDA iterations . Due to the page limitation , we did not include the figure for larger number of PGD iterations , since BPDA is a more powerful attack method than PGD . We could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations . Following recently published paper , we have included the most comprehensive performance comparison results in the paper . We have demonstrated that our method significantly outperforms existing state-of-the-art methods ."}, "1": {"review_id": "SkxOhANKDr-1", "review_text": "This paper proposes a new method to defend a neural network agains adversarial attacks (both white-box and black-box attacks). By jointly training a Generative Cleaning Network with quantized nonlinear transform, and a Detector Network, the proposed cleans the incoming attacked image and correctly classifies its true label. The authors use state-of-the-art attack methods on various models, and the proposed model consistently outperforms all baseline models, even dramatically outperforming them for some specific attack methods. Comment: Is there a reason the authors did not test the same set of attack methods for SVHN as they did for CIFAR-10?", "rating": "8: Accept", "reply_text": "We sincerely thank the Reviewer for the positive feedback and high recommendation of our paper ! Thanks for pointing out this ! In this paper , we use the benchmark datasets and compare our results against the results published in existing papers . Unfortunately , not all published papers provided complete results for both of these datasets . For example , some papers did not have results on SVHN . In this case , we left them empty . We chose not to re-implement existing methods since it will be very hard to re-produce their exact results , which might lead to unfair performance comparisons . During our experiments , we tried our best to compare with as many methods as possible if they have published results on the dataset ."}, "2": {"review_id": "SkxOhANKDr-2", "review_text": "This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. The experimental results demonstrated that the proposed approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. A few comments: 1. It does not provide theoretical reasons why the prosed method can defend against those attacks. 2. The experiments are a bit messy and the attacks' setup need to improve. 3. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees. ", "rating": "3: Weak Reject", "reply_text": "We sincerely thank Reviewer 1 for the positive and encouraging comments ! The reviewer suggests that we need to provide theoretical analysis or proof why the proposed deep neural network defense method is working . We must admit that this is really hard . The deep learning research community is still working very hard to establish theoretical analysis or proof for deep neural networks , which is however is very challenging . However , our proposed method is based on data-driven deep learning . All the defense networks are trained based on the loss functions . So , if the loss function converges , the proposed method is achieving the target performance . Following existing state-of-the-art methods published in recent ICLR/CVPR/ICCV/ECCV papers , we use the benchmark their datasets and standard evaluation protocols to demonstrate that our proposed method is significantly outperforming existing methods . For the experiments , we have results on two datasets , CIFAR-10 and SVHN with two different attack modes : white-box attack and black-box attacks . Sorry for the confusion . We will better organize these experimental results . For the attacks , we follow the standard procedure used in all existing methods . Specifically , we use advTorch evaluation package to generate all attacks ."}}