{"year": "2018", "forum": "HkMvEOlAb", "title": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization", "decision": "Accept (Poster)", "meta_review": "The reviewers concerns regarding novelty and the experimental evaluation have been resolved accordingly and all recommend acceptance. I would recommend removing the term \"unsupervised\" in clustering, as it is redundant. Clustering is, by default, assumed to be unsupervised.\n\nThere is some interest in extending this to non-vision domains, however this is beyond the scope of the current work.", "reviews": [{"review_id": "HkMvEOlAb-0", "review_text": "This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. Pipeline: -Data are augmented with domain-specific transformations. For instance, in the case of MNIST, rotations with different degrees are applied. All data are then labelled as \"original\" or \"transformed by ...(specific transformation)\". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels. -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion. -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer. Detailed Comments: (*) Pros -The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST. -Use of ACOL and GAR is interesting, also the idea to make \"labeled\" data from unlabelled ones by using data augmentation. (*) Cons -minor: in the title, I find the expression \"unsupervised clustering\" uselessly redundant since clustering is by definition unsupervised. -Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits. This is not a very challenging task. And just because something works on MNIST, does not mean it works in general. What are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)? -This is not clear what is novel here since ACOL and GAR already exist. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. My main problem was about the lack of novelty. The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv. The other issue concerned the validation of the approach on databases other than MNIST. The author also addressed this point, and I changed my scores accordingly. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We \u2019 d like to thank the reviewer for their time spending reviewing the paper and their valuable feedback and comments . We wanted to address the comments where we thought the reviewer could use some clarification especially in regards to the novelty of the approach . In response to choice of wording in title : We agree with the reviewer that the word unsupervised can be removed from the title while preserving the same meaning . In response to the choice of datasets : We believe the state-of-the-art performance of the proposed approach on the SVHN dataset may not have been properly emphasized in the article . In this paper , we used three image datasets ( not just MNIST as suggested by the reviewer ) MNIST , USPS and SVHN for comparison chosen primarily for uniformity and clarity between this article and many other recent ones published in conferences just like this one , ICLR , ICML , NIPS etc . MNIST and USPS datasets ( hand-written digits ) might be seen as simple datasets since the existing methods in the literature of clustering have already achieved very good performances on these two datasets . However , they are still used very commonly for benchmarks , especially on significantly different approaches such as the one proposed here . More importantly , unlike semi-supervised and supervised settings , SVHN ( a more realistic dataset with colored street view house numbers ) still constitutes a very challenging task for unsupervised settings . This difficulty might be hard to observe in the first version of our paper as at the time of submission for ICLR 2018 we weren \u2019 t able to find any other work studying this dataset . Thanks to one of the commenters on the paper , we looked at IMSAT [ 1 ] as the previous state-of-art approach for clustering on SVHN , which was very recently published in ICML 2017 ( a month before the submission deadline for ICLR \u2013 a reason why it wasn \u2019 t included in the first version ) . They have also presented the performances of other approaches , such as DEC [ 2 ] , on the challenging SVHN dataset . Please see the clustering performances of two approaches reported by IMSAT [ 1 ] compared to the proposed approach below : DEC : 11.9 % ( \u00b10.40 ) IMSAT : 57.3 % ( \u00b13.90 ) Our Approach : 76.8 % ( \u00b11.30 ) We will include this new article and believe this comparison would further reinforce the state-of-the-art capability and accurateness of our approach . Finally , to the best of our knowledge , clustering on the datasets such as CIFAR-10 , labelMe , ImageNet based on their raw pixel values is not a very common practice in the literature of clustering as raw pixels are not suited for this goal with color information being dominant [ 1 ] . Existing approaches perform the clustering on the extracted features from these datasets . However , this approach doesn \u2019 t fit the proposed clustering technique in this paper , because transformations generating the pseudo classes are domain-specific and so they are directly applied on the input space . Therefore , generalizing the proposed clustering technique to these datasets requires an orthogonal challenge which we already identified as future work to study how to apply these domain-specific transformations that will present rich information for the clustering task at hand . In response to the comments about novelty : We don \u2019 t think this comment \u2013 quote \u201c This is not clear what is novel here since ACOL and GAR already exist . The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not \u201d reflects the reality as both methods ACOL and GAR are completely novel and this paper , if chosen for publication , will be the very first time they appear in peer-reviewed literature ( a major reason why we chose ICLR ) . Their adaptation to unsupervised settings \u2013 is also completely novel by extension with domain specific transformation a key factor in clustering performance . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- [ 1 ] : Learning Discrete Representations via Information Maximizing Self-Augmented Training , author = { Weihua Hu , Takeru Miyato , Seiya Tokui , Eiichi Matsumoto , Masashi Sugiyama } , booktitle = { Proceedings of the 34th International Conference on Machine Learning , { ICML } 2017 , Sydney , NSW , Australia , 6-11 August 2017 } [ 2 ] : Unsupervised Deep Embedding for Clustering Analysis } , author = { Junyuan Xie , Ross B. Girshick , Ali Farhadi } , booktitle = { Proceedings of the 33nd International Conference on Machine Learning , { ICML } 2016 , New York City , NY , USA , June 19-24 , 2016 }"}, {"review_id": "HkMvEOlAb-1", "review_text": "This paper utilizes ACOL algorithm for unsupervised learning. ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9). Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. The authors use multiple data sets to study different aspects of the proposed approach. I updated my scores based on the reviewers responses. It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv! Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR). So my main problem with this paper, lack of novelty, is addressed and my score has changed. Thanks to the reviewer for clarifying this. ", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to thank the reviewer for their time spent reviewing the paper and their valuable feedback and comments . We wanted to address the two specific comments on the approach being incremental and the number of datasets . Both methods described in the article , ACOL and GAR are completely novel and this paper , if chosen for publication , will be the very first time they appear in peer-reviewed literature ( a major reason why we chose ICLR ) . Their adaptation to unsupervised settings \u2013 is also completely novel by extension with domain specific transformation a key factor in clustering performance . In this paper , we have actually used three ( not one as suggested by the reviewer ) image datasets MNIST , USPS and SVHN for comparison chosen primarily for uniformity and clarity between this article and many other recent ones published in conferences just like this one , ICLR , ICML , NIPS etc . MNIST and USPS datasets ( hand-written digits ) might be seen as simple datasets since the existing methods in the literature of clustering have already achieved very good performances on these two datasets . However , they are still used very commonly for benchmarks , especially on significantly different approaches such as the one proposed here . More importantly , unlike semi-supervised and supervised settings , SVHN ( a more realistic dataset with colored street view house numbers ) still constitutes a very challenging task for unsupervised settings . This difficulty might be hard to observe in the first version of our paper as at the time of submission for ICLR 2018 we weren \u2019 t able to find any other work studying this dataset . Thanks to one of the commenters on the paper , we looked at IMSAT [ 1 ] as the previous state-of-art approach for clustering on SVHN , which was very recently published in ICML 2017 ( a month before the submission deadline for ICLR \u2013 a reason why it wasn \u2019 t included in the first version ) . They have also presented the performances of other approaches , such as DEC [ 2 ] , on the challenging SVHN dataset . Please see the clustering performances of two approaches reported by IMSAT [ 1 ] compared to the proposed approach below : DEC : 11.9 % ( \u00b10.40 ) IMSAT : 57.3 % ( \u00b13.90 ) Our Approach : 76.8 % ( \u00b11.30 ) We will include this new article and believe this comparison would further reinforce the state-of-the-art capability and accurateness of our approach on a multitude of datasets . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- [ 1 ] : Learning Discrete Representations via Information Maximizing Self-Augmented Training , author = { Weihua Hu , Takeru Miyato , Seiya Tokui , Eiichi Matsumoto , Masashi Sugiyama } , booktitle = { Proceedings of the 34th International Conference on Machine Learning , { ICML } 2017 , Sydney , NSW , Australia , 6-11 August 2017 } [ 2 ] : Unsupervised Deep Embedding for Clustering Analysis } , author = { Junyuan Xie , Ross B. Girshick , Ali Farhadi } , booktitle = { Proceedings of the 33nd International Conference on Machine Learning , { ICML } 2016 , New York City , NY , USA , June 19-24 , 2016 }"}, {"review_id": "HkMvEOlAb-2", "review_text": "The paper is well written and clear. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task. The idea is to introduce the notion of pseudo labelling. Pseudo labelling can be obtained by transformations of original input data. The key point is the definition of the transformations. Only whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task. Since it is not known in advance what might be a good set of transformations, it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters.", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to thank the reviewer for their encouraging remarks and feedback . The reviewer makes an excellent point on the impact of transformations in clustering accuracy \u2013 specifically for datasets where domain expertise is not readily available . We believe that for image clustering problems , the focus of this article , the proposed domain transformations work sufficiently well ( state-of-the-art ) based on comparative results with the recent literature on these datasets as laid out in the article . For other domains , the exploration of the effects and optimality of transformations represents the most immediate and honestly , exciting future work which will be addressed in subsequent articles as discussed in the final section ."}], "0": {"review_id": "HkMvEOlAb-0", "review_text": "This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. Pipeline: -Data are augmented with domain-specific transformations. For instance, in the case of MNIST, rotations with different degrees are applied. All data are then labelled as \"original\" or \"transformed by ...(specific transformation)\". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels. -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion. -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer. Detailed Comments: (*) Pros -The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST. -Use of ACOL and GAR is interesting, also the idea to make \"labeled\" data from unlabelled ones by using data augmentation. (*) Cons -minor: in the title, I find the expression \"unsupervised clustering\" uselessly redundant since clustering is by definition unsupervised. -Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits. This is not a very challenging task. And just because something works on MNIST, does not mean it works in general. What are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)? -This is not clear what is novel here since ACOL and GAR already exist. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. My main problem was about the lack of novelty. The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv. The other issue concerned the validation of the approach on databases other than MNIST. The author also addressed this point, and I changed my scores accordingly. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We \u2019 d like to thank the reviewer for their time spending reviewing the paper and their valuable feedback and comments . We wanted to address the comments where we thought the reviewer could use some clarification especially in regards to the novelty of the approach . In response to choice of wording in title : We agree with the reviewer that the word unsupervised can be removed from the title while preserving the same meaning . In response to the choice of datasets : We believe the state-of-the-art performance of the proposed approach on the SVHN dataset may not have been properly emphasized in the article . In this paper , we used three image datasets ( not just MNIST as suggested by the reviewer ) MNIST , USPS and SVHN for comparison chosen primarily for uniformity and clarity between this article and many other recent ones published in conferences just like this one , ICLR , ICML , NIPS etc . MNIST and USPS datasets ( hand-written digits ) might be seen as simple datasets since the existing methods in the literature of clustering have already achieved very good performances on these two datasets . However , they are still used very commonly for benchmarks , especially on significantly different approaches such as the one proposed here . More importantly , unlike semi-supervised and supervised settings , SVHN ( a more realistic dataset with colored street view house numbers ) still constitutes a very challenging task for unsupervised settings . This difficulty might be hard to observe in the first version of our paper as at the time of submission for ICLR 2018 we weren \u2019 t able to find any other work studying this dataset . Thanks to one of the commenters on the paper , we looked at IMSAT [ 1 ] as the previous state-of-art approach for clustering on SVHN , which was very recently published in ICML 2017 ( a month before the submission deadline for ICLR \u2013 a reason why it wasn \u2019 t included in the first version ) . They have also presented the performances of other approaches , such as DEC [ 2 ] , on the challenging SVHN dataset . Please see the clustering performances of two approaches reported by IMSAT [ 1 ] compared to the proposed approach below : DEC : 11.9 % ( \u00b10.40 ) IMSAT : 57.3 % ( \u00b13.90 ) Our Approach : 76.8 % ( \u00b11.30 ) We will include this new article and believe this comparison would further reinforce the state-of-the-art capability and accurateness of our approach . Finally , to the best of our knowledge , clustering on the datasets such as CIFAR-10 , labelMe , ImageNet based on their raw pixel values is not a very common practice in the literature of clustering as raw pixels are not suited for this goal with color information being dominant [ 1 ] . Existing approaches perform the clustering on the extracted features from these datasets . However , this approach doesn \u2019 t fit the proposed clustering technique in this paper , because transformations generating the pseudo classes are domain-specific and so they are directly applied on the input space . Therefore , generalizing the proposed clustering technique to these datasets requires an orthogonal challenge which we already identified as future work to study how to apply these domain-specific transformations that will present rich information for the clustering task at hand . In response to the comments about novelty : We don \u2019 t think this comment \u2013 quote \u201c This is not clear what is novel here since ACOL and GAR already exist . The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not \u201d reflects the reality as both methods ACOL and GAR are completely novel and this paper , if chosen for publication , will be the very first time they appear in peer-reviewed literature ( a major reason why we chose ICLR ) . Their adaptation to unsupervised settings \u2013 is also completely novel by extension with domain specific transformation a key factor in clustering performance . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- [ 1 ] : Learning Discrete Representations via Information Maximizing Self-Augmented Training , author = { Weihua Hu , Takeru Miyato , Seiya Tokui , Eiichi Matsumoto , Masashi Sugiyama } , booktitle = { Proceedings of the 34th International Conference on Machine Learning , { ICML } 2017 , Sydney , NSW , Australia , 6-11 August 2017 } [ 2 ] : Unsupervised Deep Embedding for Clustering Analysis } , author = { Junyuan Xie , Ross B. Girshick , Ali Farhadi } , booktitle = { Proceedings of the 33nd International Conference on Machine Learning , { ICML } 2016 , New York City , NY , USA , June 19-24 , 2016 }"}, "1": {"review_id": "HkMvEOlAb-1", "review_text": "This paper utilizes ACOL algorithm for unsupervised learning. ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9). Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. The authors use multiple data sets to study different aspects of the proposed approach. I updated my scores based on the reviewers responses. It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv! Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR). So my main problem with this paper, lack of novelty, is addressed and my score has changed. Thanks to the reviewer for clarifying this. ", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to thank the reviewer for their time spent reviewing the paper and their valuable feedback and comments . We wanted to address the two specific comments on the approach being incremental and the number of datasets . Both methods described in the article , ACOL and GAR are completely novel and this paper , if chosen for publication , will be the very first time they appear in peer-reviewed literature ( a major reason why we chose ICLR ) . Their adaptation to unsupervised settings \u2013 is also completely novel by extension with domain specific transformation a key factor in clustering performance . In this paper , we have actually used three ( not one as suggested by the reviewer ) image datasets MNIST , USPS and SVHN for comparison chosen primarily for uniformity and clarity between this article and many other recent ones published in conferences just like this one , ICLR , ICML , NIPS etc . MNIST and USPS datasets ( hand-written digits ) might be seen as simple datasets since the existing methods in the literature of clustering have already achieved very good performances on these two datasets . However , they are still used very commonly for benchmarks , especially on significantly different approaches such as the one proposed here . More importantly , unlike semi-supervised and supervised settings , SVHN ( a more realistic dataset with colored street view house numbers ) still constitutes a very challenging task for unsupervised settings . This difficulty might be hard to observe in the first version of our paper as at the time of submission for ICLR 2018 we weren \u2019 t able to find any other work studying this dataset . Thanks to one of the commenters on the paper , we looked at IMSAT [ 1 ] as the previous state-of-art approach for clustering on SVHN , which was very recently published in ICML 2017 ( a month before the submission deadline for ICLR \u2013 a reason why it wasn \u2019 t included in the first version ) . They have also presented the performances of other approaches , such as DEC [ 2 ] , on the challenging SVHN dataset . Please see the clustering performances of two approaches reported by IMSAT [ 1 ] compared to the proposed approach below : DEC : 11.9 % ( \u00b10.40 ) IMSAT : 57.3 % ( \u00b13.90 ) Our Approach : 76.8 % ( \u00b11.30 ) We will include this new article and believe this comparison would further reinforce the state-of-the-art capability and accurateness of our approach on a multitude of datasets . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- [ 1 ] : Learning Discrete Representations via Information Maximizing Self-Augmented Training , author = { Weihua Hu , Takeru Miyato , Seiya Tokui , Eiichi Matsumoto , Masashi Sugiyama } , booktitle = { Proceedings of the 34th International Conference on Machine Learning , { ICML } 2017 , Sydney , NSW , Australia , 6-11 August 2017 } [ 2 ] : Unsupervised Deep Embedding for Clustering Analysis } , author = { Junyuan Xie , Ross B. Girshick , Ali Farhadi } , booktitle = { Proceedings of the 33nd International Conference on Machine Learning , { ICML } 2016 , New York City , NY , USA , June 19-24 , 2016 }"}, "2": {"review_id": "HkMvEOlAb-2", "review_text": "The paper is well written and clear. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task. The idea is to introduce the notion of pseudo labelling. Pseudo labelling can be obtained by transformations of original input data. The key point is the definition of the transformations. Only whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task. Since it is not known in advance what might be a good set of transformations, it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters.", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to thank the reviewer for their encouraging remarks and feedback . The reviewer makes an excellent point on the impact of transformations in clustering accuracy \u2013 specifically for datasets where domain expertise is not readily available . We believe that for image clustering problems , the focus of this article , the proposed domain transformations work sufficiently well ( state-of-the-art ) based on comparative results with the recent literature on these datasets as laid out in the article . For other domains , the exploration of the effects and optimality of transformations represents the most immediate and honestly , exciting future work which will be addressed in subsequent articles as discussed in the final section ."}}