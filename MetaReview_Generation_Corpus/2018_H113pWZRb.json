{"year": "2018", "forum": "H113pWZRb", "title": "Topology Adaptive Graph Convolutional  Networks", "decision": "Reject", "meta_review": "The authors provide an extension to GCNs of Kipf and Welling in order to incorporate information about higher order neighborhoods. The extension is well motivated (and  though I agree that it is not trivial modification of the K&W approach to the second order,  thanks to the authors for the clarification).  The improvements are relatively moderate.\n\nPros:\n-- The approach is well motivated\n-- The paper is clearly written\nCons:\n-- The originality and impact (as well as motivation) are questioned by the reviewers\n", "reviews": [{"review_id": "H113pWZRb-0", "review_text": "The authors propose a new CNN approach to graph classification that generalizes previous work. Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed. This incorporates information from more distant vertices in one propagation step. The proposed idea is not exceptional original, but the paper has several strong points: * The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one. * The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level. * The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach). * The experimental evaluation appears to be sound. A moderate improvement compared to other approaches is observed for all data sets. In summary, I think the paper can be accepted for ICLR. ----------- EDIT ----------- After reading the publications mentioned by the other reviewers as well as the following related contributions * Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018) * Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916) I agree that the relation to previous work is not adequately outlined. Therefore I have modified my rating accordingly.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive comments . 1 ) We agree with the reviewer that our method is able to leverage information at a farther distance on the graph than the GCN of Kipf & Welling 2017 . However , ours is not a simple generalization of GCN . In fact , extending GCN to the second order would not lead to our results . We clarify the fundamental difference between our method and the GCN methodology if we extended the latter to a higher order in the separate comment with title \u201c Differences between the proposed TAGCN and GCN in Kipf & Welling 2017 \u201d due to space limitation . Thank you for your attention . We have added the corresponding discussion in Section 3 of the revised version . 2 ) `` After reading the publications mentioned by the other reviewers as well as the following related contributions * Network of Graph Convolutional Networks Trained on Random Walks ( under review for ICLR 2018 ) * Graph Convolution : A High-Order and Adaptive Approach , Zhenpeng Zhou , Xiaocheng Li ( arXiv:1706.09916 ) I agree that the relation to previous work is not adequately outlined . Therefore I have modified my rating accordingly . '' We thank the reviewer for pointing out these two recent works on graph CNN . We would like to point out that our method is substantially different from these two papers . The graph convolutions in these two papers are defined based on ad-hoc methods , which do not have the physical meaning of convolution . The first paper concatenates A^k with k from 0 to 6 , and the second paper defines { \\tilde A } ^k = min { A^k + I,1 } . In contrast , our definition of convolution is based on graph signal processing , it is consistent with the convolutional theorem , and , finally , it reduces to classical convolution for the direct circle topology . The first paper reports their top 3 performers rather than reporting average performance , while we report an averaged performance over 100 trails . Our performance for Pubmed is still better than theirs even if the comparison is unfair . The second paper did not follow the usual data splitting method and so we can not compare ours to their performance directly . We respectfully disagree with the comment that these two papers are not adequately outlined in the original submission . The first paper is submitted to the same conference ICLR2018 as our paper , so , at the same time \u2013 how could we have access to it before hand ? Thus , there is no way we could refer to this paper in our original submission . The second paper appeared in arXiv with the title \u201c Graph Convolutional Networks for Molecules , \u201d which was specific to molecules with content that was quite different from its second version . The second version was submitted to arXiv on Oct , 20 , becoming only available to the public almost at the same time as we submitted our paper to ICLR2018 . Further , there is a major revision between these two versions as we can see on arXiv , and the number of pages increased from less than 5 pages and a half to 8 pages . We thank the reviewer for finding this paper for us . We also want to mention that , besides providing a solid foundation for our proposed the graph convolution operation , our method also exhibits better performance due to the fact that no approximation is needed for the convolution operation . Our method outperforms all recently proposed methods on all three datasets . In addition , for the Pubmed dataset , which is much larger than the Citeseer and Cora data sets , we have a 2.1 % improvement over GCN ( Kipf & Welling 2017 ) and 6.7 % improvement over ChebNet ( Defferrard et al.2016 ) .These performance results are averages obtained over 100 Monte Carlo runs . As far as we know and as far as we can determine , our method exhibits the best performance on the Pubmed data not only when compared with all previous available publications , as well as when compared with all papers submitted to ICLR18 , see papers below . Also , please note that , as explained by the authors , the last paper listed below fails with the Pubmed data set because of its storage complexity . Graph Partition Neural Networks for Semi-Supervised Classification , submitted to ICLR18 Attention-based Graph Neural Network for Semi-supervised Learning , submitted to ICLR18 Stochastic Training of Graph Convolutional Networks , submitted to ICLR18 Graph Attention Networks , submitted to ICLR18"}, {"review_id": "H113pWZRb-1", "review_text": "In this paper a new neural network architecture for semi-supervised graph classification is proposed. The new construction builds upon graph polynomial filters and utilizes them on each successive layer of the neural network with ReLU activation functions. In my opinion writing of this paper requires major revision. The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora, Pubmed and Citeseer benchmarks. The one layer analysis in sections 2.1, 2.2 and 2.3 is simply an explanation of graph polynomial filters, which were previously proposed and analyzed in cited work of Sandryhaila and Moura (2013). Together with the summary of other methods and introduction, it composes the first 8 pages of the paper. I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader. I also recommend stating which ideas came from the Sandryhaila and Moura (2013) work in a more pronounced manner. Next, I disagree with the statement that \"it is not clear how to keep the vertex local property when filtering in the spectrum domain\". Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain. See Eq. 18 and 19 in [1]. Finally, I should say that TAGCN idea is interesting. I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e. first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate). With more experiments and interpretation of the model, including some sort of multilayer analysis, this can be a good acceptance candidate. [1] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83\u201398, 2013.", "rating": "4: Ok but not good enough - rejection", "reply_text": "1 ) `` In my opinion writing of this paper requires major revision . The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora , Pubmed and Citeseer benchmarks. `` Reply : We have reorganized the paper and added more insights for the proposed TAGCN algorithm . We explain our proposed method in Section 2 and compare it with previous work in Section 3 to emphasize the novelty and differences of our method . We want to emphasize that the adjacency matrix polynomial filter ( graph convolution operation ) defined on the vertex domain ( our method ) is totally different from all the existing graph CNN methods available and that define the convolution in the spectrum domain . Thus , our proposed convolution , its computational complexity , and understanding of the choice of the filter size , all need adequate explanations , and these are given in Section 3 . Even some of the reviewers seem to misunderstand the GCN method in Kipf & Welling 2017 based on approximations by matrix Chebyshev polynomials in Defferrard et al.2016 with our method . Thus explaining adequately our method from different perspectives is necessary . We have further described these relationships in Section 3 in the revised version and made the architecture of our method clearer . As for performance , our method outperforms all recently proposed methods on all three datasets . In addition , for the Pubmed dataset , which is much larger than the Citeseer and Cora data sets , we have a 2.1 % improvement over GCN ( Kipf & Welling 2017 ) and 6.7 % improvement over ChebNet ( Defferrard et al.2016 ) .These performance results are averages obtained over 100 Monte Carlo runs . As far as we know and as far as we can determine , our method exhibits the best performance on the Pubmed data not only when compared with all previous available publications , as well as when compared with all papers submitted to ICLR18 , see papers below . Also , please note that , as explained by the authors , the last paper listed below fails with the Pubmed data set because of its storage complexity . Graph Partition Neural Networks for Semi-Supervised Classification , submitted to ICLR18 Attention-based Graph Neural Network for Semi-supervised Learning , submitted to ICLR18 Stochastic Training of Graph Convolutional Networks , submitted to ICLR18 Graph Attention Networks , submitted to ICLR18 2 ) `` The one layer analysis in sections 2.1 , 2.2 and 2.3 is simply an explanation of graph polynomial filters , which were previously proposed and analyzed in cited work of Sandryhaila and Moura ( 2013 ) . Together with the summary of other methods and introduction , it composes the first 8 pages of the paper . I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader . I also recommend stating which ideas came from the Sandryhaila and Moura ( 2013 ) work in a more pronounced manner. `` Reply : Thank you for your suggestion . We have moved Section 2.3 to the Appendix following your suggestion . Sections 2.1 and 2.2 explain important concepts in our proposed method : the definition of graph CNN , graph filter size , as well as how to understand graph convolution as a local feature extractor , which are important for the understanding of our graph CNN and do not appear in Sandryhaila and Moura ( 2013 ) . We better describe these subsections in the revised version and make them more succinct and clearer . 3 ) `` Next , I disagree with the statement that `` it is not clear how to keep the vertex local property when filtering in the spectrum domain '' . Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain . See Eq.18 and 19 in [ 1 ] . `` [ 1 ] David I Shuman , Sunil K Narang , Pascal Frossard , Antonio Ortega , and Pierre Vandergheynst . The emerging field of signal processing on graphs : Extending high-dimensional data analysis to networks and other irregular domains . IEEE Signal Processing Magazine , 30 ( 3 ) :83\u201398 , 2013 . Reply : Thank you for pointing out this . We have removed this sentence and referred to [ 1 ] in the revised version ."}, {"review_id": "H113pWZRb-2", "review_text": "The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional networks to graph-structured data. I find the paper interesting but not very clearly written in some sections, for instance I would better explain what is the main contribution and devote some more text to the motivation. Why is the proposed approach better than the previously published ones, and when is that there is an advantage in using it? The main contribution seems to be the use of the \"graph shift\" operator from Sandryhaila and Moura (2013), which closely resembles the one from Shuman et al. (2013). It is actually not very well explained what is the main difference. Equation (2) shows that the learnable filters g are operating on the k-th power of the normalized adjacency matrix A, so when K=1 this equals classical GCN from T. Kipf et al. By using K > 1 the method is able to leverage information at a farther distance from the reference node. Section 2.2 requires some polishing as I found hard to follow the main story the authors wanted to tell. The definition of the weight of a path seems disconnected from the main text, ins't A^k kind of a a diffusion operator or random walk? This makes me wonder what would be the performance of GCN when the k-th power of the adjacency is used. I liked Section 3, however while it is true that all methods differ in the way they do the filtering, they also differ in the way the input graph is represented (use of the adjacency or not). Experiments are performed on the usual reference benchmarks for the task and show sensible improvements with respect to the state-of-the-art. TAGCN with K=2 has twice the number of parameters of GCN, which makes the comparison not entirely fair. Did the author experiment with a comparable architecture? Also, how about using A^2 in GCN or making two GCN and concatenate them in feature space to make the representational power comparable? It is also known that these benchmarks, while being widely used, are small and result in high variance results. The authors should report statistics over multiple runs. Given the systematic parameter search, with reference to the actual validation (or test?) set I am afraid there could be some overfitting. It is quite easy to probe the test set to get best performance on these benchmarks. As a minor remark, please make figures readable also in BW. Overall I found the paper interesting but also not very clear at pointing out the major contribution and the motivation behind it. At risk of being too reductionist: it looks as learning a set of filters on different coordinate systems given by the various powers of A. GCN looks at the nearest neighbors and the paper shows that using also the 2-ring improves performance. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1 ) `` I find the paper interesting but not very clearly written in some sections , for instance I would better explain what is the main contribution and devote some more text to the motivation . Why is the proposed approach better than the previously published ones , and when is that there is an advantage in using it ? '' Reply : Thank you for your suggestion . We now provide additional explanation of the main contributions and strengths of our approach in the revised version . This paper proposes a modification to the graph convolution step in CNNs that is particularly relevant for graph structured data . Our proposed convolution is graph-based convolution and draws on techniques from graph signal processing . We define rigorously the graph convolution operation on the vertex domain as multiplication by polynomials of the graph adjacency matrix , which is consistent with the notion of convolution in graph signal processing . In graph signal processing , polynomials of the adjacency matrix are graph filters , extending to graph based data the usual concept of filters in traditional time or image based signal processing . Thus , comparing ours with existing work of graph CNNs , our paper provides a solid theoretical foundation for our proposed convolution step instead of an ad-hoc approach to convolution in CNNs for graph structured data . Further , our method avoids computing the spectrum of the graph Laplacian as in ( Bruna et al.2014 ) , or approximating the spectrum using high degree Chebyshev polynomials of the graph Laplacian matrix ( in Defferrard et al.2016 , it is suggested that one needs a 25th degree Chebyshev polynomial to provide a good approximation to the graph Laplacian spectrum ) or using high degree Cayley polynomials of the graph Laplacian matrix ( in Levie et al.2017 , 12th degree Cayley polynomials are needed ) . We also clarify that the GCN method in Kipf & Welling 2017 is a first order approximation of the Chebyshev polynomials approximation in Defferrard et al.2016 , which is very different from our method . Our method has a much lower computational complexity than the complexity of the methods proposed in Bruna et al.2014 , Defferrard et al.2016 , Levie et al.2017 , since our method only uses polynomials of the adjacency matrix with maximum degree 2 as shown in our experiments . Finally , the method that we propose exhibits better performance than existing methods . In the revised version , we have followed your suggestion and elaborated on the above two points as our main contributions and devoted more text to motivating our method . Bruna , J. , Zaremba , W. , Szlam , A. , & LeCun , Y. Spectral networks and locally connected networks on graphs , ICLR2013 Defferrard , M. , Bresson , X. , & Vandergheynst , P. Convolutional neural networks on graphs with fast localized spectral filtering . In NIPS2016 Graph Convolutional Neural Networks with Complex Rational Spectral Filters , submitted to ICLR18 Kipf , T. N. , & Welling , M. ( 2016 ) . Semi-supervised classification with graph convolutional networks . ICLR2017"}], "0": {"review_id": "H113pWZRb-0", "review_text": "The authors propose a new CNN approach to graph classification that generalizes previous work. Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed. This incorporates information from more distant vertices in one propagation step. The proposed idea is not exceptional original, but the paper has several strong points: * The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one. * The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level. * The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach). * The experimental evaluation appears to be sound. A moderate improvement compared to other approaches is observed for all data sets. In summary, I think the paper can be accepted for ICLR. ----------- EDIT ----------- After reading the publications mentioned by the other reviewers as well as the following related contributions * Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018) * Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916) I agree that the relation to previous work is not adequately outlined. Therefore I have modified my rating accordingly.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive comments . 1 ) We agree with the reviewer that our method is able to leverage information at a farther distance on the graph than the GCN of Kipf & Welling 2017 . However , ours is not a simple generalization of GCN . In fact , extending GCN to the second order would not lead to our results . We clarify the fundamental difference between our method and the GCN methodology if we extended the latter to a higher order in the separate comment with title \u201c Differences between the proposed TAGCN and GCN in Kipf & Welling 2017 \u201d due to space limitation . Thank you for your attention . We have added the corresponding discussion in Section 3 of the revised version . 2 ) `` After reading the publications mentioned by the other reviewers as well as the following related contributions * Network of Graph Convolutional Networks Trained on Random Walks ( under review for ICLR 2018 ) * Graph Convolution : A High-Order and Adaptive Approach , Zhenpeng Zhou , Xiaocheng Li ( arXiv:1706.09916 ) I agree that the relation to previous work is not adequately outlined . Therefore I have modified my rating accordingly . '' We thank the reviewer for pointing out these two recent works on graph CNN . We would like to point out that our method is substantially different from these two papers . The graph convolutions in these two papers are defined based on ad-hoc methods , which do not have the physical meaning of convolution . The first paper concatenates A^k with k from 0 to 6 , and the second paper defines { \\tilde A } ^k = min { A^k + I,1 } . In contrast , our definition of convolution is based on graph signal processing , it is consistent with the convolutional theorem , and , finally , it reduces to classical convolution for the direct circle topology . The first paper reports their top 3 performers rather than reporting average performance , while we report an averaged performance over 100 trails . Our performance for Pubmed is still better than theirs even if the comparison is unfair . The second paper did not follow the usual data splitting method and so we can not compare ours to their performance directly . We respectfully disagree with the comment that these two papers are not adequately outlined in the original submission . The first paper is submitted to the same conference ICLR2018 as our paper , so , at the same time \u2013 how could we have access to it before hand ? Thus , there is no way we could refer to this paper in our original submission . The second paper appeared in arXiv with the title \u201c Graph Convolutional Networks for Molecules , \u201d which was specific to molecules with content that was quite different from its second version . The second version was submitted to arXiv on Oct , 20 , becoming only available to the public almost at the same time as we submitted our paper to ICLR2018 . Further , there is a major revision between these two versions as we can see on arXiv , and the number of pages increased from less than 5 pages and a half to 8 pages . We thank the reviewer for finding this paper for us . We also want to mention that , besides providing a solid foundation for our proposed the graph convolution operation , our method also exhibits better performance due to the fact that no approximation is needed for the convolution operation . Our method outperforms all recently proposed methods on all three datasets . In addition , for the Pubmed dataset , which is much larger than the Citeseer and Cora data sets , we have a 2.1 % improvement over GCN ( Kipf & Welling 2017 ) and 6.7 % improvement over ChebNet ( Defferrard et al.2016 ) .These performance results are averages obtained over 100 Monte Carlo runs . As far as we know and as far as we can determine , our method exhibits the best performance on the Pubmed data not only when compared with all previous available publications , as well as when compared with all papers submitted to ICLR18 , see papers below . Also , please note that , as explained by the authors , the last paper listed below fails with the Pubmed data set because of its storage complexity . Graph Partition Neural Networks for Semi-Supervised Classification , submitted to ICLR18 Attention-based Graph Neural Network for Semi-supervised Learning , submitted to ICLR18 Stochastic Training of Graph Convolutional Networks , submitted to ICLR18 Graph Attention Networks , submitted to ICLR18"}, "1": {"review_id": "H113pWZRb-1", "review_text": "In this paper a new neural network architecture for semi-supervised graph classification is proposed. The new construction builds upon graph polynomial filters and utilizes them on each successive layer of the neural network with ReLU activation functions. In my opinion writing of this paper requires major revision. The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora, Pubmed and Citeseer benchmarks. The one layer analysis in sections 2.1, 2.2 and 2.3 is simply an explanation of graph polynomial filters, which were previously proposed and analyzed in cited work of Sandryhaila and Moura (2013). Together with the summary of other methods and introduction, it composes the first 8 pages of the paper. I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader. I also recommend stating which ideas came from the Sandryhaila and Moura (2013) work in a more pronounced manner. Next, I disagree with the statement that \"it is not clear how to keep the vertex local property when filtering in the spectrum domain\". Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain. See Eq. 18 and 19 in [1]. Finally, I should say that TAGCN idea is interesting. I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e. first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate). With more experiments and interpretation of the model, including some sort of multilayer analysis, this can be a good acceptance candidate. [1] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83\u201398, 2013.", "rating": "4: Ok but not good enough - rejection", "reply_text": "1 ) `` In my opinion writing of this paper requires major revision . The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora , Pubmed and Citeseer benchmarks. `` Reply : We have reorganized the paper and added more insights for the proposed TAGCN algorithm . We explain our proposed method in Section 2 and compare it with previous work in Section 3 to emphasize the novelty and differences of our method . We want to emphasize that the adjacency matrix polynomial filter ( graph convolution operation ) defined on the vertex domain ( our method ) is totally different from all the existing graph CNN methods available and that define the convolution in the spectrum domain . Thus , our proposed convolution , its computational complexity , and understanding of the choice of the filter size , all need adequate explanations , and these are given in Section 3 . Even some of the reviewers seem to misunderstand the GCN method in Kipf & Welling 2017 based on approximations by matrix Chebyshev polynomials in Defferrard et al.2016 with our method . Thus explaining adequately our method from different perspectives is necessary . We have further described these relationships in Section 3 in the revised version and made the architecture of our method clearer . As for performance , our method outperforms all recently proposed methods on all three datasets . In addition , for the Pubmed dataset , which is much larger than the Citeseer and Cora data sets , we have a 2.1 % improvement over GCN ( Kipf & Welling 2017 ) and 6.7 % improvement over ChebNet ( Defferrard et al.2016 ) .These performance results are averages obtained over 100 Monte Carlo runs . As far as we know and as far as we can determine , our method exhibits the best performance on the Pubmed data not only when compared with all previous available publications , as well as when compared with all papers submitted to ICLR18 , see papers below . Also , please note that , as explained by the authors , the last paper listed below fails with the Pubmed data set because of its storage complexity . Graph Partition Neural Networks for Semi-Supervised Classification , submitted to ICLR18 Attention-based Graph Neural Network for Semi-supervised Learning , submitted to ICLR18 Stochastic Training of Graph Convolutional Networks , submitted to ICLR18 Graph Attention Networks , submitted to ICLR18 2 ) `` The one layer analysis in sections 2.1 , 2.2 and 2.3 is simply an explanation of graph polynomial filters , which were previously proposed and analyzed in cited work of Sandryhaila and Moura ( 2013 ) . Together with the summary of other methods and introduction , it composes the first 8 pages of the paper . I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader . I also recommend stating which ideas came from the Sandryhaila and Moura ( 2013 ) work in a more pronounced manner. `` Reply : Thank you for your suggestion . We have moved Section 2.3 to the Appendix following your suggestion . Sections 2.1 and 2.2 explain important concepts in our proposed method : the definition of graph CNN , graph filter size , as well as how to understand graph convolution as a local feature extractor , which are important for the understanding of our graph CNN and do not appear in Sandryhaila and Moura ( 2013 ) . We better describe these subsections in the revised version and make them more succinct and clearer . 3 ) `` Next , I disagree with the statement that `` it is not clear how to keep the vertex local property when filtering in the spectrum domain '' . Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain . See Eq.18 and 19 in [ 1 ] . `` [ 1 ] David I Shuman , Sunil K Narang , Pascal Frossard , Antonio Ortega , and Pierre Vandergheynst . The emerging field of signal processing on graphs : Extending high-dimensional data analysis to networks and other irregular domains . IEEE Signal Processing Magazine , 30 ( 3 ) :83\u201398 , 2013 . Reply : Thank you for pointing out this . We have removed this sentence and referred to [ 1 ] in the revised version ."}, "2": {"review_id": "H113pWZRb-2", "review_text": "The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional networks to graph-structured data. I find the paper interesting but not very clearly written in some sections, for instance I would better explain what is the main contribution and devote some more text to the motivation. Why is the proposed approach better than the previously published ones, and when is that there is an advantage in using it? The main contribution seems to be the use of the \"graph shift\" operator from Sandryhaila and Moura (2013), which closely resembles the one from Shuman et al. (2013). It is actually not very well explained what is the main difference. Equation (2) shows that the learnable filters g are operating on the k-th power of the normalized adjacency matrix A, so when K=1 this equals classical GCN from T. Kipf et al. By using K > 1 the method is able to leverage information at a farther distance from the reference node. Section 2.2 requires some polishing as I found hard to follow the main story the authors wanted to tell. The definition of the weight of a path seems disconnected from the main text, ins't A^k kind of a a diffusion operator or random walk? This makes me wonder what would be the performance of GCN when the k-th power of the adjacency is used. I liked Section 3, however while it is true that all methods differ in the way they do the filtering, they also differ in the way the input graph is represented (use of the adjacency or not). Experiments are performed on the usual reference benchmarks for the task and show sensible improvements with respect to the state-of-the-art. TAGCN with K=2 has twice the number of parameters of GCN, which makes the comparison not entirely fair. Did the author experiment with a comparable architecture? Also, how about using A^2 in GCN or making two GCN and concatenate them in feature space to make the representational power comparable? It is also known that these benchmarks, while being widely used, are small and result in high variance results. The authors should report statistics over multiple runs. Given the systematic parameter search, with reference to the actual validation (or test?) set I am afraid there could be some overfitting. It is quite easy to probe the test set to get best performance on these benchmarks. As a minor remark, please make figures readable also in BW. Overall I found the paper interesting but also not very clear at pointing out the major contribution and the motivation behind it. At risk of being too reductionist: it looks as learning a set of filters on different coordinate systems given by the various powers of A. GCN looks at the nearest neighbors and the paper shows that using also the 2-ring improves performance. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1 ) `` I find the paper interesting but not very clearly written in some sections , for instance I would better explain what is the main contribution and devote some more text to the motivation . Why is the proposed approach better than the previously published ones , and when is that there is an advantage in using it ? '' Reply : Thank you for your suggestion . We now provide additional explanation of the main contributions and strengths of our approach in the revised version . This paper proposes a modification to the graph convolution step in CNNs that is particularly relevant for graph structured data . Our proposed convolution is graph-based convolution and draws on techniques from graph signal processing . We define rigorously the graph convolution operation on the vertex domain as multiplication by polynomials of the graph adjacency matrix , which is consistent with the notion of convolution in graph signal processing . In graph signal processing , polynomials of the adjacency matrix are graph filters , extending to graph based data the usual concept of filters in traditional time or image based signal processing . Thus , comparing ours with existing work of graph CNNs , our paper provides a solid theoretical foundation for our proposed convolution step instead of an ad-hoc approach to convolution in CNNs for graph structured data . Further , our method avoids computing the spectrum of the graph Laplacian as in ( Bruna et al.2014 ) , or approximating the spectrum using high degree Chebyshev polynomials of the graph Laplacian matrix ( in Defferrard et al.2016 , it is suggested that one needs a 25th degree Chebyshev polynomial to provide a good approximation to the graph Laplacian spectrum ) or using high degree Cayley polynomials of the graph Laplacian matrix ( in Levie et al.2017 , 12th degree Cayley polynomials are needed ) . We also clarify that the GCN method in Kipf & Welling 2017 is a first order approximation of the Chebyshev polynomials approximation in Defferrard et al.2016 , which is very different from our method . Our method has a much lower computational complexity than the complexity of the methods proposed in Bruna et al.2014 , Defferrard et al.2016 , Levie et al.2017 , since our method only uses polynomials of the adjacency matrix with maximum degree 2 as shown in our experiments . Finally , the method that we propose exhibits better performance than existing methods . In the revised version , we have followed your suggestion and elaborated on the above two points as our main contributions and devoted more text to motivating our method . Bruna , J. , Zaremba , W. , Szlam , A. , & LeCun , Y. Spectral networks and locally connected networks on graphs , ICLR2013 Defferrard , M. , Bresson , X. , & Vandergheynst , P. Convolutional neural networks on graphs with fast localized spectral filtering . In NIPS2016 Graph Convolutional Neural Networks with Complex Rational Spectral Filters , submitted to ICLR18 Kipf , T. N. , & Welling , M. ( 2016 ) . Semi-supervised classification with graph convolutional networks . ICLR2017"}}