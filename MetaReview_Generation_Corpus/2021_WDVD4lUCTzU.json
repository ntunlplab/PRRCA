{"year": "2021", "forum": "WDVD4lUCTzU", "title": "Universal Sentence Representations Learning with Conditional Masked Language Model", "decision": "Reject", "meta_review": "This paper proposes a Conditional Masked Language Modeling (CMLM) method to enhance the MLM by conditioning on the contextual information. \n\nAll of the reviewers think the results are good. However, the reviewers also think the intuition and experiments are not so convincing. The responses and revisions still not satisfy all the reviewers' major concern.", "reviews": [{"review_id": "WDVD4lUCTzU-0", "review_text": "This paper presents Conditional Masked Language Modeling ( CMLM ) , which integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences . It is shown that the English CMLM model achieves strong performance on SentEval , and outperforms models learned using ( semi- ) supervised signals . It is also found that a multilingual CMLM model co-trained with bitext retrieval ( BR ) and natural language inference ( NLI ) tasks outperforms the previous state-of-the-art multilingual models by a large margin . The paper further proposes a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics . -Strengths Learning sentence representations on large scale unlabeled corpora is an important research problem . This paper presents a heavily empirical study , with a series of experiments to evaluate the proposed sentence representation learning method . Multilingual experiments are conducted , with interesting results on language agnostic . The proposed method , as shown in Figure 1 , is somewhat new . -Weaknesses The study is mainly empirical . The authors should provide more details about the three-layer neural network as the projection P ( \u00b7 ) . Another concern is that the contribution of this paper to research community may be weak , if the code is not released and the results are not easily reproduced . -- update after reading the response -- Thanks for the authors ' response . Mainly empirical and limited in methodology novelty . So I tend to keep the score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and valuable suggestions ! Please find our responses below : 1 . The architecture of the 3-layer MLP projection $ P $ is as follows . Let h denote the dimension of the input sentence vector ( e.g.h = $ 768 $ in BERT base ; $ h = 1024 $ in BERT large ) . Let FC ( $ a $ , $ b $ , $ c $ ) denote a fully connected layer with input dimension $ a $ , output dimension $ b $ and nonlinearity function $ c $ . The three layers are FC ( $ h $ , $ 2h $ , ReLU ) , FC ( $ 2h $ , $ 2h $ , ReLU ) , FC ( $ 2h $ , $ h $ , None ) . The information has been added to the appendix . 2.Code and reproduction : we are working on making the code available publicly . Also we will release pretrained models to the public so that researchers can reproduce the results and leverage the model for their own projects . Links will be posted here once available . 3.To have a more thorough understanding of the sentence representations learnt by our models , we have included an additional experiment on the Tatoeba task from XTREME benchmark [ 1 ] that covers 36 languages , shown in Table 5 . Our model \u201c CLM+BR \u201d has the highest average performance and outperforms all baseline models on 30 out of 36 languages . If you have any other questions , please let us know ! References : [ 1 ] Hu , Junjie , et al . `` Xtreme : A massively multilingual multi-task benchmark for evaluating cross-lingual generalization . '' ICML 2020 ."}, {"review_id": "WDVD4lUCTzU-1", "review_text": "I appreciate the response from the authors to my review , as well as to others . My concerns on the intuition are most not solved . Although in this DNN dominating era , we can not expect the explainability as we had before , I still believe that a solid work should be grounded on a reasonable basis , which could be in a high level , such as BERT and SBERT . Let 's refer to the example given in the model architecture . The projection of the sentence vector of `` Life is a box of chocolates '' is left-concatenated with the masked embeddings of the second sentence . This operation is very much lacking in intuition , how come the projection of a sentence representation can be concatenated with the embeddings ? In addition , `` The second encoder shares the same weights with the encoder used to embed s1 '' , considering their inputs are very different , weight sharing for the two encoders are also problematic . Another point I just noticed , although the authors claimed that their model is better than SBERT , and did a comparison with SBERT-large , they did not compare with SBERT-base , which makes the conclusion unreliable . This paper proposes a method called `` Conditional Masked Language Model '' for unsupervised sentence representation learning . The method involves two-sentence encoder , where one sentence depends on the encoded sentence level representation of the adjacent sentence . The experimental results are good overall , as the proposed method tends to give the best results across monolingual and multilingual benchmark datasets . There are still some concerns about the novelty of this paper . First , I think the explanations for the intuition of the proposed model can be clarified , especially in the introduction section . Second , the baselines used for comparison are not complete , which makes me concern about the effectiveness of the model . The proposed model is the combination of Skip-Thought ( Kiros et al. , 2015 ) and BERT masked LM ( Devlin et al. , 2019 ) . Their experimental results show a detailed comparison of BERT but ignore much about the Skip-Thought . Although the authors mentioned the results of the Skip-Thought model on the SentEval benchmark , the encoder used in the Skip-Thought ( Kiros et al. , 2015 ) is RNN while the author used the Transformer Encoder . I would appreciate a better and fair comparison of the Skip-Thought model by using same transformer encoder and same training corpus . I am not sure why you used the concatenation when you do the masked LM . Are there any other ways to do that ? It can be more convincing to see some analysis or results here . Additionally , there is another work titled \u201c DeCLUTR : Deep Contrastive Learning for Unsupervised Textual Representations \u201d , which also focuses on unsupervised sentence representation learning . Although it is from arxiv , it would be nice that the authors can mention this work . It seems good that the authors performed many experiments over many different datasets across monolingual and multilingual . The exploration of the same language bias of the learned representations is also very interesting . To summarize , the paper is a bit limited in terms of technical contribution , particularly considering that there is not a good intuition explanation , but some analysis in this paper looks good .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your review and valuable comments ! Please find our responses below : * * 1 . `` What is the intuition for the proposed Model ? `` * * - As mentioned in the second paragraph in section 2 , the intuition behind CMLM is `` to make the encoder produce an effective sentence level embedding of the first sentence for better MLM prediction of the second sentence '' . We followed your suggestion by modifying the introduction section , especially the second paragraph , to make the intuition behind CMLM clearer . - The intuition for bitext retrieval ( BR ) is to make the multilingual representation language agnostic , which is confirmed by the outstanding zero-shot cross lingual performance on Amazon Reviews ( Table 4 ) and high cross lingual accuracy on Tatoeba ( Table 5 ) . - The intuition for cross lingual NLI finetuning is to provide supervised learning signals and improve the quality of sentence representations . This is supported by the significant improvements from NLI finetuning on Amazon Reviews Dataset ( Tabel 5 , between rows \u201c CMLM+BR \u201d and \u201c CMLM+BR+NLI \u201d ) and XEVAL ( Table 3 , between rows \u201c S3 \u201d and row \u201c S3+NLI \u201d ) . * * 2 . `` CMLM looks similar to SkipThought . The baselines used for comparison are not complete since authors should compare with baselines ( e.g.SkipThought ) using Transformer . `` * * Following your suggestion , we implement QuickThought ( a more recent and better unsupervised sentence representation learning model than SkipThought ) with Transformer and train with our data . It is denoted as \u201c QuickThought ( CC ) \u201d in Table 1 . Leveraging our data or Transformer does not make QuickThought better than our models . Also note that our model differs from SkipThought in the following aspects : + SkipThought relies on an extra decoder network while CMLM only has the encoder . + SkipThought predicts the entire sentence while CMLM predicts masked tokens only so the predictions can be done in parallel . These two differences make CMLM more efficient to train when compared with SkipThought . * * 3 . `` Are there any other ways besides using `` concatenation '' in CMLM ? It can be more convincing to see some analysis or results here . `` . * * Yes.And analysis and results were already in the paper . We are sorry if results were not presented more explicitly . Concretely , besides the \u201c concatenation \u201d , we also tried the \u201c skip connection \u201d configuration in CMLM . Results using this \u201c skip connection \u201d are presented in Table 7 , row \u201c skip \u201d . The model architecture of \u201c skip connection \u201d is as follows . Given two sentences $ s_1 $ and $ s_2 $ . By inputting $ s_2 $ to the transformer encoder , we obtain an output $ M \\in R^ { H \\times L } $ , where H denotes the hidden size and L denotes the maximum token length . Recall the sentence representation of s1 is computed as $ v \\in R^ { H } $ . We then concatenate $ v $ to each column $ m_i $ ( $ i = 1,2 , \\dots , L $ ) in $ M $ . The concatenated tensor $ M ' $ is of size $ 2H\\times L $ , We then use $ M \u2019 $ as the input for masked token prediction in $ s_2 $ . As shown in table 2 , our current configuration \u201c concatenation \u201d is better . * * 4.Citations . * * Thanks for pointing out this paper ! We \u2019 ve cited the DeCLUTR paper as suggested . * * 5.We add extra experiments on Tatoeba Semantic Retrieval Dataset to the paper . * * We further evaluate our models on the Tatoeba dataset , as shown in Table 5 . Our models \u201c CMLM+BR \u201d outperforms all baseline models by a significant margin in terms of the average performance . It also has the highest accuracy in 30 out of 36 languages ."}, {"review_id": "WDVD4lUCTzU-2", "review_text": "- I appreciate the response from authors and the additional experiments . I do think the semantic search task adds value to the paper . However , the paper continues to be centered around the SentEval benchmark results . While SentEval is a useful benchmark to evaluate sentence representations , it does n't reflect well how these representations will be used in practice . A fine-tuned BERT model will likely perform strongly on these tasks . The paper would be far more compelling if the authors can provide strong evidence that the sentence embeddings do well on tasks where using a BERT model is either less effective due to performance or computational reasons . I prefer to keep my score . This work proposes a self-supervised training objective called CMLM ( conditional masked language model ) for learning sentence representations . An encoder produces multiple fixed length representations of a given sentence and a decoder reconstructs the adjacent sentence given it \u2019 s masked version and the encoded representations . CMLM performs well on the SentEval benchmark . CMLM is further extended to the multilingual setting via bi-text retrieval contrastive training and training on NLI data . The multilingual version is shown to work well on multiple translated versions of the SentEval benchmark ( SentEval data translated into other languages using an off the shelf translation system ) and Amazon reviews ( sentiment classification ) . Pros * This work addresses the important problem of ( unsupervised ) sentence representation learning . Extracting fixed-length sentence representations from popular language model based encoders is a non-trivial problem and this work attempts to provide a solution . * Experiments go beyond the standard English setting and evaluate sentence representations in the multilingual setting as well . * Interesting modeling approaches . Cons * Experiments are weak . It is unclear to what extent the tasks + evaluation protocol considered here are reflective of language understanding . I don \u2019 t think strong baselines were considered . Some of the evaluation benchmarks considered seem arbitrary . * Model is largely based on prior work . The main contribution is not clear . There are many settings considered in the paper and it is unclear if the proposed contributions are truly significant due to weak baselines are differences in data used for training different methods . There are several issues with the experimental setup . * Evaluation protocol : It is unclear if the evaluation protocol considered is measuring language understanding capability well . Representations from the encoders are held fixed and linear classifiers are trained on top of these fixed representations on downstream tasks using labelled data . To me , this is not a setting that demands sentence vectors . It only shows that the sentence vectors capture useful features . I would suggest focusing on a setting where the advantage of the sentence vectors can be demonstrated such as a retrieval problem . * Baselines : It is unfair to compare the proposed method against baselines like BERT which are not designed to produce fixed length encodings . * Data used for pre-training : It is difficult to gauge how good the method is in comparison to other models due to differences in the data used for pre-training . Ideally , there should be a table comparing models that use the exact same resources . In Table 1 , although BERT-base/large is trained on the same data , it is not a strong baseline since mean pooled representations from the encoder are treated as a sentence representation . Ideally , the model should be compared against a skip-thought baseline or an unsupervised sentence representation learning method that uses the same resources for training . I would have expected the authors to evaluate multilingual representations on existing benchmarks as well . I don \u2019 t find the proposed benchmark XEVAL very convincing . Claims would have been stronger if authors had also included results on existing benchmarks . The authors need to make it clear exactly what resources are used for training each method . Presentation can be improved , especially the organization of the paper . It is difficult to follow the paper and identify the main contributions in the current presentation . The paper touches upon several things - conditional masked language modeling , bitext retrieval , NLI training , language agnosticism , etc , and I find the paper quite incoherent . I suggest the authors make a focused contribution and provide strong experimental evidence to support that contribution . Right now there \u2019 s too many things which makes it hard to make sense of the paper as whole . While the approaches considered in the paper have some merit , the significance of this work is unclear due to issues in the evaluation .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and valuable feedback ! Please find our responses below : * * 1 . About `` experiments are weak '' . * * The evaluation benchmarks in our paper are widely adopted in previous text representation learning works . For example , SentEval [ 1,2,3 ] , Amazon Reviews [ 4,5,6 ] and Tatoeba [ 7 , 8 ] . The benchmarks i the paper thoroughly examine the capability of a representation system , including semantic alignment ( Tatoeba ) , transfer learning to various kinds of downstream tasks ( sentiment analysis ( SST , MR , CR ) , semantic classification , NLI ( SICK-E ) , text similarity ( SICK-R ) and paraphrase detection ( MRPC ) in SentEval , XEVAL ) and zero-shot cross-lingual transfer learning ( Amazon Reviews ) . Our models consistently show strong performance across these benchmarks . We believe our evaluations are detailed and in-depth . * * 2.About `` Model is largely based on prior work ... '' * * We provide a summary of main contributions of this paper in the last paragraph in the introduction section . To reiterate , we propose an unsupervised sentence representation learning method CMLM . To the best of our knowledge , CMLM is a novel model architecture proposed for the first time . * * 3.About `` Weak Baselines '' . * * Baseline models considered in the paper include many SOTA sentence representation models : SkipThought , QuickThought , InferSent , USE and LASER . As shown in Table 1-5 , CMLM consistently outperform baseline models . To address the effects from differences in training data , we train multiple baselines with the same training data of CMLM , e.g.QuickThought ( CC ) , English BERT large/base ( CC ) . As shown in table 1 , CMLM outperform these baselines that are trained with the same data resources . * * 4.About `` Evaluation protocol '' . * * Following your suggestion , we evaluate our models on Tatoeba [ 7 , 8 ] , a multilingual retrieval benchmark , as shown in Table 5 . Our models \u201c CMLM+BR \u201d outperforms all baseline models by a significant margin in terms of the average performance . It has the highest accuracy in 30 out of 36 languages . * * 5.About `` Data used for pretraining : It is difficult to ... '' * * Following your suggestion , we train QuickThought , an unsupervised sentence representation learning method , using the same Common Crawl dumps that our models are trained on . To address the possible advantage coming from the Transformer , we use a transformer encoder in QuickThought instead of a GRU ( RNN ) in the original QuickThought implementation . The model is denoted as QuickThought ( CC ) in Table 1 . Using a transformer encoder and Common Crawl does not make QuickThought better than our model . Also notice that the model XLM-R also uses Common Crawl corpora . Results in Table 2 and 4 shows that our model CMLM still outperforms XLM-R. * * 6 . About `` paper presentation and organization '' . * * Thanks for the suggestion ! We \u2019 ve edited the paper to make the story more coherent following your suggestion . Especially , we added a paragraph in the introduction section ( the second last one ) to describe how the paper is organized . * * 7.Extra evaluations for multilingual representations on existing benchmarks . * * This is a good idea ! As mentioned above , we \u2019 ve evaluated on the Tatoeba dataset ( table 5 ) . Besides XEVAL , the multilingual representations are also evaluated on Amazon Reviews . On both Amazon Reviews and Tatoeba , our models outperform all baseline models . * * 8.About `` What resources are used to train each method '' . * * + For English and Multilingual CMLM , training data ( sec.3 and sec.4.1 ) are generated from three Common Crawl dumps ( 2020-05 , 2020-10 , 2020-16 , see https : //commoncrawl.org/the-data/get-started/ ) . English CMLM takes ~5 days using 64 Cloud TPUs ( 128 TPU chips total ) . Training Multilingual CMLM takes ~12 days using 64 Cloud TPUs . + Multitask co-training CMLM+BR takes ~5 days 64 Cloud TPUs . Information about BR training data can be found at sec.4.2.+ Cross-lingual NLI finetuning takes ~12 hours using 8 cloud TPUs . Information about data used for cross-lingual finetuning can be found at sec.4.3.References : [ 1 ] Cer , et al . `` Universal sentence encoder . '' arXiv preprint arXiv:1803.11175 ( 2018 ) . [ 2 ] Conneau , Alexis , et al . `` Supervised Learning of Universal Sentence Representations from Natural Language Inference Data . '' EMNLP 2017 . [ 3 ] Yang , et al .. '' Parameter-free Sentence Embedding via Orthogonal Basis . '' EMNLP-IJCNLP 2019 . [ 4 ] Zhou , et al . `` Cross-lingual sentiment classification with bilingual document representation learning . '' ACL 2016 . [ 5 ] Chidambaram , et al . `` Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model . '' RepL4NLP-2019 . 2019 . [ 6 ] Xu , et al . `` Cross-lingual Distillation for Text Classification . '' ACL 2017 . [ 7 ] Artetxe , et al . `` Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond . '' ACL 2019 . [ 8 ] Hu , et al . `` Xtreme : A massively multilingual multi-task benchmark for evaluating cross-lingual generalization . '' ICML 2020 ."}, {"review_id": "WDVD4lUCTzU-3", "review_text": "The authors present conditional masked language modeling ( CMLM ) , a new method for unsupervised pretraining , in which the skip-thought notion of conditioning on neighboring sentences is adopted for masked language modeling . The upshot of the proposed approach is that it generates single sentence embeddings that perform competitively on SentEval . In the multilingual setting , the authors combine their CMLM method with a bitext retrieval objective ( selecting a sentence \u2019 s translation from the other sentences of the language in the batch ) that increases performance on a version of the SentEval tasks translated into 14 other languages . In their analysis , the authors make further claims about multilingual embeddings capturing language ID information in their first principle components , a conclusion somewhat substantiated by their results . The authors provide a small amount of ablation experiments for experimental/model design choices . The underlying idea is worth pursuing , the execution and description could be improved , people will be interested in the results that are present ( but then have questions ) . Overall Why only a subset of SentEval for the English experiments ( 3.2 ) but then the full SentEval in the multilingual XEval experiment ( 4.5.1 ) ? Especially if you are trying to make a single-sentence encoder but then evaluating on SICK-E instead of SICK-R , which is arguably a more applicable eval set . Why no XLM-R in the amazon reviews analysis ( 4.5.2 ) ? Sec 1.Fig 1 : box of chocolate * s * Sec 2 . -- \u201c conditional \u201d , \u2192 \u201c conditional , \u201d -- no quantitative comparison of using max vs mean pool vs CLS embedding -- first sentence is feed \u2192 first sentence is fed -- three-layer neural network \u2192 three-layer MLP -- refer to using the same set of encoder weights for different inputs as siamese networks , as done in the sentence-bert paper https : //en.wikipedia.org/wiki/Siamese_neural_network -- v_d is used but not defined Sec 3 . -- Skip-thought originally used a sentence to predict the generation of both the preceding and succeeding sentences . This is functionally equivalent to your flip-flopping the order of the consecutive sequences . I would make the point that these steps are equivalent . Note that this is also not necessarily making the task \u201c more challenging \u201d ( and moreover I am not sure why \u201c more challenging \u201d equates with \u201c better pretraining method for language understanding \u201d -- and an ablation of this step is not included to show that it is in fact necessary and useful ) . -- similarly , no analysis of masking rate , nor explanation for why \u2018 more challenging \u2019 is better . -- \u201c We explore two transformer configurations , base and large , same as in the original BERT paper. \u201d \u2013 fragmented -- The number of projections N = 15 . \u2013 fragmented -- SentBERT \u2192 SentenceBERT or SBERT -- On the specific subset of SentEval tasks you \u2019 ve selected , the majority of the performance discrepancy is in the SICK-E task -- otherwise , the overall # \u2019 s are rather interchangeable . How does this change if you add in the rest of the SentEval tasks , and why were they omitted ? Analysis/exploration for why you get such a performance boost only on SICK-E would also be useful . -- \u201c the length \u2026 set to be 256 tokens \u201d : please clarify whether the \u201c length \u201d refers to the maximum length , or each sentence is a fixed-length chunk consisting of 256 tokens -- typo : \u201c we also exploring \u201d Sec 4 . -- If your introduced bitext retrieval objective uses batch size , experiments comparing the effect of batch size is necessary . -- Please specify the value of the margin m being used in the experiments -- Choice of number of projections is also not motivated ( and in fact contradicted by the ablation experiment finding that 15 is better ) -- the motivation and contribution for XEVAL are great -- the explanation of the dataset is lacking . What translation API was used ? How was the XEVAL score computed for each language ? Is it the full set of SentEval downstream tasks ? -- cite precedent for using the concatenation of u , v , u-v and u * v. ( or show its effect via ablation ) -- BR \u2192 CMLM+BR configuration not evaluated -- choice of different training step # \u2019 s in each configuration is not particularly motivated . -- \u201c after exploring options including [ CLS ] representations and max pooling. \u201d what was the performance drop ? -- typo : \u201c has a significant upon mBERT \u201d Sec 5 . -- It is not clear that you can make the claim that the first PC * only * encodes language-identification information ? -- I assume Figure 3 is 2-dimensional TSNE ( needs a cite ) , which comes with its own set of caveats as a visualization tool . Quantitative clustering analysis such as silhouette coefficient might be more appropriate than a plot . If Figure 3 is not t-SNE , please specify the meaning of X and Y axes . -- did not try higher than n=15 projections but claimed it was optimal -- The description of the \u201c skip \u201d ablation is unclear : please clarify what is meant by \u201c concatenated with the sequence outputs of s2 \u201d . -- typos : \u201c removing the first principal component \u2026 effectively eliminate \u201d , \u201c for both two models \u201d , \u201c representations \u2026 generally exhibits \u201d", "rating": "7: Good paper, accept", "reply_text": "* * Sec 5 * * 1 . Our claim in the paper is \u201c the first principal components in each monolingual space * * primarily * * encodes language information \u201d , not \u201c only encodes \u201d . It does not only encode language identification information because note in Table 6 , in most cases PCR yields better retrieval performance . For some languages , PCR makes the retrieval performance drop a bit , which indicates that principal components can still contain semantic information . 2.Figure 3 is a 2D PCA . The x and y axis is the direction of first and second maximum variation through the data . Using silhouette coefficient is a good idea ! We \u2019 ll add that in the final version . 3.Explanation for \u201c Concatenated with the sequence outputs of $ s_2 $ \u201d : Given two sentences $ s_1 $ and $ s_2 $ . By inputting $ s_2 $ to the transformer encoder , we obtain an output matrix $ M \\in R^ { H\\times L } $ , where $ H $ denotes the hidden size and $ L $ denotes the maximum token length . Recall the sentence representation of $ s_1 $ is computed as $ v $ ( of size $ H $ ) . We then concatenate $ v $ to each column vector $ m_i $ ( $ i = 1,2 , \\dots , L $ ) in $ M $ . The concatenated tensor $ M \u2019 $ is of size $ 2H\\times L $ , We then use $ M \u2019 $ as the input for masked token prediction in $ s_2 $ ."}], "0": {"review_id": "WDVD4lUCTzU-0", "review_text": "This paper presents Conditional Masked Language Modeling ( CMLM ) , which integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences . It is shown that the English CMLM model achieves strong performance on SentEval , and outperforms models learned using ( semi- ) supervised signals . It is also found that a multilingual CMLM model co-trained with bitext retrieval ( BR ) and natural language inference ( NLI ) tasks outperforms the previous state-of-the-art multilingual models by a large margin . The paper further proposes a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics . -Strengths Learning sentence representations on large scale unlabeled corpora is an important research problem . This paper presents a heavily empirical study , with a series of experiments to evaluate the proposed sentence representation learning method . Multilingual experiments are conducted , with interesting results on language agnostic . The proposed method , as shown in Figure 1 , is somewhat new . -Weaknesses The study is mainly empirical . The authors should provide more details about the three-layer neural network as the projection P ( \u00b7 ) . Another concern is that the contribution of this paper to research community may be weak , if the code is not released and the results are not easily reproduced . -- update after reading the response -- Thanks for the authors ' response . Mainly empirical and limited in methodology novelty . So I tend to keep the score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and valuable suggestions ! Please find our responses below : 1 . The architecture of the 3-layer MLP projection $ P $ is as follows . Let h denote the dimension of the input sentence vector ( e.g.h = $ 768 $ in BERT base ; $ h = 1024 $ in BERT large ) . Let FC ( $ a $ , $ b $ , $ c $ ) denote a fully connected layer with input dimension $ a $ , output dimension $ b $ and nonlinearity function $ c $ . The three layers are FC ( $ h $ , $ 2h $ , ReLU ) , FC ( $ 2h $ , $ 2h $ , ReLU ) , FC ( $ 2h $ , $ h $ , None ) . The information has been added to the appendix . 2.Code and reproduction : we are working on making the code available publicly . Also we will release pretrained models to the public so that researchers can reproduce the results and leverage the model for their own projects . Links will be posted here once available . 3.To have a more thorough understanding of the sentence representations learnt by our models , we have included an additional experiment on the Tatoeba task from XTREME benchmark [ 1 ] that covers 36 languages , shown in Table 5 . Our model \u201c CLM+BR \u201d has the highest average performance and outperforms all baseline models on 30 out of 36 languages . If you have any other questions , please let us know ! References : [ 1 ] Hu , Junjie , et al . `` Xtreme : A massively multilingual multi-task benchmark for evaluating cross-lingual generalization . '' ICML 2020 ."}, "1": {"review_id": "WDVD4lUCTzU-1", "review_text": "I appreciate the response from the authors to my review , as well as to others . My concerns on the intuition are most not solved . Although in this DNN dominating era , we can not expect the explainability as we had before , I still believe that a solid work should be grounded on a reasonable basis , which could be in a high level , such as BERT and SBERT . Let 's refer to the example given in the model architecture . The projection of the sentence vector of `` Life is a box of chocolates '' is left-concatenated with the masked embeddings of the second sentence . This operation is very much lacking in intuition , how come the projection of a sentence representation can be concatenated with the embeddings ? In addition , `` The second encoder shares the same weights with the encoder used to embed s1 '' , considering their inputs are very different , weight sharing for the two encoders are also problematic . Another point I just noticed , although the authors claimed that their model is better than SBERT , and did a comparison with SBERT-large , they did not compare with SBERT-base , which makes the conclusion unreliable . This paper proposes a method called `` Conditional Masked Language Model '' for unsupervised sentence representation learning . The method involves two-sentence encoder , where one sentence depends on the encoded sentence level representation of the adjacent sentence . The experimental results are good overall , as the proposed method tends to give the best results across monolingual and multilingual benchmark datasets . There are still some concerns about the novelty of this paper . First , I think the explanations for the intuition of the proposed model can be clarified , especially in the introduction section . Second , the baselines used for comparison are not complete , which makes me concern about the effectiveness of the model . The proposed model is the combination of Skip-Thought ( Kiros et al. , 2015 ) and BERT masked LM ( Devlin et al. , 2019 ) . Their experimental results show a detailed comparison of BERT but ignore much about the Skip-Thought . Although the authors mentioned the results of the Skip-Thought model on the SentEval benchmark , the encoder used in the Skip-Thought ( Kiros et al. , 2015 ) is RNN while the author used the Transformer Encoder . I would appreciate a better and fair comparison of the Skip-Thought model by using same transformer encoder and same training corpus . I am not sure why you used the concatenation when you do the masked LM . Are there any other ways to do that ? It can be more convincing to see some analysis or results here . Additionally , there is another work titled \u201c DeCLUTR : Deep Contrastive Learning for Unsupervised Textual Representations \u201d , which also focuses on unsupervised sentence representation learning . Although it is from arxiv , it would be nice that the authors can mention this work . It seems good that the authors performed many experiments over many different datasets across monolingual and multilingual . The exploration of the same language bias of the learned representations is also very interesting . To summarize , the paper is a bit limited in terms of technical contribution , particularly considering that there is not a good intuition explanation , but some analysis in this paper looks good .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your review and valuable comments ! Please find our responses below : * * 1 . `` What is the intuition for the proposed Model ? `` * * - As mentioned in the second paragraph in section 2 , the intuition behind CMLM is `` to make the encoder produce an effective sentence level embedding of the first sentence for better MLM prediction of the second sentence '' . We followed your suggestion by modifying the introduction section , especially the second paragraph , to make the intuition behind CMLM clearer . - The intuition for bitext retrieval ( BR ) is to make the multilingual representation language agnostic , which is confirmed by the outstanding zero-shot cross lingual performance on Amazon Reviews ( Table 4 ) and high cross lingual accuracy on Tatoeba ( Table 5 ) . - The intuition for cross lingual NLI finetuning is to provide supervised learning signals and improve the quality of sentence representations . This is supported by the significant improvements from NLI finetuning on Amazon Reviews Dataset ( Tabel 5 , between rows \u201c CMLM+BR \u201d and \u201c CMLM+BR+NLI \u201d ) and XEVAL ( Table 3 , between rows \u201c S3 \u201d and row \u201c S3+NLI \u201d ) . * * 2 . `` CMLM looks similar to SkipThought . The baselines used for comparison are not complete since authors should compare with baselines ( e.g.SkipThought ) using Transformer . `` * * Following your suggestion , we implement QuickThought ( a more recent and better unsupervised sentence representation learning model than SkipThought ) with Transformer and train with our data . It is denoted as \u201c QuickThought ( CC ) \u201d in Table 1 . Leveraging our data or Transformer does not make QuickThought better than our models . Also note that our model differs from SkipThought in the following aspects : + SkipThought relies on an extra decoder network while CMLM only has the encoder . + SkipThought predicts the entire sentence while CMLM predicts masked tokens only so the predictions can be done in parallel . These two differences make CMLM more efficient to train when compared with SkipThought . * * 3 . `` Are there any other ways besides using `` concatenation '' in CMLM ? It can be more convincing to see some analysis or results here . `` . * * Yes.And analysis and results were already in the paper . We are sorry if results were not presented more explicitly . Concretely , besides the \u201c concatenation \u201d , we also tried the \u201c skip connection \u201d configuration in CMLM . Results using this \u201c skip connection \u201d are presented in Table 7 , row \u201c skip \u201d . The model architecture of \u201c skip connection \u201d is as follows . Given two sentences $ s_1 $ and $ s_2 $ . By inputting $ s_2 $ to the transformer encoder , we obtain an output $ M \\in R^ { H \\times L } $ , where H denotes the hidden size and L denotes the maximum token length . Recall the sentence representation of s1 is computed as $ v \\in R^ { H } $ . We then concatenate $ v $ to each column $ m_i $ ( $ i = 1,2 , \\dots , L $ ) in $ M $ . The concatenated tensor $ M ' $ is of size $ 2H\\times L $ , We then use $ M \u2019 $ as the input for masked token prediction in $ s_2 $ . As shown in table 2 , our current configuration \u201c concatenation \u201d is better . * * 4.Citations . * * Thanks for pointing out this paper ! We \u2019 ve cited the DeCLUTR paper as suggested . * * 5.We add extra experiments on Tatoeba Semantic Retrieval Dataset to the paper . * * We further evaluate our models on the Tatoeba dataset , as shown in Table 5 . Our models \u201c CMLM+BR \u201d outperforms all baseline models by a significant margin in terms of the average performance . It also has the highest accuracy in 30 out of 36 languages ."}, "2": {"review_id": "WDVD4lUCTzU-2", "review_text": "- I appreciate the response from authors and the additional experiments . I do think the semantic search task adds value to the paper . However , the paper continues to be centered around the SentEval benchmark results . While SentEval is a useful benchmark to evaluate sentence representations , it does n't reflect well how these representations will be used in practice . A fine-tuned BERT model will likely perform strongly on these tasks . The paper would be far more compelling if the authors can provide strong evidence that the sentence embeddings do well on tasks where using a BERT model is either less effective due to performance or computational reasons . I prefer to keep my score . This work proposes a self-supervised training objective called CMLM ( conditional masked language model ) for learning sentence representations . An encoder produces multiple fixed length representations of a given sentence and a decoder reconstructs the adjacent sentence given it \u2019 s masked version and the encoded representations . CMLM performs well on the SentEval benchmark . CMLM is further extended to the multilingual setting via bi-text retrieval contrastive training and training on NLI data . The multilingual version is shown to work well on multiple translated versions of the SentEval benchmark ( SentEval data translated into other languages using an off the shelf translation system ) and Amazon reviews ( sentiment classification ) . Pros * This work addresses the important problem of ( unsupervised ) sentence representation learning . Extracting fixed-length sentence representations from popular language model based encoders is a non-trivial problem and this work attempts to provide a solution . * Experiments go beyond the standard English setting and evaluate sentence representations in the multilingual setting as well . * Interesting modeling approaches . Cons * Experiments are weak . It is unclear to what extent the tasks + evaluation protocol considered here are reflective of language understanding . I don \u2019 t think strong baselines were considered . Some of the evaluation benchmarks considered seem arbitrary . * Model is largely based on prior work . The main contribution is not clear . There are many settings considered in the paper and it is unclear if the proposed contributions are truly significant due to weak baselines are differences in data used for training different methods . There are several issues with the experimental setup . * Evaluation protocol : It is unclear if the evaluation protocol considered is measuring language understanding capability well . Representations from the encoders are held fixed and linear classifiers are trained on top of these fixed representations on downstream tasks using labelled data . To me , this is not a setting that demands sentence vectors . It only shows that the sentence vectors capture useful features . I would suggest focusing on a setting where the advantage of the sentence vectors can be demonstrated such as a retrieval problem . * Baselines : It is unfair to compare the proposed method against baselines like BERT which are not designed to produce fixed length encodings . * Data used for pre-training : It is difficult to gauge how good the method is in comparison to other models due to differences in the data used for pre-training . Ideally , there should be a table comparing models that use the exact same resources . In Table 1 , although BERT-base/large is trained on the same data , it is not a strong baseline since mean pooled representations from the encoder are treated as a sentence representation . Ideally , the model should be compared against a skip-thought baseline or an unsupervised sentence representation learning method that uses the same resources for training . I would have expected the authors to evaluate multilingual representations on existing benchmarks as well . I don \u2019 t find the proposed benchmark XEVAL very convincing . Claims would have been stronger if authors had also included results on existing benchmarks . The authors need to make it clear exactly what resources are used for training each method . Presentation can be improved , especially the organization of the paper . It is difficult to follow the paper and identify the main contributions in the current presentation . The paper touches upon several things - conditional masked language modeling , bitext retrieval , NLI training , language agnosticism , etc , and I find the paper quite incoherent . I suggest the authors make a focused contribution and provide strong experimental evidence to support that contribution . Right now there \u2019 s too many things which makes it hard to make sense of the paper as whole . While the approaches considered in the paper have some merit , the significance of this work is unclear due to issues in the evaluation .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review and valuable feedback ! Please find our responses below : * * 1 . About `` experiments are weak '' . * * The evaluation benchmarks in our paper are widely adopted in previous text representation learning works . For example , SentEval [ 1,2,3 ] , Amazon Reviews [ 4,5,6 ] and Tatoeba [ 7 , 8 ] . The benchmarks i the paper thoroughly examine the capability of a representation system , including semantic alignment ( Tatoeba ) , transfer learning to various kinds of downstream tasks ( sentiment analysis ( SST , MR , CR ) , semantic classification , NLI ( SICK-E ) , text similarity ( SICK-R ) and paraphrase detection ( MRPC ) in SentEval , XEVAL ) and zero-shot cross-lingual transfer learning ( Amazon Reviews ) . Our models consistently show strong performance across these benchmarks . We believe our evaluations are detailed and in-depth . * * 2.About `` Model is largely based on prior work ... '' * * We provide a summary of main contributions of this paper in the last paragraph in the introduction section . To reiterate , we propose an unsupervised sentence representation learning method CMLM . To the best of our knowledge , CMLM is a novel model architecture proposed for the first time . * * 3.About `` Weak Baselines '' . * * Baseline models considered in the paper include many SOTA sentence representation models : SkipThought , QuickThought , InferSent , USE and LASER . As shown in Table 1-5 , CMLM consistently outperform baseline models . To address the effects from differences in training data , we train multiple baselines with the same training data of CMLM , e.g.QuickThought ( CC ) , English BERT large/base ( CC ) . As shown in table 1 , CMLM outperform these baselines that are trained with the same data resources . * * 4.About `` Evaluation protocol '' . * * Following your suggestion , we evaluate our models on Tatoeba [ 7 , 8 ] , a multilingual retrieval benchmark , as shown in Table 5 . Our models \u201c CMLM+BR \u201d outperforms all baseline models by a significant margin in terms of the average performance . It has the highest accuracy in 30 out of 36 languages . * * 5.About `` Data used for pretraining : It is difficult to ... '' * * Following your suggestion , we train QuickThought , an unsupervised sentence representation learning method , using the same Common Crawl dumps that our models are trained on . To address the possible advantage coming from the Transformer , we use a transformer encoder in QuickThought instead of a GRU ( RNN ) in the original QuickThought implementation . The model is denoted as QuickThought ( CC ) in Table 1 . Using a transformer encoder and Common Crawl does not make QuickThought better than our model . Also notice that the model XLM-R also uses Common Crawl corpora . Results in Table 2 and 4 shows that our model CMLM still outperforms XLM-R. * * 6 . About `` paper presentation and organization '' . * * Thanks for the suggestion ! We \u2019 ve edited the paper to make the story more coherent following your suggestion . Especially , we added a paragraph in the introduction section ( the second last one ) to describe how the paper is organized . * * 7.Extra evaluations for multilingual representations on existing benchmarks . * * This is a good idea ! As mentioned above , we \u2019 ve evaluated on the Tatoeba dataset ( table 5 ) . Besides XEVAL , the multilingual representations are also evaluated on Amazon Reviews . On both Amazon Reviews and Tatoeba , our models outperform all baseline models . * * 8.About `` What resources are used to train each method '' . * * + For English and Multilingual CMLM , training data ( sec.3 and sec.4.1 ) are generated from three Common Crawl dumps ( 2020-05 , 2020-10 , 2020-16 , see https : //commoncrawl.org/the-data/get-started/ ) . English CMLM takes ~5 days using 64 Cloud TPUs ( 128 TPU chips total ) . Training Multilingual CMLM takes ~12 days using 64 Cloud TPUs . + Multitask co-training CMLM+BR takes ~5 days 64 Cloud TPUs . Information about BR training data can be found at sec.4.2.+ Cross-lingual NLI finetuning takes ~12 hours using 8 cloud TPUs . Information about data used for cross-lingual finetuning can be found at sec.4.3.References : [ 1 ] Cer , et al . `` Universal sentence encoder . '' arXiv preprint arXiv:1803.11175 ( 2018 ) . [ 2 ] Conneau , Alexis , et al . `` Supervised Learning of Universal Sentence Representations from Natural Language Inference Data . '' EMNLP 2017 . [ 3 ] Yang , et al .. '' Parameter-free Sentence Embedding via Orthogonal Basis . '' EMNLP-IJCNLP 2019 . [ 4 ] Zhou , et al . `` Cross-lingual sentiment classification with bilingual document representation learning . '' ACL 2016 . [ 5 ] Chidambaram , et al . `` Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model . '' RepL4NLP-2019 . 2019 . [ 6 ] Xu , et al . `` Cross-lingual Distillation for Text Classification . '' ACL 2017 . [ 7 ] Artetxe , et al . `` Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond . '' ACL 2019 . [ 8 ] Hu , et al . `` Xtreme : A massively multilingual multi-task benchmark for evaluating cross-lingual generalization . '' ICML 2020 ."}, "3": {"review_id": "WDVD4lUCTzU-3", "review_text": "The authors present conditional masked language modeling ( CMLM ) , a new method for unsupervised pretraining , in which the skip-thought notion of conditioning on neighboring sentences is adopted for masked language modeling . The upshot of the proposed approach is that it generates single sentence embeddings that perform competitively on SentEval . In the multilingual setting , the authors combine their CMLM method with a bitext retrieval objective ( selecting a sentence \u2019 s translation from the other sentences of the language in the batch ) that increases performance on a version of the SentEval tasks translated into 14 other languages . In their analysis , the authors make further claims about multilingual embeddings capturing language ID information in their first principle components , a conclusion somewhat substantiated by their results . The authors provide a small amount of ablation experiments for experimental/model design choices . The underlying idea is worth pursuing , the execution and description could be improved , people will be interested in the results that are present ( but then have questions ) . Overall Why only a subset of SentEval for the English experiments ( 3.2 ) but then the full SentEval in the multilingual XEval experiment ( 4.5.1 ) ? Especially if you are trying to make a single-sentence encoder but then evaluating on SICK-E instead of SICK-R , which is arguably a more applicable eval set . Why no XLM-R in the amazon reviews analysis ( 4.5.2 ) ? Sec 1.Fig 1 : box of chocolate * s * Sec 2 . -- \u201c conditional \u201d , \u2192 \u201c conditional , \u201d -- no quantitative comparison of using max vs mean pool vs CLS embedding -- first sentence is feed \u2192 first sentence is fed -- three-layer neural network \u2192 three-layer MLP -- refer to using the same set of encoder weights for different inputs as siamese networks , as done in the sentence-bert paper https : //en.wikipedia.org/wiki/Siamese_neural_network -- v_d is used but not defined Sec 3 . -- Skip-thought originally used a sentence to predict the generation of both the preceding and succeeding sentences . This is functionally equivalent to your flip-flopping the order of the consecutive sequences . I would make the point that these steps are equivalent . Note that this is also not necessarily making the task \u201c more challenging \u201d ( and moreover I am not sure why \u201c more challenging \u201d equates with \u201c better pretraining method for language understanding \u201d -- and an ablation of this step is not included to show that it is in fact necessary and useful ) . -- similarly , no analysis of masking rate , nor explanation for why \u2018 more challenging \u2019 is better . -- \u201c We explore two transformer configurations , base and large , same as in the original BERT paper. \u201d \u2013 fragmented -- The number of projections N = 15 . \u2013 fragmented -- SentBERT \u2192 SentenceBERT or SBERT -- On the specific subset of SentEval tasks you \u2019 ve selected , the majority of the performance discrepancy is in the SICK-E task -- otherwise , the overall # \u2019 s are rather interchangeable . How does this change if you add in the rest of the SentEval tasks , and why were they omitted ? Analysis/exploration for why you get such a performance boost only on SICK-E would also be useful . -- \u201c the length \u2026 set to be 256 tokens \u201d : please clarify whether the \u201c length \u201d refers to the maximum length , or each sentence is a fixed-length chunk consisting of 256 tokens -- typo : \u201c we also exploring \u201d Sec 4 . -- If your introduced bitext retrieval objective uses batch size , experiments comparing the effect of batch size is necessary . -- Please specify the value of the margin m being used in the experiments -- Choice of number of projections is also not motivated ( and in fact contradicted by the ablation experiment finding that 15 is better ) -- the motivation and contribution for XEVAL are great -- the explanation of the dataset is lacking . What translation API was used ? How was the XEVAL score computed for each language ? Is it the full set of SentEval downstream tasks ? -- cite precedent for using the concatenation of u , v , u-v and u * v. ( or show its effect via ablation ) -- BR \u2192 CMLM+BR configuration not evaluated -- choice of different training step # \u2019 s in each configuration is not particularly motivated . -- \u201c after exploring options including [ CLS ] representations and max pooling. \u201d what was the performance drop ? -- typo : \u201c has a significant upon mBERT \u201d Sec 5 . -- It is not clear that you can make the claim that the first PC * only * encodes language-identification information ? -- I assume Figure 3 is 2-dimensional TSNE ( needs a cite ) , which comes with its own set of caveats as a visualization tool . Quantitative clustering analysis such as silhouette coefficient might be more appropriate than a plot . If Figure 3 is not t-SNE , please specify the meaning of X and Y axes . -- did not try higher than n=15 projections but claimed it was optimal -- The description of the \u201c skip \u201d ablation is unclear : please clarify what is meant by \u201c concatenated with the sequence outputs of s2 \u201d . -- typos : \u201c removing the first principal component \u2026 effectively eliminate \u201d , \u201c for both two models \u201d , \u201c representations \u2026 generally exhibits \u201d", "rating": "7: Good paper, accept", "reply_text": "* * Sec 5 * * 1 . Our claim in the paper is \u201c the first principal components in each monolingual space * * primarily * * encodes language information \u201d , not \u201c only encodes \u201d . It does not only encode language identification information because note in Table 6 , in most cases PCR yields better retrieval performance . For some languages , PCR makes the retrieval performance drop a bit , which indicates that principal components can still contain semantic information . 2.Figure 3 is a 2D PCA . The x and y axis is the direction of first and second maximum variation through the data . Using silhouette coefficient is a good idea ! We \u2019 ll add that in the final version . 3.Explanation for \u201c Concatenated with the sequence outputs of $ s_2 $ \u201d : Given two sentences $ s_1 $ and $ s_2 $ . By inputting $ s_2 $ to the transformer encoder , we obtain an output matrix $ M \\in R^ { H\\times L } $ , where $ H $ denotes the hidden size and $ L $ denotes the maximum token length . Recall the sentence representation of $ s_1 $ is computed as $ v $ ( of size $ H $ ) . We then concatenate $ v $ to each column vector $ m_i $ ( $ i = 1,2 , \\dots , L $ ) in $ M $ . The concatenated tensor $ M \u2019 $ is of size $ 2H\\times L $ , We then use $ M \u2019 $ as the input for masked token prediction in $ s_2 $ ."}}