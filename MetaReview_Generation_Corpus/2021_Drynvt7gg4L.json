{"year": "2021", "forum": "Drynvt7gg4L", "title": "AdaSpeech: Adaptive Text to Speech for Custom Voice", "decision": "Accept (Poster)", "meta_review": "The paper is about adapting a voice generation model to new speakers with minimal amount of training data. The key insight in this paper is that the voice can be adapted using a small set of variables -- the bias and the variance associated with the layer that normalizes the mel-spectrogram associated with the decoder. Additionally, they characterize voice at the utterance level to capture stationary factors like background acoustic conditions and at the phoneme level to capture factors such as prosody, though there are no explicit constraints to force such representation.\n\nThe strength of the paper are:\n+ Simplicity of the approach\n+ Empirical evaluation that demonstrates its effectiveness\n\nThe weakness of the paper are:\n- analysis of what the crucial parameters of the model represent\n- lack of clarity that is obvious from several back-and-forths between the reviewers and the author.\n\nA few examples include:\n- \u201cThere is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me.\u201d\n- \u201c it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. (Both the normalization parameters and the speaker embedding itself are fine-tuned.) I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK.\u201d", "reviews": [{"review_id": "Drynvt7gg4L-0", "review_text": "# # # Summary AdaSpeech is a paper on practical TTS custom voice adaptation with the aim of reducing the amount of adapted parameters per voice to allow cloud serving of a large number of custom voices while maintaining high adaptation quality and similarity . The novel piece that enables this is the conditioning of layernorm in the model on the speaker embedding . The grammar reads slightly awkwardly in places , but the paper is understandable and well structured . Descriptions of the model , experiments , and analysis of results are well done . # # # Recommendation * * Weak Reject * * I believe this paper is not novel enough / too applied / focused on experimental results for this conference . There is little discussion on the theoretical side of the acoustic condition modelling , such as how the authors are able to determine that the utterance-level and phoneme-level vectors are modelling things like room condition . Instead , the strengths of this paper are entirely through the strong numerical results . I think this paper would be a solid accept for a more specialized conference like ICASSP or Interspeech . # # # Positives 1 . Well written , great analysis of results and ablation studies . # # # Negatives 1 . What is the loss used to train the phoneme level acoustic predictor ? MSE ? 1.How is it determined that acoustic conditions such as loudness or room conditions are actually captured by the utterance- and phoneme-level acoustic condition modelling ? My intuition would be that your phoneme-level predictor is trained only with phoneme hiddens ( textual information only ) ( do these phoneme hiddens include speaker embedding information ? ) , so at most it models some pitch or prosody information . The utterance-level can definitely model the rest , but where is the evidence ? It could end up modelling only one very specific dimension and still improve the MOS . 1.Similarly , I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information . What happens when using a speaker embedding with the utterance-level vector extracted from a reference speech for a different speaker ? 1.The paper would be better with a discussion on controllability . As a reference utterance needs to be provided , does this mean the synthesized speech will take on the prosody in the reference ? What happens if you want to synthesize a prosody for a speaker that 's not present in any of that speaker 's reference utterances ? I understand this is a big topic with ongoing research which is why it would be a big bonus if this paper can make any kind of progress in that area . 1.I am curious how your phoneme-level predictor would compare with a VAE-based setup , although I understand this can be difficult to set up so no action is required here . # # # Misc * * 2.3 Pipeline of AdaSpeech * * : `` we do not use the two matrices in each conditional layer normalization '' -- which two matrices ? * * 4.2 Method Analysis * * : What does it mean to remove conditional layer normalization ? Then you do n't have any adaptation parameters , so is it not equivalent to Baseline ( spk emb ) case ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments on our paper . We reply to your questions as follows . * * [ About the novelty ] * * We clarify the novelty of our work as follows : 1 . We leverage acoustic condition modeling ( utterance-level and phoneme-level ) to better model the acoustic conditions and improve the generalizability and adaptability of the TTS model , in order to support diverse customers with different acoustic conditions for TTS adaptation . 2.We introduce conditional layer normalization to adapt the TTS model with high voice quality using as few adaptation parameters as possible , in order to support a large number of customers with small memory storage . The problems we solve are very critical in TTS adaptation for custom voice , and the techniques we proposed handle these problems effectively and achieve good experiment results . Besides , we also point out that some previous works assume the source speech data and adaptation data are in the same domain , which is not practical in custom voice scenarios . Our method works well for this challenging scenario : the adaptation data and source data are in different domains . The experiment results in Table 1 in the paper also echo this point : 1 . When the source and target domain are the same ( both are LibriTTS datasets ) , the adaptation quality of Baseline ( spk emb ) are very close to the ground-truth audio . 2.When the source and target domain are different ( pre-trained on LibriTTS but adapted on VCTK and LJSpeech ) , the adaptation quality of Baseline ( spk emb ) has large gap to the ground-truth audio . In this scenario , AdaSpeech achieves much better quality compared with Baseline ( spk emb ) , and greatly reduces the parameters in inference by 14.1M/4.9K=2878 times compared with Baseline ( decoder ) . * * [ The fitness of our paper to machine learning conference ] * * We want to point out that : 1 . Many previous application-oriented papers on TTS ( including TTS adaptation ) , e.g. , [ 1,2,3,4,5,6,7,8 ] , have been published in a variety of machine learning conferences , including ICLR , ICML and NeurIPS . 2.Many algorithmic/applied papers without theoretical analysis are published in ICLR , e.g. , [ 9,10 ] . 3.Many papers with only experimental studies are published in ICLR , e.g. , [ 11,12 ] . These are all good works . Our work solves an important problem in TTS adaptation and has good method formulation and solid experiment results , which we think well fits the standard of ICLR 2021 conference . [ 1 ] Deep Voice : Real-time Neural Text-to-Speech , ICML 2017 [ 2 ] Deep Voice 2 : Multi-Speaker Neural Text-to-Speech , NIPS 2017 [ 3 ] Deep Voice 3 : Scaling Text-to-Speech with Convolutional Sequence Learning , ICLR 2018 [ 4 ] Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron , ICML 2018 [ 5 ] Neural Voice Cloning with a Few Samples , NeurIPS 2018 [ 6 ] Transfer Learning from Speaker Verification to Multi-speaker Text-To-Speech Synthesis , NeurIPS 2018 [ 7 ] Sample Efficient Adaptive Text-to-Speech , ICLR 2019 [ 8 ] FastSpeech : Fast , Robust and Controllable Text to Speech , NeurIPS 2019 [ 9 ] A Universal Music Translation Network , ICLR 2019 [ 10 ] BERTScore : Evaluating Text Generation with BERT , ICLR 2020 [ 11 ] An Empirical Study of Example Forgetting during Deep Neural Network Learning , ICLR 2019 [ 12 ] Cross-Lingual Ability of Multilingual BERT : An Empirical Study , ICLR 2020"}, {"review_id": "Drynvt7gg4L-1", "review_text": "The authors propose an interesting text-to-speech adaptation method for high quality and efficient customization of new voice . The proposed method consists of two-stage modeling : multi-phonetic-level acoustic condition modeling and conditional layer normalization . In the first stage modeling , the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches . In the second stage modeling , they employ conditional layer normalization for efficient adaptation . Overall , I vote for ACCEPTING . The idea of TTS adaptation for customization of new voice is appealing to me . The proposed approach seems new and technically decent . Their experimental results are good and meaningful . The overall structure of the paper is systematic and well-written despite its minor errors and contains plenty of solid experimental results and discussion . Pros - This paper takes one important issue of current speech synthesis area : TTS adaptation to new voice . - Its multi-phonetic-level acoustic condition modeling approach seem technically new and interesting , - Its comprehensive experimental results well showed the effectiveness of the proposed approach . Cons This paper still has some issues that are conceptually not so convincing , which need to be clarified in the rebuttal session . - In paper , it is said that the phoneme-level acoustic encoder uses phoneme-level Mel features as its input . In the inference , the phoneme-level acoustic predictor uses phoneme hiddens as its input to predict phoneme-level vectors . I think the Mel features used in phoneme-level acoustic encoder contain personal voice information . On the contrary , the phoneme hiddens used in phoneme-level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1 . Please clarify this issue . - The overall structure of acoustic condition modeling in Figure 2 ( a ) is not so clear . For higher reproducibility , authors need to describe it in more detail . The addition of three , that is , speaker , utterance , and phoneme level vectors to the output of phoneme encoder can be done either in element-by-element or by concatenation . - In section 2.3 , authors described that during inference , they do not use the two matrices , but it seems difficult to understand how gamma and beta variables could be calculated using eq.1 without them ? Some typos : - In pages 2 and 7 , describe full representation of MOS , SMOS , and CMOS , respectively . - In page 3 , random chosen -- > randomly chosen - In pages 2 , 4 , while ensure -- > while ensuring", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive comments . We reply to your questions and comments as follows . * * [ How the phoneme-level acoustic predictor can predict the acoustic information beyond prosody such as personal voice information purely based on phoneme sequence ? Please clarify this issue . ] * * We clarify this problem with the following points : 1 . We let the phoneme- and utterance-level acoustic encoders to extract necessary information from different granularities to predict the correct mel-spectrogram , instead of overfitting due to lack of enough acoustic information . 2.The utterance-level acoustic encoder extracts a single vector to summarize the whole utterance , which tends to model general personal voice information . Our experiments also show that utterance-level vector can model personal voice information . 3.The phoneme-level acoustic encoder extracts a sequence of low-dimension vectors ( the dimension is only 4 as shown in the Model Configurations part in Section 3 of the paper ) from the averaged speech sequence ( averaged from frame level into phoneme level ) , which tends to model fine-grained information like prosody , instead of personal voice information . 4.The low hidden dimension of the phoneme-level vectors is very important to ensure only phoneme-level information like prosody is extracted , instead of general personal voice information . We conducted preliminary studies on the dimension of the phoneme-level vectors and found that larger dimensions result in worse adaptation quality . 5.Therefore , in inference , the phoneme-level acoustic predictor can predict information like prosody based on phoneme encoder . * * [ The overall structure of acoustic condition modeling in Figure 2 ( a ) is not so clear . ] * * The three are added together by element-wise addition . We have made it clearer in the revised paper ( in the caption of Figure 2 ) . * * [ In section 2.3 , authors described that during inference , they do not use the two matrices , but it seems difficult to understand how gamma and beta variables could be calculated using eq.1 without them ? ] * * During inference , we do not DIRECTLY use them for deployment since they still have large parameters . Instead , since the two matrices $ W_ { c } ^ { \\gamma } $ , $ W_ { c } ^ { \\beta } $ and $ E^ { s } $ are all fixed for each specific speaker during inference , we use the two matrices to calculate the scale and bias vectors from the speaker embedding $ E^ { s } $ ahead of time and use the scale and bias vectors for deployment . In this way , we can further reduce the memory storage for each speaker ( as shown in Table 1 in the paper ) . We have improved the text description and made it clear in the revised paper . * * [ Describe full representation of MOS , SMOS , and CMOS ] * * MOS : mean opinion score . SMOS : similarity mean opinion score . CMOS : comparison mean opinion score . We have added the full representation in the revised paper . * * [ Other typos ] * * We have fixed these typos and improved the paper writing in the revised version ."}, {"review_id": "Drynvt7gg4L-2", "review_text": "In this paper , the authors present AdaSpeech , a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters . The model is based on the TTS model in FastSpeech 2 , with several additional components . The authors show that AdaSpeech has improved results over other baselines . They also provide an interesting ablation study . Overall , the model architecture is interesting and results seem to show its validity . However , I was wondering why the authors did n't compare their results to other known multi-speaker systems ( e.g.multispeech or deepvoice 2 which were mentioned in the paper ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your positive comments . We reply to your question on `` why did not compare with other known multi-speaker systems ( e.g.multispeech or deepvoice 2 ) '' as follows . Our techniques are for high-quality and efficient TTS adaptation but not aim to model multi-speaker TTS itself , and thus are not directly comparable with other multi-speaker systems such as MultiSpeech or DeepVoice 2 . However , we can apply our techniques including acoustic condition modeling and conditional layer normalization on other multi-speaker systems for TTS adaptation . We applied on MultiSpeech ( which is basically a Transformer based multi-speaker system , and thus our techniques can be directly applied without much change ) and compared the adaptation quality between our method and the setting that fine-tunes speaker embedding or decoder . Experiment results show that : 1 . Compared with the setting that fine-tunes speaker embedding , our method achieves a CMOS score of 0.395 , which shows much better adaptation quality . 2.Compared with the setting that fine-tunes decoder , our method achieves a CMOS score of -0.02 ( usually CMOS score in the range [ -0.05 , +0.05 ] means on par quality ) , and greatly reduces the adaptation parameters for deployment ( 4.9K vs 14.1M ) , which is consistent with the results in our paper . These experiments demonstrate that our method is generally applicable to other multi-speaker systems ."}, {"review_id": "Drynvt7gg4L-3", "review_text": "This paper proposes AdaSpeech , a Transformer-based TTS architecture derived from FastSpeech , but multi-speaker , and focussed on the task of low-resource , robust , and low-dimensional speaker adaptation . The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder . I do n't have a clear intuition for exactly what kind of effect this would have on the phoneme embeddings and their mapping to spectral features , given that there are several non-linearities involved , but it certainly is a strong restriction . A global acoustic embedding conditions the decoder in addition to speaker embeddings , in the hopes of accounting for recording conditions , and , I suppose , timbre , which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference . There is also a phoneme-level acoustic embedding which is used in the same way , which at inference is taken from random sentences ( why not in training ? ) , and , I guess , is supposed to cover phoneme-level idiosyncrasies of the speaker , although this is n't clear to me . However notice that , if I 'm not mistaken , these acoustic embeddings are used zero-shot ; it is * only * the speaker embedding that is the input to fine-tuning , and this only via the normalization parameters . ( Both the normalization parameters * and * the speaker embedding itself are fine-tuned . ) I am not sure what the speaker-embedding is left to do with all this acoustic-level input , but OK . The authors assert in section 2.2 that zero-shot is not enough , but they do not cite a paper that does exactly what they did . This would be useful , or , even more welcome , an ablation study with the acoustic embeddings but not the speaker embedding . Looking at Figure 4b just underscores this point for me . The result is that , within the margin of error , this method is just as good in terms of speaker similarity as fine-tuning the entire decoder . Having listened to the examples they gave , I do find that there are speakers for which it is clearly not as good , but this is not reflected in the evaluator 's results . Overall , this is very exciting work , as it not only promises space-efficient voice cloning , but , in doing so , suggests better disentanglement of speaker and phoneme properties in multi-speaker synthesis .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your detailed and positive comments . We reply to your questions as follows . * * [ The authors assert in section 2.2 that zero-shot is not enough , but they do not cite a paper that does exactly what they did ] * * In zero-shot setting [ 1 ] [ 2 ] [ 3 ] , they usually use a speaker encoder ( not fine-tuned ) to extract the characteristics of speaker from a reference audio , which is used to synthesize speech with the corresponding speaker characteristics . We have added these citations in the revised paper . [ 1 ] Neural Voice Cloning with a Few Samples , NeurIPS 2018 [ 2 ] Transfer Learning from Speaker Verification to Multi-speaker Text-To-Speech Synthesis , NeurIPS 2018 [ 3 ] Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings , ICASSP 2020 * * [ This would be useful , or even more welcome , an ablation study with the acoustic embeddings but not the speaker embedding . ] * * We have conducted this experiment . Using the acoustic embeddings but removing the speaker embedding causes a CMOS drop of -0.249 on the VCTK datasets compared with the setting that uses both acoustic embeddings and speaker embedding , which verifies the effectiveness of speaker embedding to capture the speaker-level acoustic information . * * [ Having listened to the examples they gave , I do find that there are speakers for which it is clearly not as good , but this is not reflected in the evaluator 's results . ] * * MOS is obtained by averaging the scores among all the test utterances . Since we randomly choose the demo cases , there may exist some rare cases where AdaSpeech performs close to or slightly worse than Baseline ( decoder ) . But generally , AdaSpeech is slightly better than Baseline ( decoder ) . We also show more demo voices by AdaSpeech and Baseline ( decoder ) on the demo page ( https : //adaspeech.github.io/ # AnonReviewer2 ) . Note that our advantage over Baseline ( decoder ) is that we can greatly reduce the adaptation parameters ( 4.9K vs 14.1M ) , and achieve on par or slightly better adaptation quality ."}], "0": {"review_id": "Drynvt7gg4L-0", "review_text": "# # # Summary AdaSpeech is a paper on practical TTS custom voice adaptation with the aim of reducing the amount of adapted parameters per voice to allow cloud serving of a large number of custom voices while maintaining high adaptation quality and similarity . The novel piece that enables this is the conditioning of layernorm in the model on the speaker embedding . The grammar reads slightly awkwardly in places , but the paper is understandable and well structured . Descriptions of the model , experiments , and analysis of results are well done . # # # Recommendation * * Weak Reject * * I believe this paper is not novel enough / too applied / focused on experimental results for this conference . There is little discussion on the theoretical side of the acoustic condition modelling , such as how the authors are able to determine that the utterance-level and phoneme-level vectors are modelling things like room condition . Instead , the strengths of this paper are entirely through the strong numerical results . I think this paper would be a solid accept for a more specialized conference like ICASSP or Interspeech . # # # Positives 1 . Well written , great analysis of results and ablation studies . # # # Negatives 1 . What is the loss used to train the phoneme level acoustic predictor ? MSE ? 1.How is it determined that acoustic conditions such as loudness or room conditions are actually captured by the utterance- and phoneme-level acoustic condition modelling ? My intuition would be that your phoneme-level predictor is trained only with phoneme hiddens ( textual information only ) ( do these phoneme hiddens include speaker embedding information ? ) , so at most it models some pitch or prosody information . The utterance-level can definitely model the rest , but where is the evidence ? It could end up modelling only one very specific dimension and still improve the MOS . 1.Similarly , I highly doubt the utterance-level acoustic condition modelling does not also capture speaker information . What happens when using a speaker embedding with the utterance-level vector extracted from a reference speech for a different speaker ? 1.The paper would be better with a discussion on controllability . As a reference utterance needs to be provided , does this mean the synthesized speech will take on the prosody in the reference ? What happens if you want to synthesize a prosody for a speaker that 's not present in any of that speaker 's reference utterances ? I understand this is a big topic with ongoing research which is why it would be a big bonus if this paper can make any kind of progress in that area . 1.I am curious how your phoneme-level predictor would compare with a VAE-based setup , although I understand this can be difficult to set up so no action is required here . # # # Misc * * 2.3 Pipeline of AdaSpeech * * : `` we do not use the two matrices in each conditional layer normalization '' -- which two matrices ? * * 4.2 Method Analysis * * : What does it mean to remove conditional layer normalization ? Then you do n't have any adaptation parameters , so is it not equivalent to Baseline ( spk emb ) case ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments on our paper . We reply to your questions as follows . * * [ About the novelty ] * * We clarify the novelty of our work as follows : 1 . We leverage acoustic condition modeling ( utterance-level and phoneme-level ) to better model the acoustic conditions and improve the generalizability and adaptability of the TTS model , in order to support diverse customers with different acoustic conditions for TTS adaptation . 2.We introduce conditional layer normalization to adapt the TTS model with high voice quality using as few adaptation parameters as possible , in order to support a large number of customers with small memory storage . The problems we solve are very critical in TTS adaptation for custom voice , and the techniques we proposed handle these problems effectively and achieve good experiment results . Besides , we also point out that some previous works assume the source speech data and adaptation data are in the same domain , which is not practical in custom voice scenarios . Our method works well for this challenging scenario : the adaptation data and source data are in different domains . The experiment results in Table 1 in the paper also echo this point : 1 . When the source and target domain are the same ( both are LibriTTS datasets ) , the adaptation quality of Baseline ( spk emb ) are very close to the ground-truth audio . 2.When the source and target domain are different ( pre-trained on LibriTTS but adapted on VCTK and LJSpeech ) , the adaptation quality of Baseline ( spk emb ) has large gap to the ground-truth audio . In this scenario , AdaSpeech achieves much better quality compared with Baseline ( spk emb ) , and greatly reduces the parameters in inference by 14.1M/4.9K=2878 times compared with Baseline ( decoder ) . * * [ The fitness of our paper to machine learning conference ] * * We want to point out that : 1 . Many previous application-oriented papers on TTS ( including TTS adaptation ) , e.g. , [ 1,2,3,4,5,6,7,8 ] , have been published in a variety of machine learning conferences , including ICLR , ICML and NeurIPS . 2.Many algorithmic/applied papers without theoretical analysis are published in ICLR , e.g. , [ 9,10 ] . 3.Many papers with only experimental studies are published in ICLR , e.g. , [ 11,12 ] . These are all good works . Our work solves an important problem in TTS adaptation and has good method formulation and solid experiment results , which we think well fits the standard of ICLR 2021 conference . [ 1 ] Deep Voice : Real-time Neural Text-to-Speech , ICML 2017 [ 2 ] Deep Voice 2 : Multi-Speaker Neural Text-to-Speech , NIPS 2017 [ 3 ] Deep Voice 3 : Scaling Text-to-Speech with Convolutional Sequence Learning , ICLR 2018 [ 4 ] Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron , ICML 2018 [ 5 ] Neural Voice Cloning with a Few Samples , NeurIPS 2018 [ 6 ] Transfer Learning from Speaker Verification to Multi-speaker Text-To-Speech Synthesis , NeurIPS 2018 [ 7 ] Sample Efficient Adaptive Text-to-Speech , ICLR 2019 [ 8 ] FastSpeech : Fast , Robust and Controllable Text to Speech , NeurIPS 2019 [ 9 ] A Universal Music Translation Network , ICLR 2019 [ 10 ] BERTScore : Evaluating Text Generation with BERT , ICLR 2020 [ 11 ] An Empirical Study of Example Forgetting during Deep Neural Network Learning , ICLR 2019 [ 12 ] Cross-Lingual Ability of Multilingual BERT : An Empirical Study , ICLR 2020"}, "1": {"review_id": "Drynvt7gg4L-1", "review_text": "The authors propose an interesting text-to-speech adaptation method for high quality and efficient customization of new voice . The proposed method consists of two-stage modeling : multi-phonetic-level acoustic condition modeling and conditional layer normalization . In the first stage modeling , the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches . In the second stage modeling , they employ conditional layer normalization for efficient adaptation . Overall , I vote for ACCEPTING . The idea of TTS adaptation for customization of new voice is appealing to me . The proposed approach seems new and technically decent . Their experimental results are good and meaningful . The overall structure of the paper is systematic and well-written despite its minor errors and contains plenty of solid experimental results and discussion . Pros - This paper takes one important issue of current speech synthesis area : TTS adaptation to new voice . - Its multi-phonetic-level acoustic condition modeling approach seem technically new and interesting , - Its comprehensive experimental results well showed the effectiveness of the proposed approach . Cons This paper still has some issues that are conceptually not so convincing , which need to be clarified in the rebuttal session . - In paper , it is said that the phoneme-level acoustic encoder uses phoneme-level Mel features as its input . In the inference , the phoneme-level acoustic predictor uses phoneme hiddens as its input to predict phoneme-level vectors . I think the Mel features used in phoneme-level acoustic encoder contain personal voice information . On the contrary , the phoneme hiddens used in phoneme-level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1 . Please clarify this issue . - The overall structure of acoustic condition modeling in Figure 2 ( a ) is not so clear . For higher reproducibility , authors need to describe it in more detail . The addition of three , that is , speaker , utterance , and phoneme level vectors to the output of phoneme encoder can be done either in element-by-element or by concatenation . - In section 2.3 , authors described that during inference , they do not use the two matrices , but it seems difficult to understand how gamma and beta variables could be calculated using eq.1 without them ? Some typos : - In pages 2 and 7 , describe full representation of MOS , SMOS , and CMOS , respectively . - In page 3 , random chosen -- > randomly chosen - In pages 2 , 4 , while ensure -- > while ensuring", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive comments . We reply to your questions and comments as follows . * * [ How the phoneme-level acoustic predictor can predict the acoustic information beyond prosody such as personal voice information purely based on phoneme sequence ? Please clarify this issue . ] * * We clarify this problem with the following points : 1 . We let the phoneme- and utterance-level acoustic encoders to extract necessary information from different granularities to predict the correct mel-spectrogram , instead of overfitting due to lack of enough acoustic information . 2.The utterance-level acoustic encoder extracts a single vector to summarize the whole utterance , which tends to model general personal voice information . Our experiments also show that utterance-level vector can model personal voice information . 3.The phoneme-level acoustic encoder extracts a sequence of low-dimension vectors ( the dimension is only 4 as shown in the Model Configurations part in Section 3 of the paper ) from the averaged speech sequence ( averaged from frame level into phoneme level ) , which tends to model fine-grained information like prosody , instead of personal voice information . 4.The low hidden dimension of the phoneme-level vectors is very important to ensure only phoneme-level information like prosody is extracted , instead of general personal voice information . We conducted preliminary studies on the dimension of the phoneme-level vectors and found that larger dimensions result in worse adaptation quality . 5.Therefore , in inference , the phoneme-level acoustic predictor can predict information like prosody based on phoneme encoder . * * [ The overall structure of acoustic condition modeling in Figure 2 ( a ) is not so clear . ] * * The three are added together by element-wise addition . We have made it clearer in the revised paper ( in the caption of Figure 2 ) . * * [ In section 2.3 , authors described that during inference , they do not use the two matrices , but it seems difficult to understand how gamma and beta variables could be calculated using eq.1 without them ? ] * * During inference , we do not DIRECTLY use them for deployment since they still have large parameters . Instead , since the two matrices $ W_ { c } ^ { \\gamma } $ , $ W_ { c } ^ { \\beta } $ and $ E^ { s } $ are all fixed for each specific speaker during inference , we use the two matrices to calculate the scale and bias vectors from the speaker embedding $ E^ { s } $ ahead of time and use the scale and bias vectors for deployment . In this way , we can further reduce the memory storage for each speaker ( as shown in Table 1 in the paper ) . We have improved the text description and made it clear in the revised paper . * * [ Describe full representation of MOS , SMOS , and CMOS ] * * MOS : mean opinion score . SMOS : similarity mean opinion score . CMOS : comparison mean opinion score . We have added the full representation in the revised paper . * * [ Other typos ] * * We have fixed these typos and improved the paper writing in the revised version ."}, "2": {"review_id": "Drynvt7gg4L-2", "review_text": "In this paper , the authors present AdaSpeech , a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters . The model is based on the TTS model in FastSpeech 2 , with several additional components . The authors show that AdaSpeech has improved results over other baselines . They also provide an interesting ablation study . Overall , the model architecture is interesting and results seem to show its validity . However , I was wondering why the authors did n't compare their results to other known multi-speaker systems ( e.g.multispeech or deepvoice 2 which were mentioned in the paper ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your positive comments . We reply to your question on `` why did not compare with other known multi-speaker systems ( e.g.multispeech or deepvoice 2 ) '' as follows . Our techniques are for high-quality and efficient TTS adaptation but not aim to model multi-speaker TTS itself , and thus are not directly comparable with other multi-speaker systems such as MultiSpeech or DeepVoice 2 . However , we can apply our techniques including acoustic condition modeling and conditional layer normalization on other multi-speaker systems for TTS adaptation . We applied on MultiSpeech ( which is basically a Transformer based multi-speaker system , and thus our techniques can be directly applied without much change ) and compared the adaptation quality between our method and the setting that fine-tunes speaker embedding or decoder . Experiment results show that : 1 . Compared with the setting that fine-tunes speaker embedding , our method achieves a CMOS score of 0.395 , which shows much better adaptation quality . 2.Compared with the setting that fine-tunes decoder , our method achieves a CMOS score of -0.02 ( usually CMOS score in the range [ -0.05 , +0.05 ] means on par quality ) , and greatly reduces the adaptation parameters for deployment ( 4.9K vs 14.1M ) , which is consistent with the results in our paper . These experiments demonstrate that our method is generally applicable to other multi-speaker systems ."}, "3": {"review_id": "Drynvt7gg4L-3", "review_text": "This paper proposes AdaSpeech , a Transformer-based TTS architecture derived from FastSpeech , but multi-speaker , and focussed on the task of low-resource , robust , and low-dimensional speaker adaptation . The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder . I do n't have a clear intuition for exactly what kind of effect this would have on the phoneme embeddings and their mapping to spectral features , given that there are several non-linearities involved , but it certainly is a strong restriction . A global acoustic embedding conditions the decoder in addition to speaker embeddings , in the hopes of accounting for recording conditions , and , I suppose , timbre , which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference . There is also a phoneme-level acoustic embedding which is used in the same way , which at inference is taken from random sentences ( why not in training ? ) , and , I guess , is supposed to cover phoneme-level idiosyncrasies of the speaker , although this is n't clear to me . However notice that , if I 'm not mistaken , these acoustic embeddings are used zero-shot ; it is * only * the speaker embedding that is the input to fine-tuning , and this only via the normalization parameters . ( Both the normalization parameters * and * the speaker embedding itself are fine-tuned . ) I am not sure what the speaker-embedding is left to do with all this acoustic-level input , but OK . The authors assert in section 2.2 that zero-shot is not enough , but they do not cite a paper that does exactly what they did . This would be useful , or , even more welcome , an ablation study with the acoustic embeddings but not the speaker embedding . Looking at Figure 4b just underscores this point for me . The result is that , within the margin of error , this method is just as good in terms of speaker similarity as fine-tuning the entire decoder . Having listened to the examples they gave , I do find that there are speakers for which it is clearly not as good , but this is not reflected in the evaluator 's results . Overall , this is very exciting work , as it not only promises space-efficient voice cloning , but , in doing so , suggests better disentanglement of speaker and phoneme properties in multi-speaker synthesis .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your detailed and positive comments . We reply to your questions as follows . * * [ The authors assert in section 2.2 that zero-shot is not enough , but they do not cite a paper that does exactly what they did ] * * In zero-shot setting [ 1 ] [ 2 ] [ 3 ] , they usually use a speaker encoder ( not fine-tuned ) to extract the characteristics of speaker from a reference audio , which is used to synthesize speech with the corresponding speaker characteristics . We have added these citations in the revised paper . [ 1 ] Neural Voice Cloning with a Few Samples , NeurIPS 2018 [ 2 ] Transfer Learning from Speaker Verification to Multi-speaker Text-To-Speech Synthesis , NeurIPS 2018 [ 3 ] Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings , ICASSP 2020 * * [ This would be useful , or even more welcome , an ablation study with the acoustic embeddings but not the speaker embedding . ] * * We have conducted this experiment . Using the acoustic embeddings but removing the speaker embedding causes a CMOS drop of -0.249 on the VCTK datasets compared with the setting that uses both acoustic embeddings and speaker embedding , which verifies the effectiveness of speaker embedding to capture the speaker-level acoustic information . * * [ Having listened to the examples they gave , I do find that there are speakers for which it is clearly not as good , but this is not reflected in the evaluator 's results . ] * * MOS is obtained by averaging the scores among all the test utterances . Since we randomly choose the demo cases , there may exist some rare cases where AdaSpeech performs close to or slightly worse than Baseline ( decoder ) . But generally , AdaSpeech is slightly better than Baseline ( decoder ) . We also show more demo voices by AdaSpeech and Baseline ( decoder ) on the demo page ( https : //adaspeech.github.io/ # AnonReviewer2 ) . Note that our advantage over Baseline ( decoder ) is that we can greatly reduce the adaptation parameters ( 4.9K vs 14.1M ) , and achieve on par or slightly better adaptation quality ."}}