{"year": "2021", "forum": "9FWas6YbmB3", "title": "DrNAS: Dirichlet Neural Architecture Search", "decision": "Accept (Poster)", "meta_review": "This paper proposes Dirichlet Neural Architecture Search (DrNAS), a new NAS algorithm that formulates NAS as a distribution architecture search problem. The paper shows theoretically that DrNAS implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search) and presents very strong empirical results on several benchmarks, including the tabular benchmark NAS-Bench-201. \n\nThe reviews and discussion put this paper very close to the acceptance threshold, so I read it in detail myself to act as a tie-breaker.\nI see a lot of positive aspects of this paper:\n+ it tackles a very important and timely problem\n+ the method implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search)\n+ empirical results are very strong\n+ the paper includes insightful ablation studies for the most important parts of the algorithm\n+ the progressive architecture learning approach is a general contribution that reduces the memory complexity and also improves basic DARTS\n+ the paper uses a tabular benchmark to yield results that are directly comparable to those of other papers\n+ the method's hyperparameters are kept fixed across the different benchmarks, which underlines its robustness.\n\nThere are also some negative aspects:\n- The method was not originally derived from a Bayesian point of view. Per request of the reviewers, the relation to variational inference has been added to the appendix, but the difference between the L2 regularization and using the explicit KL regularization that falls out of the Bayesian treatment remains. Nevertheless, the new appendix helps to clear up the relationship.\n\n- The paper does not mention availability of the code. This is a must in modern NAS research, as many papers have exposed the poor reproducibility of research in NAS. Fortunately for the authors, I have already seen (independently of this submission) that the code is available on github, but I urge the authors to provide an anonymous repo for review in future submissions, since it was purely by chance that I saw it this time.\n\n- Regarding the point of exploration vs. exploitation, the authors emphasize that in contrast to Gumbel-softmax based methods, such as GDAS and SNAS, with DrNAS there is no need for a cooling schedule. While it is nice to keep the number of hyperparameters small, this also appears to give up control of when the method switches from exploration to exploitation. In practical applications of AutoML, there will be a time budget, and while a cooling schedule can be adapted to fit this budget, it would be suboptimal if DrNAS is still in the exploration phase by the end of the budget, or has already switched to exploitation after, e.g., 5% of the budget. It would be good if the authors could briefly discuss this issue in their final version (if only by acknowledging that this can be a problem).\n\n- Minor negative points\n  * The paper sometimes uses jargon, and I believe not even always correctly: e.g., even by googling I did not find such a thing as the \"iregularized incomplete beta function\", it's also not in the original reference by Jankowiak. I only found the \"incomplete beta function\" (and the regular one). \n  * The author names for several references are garbled. This is likely due to not replacing a comma between the authors with an \"and\" in the bibtex file.\n  * The paper lacks citations for several of the methods it uses, e.g., Adam, cutout, cosine annealing, label smoothing, auxiliary towers, etc. There is no limit on references, and it is standard to cite these concepts to remain more self-contained.\n * The experimental results of GDAS on NB201-CIFAR-100 do not seem to align with the numbers in the NB201 paper. Did you use the numbers from the paper or rerun this method yourself? Please clarify and check this for the final version. The point of tabular benchmarks is to have comparability and consistency across papers! \n * Please have the paper proofread for Grammar again, there are several avoidable errors. E.g., in the first sentence, \"lots of attentions\" --> \"lots of attention\". Also things like \"alone\" -> \"along\", \"down\"->\"done\" etc.\n\nOverall, I think this is a very nice paper, introducing an empirically very strong NAS method that is also theoretically shown to implicitly regularize the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search). I am therefore recommending acceptance. I would like to ask the authors to go through all the reviews again and fix any remaining points in the paper for the final version.", "reviews": [{"review_id": "9FWas6YbmB3-0", "review_text": "Summary : The paper proposes Dirichlet Neural Architecture Search which formulates Neural architecture search as a distribution architecture search problem . They derive a bound that shows that their formulation implicitly regularizes the Hessian norm with respect to the architecture parameters which has been shown to allow more robust architecture search . They empirically show that their method keeps the dominant eigenvalue of the Hessian much lower compared to DARTS . To reduce memory usage and increase training speed , they incorporate partial channel connections and propose progressively widening the channel connections during training . They demonstrate strong empirical results on several benchmark datasets and NASbench201 . Reasons for score : Overall I would recommend accepting this paper . It proposes a new efficient distribution learning-based NAS algorithm which regularizes the Hessian with respect to the architecture parameters . The algorithm is demonstrated by efficiently finding high performing networks on Cifar10 , Cifar100 , and Imagenet . They provide ablation experiments that demonstrate the effects of different parts of their final algorithm . While the benefits of using the Dirichlet distribution could be somewhat better demonstrated empirically separate from the progressive search , compared to existing NAS distribution learning algorithms which do n't discretely sample , it is beneficial that the trade-off between exploration and exploitation can be controlled by a penalty term as compared to the popular Gumbel-softmax based methods which arbitrarily anneal the temperature parameter . Pros : 1.Proposed approach is shown to regularize the Hessian with respect to the architecture parameters which works have shown to lead to better network generalization . 2.The paper provides many results that show that it can find high performing networks quickly with limited compute quite quickly . 3.Provides ablation experiments are which demonstrate the benefits of different parts of the algorithm . Cons : 1.The paper would be improved greatly if the robustness of the search and final network architectures was empirically explored . This could be accomplished by analyzing the architecture distribution while searching on NASBench-201 . 2.The direct effect of the regularizer parameter on the hessian could be better explored . For example something like figure A4 with different values for $ \\lambda $ would be helpful . 2.The paper would benefit from a proper baseline for the proposed progressive search method . Currently it is somewhat unclear if DrNAS with more time and memory would perform better if used to search the architecture space without a proxy or if the progressive search is regularizing the search to perform better . The reverse experiment of using the progressive search with another distribution learning algorithm would also be beneficial . Questions : Was there a particular reason $ \\lambda $ = 1e-3 was used in the experiments ? In table 4 , do you know if there is a particular reason there is no variance in the results for DARTS , ENAS , and your algorithm . It seems a bit strange that DARTS and ENAS all appear to be stuck in the same quite poor performing local minima in this experiment on CIFAR10 . Updates : Thanks to the authors for addressing my concerns and responding to my questions . The newly added experimental results make the paper stronger and addressed many of my concerns . I recommend this paper to be accepted .", "rating": "7: Good paper, accept", "reply_text": "* a . `` Was there a particular reason \\lambda = 1e-3 was used in the experiments ? `` * 1e-3 is commonly used as the scale of the weight decay in differentiable NAS , and we also use this number for our regularization term from the very beginning ( without tuning it ) . The ablation study on this parameter can be found in Appendix A.3 . As shown in Table 5 , DrNAS \u2019 s performance is insensitive to $ \\lambda $ . \\ \\ * b . `` In table 4 , do you know if there is a particular reason there is no variance in the results for DARTS , ENAS , and your algorithm . It seems a bit strange that DARTS and ENAS all appear to be stuck in the same quite poor performing local minima in this experiment on CIFAR10 . `` * For Table 4 , we run the search algorithm 4 times with different random seeds . The final architectures discovered by DrNAS under these 4 independent runs are actually identical . In this case , when we query the 201 database for the architectures ' performance , we get the same result , and hence there is no variance . It also shows that the performance of DrNAS is robust to the randomness of the search phase . ENAS and DARTS also have no variance on NAS-Bench-201 . On the 201 space , DARTS always degenerates to a network filled with skip connections when searched on CIFAR10 , CIFAR100 , and ImageNet16-120 . This is the same for ENAS on CIFAR10 . Note that in the original NAS-Bench-201 paper [ 2 ] , ENAS and DARTS also have 0 variance . \\ \\ [ References ] 1 . Chen et al.Stabilizing Differentiable Architecture Search via Perturbation-based Regularization . ICML 2020 2 . Dong et al.NAS-Bench-201 : Extending the Scope of Reproducible Neural Architecture Search . ICLR 2020 3 . Xu el al.PC-DARTS : Partial Channel Connections for Memory-Efficient Architecture Search . ICLR 2020"}, {"review_id": "9FWas6YbmB3-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The authors present a new differentiable Neural Architecture Search ( NAS ) method which places a Dirichlet prior on the edges of a cell for NAS : DrNAS . The authors adapt the original bilevel optimisation to incorporate this new prior over the operation mixing weight . The authors add a regularisation to the Dirichlet parameters and effectively study its effect in an ablation . The authors also introduce a progressive architecture learning to make the optimisation more computationally efficient . DrNAS produces very strong results on three different NAS scenarios and is compared to a wide variety of baselines . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall I vote for a weak reject . I think there are some issues surrounding the technical presentation of the work - see the Cons . Also I have some issues with the notation ( see below ) , fixing these will help a lot with the readability of the paper . I have asked for some clarification and I am happy to raise my score in light of clarification from the authors . The experimental analysis on the other hand is very strong . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : Interesting use of the Dirichlet prior for selecting an edge in a cell in the search space for NAS . A hyperparameter is introduced to regularise the Dirichlet parameters and an suitable ablation is provided to demonstrate the regularisation \u2019 s sensitivity . Inference of the prior \u2019 s parameters is performed using the new pathwise estimator [ 4 ] . Implicitly controlling the largest eigenvalue of the Hessian is very important for producing good results in differentiable NAS [ 1 ] . The authors experimentally show that DrNAS is very effective in doing so . I have a question with regards to Proposition 1 ( Question 3 ) , if it is indeed correct then the authors also provide an interesting bound to demonstrate that the objective in Equation 2 is implicitly attenuating the largest eigenvalue of the Hessian of the validation loss wrt to the Dirichlet parameters . DrNAS produces very strong results compared to a variety of benchmarks on three different NAS scenarios . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : Claims in the text are not justified in the paper , see Question 2 and Question 3 . General issues surrounding notation * * . The formulation of DrNAS is not strictly formulated within a Bayesian framework with prior and likelihood to infer a posterior over operation mixing weights . The L2 regularization can be formulated as a Gaussian prior about the Dirichlet parameters . Instead the Dirichlet prior is added to the bilevel optimisation in Equation 1 in an ad hoc manner . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions : Section 2.2 , paragraph 1 : if \\theta is drawn from a distribution why is it less prone to overfitting ? The references are text books . In the paper , for instance in the abstract , you claim that DrNAS encourages better exploration . Of course the model has some stochasticity by sampling from a Dirichlet , but what explicit evidence do you have that having \\theta ~ Dirichlet leads to better exploration of the search space for NAS ? In proposition 1 : it is assumed that \\nabla^2_\\mu \\tilde { L } _ { val } is psd , in general the Hessian of a loss function is pd and psd if L is strictly convex and convex wrt \\mu respectively . Does this assumption hold for \\tilde { L } _ { val } ? You use the pathwise derivative estimator , which is not very popular ( I might be wrong here ) , and you also work the Laplace approximation for Proposition 1 and in related work [ 3 ] . What is the advantage of the pathwise derivative estimator versus the Laplace for inference over Dirichlet parameters ? You include stds in Table 4 , but not in Table 3 ? How are the errors in Table 3 calculated ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Some typos and notations issues and other comments * * : Section 2.2 , paragraph 3 , first sentence no \u201c the \u201d before \u201c exploration \u201d needed . Section 2.2 , paragraph 3 , \\beta_0 is not defined . Equation 2 , d ( . ) not defined . Equation 3 needs a space . Title in section 2.3 needs a \u201c the \u201d . Section 2.3 \\alpha and \\mu are not defined . Although these works regard Continual Learning ( CL ) : it is worth mentioning two related works including [ 5 ] uses a Dirichlet prior over networks for CL and [ 6 ] learns new neurons in a Bayesian NN using an Indian Buffet Process prior . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # [ 1 ] A. Zela , T. Elsken , T. Saikia , Y. Marrakchi , T. Brox , and F. Hutter , \u201c Understanding And Robustifying Differentiable Architecture Search , \u201d in ICLR , 2020 . [ 2 ] S. Lee , J. Ha , D. Zhang , and G. Kim , \u201c A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning , \u201d in ICLR , 2020 . [ 3 ] C. Sutton and A. Srivastava , \u201c Autoencoding Variational Inference for Topic Models , \u201d in ICLR , 2017 . [ 4 ] M. Jankowiak and F. Obermeyer , \u201c Pathwise Derivatives Beyond the Reparameterization Trick , \u201d ICML . 2018 . [ 5 ] S. Lee , J. Ha , D. Zhang , and G. Kim , \u201c A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning , \u201d in International Conference on Learning Representations , 2020 . [ 6 ] S. Kessler , V. Nguyen , S. Zohren , and S. Roberts , \u201c Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning , \u201d arxiv:1912.02290 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed feedback . Below are our responses regarding your questions and concerns . [ Cons ] * a . `` The formulation of DrNAS is not strictly formulated within a Bayesian framework with prior and likelihood to infer a posterior over operation mixing weights . `` * Thank you for your suggestion on formulating DrNAS in the Bayesian framework . There is a connection between our method and variational inference . We add the connection to Bayesian in Appendix A.5 of the revised paper . \\ \\ [ Questions ] * a . `` if \\theta is drawn from a distribution why is it less prone to overfitting ? The references are text books . `` * This claim is from two lines of research . In the AutoML literature , Nguyen et al . [ 1 ] observe that in hyperparameter optimization , the overfitting to the validation set is highly correlated with the curvature of the validation loss . Zela et al . [ 2 ] explore it further in the NAS scenario that regularizing the largest dominant eigenvalue can reduce overfitting , as mentioned in our paper . Sampling $ \\theta $ from Dirichlet implicitly regularizes such Hessian and thus is less prone to overfitting compared with the point estimate of $ \\theta $ in DARTS , as shown in Figure 2 of the revised paper . In Bayesian literature , if we view $ \\theta $ as a parameter in the supernet just like the model weights , then modeling this parameter using the Dirichlet distribution is related to the Bayesian Inference for Neural networks . There are some discussions on the effect of the Bayesian Neural Network in reducing overfitting [ 3 , 4 , 5 ] . \\ \\ * b . `` In the paper , for instance in the abstract , you claim that DrNAS encourages better exploration . Of course the model has some stochasticity by sampling from a Dirichlet , but what explicit evidence do you have that having \\theta ~ Dirichlet leads to better exploration of the search space for NAS ? `` * Thank you for the suggestion . We add a figure ( Figure 1 ) in Section 4.4 of the revised paper to demonstrate it . As we can see , initially the Dirichlet explores more diverse architectures in the space . The exploration is reduced gradually towards the end as DrNAS becomes more confident about the current choice . The shift from exploration to exploitation is controlled by the Dirichlet concentration $ \\beta $ , which is learned from the data . \\ \\ * c. `` In proposition 1 : it is assumed that \\nabla^2_\\mu \\tilde { L } { val } is psd , in general the Hessian of a loss function is pd and psd if L is strictly convex and convex wrt \\mu respectively . Does this assumption hold for \\tilde { L } { val } ? `` * Empirically , we find that the Hessian is close to PSD . For example , on NAS-Bench-201 , 92 % of its eigenvalues are positive . The eigen spectrum ranges from -0.016 to 0.03 . We further study such implicit regularization empirically . As shown in Figure 2 of Appendix A.4 , the dominant eigenvalue of Hessian retains at a low level throughout the search phase , whereas its value of DARTS expands about 10 times . \\ \\ * d. `` You use the pathwise derivative estimator , which is not very popular ( I might be wrong here ) , and you also work the Laplace approximation for Proposition 1 and in related work [ 3 ] . What is the advantage of the pathwise derivative estimator versus the Laplace for inference over Dirichlet parameters ? `` * Pathwise Derivative for Dirichlet distribution is the de-facto implementation in popular deep learning libraries such as TensorFlow and the latest version of Pytorch . It is very efficient to compute and it is almost as fast as softmax in both forward and backward pass . We use the Laplacian approximation exclusively in our proposition because it facilitates an explicit mathematical form of reparametrization for Dirichlet distribution , which allows us to derive the bound . \\ \\ * e. `` You include stds in Table 4 , but not in Table 3 ? How are the errors in Table 3 calculated ? `` * Most works report a single number on ImageNet due to time complexity , e.g. , Table 2 in the PCDARTS paper [ 6 ] . We perform another run of the ImageNet experiment and the variance is +- 0.1 % . \\ \\ [ Typos and notations ] Thank you for pointing it out . We fix those issues in the revised draft ( colored except for grammatical changes ) . The citations you recommend are also added to the paper ( Section 2.2 paragraph 2 ) . \\ \\ [ References ] 1 . Nyguyen et al.Stable bayesian optimization . JSDSA 2018. doi : 10.1007/s41060-018-0119-9 . 2.Zela et al.Understanding and Robustifying Differentiable Architecture Search . ICLR 2020 3 . Blundell et al.Weight Uncertainty in Neural Networks . ICML 2015 4 . Neal.Bayesian learning for neural networks . 1995 5.Mackay.Bayesian interpolation . Neural Computation . 1992 6.Xu et al.Partial channel connections for memory-efficient architecture search . ICLR 2020"}, {"review_id": "9FWas6YbmB3-2", "review_text": "This paper proposes a differentiable NAS algorithm based on the Dirichlet architecture distribution . Different from the previous differentiable and stochastic NAS algorithms that used the Gumbel-softmax trick or the Categorical distribution , the proposed DrNAS does not require any temperature scheduling for balancing exploration and exploitation , and moreover it does not suffer premature convergence and instability during search . In addition , in order to reduce the memory consumption when searching based on the super-net that mixes all possible operations , the proposed DrNAS applies the progressive learning scheme by combining the network widening with the operation pruning . Pros.The proposed algorithm is technically sound , and the motivation and both of the theoretical and empirical analysis are reasonable to support the use of the Dirichlet architecture distribution with the progressive learning . The experimental results also show that the proposed DrNAS consistently outperforms all previous NAS algorithms on CIFAR-10 , ImageNet , and especially NAS-Bench-201 . Cons.My main concern of the proposed method is how to produce the sparse solution during search to reduce the architectural bias between the search and retraining phases . This naturally brings up the question of how and when it automatically changes the exploration to the exploitation during search , like a temperature annealing for the Gumbel-softmax trick . How can the proposed method remove the retraining ? When performing NAS on ImageNet , why did the proposed method use the proxy task with the reduced training set , even though it can retain a low memory overhead like a discrete architecture sampling such as DSNAS ? How is the performance variance of the proposed NAS method on ImageNet ? Minor : what if the proposed method searches different cell structures for each layer without the repetition ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive feedback . Below are our responses regarding your concerns . [ Cons ] * a . `` My main concern of the proposed method is how to produce the sparse solution during search to reduce the architectural bias between the search and retraining phases . This naturally brings up the question of how and when it automatically changes the exploration to the exploitation during search , like a temperature annealing for the Gumbel-softmax trick . How can the proposed method remove the retraining ? `` * Thank you for bringing it up . As you pointed out , some methods enforce sparsity during the search phrase ( i.e. , single-path sampling-based methods such as GDAS , SNAS at low temperature , and PARSEC ) . However , these methods suffer from training instability due to model forgetting [ 1 , 2 ] . In DrNAS , the sparsity and variance of the sampled $ \\theta $ are fully controlled by the Dirichlet concentration $ \\beta $ , which is learned from the data . It is able to significantly outperform the above mentioned single-path methods across datasets and search spaces ( e.g. , we achieve 2.46 % average test error on CIFAR10 , while the number is 2.93 % , 2.85 % , and 2.81 % for GDAS , SNAS , and PARSEC ) . In terms of exploration v.s . exploitation . As shown in Figure 1 of the revised paper , DrNAS learns to encourage exploration at the early stages and then gradually reduce it towards the end as the algorithm becomes more confident of the current choice . This is different from Gumbel-softmax as one has to manually tune the temperature annealing schedule carefully . Annealing the temperature too soon results in early convergence , whereas doing it too late will hurt exploitation . For example , although both employ Gumbel-softmax , GDAS uses a completely different scheduling ( 10 - > 1 ) from SNAS ( 1 - > 0.03 ) . Following most previous differentiable NAS works like DARTS [ 5 ] and PCDARTS [ 3 ] , the best architecture discovered by DrNAS will be retrained from scratch . Our focus is to identify a high accuracy architecture from the search space , rather than producing an already-to-deploy network . We achieve better results than those methods proposed to remove retraining ( ImageNet top-1 test error , DrNAS : 23.7 % , SNAS : 27.3 % , DSNAS : 25.7 % ) . \\ \\ * b . `` When performing NAS on ImageNet , why did the proposed method use the proxy task with the reduced training set , even though it can retain a low memory overhead like a discrete architecture sampling such as DSNAS ? How is the performance variance of the proposed NAS method on ImageNet ? `` * Searching on the full ImageNet is computationally expensive on the commonly used DARTS \u2019 space . Therefore , we follow [ 3 , 4 ] to sample a subset of ImageNet for the search task . In comparison , DSNAS uses the mobilenet space , which is simpler than the DARTS \u2019 space . Empirically the subset is enough for our algorithm to identify a top-performing architecture on ImageNet . We achieve a 23.7 % top-1 error , which is much better than the 25.7 % top-1 error of DSNAS . Most works report a single number on ImageNet due to time complexity , e.g. , Table 2 in the PCDARTS paper [ 3 ] . We perform another run of the ImageNet experiment and the variance is +- 0.1 % . \\ \\ [ minor ] a . \u201c what if the proposed method searches different cell structures for each layer without the repetition ? \u201d We follow the common settings in the differentiable NAS community to construct the search space by repeating normal and reduction cells . We think extending DrNAS to a search space where each cell has a different structure is plausible , and we are happy to investigate it as a next step . \\ \\ [ References ] 1 . Zhang et al.Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization . CVPR 2020 2 . Niu et al.Disturbance-immune Weight Sharing for Neural Architecture Search . Arxiv:2003.13089 3 . Xu et al.Partial channel connections for memory-efficient architecture search . ICLR 2020 4 . FBNet : Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search . CVPR 2019 5 . Liu et al.DARTS : Differentiable Architecture Search . ICLR 2018"}, {"review_id": "9FWas6YbmB3-3", "review_text": "Summary : This work proposes a modified DARTS optimiser for NAS which assumes a factorised dirichlet distribution over architecture parameters . It uses pathwise derivatives to learn an MLE estimate of these concentration parameters and adds appropriate regulariser terms to stabilise the training . Furthermore , it employs a protocol for progressively increasing channel fraction to stabilise training within a computation budget . The paper is easy to follow , and relevant experiments have been included . - With the given probabilistic formulation of this work , it would be useful to include details on how the factorisation of appropriate distributions varies between PARSEC and DrNAS ? It might be useful to ground the modelling assumptions within a prior and approximate-posterior framework for better clarity . - On the same line it would be useful to get insights on how the obtained models qualitatively differ from ProxylassNAS , PARSEC and SNAS . - The interplay of progressive learning with modelling assumption is a bit unclear . The number of parameters and test error in Table 2 are inversely correlated across SNAS , ProxylessNAS , PARSEC and DrNAS which is perhaps not as surprising . I was wondering if authors have any insights on what aspect of the algo ( with and without the two stage progressive learning ) contributed to network size . - Would it be possible to employ the progressive learning policy mentioned in section 4.1 across other DART flavours and understand its impact on model performance ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive feedback . Below are our responses to your comments . * a . `` With the given probabilistic formulation of this work , it would be useful to include details on how the factorisation of appropriate distributions varies between PARSEC and DrNAS ? It might be useful to ground the modelling assumptions within a prior and approximate-posterior framework for better clarity . `` * Thank you for your suggestions . As you pointed out , there is a connection between our model with Variational Inference . We add it in Appendix A.5 . DrNAS produces samples of continuous $ \\theta $ from the Dirichlet distribution , and the continuous $ \\theta $ will serve as the operation mixing weights directly to train the supernet . The gradient can be backpropagated through the Dirichlet samples efficiently via pathwise derivatives . On the other hand , as mentioned in the related work section , PARSEC samples discrete architectures from a categorical distribution , which is learned by Monte Carlo estimation . The primary goal of its discrete sampling is to reduce the memory cost . However , such discrete sampling has been shown to cause training instability due to model forgetting [ 1 , 2 ] , leading to inferior performance ( 2.81 % test error on CIFAR10 , 0.35 % worse than DrNAS ) . \\ \\ * b . `` On the same line it would be useful to get insights on how the obtained models qualitatively differ from ProxylassNAS , PARSEC and SNAS . `` * We focus on comparing the normal cell found by these methods here because normal cells dominate the final architecture ( 18 out of 20 cells ) . On DARTS \u2019 search space , the normal cell found by SNAS is a shallow and wide one , and half of the edges are filled with non-parametric skip connections ( Figure 2.a of their paper ) . The lack of depth and parametric operations lead to its comparably poor performance ( 2.85 % on CIFAR10 ) . In contrast , the cell found by DrNAS is deeper and consists of many parametric operations ( as shown in Figure 4.a of the revised paper ) , leading to significantly improved performance ( 2.46 % average test error on CIFAR10 ) . Moreover , the derived architecture of PARSEC ( 2.81 % on CIFAR10 ) performs worse than ours ( 2.46 % on CIFAR10 ) , possibly due to an excessive number of dilated convolutions ( Figure 4 at the bottom of their paper ) . Most previous works on differentiable NAS use DARTS \u2019 search space . But ProxylessNAS uses its own search space to evaluate the search algorithm , so its obtained model is not quite comparable to ours . \\ \\ * c. `` The interplay of progressive learning with modelling assumption is a bit unclear . The number of parameters and test error in Table 2 are inversely correlated across SNAS , ProxylessNAS , PARSEC and DrNAS which is perhaps not as surprising . I was wondering if authors have any insights on what aspect of the algo ( with and without the two stage progressive learning ) contributed to network size . `` * Network with more parameters is not guaranteed to produce higher accuracy ( e.g. , SDARTS-ADV [ 3 ] has an error of 2.61 % with 3.3M parameters but PARSEC has an error of 2.81 % with 3.7M parameters ) . The focus of our paper is to identify a high accuracy network from the given search space . We propose progressive learning to enable a direct search on the target task , which can narrow the gap between search and evaluation and lead to better performance . As shown in Table 2 and Table 3 , the size of the networks discovered by DrNAS with and without progressive learning are roughly the same ( 4.1M v.s.4.0M on CIFAR10 and 7.1M v.s . 7.3M on ImageNet ) , so we think that the model size is primarily related to Dirichlet distribution rather than progressive learning . \\ \\ * d. `` Would it be possible to employ the progressive learning policy mentioned in section 4.1 across other DART flavors and understand its impact on model performance ? `` * Thank you for your suggestions , we ablate the progressive algorithm by applying it to DARTS and another sampling-based method SDARTS-RS [ 3 ] . With a direct search on the target task , the test error of DARTS on CIFAR10 reduces from 3.0 % to 2.72 % . We also achieve improvement on SDARTS-RS by reducing its error from 2.67 % to 2.64 % . All the results are the average over 4 independent runs . We observe more improvement on the original DARTS . This is because apart from the benefit of a direct search , the partial connection in our progressive learning is shown to have some regularization effect on DARTS [ 4 ] . In comparison , the sampling-based method SDARTS-RS and our DrNAS have already regularized the search ( regularize the Hessian norm ) . \\ \\ [ References ] 1 . Zhang et al.Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization . CVPR 2020 2 . Niu et al.Disturbance-immune Weight Sharing for Neural Architecture Search . Arxiv:2003.13089 3 . Chen et al.Stabilizing Differentiable Architecture Search via Perturbation-based Regularization . ICML 2020 4 . Xu el al.PC-DARTS : Partial Channel Connections for Memory-Efficient Architecture Search . ICLR 2020"}], "0": {"review_id": "9FWas6YbmB3-0", "review_text": "Summary : The paper proposes Dirichlet Neural Architecture Search which formulates Neural architecture search as a distribution architecture search problem . They derive a bound that shows that their formulation implicitly regularizes the Hessian norm with respect to the architecture parameters which has been shown to allow more robust architecture search . They empirically show that their method keeps the dominant eigenvalue of the Hessian much lower compared to DARTS . To reduce memory usage and increase training speed , they incorporate partial channel connections and propose progressively widening the channel connections during training . They demonstrate strong empirical results on several benchmark datasets and NASbench201 . Reasons for score : Overall I would recommend accepting this paper . It proposes a new efficient distribution learning-based NAS algorithm which regularizes the Hessian with respect to the architecture parameters . The algorithm is demonstrated by efficiently finding high performing networks on Cifar10 , Cifar100 , and Imagenet . They provide ablation experiments that demonstrate the effects of different parts of their final algorithm . While the benefits of using the Dirichlet distribution could be somewhat better demonstrated empirically separate from the progressive search , compared to existing NAS distribution learning algorithms which do n't discretely sample , it is beneficial that the trade-off between exploration and exploitation can be controlled by a penalty term as compared to the popular Gumbel-softmax based methods which arbitrarily anneal the temperature parameter . Pros : 1.Proposed approach is shown to regularize the Hessian with respect to the architecture parameters which works have shown to lead to better network generalization . 2.The paper provides many results that show that it can find high performing networks quickly with limited compute quite quickly . 3.Provides ablation experiments are which demonstrate the benefits of different parts of the algorithm . Cons : 1.The paper would be improved greatly if the robustness of the search and final network architectures was empirically explored . This could be accomplished by analyzing the architecture distribution while searching on NASBench-201 . 2.The direct effect of the regularizer parameter on the hessian could be better explored . For example something like figure A4 with different values for $ \\lambda $ would be helpful . 2.The paper would benefit from a proper baseline for the proposed progressive search method . Currently it is somewhat unclear if DrNAS with more time and memory would perform better if used to search the architecture space without a proxy or if the progressive search is regularizing the search to perform better . The reverse experiment of using the progressive search with another distribution learning algorithm would also be beneficial . Questions : Was there a particular reason $ \\lambda $ = 1e-3 was used in the experiments ? In table 4 , do you know if there is a particular reason there is no variance in the results for DARTS , ENAS , and your algorithm . It seems a bit strange that DARTS and ENAS all appear to be stuck in the same quite poor performing local minima in this experiment on CIFAR10 . Updates : Thanks to the authors for addressing my concerns and responding to my questions . The newly added experimental results make the paper stronger and addressed many of my concerns . I recommend this paper to be accepted .", "rating": "7: Good paper, accept", "reply_text": "* a . `` Was there a particular reason \\lambda = 1e-3 was used in the experiments ? `` * 1e-3 is commonly used as the scale of the weight decay in differentiable NAS , and we also use this number for our regularization term from the very beginning ( without tuning it ) . The ablation study on this parameter can be found in Appendix A.3 . As shown in Table 5 , DrNAS \u2019 s performance is insensitive to $ \\lambda $ . \\ \\ * b . `` In table 4 , do you know if there is a particular reason there is no variance in the results for DARTS , ENAS , and your algorithm . It seems a bit strange that DARTS and ENAS all appear to be stuck in the same quite poor performing local minima in this experiment on CIFAR10 . `` * For Table 4 , we run the search algorithm 4 times with different random seeds . The final architectures discovered by DrNAS under these 4 independent runs are actually identical . In this case , when we query the 201 database for the architectures ' performance , we get the same result , and hence there is no variance . It also shows that the performance of DrNAS is robust to the randomness of the search phase . ENAS and DARTS also have no variance on NAS-Bench-201 . On the 201 space , DARTS always degenerates to a network filled with skip connections when searched on CIFAR10 , CIFAR100 , and ImageNet16-120 . This is the same for ENAS on CIFAR10 . Note that in the original NAS-Bench-201 paper [ 2 ] , ENAS and DARTS also have 0 variance . \\ \\ [ References ] 1 . Chen et al.Stabilizing Differentiable Architecture Search via Perturbation-based Regularization . ICML 2020 2 . Dong et al.NAS-Bench-201 : Extending the Scope of Reproducible Neural Architecture Search . ICLR 2020 3 . Xu el al.PC-DARTS : Partial Channel Connections for Memory-Efficient Architecture Search . ICLR 2020"}, "1": {"review_id": "9FWas6YbmB3-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The authors present a new differentiable Neural Architecture Search ( NAS ) method which places a Dirichlet prior on the edges of a cell for NAS : DrNAS . The authors adapt the original bilevel optimisation to incorporate this new prior over the operation mixing weight . The authors add a regularisation to the Dirichlet parameters and effectively study its effect in an ablation . The authors also introduce a progressive architecture learning to make the optimisation more computationally efficient . DrNAS produces very strong results on three different NAS scenarios and is compared to a wide variety of baselines . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall I vote for a weak reject . I think there are some issues surrounding the technical presentation of the work - see the Cons . Also I have some issues with the notation ( see below ) , fixing these will help a lot with the readability of the paper . I have asked for some clarification and I am happy to raise my score in light of clarification from the authors . The experimental analysis on the other hand is very strong . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : Interesting use of the Dirichlet prior for selecting an edge in a cell in the search space for NAS . A hyperparameter is introduced to regularise the Dirichlet parameters and an suitable ablation is provided to demonstrate the regularisation \u2019 s sensitivity . Inference of the prior \u2019 s parameters is performed using the new pathwise estimator [ 4 ] . Implicitly controlling the largest eigenvalue of the Hessian is very important for producing good results in differentiable NAS [ 1 ] . The authors experimentally show that DrNAS is very effective in doing so . I have a question with regards to Proposition 1 ( Question 3 ) , if it is indeed correct then the authors also provide an interesting bound to demonstrate that the objective in Equation 2 is implicitly attenuating the largest eigenvalue of the Hessian of the validation loss wrt to the Dirichlet parameters . DrNAS produces very strong results compared to a variety of benchmarks on three different NAS scenarios . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : Claims in the text are not justified in the paper , see Question 2 and Question 3 . General issues surrounding notation * * . The formulation of DrNAS is not strictly formulated within a Bayesian framework with prior and likelihood to infer a posterior over operation mixing weights . The L2 regularization can be formulated as a Gaussian prior about the Dirichlet parameters . Instead the Dirichlet prior is added to the bilevel optimisation in Equation 1 in an ad hoc manner . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions : Section 2.2 , paragraph 1 : if \\theta is drawn from a distribution why is it less prone to overfitting ? The references are text books . In the paper , for instance in the abstract , you claim that DrNAS encourages better exploration . Of course the model has some stochasticity by sampling from a Dirichlet , but what explicit evidence do you have that having \\theta ~ Dirichlet leads to better exploration of the search space for NAS ? In proposition 1 : it is assumed that \\nabla^2_\\mu \\tilde { L } _ { val } is psd , in general the Hessian of a loss function is pd and psd if L is strictly convex and convex wrt \\mu respectively . Does this assumption hold for \\tilde { L } _ { val } ? You use the pathwise derivative estimator , which is not very popular ( I might be wrong here ) , and you also work the Laplace approximation for Proposition 1 and in related work [ 3 ] . What is the advantage of the pathwise derivative estimator versus the Laplace for inference over Dirichlet parameters ? You include stds in Table 4 , but not in Table 3 ? How are the errors in Table 3 calculated ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Some typos and notations issues and other comments * * : Section 2.2 , paragraph 3 , first sentence no \u201c the \u201d before \u201c exploration \u201d needed . Section 2.2 , paragraph 3 , \\beta_0 is not defined . Equation 2 , d ( . ) not defined . Equation 3 needs a space . Title in section 2.3 needs a \u201c the \u201d . Section 2.3 \\alpha and \\mu are not defined . Although these works regard Continual Learning ( CL ) : it is worth mentioning two related works including [ 5 ] uses a Dirichlet prior over networks for CL and [ 6 ] learns new neurons in a Bayesian NN using an Indian Buffet Process prior . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # [ 1 ] A. Zela , T. Elsken , T. Saikia , Y. Marrakchi , T. Brox , and F. Hutter , \u201c Understanding And Robustifying Differentiable Architecture Search , \u201d in ICLR , 2020 . [ 2 ] S. Lee , J. Ha , D. Zhang , and G. Kim , \u201c A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning , \u201d in ICLR , 2020 . [ 3 ] C. Sutton and A. Srivastava , \u201c Autoencoding Variational Inference for Topic Models , \u201d in ICLR , 2017 . [ 4 ] M. Jankowiak and F. Obermeyer , \u201c Pathwise Derivatives Beyond the Reparameterization Trick , \u201d ICML . 2018 . [ 5 ] S. Lee , J. Ha , D. Zhang , and G. Kim , \u201c A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning , \u201d in International Conference on Learning Representations , 2020 . [ 6 ] S. Kessler , V. Nguyen , S. Zohren , and S. Roberts , \u201c Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning , \u201d arxiv:1912.02290 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed feedback . Below are our responses regarding your questions and concerns . [ Cons ] * a . `` The formulation of DrNAS is not strictly formulated within a Bayesian framework with prior and likelihood to infer a posterior over operation mixing weights . `` * Thank you for your suggestion on formulating DrNAS in the Bayesian framework . There is a connection between our method and variational inference . We add the connection to Bayesian in Appendix A.5 of the revised paper . \\ \\ [ Questions ] * a . `` if \\theta is drawn from a distribution why is it less prone to overfitting ? The references are text books . `` * This claim is from two lines of research . In the AutoML literature , Nguyen et al . [ 1 ] observe that in hyperparameter optimization , the overfitting to the validation set is highly correlated with the curvature of the validation loss . Zela et al . [ 2 ] explore it further in the NAS scenario that regularizing the largest dominant eigenvalue can reduce overfitting , as mentioned in our paper . Sampling $ \\theta $ from Dirichlet implicitly regularizes such Hessian and thus is less prone to overfitting compared with the point estimate of $ \\theta $ in DARTS , as shown in Figure 2 of the revised paper . In Bayesian literature , if we view $ \\theta $ as a parameter in the supernet just like the model weights , then modeling this parameter using the Dirichlet distribution is related to the Bayesian Inference for Neural networks . There are some discussions on the effect of the Bayesian Neural Network in reducing overfitting [ 3 , 4 , 5 ] . \\ \\ * b . `` In the paper , for instance in the abstract , you claim that DrNAS encourages better exploration . Of course the model has some stochasticity by sampling from a Dirichlet , but what explicit evidence do you have that having \\theta ~ Dirichlet leads to better exploration of the search space for NAS ? `` * Thank you for the suggestion . We add a figure ( Figure 1 ) in Section 4.4 of the revised paper to demonstrate it . As we can see , initially the Dirichlet explores more diverse architectures in the space . The exploration is reduced gradually towards the end as DrNAS becomes more confident about the current choice . The shift from exploration to exploitation is controlled by the Dirichlet concentration $ \\beta $ , which is learned from the data . \\ \\ * c. `` In proposition 1 : it is assumed that \\nabla^2_\\mu \\tilde { L } { val } is psd , in general the Hessian of a loss function is pd and psd if L is strictly convex and convex wrt \\mu respectively . Does this assumption hold for \\tilde { L } { val } ? `` * Empirically , we find that the Hessian is close to PSD . For example , on NAS-Bench-201 , 92 % of its eigenvalues are positive . The eigen spectrum ranges from -0.016 to 0.03 . We further study such implicit regularization empirically . As shown in Figure 2 of Appendix A.4 , the dominant eigenvalue of Hessian retains at a low level throughout the search phase , whereas its value of DARTS expands about 10 times . \\ \\ * d. `` You use the pathwise derivative estimator , which is not very popular ( I might be wrong here ) , and you also work the Laplace approximation for Proposition 1 and in related work [ 3 ] . What is the advantage of the pathwise derivative estimator versus the Laplace for inference over Dirichlet parameters ? `` * Pathwise Derivative for Dirichlet distribution is the de-facto implementation in popular deep learning libraries such as TensorFlow and the latest version of Pytorch . It is very efficient to compute and it is almost as fast as softmax in both forward and backward pass . We use the Laplacian approximation exclusively in our proposition because it facilitates an explicit mathematical form of reparametrization for Dirichlet distribution , which allows us to derive the bound . \\ \\ * e. `` You include stds in Table 4 , but not in Table 3 ? How are the errors in Table 3 calculated ? `` * Most works report a single number on ImageNet due to time complexity , e.g. , Table 2 in the PCDARTS paper [ 6 ] . We perform another run of the ImageNet experiment and the variance is +- 0.1 % . \\ \\ [ Typos and notations ] Thank you for pointing it out . We fix those issues in the revised draft ( colored except for grammatical changes ) . The citations you recommend are also added to the paper ( Section 2.2 paragraph 2 ) . \\ \\ [ References ] 1 . Nyguyen et al.Stable bayesian optimization . JSDSA 2018. doi : 10.1007/s41060-018-0119-9 . 2.Zela et al.Understanding and Robustifying Differentiable Architecture Search . ICLR 2020 3 . Blundell et al.Weight Uncertainty in Neural Networks . ICML 2015 4 . Neal.Bayesian learning for neural networks . 1995 5.Mackay.Bayesian interpolation . Neural Computation . 1992 6.Xu et al.Partial channel connections for memory-efficient architecture search . ICLR 2020"}, "2": {"review_id": "9FWas6YbmB3-2", "review_text": "This paper proposes a differentiable NAS algorithm based on the Dirichlet architecture distribution . Different from the previous differentiable and stochastic NAS algorithms that used the Gumbel-softmax trick or the Categorical distribution , the proposed DrNAS does not require any temperature scheduling for balancing exploration and exploitation , and moreover it does not suffer premature convergence and instability during search . In addition , in order to reduce the memory consumption when searching based on the super-net that mixes all possible operations , the proposed DrNAS applies the progressive learning scheme by combining the network widening with the operation pruning . Pros.The proposed algorithm is technically sound , and the motivation and both of the theoretical and empirical analysis are reasonable to support the use of the Dirichlet architecture distribution with the progressive learning . The experimental results also show that the proposed DrNAS consistently outperforms all previous NAS algorithms on CIFAR-10 , ImageNet , and especially NAS-Bench-201 . Cons.My main concern of the proposed method is how to produce the sparse solution during search to reduce the architectural bias between the search and retraining phases . This naturally brings up the question of how and when it automatically changes the exploration to the exploitation during search , like a temperature annealing for the Gumbel-softmax trick . How can the proposed method remove the retraining ? When performing NAS on ImageNet , why did the proposed method use the proxy task with the reduced training set , even though it can retain a low memory overhead like a discrete architecture sampling such as DSNAS ? How is the performance variance of the proposed NAS method on ImageNet ? Minor : what if the proposed method searches different cell structures for each layer without the repetition ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive feedback . Below are our responses regarding your concerns . [ Cons ] * a . `` My main concern of the proposed method is how to produce the sparse solution during search to reduce the architectural bias between the search and retraining phases . This naturally brings up the question of how and when it automatically changes the exploration to the exploitation during search , like a temperature annealing for the Gumbel-softmax trick . How can the proposed method remove the retraining ? `` * Thank you for bringing it up . As you pointed out , some methods enforce sparsity during the search phrase ( i.e. , single-path sampling-based methods such as GDAS , SNAS at low temperature , and PARSEC ) . However , these methods suffer from training instability due to model forgetting [ 1 , 2 ] . In DrNAS , the sparsity and variance of the sampled $ \\theta $ are fully controlled by the Dirichlet concentration $ \\beta $ , which is learned from the data . It is able to significantly outperform the above mentioned single-path methods across datasets and search spaces ( e.g. , we achieve 2.46 % average test error on CIFAR10 , while the number is 2.93 % , 2.85 % , and 2.81 % for GDAS , SNAS , and PARSEC ) . In terms of exploration v.s . exploitation . As shown in Figure 1 of the revised paper , DrNAS learns to encourage exploration at the early stages and then gradually reduce it towards the end as the algorithm becomes more confident of the current choice . This is different from Gumbel-softmax as one has to manually tune the temperature annealing schedule carefully . Annealing the temperature too soon results in early convergence , whereas doing it too late will hurt exploitation . For example , although both employ Gumbel-softmax , GDAS uses a completely different scheduling ( 10 - > 1 ) from SNAS ( 1 - > 0.03 ) . Following most previous differentiable NAS works like DARTS [ 5 ] and PCDARTS [ 3 ] , the best architecture discovered by DrNAS will be retrained from scratch . Our focus is to identify a high accuracy architecture from the search space , rather than producing an already-to-deploy network . We achieve better results than those methods proposed to remove retraining ( ImageNet top-1 test error , DrNAS : 23.7 % , SNAS : 27.3 % , DSNAS : 25.7 % ) . \\ \\ * b . `` When performing NAS on ImageNet , why did the proposed method use the proxy task with the reduced training set , even though it can retain a low memory overhead like a discrete architecture sampling such as DSNAS ? How is the performance variance of the proposed NAS method on ImageNet ? `` * Searching on the full ImageNet is computationally expensive on the commonly used DARTS \u2019 space . Therefore , we follow [ 3 , 4 ] to sample a subset of ImageNet for the search task . In comparison , DSNAS uses the mobilenet space , which is simpler than the DARTS \u2019 space . Empirically the subset is enough for our algorithm to identify a top-performing architecture on ImageNet . We achieve a 23.7 % top-1 error , which is much better than the 25.7 % top-1 error of DSNAS . Most works report a single number on ImageNet due to time complexity , e.g. , Table 2 in the PCDARTS paper [ 3 ] . We perform another run of the ImageNet experiment and the variance is +- 0.1 % . \\ \\ [ minor ] a . \u201c what if the proposed method searches different cell structures for each layer without the repetition ? \u201d We follow the common settings in the differentiable NAS community to construct the search space by repeating normal and reduction cells . We think extending DrNAS to a search space where each cell has a different structure is plausible , and we are happy to investigate it as a next step . \\ \\ [ References ] 1 . Zhang et al.Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization . CVPR 2020 2 . Niu et al.Disturbance-immune Weight Sharing for Neural Architecture Search . Arxiv:2003.13089 3 . Xu et al.Partial channel connections for memory-efficient architecture search . ICLR 2020 4 . FBNet : Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search . CVPR 2019 5 . Liu et al.DARTS : Differentiable Architecture Search . ICLR 2018"}, "3": {"review_id": "9FWas6YbmB3-3", "review_text": "Summary : This work proposes a modified DARTS optimiser for NAS which assumes a factorised dirichlet distribution over architecture parameters . It uses pathwise derivatives to learn an MLE estimate of these concentration parameters and adds appropriate regulariser terms to stabilise the training . Furthermore , it employs a protocol for progressively increasing channel fraction to stabilise training within a computation budget . The paper is easy to follow , and relevant experiments have been included . - With the given probabilistic formulation of this work , it would be useful to include details on how the factorisation of appropriate distributions varies between PARSEC and DrNAS ? It might be useful to ground the modelling assumptions within a prior and approximate-posterior framework for better clarity . - On the same line it would be useful to get insights on how the obtained models qualitatively differ from ProxylassNAS , PARSEC and SNAS . - The interplay of progressive learning with modelling assumption is a bit unclear . The number of parameters and test error in Table 2 are inversely correlated across SNAS , ProxylessNAS , PARSEC and DrNAS which is perhaps not as surprising . I was wondering if authors have any insights on what aspect of the algo ( with and without the two stage progressive learning ) contributed to network size . - Would it be possible to employ the progressive learning policy mentioned in section 4.1 across other DART flavours and understand its impact on model performance ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive feedback . Below are our responses to your comments . * a . `` With the given probabilistic formulation of this work , it would be useful to include details on how the factorisation of appropriate distributions varies between PARSEC and DrNAS ? It might be useful to ground the modelling assumptions within a prior and approximate-posterior framework for better clarity . `` * Thank you for your suggestions . As you pointed out , there is a connection between our model with Variational Inference . We add it in Appendix A.5 . DrNAS produces samples of continuous $ \\theta $ from the Dirichlet distribution , and the continuous $ \\theta $ will serve as the operation mixing weights directly to train the supernet . The gradient can be backpropagated through the Dirichlet samples efficiently via pathwise derivatives . On the other hand , as mentioned in the related work section , PARSEC samples discrete architectures from a categorical distribution , which is learned by Monte Carlo estimation . The primary goal of its discrete sampling is to reduce the memory cost . However , such discrete sampling has been shown to cause training instability due to model forgetting [ 1 , 2 ] , leading to inferior performance ( 2.81 % test error on CIFAR10 , 0.35 % worse than DrNAS ) . \\ \\ * b . `` On the same line it would be useful to get insights on how the obtained models qualitatively differ from ProxylassNAS , PARSEC and SNAS . `` * We focus on comparing the normal cell found by these methods here because normal cells dominate the final architecture ( 18 out of 20 cells ) . On DARTS \u2019 search space , the normal cell found by SNAS is a shallow and wide one , and half of the edges are filled with non-parametric skip connections ( Figure 2.a of their paper ) . The lack of depth and parametric operations lead to its comparably poor performance ( 2.85 % on CIFAR10 ) . In contrast , the cell found by DrNAS is deeper and consists of many parametric operations ( as shown in Figure 4.a of the revised paper ) , leading to significantly improved performance ( 2.46 % average test error on CIFAR10 ) . Moreover , the derived architecture of PARSEC ( 2.81 % on CIFAR10 ) performs worse than ours ( 2.46 % on CIFAR10 ) , possibly due to an excessive number of dilated convolutions ( Figure 4 at the bottom of their paper ) . Most previous works on differentiable NAS use DARTS \u2019 search space . But ProxylessNAS uses its own search space to evaluate the search algorithm , so its obtained model is not quite comparable to ours . \\ \\ * c. `` The interplay of progressive learning with modelling assumption is a bit unclear . The number of parameters and test error in Table 2 are inversely correlated across SNAS , ProxylessNAS , PARSEC and DrNAS which is perhaps not as surprising . I was wondering if authors have any insights on what aspect of the algo ( with and without the two stage progressive learning ) contributed to network size . `` * Network with more parameters is not guaranteed to produce higher accuracy ( e.g. , SDARTS-ADV [ 3 ] has an error of 2.61 % with 3.3M parameters but PARSEC has an error of 2.81 % with 3.7M parameters ) . The focus of our paper is to identify a high accuracy network from the given search space . We propose progressive learning to enable a direct search on the target task , which can narrow the gap between search and evaluation and lead to better performance . As shown in Table 2 and Table 3 , the size of the networks discovered by DrNAS with and without progressive learning are roughly the same ( 4.1M v.s.4.0M on CIFAR10 and 7.1M v.s . 7.3M on ImageNet ) , so we think that the model size is primarily related to Dirichlet distribution rather than progressive learning . \\ \\ * d. `` Would it be possible to employ the progressive learning policy mentioned in section 4.1 across other DART flavors and understand its impact on model performance ? `` * Thank you for your suggestions , we ablate the progressive algorithm by applying it to DARTS and another sampling-based method SDARTS-RS [ 3 ] . With a direct search on the target task , the test error of DARTS on CIFAR10 reduces from 3.0 % to 2.72 % . We also achieve improvement on SDARTS-RS by reducing its error from 2.67 % to 2.64 % . All the results are the average over 4 independent runs . We observe more improvement on the original DARTS . This is because apart from the benefit of a direct search , the partial connection in our progressive learning is shown to have some regularization effect on DARTS [ 4 ] . In comparison , the sampling-based method SDARTS-RS and our DrNAS have already regularized the search ( regularize the Hessian norm ) . \\ \\ [ References ] 1 . Zhang et al.Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization . CVPR 2020 2 . Niu et al.Disturbance-immune Weight Sharing for Neural Architecture Search . Arxiv:2003.13089 3 . Chen et al.Stabilizing Differentiable Architecture Search via Perturbation-based Regularization . ICML 2020 4 . Xu el al.PC-DARTS : Partial Channel Connections for Memory-Efficient Architecture Search . ICLR 2020"}}