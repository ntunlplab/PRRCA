{"year": "2019", "forum": "SJf6BhAqK7", "title": "Variadic Learning by Bayesian Nonparametric Deep Embedding", "decision": "Reject", "meta_review": "All reviewers wrote strong and long reviews with good feedback but do not believe the work is currently ready for publication.\nI encourage the authors to update and resubmit.\n", "reviews": [{"review_id": "SJf6BhAqK7-0", "review_text": "Update after Author Rebuttal -------------- After reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The \"hard/soft\" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of \"borderline rejection\" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification. (Original review text below. Detailed replies to authors are in posts below their responses). Review Summary -------------- While the focus on variadic learning is interesting, I think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for ICLR. I think there is the potential for some neat ideas here and I hope the authors prepare stronger versions in the future. However, the current version is unfortunately not comprehensible or reproducible. Paper Summary ------------- The paper investigates developing an effective ML method for the \"variadic\" regime, where the method might be required to perform learning from few or many examples (shots) and few or many classes (ways). The term \"variadic\" comes from use in computer science for functions that can a flexible number of arguments. There may also be unlabeled data available in the few shot case, creating semi-supervised learning opportunities. The specific method proposed is called BANDE: Bayesian Nonparametric Deep Embedding. The idea is that each data point's feature vector x_i is transformed into an embedding vector h(x_i) using a neural network, and then clustering occurs in the embedding space via a single-pass of the DP-means algorithm (Kulis & Jordan 2012). Each cluster is assumed to correspond to one \"class\" in the eventual classification problem, though each class might have multiple clusters (and thus be multi-modal). Learning occurs in an episodic manner. After each episode (single-pass of DP-means), each point in a query set is embedded to its feature vector, then fed into each cluster's Gaussian likelihoods to produce a normalized cluster-assignment-probability vector that sums to one. This vector is then fed into a cross-entropy loss, where the true class's nearest cluster (largest probability value) is taken to be the true cluster. This loss is used to perform gradient updates of the embedding neural network. There is also a \"cumulative\" version of the method called BANDE-C. This version keeps track of cluster means from previous episodes and allows new episodes to be initialized with these. Experiments examine the proposed approach across image categorization tasks on Omniglot, mini-ImageNet, and CIFAR datasets. Strengths --------- * I like that many clusters are used for each true class label, which is better than rigid one-to-one assumptions. Limitations ----------- * Can only be used for classification, not regression * The DP-means procedure does not account for the cluster-specific variance information that is used at other steps of the algorithm Significance and Originality ---------------------------- To me, the method appears original. Any method that could really succeed across various variadic settings would be significant. Presentation Concerns --------------------- I have serious concerns about the presentation quality of this paper. Each section needs careful reorganization as well as rewording. ## P1: Algo. 1 contains numerous omissions that make it as written not correct. * the number of clusters count variable \"n\" is not updated anywhere. As writting this algo can only update one extra cluster beyond the original n. * the variable \"c\" is unbound in the else clause. You need a line that clarifies that c = argmin_{c in 1 ... n} d_ic Would be careful about saying that \"a single pass is sufficient\"... you have *chosen* to do only one pass. When doing k-means, we could also make this choice. Certainly the DP-means objective could keep improving with multiple passes. ## P2: Many figures and tables lack appropriate captions/labels Table 1: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make very clear here how much labeled data was used. Table 2: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make how many labeled and unlabeled examples were used easier to find. ## P3: Descriptions of episodic learning and overall algorithm clarity Readers unfamiliar with episodic learning are not helped with the limited coverage provided here in 3.1 and 3.2. When exactly is the \"support\" set used and the \"query\" set used? How do unlabeled points get used (both support and query appear fully labeled)? What is n? What is k? What is T? Why are some points in Q denoted with apostrophes but not others? Providing a more formal step-by-step description (perhaps with pseudocode) will be crucial. In Sec. 3.2, the paragraph that starts with \"The loss is defined\" is very hard to read and parse. I suggest adding math to formally define the loss with equations. What parameters are being optimized? Which ones are fixed? Additionally, in Sec. 3.2: \"computed in the same way as standard prototypical networks\"... what is the procedure exactly? If your method relies on a procedure, you should specify it in this paper and not make readers guess or lookup a procedure elsewhere. ## P4: Many steps of the algorithm are not detailed The paper claims to set \\lambda using a technique from another paper, but does not summarize this technique. This makes things nearly impossible to reproduce. Please add such details in the appendix. Major Technical Concerns ------------------------ ## Alg. 1 concerns: Requires two (not one) passes and mixes hard and soft assingments and different variance assumptions awkwardly The BANDE algorithm (Alg. 1) has some unjustified properties. Hard assignment decisions which assume vanishing variances are used to find a closest cluster, but then later soft assignments with non-zero variances are used. This is a bit heuristic and lacks justification... why not use soft assignment throughout? The DP means procedure is derived from a specific objective function that assumes hard assignment. Seems weird to use it for convenience and then discard instead of coming up with the small fix that would make soft assignment consistent throughout. Furthermore, The authors claim it is a one pass algorithm, but in fact as written in Alg. 1 it seems to require two passes: the first pass keeps an original set of cluster centers fixed and then creates new centers whenever an example's distance to the closest center exceeds \\lambda. But then, the *soft* assignment step that updates \"z\" requires again the distance from each point to all centers be computed, which requires another pass (since some new clusters may exist which did not when the point was first visited). While the new soft values will be close to zero, they will not be *exactly* zero, and thus they matter. ## Unclear if/how cluster-specific variance parameters learned From the text on top of page 4, it seems that the paper assumes that there exist cluster-specific variances \\sigma_c. However, these are not mentioned elsewhere, only a general (not cluster-specific) label variance \\sigma and fixed unlabeled variance sigma_u are used. ## Experiments lack comparison to internal baselines The paper doesn't evaluate sensitivity to key fixed hyperparameters (e.g. \\alpha, \\lambda) or compare variants of their approach (with and without soft clustering step, with and without multimodality via DP-means). It is difficult to tell which design choices of the method are most crucial. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed feedback , in particular the attention to the technical aspects of the clustering steps and probabilistic interpretations in our work , and the comments on clarity and accessibility for audiences less familiar with meta-learning and few-shot learning . We agree with the reviewer on the importance of multi-modal clustering as `` better than rigid , one-to-one assumptions '' of prior work , which we show by experiment on alphabet recognition in section 4.3 and improved semi-supervised few-shot classification in section 4.1 . We likewise agree that methods that `` really succeed across various variadic settings would be significant '' which is why we propose it in this work and investigate it by experiment in section 4.2 . Regarding concerns of clarity and reproducibility , we are incorporating the feedback of the reviews into a revision to be posted during the rebuttal period and will release code after decision ( omitted here only to preserve anonymity ) . Our comprehensive code release will cover our model , experimental evaluation and training settings , all few-shot baselines ( including prototypical networks , semi-supervised prototypical networks , and our variadic extensions of MAML and few-shot graph networks ) , and datasets . This will help safeguard reproducibility for future work and serve as a reference implementation of the variadic setting . We now clarify our method and experiments to address the reviewer 's technical concerns . hard/soft assignments and probabilistic interpretation : We thank the reviewer for their theoretical precision . We are in full agreement , and wish to point out that we identify and experiment with fully hard ( sec.3.4 ) and fully soft variants of our method ( appendix A4 ) for this reason of probabilistic justification . We choose the hard-soft hybrid for our main results , as mentioned in the paper , because it is marginally more accurate in experiments . We appreciate the feedback on this point , and are revising the text to make these variants more clear . number of passes/clustering steps : We will clarify our language to use the term \u201c clustering iteration \u201d instead of passes/clustering steps . In the fully hard model , an iteration corresponds to the assignment of all labeled and unlabeled points to clusters , and then an update of the means of all clusters . In the fully soft model , an iteration corresponds to computing soft assignments for all points , and then updating the means . In the hard-soft hybrid , we use the \u201c hard \u201d step to compute a set of cluster means , and then perform a `` soft '' clustering step in order to update these cluster means . cluster-specific variances : \\sigma and \\sigma_u are learned and are shared across all labeled and all unlabeled clusters respectively . \\sigma_c was a typo for \\sigma as it is the variance of class clusters . The only exception to learning these variances , as noted , is Section 4.3 where they are fixed . internal baselines/ablations : We agree with the reviewer on this list of ablations/internal baselines , so much so that we have already experimented with them in the development of the method : the selected multi-modal clustering with hard-soft assignment was best . For exposition we chose to focus on the hard-soft variant as our method and compare to competing works like Ren et al.and Finn et al. , but for completeness we will include these ablation experiments in our revision to the text ( to be posted during the rebuttal period ) . \u201c our method can only be used for classification , and not regression \u201d : While true , this weakness holds for prior prototypical methods too by Snell et al.and Ren et al.so our work is no more and no less limited in this regard ."}, {"review_id": "SJf6BhAqK7-1", "review_text": "The paper proposes a meta-learning method that utilizes unlabeled examples along with labeled examples. The technique proposed is very similar to the one by (Ren et al. 2018), only differing in the choice of a different clustering algorithm (Kulis and Jordan, 2012) instead of soft k-means as used by Ren et al. I feel the contrast to Ren et al, is not provided to the degree it should be. The Appendix paragraph A4 is not sufficient in terms of explaining why this method is conceptually different or significantly better than the related approach. It is hard for me to certify the merits of their work, including explaining the experimental results. I also do not understand the significance of \"multi-model clustering\" in this context. Also, by their definition of \"variadic\", how is this more variadic than Ren et al. or Snell et al.? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for raising three key points of our work : ( 1 ) clustering algorithm choices and our difference with Ren et al. , ( 2 ) our technical contribution of extending prototypical methods to multi-modal representation for handling more complicated data distributions , and ( 3 ) our empirical contribution of proposing and thoroughly investigating the variadic setting of any-shot/any-way generalization . > the contrast to Ren et al , is not provided to the degree it should be > only differing in the choice of a different clustering algorithm The difference in choice of clustering is crucial : - our method is capable of adaptive , multi-modal clustering unlike the fixed , uni-modal clustering of Ren et al.and Snell et al.This gives an improvement of +3 points accuracy on the standard few-shot benchmark of 5-way , 5-shot mini-ImageNet classification ( Table 2 ) , extends prototypical nets to problems without any labeled data ( see next bullet point ) , and for more diverse classes like alphabets our accuracy is ~25 points higher . - our method handles labeled data by the same clustering rule unlike the heuristics of Ren et al.for unlabeled data , making inference in our method possible for zero labeled examples ( of any kind , including meta-data as in zero-shot learning ) whereas Ren et al.and Snell et al.are undefined in this setting . Section 4.3 shows high quality clustering without labels ( Table 3 ) , and 10-25 point improvements on prior work for learning more diverse classes like alphabets instead of single characters ( Table 4 ) , underlining the importance of multiple modes . - We shed further light on the choice of clustering with the lens of probabilistic interpretation : we derive an approximate interpretation of Ren et al . ( Appendix A4 ) , which lacked theoretical justification , while explaining the direct interpretation of the hard variant of our own method ( Section 3.4 ) . > significance of `` multi-model clustering '' Multi-modality is a key and distinguishing property of our method that is necessary for the quality of our results . Please refer to figure 1 for a schematic of the difference among Snell et . al , Ren et al. , and BANDE ( ours ) : note that having multiple modes lets BANDE more accurately cluster the labeled and unlabeled data alike . Among these methods , only BANDE can adjust its capacity to model simple , compact classes with a single mode while simultaneously modeling diverse , complicated classes with multiple modes . We achieve higher accuracy than Ren et al.for semi-supervised few-shot learning ( Table 2 ) . Furthermore , Table 4 in particular highlights the needs for multi-modal representation : a full alphabet is not uni-modal in the learned embedding , unlike a single character , and here we show major ( 10-25 ) point gains over the prototypical nets of Snell et al.and Ren et al.that assume each class has a uni-modal data distribution . > by their definition of `` variadic '' , how is this more variadic than Ren et al.or Snell et al . ? Snell et al. , Ren et al. , and our method do indeed generalize better across shot and way as we show ( Figure 2 ) . Our first contribution is in evaluating this generalization at all in our novel experiments : we cover extreme way at 1692 Omniglot classes ( Figure 3 ) , extreme shot at zero labeled examples for clustering ( Table 3 ) and at scaling episodic optimization to the supervised learning regime of 50k labeled examples on CIFAR-10 and CIFAR-100 . Existing work was restricted to the few-shot settings of Section 4.1 with training/testing on the same way and shot . BANDE ( ours ) is more variadic than Ren et al.and Snell et al.in 1. handling the case of purely unlabeled data and 2. handling more diverse data with complicated class distributions such as alphabet classes instead of character classes ( section 4.3 ) . We forecast that meta-learning , as it scales to more diverse data distributions , will encounter more tasks like our alphabet recognition experiments in the variety and even hierarchy of classes , where our adaptive , multi-modal clustering helps significantly ( Table 4 ) . While we expect further progress to improve on Ren et al. , Snell et al. , and our own method , the main point here is to encourage this kind of shot/way generalization to reconcile the distant poles of small-scale and large-scale learning ."}, {"review_id": "SJf6BhAqK7-2", "review_text": "This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means. The method is tested on several scenarios and datasets, showing promising results in prediction accuracy. The idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. Related to this latter point, the use of the term \u201cBayesian nonparametric\u201d is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn\u2019t present any element related to parameters inference, uncertainty estimation, \u2026 Even the fact that the method uses an algorithm illustrated in [1] doesn\u2019t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. Finally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]). For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods. [1] Kulis and Jordan, Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012 [2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. \"Unsupervised deep embedding for clustering analysis.\" International conference on machine learning. 2016. [3] Ji, Pan, et al. \"Deep subspace clustering networks.\" Advances in Neural Information Processing Systems. 2017. [4] Jiang, Zhuxi, et al. \"Variational deep embedding: An unsupervised and generative approach to clustering.\" IJCAI 2017 [5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. \"Semantic autoencoder for zero-shot learning. CVPR 2017.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> proposes a learning method based on deep subspace clustering > substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper ( e.g . [ 2-5 ] ) We thank the reviewer for bringing up deep subspace embedding . While our work and these are generally related by metric learning , they are quite separate in approach and purpose . Ours is a meta-learning approach for multi-modal representation ( that is , having an adaptive number of centroids per class ) of labeled and unlabeled data , it is optimized for classification tasks , and it is evaluated by generalization to new data and tasks . The cited [ 2-5 ] address unsupervised clustering , have fixed numbers of clusters , and are evaluated by clustering metrics on the same data they are optimized on . Most significantly , these works * do not consider generalization * : the clustering methods are optimized on the data that is to be clustered and do not experiment on held-out tasks/classes as in meta-learning settings like ours . Only [ 5 ] can incorporate labeled data , and in their experiments they train and test on the same classes , without generalization , on a tiny synthetic dataset and the Oxford flowers dataset of 17 classes and < 1000 images . [ 2 , 3 , 4 , 5 ] learn and evaluate unsupervised and zero-shot clustering models on the same train/test data with the same classes without generalization experiments . [ 2 ] can not incorporate labeled data , requires pre-training , and shows results on the toy datasets of MNIST and STL-10 . [ 3 ] can not incorporate labeled data and is only evaluated on the simple face and object datasets Yale B , ORL , and COIL . [ 4 ] addresses generative modeling and unsupervised clustering for problems , not few-shot learning and classification , and its experiments are restricted to small-scale datasets with 10 or fewer clusters . [ 5 ] focuses on zero-shot learning with a linear auto-encoder on off-the-shelf features , and its `` supervised clustering '' section has only a 3-class synthetic dataset and a 17-class dataset of flower images where the clustering is optimized for the same 17 flower species it is evaluated on . > novelty of the proposed contribution is questionable Here is a brief summary of our key , novel contributions : technical novelty : our method is capable of adaptive , multi-modal clustering unlike the fixed , uni-modal clustering of Ren et al.and Snell et al.by our reconciliation of DP-means from Kulis et al.with end-to-end learning ( section 3.2 ) . empirical novelty : we propose and thoroughly investigate our `` variadic '' setting of any-shot/any-way generalization ( section 4.2 ) , find that several popular methods degrade in this setting ( MAML , Reptile , few-shot graph nets ) , show that it is possible to learn a large-scale classifier ( 1692-way character recognition ) from small-scale episodic optimization ( 5-way 1-shot tasks ) , show that episodic optimization of a prototypical method rivals the accuracy from large-scale SGD optimization of a strong fully-parametric baseline optimized by SGD on CIFAR-10/100 , and evaluate few-shot learning of alphabets instead of characters to examine accuracy on more complex data distributions . theoretical novelty : We shed further light on prototypical network methods with the lens of probabilistic interpretation . We derive an approximate interpretation of Ren et al . ( Appendix A4 ) , which lacked theoretical justification , and explain the direct interpretation of the hard variant of our own method ( Section 3.4 ) . > method is tested on several scenarios and datasets , showing promising results in prediction accuracy We thank the reviewer for commenting on our breadth of evaluation and promising results . To reinforce this point , we note that our experiments cover several problem statements : few-shot fully-supervised/semi-supervised classification ( Section 4.1 , Tables 1 & 2 ) , our proposed variadic setting of any-shot/any-way generalization ( Section 4.2 ) , purely unsupervised clustering ( Section 4.3 , table 3 ) and transfer learning from super-class training to sub-class recognition ( Section 4.3 , table 4 ) . We approach each of these problems by meta-learning through episodic optimization of classification tasks , and these experiments focus on generalization to new tasks ( of held-out classes , different settings of shot and way , or discovery of sub-classes from super-class training ) ."}], "0": {"review_id": "SJf6BhAqK7-0", "review_text": "Update after Author Rebuttal -------------- After reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The \"hard/soft\" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of \"borderline rejection\" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification. (Original review text below. Detailed replies to authors are in posts below their responses). Review Summary -------------- While the focus on variadic learning is interesting, I think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for ICLR. I think there is the potential for some neat ideas here and I hope the authors prepare stronger versions in the future. However, the current version is unfortunately not comprehensible or reproducible. Paper Summary ------------- The paper investigates developing an effective ML method for the \"variadic\" regime, where the method might be required to perform learning from few or many examples (shots) and few or many classes (ways). The term \"variadic\" comes from use in computer science for functions that can a flexible number of arguments. There may also be unlabeled data available in the few shot case, creating semi-supervised learning opportunities. The specific method proposed is called BANDE: Bayesian Nonparametric Deep Embedding. The idea is that each data point's feature vector x_i is transformed into an embedding vector h(x_i) using a neural network, and then clustering occurs in the embedding space via a single-pass of the DP-means algorithm (Kulis & Jordan 2012). Each cluster is assumed to correspond to one \"class\" in the eventual classification problem, though each class might have multiple clusters (and thus be multi-modal). Learning occurs in an episodic manner. After each episode (single-pass of DP-means), each point in a query set is embedded to its feature vector, then fed into each cluster's Gaussian likelihoods to produce a normalized cluster-assignment-probability vector that sums to one. This vector is then fed into a cross-entropy loss, where the true class's nearest cluster (largest probability value) is taken to be the true cluster. This loss is used to perform gradient updates of the embedding neural network. There is also a \"cumulative\" version of the method called BANDE-C. This version keeps track of cluster means from previous episodes and allows new episodes to be initialized with these. Experiments examine the proposed approach across image categorization tasks on Omniglot, mini-ImageNet, and CIFAR datasets. Strengths --------- * I like that many clusters are used for each true class label, which is better than rigid one-to-one assumptions. Limitations ----------- * Can only be used for classification, not regression * The DP-means procedure does not account for the cluster-specific variance information that is used at other steps of the algorithm Significance and Originality ---------------------------- To me, the method appears original. Any method that could really succeed across various variadic settings would be significant. Presentation Concerns --------------------- I have serious concerns about the presentation quality of this paper. Each section needs careful reorganization as well as rewording. ## P1: Algo. 1 contains numerous omissions that make it as written not correct. * the number of clusters count variable \"n\" is not updated anywhere. As writting this algo can only update one extra cluster beyond the original n. * the variable \"c\" is unbound in the else clause. You need a line that clarifies that c = argmin_{c in 1 ... n} d_ic Would be careful about saying that \"a single pass is sufficient\"... you have *chosen* to do only one pass. When doing k-means, we could also make this choice. Certainly the DP-means objective could keep improving with multiple passes. ## P2: Many figures and tables lack appropriate captions/labels Table 1: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make very clear here how much labeled data was used. Table 2: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make how many labeled and unlabeled examples were used easier to find. ## P3: Descriptions of episodic learning and overall algorithm clarity Readers unfamiliar with episodic learning are not helped with the limited coverage provided here in 3.1 and 3.2. When exactly is the \"support\" set used and the \"query\" set used? How do unlabeled points get used (both support and query appear fully labeled)? What is n? What is k? What is T? Why are some points in Q denoted with apostrophes but not others? Providing a more formal step-by-step description (perhaps with pseudocode) will be crucial. In Sec. 3.2, the paragraph that starts with \"The loss is defined\" is very hard to read and parse. I suggest adding math to formally define the loss with equations. What parameters are being optimized? Which ones are fixed? Additionally, in Sec. 3.2: \"computed in the same way as standard prototypical networks\"... what is the procedure exactly? If your method relies on a procedure, you should specify it in this paper and not make readers guess or lookup a procedure elsewhere. ## P4: Many steps of the algorithm are not detailed The paper claims to set \\lambda using a technique from another paper, but does not summarize this technique. This makes things nearly impossible to reproduce. Please add such details in the appendix. Major Technical Concerns ------------------------ ## Alg. 1 concerns: Requires two (not one) passes and mixes hard and soft assingments and different variance assumptions awkwardly The BANDE algorithm (Alg. 1) has some unjustified properties. Hard assignment decisions which assume vanishing variances are used to find a closest cluster, but then later soft assignments with non-zero variances are used. This is a bit heuristic and lacks justification... why not use soft assignment throughout? The DP means procedure is derived from a specific objective function that assumes hard assignment. Seems weird to use it for convenience and then discard instead of coming up with the small fix that would make soft assignment consistent throughout. Furthermore, The authors claim it is a one pass algorithm, but in fact as written in Alg. 1 it seems to require two passes: the first pass keeps an original set of cluster centers fixed and then creates new centers whenever an example's distance to the closest center exceeds \\lambda. But then, the *soft* assignment step that updates \"z\" requires again the distance from each point to all centers be computed, which requires another pass (since some new clusters may exist which did not when the point was first visited). While the new soft values will be close to zero, they will not be *exactly* zero, and thus they matter. ## Unclear if/how cluster-specific variance parameters learned From the text on top of page 4, it seems that the paper assumes that there exist cluster-specific variances \\sigma_c. However, these are not mentioned elsewhere, only a general (not cluster-specific) label variance \\sigma and fixed unlabeled variance sigma_u are used. ## Experiments lack comparison to internal baselines The paper doesn't evaluate sensitivity to key fixed hyperparameters (e.g. \\alpha, \\lambda) or compare variants of their approach (with and without soft clustering step, with and without multimodality via DP-means). It is difficult to tell which design choices of the method are most crucial. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed feedback , in particular the attention to the technical aspects of the clustering steps and probabilistic interpretations in our work , and the comments on clarity and accessibility for audiences less familiar with meta-learning and few-shot learning . We agree with the reviewer on the importance of multi-modal clustering as `` better than rigid , one-to-one assumptions '' of prior work , which we show by experiment on alphabet recognition in section 4.3 and improved semi-supervised few-shot classification in section 4.1 . We likewise agree that methods that `` really succeed across various variadic settings would be significant '' which is why we propose it in this work and investigate it by experiment in section 4.2 . Regarding concerns of clarity and reproducibility , we are incorporating the feedback of the reviews into a revision to be posted during the rebuttal period and will release code after decision ( omitted here only to preserve anonymity ) . Our comprehensive code release will cover our model , experimental evaluation and training settings , all few-shot baselines ( including prototypical networks , semi-supervised prototypical networks , and our variadic extensions of MAML and few-shot graph networks ) , and datasets . This will help safeguard reproducibility for future work and serve as a reference implementation of the variadic setting . We now clarify our method and experiments to address the reviewer 's technical concerns . hard/soft assignments and probabilistic interpretation : We thank the reviewer for their theoretical precision . We are in full agreement , and wish to point out that we identify and experiment with fully hard ( sec.3.4 ) and fully soft variants of our method ( appendix A4 ) for this reason of probabilistic justification . We choose the hard-soft hybrid for our main results , as mentioned in the paper , because it is marginally more accurate in experiments . We appreciate the feedback on this point , and are revising the text to make these variants more clear . number of passes/clustering steps : We will clarify our language to use the term \u201c clustering iteration \u201d instead of passes/clustering steps . In the fully hard model , an iteration corresponds to the assignment of all labeled and unlabeled points to clusters , and then an update of the means of all clusters . In the fully soft model , an iteration corresponds to computing soft assignments for all points , and then updating the means . In the hard-soft hybrid , we use the \u201c hard \u201d step to compute a set of cluster means , and then perform a `` soft '' clustering step in order to update these cluster means . cluster-specific variances : \\sigma and \\sigma_u are learned and are shared across all labeled and all unlabeled clusters respectively . \\sigma_c was a typo for \\sigma as it is the variance of class clusters . The only exception to learning these variances , as noted , is Section 4.3 where they are fixed . internal baselines/ablations : We agree with the reviewer on this list of ablations/internal baselines , so much so that we have already experimented with them in the development of the method : the selected multi-modal clustering with hard-soft assignment was best . For exposition we chose to focus on the hard-soft variant as our method and compare to competing works like Ren et al.and Finn et al. , but for completeness we will include these ablation experiments in our revision to the text ( to be posted during the rebuttal period ) . \u201c our method can only be used for classification , and not regression \u201d : While true , this weakness holds for prior prototypical methods too by Snell et al.and Ren et al.so our work is no more and no less limited in this regard ."}, "1": {"review_id": "SJf6BhAqK7-1", "review_text": "The paper proposes a meta-learning method that utilizes unlabeled examples along with labeled examples. The technique proposed is very similar to the one by (Ren et al. 2018), only differing in the choice of a different clustering algorithm (Kulis and Jordan, 2012) instead of soft k-means as used by Ren et al. I feel the contrast to Ren et al, is not provided to the degree it should be. The Appendix paragraph A4 is not sufficient in terms of explaining why this method is conceptually different or significantly better than the related approach. It is hard for me to certify the merits of their work, including explaining the experimental results. I also do not understand the significance of \"multi-model clustering\" in this context. Also, by their definition of \"variadic\", how is this more variadic than Ren et al. or Snell et al.? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for raising three key points of our work : ( 1 ) clustering algorithm choices and our difference with Ren et al. , ( 2 ) our technical contribution of extending prototypical methods to multi-modal representation for handling more complicated data distributions , and ( 3 ) our empirical contribution of proposing and thoroughly investigating the variadic setting of any-shot/any-way generalization . > the contrast to Ren et al , is not provided to the degree it should be > only differing in the choice of a different clustering algorithm The difference in choice of clustering is crucial : - our method is capable of adaptive , multi-modal clustering unlike the fixed , uni-modal clustering of Ren et al.and Snell et al.This gives an improvement of +3 points accuracy on the standard few-shot benchmark of 5-way , 5-shot mini-ImageNet classification ( Table 2 ) , extends prototypical nets to problems without any labeled data ( see next bullet point ) , and for more diverse classes like alphabets our accuracy is ~25 points higher . - our method handles labeled data by the same clustering rule unlike the heuristics of Ren et al.for unlabeled data , making inference in our method possible for zero labeled examples ( of any kind , including meta-data as in zero-shot learning ) whereas Ren et al.and Snell et al.are undefined in this setting . Section 4.3 shows high quality clustering without labels ( Table 3 ) , and 10-25 point improvements on prior work for learning more diverse classes like alphabets instead of single characters ( Table 4 ) , underlining the importance of multiple modes . - We shed further light on the choice of clustering with the lens of probabilistic interpretation : we derive an approximate interpretation of Ren et al . ( Appendix A4 ) , which lacked theoretical justification , while explaining the direct interpretation of the hard variant of our own method ( Section 3.4 ) . > significance of `` multi-model clustering '' Multi-modality is a key and distinguishing property of our method that is necessary for the quality of our results . Please refer to figure 1 for a schematic of the difference among Snell et . al , Ren et al. , and BANDE ( ours ) : note that having multiple modes lets BANDE more accurately cluster the labeled and unlabeled data alike . Among these methods , only BANDE can adjust its capacity to model simple , compact classes with a single mode while simultaneously modeling diverse , complicated classes with multiple modes . We achieve higher accuracy than Ren et al.for semi-supervised few-shot learning ( Table 2 ) . Furthermore , Table 4 in particular highlights the needs for multi-modal representation : a full alphabet is not uni-modal in the learned embedding , unlike a single character , and here we show major ( 10-25 ) point gains over the prototypical nets of Snell et al.and Ren et al.that assume each class has a uni-modal data distribution . > by their definition of `` variadic '' , how is this more variadic than Ren et al.or Snell et al . ? Snell et al. , Ren et al. , and our method do indeed generalize better across shot and way as we show ( Figure 2 ) . Our first contribution is in evaluating this generalization at all in our novel experiments : we cover extreme way at 1692 Omniglot classes ( Figure 3 ) , extreme shot at zero labeled examples for clustering ( Table 3 ) and at scaling episodic optimization to the supervised learning regime of 50k labeled examples on CIFAR-10 and CIFAR-100 . Existing work was restricted to the few-shot settings of Section 4.1 with training/testing on the same way and shot . BANDE ( ours ) is more variadic than Ren et al.and Snell et al.in 1. handling the case of purely unlabeled data and 2. handling more diverse data with complicated class distributions such as alphabet classes instead of character classes ( section 4.3 ) . We forecast that meta-learning , as it scales to more diverse data distributions , will encounter more tasks like our alphabet recognition experiments in the variety and even hierarchy of classes , where our adaptive , multi-modal clustering helps significantly ( Table 4 ) . While we expect further progress to improve on Ren et al. , Snell et al. , and our own method , the main point here is to encourage this kind of shot/way generalization to reconcile the distant poles of small-scale and large-scale learning ."}, "2": {"review_id": "SJf6BhAqK7-2", "review_text": "This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means. The method is tested on several scenarios and datasets, showing promising results in prediction accuracy. The idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. Related to this latter point, the use of the term \u201cBayesian nonparametric\u201d is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn\u2019t present any element related to parameters inference, uncertainty estimation, \u2026 Even the fact that the method uses an algorithm illustrated in [1] doesn\u2019t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. Finally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]). For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods. [1] Kulis and Jordan, Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012 [2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. \"Unsupervised deep embedding for clustering analysis.\" International conference on machine learning. 2016. [3] Ji, Pan, et al. \"Deep subspace clustering networks.\" Advances in Neural Information Processing Systems. 2017. [4] Jiang, Zhuxi, et al. \"Variational deep embedding: An unsupervised and generative approach to clustering.\" IJCAI 2017 [5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. \"Semantic autoencoder for zero-shot learning. CVPR 2017.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> proposes a learning method based on deep subspace clustering > substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper ( e.g . [ 2-5 ] ) We thank the reviewer for bringing up deep subspace embedding . While our work and these are generally related by metric learning , they are quite separate in approach and purpose . Ours is a meta-learning approach for multi-modal representation ( that is , having an adaptive number of centroids per class ) of labeled and unlabeled data , it is optimized for classification tasks , and it is evaluated by generalization to new data and tasks . The cited [ 2-5 ] address unsupervised clustering , have fixed numbers of clusters , and are evaluated by clustering metrics on the same data they are optimized on . Most significantly , these works * do not consider generalization * : the clustering methods are optimized on the data that is to be clustered and do not experiment on held-out tasks/classes as in meta-learning settings like ours . Only [ 5 ] can incorporate labeled data , and in their experiments they train and test on the same classes , without generalization , on a tiny synthetic dataset and the Oxford flowers dataset of 17 classes and < 1000 images . [ 2 , 3 , 4 , 5 ] learn and evaluate unsupervised and zero-shot clustering models on the same train/test data with the same classes without generalization experiments . [ 2 ] can not incorporate labeled data , requires pre-training , and shows results on the toy datasets of MNIST and STL-10 . [ 3 ] can not incorporate labeled data and is only evaluated on the simple face and object datasets Yale B , ORL , and COIL . [ 4 ] addresses generative modeling and unsupervised clustering for problems , not few-shot learning and classification , and its experiments are restricted to small-scale datasets with 10 or fewer clusters . [ 5 ] focuses on zero-shot learning with a linear auto-encoder on off-the-shelf features , and its `` supervised clustering '' section has only a 3-class synthetic dataset and a 17-class dataset of flower images where the clustering is optimized for the same 17 flower species it is evaluated on . > novelty of the proposed contribution is questionable Here is a brief summary of our key , novel contributions : technical novelty : our method is capable of adaptive , multi-modal clustering unlike the fixed , uni-modal clustering of Ren et al.and Snell et al.by our reconciliation of DP-means from Kulis et al.with end-to-end learning ( section 3.2 ) . empirical novelty : we propose and thoroughly investigate our `` variadic '' setting of any-shot/any-way generalization ( section 4.2 ) , find that several popular methods degrade in this setting ( MAML , Reptile , few-shot graph nets ) , show that it is possible to learn a large-scale classifier ( 1692-way character recognition ) from small-scale episodic optimization ( 5-way 1-shot tasks ) , show that episodic optimization of a prototypical method rivals the accuracy from large-scale SGD optimization of a strong fully-parametric baseline optimized by SGD on CIFAR-10/100 , and evaluate few-shot learning of alphabets instead of characters to examine accuracy on more complex data distributions . theoretical novelty : We shed further light on prototypical network methods with the lens of probabilistic interpretation . We derive an approximate interpretation of Ren et al . ( Appendix A4 ) , which lacked theoretical justification , and explain the direct interpretation of the hard variant of our own method ( Section 3.4 ) . > method is tested on several scenarios and datasets , showing promising results in prediction accuracy We thank the reviewer for commenting on our breadth of evaluation and promising results . To reinforce this point , we note that our experiments cover several problem statements : few-shot fully-supervised/semi-supervised classification ( Section 4.1 , Tables 1 & 2 ) , our proposed variadic setting of any-shot/any-way generalization ( Section 4.2 ) , purely unsupervised clustering ( Section 4.3 , table 3 ) and transfer learning from super-class training to sub-class recognition ( Section 4.3 , table 4 ) . We approach each of these problems by meta-learning through episodic optimization of classification tasks , and these experiments focus on generalization to new tasks ( of held-out classes , different settings of shot and way , or discovery of sub-classes from super-class training ) ."}}