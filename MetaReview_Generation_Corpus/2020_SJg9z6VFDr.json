{"year": "2020", "forum": "SJg9z6VFDr", "title": "Ordinary differential equations on graph networks", "decision": "Reject", "meta_review": "This paper introduces a few ideas to potentially improve the performance of neural ODEs on graph networks.  However, the reviewers disagreed about the motivations for the proposed modifications.  Specifically, it's not clear that neural ODEs provide a more advantageous parameterization in this setting than standard discrete networks.\n\nIt's also not clear at all why the authors are discussion graph neural networks in particular, as all of their proposed changes would apply to all types of network.\n\nAnother major problem I had with this paper was the assertion that the running the original system backwards leads to large numerical error.  This is a plausible claim, but it was never verified.  It's extremely easy to check (e.g. by comparing the reconstructed initial state at t0 with the true original state at t0, or by comparing gradients computed by different methods).  It's also not clear if the authors enforced the constraints on their dynamics function needed to ensure that a unique solution exists in the first place.", "reviews": [{"review_id": "SJg9z6VFDr-0", "review_text": "Summary: This paper proposed a deep model called graph-ODE (GODE), which extends the continuous deep model, neural ODE [1], to graph structured data. Two methods of computing the gradient are proposed, one is the adjoint method proposed in [1] which is memory-efficient but not perfectly accurate, and the other method is to discretize the ODE and compute an accurate gradient, which is also memory-efficient but relies on an invertible architecture. In general, this paper contains many interesting thoughts, but to me, it seems these ideas are not new and have been proposed before[1,2,3,4]. I could not find a strong contribution of this paper. - - - about the ODE stability issue Among invertible deep models, the advantage of ODE-based continuous models [1] is: there is *no restriction* on the form of f. The drawback is the computed gradient has an error, depending on discretization step size and stability. This paper pointed out the stability problem (which is also discussed in [2,3]), but do not provide a solution in the domain of continuous deep models. Instead, the solution they provided is to use a discretized version and compute the gradient accurately. Then it becomes a standard invertible DL model with discrete layers, where the invertible building blocks have a specific *restricted form*. The 'adjoint method with discrete-time' in Eq (10) is the same as the chain rule, which has also be pointed out in [4]. To this point, I think GODE is in the class of discrete invertible DL models trained by gradient descent. I think it less related to continuous models, except the step size can be adaptive in the forward pass. - - - about the invertible building block The proposed invertible building block replaces 'sum' in [5] by a function \\psi. This is not novel enough to serve as a contribution. - - - comparison to graph neural network I think it is interesting to apply ODE techniques or other invertible architectures to graph-structured data, for which I didn't see similar works before and could be a contribution of this paper. However, for the experimental results shown in table 3 and 4, the improvement is really small. A stronger result is needed to demonstrate the advantages. [1] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems. 2018. [2] Chang, Bo, et al. \"Reversible architectures for arbitrarily deep residual neural networks.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [3] Behrmann, Jens, et al. \"Invertible Residual Networks.\" International Conference on Machine Learning. 2019. [4] Li, Qianxiao, et al. \"Maximum principle based algorithms for deep learning.\" The Journal of Machine Learning Research 18.1 (2017): 5998-6026. [5] Gomez, Aidan N., et al. \"The reversible residual network: Backpropagation without storing activations.\" Advances in neural information processing systems. 2017. -------------after reading the response I'd like to thank the authors for their explanations. However, the authors' explanation of the benefit of transforming the invertible building block into an ODE model is still not convincing to me. The authors explained that ODE solutions can represent highly nonlinear functions. However, discrete NN also represents highly nonlinear functions (e.g. with Relu activation, they are *piecewise* linear, but they are highly nonlinear)! From my point of view, their difference is that ODE model is more smooth. However, the benefit of using a smoother model is still unclear. For the example that the authors provided, $\\sin x$, why being able to represent that kind of function is an advantage for the graph classification problem? Why is this a good model bias? I think the authors' responses are still not convincing enough, so I choose to retain the score.", "rating": "1: Reject", "reply_text": "We sincerely thank you for review and totally agree with your comments on ODE solver for free-form continuous functions . However , we want to point out that our method is a free-form solver for continuous models , and extensive experiments are conducted in appendix C ( moved to Sec 5.3 in revision ) . The reviewer might overlooked this part because we did not put enough emphasis in the main paper . We will address your concerns in the following . 1. about the ODE stability issue We put a detailed description on this issue in thread A https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=B1eeWWA7sr We will briefly address your concern here . ( a ) Our method can handle free-form functions , which is theoretically discussed in thread A.1 . For experiments in thread A.3 , we directly modified a ResNet18 into an ODE , where $ f $ is a stack of conv-bn-relu-conv-bn layers WITHOUT the invertible structure . In fact , invertible block is not the key to our ODE solver ; it \u2019 s only the key to low memory consumption . This is discussed in thread B , section 1 to 3 https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=SkgzfxCmoH Memory consumption for naive direct backprop is $ O ( N_f\\times N_t\\times K ) $ , our method for free-form can be reduced to $ O ( N_f + N_t ) $ , for invertible blocks is $ O ( N_f ) $ We apologize for the confusion ; we put too little description of the ability to handle free-form , and wrote too much about invertible block with the thinking that most readers might not be familiar . We will revise this in the paper . ( b ) Our method is dealing with a continuous model . This is theoretically discussed in thread A.2 and supported with experiments in A.3 . ( d ) . We agree with reviewers that uncareful discretization will convert a continuous model into a discrete model . However , we clarify that this is not the case with our method . A continuous function is robust to different ODE solvers , while discrete models are sensitive to solvers , because different ODE solvers are equivalent to different depths . A test for continuous model is : at inference time , switch to different ODE solvers of different orders ; a continuous model outputs very robust results . Our method passed this test , as shown in thread A.3 . ( d ) . You can check this with our code and pre-trained weights https : //www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa ? dl=0 2. about the invertible building block We admit that we did not extensively experiment with different forms of $ \\psi $ ; we will revise our paper and not claim this as our main contribution . However , we want to clarify that we define a space to search for invertible blocks , which is important for the field of \u201c normalizing flow \u201d . There are many works on normalizing flow which uses different $ \\psi $ . For example , $ \\psi ( \\alpha , \\beta ) = \\alpha \\cdot sigmoid ( \\beta ) $ in [ 2 ] , $ \\psi ( \\alpha , \\beta ) = \\alpha + \\beta $ in [ 3 ] , $ \\psi ( \\alpha , \\beta ) = \\alpha \\cdot exp ( \\beta ) $ in [ 4 ] , $ \\psi ( \\alpha , \\beta ) = \\alpha \\cdot \\beta $ in [ 5 ] . To our knowledge , we are the first to define the searching space of $ \\psi $ . 3. empirical performance We validate our method both in CNN-ODE ( thread A.3 ) and GNN-ODE ( thread C ) . The CNN-ODE is a * free-form * function ; for GNN-ODE , we tested both free-form and invertible block ( restricted form ) . For experiments on CIFAR with CNN-ODE , our method reduces error rate from 19 % to 5 % compared with training ODE using the solver in [ 1 ] . Furthermore , our model is robust to different ODE solvers during inference . Finally , our model has the same number of parameters as a ResNet18 , but outperforms standard ResNet50 and ResNet101 . For experiments with GNN , we re-run experiments and performed paired t-tests . Results are summarized in Thread C ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=HklsTJ07jB ) . For most cases , our model outperforms the discrete baseline at a 1 % significance level . [ 1 ] Chen , Tian Qi , et al . `` Neural ordinary differential equations . '' Advances in neural information processing systems . 2018 . [ 2 ] Huang , Chin-Wei , et al . `` Neural autoregressive flows . '' arXiv preprint arXiv:1804.00779 ( 2018 ) . [ 3 ] Dinh , Laurent , David Krueger , and Yoshua Bengio . `` Nice : Non-linear independent components estimation . '' arXiv preprint arXiv:1410.8516 ( 2014 ) . [ 4 ] Dinh , Laurent , Jascha Sohl-Dickstein , and Samy Bengio . `` Density estimation using real nvp . '' arXiv preprint arXiv:1605.08803 ( 2016 ) . [ 5 ] Liao , Huadong , Jiawei He , and Kunxian Shu . `` Generative Model with Dynamic Linear Flow . '' arXiv preprint arXiv:1905.03239 ( 2019 ) ."}, {"review_id": "SJg9z6VFDr-1", "review_text": "Summary: This work extends Neural ODE to graph networks and compares the continuous adjoint method with propagation through the ODE solver. The paper addresses an interesting and important problem and it is well-written in general. To determine the significance of this work, I have two questions: Question: 1. What is the major difference between the original Neural ODE and the Graph Neural ODE? For example, In graph networks, each node\u2019s representation may depend on its neighbor nodes. Will this impact the way you formulate the adjoints or compute the derivative? 2. It seems in Mechanical engineering, various adjoint methods such as Discrete adjoint (e.g. [1]) has been studied. How does the direct propagation through solver related to the these Discrete adjoint methods? Some minor comments for experiments: The authors have 10 runs and take the best one. How about the average? which maybe a better indicator for stability. How is the runtime comparing normal NN, adjoint, and direct propagation. Runtime has been a major disadvantage for Neural ODE. Decision: Overall, the novelty seems somewhat incremental, but I still feel the work is concrete and meaningful. I vote for weak accept. Looking forward to the code. [1] A Discrete Adjoint-Based Approach for Optimization Problems on Three-Dimensional Unstructured Meshes. Dimitri J. Mavriplis", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for your feedback . We address the reviewer \u2019 s concerns : 1 . Difference between neural ODE and graph ODE . From the perspective of the model , they are both ODE models $ \\frac { dz } { dt } =f ( z , t ) $ , except the actual form of $ f $ is different . The adjoint and gradient are calculated in the same way . Our method is a generic solution and can handle both cases . 2.Relation to discrete adjoint method To our knowledge , discrete adjoint method is a discretize-then-optimize method , where the continuous function is discretized at pre-defined grids [ 1 ] . With pre-defined grids , the continuous problem is actually transformed into a discrete problem with more constraints . This is related to R2 \u2019 s concern that we are not directly solving the continuous problem . Our method is an optimize-then-discretize approach . The grid points are not predefined , instead they are adaptively computed under error control . This is related to our discussion in thread A.2 . For our method , because the grid is adaptively determined , the grid discretization will be very different for different input data ; while the grid remains the same for all data in discrete-adjoint methods . Furthermore , because our methods are adaptive , the integration value is robust to different ODE solvers , as long as the error tolerance is small enough . However , with fixed grid , there \u2019 s no control on error , hence different solvers could generate very different results . This argument is related to thread A.2 , clarifying that our method is truly solving a continuous problem , rather than converting it to a discrete problem . 3.Experiments The numbers reported in our paper are averages among 10 runs , with standard deviation also reported . We will clarify this in the paper . 4.Running time For a ResNet18 , and its direct modification into an ODE , the discrete model has the shortest running time ; the running time of ODE is roughly 4 times longer than the discrete model , when trained both with our method and the adjoint method in [ 2 ] . The longer running time comes from the ODE model , where multiple steps need to be determined for numerical integration . 5.Code and pretrained weights : https : //www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa ? dl=0 6 . Considering the comments and confusion from all reviewers , we clarify our contributions : ( a ) A general method for ODE solver that works well in practice -- -- Our method is applicable to both CNN-ODE and graph-ODE of different structures , and deals with a fundamental problem : how to estimate the gradient . -- -- To our knowledge , this is the first paper that makes neural-ODE achieve comparable or even higher accuracy in benchmark classification tasks compared to a state-of-the-art discrete-layer network . -- -- -A general method for free-form functions , and truly designed for continuous models , as discussed in thread A . ( b ) Application to graph networks enables new theoretical aspects , such as over-smoothing phenomena , which is in part discussed in response to R1 ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=B1esk0ZHjS ) [ 1 ] Nadarajah , Siva , and Antony Jameson . `` Studies of the continuous and discrete adjoint approaches to viscous automatic aerodynamic shape optimization . `` 2001 . [ 2 ] Chen , Tian Qi , et al . `` Neural ordinary differential equations . '' Advances in neural information processing systems . 2018 ."}, {"review_id": "SJg9z6VFDr-2", "review_text": "Summary The authors discussed that most graph NNs to date considered discrete layers and hence are difficult to model diffusion processes on graphs. This paper proposed the Neural ODE on a graph, termed with a graph ODE, to tackle this problem. The authors gave a sufficient condition under which the adjoint method gets instable and pointed out potential issues of training Neural ODEs using it. To overcome the instability issue, the author proposed to backpropagate errors directly at discretized points. Since the naive implementation of the direct method is memory-consuming, the authors used invertible blocks as building blocks of graph ODEs, which do not store the intermediate activations for backward propagation. Finally, the authors conducted empirical studies to see the effectiveness of the proposed method. Decision I recommend rejecting the paper weakly because I think the extension of Neural ODEs to graphs is straightforward and that the empirical study is not strong enough to compensate for the weakness of the novelty. Theoretical justification of numerical instability of Neural ODEs (Proposition 1) and its empirical verification (Section 5.4) give new insights for understanding Neural ODEs. However, if I understand correctly, the formulation of graph ODEs do not use the internal structures of graph NNs, even the fact that the underlying neural network is a graph NN. Therefore, I think the extension from Neural ODEs to graphs is a bit too straightforward. The authors proposed a method which improves the stability and memory-efficiency of training. We can apply this method to general Neural ODEs, too. In addition, we can attribute the idea of using invertible blocks to existing works (Gomez et al., 2017). Finally, regarding the empirical performance of graph ODEs, the performance improvement from existing GNNs is within the standard deviations. Therefore, I think the empirical result is not sufficiently strong to justify the novelty of applying Neural ODEs to graphs. Taking these things into account, although the authors gave a new result on Neural ODEs, I think the contribution is limited from the viewpoint of the study of graph NNs. Suggestion - As I wrote in the Decision section, the theoretical results are not restricted to graph ODEs but valid for general neural ODEs. Therefore, I think the authors do not have to restrict the application areas to graph ODEs. The possible direction of the paper is to further analysis of training neural ODEs (e.g., instability). On the other hand, if the authors are interested in the extension of neural ODEs to graphs, I expect a more detailed relationship between the neural ODEs framework and underlying GNNs. For example, I am curious how the topological information of graphs affects the graph ODEs via spectral-type graph convolution operations and what is the relationship to the oversmoothing phenomena (Li et al., 2018). - Since Theorem 1 is applicable not only graph ODEs but also Neural ODEs, it implies that ordinal Neural ODEs are also vulnerable to the instability. The experiments in Dupont (2019), which this paper referenced, pointed out the instability of Neural ODEs. I am wondering the proposed method can enhance the training of neural ODEs, too. [Li et al., 2018] Li, Qimai, Zhichao Han, and Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.", "rating": "3: Weak Reject", "reply_text": "We sincerely thank the reviewer for your efforts and comments . We will address the reviewer \u2019 s concerns and state our contributions : 1 . Contribution to graph neural networks , connection with over-smoothing phenomena -- -- - ( a ) intuition We sincerely thank the reviewer for pointing out that ODE and the over-smoothing issue of GCN could be related . In fact , we did not start this project with neural ODE ; instead we started with an intuition that GNN could be related to a continuous \u201c heat transfer \u201d model , then we tried to build a good model for the continuous process , so we started looking at ODEs but found current method can not train an ODE well . In the heat-transfer model , the feature of each node is analogous to temperature , and the messaging passing process is analogous to heat transfer between nodes . Time of heat transfer process is analogous to depths of discrete-layer GCN models . If time is too short , there will be insufficient heat transfer , equivalent to a node not having enough information from its neighbors to update ; if time is too long , all nodes end up in the same temperature , corresponding to the over-smoothing issue with GNN . -- -- ( b ) Mathematical analysis We sincerely thank the reviewer for pointing out previous work on over-smoothing , which leads us from intuition to some mathematical analysis . We show our analysis below : It \u2019 s demonstrated that graph convolution is a special case of Laplacian smoothing in [ 1 ] , which can be written as $ Y = ( I - \\gamma \\tilde { D } ^ { -1 } \\tilde { L } ) X $ where $ X $ and $ Y $ are the input and output of a graph-conv layer respectively , $ \\tilde { A } =A+I $ where $ A $ is the adjacency matrix , and $ \\tilde { D } $ is the corresponding degree matrix of $ \\tilde { A } $ , and $ \\gamma $ is a positive scaling constant . If we replace normalized Laplacian with symmetricly normalized Laplacian , the continuous process becomes : $ Y = ( I - \\gamma \\tilde { D } ^ { -1/2 } \\tilde { L } \\tilde { D } ^ { -1/2 } ) X $ When it \u2019 s modified from a discrete model to a continuous model , the continuous smoothing process is : $ \\frac { dX } { dt } = - \\gamma \\tilde { D } ^ { -1/2 } \\tilde { L } \\tilde { D } ^ { -1/2 } X $ Since all eigenvalues of the symmetrically normalized Laplacian are real and non-negative , then all eigenvalues of the above ODE are real and non-positive . Suppose all eigenvalues of the normalized Laplacian are non-zero . In this case , the ODE has only negative eigenvalues , hence the ODE above is $ asymptotically \\ stable $ [ 2 ] . Hence as time $ t $ grows sufficiently large , all trajectories are close enough . In experiments , this suggests if integration time $ T $ is large enough , all nodes ( from different classes ) will have very similar features , thus the classification accuracy will drop . -- -- ( c ) Experiments We validate this with experiments . Node classification accuracy varying with integration time is measured on the Cora dataset . Results are shown below : \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Integration time 0.5 1.0 1.5 2.0 5.0 10.0 20.0 100.0 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Accuracy 80.5 81.6 80.1 80.1 79.3 77.6 68.9 35.1 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 When integration time is small ( 0.5 ) , nodes do not aggregate neighbor information . When integration time is too long , node attributes tend to the same value because of asymptotic behaviour of ODE , as mentioned above . Both cases generate inferior accuracy . 2.Generalization to general ODEs . We show that our method can be applied to general ODEs , with free-form functions , can deal with continuous models , and are robust to different ODE solvers . Details are in appendix C of original version ( Sec 5.3 in revision ) , and we post it in thread A ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=B1eeWWA7sr ) . 3.Our method supports free-form functions and does not depend on invertible block Details are in thread B ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=SkgzfxCmoH ) . Our method is memory efficient for free-form functions , with the special case of invertible block we achieve even lower memory . Hence the contribution can not be simply attributed to invertible block . 4.Better performance for both graph network and CNN We validate our method both in CNN-ODE ( thread A.3 ) and GNN-ODE ( thread C https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=HklsTJ07jB ) . The CNN-ODE is a * free-form * function ; for GNN-ODE , we tested both free-form and invertible block ( restricted form ) . Our method significantly outperforms ( under paired t-test ) discrete models for graph networks ; it reduces error rate from 19 % to 5 % on CIFAR10 compared with results using the adjoint method [ 3 ] and Augmented-NODE [ 4 ] . [ 1 ] Li , Qimai , et al . `` Deeper insights into graph convolutional networks for semi-supervised learning '' 2018 . [ 2 ] Lyapunov , A. M. The General Problem of the Stability of Motion [ 3 ] Chen , Tian Qi , et al . `` Neural ordinary differential equations . '' 2018 [ 4 ] Dupont , et al . `` Augmented neural odes . '' ( 2019 ) ."}], "0": {"review_id": "SJg9z6VFDr-0", "review_text": "Summary: This paper proposed a deep model called graph-ODE (GODE), which extends the continuous deep model, neural ODE [1], to graph structured data. Two methods of computing the gradient are proposed, one is the adjoint method proposed in [1] which is memory-efficient but not perfectly accurate, and the other method is to discretize the ODE and compute an accurate gradient, which is also memory-efficient but relies on an invertible architecture. In general, this paper contains many interesting thoughts, but to me, it seems these ideas are not new and have been proposed before[1,2,3,4]. I could not find a strong contribution of this paper. - - - about the ODE stability issue Among invertible deep models, the advantage of ODE-based continuous models [1] is: there is *no restriction* on the form of f. The drawback is the computed gradient has an error, depending on discretization step size and stability. This paper pointed out the stability problem (which is also discussed in [2,3]), but do not provide a solution in the domain of continuous deep models. Instead, the solution they provided is to use a discretized version and compute the gradient accurately. Then it becomes a standard invertible DL model with discrete layers, where the invertible building blocks have a specific *restricted form*. The 'adjoint method with discrete-time' in Eq (10) is the same as the chain rule, which has also be pointed out in [4]. To this point, I think GODE is in the class of discrete invertible DL models trained by gradient descent. I think it less related to continuous models, except the step size can be adaptive in the forward pass. - - - about the invertible building block The proposed invertible building block replaces 'sum' in [5] by a function \\psi. This is not novel enough to serve as a contribution. - - - comparison to graph neural network I think it is interesting to apply ODE techniques or other invertible architectures to graph-structured data, for which I didn't see similar works before and could be a contribution of this paper. However, for the experimental results shown in table 3 and 4, the improvement is really small. A stronger result is needed to demonstrate the advantages. [1] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems. 2018. [2] Chang, Bo, et al. \"Reversible architectures for arbitrarily deep residual neural networks.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018. [3] Behrmann, Jens, et al. \"Invertible Residual Networks.\" International Conference on Machine Learning. 2019. [4] Li, Qianxiao, et al. \"Maximum principle based algorithms for deep learning.\" The Journal of Machine Learning Research 18.1 (2017): 5998-6026. [5] Gomez, Aidan N., et al. \"The reversible residual network: Backpropagation without storing activations.\" Advances in neural information processing systems. 2017. -------------after reading the response I'd like to thank the authors for their explanations. However, the authors' explanation of the benefit of transforming the invertible building block into an ODE model is still not convincing to me. The authors explained that ODE solutions can represent highly nonlinear functions. However, discrete NN also represents highly nonlinear functions (e.g. with Relu activation, they are *piecewise* linear, but they are highly nonlinear)! From my point of view, their difference is that ODE model is more smooth. However, the benefit of using a smoother model is still unclear. For the example that the authors provided, $\\sin x$, why being able to represent that kind of function is an advantage for the graph classification problem? Why is this a good model bias? I think the authors' responses are still not convincing enough, so I choose to retain the score.", "rating": "1: Reject", "reply_text": "We sincerely thank you for review and totally agree with your comments on ODE solver for free-form continuous functions . However , we want to point out that our method is a free-form solver for continuous models , and extensive experiments are conducted in appendix C ( moved to Sec 5.3 in revision ) . The reviewer might overlooked this part because we did not put enough emphasis in the main paper . We will address your concerns in the following . 1. about the ODE stability issue We put a detailed description on this issue in thread A https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=B1eeWWA7sr We will briefly address your concern here . ( a ) Our method can handle free-form functions , which is theoretically discussed in thread A.1 . For experiments in thread A.3 , we directly modified a ResNet18 into an ODE , where $ f $ is a stack of conv-bn-relu-conv-bn layers WITHOUT the invertible structure . In fact , invertible block is not the key to our ODE solver ; it \u2019 s only the key to low memory consumption . This is discussed in thread B , section 1 to 3 https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=SkgzfxCmoH Memory consumption for naive direct backprop is $ O ( N_f\\times N_t\\times K ) $ , our method for free-form can be reduced to $ O ( N_f + N_t ) $ , for invertible blocks is $ O ( N_f ) $ We apologize for the confusion ; we put too little description of the ability to handle free-form , and wrote too much about invertible block with the thinking that most readers might not be familiar . We will revise this in the paper . ( b ) Our method is dealing with a continuous model . This is theoretically discussed in thread A.2 and supported with experiments in A.3 . ( d ) . We agree with reviewers that uncareful discretization will convert a continuous model into a discrete model . However , we clarify that this is not the case with our method . A continuous function is robust to different ODE solvers , while discrete models are sensitive to solvers , because different ODE solvers are equivalent to different depths . A test for continuous model is : at inference time , switch to different ODE solvers of different orders ; a continuous model outputs very robust results . Our method passed this test , as shown in thread A.3 . ( d ) . You can check this with our code and pre-trained weights https : //www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa ? dl=0 2. about the invertible building block We admit that we did not extensively experiment with different forms of $ \\psi $ ; we will revise our paper and not claim this as our main contribution . However , we want to clarify that we define a space to search for invertible blocks , which is important for the field of \u201c normalizing flow \u201d . There are many works on normalizing flow which uses different $ \\psi $ . For example , $ \\psi ( \\alpha , \\beta ) = \\alpha \\cdot sigmoid ( \\beta ) $ in [ 2 ] , $ \\psi ( \\alpha , \\beta ) = \\alpha + \\beta $ in [ 3 ] , $ \\psi ( \\alpha , \\beta ) = \\alpha \\cdot exp ( \\beta ) $ in [ 4 ] , $ \\psi ( \\alpha , \\beta ) = \\alpha \\cdot \\beta $ in [ 5 ] . To our knowledge , we are the first to define the searching space of $ \\psi $ . 3. empirical performance We validate our method both in CNN-ODE ( thread A.3 ) and GNN-ODE ( thread C ) . The CNN-ODE is a * free-form * function ; for GNN-ODE , we tested both free-form and invertible block ( restricted form ) . For experiments on CIFAR with CNN-ODE , our method reduces error rate from 19 % to 5 % compared with training ODE using the solver in [ 1 ] . Furthermore , our model is robust to different ODE solvers during inference . Finally , our model has the same number of parameters as a ResNet18 , but outperforms standard ResNet50 and ResNet101 . For experiments with GNN , we re-run experiments and performed paired t-tests . Results are summarized in Thread C ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=HklsTJ07jB ) . For most cases , our model outperforms the discrete baseline at a 1 % significance level . [ 1 ] Chen , Tian Qi , et al . `` Neural ordinary differential equations . '' Advances in neural information processing systems . 2018 . [ 2 ] Huang , Chin-Wei , et al . `` Neural autoregressive flows . '' arXiv preprint arXiv:1804.00779 ( 2018 ) . [ 3 ] Dinh , Laurent , David Krueger , and Yoshua Bengio . `` Nice : Non-linear independent components estimation . '' arXiv preprint arXiv:1410.8516 ( 2014 ) . [ 4 ] Dinh , Laurent , Jascha Sohl-Dickstein , and Samy Bengio . `` Density estimation using real nvp . '' arXiv preprint arXiv:1605.08803 ( 2016 ) . [ 5 ] Liao , Huadong , Jiawei He , and Kunxian Shu . `` Generative Model with Dynamic Linear Flow . '' arXiv preprint arXiv:1905.03239 ( 2019 ) ."}, "1": {"review_id": "SJg9z6VFDr-1", "review_text": "Summary: This work extends Neural ODE to graph networks and compares the continuous adjoint method with propagation through the ODE solver. The paper addresses an interesting and important problem and it is well-written in general. To determine the significance of this work, I have two questions: Question: 1. What is the major difference between the original Neural ODE and the Graph Neural ODE? For example, In graph networks, each node\u2019s representation may depend on its neighbor nodes. Will this impact the way you formulate the adjoints or compute the derivative? 2. It seems in Mechanical engineering, various adjoint methods such as Discrete adjoint (e.g. [1]) has been studied. How does the direct propagation through solver related to the these Discrete adjoint methods? Some minor comments for experiments: The authors have 10 runs and take the best one. How about the average? which maybe a better indicator for stability. How is the runtime comparing normal NN, adjoint, and direct propagation. Runtime has been a major disadvantage for Neural ODE. Decision: Overall, the novelty seems somewhat incremental, but I still feel the work is concrete and meaningful. I vote for weak accept. Looking forward to the code. [1] A Discrete Adjoint-Based Approach for Optimization Problems on Three-Dimensional Unstructured Meshes. Dimitri J. Mavriplis", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for your feedback . We address the reviewer \u2019 s concerns : 1 . Difference between neural ODE and graph ODE . From the perspective of the model , they are both ODE models $ \\frac { dz } { dt } =f ( z , t ) $ , except the actual form of $ f $ is different . The adjoint and gradient are calculated in the same way . Our method is a generic solution and can handle both cases . 2.Relation to discrete adjoint method To our knowledge , discrete adjoint method is a discretize-then-optimize method , where the continuous function is discretized at pre-defined grids [ 1 ] . With pre-defined grids , the continuous problem is actually transformed into a discrete problem with more constraints . This is related to R2 \u2019 s concern that we are not directly solving the continuous problem . Our method is an optimize-then-discretize approach . The grid points are not predefined , instead they are adaptively computed under error control . This is related to our discussion in thread A.2 . For our method , because the grid is adaptively determined , the grid discretization will be very different for different input data ; while the grid remains the same for all data in discrete-adjoint methods . Furthermore , because our methods are adaptive , the integration value is robust to different ODE solvers , as long as the error tolerance is small enough . However , with fixed grid , there \u2019 s no control on error , hence different solvers could generate very different results . This argument is related to thread A.2 , clarifying that our method is truly solving a continuous problem , rather than converting it to a discrete problem . 3.Experiments The numbers reported in our paper are averages among 10 runs , with standard deviation also reported . We will clarify this in the paper . 4.Running time For a ResNet18 , and its direct modification into an ODE , the discrete model has the shortest running time ; the running time of ODE is roughly 4 times longer than the discrete model , when trained both with our method and the adjoint method in [ 2 ] . The longer running time comes from the ODE model , where multiple steps need to be determined for numerical integration . 5.Code and pretrained weights : https : //www.dropbox.com/sh/sgaid3efh4eqmjl/AAB-DFXvNq_Pf313UqSLl4VPa ? dl=0 6 . Considering the comments and confusion from all reviewers , we clarify our contributions : ( a ) A general method for ODE solver that works well in practice -- -- Our method is applicable to both CNN-ODE and graph-ODE of different structures , and deals with a fundamental problem : how to estimate the gradient . -- -- To our knowledge , this is the first paper that makes neural-ODE achieve comparable or even higher accuracy in benchmark classification tasks compared to a state-of-the-art discrete-layer network . -- -- -A general method for free-form functions , and truly designed for continuous models , as discussed in thread A . ( b ) Application to graph networks enables new theoretical aspects , such as over-smoothing phenomena , which is in part discussed in response to R1 ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=B1esk0ZHjS ) [ 1 ] Nadarajah , Siva , and Antony Jameson . `` Studies of the continuous and discrete adjoint approaches to viscous automatic aerodynamic shape optimization . `` 2001 . [ 2 ] Chen , Tian Qi , et al . `` Neural ordinary differential equations . '' Advances in neural information processing systems . 2018 ."}, "2": {"review_id": "SJg9z6VFDr-2", "review_text": "Summary The authors discussed that most graph NNs to date considered discrete layers and hence are difficult to model diffusion processes on graphs. This paper proposed the Neural ODE on a graph, termed with a graph ODE, to tackle this problem. The authors gave a sufficient condition under which the adjoint method gets instable and pointed out potential issues of training Neural ODEs using it. To overcome the instability issue, the author proposed to backpropagate errors directly at discretized points. Since the naive implementation of the direct method is memory-consuming, the authors used invertible blocks as building blocks of graph ODEs, which do not store the intermediate activations for backward propagation. Finally, the authors conducted empirical studies to see the effectiveness of the proposed method. Decision I recommend rejecting the paper weakly because I think the extension of Neural ODEs to graphs is straightforward and that the empirical study is not strong enough to compensate for the weakness of the novelty. Theoretical justification of numerical instability of Neural ODEs (Proposition 1) and its empirical verification (Section 5.4) give new insights for understanding Neural ODEs. However, if I understand correctly, the formulation of graph ODEs do not use the internal structures of graph NNs, even the fact that the underlying neural network is a graph NN. Therefore, I think the extension from Neural ODEs to graphs is a bit too straightforward. The authors proposed a method which improves the stability and memory-efficiency of training. We can apply this method to general Neural ODEs, too. In addition, we can attribute the idea of using invertible blocks to existing works (Gomez et al., 2017). Finally, regarding the empirical performance of graph ODEs, the performance improvement from existing GNNs is within the standard deviations. Therefore, I think the empirical result is not sufficiently strong to justify the novelty of applying Neural ODEs to graphs. Taking these things into account, although the authors gave a new result on Neural ODEs, I think the contribution is limited from the viewpoint of the study of graph NNs. Suggestion - As I wrote in the Decision section, the theoretical results are not restricted to graph ODEs but valid for general neural ODEs. Therefore, I think the authors do not have to restrict the application areas to graph ODEs. The possible direction of the paper is to further analysis of training neural ODEs (e.g., instability). On the other hand, if the authors are interested in the extension of neural ODEs to graphs, I expect a more detailed relationship between the neural ODEs framework and underlying GNNs. For example, I am curious how the topological information of graphs affects the graph ODEs via spectral-type graph convolution operations and what is the relationship to the oversmoothing phenomena (Li et al., 2018). - Since Theorem 1 is applicable not only graph ODEs but also Neural ODEs, it implies that ordinal Neural ODEs are also vulnerable to the instability. The experiments in Dupont (2019), which this paper referenced, pointed out the instability of Neural ODEs. I am wondering the proposed method can enhance the training of neural ODEs, too. [Li et al., 2018] Li, Qimai, Zhichao Han, and Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.", "rating": "3: Weak Reject", "reply_text": "We sincerely thank the reviewer for your efforts and comments . We will address the reviewer \u2019 s concerns and state our contributions : 1 . Contribution to graph neural networks , connection with over-smoothing phenomena -- -- - ( a ) intuition We sincerely thank the reviewer for pointing out that ODE and the over-smoothing issue of GCN could be related . In fact , we did not start this project with neural ODE ; instead we started with an intuition that GNN could be related to a continuous \u201c heat transfer \u201d model , then we tried to build a good model for the continuous process , so we started looking at ODEs but found current method can not train an ODE well . In the heat-transfer model , the feature of each node is analogous to temperature , and the messaging passing process is analogous to heat transfer between nodes . Time of heat transfer process is analogous to depths of discrete-layer GCN models . If time is too short , there will be insufficient heat transfer , equivalent to a node not having enough information from its neighbors to update ; if time is too long , all nodes end up in the same temperature , corresponding to the over-smoothing issue with GNN . -- -- ( b ) Mathematical analysis We sincerely thank the reviewer for pointing out previous work on over-smoothing , which leads us from intuition to some mathematical analysis . We show our analysis below : It \u2019 s demonstrated that graph convolution is a special case of Laplacian smoothing in [ 1 ] , which can be written as $ Y = ( I - \\gamma \\tilde { D } ^ { -1 } \\tilde { L } ) X $ where $ X $ and $ Y $ are the input and output of a graph-conv layer respectively , $ \\tilde { A } =A+I $ where $ A $ is the adjacency matrix , and $ \\tilde { D } $ is the corresponding degree matrix of $ \\tilde { A } $ , and $ \\gamma $ is a positive scaling constant . If we replace normalized Laplacian with symmetricly normalized Laplacian , the continuous process becomes : $ Y = ( I - \\gamma \\tilde { D } ^ { -1/2 } \\tilde { L } \\tilde { D } ^ { -1/2 } ) X $ When it \u2019 s modified from a discrete model to a continuous model , the continuous smoothing process is : $ \\frac { dX } { dt } = - \\gamma \\tilde { D } ^ { -1/2 } \\tilde { L } \\tilde { D } ^ { -1/2 } X $ Since all eigenvalues of the symmetrically normalized Laplacian are real and non-negative , then all eigenvalues of the above ODE are real and non-positive . Suppose all eigenvalues of the normalized Laplacian are non-zero . In this case , the ODE has only negative eigenvalues , hence the ODE above is $ asymptotically \\ stable $ [ 2 ] . Hence as time $ t $ grows sufficiently large , all trajectories are close enough . In experiments , this suggests if integration time $ T $ is large enough , all nodes ( from different classes ) will have very similar features , thus the classification accuracy will drop . -- -- ( c ) Experiments We validate this with experiments . Node classification accuracy varying with integration time is measured on the Cora dataset . Results are shown below : \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Integration time 0.5 1.0 1.5 2.0 5.0 10.0 20.0 100.0 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Accuracy 80.5 81.6 80.1 80.1 79.3 77.6 68.9 35.1 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 When integration time is small ( 0.5 ) , nodes do not aggregate neighbor information . When integration time is too long , node attributes tend to the same value because of asymptotic behaviour of ODE , as mentioned above . Both cases generate inferior accuracy . 2.Generalization to general ODEs . We show that our method can be applied to general ODEs , with free-form functions , can deal with continuous models , and are robust to different ODE solvers . Details are in appendix C of original version ( Sec 5.3 in revision ) , and we post it in thread A ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=B1eeWWA7sr ) . 3.Our method supports free-form functions and does not depend on invertible block Details are in thread B ( https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=SkgzfxCmoH ) . Our method is memory efficient for free-form functions , with the special case of invertible block we achieve even lower memory . Hence the contribution can not be simply attributed to invertible block . 4.Better performance for both graph network and CNN We validate our method both in CNN-ODE ( thread A.3 ) and GNN-ODE ( thread C https : //openreview.net/forum ? id=SJg9z6VFDr & noteId=HklsTJ07jB ) . The CNN-ODE is a * free-form * function ; for GNN-ODE , we tested both free-form and invertible block ( restricted form ) . Our method significantly outperforms ( under paired t-test ) discrete models for graph networks ; it reduces error rate from 19 % to 5 % on CIFAR10 compared with results using the adjoint method [ 3 ] and Augmented-NODE [ 4 ] . [ 1 ] Li , Qimai , et al . `` Deeper insights into graph convolutional networks for semi-supervised learning '' 2018 . [ 2 ] Lyapunov , A. M. The General Problem of the Stability of Motion [ 3 ] Chen , Tian Qi , et al . `` Neural ordinary differential equations . '' 2018 [ 4 ] Dupont , et al . `` Augmented neural odes . '' ( 2019 ) ."}}