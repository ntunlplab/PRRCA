{"year": "2019", "forum": "SJekyhCctQ", "title": "Detecting Adversarial Examples Via Neural Fingerprinting", "decision": "Reject", "meta_review": "* Strengths\n\nThe paper proposes a novel and interesting method for detecting adversarial examples, which has the advantage of being based on general \u201cfingerprint statistics\u201d of a model and is not restricted to any specific threat model (in contrast to much of the work in the area which is restricted to adversarial examples in some L_p norm ball). The writing is clear and the experiments are extensive.\n\n* Weaknesses\n\nThe experiments are thorough. However, they contain a subtle but important flaw. During discussion it was revealed that the attacks used to evaluate the method fail to reduce accuracy even at large values of epsilon where there are simple adversarial attacks that should reduce the accuracy to zero. This casts doubt on whether the attacks at small values of epsilon really are providing a good measure of the method\u2019s robustness.\n\n* Discussion\n\nThere was substantial disagreement about the paper, with R1 feeling that the evaluation issues were serious enough to merit rejection and R3 feeling that they were not a large issue. In discussion with me, both R1 and R3 agreed that if an attack were demonstrated to break the method, that would be grounds for rejection. They also both agreed that there probably is an attack that breaks the method. A potential key difference is that R3 thinks this might be quite difficult to find and so merits publishing the paper to motivate stronger attacks.\n\nI ultimately agree with R1 that the evaluation issues are indeed serious. One reason for this is that there is by now a long record of adversarial defense papers posting impressive numbers that are often invalidated within a short period (often less than a month or so) of the paper being published. The \u201cObfuscated Gradients\u201d paper of Athalye, Carlini, and Wagner suggests several basic sanity checks to help avoid this. One of the sanity checks (which the present paper fails) is to test that attacks work when epsilon is large. This is not an arbitrary test but gets at a key issue---any given attack provides only an *upper bound* on the worst-case accuracy of a method. For instance, if an attack only brings the accuracy of a method down to 80% at epsilon=1 (when we know the true accuracy should be 0%), then at epsilon=0.01 we know that the measured accuracy of the attack comes 80% from the over-optimistic accuracy at epsilon=1 and at most 20% from the true accuracy at epsilon=0.01. If the measured accuracy at epsilon=1 is close to 100%, then accuracy at lower values of epsilon provides basically no information. This means that the experiments as currently performed give no information about the true accuracy of the method, which is a serious issue that the authors should address before the paper can be accepted.", "reviews": [{"review_id": "SJekyhCctQ-0", "review_text": "This work introduces a novel defense method \"Neural Fingerprinting\" against adversarial examples. In the training process, this method embeds a set of characteristic labeled samples so that responses of the model around real data show a specific pattern. The defender can detect if a given query is adversarial or not by checking the pattern at test time. Strong point: The strong point is that the proposed method seems to be appropriate and technically original. The performance is well investigated and compared with several competitors. The organization is good and the idea is clearly stated. Weak point: One question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability. Comments: The presented method seems to have a certain similarity with digital watermarking of deep neural networks, for example: https://gzs715.github.io/pubs/WATERMARK_ASIACCS18.pdf It would be interesting to mention to these methods in the related work section. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . As detailed in our response to Reviewer 1 , we are working on new experiments with randomized restarts for PGD with * many * iterations . We will report back with the numbers for our experiments with randomized restarts soon . Please see our response to Reviewer 1 for a detailed discussion with regard to the details of our new experiments and specific comments on gradient masking . Even though our loss is non-convex , the fingerprint loss L_ { fp } is differentiable and continuous , and it is not obvious to us that any of the attacks we study are likely to succeed against NeuralFP , even with a larger distortion budget . We believe this is because finding \u201c realistic \u201d images in a high-dimensional space starting from the vicinity of the current image may not be an easy task . Besides the experiments with randomized restarts , which we are currently running , if you have further specific suggestions to improve the paper and the evaluation , we would be glad to hear them . We would like to also point out that the Carlini-Wagner attack ( https : //arxiv.org/abs/1608.04644 ) was compared with that of exhaustive search using a solver-based approach ( https : //arxiv.org/abs/1709.10207 ) . It was shown that the Carlini-Wagner attack does not necessarily succeed in finding the optimal attack . Note that exhaustive search comes with tremendous computation costs ( the problem is NP-hard ! ) . This implies that there is no guarantee that the CW attack does indeed find the global minima , and finding the global optima through exhaustive search might be intractable . Also , we would like to note again that the SPSA attack and black-box attacks have been shown to overcome defenses that mask gradients . However , NeuralFP performs quite well against these attacks ."}, {"review_id": "SJekyhCctQ-1", "review_text": "This paper proposes a new technique for detecting adversarial examples by introducing \"fingerprints\" into the landscape while training, and exploiting the fingerprints at test time to detect adversarial examples. The idea is novel and the paper is well-written, but concerns about gradient masking prevent me from recommending acceptance just yet. Positives: The paper is extremely well-written, and the approach is clear and presented well. The authors also clearly put significant effort into the evaluation, and accurately/consistently describe threat models that they consider. The approach is also clearly novel, and is interesting. Concerns: My biggest concern is that this detection mechanism masks gradients in its loss function. The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such \u201cunfriendly\u201d landscapes can usually be circumvented) Some concrete evaluation concerns and experiments that the authors can run to alleviate them: - Figure 5 shows adversarial robustness even against eta = 0.25---at this value of epsilon, the attacker should be able to create realistic images of other classes (even without PGD), so this suggests that the loss is somehow making examples hard to find rather than removing them. The authors should address this issue. - Showing that at a sufficiently high eta attacks start to succeed is also useful - Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* more steps for binary search (right now it looks like the binary search is looking in a space of size 10^6 with 10 steps, which only has a granularity of about 5k, which means you never see any value < 5k in a bisection search, which casts into doubt all of these results) - The AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough) - The method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method. I also apologize if some of these concerns about gradient masking seem unsubstantiated; that said, I tried to run the code given in the paper, but got several OOM and other errors (utils modules not found, and PyTorch deprecations), even on a machine with 8 12GB-memory GPUs. If the authors can provide instructions for running the code I will be happy to test it and alleviate some of my own concerns. I also tried to reimplement the approach, but did not manage to finish before the review deadline. If I am able to reimplement the approach I will update my review accordingly. Some smaller comments on the paper: A consolidated set of tables for attack parameters in an appendix is needed - Page 4 last paragraph line 4 find the subset that \u201csatisfies\u201d instead of \u201csatisfy\u201d - Page 5 paragraph 1 line 1 defender,for needs a space before for - Page 5 paragraph right before theorem 1 last line Here, for detection needs a , after detection - Page 7 paragraph 1 line 2 (2 hidden layers the 2 should be written two - Page 7 second last paragraph line 2 \u201cis chosen\u201d instead of \u201care chosen\u201d - Page 7 last paragraph line 2 \u201cacross attacks\u201d needs a , after - Page 9 table 6 label line 2 \u201cdoes not shown\u201d should be \u201cshow\u201d instead of \u201cshown\u201d - Page 9 last line \u201cmeasure of robustness\u201d remove \"of\" [1] https://arxiv.org/pdf/1802.00420.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "Before , we address your other issues we would like to immediately get you to be able to run our code . We run our code on a machine with 2 GPUs , so the OOM error is quite surprising . Could you please let us know if you have access to AWS instances ? We have been able to run our experiments on the AWS DeepLearning AMI . I can make a public AMI and share it with you if that 's easier . I am able to run our code for MNIST on an Ubuntu 16.04 machine with a 16GB RAM ( no GPU ) . The commands I am running on the Ubuntu machine are as follows : 1. cd into the directory with run.sh after extracting the files from the tar-ball 2 . Run the command 'export PYTHONPATH= $ ( pwd ) : $ PYTHONPATH ' 3 . Run the command './run.sh mnist train attack eval nogrid 50 0.1 10 ' Also , this runs with python 2.7 and we have not tested with python 3 or above . We have tested our code on multiple linux platforms with the following specific software versions : > > > torch.__version__ ' 0.3.0.post4 ' > > > torchvision.__version__ ' 0.2.0 ' > > > tensorflow ' 1.4.0 ' > > > keras ' 2.1.0 ' We are actively currently working on running PGD ( with random restarts ) and SPSA with many more iterations . We will provide detailed answers to your questions , and do our best to get you running with the code ASAP ! Engineering the whole project is considerable effort , we strongly recommend trying to run our scripts which we have shared ."}, {"review_id": "SJekyhCctQ-2", "review_text": "This paper proposes a method for the detection of adversarial examples via what the authors term \"neural fingerprinting\" (NeuralFP). Essentially, a reference collection of perturbations are applied to the training data so as to learn the effects on the classification decision. The premise here is that on average, normal examples from a common class would have similar changes in the classification decision when reference perturbations are applied, whereas adversarial examples (particularly those off the local submanifold) may have a markedly different set of changes from what was expected for the targeted class. These reference perturbations as well as the anticipated output perturbations together form the \"fingerprints\". To measure the difference between observed outputs and fingerprints, the average (squared?) Euclidean distance is used. Given a fixed set of input fingerprints (presumably chosen so as to provide coverage of the range of possible perturbation directions), the authors use the distance formula as a regression loss (\"fingerprint loss\") to train the choice of output fingerprints. Although the authors do not explicitly state it this way, this secondary training objective encourages a K-Means style clustering of output perturbations where the output fingerprints serve as cluster representatives. This learning formulation is to my mind is both very innovative and extremely effective, as demonstrated by the authors' experimental results. Their experiments show superlative performance (near perfect detection!) against essentially the full range of state-of-the-art attacks. They give careful attention to the mode of attack, and show excellent performance even for adaptive white-box attacks, in which existing attack methods are given the opportunity to minimize the fingerprint loss. The presentation of the paper is excellent - clear, well-motivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick. Overall, the reported results are so good, and the approach so convincing, that one wonders what the weaknesses of the approach might be (if any). Questions that do come to mind are: * Can an adversarial strategy can be developed that could execute a successful attack while minimizing the fingerprint loss. * Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades? * What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.) Overall, this is a very strong and important result, fully deserving of acceptance. P.S. Two sets of typos that need attention: * In Equation 3, the Euclidean norm is taken. In Equation 5, the squared Euclidean norm is taken. Presumably, one of these is a typo. Which? * In the definition of delta-min and delta-max in the first paragraph of Section 2.2, y-hat should be w-hat. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for the comments and the feedback , we are glad you found our contributions interesting and promising . Addressing your questions : -- \u201c Can an adversarial strategy be developed that could execute a successful attack while minimizing the fingerprint loss. \u201d This is an interesting question , if one can indeed develop a new attack to break NeuralFP . We are actively investigating this , and we believe that NeuralFP will result in better attacks being formulated and eventually , better defenses . -- \u201c Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades ? \u201d From the few experiments we report on MiniImagenet-20 , we see good detection rates on FGSM and BIM . Also , when moving from MNIST to CIFAR-10 , the classification performance degrades quite a bit , while NeuralFP \u2019 s performance nearly stays the same . We think these are promising signs that NeuralFP would be beneficial even on fragmented data-sets where classification is harder . However , conclusively answering this question concretely would need a thorough investigation . -- \u201c What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds ... would fewer perturbations be required ? ( It would seem that reducing the number of perturbations needed could have a significant effect on training time . ) \u201d The increased training time ( and memory ) is indeed one of the biggest drawbacks of our approach , and the suggested idea of using a more principled approach for choosing the perturbation directions could indeed result in faster training , and possibly , better detection of outliers . We believe this is the immediate next step towards making NeuralFP more accessible on more complex tasks like ImageNet . Thanks for pointing out the connection to K-means clustering , that is an insightful interpretation . We will consider this and see if it sheds more light on our approach , and helps us improve NeuralFP ."}], "0": {"review_id": "SJekyhCctQ-0", "review_text": "This work introduces a novel defense method \"Neural Fingerprinting\" against adversarial examples. In the training process, this method embeds a set of characteristic labeled samples so that responses of the model around real data show a specific pattern. The defender can detect if a given query is adversarial or not by checking the pattern at test time. Strong point: The strong point is that the proposed method seems to be appropriate and technically original. The performance is well investigated and compared with several competitors. The organization is good and the idea is clearly stated. Weak point: One question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability. Comments: The presented method seems to have a certain similarity with digital watermarking of deep neural networks, for example: https://gzs715.github.io/pubs/WATERMARK_ASIACCS18.pdf It would be interesting to mention to these methods in the related work section. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . As detailed in our response to Reviewer 1 , we are working on new experiments with randomized restarts for PGD with * many * iterations . We will report back with the numbers for our experiments with randomized restarts soon . Please see our response to Reviewer 1 for a detailed discussion with regard to the details of our new experiments and specific comments on gradient masking . Even though our loss is non-convex , the fingerprint loss L_ { fp } is differentiable and continuous , and it is not obvious to us that any of the attacks we study are likely to succeed against NeuralFP , even with a larger distortion budget . We believe this is because finding \u201c realistic \u201d images in a high-dimensional space starting from the vicinity of the current image may not be an easy task . Besides the experiments with randomized restarts , which we are currently running , if you have further specific suggestions to improve the paper and the evaluation , we would be glad to hear them . We would like to also point out that the Carlini-Wagner attack ( https : //arxiv.org/abs/1608.04644 ) was compared with that of exhaustive search using a solver-based approach ( https : //arxiv.org/abs/1709.10207 ) . It was shown that the Carlini-Wagner attack does not necessarily succeed in finding the optimal attack . Note that exhaustive search comes with tremendous computation costs ( the problem is NP-hard ! ) . This implies that there is no guarantee that the CW attack does indeed find the global minima , and finding the global optima through exhaustive search might be intractable . Also , we would like to note again that the SPSA attack and black-box attacks have been shown to overcome defenses that mask gradients . However , NeuralFP performs quite well against these attacks ."}, "1": {"review_id": "SJekyhCctQ-1", "review_text": "This paper proposes a new technique for detecting adversarial examples by introducing \"fingerprints\" into the landscape while training, and exploiting the fingerprints at test time to detect adversarial examples. The idea is novel and the paper is well-written, but concerns about gradient masking prevent me from recommending acceptance just yet. Positives: The paper is extremely well-written, and the approach is clear and presented well. The authors also clearly put significant effort into the evaluation, and accurately/consistently describe threat models that they consider. The approach is also clearly novel, and is interesting. Concerns: My biggest concern is that this detection mechanism masks gradients in its loss function. The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such \u201cunfriendly\u201d landscapes can usually be circumvented) Some concrete evaluation concerns and experiments that the authors can run to alleviate them: - Figure 5 shows adversarial robustness even against eta = 0.25---at this value of epsilon, the attacker should be able to create realistic images of other classes (even without PGD), so this suggests that the loss is somehow making examples hard to find rather than removing them. The authors should address this issue. - Showing that at a sufficiently high eta attacks start to succeed is also useful - Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* more steps for binary search (right now it looks like the binary search is looking in a space of size 10^6 with 10 steps, which only has a granularity of about 5k, which means you never see any value < 5k in a bisection search, which casts into doubt all of these results) - The AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough) - The method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method. I also apologize if some of these concerns about gradient masking seem unsubstantiated; that said, I tried to run the code given in the paper, but got several OOM and other errors (utils modules not found, and PyTorch deprecations), even on a machine with 8 12GB-memory GPUs. If the authors can provide instructions for running the code I will be happy to test it and alleviate some of my own concerns. I also tried to reimplement the approach, but did not manage to finish before the review deadline. If I am able to reimplement the approach I will update my review accordingly. Some smaller comments on the paper: A consolidated set of tables for attack parameters in an appendix is needed - Page 4 last paragraph line 4 find the subset that \u201csatisfies\u201d instead of \u201csatisfy\u201d - Page 5 paragraph 1 line 1 defender,for needs a space before for - Page 5 paragraph right before theorem 1 last line Here, for detection needs a , after detection - Page 7 paragraph 1 line 2 (2 hidden layers the 2 should be written two - Page 7 second last paragraph line 2 \u201cis chosen\u201d instead of \u201care chosen\u201d - Page 7 last paragraph line 2 \u201cacross attacks\u201d needs a , after - Page 9 table 6 label line 2 \u201cdoes not shown\u201d should be \u201cshow\u201d instead of \u201cshown\u201d - Page 9 last line \u201cmeasure of robustness\u201d remove \"of\" [1] https://arxiv.org/pdf/1802.00420.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "Before , we address your other issues we would like to immediately get you to be able to run our code . We run our code on a machine with 2 GPUs , so the OOM error is quite surprising . Could you please let us know if you have access to AWS instances ? We have been able to run our experiments on the AWS DeepLearning AMI . I can make a public AMI and share it with you if that 's easier . I am able to run our code for MNIST on an Ubuntu 16.04 machine with a 16GB RAM ( no GPU ) . The commands I am running on the Ubuntu machine are as follows : 1. cd into the directory with run.sh after extracting the files from the tar-ball 2 . Run the command 'export PYTHONPATH= $ ( pwd ) : $ PYTHONPATH ' 3 . Run the command './run.sh mnist train attack eval nogrid 50 0.1 10 ' Also , this runs with python 2.7 and we have not tested with python 3 or above . We have tested our code on multiple linux platforms with the following specific software versions : > > > torch.__version__ ' 0.3.0.post4 ' > > > torchvision.__version__ ' 0.2.0 ' > > > tensorflow ' 1.4.0 ' > > > keras ' 2.1.0 ' We are actively currently working on running PGD ( with random restarts ) and SPSA with many more iterations . We will provide detailed answers to your questions , and do our best to get you running with the code ASAP ! Engineering the whole project is considerable effort , we strongly recommend trying to run our scripts which we have shared ."}, "2": {"review_id": "SJekyhCctQ-2", "review_text": "This paper proposes a method for the detection of adversarial examples via what the authors term \"neural fingerprinting\" (NeuralFP). Essentially, a reference collection of perturbations are applied to the training data so as to learn the effects on the classification decision. The premise here is that on average, normal examples from a common class would have similar changes in the classification decision when reference perturbations are applied, whereas adversarial examples (particularly those off the local submanifold) may have a markedly different set of changes from what was expected for the targeted class. These reference perturbations as well as the anticipated output perturbations together form the \"fingerprints\". To measure the difference between observed outputs and fingerprints, the average (squared?) Euclidean distance is used. Given a fixed set of input fingerprints (presumably chosen so as to provide coverage of the range of possible perturbation directions), the authors use the distance formula as a regression loss (\"fingerprint loss\") to train the choice of output fingerprints. Although the authors do not explicitly state it this way, this secondary training objective encourages a K-Means style clustering of output perturbations where the output fingerprints serve as cluster representatives. This learning formulation is to my mind is both very innovative and extremely effective, as demonstrated by the authors' experimental results. Their experiments show superlative performance (near perfect detection!) against essentially the full range of state-of-the-art attacks. They give careful attention to the mode of attack, and show excellent performance even for adaptive white-box attacks, in which existing attack methods are given the opportunity to minimize the fingerprint loss. The presentation of the paper is excellent - clear, well-motivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick. Overall, the reported results are so good, and the approach so convincing, that one wonders what the weaknesses of the approach might be (if any). Questions that do come to mind are: * Can an adversarial strategy can be developed that could execute a successful attack while minimizing the fingerprint loss. * Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades? * What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.) Overall, this is a very strong and important result, fully deserving of acceptance. P.S. Two sets of typos that need attention: * In Equation 3, the Euclidean norm is taken. In Equation 5, the squared Euclidean norm is taken. Presumably, one of these is a typo. Which? * In the definition of delta-min and delta-max in the first paragraph of Section 2.2, y-hat should be w-hat. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for the comments and the feedback , we are glad you found our contributions interesting and promising . Addressing your questions : -- \u201c Can an adversarial strategy be developed that could execute a successful attack while minimizing the fingerprint loss. \u201d This is an interesting question , if one can indeed develop a new attack to break NeuralFP . We are actively investigating this , and we believe that NeuralFP will result in better attacks being formulated and eventually , better defenses . -- \u201c Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades ? \u201d From the few experiments we report on MiniImagenet-20 , we see good detection rates on FGSM and BIM . Also , when moving from MNIST to CIFAR-10 , the classification performance degrades quite a bit , while NeuralFP \u2019 s performance nearly stays the same . We think these are promising signs that NeuralFP would be beneficial even on fragmented data-sets where classification is harder . However , conclusively answering this question concretely would need a thorough investigation . -- \u201c What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds ... would fewer perturbations be required ? ( It would seem that reducing the number of perturbations needed could have a significant effect on training time . ) \u201d The increased training time ( and memory ) is indeed one of the biggest drawbacks of our approach , and the suggested idea of using a more principled approach for choosing the perturbation directions could indeed result in faster training , and possibly , better detection of outliers . We believe this is the immediate next step towards making NeuralFP more accessible on more complex tasks like ImageNet . Thanks for pointing out the connection to K-means clustering , that is an insightful interpretation . We will consider this and see if it sheds more light on our approach , and helps us improve NeuralFP ."}}