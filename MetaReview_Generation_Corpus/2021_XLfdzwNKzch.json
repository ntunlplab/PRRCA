{"year": "2021", "forum": "XLfdzwNKzch", "title": "SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning", "decision": "Accept (Poster)", "meta_review": "The paper proposes a novel method for greed layer-wise training by considering the learning signal from either backprop or from the additional auxiliary losses. SEarching for DecOupled Neural Architecture learns to identify the decoupled blocks by learning the gating parameters similar to gradient-based architecture search algorithms, such as DARTs.  The empirical experiments demonstrated the effectiveness of SEDONA on CIFAR and TinyImageNet using various ResNet architectures. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in the way that satisfied the reviewers. The ideas in this paper are interesting and are broadly applicable. Additional experiments / discussions on the tradeoff between initial search cost and accuracy should be included in the final version. ", "reviews": [{"review_id": "XLfdzwNKzch-0", "review_text": "The paper proposes a method for decoupled training of neural networks called SEDONA . In the spirit of recent trends in greedy layer-wise and indirect training , SEDONA allows gradient information to flow either from the next layer as in backpropagation or from an auxiliary head , trying to make a prediction using the current layer 's output . Since a direct search for the best decoupled configuration results into probing a combinatorial number of splits , authors propose a continuously relaxed formulation which they later discretize . Transferring the found decoupled configurations to datasets different from the `` pretraining '' ones shows improvements in terms of validation accuracy and training time . I find the overall topic of asynchronous or backpropagation-free training of neural networks very interesting and the submitted paper particularly relevant to this topic . The proposed search algorithm is novel to my knowledge and may be found useful in the community . However , I think authors should have put more effort into understanding SEDONA . The major points are the following : 1 . The effect of discretization is not studied . I would be interested in seeing how well does a continuous configuration perform and what kind of accuracy loss ( if any ) is caused by discretization . 2.I wonder why SEDONA performs better than backprogation ( sometimes much better as in Tiny ImageNet ) ? Is it because not following the true ( stochastic ) gradient acts as regularization ? One way of assessing this would be to also provide training loss or accuracy values . Why is your top1 error value for ResNet-152 is about two percent higher that in the original paper by He et al , 2015 ? 3.If the update directions configured by SEDONA lead to such a good performance , how do they correspond to the true gradients ? Do they correlate positively ? To what extent can such decoupled feedback implement credit assignment in a neural network ? I believe that addressing these questions is very important prior to publication of the method . Minor comments : 1 . $ \\theta $ does not appear on the RHS of ( 1 ) . 2.Auxiliary head ensembling is not explained very clearly .", "rating": "7: Good paper, accept", "reply_text": "* * 4 . ( Q3 ) How do the update direction of SEDONA correspond to the true gradients ? Do they correlate positively ? * * * * A . * * We found that both the gradients of SEDONA and DGL correlate positively with the true ( backprop ) gradients . To measure the correlation , we computed the cosine similarity between backprop gradients and SEDONA/DGL gradients of 3x3 convolution kernel in each layer on CIFAR-10 . Below we show the average cosine similarities for each timestep interval as well as the overall cosine similarity . However , the correlation in SEDONA is higher than in DGL at all time step intervals across all architectures . | ( VGG-19 ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; ( 0,12.8K ] & nbsp ; | & nbsp ; ( 12.8K,25.6K ] & nbsp ; | & nbsp ; ( 25.6K,38.4K ] & nbsp ; | & nbsp ; ( 38.4K,51.2K ] & nbsp ; | & nbsp ; ( 51.2K,64K ] & nbsp ; | & nbsp ; Overall & nbsp ; | | : -| : -- : | : :| : :| : :| : - : | : :| | DGL | 0.7995 | 0.8115 | 0.7861 | 0.7288 | 0.6529 | 0.7558 | | SEDONA | 0.8007 | 0.8453 | 0.8230 | 0.7673 | 0.6876 | 0.7848 | | ( ResNet-50 ) & nbsp ; & nbsp ; | & nbsp ; ( 0,12.8K ] & nbsp ; | & nbsp ; ( 12.8K,25.6K ] & nbsp ; | & nbsp ; ( 25.6K,38.4K ] & nbsp ; | & nbsp ; ( 38.4K,51.2K ] & nbsp ; | & nbsp ; ( 51.2K,64K ] & nbsp ; | & nbsp ; Overall & nbsp ; | | : -| : -- : | : :| : :| : :| : - : | : :| | DGL | 0.7432 | 0.6925 | 0.6008 | 0.4487 | 0.3497 | 0.5670 | | SEDONA | 0.8128 | 0.8166 | 0.7583 | 0.6439 | 0.5094 | 0.7082 | | ( ResNet-101 * * * * A . * * Thank you for pointing out missing $ \\theta $ . We will update Equation ( 1 ) accordingly . * * 6 . ( C2 ) Details on auxiliary head ensemble * * * * A . * * For ensembling , we sum the log-softmax outputs from the last two blocks \u2019 auxiliary headers ( i.e.auxiliary headers of block $ K $ and $ K-1 $ ) and use it for prediction . We will add this to the draft . * * References * * [ 1 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In CVPR , 2016 . [ 2 ] https : //pytorch.org/hub/pytorch_vision_resnet"}, {"review_id": "XLfdzwNKzch-1", "review_text": "# # # Summary This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup . The approach achieves better performance than using backprop on small datasets ( CIFAR10 and TinyImageNet ) , and comparable or slightly improved performance on ImageNet with 2x claimed training speedup . Learned network architecture choices seem to transfer between datasets . # # # Strong points S1 : The method outperforms comparable baselines , and significantly improves performance over standard backprop for small datasets ( CIFAR10 and TinyImageNet ) S2 : The learned network architectures transfer between tasks . S3 : On ImageNet , the method achieves slightly improved performance with claimed 2x training speedup . # # # Weak points W1 : This seems like a pretty complicated approach to only get 2x speedup in training , and besides slight improvement in performance , that seems like the only benefit this method achieves for realistically large datasets like ImageNet . W2 : `` Speedup '' is not really defined . i assume this is the wall-clock time to achieve convergence , but I could n't find a definition of convergence or stopping criterion , or how speedup is measured . W3 : Complicated optimization tricks seem necessary to get the bilevel optimization to work ( Section 3.3 : `` In bilevel optimization , meta variables ( \u03b1 , \u03b2 ) depend on the learning trajectory of layer and auxiliary weights ( \u03b8 , \u03c6 ) ( i.e.a sequence of values of ( \u03b8 , \u03c6 ) during inner optimization ) . As a consequence , there exists a risk of overfitting the meta variables to a specific episode '' ) . If I have the choice between 2x training time with straight-forward optimization , versus 1x training time with complicated bilevel optimization that needs to be tuned , I would probably choose the former option , since I need to spend a lot less time debugging and tuning parameters . # # # Recommendation I think this paper is marginally below the acceptance threshold . The results seem decent , though a little underwhelming ( see W1 ) , but the biggest concern for me is that key concepts are unclear or ambiguous ( e.g.definition of speedup is unclear , see W2 , `` confidence of \\alpha at lower layers '' , see C2 , and how the `` backprop '' gradients are computed and used in equation ( 2 ) , see Q2 ) . If these issues were clarified I would be inclined to increase my score . But as it stands , this feels like a complicated method without much payoff ( only 2x training speedup with slightly improved performance ) . It 's possible that I 'm missing something or not understanding the potential impact of the results , and explicit discussion of this and/or improved clarity of the description and evaluation of the method may help . # # # Questions Q1 : What is the absolute wall-clock time for training that the speedup is reported on ? Q2 : Given that the gradients used to train each block are a combination of local and backpropped gradients , how does this method avoid the locking problem , and thus provide speedup ? # # # Other comments C1 : The `` 2.02x '' speedup number looks a little funny ; are two decimal places justified ? Perhaps better to just say `` 2x speedup '' ? C2 : In Section 4.4 , the paper says , `` Surprisingly , SEDONA yields much more confident values of $ \\alpha_1^ { ( l ) } $ ( i.e.close to either 1 or 0 ) at lower layers than upper layers , '' , but when I look at Figure 4 , at first glance I thought that \\alpha is _less_ confident at lower iterations , according to the provided definition , compared to later iterations ( which are all at 1 ) . But then I realized the plot uses a log scale . Consider using a linear scale here instead ? Or at least describe the confidence in terms of the log scale . C3 : I find the usage of all-caps CONV to be a little odd , at least I 've never seem that before . Maybe just say `` convolution '' ? Does n't seem like CONV saves much space . C4 : I find this key statement to be unclear : `` denotes whether layer l should utilize local gradients ( i.e.the last layer of a block ) or backpropagated gradients ( i.e.the inside layer ) . '' Is this saying that if a local gradient is used for a layer , then it is a candidate for being the last layer of a block , and if backpropped gradient is used , then it is a candidate for being the inside layer of a block ? Some rewording of this sentence should help . C5 : Typos , in Appendix B : `` to flat the learning landscape '' - > `` to flatten the learning landscape '' , Section 4.1 : `` to let auxiliary networks in the pool computationally lightweight '' - > `` to let auxiliary networks in the pool be computationally lightweight '' , Section 3.3 : `` Such warm start '' - > `` Such a warm start '' , Section 1 : `` If local signals are lousy representative of the global goal '' - > `` If local signals are not representative of the global goal ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * 5 . ( C1 ) Perhaps better to just say `` 2x speedup '' ? * * * * A . * * Thanks for the suggestion . We will update the draft soon . * * 6 . ( C2 ) When I look at Figure 4 , at first glance I thought that \\alpha is less confident at lower iterations , according to the provided definition , compared to later iterations ( which are all at 1 ) . Consider using a linear scale here instead ? * * * * A . * * We initially chose the log scale because values of $ \\bar { \\alpha } ^ { ( l ) } _ { 1 } $ at first few layers are too close to 0 , and were not visible in the linear scale plot . However , we acknowledge that using log scale made it more difficult to understand the notion of confidence . Following your feedback , we will revise Figure 4 using a linear scale . * * 7 . ( C3 ) Maybe just say `` convolution '' ? * * * * A . * * We will replace \u201c CONV \u201d with \u201c convolution \u201d in the final draft . * * 8 . ( C4 ) The key statement seems to be unclear : `` denotes whether layer l should utilize local gradients ( i.e.the last layer of a block ) or backpropagated gradients ( i.e.the inside layer ) . `` * * * * A . * * Your understanding is correct . The structure of blocks can be expressed using signal variables $ \\alpha^ { ( l ) } $ . If $ \\alpha^ { ( l ) } =1 $ , the layer $ l $ is trained using local gradients , therefore becoming the last layer in a gradient-isolated block . If $ \\alpha^ { ( l ) } =0 $ , the layer $ l $ is trained using gradients backpropagated from its upper layer within a block , therefore becoming an inside layer of a gradient-isolated block ( see Equation ( 2 ) ) . We will rewrite the sentences clearly . * * 9 . ( C5 ) Typos * * * * A . * * Thank you for pointing out typos , which will be corrected . * * References * * [ 1 ] Eugene Belilovsky , Michael Eickenberg , and Edouard Oyallon . Decoupled greedy learning of cnns . In ICML , 2020 . [ 2 ] https : //pytorch.org/hub/pytorch_vision_resnet [ 3 ] Zhouyuan Huo , Bin Gu , Qian Yang , and Heng Huang . Decoupled parallel backpropagation with convergence guarantee . In ICML , 2018 . [ 4 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In CVPR , 2016 ."}, {"review_id": "XLfdzwNKzch-2", "review_text": "Summary Of Contributions : This paper proposes to automate the design of auxiliary network and its allocation under decoupled neural network scheme , a design that speeds up network training and potentially boost model accuracy . The approach is validated with leading performance on ResNet and VGGNet under various datasets . Strengths : The idea to search auxiliary network for decoupled neural network is novel , and the proposed method is verified on multiple widely used datasets . Weaknesses : 1 . It \u2019 s unclear if the proposed differentiable search algorithm works better than DARTs with direction comparison , as the proposed method is basically DARTs with some changes ( e.g.weight sampling , extra inner optimization steps per outer optimization ) , that are also applied to typical NAS tasks , a comparison with DARTs is needed to validate how much of the gain is from the proposed method rather than the fact of automating network design itself . 2.It seems that the decoupled neural network scheme has nearly no gains in terms of Top-1 without the auxiliary heads ensembled as shown in Table 3 . The details should be provided on how the ensemble works , are ensembled auxiliary heads kept during the testing stage ? If so , Table 3 can not tell if the gain is from the ensembled aux heads or decoupled neural network scheme . Network trained with Backprop method and identical extra aux head for ensemble might reach the same level of performance . 3.Experiment with DGL in Table3 ( b ) is problematic , DGL in its original paper only provides ResNet-152 result with K=2 , and it is better than Backprop according to their experiment . Yet the authors set K=4 in Table 3 and got a contrary result . This brought up the suspect of cherry picking of hyperparameters and can only be addressed with stricter comparison . 4.One of the advantages to adopt a decoupled neural network scheme is training speedup , while the network based on compared methods is instantly available , the proposed method requires extra time to search before training . With this extra cost , how much the training speedup benefit is still left ? The search cost should be indicated and the overall time cost should be discussed .", "rating": "7: Good paper, accept", "reply_text": "* * 5 . ( Q4 ) While the network based on compared methods is instantly available , the proposed method requires extra time to search before training . * * * * A . * * The review is correct that the compared methods do not require additional search steps . However , these methods yield worse performance than backprop . Moreover , their performances become worse as $ K $ increases ( Figure 3 ) , so the speedup is also limited in practice due to large sacrifices of accuracy . On the other hand , the decoupling configurations found by SEDONA do not suffer from these issues . SEDONA shows consistently similar or better performance with multiple off-the-shelf CNNs in multiple datasets compared to backprop . Moreover , SEDONA still performs similar or better than backprop up to $ K=16 $ ( Figure 3 ) . * * 6 . ( Q4 ) The search cost should be indicated . * * * * A . * * Search time of SEDONA is 0.6 , 1.1 , 1.9 and 3 days for VGG-19 , ResNet-50 , ResNet-101 and ResNet-152 , respectively , as described in Table 14 in Appendix ( Table 10 in the old draft ) . * * 7 . ( Q4 ) With this extra cost , how much of the training speedup benefit is still left ? The overall time cost should be discussed . * * * * A . * * Although SEDONA requires additional search time , the search stage is performed only once per network architecture . No additional search is required no matter whether the dataset changes or the number of desired blocks $ K $ changes . Moreover , since training times of ResNet-152 on ImageNet using backprop and SEDONA were 158.5 hours and 78.3 hours , respectively , the search cost is canceled out even when trained once . * * References * * [ 1 ] Eugene Belilovsky , Michael Eickenberg , and Edouard Oyallon . Greedy layerwise learning can scale to imagenet . In ICML , 2019 . [ 2 ] Eugene Belilovsky , Michael Eickenberg , and Edouard Oyallon . Decoupled greedy learning of cnns . In ICML , 2020 . [ 3 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In CVPR , 2016 ."}], "0": {"review_id": "XLfdzwNKzch-0", "review_text": "The paper proposes a method for decoupled training of neural networks called SEDONA . In the spirit of recent trends in greedy layer-wise and indirect training , SEDONA allows gradient information to flow either from the next layer as in backpropagation or from an auxiliary head , trying to make a prediction using the current layer 's output . Since a direct search for the best decoupled configuration results into probing a combinatorial number of splits , authors propose a continuously relaxed formulation which they later discretize . Transferring the found decoupled configurations to datasets different from the `` pretraining '' ones shows improvements in terms of validation accuracy and training time . I find the overall topic of asynchronous or backpropagation-free training of neural networks very interesting and the submitted paper particularly relevant to this topic . The proposed search algorithm is novel to my knowledge and may be found useful in the community . However , I think authors should have put more effort into understanding SEDONA . The major points are the following : 1 . The effect of discretization is not studied . I would be interested in seeing how well does a continuous configuration perform and what kind of accuracy loss ( if any ) is caused by discretization . 2.I wonder why SEDONA performs better than backprogation ( sometimes much better as in Tiny ImageNet ) ? Is it because not following the true ( stochastic ) gradient acts as regularization ? One way of assessing this would be to also provide training loss or accuracy values . Why is your top1 error value for ResNet-152 is about two percent higher that in the original paper by He et al , 2015 ? 3.If the update directions configured by SEDONA lead to such a good performance , how do they correspond to the true gradients ? Do they correlate positively ? To what extent can such decoupled feedback implement credit assignment in a neural network ? I believe that addressing these questions is very important prior to publication of the method . Minor comments : 1 . $ \\theta $ does not appear on the RHS of ( 1 ) . 2.Auxiliary head ensembling is not explained very clearly .", "rating": "7: Good paper, accept", "reply_text": "* * 4 . ( Q3 ) How do the update direction of SEDONA correspond to the true gradients ? Do they correlate positively ? * * * * A . * * We found that both the gradients of SEDONA and DGL correlate positively with the true ( backprop ) gradients . To measure the correlation , we computed the cosine similarity between backprop gradients and SEDONA/DGL gradients of 3x3 convolution kernel in each layer on CIFAR-10 . Below we show the average cosine similarities for each timestep interval as well as the overall cosine similarity . However , the correlation in SEDONA is higher than in DGL at all time step intervals across all architectures . | ( VGG-19 ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; ( 0,12.8K ] & nbsp ; | & nbsp ; ( 12.8K,25.6K ] & nbsp ; | & nbsp ; ( 25.6K,38.4K ] & nbsp ; | & nbsp ; ( 38.4K,51.2K ] & nbsp ; | & nbsp ; ( 51.2K,64K ] & nbsp ; | & nbsp ; Overall & nbsp ; | | : -| : -- : | : :| : :| : :| : - : | : :| | DGL | 0.7995 | 0.8115 | 0.7861 | 0.7288 | 0.6529 | 0.7558 | | SEDONA | 0.8007 | 0.8453 | 0.8230 | 0.7673 | 0.6876 | 0.7848 | | ( ResNet-50 ) & nbsp ; & nbsp ; | & nbsp ; ( 0,12.8K ] & nbsp ; | & nbsp ; ( 12.8K,25.6K ] & nbsp ; | & nbsp ; ( 25.6K,38.4K ] & nbsp ; | & nbsp ; ( 38.4K,51.2K ] & nbsp ; | & nbsp ; ( 51.2K,64K ] & nbsp ; | & nbsp ; Overall & nbsp ; | | : -| : -- : | : :| : :| : :| : - : | : :| | DGL | 0.7432 | 0.6925 | 0.6008 | 0.4487 | 0.3497 | 0.5670 | | SEDONA | 0.8128 | 0.8166 | 0.7583 | 0.6439 | 0.5094 | 0.7082 | | ( ResNet-101 * * * * A . * * Thank you for pointing out missing $ \\theta $ . We will update Equation ( 1 ) accordingly . * * 6 . ( C2 ) Details on auxiliary head ensemble * * * * A . * * For ensembling , we sum the log-softmax outputs from the last two blocks \u2019 auxiliary headers ( i.e.auxiliary headers of block $ K $ and $ K-1 $ ) and use it for prediction . We will add this to the draft . * * References * * [ 1 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In CVPR , 2016 . [ 2 ] https : //pytorch.org/hub/pytorch_vision_resnet"}, "1": {"review_id": "XLfdzwNKzch-1", "review_text": "# # # Summary This paper proposes a differentiable architecture search approach for splitting a deep network into locally-trained blocks to achieve training speedup . The approach achieves better performance than using backprop on small datasets ( CIFAR10 and TinyImageNet ) , and comparable or slightly improved performance on ImageNet with 2x claimed training speedup . Learned network architecture choices seem to transfer between datasets . # # # Strong points S1 : The method outperforms comparable baselines , and significantly improves performance over standard backprop for small datasets ( CIFAR10 and TinyImageNet ) S2 : The learned network architectures transfer between tasks . S3 : On ImageNet , the method achieves slightly improved performance with claimed 2x training speedup . # # # Weak points W1 : This seems like a pretty complicated approach to only get 2x speedup in training , and besides slight improvement in performance , that seems like the only benefit this method achieves for realistically large datasets like ImageNet . W2 : `` Speedup '' is not really defined . i assume this is the wall-clock time to achieve convergence , but I could n't find a definition of convergence or stopping criterion , or how speedup is measured . W3 : Complicated optimization tricks seem necessary to get the bilevel optimization to work ( Section 3.3 : `` In bilevel optimization , meta variables ( \u03b1 , \u03b2 ) depend on the learning trajectory of layer and auxiliary weights ( \u03b8 , \u03c6 ) ( i.e.a sequence of values of ( \u03b8 , \u03c6 ) during inner optimization ) . As a consequence , there exists a risk of overfitting the meta variables to a specific episode '' ) . If I have the choice between 2x training time with straight-forward optimization , versus 1x training time with complicated bilevel optimization that needs to be tuned , I would probably choose the former option , since I need to spend a lot less time debugging and tuning parameters . # # # Recommendation I think this paper is marginally below the acceptance threshold . The results seem decent , though a little underwhelming ( see W1 ) , but the biggest concern for me is that key concepts are unclear or ambiguous ( e.g.definition of speedup is unclear , see W2 , `` confidence of \\alpha at lower layers '' , see C2 , and how the `` backprop '' gradients are computed and used in equation ( 2 ) , see Q2 ) . If these issues were clarified I would be inclined to increase my score . But as it stands , this feels like a complicated method without much payoff ( only 2x training speedup with slightly improved performance ) . It 's possible that I 'm missing something or not understanding the potential impact of the results , and explicit discussion of this and/or improved clarity of the description and evaluation of the method may help . # # # Questions Q1 : What is the absolute wall-clock time for training that the speedup is reported on ? Q2 : Given that the gradients used to train each block are a combination of local and backpropped gradients , how does this method avoid the locking problem , and thus provide speedup ? # # # Other comments C1 : The `` 2.02x '' speedup number looks a little funny ; are two decimal places justified ? Perhaps better to just say `` 2x speedup '' ? C2 : In Section 4.4 , the paper says , `` Surprisingly , SEDONA yields much more confident values of $ \\alpha_1^ { ( l ) } $ ( i.e.close to either 1 or 0 ) at lower layers than upper layers , '' , but when I look at Figure 4 , at first glance I thought that \\alpha is _less_ confident at lower iterations , according to the provided definition , compared to later iterations ( which are all at 1 ) . But then I realized the plot uses a log scale . Consider using a linear scale here instead ? Or at least describe the confidence in terms of the log scale . C3 : I find the usage of all-caps CONV to be a little odd , at least I 've never seem that before . Maybe just say `` convolution '' ? Does n't seem like CONV saves much space . C4 : I find this key statement to be unclear : `` denotes whether layer l should utilize local gradients ( i.e.the last layer of a block ) or backpropagated gradients ( i.e.the inside layer ) . '' Is this saying that if a local gradient is used for a layer , then it is a candidate for being the last layer of a block , and if backpropped gradient is used , then it is a candidate for being the inside layer of a block ? Some rewording of this sentence should help . C5 : Typos , in Appendix B : `` to flat the learning landscape '' - > `` to flatten the learning landscape '' , Section 4.1 : `` to let auxiliary networks in the pool computationally lightweight '' - > `` to let auxiliary networks in the pool be computationally lightweight '' , Section 3.3 : `` Such warm start '' - > `` Such a warm start '' , Section 1 : `` If local signals are lousy representative of the global goal '' - > `` If local signals are not representative of the global goal ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * 5 . ( C1 ) Perhaps better to just say `` 2x speedup '' ? * * * * A . * * Thanks for the suggestion . We will update the draft soon . * * 6 . ( C2 ) When I look at Figure 4 , at first glance I thought that \\alpha is less confident at lower iterations , according to the provided definition , compared to later iterations ( which are all at 1 ) . Consider using a linear scale here instead ? * * * * A . * * We initially chose the log scale because values of $ \\bar { \\alpha } ^ { ( l ) } _ { 1 } $ at first few layers are too close to 0 , and were not visible in the linear scale plot . However , we acknowledge that using log scale made it more difficult to understand the notion of confidence . Following your feedback , we will revise Figure 4 using a linear scale . * * 7 . ( C3 ) Maybe just say `` convolution '' ? * * * * A . * * We will replace \u201c CONV \u201d with \u201c convolution \u201d in the final draft . * * 8 . ( C4 ) The key statement seems to be unclear : `` denotes whether layer l should utilize local gradients ( i.e.the last layer of a block ) or backpropagated gradients ( i.e.the inside layer ) . `` * * * * A . * * Your understanding is correct . The structure of blocks can be expressed using signal variables $ \\alpha^ { ( l ) } $ . If $ \\alpha^ { ( l ) } =1 $ , the layer $ l $ is trained using local gradients , therefore becoming the last layer in a gradient-isolated block . If $ \\alpha^ { ( l ) } =0 $ , the layer $ l $ is trained using gradients backpropagated from its upper layer within a block , therefore becoming an inside layer of a gradient-isolated block ( see Equation ( 2 ) ) . We will rewrite the sentences clearly . * * 9 . ( C5 ) Typos * * * * A . * * Thank you for pointing out typos , which will be corrected . * * References * * [ 1 ] Eugene Belilovsky , Michael Eickenberg , and Edouard Oyallon . Decoupled greedy learning of cnns . In ICML , 2020 . [ 2 ] https : //pytorch.org/hub/pytorch_vision_resnet [ 3 ] Zhouyuan Huo , Bin Gu , Qian Yang , and Heng Huang . Decoupled parallel backpropagation with convergence guarantee . In ICML , 2018 . [ 4 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In CVPR , 2016 ."}, "2": {"review_id": "XLfdzwNKzch-2", "review_text": "Summary Of Contributions : This paper proposes to automate the design of auxiliary network and its allocation under decoupled neural network scheme , a design that speeds up network training and potentially boost model accuracy . The approach is validated with leading performance on ResNet and VGGNet under various datasets . Strengths : The idea to search auxiliary network for decoupled neural network is novel , and the proposed method is verified on multiple widely used datasets . Weaknesses : 1 . It \u2019 s unclear if the proposed differentiable search algorithm works better than DARTs with direction comparison , as the proposed method is basically DARTs with some changes ( e.g.weight sampling , extra inner optimization steps per outer optimization ) , that are also applied to typical NAS tasks , a comparison with DARTs is needed to validate how much of the gain is from the proposed method rather than the fact of automating network design itself . 2.It seems that the decoupled neural network scheme has nearly no gains in terms of Top-1 without the auxiliary heads ensembled as shown in Table 3 . The details should be provided on how the ensemble works , are ensembled auxiliary heads kept during the testing stage ? If so , Table 3 can not tell if the gain is from the ensembled aux heads or decoupled neural network scheme . Network trained with Backprop method and identical extra aux head for ensemble might reach the same level of performance . 3.Experiment with DGL in Table3 ( b ) is problematic , DGL in its original paper only provides ResNet-152 result with K=2 , and it is better than Backprop according to their experiment . Yet the authors set K=4 in Table 3 and got a contrary result . This brought up the suspect of cherry picking of hyperparameters and can only be addressed with stricter comparison . 4.One of the advantages to adopt a decoupled neural network scheme is training speedup , while the network based on compared methods is instantly available , the proposed method requires extra time to search before training . With this extra cost , how much the training speedup benefit is still left ? The search cost should be indicated and the overall time cost should be discussed .", "rating": "7: Good paper, accept", "reply_text": "* * 5 . ( Q4 ) While the network based on compared methods is instantly available , the proposed method requires extra time to search before training . * * * * A . * * The review is correct that the compared methods do not require additional search steps . However , these methods yield worse performance than backprop . Moreover , their performances become worse as $ K $ increases ( Figure 3 ) , so the speedup is also limited in practice due to large sacrifices of accuracy . On the other hand , the decoupling configurations found by SEDONA do not suffer from these issues . SEDONA shows consistently similar or better performance with multiple off-the-shelf CNNs in multiple datasets compared to backprop . Moreover , SEDONA still performs similar or better than backprop up to $ K=16 $ ( Figure 3 ) . * * 6 . ( Q4 ) The search cost should be indicated . * * * * A . * * Search time of SEDONA is 0.6 , 1.1 , 1.9 and 3 days for VGG-19 , ResNet-50 , ResNet-101 and ResNet-152 , respectively , as described in Table 14 in Appendix ( Table 10 in the old draft ) . * * 7 . ( Q4 ) With this extra cost , how much of the training speedup benefit is still left ? The overall time cost should be discussed . * * * * A . * * Although SEDONA requires additional search time , the search stage is performed only once per network architecture . No additional search is required no matter whether the dataset changes or the number of desired blocks $ K $ changes . Moreover , since training times of ResNet-152 on ImageNet using backprop and SEDONA were 158.5 hours and 78.3 hours , respectively , the search cost is canceled out even when trained once . * * References * * [ 1 ] Eugene Belilovsky , Michael Eickenberg , and Edouard Oyallon . Greedy layerwise learning can scale to imagenet . In ICML , 2019 . [ 2 ] Eugene Belilovsky , Michael Eickenberg , and Edouard Oyallon . Decoupled greedy learning of cnns . In ICML , 2020 . [ 3 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In CVPR , 2016 ."}}