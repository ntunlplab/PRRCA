{"year": "2020", "forum": "r1g1CAEKDH", "title": "Wyner VAE: A Variational Autoencoder with Succinct Common Representation Learning", "decision": "Reject", "meta_review": "This paper adds a new model to the literature on representation learning from correlated variables with some common and some \"private\" dimensions, and takes a variational approach based on Wyner's common information.  The literature in this area includes models where both of the correlated variables are assumed to be available as input at all times, as well as models where only one of the two may be available; the proposed approach falls into the first category.  Pros:  The reviewers generally agree, as do I, that the motivation is very interesting and the resulting model is reasonable and produces solid results.  Cons:  The model is somewhat complex and the paper is lacking a careful ablation study on the components.  In addition, the results are not a clear \"win\" for the proposed model.  The authors have started to do an ablation study, and I think eventually an interesting story is likely to come out of that.  But at the moment the paper feels a bit too preliminary/inconclusive for publication.", "reviews": [{"review_id": "r1g1CAEKDH-0", "review_text": "This paper presents a variational auto-encoder approach for paired data variables, with a separate notion of common and individual randomness. On top of data reconstruction, Wyner's common information is added as a regularization term to promote the succinctness of the latent common representation in particular. Besides, KL divergence based constraints are proposed to encourage consistency of different modeling components. Although the consistency imposed by KL can be weak, this helps avoid making unrealistic assumptions. To make the proposed objective function tractable, the authors make use of several standard techniques such as variational bounds and sampling-based approximations. The proposed approach can be tailored for multiple purposes, including joint/conditional generative modeling, two variable auto-encoding, style extraction and control, as illustrated in Figure 2 and Figure 3. It also supports the multi-stage training scheme, adding more flexibility. The authors did a great job in the literature review and experimental comparison. Table 1 and Table 2 demonstrate the position and distinctions of the proposed Wyner VAE approach. The comparison to the information bottleneck is also interesting. The numerical studies are extensive. The authors demonstrate performance improvement quantitatively over several baseline competitors, although the margin is not big. With appropriate regularization, the qualitative results indeed look better subjectively. The proposed approach stands out as a \"swiss army knife\" approach for two-variable VAE, which could support versatile needs. The shared and individual randomness are proved to be useful in style extraction and style control in the experiments. Overall, I feel the experiment evaluations and explanations are convincing and informative. \u2022 The Alice Bob example is helpful to understand the main purpose of this paper \u2022 The authors introduced the theoretical background of Wyner's common information --- the two contexts it arises. These texts are interesting to read, but I feel the connection to the following learning problem is weak. It would be nice to have more justifications on the choice in the context of VAE learning (optimality?), as opposed to other seemingly more natural choices. For example, the sum of mutual information I(X; Z)+I(Y; Z), or other forms of common information \u2022 Although the proposed approach involves many terms, in general, the presentation is good with clear notations. I am just confused about p_tilde (u,v|x,y,z) in (8). Is it simply p(u)p(v)? \u2022 There is a typo in (19) and (20)", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive comments . Here are our rejoinder to the comments . * * * On optimality * * * As alluded in Introduction ( Section 1 ) and Concluding Remarks ( Section 5 ) , we believe that there exists a relation between the common information $ I ( X , Y ; Z ) $ and generalizability of the learned model ( e.g. , good test log-likelihood performance ) , and such a theoretical guarantee is currently under investigation . Although it is not the sum of mutual information $ I ( X ; Z ) + I ( Y ; Z ) $ suggested by the reviewer , we compared the Wyner \u2019 s common information with the ( variational ) information bottleneck principle in Section 3 ( pp.6 -- 7 ) and Appendix C ( p. 17 ) , which aims to minimize a certain form of mutual information related objective . * * * On $ \\tilde { p } ( u , v|x , y , z ) $ * * * $ \\tilde { p } ( u , v|x , y , z ) $ is the conditional distribution of $ u , v $ given $ x , y , z $ , induced by $ p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) $ , meaning that $ \\tilde { p } ( u , v|x , y , z ) = p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) / { \\int p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) du dv } $ . This is why we can replace $ $ p ( z ) \\tilde { p } ( x|z ) \\tilde { p } ( y|z ) \\tilde { p } ( u , v|x , y , z ) $ $ in eq . ( 8 ) with $ $ p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) $ $ in eq . ( 9 ) . * * * On typo * * * Thanks for pointing that out . We will update the manuscript with fixing the typo in ( 19 ) and ( 20 ) ."}, {"review_id": "r1g1CAEKDH-1", "review_text": "The paper proposed a variational autoencoder for a pair of correlated observation variables, motivated by Wyner's common information. An instantiation of the model using reverse KL divergence metric is also provided for model inference. Experimental results on simulated dataset and real image datasets show the effectiveness of the proposed generative model. The paper also provides a comprehensive appendix with technical and experimental details. Overall, the paper is technically sound and well supported by theory and experiments. Here are a few specific comments and questions about the technical content: 1) For most of the data modeling tasks in real applications, we will most likely have one observational variable (speech segment, document or image). How would the proposed VAE model be applied when the two correlated variables are not explicitly observed or defined? It would be nice if the authors can provide some discussion on the general applicability of the proposed model. 2) A common challenge of information theoretic VAE is the analytical intractability of divergence measure. For example in Zhao et al. 2018, the maximum mean discrepancy measure is used to approximate the divergence. It is unclear to me whether the objective function in Section 2.3 (using the reverse KL divergence measure) is analytically tractable or not. If not, what kind of approximations (e.g., sampling) did the author used? 3) In the experimental section, the authors showed that for joint and conditional distribution modeling tasks, the optimal regularization parameter might be different. This implies that for a specific task, the user is responsible to choose a proper \\lambda value. I wonder whether the authors can provide some general guidelines on how to choose this important parameter in practice, as the results seem to be highly dependent on the choice.", "rating": "6: Weak Accept", "reply_text": "We appreciate all the constructive comments . The following are the rejoinder to the technical points . * * * 1 ) On general applicability * * * The proposed Wyner VAE model explicitly assumes that we are given a pair of correlated observations . We have a future plan for extending our framework for ( fully or partially ) unpaired data . If this does not answer your question , could you elaborate on what you meant by two correlated variables are not explicitly observed ? * * * 2 ) On tractability of KL divergence measure * * * The ( reverse ) KL divergence is not analytically tractable , so we used Monte Carlo sampling for approximation as you mentioned , which is the standard technique in the VAE literature . * * * 3 ) On a general guideline for choosing $ \\lambda $ * * * As typically done in any statistical method , $ \\lambda $ can be chosen by cross-validation . For example , given a split of dataset into three parts { training , test , validation } , we train a model with a set of $ \\lambda $ choices using training dataset , and find the best achieved performance for each $ \\lambda $ based on the validation dataset . The final $ \\lambda $ can then be chosen based on the performance on the test split ."}, {"review_id": "r1g1CAEKDH-2", "review_text": "This paper presents a method for learning latent-variable models of paired data that decomposes the latent representation into common and local representations. The approach is motivated by Wyner\u2019s common information, and is made tractable through a variational formulation. On experiments with MoG, MNIST, SVHN, and CelebA, the paper shows that penalizing the complexity of the common representation can improve style-content disentanglement and conditional sampling. While I found the formulation and motivation from Wyner\u2019s common information and Cuff\u2019s channel synthesis interesting, the resulting model and experiments were unconvincing. There are many terms in the Wyner model proposed, and there are no ablations demonstrating which terms are important and which are not (e.g. do you need both L_xx and L_xy?). On the toy MoG experiment, the common information is known but trained models do not recover the right amount of information. For representation learning, accuracy decreases as the penalty on the common information increases, and for NLL, the joint and conditional NLL is often similar to existing work (CVAE and JVAE). The main win appears to be for style-content disentanglement, but the results there are qualitative and often change the content when only style is changed. It\u2019s also puzzling as to why CVAE overfits so severely in a subset of the experiments. Without a more thorough evaluation of what terms in the loss matter, and showing that the technique recovers something like Wyner\u2019s common information (by extracting the right information on a toy model), I cannot recommend this paper for acceptance. Minor comments: * Eqn 1: the constraint \u201cSubject to X-Z-Y\u201d is confusing. Do you mean X-Z-Y is a Markov network (undirected) and not a Markov chain (directed), in which case X -> Z <- Y does not correspond to X-Z-Y? (This is addressed in Eqn 3-4, but should be fixed here) * How is Wyner\u2019s common information related to the multivaraite mutual information I(X; Z; Y)? * When describing distributed simulation, please include the variables and objective like you do for common information * s/Markovity condition/independence assumption? * Why are both losses in Eqn 4 and Eqn 5 needed? Couldn\u2019t you use either to train q(z|x)? * Eqn 10 (and most of your objectives) are still intractable due to the q(x)/q(x,y) terms, you should note that it is constant and dropped from the objective * \u201cStyle control\u201d: could you define what you mean by style in this context? Prior work could likely also do \u201cstyle control\u201d by e.g. interpolating subsets of dimensions. * Related work: https://openreview.net/forum?id=rkVOXhAqY7 (CEB) that may result in a similar objective as Wyner\u2019s common info * When comparing to JVAE/JMVAE, it seems like the main difference is suing a latent-variable in the decoder, but the framework is still the same. It\u2019d be useful to spend more time comparing/contrasting with this prior work. * Fig 4: would be useful to have a picture of the samples (right now they\u2019re just in appendix) * Fig 4: in the data generating process, I believe I((X, Y); Z) = ln(5) = 1.6 nats, but none of your models converge to rates around there. Why not? * Unlike other approaches (JVAE, CVAE) you have additional terms in your loss for conditional prediction at training time. It seems like these may be giving you gains, and it\u2019d be useful to perform ablations over the terms in Eqn 15 to figure out what is impacting performance in Fig 4. E.g. if you set \\alpha_{x -> y} and \\alpha{y -> x} to 0 * Why do the CVAE models overfit? Have you tried optimizing the variational parameters on the test set if it\u2019s due to amortization gap? * Fig 5b: what\u2019s the variance you\u2019re plotting? In representation learning, we often just care about downstream accuracy, and it looks like \\lam = 0 performs best there. * Table 3: isn\u2019t this showing that \\lam = 0 works the best for joint NLL and close to best for conditional NLL on the MoG task? What\u2019s the discrepancy with Fig 5 where conditional NLL is highly dependent on \\lambda for MNIST? * Fig 6 (a1-f1): for this MNIST add 1 dataset, the only common info should be the label. But it looks like the labels do change for non-zero values of lambda (b1 there\u2019s a 2 -> 7), c1 1 -> 9) * Table 4: would be useful to train CVAE yourself, as the small differences in numbers could just be due to tuning / experimental setup. You also should bold everything that\u2019s within stderr (i.e. \\lambda = 0 and \\lambda = 0.15 are equally good) * Fig 9: would be useful to include CVAE, and Wyner VAE in this plot, i.e. is it only using the mode information in the latent variable when doing conditional sampling? * CelebA results look interesting, but there\u2019s been other work on generation from attributes that presents more visually compelling results, e.g. https://arxiv.org/abs/1706.00409", "rating": "3: Weak Reject", "reply_text": "We thank for your critical reviews and constructive comments . While we are working on the answers , we found that part of one of your question was somehow deleted . ( `` * s/Markovity condition/independence assumption ? '' ) Could you restate your question ?"}, {"review_id": "r1g1CAEKDH-3", "review_text": "The paper proposes Wyner VAE, a variational autoencoder for a five-variable graphical model. The five variables consist of two observable views, X and Y, with a shared latent variable Z, while the two other latent variables U and V control the randomness of X and Y, respectively, independent of others. The overall objective is motivated by Wyner's mutual information minimization. Using a few common techniques to bound KL divergence, the paper arrives at a few terms that are reminiscent to the variational lower bounds, including terms that constraints the hidden variable to be similar to the prior, and two cross-modal reconstruction terms. The paper includes a comparison between similar models, both in terms of formulation and experiments. The results show that Wyner VAE has certain nice properties that is lacking in other models. I am giving a score of 6. The formulation and the lower bound for learning are the strengths of the paper. The lack of discussion on some of the common problems for VAEs and the experiments are the weakness of the paper. The paper deserves credit to motivate the problem from Wyner's viewpoint. However, after the derivation, the framework falls back to common VAEs, and is not very different from VCCA-private. Comparing Wyner VAE and VCCA-private, I can see why it is hard to justify the difference of the two based on the formulations, but I also find it unconvincing from the experiments to conclude that one is better than the other, especially when there are many hyperparameters to tune. The paper should state this clearly when comparing the two. In terms of the common problems for VAEs, the paper does not address a few degenerate cases. The first case is where all the information gets pushed to the shared latent variable Z, leaving little private information for U and V. The second case is that little information about both views gets pushed to the shared latent variable Z, while the decoders are doing all the heavy lifting to reconstruct the observables. To avoid these degenerate cases, more constraints might be needed for the models, such as the ones proposed in (Hsu et al., 2017). Finally, as with most VAE models, the paper does not discuss how well lower bounds approximates the likelihood and the resulting learned representation when the gap between the likelihood and the objective is large. The experiments in the paper are somewhat lacking. Figure 7 is where the capability of the model is fully demonstrated: a clear distinction between the shared and the private views are shown. It would be great if the paper can include experiments on a real-world multi-view data set, such as images paired with texts, speech paired with images, speech paired with video, etc. Unsupervised learning of disentangled and interpretable representations from sequential data Wei-Ning Hsu, Yu Zhang, James Glass Neurips, 2017 ", "rating": "6: Weak Accept", "reply_text": "We appreciate all the constructive comments . The following are the rejoinder to the technical points . * * * On Wyner VAE vs. VCCA-private * * * As you pointed out , Wyner VAE and VCCA-private share the same generative model ( decoder model ) , but the difference lies in the encoder model ; see Appendix C. Wyner VAE assumes an encoder model $ q ( z , u , v|x , y ) =q ( z|x , y ) q ( u|z , x ) q ( v|z , y ) $ , which captures a consistent conditional independence to the Markovity condition in the decoder model . On the other hand , VCCA-private assumes an encoder model $ q ( z , u , v|x , y ) =q ( z|x ) q ( u|x ) q ( v|y ) $ , which is a rather ad-hoc conditional independence structure . We believe that it inherently implies a limited generative performance , which is demonstrated with MNIST -- MNIST add-1 dataset ; see Fig.6 ( f1 , f2 ) . * * * On degenerate cases in Wyner VAE * * * The two degenerate cases you pointed out are addressed in the current Wyner VAE model as follows . The regularization weight $ \\lambda > 0 $ on the common information $ I ( X , Y ; Z ) $ is to avoid the first degenerate case where \u201c all the information gets pushed to the shared latent variable $ Z $ , leaving little private information for $ U $ and $ V $ \u201d . We also remark that JVAE/JMVAE models this degenerate case , lacking the local ( private ) random variables $ U $ and $ V $ \u2014the shared latent variable $ Z $ captures all information to generate both $ X $ and $ Y $ , and it leads to a poor generalization performance ; see Table 3 . The second degenerate case may occur when we put too large regularization weight $ \\lambda $ and/or when the amount common information $ I ( X , Y ; Z ) $ is relatively small compared to the local information . For the latter case , we note that the conditional losses in eq . ( 14 ) can help the model avoid such degeneracy , as the shared variable $ Z $ is forced to capture the common information of $ X $ and $ Y $ to make the conditional losses small . We will add an additional experiments demonstrating this effect . * * * On the lack of discussion regarding variational approximation * * * Thanks for your comment . We will address the problem on the variational approximation and the resulting learned representation in the future work . * * * On experiment with real-world dataset * * * Thanks for the pointer . In the current manuscript , we focused on showing the validity and the potential of our framework . We plan to extend the fundamental concepts introduced in the current work to practical applications such as frame interpolation/ extrapolation for single- or multi-view videos per your suggestion ."}], "0": {"review_id": "r1g1CAEKDH-0", "review_text": "This paper presents a variational auto-encoder approach for paired data variables, with a separate notion of common and individual randomness. On top of data reconstruction, Wyner's common information is added as a regularization term to promote the succinctness of the latent common representation in particular. Besides, KL divergence based constraints are proposed to encourage consistency of different modeling components. Although the consistency imposed by KL can be weak, this helps avoid making unrealistic assumptions. To make the proposed objective function tractable, the authors make use of several standard techniques such as variational bounds and sampling-based approximations. The proposed approach can be tailored for multiple purposes, including joint/conditional generative modeling, two variable auto-encoding, style extraction and control, as illustrated in Figure 2 and Figure 3. It also supports the multi-stage training scheme, adding more flexibility. The authors did a great job in the literature review and experimental comparison. Table 1 and Table 2 demonstrate the position and distinctions of the proposed Wyner VAE approach. The comparison to the information bottleneck is also interesting. The numerical studies are extensive. The authors demonstrate performance improvement quantitatively over several baseline competitors, although the margin is not big. With appropriate regularization, the qualitative results indeed look better subjectively. The proposed approach stands out as a \"swiss army knife\" approach for two-variable VAE, which could support versatile needs. The shared and individual randomness are proved to be useful in style extraction and style control in the experiments. Overall, I feel the experiment evaluations and explanations are convincing and informative. \u2022 The Alice Bob example is helpful to understand the main purpose of this paper \u2022 The authors introduced the theoretical background of Wyner's common information --- the two contexts it arises. These texts are interesting to read, but I feel the connection to the following learning problem is weak. It would be nice to have more justifications on the choice in the context of VAE learning (optimality?), as opposed to other seemingly more natural choices. For example, the sum of mutual information I(X; Z)+I(Y; Z), or other forms of common information \u2022 Although the proposed approach involves many terms, in general, the presentation is good with clear notations. I am just confused about p_tilde (u,v|x,y,z) in (8). Is it simply p(u)p(v)? \u2022 There is a typo in (19) and (20)", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive comments . Here are our rejoinder to the comments . * * * On optimality * * * As alluded in Introduction ( Section 1 ) and Concluding Remarks ( Section 5 ) , we believe that there exists a relation between the common information $ I ( X , Y ; Z ) $ and generalizability of the learned model ( e.g. , good test log-likelihood performance ) , and such a theoretical guarantee is currently under investigation . Although it is not the sum of mutual information $ I ( X ; Z ) + I ( Y ; Z ) $ suggested by the reviewer , we compared the Wyner \u2019 s common information with the ( variational ) information bottleneck principle in Section 3 ( pp.6 -- 7 ) and Appendix C ( p. 17 ) , which aims to minimize a certain form of mutual information related objective . * * * On $ \\tilde { p } ( u , v|x , y , z ) $ * * * $ \\tilde { p } ( u , v|x , y , z ) $ is the conditional distribution of $ u , v $ given $ x , y , z $ , induced by $ p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) $ , meaning that $ \\tilde { p } ( u , v|x , y , z ) = p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) / { \\int p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) du dv } $ . This is why we can replace $ $ p ( z ) \\tilde { p } ( x|z ) \\tilde { p } ( y|z ) \\tilde { p } ( u , v|x , y , z ) $ $ in eq . ( 8 ) with $ $ p ( z ) p ( u ) p ( v ) p ( x|z , u ) p ( y|z , v ) $ $ in eq . ( 9 ) . * * * On typo * * * Thanks for pointing that out . We will update the manuscript with fixing the typo in ( 19 ) and ( 20 ) ."}, "1": {"review_id": "r1g1CAEKDH-1", "review_text": "The paper proposed a variational autoencoder for a pair of correlated observation variables, motivated by Wyner's common information. An instantiation of the model using reverse KL divergence metric is also provided for model inference. Experimental results on simulated dataset and real image datasets show the effectiveness of the proposed generative model. The paper also provides a comprehensive appendix with technical and experimental details. Overall, the paper is technically sound and well supported by theory and experiments. Here are a few specific comments and questions about the technical content: 1) For most of the data modeling tasks in real applications, we will most likely have one observational variable (speech segment, document or image). How would the proposed VAE model be applied when the two correlated variables are not explicitly observed or defined? It would be nice if the authors can provide some discussion on the general applicability of the proposed model. 2) A common challenge of information theoretic VAE is the analytical intractability of divergence measure. For example in Zhao et al. 2018, the maximum mean discrepancy measure is used to approximate the divergence. It is unclear to me whether the objective function in Section 2.3 (using the reverse KL divergence measure) is analytically tractable or not. If not, what kind of approximations (e.g., sampling) did the author used? 3) In the experimental section, the authors showed that for joint and conditional distribution modeling tasks, the optimal regularization parameter might be different. This implies that for a specific task, the user is responsible to choose a proper \\lambda value. I wonder whether the authors can provide some general guidelines on how to choose this important parameter in practice, as the results seem to be highly dependent on the choice.", "rating": "6: Weak Accept", "reply_text": "We appreciate all the constructive comments . The following are the rejoinder to the technical points . * * * 1 ) On general applicability * * * The proposed Wyner VAE model explicitly assumes that we are given a pair of correlated observations . We have a future plan for extending our framework for ( fully or partially ) unpaired data . If this does not answer your question , could you elaborate on what you meant by two correlated variables are not explicitly observed ? * * * 2 ) On tractability of KL divergence measure * * * The ( reverse ) KL divergence is not analytically tractable , so we used Monte Carlo sampling for approximation as you mentioned , which is the standard technique in the VAE literature . * * * 3 ) On a general guideline for choosing $ \\lambda $ * * * As typically done in any statistical method , $ \\lambda $ can be chosen by cross-validation . For example , given a split of dataset into three parts { training , test , validation } , we train a model with a set of $ \\lambda $ choices using training dataset , and find the best achieved performance for each $ \\lambda $ based on the validation dataset . The final $ \\lambda $ can then be chosen based on the performance on the test split ."}, "2": {"review_id": "r1g1CAEKDH-2", "review_text": "This paper presents a method for learning latent-variable models of paired data that decomposes the latent representation into common and local representations. The approach is motivated by Wyner\u2019s common information, and is made tractable through a variational formulation. On experiments with MoG, MNIST, SVHN, and CelebA, the paper shows that penalizing the complexity of the common representation can improve style-content disentanglement and conditional sampling. While I found the formulation and motivation from Wyner\u2019s common information and Cuff\u2019s channel synthesis interesting, the resulting model and experiments were unconvincing. There are many terms in the Wyner model proposed, and there are no ablations demonstrating which terms are important and which are not (e.g. do you need both L_xx and L_xy?). On the toy MoG experiment, the common information is known but trained models do not recover the right amount of information. For representation learning, accuracy decreases as the penalty on the common information increases, and for NLL, the joint and conditional NLL is often similar to existing work (CVAE and JVAE). The main win appears to be for style-content disentanglement, but the results there are qualitative and often change the content when only style is changed. It\u2019s also puzzling as to why CVAE overfits so severely in a subset of the experiments. Without a more thorough evaluation of what terms in the loss matter, and showing that the technique recovers something like Wyner\u2019s common information (by extracting the right information on a toy model), I cannot recommend this paper for acceptance. Minor comments: * Eqn 1: the constraint \u201cSubject to X-Z-Y\u201d is confusing. Do you mean X-Z-Y is a Markov network (undirected) and not a Markov chain (directed), in which case X -> Z <- Y does not correspond to X-Z-Y? (This is addressed in Eqn 3-4, but should be fixed here) * How is Wyner\u2019s common information related to the multivaraite mutual information I(X; Z; Y)? * When describing distributed simulation, please include the variables and objective like you do for common information * s/Markovity condition/independence assumption? * Why are both losses in Eqn 4 and Eqn 5 needed? Couldn\u2019t you use either to train q(z|x)? * Eqn 10 (and most of your objectives) are still intractable due to the q(x)/q(x,y) terms, you should note that it is constant and dropped from the objective * \u201cStyle control\u201d: could you define what you mean by style in this context? Prior work could likely also do \u201cstyle control\u201d by e.g. interpolating subsets of dimensions. * Related work: https://openreview.net/forum?id=rkVOXhAqY7 (CEB) that may result in a similar objective as Wyner\u2019s common info * When comparing to JVAE/JMVAE, it seems like the main difference is suing a latent-variable in the decoder, but the framework is still the same. It\u2019d be useful to spend more time comparing/contrasting with this prior work. * Fig 4: would be useful to have a picture of the samples (right now they\u2019re just in appendix) * Fig 4: in the data generating process, I believe I((X, Y); Z) = ln(5) = 1.6 nats, but none of your models converge to rates around there. Why not? * Unlike other approaches (JVAE, CVAE) you have additional terms in your loss for conditional prediction at training time. It seems like these may be giving you gains, and it\u2019d be useful to perform ablations over the terms in Eqn 15 to figure out what is impacting performance in Fig 4. E.g. if you set \\alpha_{x -> y} and \\alpha{y -> x} to 0 * Why do the CVAE models overfit? Have you tried optimizing the variational parameters on the test set if it\u2019s due to amortization gap? * Fig 5b: what\u2019s the variance you\u2019re plotting? In representation learning, we often just care about downstream accuracy, and it looks like \\lam = 0 performs best there. * Table 3: isn\u2019t this showing that \\lam = 0 works the best for joint NLL and close to best for conditional NLL on the MoG task? What\u2019s the discrepancy with Fig 5 where conditional NLL is highly dependent on \\lambda for MNIST? * Fig 6 (a1-f1): for this MNIST add 1 dataset, the only common info should be the label. But it looks like the labels do change for non-zero values of lambda (b1 there\u2019s a 2 -> 7), c1 1 -> 9) * Table 4: would be useful to train CVAE yourself, as the small differences in numbers could just be due to tuning / experimental setup. You also should bold everything that\u2019s within stderr (i.e. \\lambda = 0 and \\lambda = 0.15 are equally good) * Fig 9: would be useful to include CVAE, and Wyner VAE in this plot, i.e. is it only using the mode information in the latent variable when doing conditional sampling? * CelebA results look interesting, but there\u2019s been other work on generation from attributes that presents more visually compelling results, e.g. https://arxiv.org/abs/1706.00409", "rating": "3: Weak Reject", "reply_text": "We thank for your critical reviews and constructive comments . While we are working on the answers , we found that part of one of your question was somehow deleted . ( `` * s/Markovity condition/independence assumption ? '' ) Could you restate your question ?"}, "3": {"review_id": "r1g1CAEKDH-3", "review_text": "The paper proposes Wyner VAE, a variational autoencoder for a five-variable graphical model. The five variables consist of two observable views, X and Y, with a shared latent variable Z, while the two other latent variables U and V control the randomness of X and Y, respectively, independent of others. The overall objective is motivated by Wyner's mutual information minimization. Using a few common techniques to bound KL divergence, the paper arrives at a few terms that are reminiscent to the variational lower bounds, including terms that constraints the hidden variable to be similar to the prior, and two cross-modal reconstruction terms. The paper includes a comparison between similar models, both in terms of formulation and experiments. The results show that Wyner VAE has certain nice properties that is lacking in other models. I am giving a score of 6. The formulation and the lower bound for learning are the strengths of the paper. The lack of discussion on some of the common problems for VAEs and the experiments are the weakness of the paper. The paper deserves credit to motivate the problem from Wyner's viewpoint. However, after the derivation, the framework falls back to common VAEs, and is not very different from VCCA-private. Comparing Wyner VAE and VCCA-private, I can see why it is hard to justify the difference of the two based on the formulations, but I also find it unconvincing from the experiments to conclude that one is better than the other, especially when there are many hyperparameters to tune. The paper should state this clearly when comparing the two. In terms of the common problems for VAEs, the paper does not address a few degenerate cases. The first case is where all the information gets pushed to the shared latent variable Z, leaving little private information for U and V. The second case is that little information about both views gets pushed to the shared latent variable Z, while the decoders are doing all the heavy lifting to reconstruct the observables. To avoid these degenerate cases, more constraints might be needed for the models, such as the ones proposed in (Hsu et al., 2017). Finally, as with most VAE models, the paper does not discuss how well lower bounds approximates the likelihood and the resulting learned representation when the gap between the likelihood and the objective is large. The experiments in the paper are somewhat lacking. Figure 7 is where the capability of the model is fully demonstrated: a clear distinction between the shared and the private views are shown. It would be great if the paper can include experiments on a real-world multi-view data set, such as images paired with texts, speech paired with images, speech paired with video, etc. Unsupervised learning of disentangled and interpretable representations from sequential data Wei-Ning Hsu, Yu Zhang, James Glass Neurips, 2017 ", "rating": "6: Weak Accept", "reply_text": "We appreciate all the constructive comments . The following are the rejoinder to the technical points . * * * On Wyner VAE vs. VCCA-private * * * As you pointed out , Wyner VAE and VCCA-private share the same generative model ( decoder model ) , but the difference lies in the encoder model ; see Appendix C. Wyner VAE assumes an encoder model $ q ( z , u , v|x , y ) =q ( z|x , y ) q ( u|z , x ) q ( v|z , y ) $ , which captures a consistent conditional independence to the Markovity condition in the decoder model . On the other hand , VCCA-private assumes an encoder model $ q ( z , u , v|x , y ) =q ( z|x ) q ( u|x ) q ( v|y ) $ , which is a rather ad-hoc conditional independence structure . We believe that it inherently implies a limited generative performance , which is demonstrated with MNIST -- MNIST add-1 dataset ; see Fig.6 ( f1 , f2 ) . * * * On degenerate cases in Wyner VAE * * * The two degenerate cases you pointed out are addressed in the current Wyner VAE model as follows . The regularization weight $ \\lambda > 0 $ on the common information $ I ( X , Y ; Z ) $ is to avoid the first degenerate case where \u201c all the information gets pushed to the shared latent variable $ Z $ , leaving little private information for $ U $ and $ V $ \u201d . We also remark that JVAE/JMVAE models this degenerate case , lacking the local ( private ) random variables $ U $ and $ V $ \u2014the shared latent variable $ Z $ captures all information to generate both $ X $ and $ Y $ , and it leads to a poor generalization performance ; see Table 3 . The second degenerate case may occur when we put too large regularization weight $ \\lambda $ and/or when the amount common information $ I ( X , Y ; Z ) $ is relatively small compared to the local information . For the latter case , we note that the conditional losses in eq . ( 14 ) can help the model avoid such degeneracy , as the shared variable $ Z $ is forced to capture the common information of $ X $ and $ Y $ to make the conditional losses small . We will add an additional experiments demonstrating this effect . * * * On the lack of discussion regarding variational approximation * * * Thanks for your comment . We will address the problem on the variational approximation and the resulting learned representation in the future work . * * * On experiment with real-world dataset * * * Thanks for the pointer . In the current manuscript , we focused on showing the validity and the potential of our framework . We plan to extend the fundamental concepts introduced in the current work to practical applications such as frame interpolation/ extrapolation for single- or multi-view videos per your suggestion ."}}