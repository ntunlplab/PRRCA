{"year": "2018", "forum": "B1IDRdeCW", "title": "The High-Dimensional Geometry of Binary Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper analyzes mathematically why weights of trained networks can be replaced with ternary weights without much loss in accuracy. Understanding this is an important problem, as binary or ternary weights can be much more efficient on limited hardware, and we've seen much empirical success of binarization schemes. This paper shows that the continuous angles and dot products are well approximated in the discretized network. The paper concludes with an input rotation trick to fix discretization failures in the first layer.\n\nOverall, the contribution seems substantial, and the reviewers haven't found any significant issues. One reviewer wasn't convinced of the problem's importance, but I disagree here. I think the paper will plausibly be helpful for guiding architectural and algorithmic decisions. I recommend acceptance.\n", "reviews": [{"review_id": "B1IDRdeCW-0", "review_text": "This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that: (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. (3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer. The first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge. The second observation is much less clear to me. Specifically, a. The author claim that \u201cA sufficient condition for \\delta u to be the same in both cases is L\u2019(x = f(u)) ~ L\u2019(x = g(u))\u201d. However, I\u2019m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. b. Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified. c. For BNNs, where both the weights and activations are binarized, shouldn\u2019t we compare weights*activations to (binarized weights)*(binarized activations)? d. To make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3. e. It is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?), perhaps this should be further clarified. The third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized. To summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. %%% After Author's response %%% a. My mistake. Perhaps it should be clarified in the text that u are the weights. I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation). Following the author's response and revisions, I have raised my grade. ", "rating": "7: Good paper, accept", "reply_text": "Reviewer 3 : thank you for your detailed questions and suggestions for our paper . a.In this argument , u is the weights , and f , g are the identity function / the pointwise binarize function . The earlier layers don \u2019 t impact the weights . I \u2019 m not sure I understand your comment . b.During training , all of the weights and activations are binarized [ except the first layer activations which are the input , and the last layer weights which interface with the output ] . We can extract out the values prior to binarization . In this sense , there isn \u2019 t any accumulation of errors . In other words , Figs 3 and 5 don \u2019 t reflect any accumulation of errors . c. The reason for only changing one of the binarization of the weights or activations is that corresponds to removing one of the binarize blocks . However , for the sake of completeness , I included this figure in the SI as well . d. The point of the permutation was to generate a distribution with the same marginal statistics , but with no correlational structure . Each set of activations were independently permuted [ clarified in the paper ] . e. Batch normalization subtracts the mean and divides by the standard deviation , then multiplies by a learnable constant and adds a learnable constant . So there is implicitly a learnable constant multiplying the result of the dot products . However , empirically , the learnable additive constant is zero , so the multiplicative constant is not necessary for all but the last layer at test time because the output of the batch norm layer is subsequently binarized with a threshold of zero . As far as your question about rotation being a problem with MNIST , the suggestion of our paper is to apply a random rotation to the image as if it is a vector , not to rotate in image space . [ Rotation in image space wouldn \u2019 t fix the problem because it preserves the correlations between neighboring pixels ] . It is the same random rotation for all inputs [ added a word to make this more clear ] . In the case of MNIST , this is akin to the fact that most of the variance in the dataset happens in the middle of the image and there is almost no variance in the pixels on the edge . The rotation spreads the variance more evenly among the pixels . Of course one potential problem is that the convolutional structure is broken . One other point as far as the last section is concerned , a number of papers have reported difficulties with the first layer , and we are the first ( to my knowledge ) to connect this issue to correlations in the data reducing the effective dimensionality of the input . Maybe it isn \u2019 t the most ground breaking point , but it seemed worth including to me . Thanks again for your detailed comments ."}, {"review_id": "B1IDRdeCW-1", "review_text": "This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). My main concerns are on the usage of the given observations. 1. Can the observations be used to explain more recent works? Indeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network. However, as the authors mentioned, there are more recent works which give better performance than this one. For example, we can use +1, 0, -1 to approximate the weights. Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance. So, how can the given observations be used to explain more recent works? 2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)? The authors call their findings theory. From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations. Besides, giving interesting observations is not good enough. I wish to see how they can be used to improve binary networks. Reference [a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017", "rating": "4: Ok but not good enough - rejection", "reply_text": "Reviewer 2 : thank you for your consideration of our paper . 1.I agree that our observations are relevant to understanding the work that seeks to use a ternary representation instead of a binary one [ or a higher order quantization ] . I \u2019 m working on some additional experiments , but it requires a substantial amount of work so that will be included in the next revision if I can get it done in time . Thanks for your pointer to the network sketching paper - I think that our work has interesting connections to it . However , an analysis of this paper is outside the scope of this work . 2.The goal of this paper is to explain why the Courbariaux paper worked as well as it did . I agree that it would be an interesting research direction to improve their work . As far as your comment that the paper just does some experiments and presents observations , as the other reviewers have noted , the dotted lines in Fig 2b and 2c are theoretical predictions based on assuming a rotationally invariant distribution for the weights , [ the proofs are in the SI ] , and the colored curves/points are the experimental results . There is a close correspondence between the theory and the experiments . More broadly , I agree that it would be great if this analysis led to a technique for improving performance of binary neural networks . However , I believe that the results already paint an insightful picture for understanding binary neural networks that would be useful to share with the community . Regardless , thank you for your suggestions for further experiments , hopefully I can improve the paper to your liking ."}, {"review_id": "B1IDRdeCW-2", "review_text": "This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector. It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \"Dot Product Proportionality Property.\" It also proposes \"Generalized Binarization Transformation\" for the first layer of a neural network. In general, I think the paper is written clearly and in detail. Some typos and minor issues are listed in the \"Cons\" part below. Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. Cons: * it seems that there are quite some typos in the paper, for example: 1. Section 1, in the second contribution, there are two \"then\"s. 2. Section 1, the citation format of \"Bengio et al. (2013)\" should be \"(Bengio et al. 2013)\". * Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD. * Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear. * In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(\\row) to make it clearer? I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it. ", "rating": "7: Good paper, accept", "reply_text": "Reviewer 1 : thank you very much for your comments . * Typos fixed . * Ordering of the papers : the Han Deep Compression paper cites the Han Learning weights paper . [ So that order is correct ] . However , a citation to the DSD paper , which is more recent than those two earlier papers , is missing . I added a citation this to the paper . * Fig 2 ( b ) shows the angle distributions separated by layer [ and each layer has different dimensional vectors ] . Each of these distributions is peaked with some standard deviation . The theory predicts that these distributions have standard deviation that scales as 1/sqrt ( d ) . The plot in 2 ( c ) shows the standard deviation as a function of dimension of the curves in 2 ( b ) with a dotted line that corresponds to 1/sqrt ( d ) [ on a log-log plot ] . The dots fall roughly along this dotted line ( especially for higher dimensions ) . For the sake of clarity , I added the zoomed in version of the plot in 2 ( b ) in the appendix . If you have any suggestions for how to make this more clear , I am happy to fix it . * The proof for the pdf of g ( rho ) is a bit involved in the paper that I cited . I came up with a new proof that is quite a bit simpler and included it ."}], "0": {"review_id": "B1IDRdeCW-0", "review_text": "This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that: (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. (3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer. The first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge. The second observation is much less clear to me. Specifically, a. The author claim that \u201cA sufficient condition for \\delta u to be the same in both cases is L\u2019(x = f(u)) ~ L\u2019(x = g(u))\u201d. However, I\u2019m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. b. Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified. c. For BNNs, where both the weights and activations are binarized, shouldn\u2019t we compare weights*activations to (binarized weights)*(binarized activations)? d. To make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3. e. It is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?), perhaps this should be further clarified. The third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized. To summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. %%% After Author's response %%% a. My mistake. Perhaps it should be clarified in the text that u are the weights. I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation). Following the author's response and revisions, I have raised my grade. ", "rating": "7: Good paper, accept", "reply_text": "Reviewer 3 : thank you for your detailed questions and suggestions for our paper . a.In this argument , u is the weights , and f , g are the identity function / the pointwise binarize function . The earlier layers don \u2019 t impact the weights . I \u2019 m not sure I understand your comment . b.During training , all of the weights and activations are binarized [ except the first layer activations which are the input , and the last layer weights which interface with the output ] . We can extract out the values prior to binarization . In this sense , there isn \u2019 t any accumulation of errors . In other words , Figs 3 and 5 don \u2019 t reflect any accumulation of errors . c. The reason for only changing one of the binarization of the weights or activations is that corresponds to removing one of the binarize blocks . However , for the sake of completeness , I included this figure in the SI as well . d. The point of the permutation was to generate a distribution with the same marginal statistics , but with no correlational structure . Each set of activations were independently permuted [ clarified in the paper ] . e. Batch normalization subtracts the mean and divides by the standard deviation , then multiplies by a learnable constant and adds a learnable constant . So there is implicitly a learnable constant multiplying the result of the dot products . However , empirically , the learnable additive constant is zero , so the multiplicative constant is not necessary for all but the last layer at test time because the output of the batch norm layer is subsequently binarized with a threshold of zero . As far as your question about rotation being a problem with MNIST , the suggestion of our paper is to apply a random rotation to the image as if it is a vector , not to rotate in image space . [ Rotation in image space wouldn \u2019 t fix the problem because it preserves the correlations between neighboring pixels ] . It is the same random rotation for all inputs [ added a word to make this more clear ] . In the case of MNIST , this is akin to the fact that most of the variance in the dataset happens in the middle of the image and there is almost no variance in the pixels on the edge . The rotation spreads the variance more evenly among the pixels . Of course one potential problem is that the convolutional structure is broken . One other point as far as the last section is concerned , a number of papers have reported difficulties with the first layer , and we are the first ( to my knowledge ) to connect this issue to correlations in the data reducing the effective dimensionality of the input . Maybe it isn \u2019 t the most ground breaking point , but it seemed worth including to me . Thanks again for your detailed comments ."}, "1": {"review_id": "B1IDRdeCW-1", "review_text": "This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). My main concerns are on the usage of the given observations. 1. Can the observations be used to explain more recent works? Indeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network. However, as the authors mentioned, there are more recent works which give better performance than this one. For example, we can use +1, 0, -1 to approximate the weights. Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance. So, how can the given observations be used to explain more recent works? 2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)? The authors call their findings theory. From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations. Besides, giving interesting observations is not good enough. I wish to see how they can be used to improve binary networks. Reference [a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017", "rating": "4: Ok but not good enough - rejection", "reply_text": "Reviewer 2 : thank you for your consideration of our paper . 1.I agree that our observations are relevant to understanding the work that seeks to use a ternary representation instead of a binary one [ or a higher order quantization ] . I \u2019 m working on some additional experiments , but it requires a substantial amount of work so that will be included in the next revision if I can get it done in time . Thanks for your pointer to the network sketching paper - I think that our work has interesting connections to it . However , an analysis of this paper is outside the scope of this work . 2.The goal of this paper is to explain why the Courbariaux paper worked as well as it did . I agree that it would be an interesting research direction to improve their work . As far as your comment that the paper just does some experiments and presents observations , as the other reviewers have noted , the dotted lines in Fig 2b and 2c are theoretical predictions based on assuming a rotationally invariant distribution for the weights , [ the proofs are in the SI ] , and the colored curves/points are the experimental results . There is a close correspondence between the theory and the experiments . More broadly , I agree that it would be great if this analysis led to a technique for improving performance of binary neural networks . However , I believe that the results already paint an insightful picture for understanding binary neural networks that would be useful to share with the community . Regardless , thank you for your suggestions for further experiments , hopefully I can improve the paper to your liking ."}, "2": {"review_id": "B1IDRdeCW-2", "review_text": "This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector. It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \"Dot Product Proportionality Property.\" It also proposes \"Generalized Binarization Transformation\" for the first layer of a neural network. In general, I think the paper is written clearly and in detail. Some typos and minor issues are listed in the \"Cons\" part below. Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. Cons: * it seems that there are quite some typos in the paper, for example: 1. Section 1, in the second contribution, there are two \"then\"s. 2. Section 1, the citation format of \"Bengio et al. (2013)\" should be \"(Bengio et al. 2013)\". * Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD. * Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear. * In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(\\row) to make it clearer? I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it. ", "rating": "7: Good paper, accept", "reply_text": "Reviewer 1 : thank you very much for your comments . * Typos fixed . * Ordering of the papers : the Han Deep Compression paper cites the Han Learning weights paper . [ So that order is correct ] . However , a citation to the DSD paper , which is more recent than those two earlier papers , is missing . I added a citation this to the paper . * Fig 2 ( b ) shows the angle distributions separated by layer [ and each layer has different dimensional vectors ] . Each of these distributions is peaked with some standard deviation . The theory predicts that these distributions have standard deviation that scales as 1/sqrt ( d ) . The plot in 2 ( c ) shows the standard deviation as a function of dimension of the curves in 2 ( b ) with a dotted line that corresponds to 1/sqrt ( d ) [ on a log-log plot ] . The dots fall roughly along this dotted line ( especially for higher dimensions ) . For the sake of clarity , I added the zoomed in version of the plot in 2 ( b ) in the appendix . If you have any suggestions for how to make this more clear , I am happy to fix it . * The proof for the pdf of g ( rho ) is a bit involved in the paper that I cited . I came up with a new proof that is quite a bit simpler and included it ."}}