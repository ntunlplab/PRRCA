{"year": "2021", "forum": "KG4igOosnw8", "title": "Discriminative Representation Loss (DRL): A More Efficient Approach than Gradient Re-Projection in Continual Learning", "decision": "Reject", "meta_review": "This paper explores the connection between diversity of gradients and discriminativeness of representations. Based on the observations, authors propose Discriminative Representation Loss (DRL).\n\nThis paper resulted in a lot of discussions and specifically, R5's detailed comments helped the authors improve their paper. Authors did a good job in making significant improvements to the paper based on the reviews, including better connecting the theory and experiments. However, after much discussion it was felt that experiments and analysis still needed improvement, leading to a decision to reject. The authors are encouraged to use the reviewers' post-discussion updates to further improve and submit to a future venue.", "reviews": [{"review_id": "KG4igOosnw8-0", "review_text": "This paper analyzes the relationship of gradient diversity to the performance of continual learning systems , the analysis inspires a novel loss which shows some improvement in the continual learning setting . Strengths : -The empirical analysis is interesting making connections to Liu 2020 and some effective visualizations . The linear cases is studied in detail and provides intuition for the method -The proposed loss gives some improvement in larger memory settings Weaknesses : -Proposed method is too informally motivated by diversity analysis . It \u2019 s not clear to the reviewer how the theoretical observation regarding the linear case can be extended to the non-linear case . -Experimental results are promising but not completely convincing especially with the 2 additional hyperparameters : 1 . The authors use buffer settings much higher than prior works ( e.g . [ a , b ] consider memories of 200,500,1000 ) , this choice is not explained nor lack of comparison to any existing published result 2 . There are several works e.g . [ a , b , c ] which consider the same setting as this work and have better performance than the baselines shown . For example for CIFAR-10 [ a ] uses smaller memory size and has higher performance than shown here . -Cross entropy is already indirectly bounding something similar to the L_bt-L_wi ( [ e ] ) , so the objective with 2 hyper-parameters seems overly cumbersome . -Related Work is a bit sparse . Besides the works mentioned above there is several considering metric learning based approaches for continual learning which should be discussed ( e.g . [ d ] ) Question : -Are the hidden representations normalized ? -The minibatches used are small and thus can have few same class pairs , how do the authors assure that L_wi can be optimized in this case , particularly with say 100 classes as in CIFAR-100 . Overall , I find the work promising . The observations regarding gradient diversity might be the basis of future more effective methods . On the other hand the exact method proposed and results are not completely convincing in their current state . [ a ] Aljundi et al \u201c Online Continual Learning with Maximally Interfered Retrieval \u201d [ b ] Ji et al \u201c Automatic recall machines : Internal replay , continual learning and the brain \u201d [ c ] Caccia et al \u201c Online Learned Continual Compression with Adaptive Quantization Modules \u201d [ d ] Li et al \u201c Better Knowledge Retention through Metric Learning \u201d [ e ] Boudiaf `` A unifying mutual information view of metric learning : cross-entropy vs. pairwise losses '' - Post-rebuttal : After reading the rebuttal I maintain my score . The observations seem promising , the method a bit cumbersome but also interesting , but neither of them are fully fleshed out . The authors should either greatly expand the empirical analysis ( in the non-linear setting ) of their claims on intra and inter class variability in CL and/or make the experiments of the DRL method more convincing and varied in scope", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate R3 for recognizing the contribution of our work and will clarify most of the issues in the revision . In the following we address R3 \u2019 s main concerns one by one . \u201c -Proposed method is too informally motivated by diversity analysis . It \u2019 s not clear to the reviewer how the theoretical observation regarding the linear case can be extended to the non-linear case. \u201d The Theorems explain in which cases we are likely to get negative inner product of gradients , which gives two conditions : 1 ) . positive inner product of inter-class representations with unconfident predictions ; 2 ) negative inner product of within-class representations . It indicates decreasing the similarity of inter-class representations can be helpful with reducing negative inner product of gradients . We empirically verify this on a non-linear model by the experiments in Fig.2 , which demonstrates smaller representation similarity of inter-class samples leads to less negative gradient similarity . We provide more empirical study of connecting the non-linear case to the linear case in Sec.2 in the revision , which shows the non-linear model generally behaves very similar with linear model and aligns with the theorems . \u201c -Experimental results are promising but not completely convincing especially with the 2 additional hyperparameters : The authors use buffer settings much higher than prior works ( e.g . [ a , b ] consider memories of 200,500,1000 ) , this choice is not explained nor lack of comparison to any existing published result . There are several works e.g . [ a , b , c ] which consider the same setting as this work and have better performance than the baselines shown . For example , for CIFAR-10 [ a ] uses smaller memory size and has higher performance than shown here. \u201d We tested a fixed memory size for a fair comparison with GSS because it does not select samples by class index when forming the episodic memory . Regarding the experiments of CIFAR-10 , the training size per task is 9750 samples and the network is a standard ResNet18 in [ a , b , c ] , whereas in our settings , the training size is 3000 samples per task and the network is a reduced ResNet18 as the same as Chaudhry et.al . ( 2019a ) , which increases the difficulty of this benchmark and shows data efficiency of our method . Referring to the experiments of split-MNIST in [ a ] , they use a larger network with 400 hidden units per layer whereas we use 100 , and they use a memory with 50 samples per class whereas we use 300 in total ( 30 per class in the final task ) , and our method gives a better avg . accuracy. \u201c -Cross entropy is already indirectly bounding something similar to the L_bt-L_wi ( [ e ] ) , so the objective with 2 hyper-parameters seems overly cumbersome. \u201d The objective of DRL is L_bt + \\alpha L_wi , in which the L_wi has an opposite sign of the suggested form in [ e ] , - L_wi corresponds to learning compact representations within classes , and in our method , +L_wi corresponds to preventing over compactness within classes . So \\alpha controls an opposite strength of L_wi which is not bounded by cross entropy . \u201c Question : -Are the hidden representations normalized ? -The minibatches used are small and thus can have few same class pairs , how do the authors assure that L_wi can be optimized in this case , particularly with say 100 classes as in CIFAR-100. \u201d The hidden representations are normalized in Multisimilarity and R-Margin as it is explicitly required in the two papers . They are not normalized in DRL as we found there is no significant difference without normalization . We guarantee the positive pairs in each batch as described in the last three sentences of Sec.4 and Alg . 2 in the Appendix B ."}, {"review_id": "KG4igOosnw8-1", "review_text": "# # # # # # # # # # # # # # Summary # # # # # # # # # # # # # # This submission draws a connection between deep representation learning and continual learning . The authors include theoretical and empirical analyses that suggest that the learned model for continual learning should both separate the representations of instances of different classes and separate the representations of instances within a single class . This motivates a new representation-learning-based continual learning method called DRL , which the authors evaluate empirically against various baselines . # # # # # # # # # # # # # # Strengths # # # # # # # # # # # # # # 1 . The problem studied in this work is interesting . A deeper understanding of what types of representations could enable continual learning could potentially be very impactful to the field . 2.The paper includes both theoretical and empirical analyses . 3.The experimental setting is described in detail and includes a comprehensive set of baselines . # # # # # # # # # # # # # # Weaknesses # # # # # # # # # # # # # # 1 . The theoretical findings seem to be somewhat disconnected from the empirical study in Section 2.1 , and the analysis connecting the two is unconvincing . Moreover , given this disconnect , the empirical study should have been much more complete , including various data sets . 2.The writing and the structure of the paper is quite unclear , leaving the reader confused at times , having to jump back and forth to draw connections between different sections that are not explicitly described in the text . 3.The experimental evaluation is inconsistent in showing the advantage of their method , and this is not analyzed in any depth . # # # # # # # # # # # # # # Recommendation # # # # # # # # # # # # # # Unfortunately , I lean towards recommending the rejection of this paper . While the studied problem is of high relevance , whose answer could be very impactful , I find the theoretical and empirical analyses in this work to be unconvincing , and thus the claims made throughout the paper to be insufficiently founded . The authors use very simple toy experiments and theorems on linear models to motivate their claim that learned representations should be very different across different classes . While using theory from linear models is a common technique for motivating empirical study of deep learning models , a much more comprehensive empirical study would be needed to validate that the theoretical findings extend to the deep learning setting . Moreover , the paper should be substantially revised both to add clarity to the motivation of the problem , and to more carefully analyze the connections between the theoretical and empirical analyses in Section 2.1 . # # # # # # # # # # # # # # Arguments # # # # # # # # # # # # # # The biggest reason I lean towards rejecting this paper is that I am unconvinced by the claims made throughout Section 2 . First , the section begins by suggesting that instances with similar gradients are those most useful for generalization . This seems to contradict the findings of Aljundi et al. , which chooses to keep samples with maximum gradient diversity in the episodic memory . This apparent contradiction should be carefully discussed and analyzed . Then , the experiment of Figure 1 suggests that : the gradients with the most diverse gradients are those close to the decision boundary , and in order to avoid forgetting previously seen tasks , samples with diverse representations help maintain good decision boundaries as new tasks are learned . The former claim is further studied in the rest of the section , but why does this experiment suggest that we would like to keep samples with diverse gradients ? I keep seeing conflicting indications that we should store samples with diverse or similar gradients . On the other hand , the latter claim is not substantiated beyond these toy experiments . It seems to be specific to incremental-class learning , and would probably not hold for other variants of continual learning that do n't require the same model to discern between the new class and the previous ones . In Theorem 1 , the authors claim that points close to the decision boundary are likely to yield negative inner product of their gradients . Figure 4 in the Appendix seeks to illustrate this , and could potentially be very helpful towards understanding this work better , but 1 ) the figure is missing a y-axis label and 2 ) the caption is confusing and contradictory between the sub-figure captions and the full figure caption , which makes it hard to take much away from it . Moreover , the claim hinges on the assumption that < x_n , x_m > is likely to be positive if classes are near the decision boundary . This does n't seem to be addressed in Figure 4 , and I do n't believe it is necessarily the case in high-dimensional spaces , where points can be very different and still close to the decision boundary . Are the authors assuming that _all_ inner products of the representations are positive , since they use ReLU activations ? This should be explicitly clarified . In Theorem 2 , the authors mention that a deep net can be viewed as a representation extractor , and that ReLU activation would lead to positive inner products for the representations . 1.First , this seems vacuous , since all inner products ( even across classes ) would be positive . 2.Second , the inner product of the gradients _of the final linear layer_ are the ones that are guaranteed to have positive inner products , but this claim says nothing about the gradients of remaining layers in the network . ~~Since recent work [ 1 ] has shown that forgetting primarily occurs at the shallow layers of the network , this does not seem to be a helpful theorem.~~ It is still useful , since recent work [ 1 ] showed that forgetting primarily occurs at the deepest layers of the network , but this should be stated clearly . Then , the authors move to a simple experiment using deep nets , which is supposed to help extend the findings of Theorems 1 and 2 to the deep learning case . However , the connections between these two appear to be very weak and poorly described . - It is unclear why representations are obtained by concatenating the outputs of all layers , or how this relates to Theorems 1 and 2 . Why not use directly the input feature representation , or better yet use only the final hidden representation before the output layer as hinted at in Theorem 2 ? This choice is not justified . - Are the gradients also obtained by concatenating all layers ' gradients ? This _should_ certainly be the case , since we want to learn about the gradient of the entire model . - It seems that in Theorem 1 we care only about the sign of the gradient : positive representation similarity likely leads to negative gradient similarity . In Figure 2 , _all_ points have positive representation similarity , so it 's unclear how we can relate these two . It seems that we should now look for decreasing gradient similarity as representation similarity increases between different classes , but this is not stated or analyzed . Also , there is no mention of \\beta , \\delta , or \\alpha , all of which could be explicitly computed and used to analyze these results . Overall , the Theorems seem to give results only about signs and not distances , whereas the deep learning experiments try to extend the analysis with distances . This means that distances are actually far more useful for relating the findings to metric learning . Since the empirical findings are actually different from the theoretical ones , these experiments should be far more extensive to justify the proposed method , including analyses on more complex data sets , and a much clearer description of exactly how the experiments are carried out . The take-away from Section 2.1 seems to be that similar representations across different classes lead to conflicting gradients . This is only hinted at in various ways , but I do n't believe it is directly stated . Clarifying this would tremendously help lead into Section 2.2 . Moreover , it is never explained what the intuition behind this is , which would also be very helpful to increase the reader 's ability to follow the paper . I would expect that the intuition is that having similar representations across different tasks would lead the network to somehow confuse the different classes , but I do n't believe this is explained . In Section 2.2 , I find the motivation for learning a diverse set of representations for each class somewhat unconvincing . The experiments of Figure 1 show that the learner suffers forgetting on the previous tasks because it only keeps points with similar representation . However , the only reason why this is a problem is that there exist other points with very dissimilar representations , which will then be ignored by the updated margin . If we learned a representation where all points within a class are similar , then that would not seem to be a problem . While this is somewhat analyzed in the experiments in Table 4 via the rho-spectrum , this analysis is quite incomplete . Moreover , Table 4 shows no correlation between forgetting and rho-spectrum , which is what Figure 1 suggests we should expect . This is not discussed in any way . To add to the confusion in the analysis , the experiments in Section 5 use inconsistent definitions of what the representation is , with all layers being used as the representation in some cases ( just like in the analysis of Figure 2 ) , but only the final representation and the logits being used in other cases ( which , as an aside , violates the claim that the inner products are necessarily positive ) . The empirical results somewhat inconsistently show that the proposed method is better than baselines . However , considering that MNIST-based data sets are usually not very indicative of performance in more complex data sets , I would focus my analysis on the CIFAR-10 and CIFAR-100 experiments . Here , the proposed method was best in one case but not in another . This would require a much more in-depth analysis , but the whole results analysis is limited to a single paragraph . [ 1 ] Ramasesh , V. V. , Dyer , E. , & Raghu , M. ( 2020 ) . Anatomy of catastrophic forgetting : Hidden representations and task semantics . arXiv preprint arXiv:2007.07400 . # # # # # # # # # # # # # # Additional feedback # # # # # # # # # # # # # # The following points are provided as feedback to hopefully help better shape the submitted manuscript , but did not impact my recommendation in a major way . Abstract - DRL is efficient w.r.t.what ? - The intro seems to suggest it 's computational efficiency - The first time we actually get to see what this means is at the end of the paper , buried in the last paragraph of Section 5 . Since this seems to be such a relevant point ( enough to point to it in the Appendix and Introduction ) , it would be relevant to bring it up earlier and give more details about how it compares to other methods . Intro - I would n't consider that methods for growing the model size fall in the same category as regularization-based methods . Their focus is n't preserving knowledge of past tasks ' models , but instead adding the current task 's knowledge into a separate model . - Paragraph 4 : `` direction that closest '' -- > `` direction that [ is ] closest '' - Why such a detailed summary of gradient-based methods ? Push to separate Related Work section . - Again it 's unclear what the metric for efficiency is . Context seems to suggest it 's computation time , but one could also think it 's data efficiency or memory capacity ( since the former is commonly important in lifelong learning and the latter is mentioned in this section ) . Sec 2.1 - Why two different notations for x_n with and without boldface ? They both seem to denote a single data point . - Theorem 1 is the first time the authors mention negative inner product of the gradients . Before , only the terms `` similar '' and `` diverse '' were used . I suggest making this clearer in the explanation throughout . - Deep learning experiments - Is the correlation coefficient between the x axis ( representation similarity ) and the y axis ( gradient similarity ) ? - It would be helpful to clarify explicitly in the text that in Figure 2 the blue dots should be analyzed under Theorem 1 and the orange dots under Theorem 2 . - The final statement about discriminative representations of task 1 vs task 2 does n't seem to add much value , so my suggestion would be to drop it or clarify how it 's useful to the argument of the paper . Sec 2.2 - The first sentence in second paragraph seems disconnected from what follows . What 's the point of it ? Where will the unused information be leveraged ? Sec 3 - Is the representation h_i the same as in the experiments of Figure 2 , i.e. , the concatenated output of all layers ? - It is made clear later that the definition of h_i is not consistent throughout the experiments . Here , the authors should clarify this and explain how this definition should be chosen , and why it is not possible to use a consistent definition . - Figure 3 is missing a y-axis label . - The effects on the similarities within classes seem to be very small in Figure 3 . Is this because \\alpha is small ? Sec 4 - The statement that in the incremental class setting the method can work without task boundaries seems vacuous : whenever the learner sees a new class , evidently it corresponds to a new task . - This is the first time that task-incremental vs class-incremental settings are introduced , without explaining exactly what they are . The manuscript should be self-contained and so it should give a clear description of these settings if they are relevant to the submission ( as they clearly are based on the statements in this section ) . Sec 5 - Fashin-MNIST -- > Fashion-MNIST - Details of the experimental setting are quite comprehensive ( + ) . - In addition , We -- > In addition , we - What is the reason the proposed method achieves a better trade-off between forgetting and intransigence ? This is not mentioned at all at any point in the paper up to this point . - Why only compare computation time on MNIST and not on remaining benchmarks ? Since computational complexity depends on the representation size , it is thus clearly dependent on the choice of representation , which inconsistently varies across benchmarks . Also , what is the theoretical complexity of the baselines ? Are there settings where they would , at least theoretically , be faster ? This comparison would strengthen the claim that the method is more efficient . - I would also like to see an ablative study removing each of the terms in the regularization function in turn . If ER is not implemented using the same sampling strategy as BER , then I 'd also like to see the effect of BER training on its own . This would help tease apart the contributions of this work better . Appendices - How are the hyper-parameters of DRL chosen in Table 6 ? -- Updates during discussion -- I have updated my score from a 4 to a 5 based on the authors ' response . My justification is below in my comment to the authors .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We really appreciate R5 for giving such thorough review comments and will clarify most of the issues in the revision . In the following we address R5 \u2019 s main concerns . We first clarify two crucial issues that seem to have informed some of these concerns : 1 . We do NOT suggest storing samples with most diverse gradients into the memory , it is suggested in GSS ( Aljundi et al.2019 ) .The experiments in Fig.1 verifies this idea and the results show it may NOT be sufficient for learning new decision boundaries in continual learning . 2.The Theorems do NOT require the inner product of representations always to be positive . We analyze in which cases the inner product of gradients tends to be negative by the Theorems . Using ReLU-like activations is taken as a common case in practice for the analysis . If checking the Theorems by assuming the inner product of representations is negative , they are still valid , e.g.according to Theorem 2 , negative inner product of within-class representations results in negative inner product of gradients , which indicates non-negative similarity of representations is preferred within classes . We provide more empirical study in Sec.2 in the revision for a better understanding of the theorems . \u201c The biggest reason I lean towards rejecting this paper is that I am unconvinced by the claims made throughout Section 2 . First , the section begins by suggesting that instances with similar gradients are those most useful for generalization . This seems to contradict the findings of Aljundi et al. , which chooses to keep samples with maximum gradient diversity in the episodic memory . This apparent contradiction should be carefully discussed and analyzed. \u201d The final sentence in the first paragraph of section 2 is : \u201c This in turn indicates that samples that lead to the most diverse gradients provide the most difficulty of generalization. \u201d We see the confusion caused by this phrase and will rephrase it as \u2018 It indicates that samples that lead to the most diverse gradients provide the most information that is useful for generalization , which is consistent with Aljundi et al . ( 2019 ) . \u2018 \u201c Then , the experiment of Figure 1 suggests that : the gradients with the most diverse gradients are those close to the decision boundary , and in order to avoid forgetting previously seen tasks , samples with diverse representations help maintain good decision boundaries as new tasks are learned . The former claim is further studied in the rest of the section , but why does this experiment suggest that we would like to keep samples with diverse gradients ? I keep seeing conflicting indications that we should store samples with diverse or similar gradients . On the other hand , the latter claim is not substantiated beyond these toy experiments . \u201c We appreciate that there is some subtlety in the experiments of Fig.1 , so we will attempt to clarify . The first experiment in Fig1.a identifies which samples produce the most diverse gradients that could be important in generalization . The results show those samples are close to the decision boundary . Then we tested storing those samples into the memory ( Fig1.b ) which was actually testing GSS ( Aljundi et al. , 2019 ) because those samples are chosen by GSS , we compared the results with storing randomly chosen samples ( Fig1.c ) . The performance of GSS is worse than the random samples since only storing samples with most diverse gradients of the current task may not be sufficient for learning new decision boundaries . Hence , the experiment suggests it is NOT enough to only store samples with the most diverse gradients . We do NOT suggest storing samples with similar gradients . \u201c It seems to be specific to incremental-class learning , and would probably not hold for other variants of continual learning that do n't require the same model to discern between the new class and the previous ones. \u201d Although the motivation is from class-incremental learning , the insights can be useful for instance-incremental learning as well , such as Permuted-MNIST tasks , and the experimental results in Sec.5 demonstrate this . For example , the previously learned decision boundary relies on several dimensions that are not discriminative for new instances , so if the memory only contains samples close to the previous boundary , it may be difficult to learn a new boundary that works for both old and new instances . \u201c In Theorem 1 , the authors claim that points close to the decision boundary are likely to yield negative inner product of their gradients . Figure 4 in the Appendix seeks to illustrate this , and could potentially be very helpful towards understanding this work better , but 1 ) the figure is missing a y-axis label and 2 ) the caption is confusing and contradictory between the sub-figure captions and the full figure caption ... \u201d Sorry for the confusing figure . We removed figure 4 from the Appendix and added new experiments and figures in Sec.2 in the revision for a better understanding ."}, {"review_id": "KG4igOosnw8-2", "review_text": "This paper presents a novel way of making full use of compact episodic memory to alleviate catastrophic forgetting in continual learning . This is done by adding the proposed discriminative representation loss to regularize the gradients produced by new samples . Authors gave insightful analysis on the influence of gradient diversity to the performance of continual learning , and proposed a regularization that connects metric learning and continual learning . However , there are still some issues to be addressed as below . 1.Authors do not explain how to determine weights of the two terms in the proposed discriminative representation loss to achieve good balance between discrmination capability on the current task and generalization on future task . 2.No experiment on the influence of memory size and batch size on the proposed method . These are important hyperparameters for the proposed method . So there should be experiment inspecting its influnece on the performance . 3.There is also no experiment on the influence of different memory update rule on the proposed method . It is not clear how well the proposed method is robust to different memory update rules . 4.Experiments on more diverse datasets should be done to prove its effectiveness , such as TinyImageNet , and MIT Scenes/Oxford Flowers/UCSD Birds/Stanford Cars .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate R1 for recognizing the contribution of our work and will add more experiments in the revision . In the following we address R1 \u2019 s main concerns one by one . \u201c Authors do not explain how to determine weights of the two terms in the proposed discriminative representation loss to achieve good balance between discrmination capability on the current task and generalization on future task. \u201d Regarding the two terms in DRL , a larger weight on L_wi is for less compact representations within classes , but a too dispersed representation space may include too much noise . For datasets that present more difficulty in learning compact representations , we would prefer a smaller weight on L_wi , we therefore set smaller \\alpha for CIFAR datasets in our experiments . A larger weight on L_bt is more resistant to forgetting but may be less capable of transferring to a new task , for datasets that are less compatible between tasks a smaller weight on L_bt would be preferred , as we set the largest \\lambda on Permuted MNIST and the smallest \\lambda on CIFAR-100 in our experiments . Table 6 in the appendix shows the hyperparamters used in all the experiments . \u201c No experiment on the influence of memory size and batch size on the proposed method . These are important hyperparameters for the proposed method . So , there should be experiment inspecting its influnece on the performance. \u201d The memory size is the larger the better for performance which is consistent in the literature of continual learning and a perfect memory would lead to a perfect performance as suggested in Knoblauch et al . ( 2020 ) .We provide experiment results of comparing different memory sizes in the Appendix E in the revision . Most of the compared baselines use a small batch size in their original papers ( such as 10 in A-GEM & GSS , 20 in ER ) , and we did not find large batch size can improve the performance , also the batch size highly relates to the computational complexity of most methods , so we keep it small in all experiments . Knoblauch , Jeremias , Hisham Husain , and Tom Diethe . `` Optimal Continual Learning has Perfect Memory and is NP-hard . '' ICML ( 2020 ) . \u201c There is also no experiment on the influence of different memory update rule on the proposed method . It is not clear how well the proposed method is robust to different memory update rules. \u201d The performance difference between DRL+ ER and DRL+BER is similar as the difference between ER and BER , DRL+ER is better than ER but worse than DRL+BER . We included results of comparing DRL with different replay strategies in Appendix F in the revision . \u201c Experiments on more diverse datasets should be done to prove its effectiveness , such as TinyImageNet , and MIT Scenes/Oxford Flowers/UCSD Birds/Stanford Cars. \u201d We provided experimental results on TinyImageNet in the revision , in which DRL outperforms other baselines ."}], "0": {"review_id": "KG4igOosnw8-0", "review_text": "This paper analyzes the relationship of gradient diversity to the performance of continual learning systems , the analysis inspires a novel loss which shows some improvement in the continual learning setting . Strengths : -The empirical analysis is interesting making connections to Liu 2020 and some effective visualizations . The linear cases is studied in detail and provides intuition for the method -The proposed loss gives some improvement in larger memory settings Weaknesses : -Proposed method is too informally motivated by diversity analysis . It \u2019 s not clear to the reviewer how the theoretical observation regarding the linear case can be extended to the non-linear case . -Experimental results are promising but not completely convincing especially with the 2 additional hyperparameters : 1 . The authors use buffer settings much higher than prior works ( e.g . [ a , b ] consider memories of 200,500,1000 ) , this choice is not explained nor lack of comparison to any existing published result 2 . There are several works e.g . [ a , b , c ] which consider the same setting as this work and have better performance than the baselines shown . For example for CIFAR-10 [ a ] uses smaller memory size and has higher performance than shown here . -Cross entropy is already indirectly bounding something similar to the L_bt-L_wi ( [ e ] ) , so the objective with 2 hyper-parameters seems overly cumbersome . -Related Work is a bit sparse . Besides the works mentioned above there is several considering metric learning based approaches for continual learning which should be discussed ( e.g . [ d ] ) Question : -Are the hidden representations normalized ? -The minibatches used are small and thus can have few same class pairs , how do the authors assure that L_wi can be optimized in this case , particularly with say 100 classes as in CIFAR-100 . Overall , I find the work promising . The observations regarding gradient diversity might be the basis of future more effective methods . On the other hand the exact method proposed and results are not completely convincing in their current state . [ a ] Aljundi et al \u201c Online Continual Learning with Maximally Interfered Retrieval \u201d [ b ] Ji et al \u201c Automatic recall machines : Internal replay , continual learning and the brain \u201d [ c ] Caccia et al \u201c Online Learned Continual Compression with Adaptive Quantization Modules \u201d [ d ] Li et al \u201c Better Knowledge Retention through Metric Learning \u201d [ e ] Boudiaf `` A unifying mutual information view of metric learning : cross-entropy vs. pairwise losses '' - Post-rebuttal : After reading the rebuttal I maintain my score . The observations seem promising , the method a bit cumbersome but also interesting , but neither of them are fully fleshed out . The authors should either greatly expand the empirical analysis ( in the non-linear setting ) of their claims on intra and inter class variability in CL and/or make the experiments of the DRL method more convincing and varied in scope", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate R3 for recognizing the contribution of our work and will clarify most of the issues in the revision . In the following we address R3 \u2019 s main concerns one by one . \u201c -Proposed method is too informally motivated by diversity analysis . It \u2019 s not clear to the reviewer how the theoretical observation regarding the linear case can be extended to the non-linear case. \u201d The Theorems explain in which cases we are likely to get negative inner product of gradients , which gives two conditions : 1 ) . positive inner product of inter-class representations with unconfident predictions ; 2 ) negative inner product of within-class representations . It indicates decreasing the similarity of inter-class representations can be helpful with reducing negative inner product of gradients . We empirically verify this on a non-linear model by the experiments in Fig.2 , which demonstrates smaller representation similarity of inter-class samples leads to less negative gradient similarity . We provide more empirical study of connecting the non-linear case to the linear case in Sec.2 in the revision , which shows the non-linear model generally behaves very similar with linear model and aligns with the theorems . \u201c -Experimental results are promising but not completely convincing especially with the 2 additional hyperparameters : The authors use buffer settings much higher than prior works ( e.g . [ a , b ] consider memories of 200,500,1000 ) , this choice is not explained nor lack of comparison to any existing published result . There are several works e.g . [ a , b , c ] which consider the same setting as this work and have better performance than the baselines shown . For example , for CIFAR-10 [ a ] uses smaller memory size and has higher performance than shown here. \u201d We tested a fixed memory size for a fair comparison with GSS because it does not select samples by class index when forming the episodic memory . Regarding the experiments of CIFAR-10 , the training size per task is 9750 samples and the network is a standard ResNet18 in [ a , b , c ] , whereas in our settings , the training size is 3000 samples per task and the network is a reduced ResNet18 as the same as Chaudhry et.al . ( 2019a ) , which increases the difficulty of this benchmark and shows data efficiency of our method . Referring to the experiments of split-MNIST in [ a ] , they use a larger network with 400 hidden units per layer whereas we use 100 , and they use a memory with 50 samples per class whereas we use 300 in total ( 30 per class in the final task ) , and our method gives a better avg . accuracy. \u201c -Cross entropy is already indirectly bounding something similar to the L_bt-L_wi ( [ e ] ) , so the objective with 2 hyper-parameters seems overly cumbersome. \u201d The objective of DRL is L_bt + \\alpha L_wi , in which the L_wi has an opposite sign of the suggested form in [ e ] , - L_wi corresponds to learning compact representations within classes , and in our method , +L_wi corresponds to preventing over compactness within classes . So \\alpha controls an opposite strength of L_wi which is not bounded by cross entropy . \u201c Question : -Are the hidden representations normalized ? -The minibatches used are small and thus can have few same class pairs , how do the authors assure that L_wi can be optimized in this case , particularly with say 100 classes as in CIFAR-100. \u201d The hidden representations are normalized in Multisimilarity and R-Margin as it is explicitly required in the two papers . They are not normalized in DRL as we found there is no significant difference without normalization . We guarantee the positive pairs in each batch as described in the last three sentences of Sec.4 and Alg . 2 in the Appendix B ."}, "1": {"review_id": "KG4igOosnw8-1", "review_text": "# # # # # # # # # # # # # # Summary # # # # # # # # # # # # # # This submission draws a connection between deep representation learning and continual learning . The authors include theoretical and empirical analyses that suggest that the learned model for continual learning should both separate the representations of instances of different classes and separate the representations of instances within a single class . This motivates a new representation-learning-based continual learning method called DRL , which the authors evaluate empirically against various baselines . # # # # # # # # # # # # # # Strengths # # # # # # # # # # # # # # 1 . The problem studied in this work is interesting . A deeper understanding of what types of representations could enable continual learning could potentially be very impactful to the field . 2.The paper includes both theoretical and empirical analyses . 3.The experimental setting is described in detail and includes a comprehensive set of baselines . # # # # # # # # # # # # # # Weaknesses # # # # # # # # # # # # # # 1 . The theoretical findings seem to be somewhat disconnected from the empirical study in Section 2.1 , and the analysis connecting the two is unconvincing . Moreover , given this disconnect , the empirical study should have been much more complete , including various data sets . 2.The writing and the structure of the paper is quite unclear , leaving the reader confused at times , having to jump back and forth to draw connections between different sections that are not explicitly described in the text . 3.The experimental evaluation is inconsistent in showing the advantage of their method , and this is not analyzed in any depth . # # # # # # # # # # # # # # Recommendation # # # # # # # # # # # # # # Unfortunately , I lean towards recommending the rejection of this paper . While the studied problem is of high relevance , whose answer could be very impactful , I find the theoretical and empirical analyses in this work to be unconvincing , and thus the claims made throughout the paper to be insufficiently founded . The authors use very simple toy experiments and theorems on linear models to motivate their claim that learned representations should be very different across different classes . While using theory from linear models is a common technique for motivating empirical study of deep learning models , a much more comprehensive empirical study would be needed to validate that the theoretical findings extend to the deep learning setting . Moreover , the paper should be substantially revised both to add clarity to the motivation of the problem , and to more carefully analyze the connections between the theoretical and empirical analyses in Section 2.1 . # # # # # # # # # # # # # # Arguments # # # # # # # # # # # # # # The biggest reason I lean towards rejecting this paper is that I am unconvinced by the claims made throughout Section 2 . First , the section begins by suggesting that instances with similar gradients are those most useful for generalization . This seems to contradict the findings of Aljundi et al. , which chooses to keep samples with maximum gradient diversity in the episodic memory . This apparent contradiction should be carefully discussed and analyzed . Then , the experiment of Figure 1 suggests that : the gradients with the most diverse gradients are those close to the decision boundary , and in order to avoid forgetting previously seen tasks , samples with diverse representations help maintain good decision boundaries as new tasks are learned . The former claim is further studied in the rest of the section , but why does this experiment suggest that we would like to keep samples with diverse gradients ? I keep seeing conflicting indications that we should store samples with diverse or similar gradients . On the other hand , the latter claim is not substantiated beyond these toy experiments . It seems to be specific to incremental-class learning , and would probably not hold for other variants of continual learning that do n't require the same model to discern between the new class and the previous ones . In Theorem 1 , the authors claim that points close to the decision boundary are likely to yield negative inner product of their gradients . Figure 4 in the Appendix seeks to illustrate this , and could potentially be very helpful towards understanding this work better , but 1 ) the figure is missing a y-axis label and 2 ) the caption is confusing and contradictory between the sub-figure captions and the full figure caption , which makes it hard to take much away from it . Moreover , the claim hinges on the assumption that < x_n , x_m > is likely to be positive if classes are near the decision boundary . This does n't seem to be addressed in Figure 4 , and I do n't believe it is necessarily the case in high-dimensional spaces , where points can be very different and still close to the decision boundary . Are the authors assuming that _all_ inner products of the representations are positive , since they use ReLU activations ? This should be explicitly clarified . In Theorem 2 , the authors mention that a deep net can be viewed as a representation extractor , and that ReLU activation would lead to positive inner products for the representations . 1.First , this seems vacuous , since all inner products ( even across classes ) would be positive . 2.Second , the inner product of the gradients _of the final linear layer_ are the ones that are guaranteed to have positive inner products , but this claim says nothing about the gradients of remaining layers in the network . ~~Since recent work [ 1 ] has shown that forgetting primarily occurs at the shallow layers of the network , this does not seem to be a helpful theorem.~~ It is still useful , since recent work [ 1 ] showed that forgetting primarily occurs at the deepest layers of the network , but this should be stated clearly . Then , the authors move to a simple experiment using deep nets , which is supposed to help extend the findings of Theorems 1 and 2 to the deep learning case . However , the connections between these two appear to be very weak and poorly described . - It is unclear why representations are obtained by concatenating the outputs of all layers , or how this relates to Theorems 1 and 2 . Why not use directly the input feature representation , or better yet use only the final hidden representation before the output layer as hinted at in Theorem 2 ? This choice is not justified . - Are the gradients also obtained by concatenating all layers ' gradients ? This _should_ certainly be the case , since we want to learn about the gradient of the entire model . - It seems that in Theorem 1 we care only about the sign of the gradient : positive representation similarity likely leads to negative gradient similarity . In Figure 2 , _all_ points have positive representation similarity , so it 's unclear how we can relate these two . It seems that we should now look for decreasing gradient similarity as representation similarity increases between different classes , but this is not stated or analyzed . Also , there is no mention of \\beta , \\delta , or \\alpha , all of which could be explicitly computed and used to analyze these results . Overall , the Theorems seem to give results only about signs and not distances , whereas the deep learning experiments try to extend the analysis with distances . This means that distances are actually far more useful for relating the findings to metric learning . Since the empirical findings are actually different from the theoretical ones , these experiments should be far more extensive to justify the proposed method , including analyses on more complex data sets , and a much clearer description of exactly how the experiments are carried out . The take-away from Section 2.1 seems to be that similar representations across different classes lead to conflicting gradients . This is only hinted at in various ways , but I do n't believe it is directly stated . Clarifying this would tremendously help lead into Section 2.2 . Moreover , it is never explained what the intuition behind this is , which would also be very helpful to increase the reader 's ability to follow the paper . I would expect that the intuition is that having similar representations across different tasks would lead the network to somehow confuse the different classes , but I do n't believe this is explained . In Section 2.2 , I find the motivation for learning a diverse set of representations for each class somewhat unconvincing . The experiments of Figure 1 show that the learner suffers forgetting on the previous tasks because it only keeps points with similar representation . However , the only reason why this is a problem is that there exist other points with very dissimilar representations , which will then be ignored by the updated margin . If we learned a representation where all points within a class are similar , then that would not seem to be a problem . While this is somewhat analyzed in the experiments in Table 4 via the rho-spectrum , this analysis is quite incomplete . Moreover , Table 4 shows no correlation between forgetting and rho-spectrum , which is what Figure 1 suggests we should expect . This is not discussed in any way . To add to the confusion in the analysis , the experiments in Section 5 use inconsistent definitions of what the representation is , with all layers being used as the representation in some cases ( just like in the analysis of Figure 2 ) , but only the final representation and the logits being used in other cases ( which , as an aside , violates the claim that the inner products are necessarily positive ) . The empirical results somewhat inconsistently show that the proposed method is better than baselines . However , considering that MNIST-based data sets are usually not very indicative of performance in more complex data sets , I would focus my analysis on the CIFAR-10 and CIFAR-100 experiments . Here , the proposed method was best in one case but not in another . This would require a much more in-depth analysis , but the whole results analysis is limited to a single paragraph . [ 1 ] Ramasesh , V. V. , Dyer , E. , & Raghu , M. ( 2020 ) . Anatomy of catastrophic forgetting : Hidden representations and task semantics . arXiv preprint arXiv:2007.07400 . # # # # # # # # # # # # # # Additional feedback # # # # # # # # # # # # # # The following points are provided as feedback to hopefully help better shape the submitted manuscript , but did not impact my recommendation in a major way . Abstract - DRL is efficient w.r.t.what ? - The intro seems to suggest it 's computational efficiency - The first time we actually get to see what this means is at the end of the paper , buried in the last paragraph of Section 5 . Since this seems to be such a relevant point ( enough to point to it in the Appendix and Introduction ) , it would be relevant to bring it up earlier and give more details about how it compares to other methods . Intro - I would n't consider that methods for growing the model size fall in the same category as regularization-based methods . Their focus is n't preserving knowledge of past tasks ' models , but instead adding the current task 's knowledge into a separate model . - Paragraph 4 : `` direction that closest '' -- > `` direction that [ is ] closest '' - Why such a detailed summary of gradient-based methods ? Push to separate Related Work section . - Again it 's unclear what the metric for efficiency is . Context seems to suggest it 's computation time , but one could also think it 's data efficiency or memory capacity ( since the former is commonly important in lifelong learning and the latter is mentioned in this section ) . Sec 2.1 - Why two different notations for x_n with and without boldface ? They both seem to denote a single data point . - Theorem 1 is the first time the authors mention negative inner product of the gradients . Before , only the terms `` similar '' and `` diverse '' were used . I suggest making this clearer in the explanation throughout . - Deep learning experiments - Is the correlation coefficient between the x axis ( representation similarity ) and the y axis ( gradient similarity ) ? - It would be helpful to clarify explicitly in the text that in Figure 2 the blue dots should be analyzed under Theorem 1 and the orange dots under Theorem 2 . - The final statement about discriminative representations of task 1 vs task 2 does n't seem to add much value , so my suggestion would be to drop it or clarify how it 's useful to the argument of the paper . Sec 2.2 - The first sentence in second paragraph seems disconnected from what follows . What 's the point of it ? Where will the unused information be leveraged ? Sec 3 - Is the representation h_i the same as in the experiments of Figure 2 , i.e. , the concatenated output of all layers ? - It is made clear later that the definition of h_i is not consistent throughout the experiments . Here , the authors should clarify this and explain how this definition should be chosen , and why it is not possible to use a consistent definition . - Figure 3 is missing a y-axis label . - The effects on the similarities within classes seem to be very small in Figure 3 . Is this because \\alpha is small ? Sec 4 - The statement that in the incremental class setting the method can work without task boundaries seems vacuous : whenever the learner sees a new class , evidently it corresponds to a new task . - This is the first time that task-incremental vs class-incremental settings are introduced , without explaining exactly what they are . The manuscript should be self-contained and so it should give a clear description of these settings if they are relevant to the submission ( as they clearly are based on the statements in this section ) . Sec 5 - Fashin-MNIST -- > Fashion-MNIST - Details of the experimental setting are quite comprehensive ( + ) . - In addition , We -- > In addition , we - What is the reason the proposed method achieves a better trade-off between forgetting and intransigence ? This is not mentioned at all at any point in the paper up to this point . - Why only compare computation time on MNIST and not on remaining benchmarks ? Since computational complexity depends on the representation size , it is thus clearly dependent on the choice of representation , which inconsistently varies across benchmarks . Also , what is the theoretical complexity of the baselines ? Are there settings where they would , at least theoretically , be faster ? This comparison would strengthen the claim that the method is more efficient . - I would also like to see an ablative study removing each of the terms in the regularization function in turn . If ER is not implemented using the same sampling strategy as BER , then I 'd also like to see the effect of BER training on its own . This would help tease apart the contributions of this work better . Appendices - How are the hyper-parameters of DRL chosen in Table 6 ? -- Updates during discussion -- I have updated my score from a 4 to a 5 based on the authors ' response . My justification is below in my comment to the authors .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We really appreciate R5 for giving such thorough review comments and will clarify most of the issues in the revision . In the following we address R5 \u2019 s main concerns . We first clarify two crucial issues that seem to have informed some of these concerns : 1 . We do NOT suggest storing samples with most diverse gradients into the memory , it is suggested in GSS ( Aljundi et al.2019 ) .The experiments in Fig.1 verifies this idea and the results show it may NOT be sufficient for learning new decision boundaries in continual learning . 2.The Theorems do NOT require the inner product of representations always to be positive . We analyze in which cases the inner product of gradients tends to be negative by the Theorems . Using ReLU-like activations is taken as a common case in practice for the analysis . If checking the Theorems by assuming the inner product of representations is negative , they are still valid , e.g.according to Theorem 2 , negative inner product of within-class representations results in negative inner product of gradients , which indicates non-negative similarity of representations is preferred within classes . We provide more empirical study in Sec.2 in the revision for a better understanding of the theorems . \u201c The biggest reason I lean towards rejecting this paper is that I am unconvinced by the claims made throughout Section 2 . First , the section begins by suggesting that instances with similar gradients are those most useful for generalization . This seems to contradict the findings of Aljundi et al. , which chooses to keep samples with maximum gradient diversity in the episodic memory . This apparent contradiction should be carefully discussed and analyzed. \u201d The final sentence in the first paragraph of section 2 is : \u201c This in turn indicates that samples that lead to the most diverse gradients provide the most difficulty of generalization. \u201d We see the confusion caused by this phrase and will rephrase it as \u2018 It indicates that samples that lead to the most diverse gradients provide the most information that is useful for generalization , which is consistent with Aljundi et al . ( 2019 ) . \u2018 \u201c Then , the experiment of Figure 1 suggests that : the gradients with the most diverse gradients are those close to the decision boundary , and in order to avoid forgetting previously seen tasks , samples with diverse representations help maintain good decision boundaries as new tasks are learned . The former claim is further studied in the rest of the section , but why does this experiment suggest that we would like to keep samples with diverse gradients ? I keep seeing conflicting indications that we should store samples with diverse or similar gradients . On the other hand , the latter claim is not substantiated beyond these toy experiments . \u201c We appreciate that there is some subtlety in the experiments of Fig.1 , so we will attempt to clarify . The first experiment in Fig1.a identifies which samples produce the most diverse gradients that could be important in generalization . The results show those samples are close to the decision boundary . Then we tested storing those samples into the memory ( Fig1.b ) which was actually testing GSS ( Aljundi et al. , 2019 ) because those samples are chosen by GSS , we compared the results with storing randomly chosen samples ( Fig1.c ) . The performance of GSS is worse than the random samples since only storing samples with most diverse gradients of the current task may not be sufficient for learning new decision boundaries . Hence , the experiment suggests it is NOT enough to only store samples with the most diverse gradients . We do NOT suggest storing samples with similar gradients . \u201c It seems to be specific to incremental-class learning , and would probably not hold for other variants of continual learning that do n't require the same model to discern between the new class and the previous ones. \u201d Although the motivation is from class-incremental learning , the insights can be useful for instance-incremental learning as well , such as Permuted-MNIST tasks , and the experimental results in Sec.5 demonstrate this . For example , the previously learned decision boundary relies on several dimensions that are not discriminative for new instances , so if the memory only contains samples close to the previous boundary , it may be difficult to learn a new boundary that works for both old and new instances . \u201c In Theorem 1 , the authors claim that points close to the decision boundary are likely to yield negative inner product of their gradients . Figure 4 in the Appendix seeks to illustrate this , and could potentially be very helpful towards understanding this work better , but 1 ) the figure is missing a y-axis label and 2 ) the caption is confusing and contradictory between the sub-figure captions and the full figure caption ... \u201d Sorry for the confusing figure . We removed figure 4 from the Appendix and added new experiments and figures in Sec.2 in the revision for a better understanding ."}, "2": {"review_id": "KG4igOosnw8-2", "review_text": "This paper presents a novel way of making full use of compact episodic memory to alleviate catastrophic forgetting in continual learning . This is done by adding the proposed discriminative representation loss to regularize the gradients produced by new samples . Authors gave insightful analysis on the influence of gradient diversity to the performance of continual learning , and proposed a regularization that connects metric learning and continual learning . However , there are still some issues to be addressed as below . 1.Authors do not explain how to determine weights of the two terms in the proposed discriminative representation loss to achieve good balance between discrmination capability on the current task and generalization on future task . 2.No experiment on the influence of memory size and batch size on the proposed method . These are important hyperparameters for the proposed method . So there should be experiment inspecting its influnece on the performance . 3.There is also no experiment on the influence of different memory update rule on the proposed method . It is not clear how well the proposed method is robust to different memory update rules . 4.Experiments on more diverse datasets should be done to prove its effectiveness , such as TinyImageNet , and MIT Scenes/Oxford Flowers/UCSD Birds/Stanford Cars .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate R1 for recognizing the contribution of our work and will add more experiments in the revision . In the following we address R1 \u2019 s main concerns one by one . \u201c Authors do not explain how to determine weights of the two terms in the proposed discriminative representation loss to achieve good balance between discrmination capability on the current task and generalization on future task. \u201d Regarding the two terms in DRL , a larger weight on L_wi is for less compact representations within classes , but a too dispersed representation space may include too much noise . For datasets that present more difficulty in learning compact representations , we would prefer a smaller weight on L_wi , we therefore set smaller \\alpha for CIFAR datasets in our experiments . A larger weight on L_bt is more resistant to forgetting but may be less capable of transferring to a new task , for datasets that are less compatible between tasks a smaller weight on L_bt would be preferred , as we set the largest \\lambda on Permuted MNIST and the smallest \\lambda on CIFAR-100 in our experiments . Table 6 in the appendix shows the hyperparamters used in all the experiments . \u201c No experiment on the influence of memory size and batch size on the proposed method . These are important hyperparameters for the proposed method . So , there should be experiment inspecting its influnece on the performance. \u201d The memory size is the larger the better for performance which is consistent in the literature of continual learning and a perfect memory would lead to a perfect performance as suggested in Knoblauch et al . ( 2020 ) .We provide experiment results of comparing different memory sizes in the Appendix E in the revision . Most of the compared baselines use a small batch size in their original papers ( such as 10 in A-GEM & GSS , 20 in ER ) , and we did not find large batch size can improve the performance , also the batch size highly relates to the computational complexity of most methods , so we keep it small in all experiments . Knoblauch , Jeremias , Hisham Husain , and Tom Diethe . `` Optimal Continual Learning has Perfect Memory and is NP-hard . '' ICML ( 2020 ) . \u201c There is also no experiment on the influence of different memory update rule on the proposed method . It is not clear how well the proposed method is robust to different memory update rules. \u201d The performance difference between DRL+ ER and DRL+BER is similar as the difference between ER and BER , DRL+ER is better than ER but worse than DRL+BER . We included results of comparing DRL with different replay strategies in Appendix F in the revision . \u201c Experiments on more diverse datasets should be done to prove its effectiveness , such as TinyImageNet , and MIT Scenes/Oxford Flowers/UCSD Birds/Stanford Cars. \u201d We provided experimental results on TinyImageNet in the revision , in which DRL outperforms other baselines ."}}