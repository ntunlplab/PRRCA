{"year": "2019", "forum": "H1lJws05K7", "title": "On the Selection of Initialization and Activation Function for Deep Neural Networks", "decision": "Reject", "meta_review": "The paper attempts to extend the recent analysis of random deep networks to alternative activation functions.  Unfortunately, none of the reviewers recommended the paper be accepted.  The current presentation suffers from a lack of clarity and a sufficiently convincing supporting argument/evidence to satisfy the reviewers.  The contribution is perceived as too incremental in light of previous work.", "reviews": [{"review_id": "H1lJws05K7-0", "review_text": "The authors prove some theoretical results under the mean field regime and support their conclusions with a small number of experiments. Their central argument is that a correlation curve that leads to sub-exponential correlation convergence (edge of chaos) can still lead to rapid convergence if the rate is e.g. quadratic. They show that this is the case for ReLU and argue that we must ensure not only sub-exponential convergence, but also have a correlation curve that is close to the identity everywhere. They suggest activation functions that attain conditions as laid out in propositions 4/5 as an alternative. The paper has many flaws: - the value of the theoretical results is unclear - the paper contains many statements that are either incorrect or overly sweeping - the experimental setup and results are questionnable Theoretical results: **Proposition 1: pretty trivial, not much value in itself **Proposition 2: Pretty obvious to the experienced reader, but nonetheless a valuable if narrow result. **Proposition 3: Interesting if narrow result. Unfortunately, it is not clear what the ultimate takeaway is. Is quadratic correlation convergence \"fast\"? Is it \"slow\"? Are you implying that we should find activation function where at EOC convergence is slower than quadratic? Do those activation functions exist? It would be good to compare this result against similar results for other activation functions. For example, do swish / SeLU etc. have a convergence rate that is less than quadratic? **Proposition 4: The conditions of proposition 4 are highly technical. It is not clear how one should go about verifying these conditions for an arbitrary activation function, let alone how one could generate new activation functions that satisfy these conditions. In fact, for an arbitrary nonlinearity, verifying the conditions of proposition 4 seems harder than verifying f(x) - x \\approx 0 directly. Hence, proposition 4 has little to no value. Further, it is not even clear whether f(x) - x \\approx 0 is actually desirable. For example, the activation function phi(x)=x achieves f(x) = x. But does that mean the identity is a good activation function for deep networks? Clearly not. **Proposition 5: The conditions of prop 5 are somewhat simpler than those of prop 4, but since we cannot eliminate the complicated condition (ii) from prop 4, it doesn't help much. **Proposition 6: True, but the fact that we have f(x) - x \\approx 0 for swish when q is small is kind of obvious. When q is small, \\phi_swish(x) \\approx 0.5x, and so swish is approximately linear and so its correlation curve is approximately the identity. We don't need to take a detour via propposition 4 to realize this. Presentation issues: - While I understand the point figures 1, 2 and 4b are trying to make, I don't understand what those figures actually depict. They are insufficiently labeled. For example, what does each axis represent? - You claim that for ReLU, EOC = {(0,\\sqrt{2})}. But this is not true. By definition 2, EOC is a subset of D_\\phi,var. But {(0,\\sqrt{2})} is not in D_\\phi,var, because it simply leaves all variances unchanged and does not cause them to converge to a single value. You acknowledge this by saying \"For this class of activation functions, we see (Proposition 2) that the variance is unchanged (qal = qa1) on the EOC, so that q does not formally exist in the sense that the limit of qal depends on a. However,this does not impact the analysis of the correlations.\" Section 2 is full of complicated definitions and technical results. If you expect the reader to plow through them all, then you should really stick to those definitions from then on. Declaring that it's fine to ignore your own definitions at the beginning of the very next section is bad presentation. This problem becomes even worse in section 3.2, where it is not clear which definition is actually used for EOC in your main result (prop 4), making prop 4 even harder to parse than it already is. Correctness issues: - \"In this chaotic regime, it has been observed in Schoenholz et al. (2017) that the correlations converge to some random value c < 1\" Actually, the correlation converges deterministically, so c is not random. - \"This means that very close inputs (in terms of correlation) lead to very different outputs. Therefore, in the chaotic phase, the output function of the neural network is non-continuous everywhere.\" Actually, the function computed by a plain tanh network is continuous everywhere. I think you mean something like \"the output can change drastically under small changes to the input\". But this concept is not the same as discontinuity, which has an established formal definition. - \"In unreported experiments, we observed that numerical convergence towards 1 for l \u2265 50 on the EOC.\" Covergence of a sequence is a property of the limit of the sequence, and not of the 50th element. This statement makes no sense. Also if you give a subjective interpretation of those experimental results, you should present the actual results first. - \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions.\" This statement is very vague and sweeping. Also, one could argue that the fact that ReLU is much more popular and tends to give better results than tanh in practice disproves the statement outright. - \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions. However, these functions suffer from the vanishing gradient problem during back-propagation\" At the edge of chaos, vanishing gradients are impossible! As Schoenholz showed, at the edge of chaos, \\chi_1=1, but \\chi_1 is also the rate of growth of the gradient. Pascanu et al (2013) discussed vanishing gradients in RNNs, which is a different story. - \"Other activation functions that have been shown to outperform empirically ReLU such as ELU (Clevert et al. (2016)), SELU (Klambauer et al. (2017)) and Softplus also satisfy the conditions of Proposition 4 (see Supplementary Material for ELU).\" Firstly, SeLU does not satisfy proposition 4. f(x) \\approx x requires \\phi to be close to a linear function in the range where the pre-activations occur. Since SeLU has a kink at 0, it cannot be close to a linear function no matter how small the pre-activations are. Secondly, softplus also doesn't satisfy proposition 4, as \\phi(0) = 0 does not hold. Thirdly, this statement is too sweeping. If ELU / SELU / Softplus \"outperform\" ReLU, why is ReLU still used in practice? At best, those nonlinearities have been shown to outperform in a few scenarios. - \"We proved in Section 3.2 that the Tanh activation guarantees better information propagation through the network when initialized on the EOC.\" Prop 4 only applies in the limit as \\sigma_b converges to 0. So you can't claim that you showed tanh as \"better information propagation\" in general. - \"However, for deeper networks (L \u2265 40), Tanh is stuck at a very low test accuracy, this is due to the fact that a lot of parameters remain essentially unchanged because the gradient is very small.\" But in figure 6b the accuracy for tanh is decreasing rapidly, so therefore the parameters are not remaining \"essentially unchanged\", as this would also cause the accuracy to remain essentially unchanged. Also, if the parameter changes are too small ... why not increase the learning rate? - \"To obtain much richer priors, our results indicate that we need to select not only parameters (\u03c3b , \u03c3w ) on the EOC but also an activation function satisfying Proposition 4.\" Prop 4 only applies when \\sigma_b is small, so you additionally need to make sure \\sigma_b small. - \"In the ordered phase, we know that the output converges exponentially to a fixed value (same value for all Xi), thus a small change in w and b will not change significantly the value of the loss function, therefore the gradient is approximately zero and the gradient descent algorithm will be stuck around the initial value.\" But you are using Adam, not gradient descent! Adam explicitly corrects for this kind of gradient vanishing, so a small gradient can't be the reason for the lack of training success. Experimental issues: - \"We use the Adam optimizer with learning rate lr = 0.001.\" You must tune the learning rate independently for each architecture for an ubiased comparison. - In figure 6b, why does tanh start with a high accuracy and end up with a low accuracy? I've never seen a training curve like this ... This suggests something is wrong with your setup. - You should run more experiments with a larger variety of activation functions. Minor comments: - \"Therefore, it is easy to see that for any (\u03c3b , \u03c3w ) such that F is increasing and admits at least one fixed point,wehaveK\u03c6,corr(\u03c3b,\u03c3w) \u2265 qwhereqistheminimalfixedpoint;i.e. q := min{x : F(x) = x}.\" I believe this statement is true, but I also think it requires more justification. - At the end of page 3, I think \\epsilon_r should be \\epsilon_q There are some good ideas here, but they need to be developed/refined/polished much further before publication. The above (non-exhaustive) list of issues will hopefully be helpful for this. ### Addendum ### After an in-depth discussion with the authors (see below), my opinion on the paper has not changed. All of my major criticisms remain: (1) There are far easier ways of achieving f(x) ~ x than propositions 4/5/7, i.e. we simply have to choose \\phi(x) approximately linear. (2) The experiments are too narrow, and learning rates are badly chosen. (3) The authors do not discuss the fact that as f(x) gets too close to x, performance actually degrades as \\phi(x) gets too close to a linear function. (Many other criticisms also remain.) The one criticism that the authors disputed until the end of the discussion is criticism (1). Their argument seems to hinge on the fact that their paper provides a path to construct activation function that avoid \"structural vanishing gradients\", which they claim 'tanh' suffers from. While they acknowledge that tanh does not necessarily suffer from \"regular\" vanishing gradients (as shown by \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" and \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks\"), they claim it suffers from structural vanishing gradients. I do not believe that there is such a thing as structural vanishing gradients. However, even if such a concept did exist, it falls on the the authors to provide a clear definition / explanation, which they neither do in the paper nor the rebuttal.", "rating": "3: Clear rejection", "reply_text": "- Reviewer \u2019 s comments on theoretical results : * * \u201c Proposition 1 : pretty trivial , not much value in itself \u201d We agree that the proposition is trivial and this is stated explicitly after the Proposition . However , we think that after defining the Domains of Convergence , it is logical to give some sufficient conditions for ( sigma_b , sigma_w ) to be in those domains of convergence . * * \u201c Proposition 2 : Pretty obvious to the experienced reader , but nonetheless a valuable if narrow result \u201d . This Proposition gives explicitly the edge of chaos for an important set of activation functions . This result is indeed useful since it provides the parameters ( sigma_b , sigma_w ) that one should use for ReLU , Leaky-RELU etc . * * \u201c Proposition 3 : Interesting if narrow result . Unfortunately , it is not clear what the ultimate takeaway is . Is quadratic correlation convergence `` fast '' ? Is it `` slow '' ? Are you implying that we should find activation function where at EOC convergence is slower than quadratic ? \u201d It is suggested that that \u2018 it is not clear what the ultimate takeaway is \u2019 . In the text just before Proposition 3 , we explain that this proposition establishes that the convergence rate of the correlation of a ReLU network is 1/l^2 instead of the exponential rate exp ( -b l ) . Hence compared to the rate outside the edge of chaos , this is a slow rate , which in practice means that the correlation ( the information ) propagates deeper inside the network and allows for use of many more layers . This proposition indeed only deals with RELU , which makes it seem narrow , RELU activation functions are however widely used in practice . It is expected that similar phenomena hold under RELU-like activation functions . We did not pursue this here since we propose other activation functions which we believe are better behaved \u2013 as explained by our theory ( prop 4 and 5 ) and the empirical evidence provided in Ramachandran et al ( 2017 ) . * * Proposition 4 : The reviewer suggests that \u201c The conditions of proposition 4 are highly technical . It is not clear how one should go about verifying these conditions for an arbitrary activation function \u201d . We agree that the conditions can be difficult to verify theoretically , but in many cases , they can be verified numerically ; see the Appendix for the ELU activation function . Proposition 5 provides additionally a simpler set of assumptions , which has been verified for important activation functions . * * \u201c \u2026for an arbitrary nonlinearity , verifying the conditions of Proposition 4 seems harder than verifying f ( x ) - x \\approx 0 directly . Hence , Proposition 4 has little to no value\u2026 \u201d We disagree with this comment . First , it is difficult to see how one could verify that f ( x ) - x \\approx 0 as sigma_b goes to zero without having any condition on the limit of q . Recall that f depends on q , and q may diverge , hence the use of condition ( iii ) in Prop4 . Moreover , condition ( iv ) is necessary for uniform convergence of the correlation function f to the identity function . We could have stated a weaker condition , but it be would more complicated to verify numerically . * * \u201c it is not even clear whether f ( x ) - x \\approx 0 is actually desirable . For example , the activation function phi ( x ) =x achieves f ( x ) = x . But does that mean the identity is a good activation function for deep networks ? Clearly not. \u201d : You are quite right the identity function , like any polynomial functions , is not a good activation function . We have explicitly mentioned in the introduction that for phi to be a ` good \u2019 activation function then it needs to be non-polynomial , phi should not suffer from the gradient vanishing problem and it should have a good information propagation , this last condition being the focus of our paper . Having f ( x ) - x \\approx 0 is a desirable property to obtain a good information propagation , since c^ { l+1 } = f ( c^ { l } ) . Therefore , having f close to the identity slows down the convergence of the correlation to 1 , which means the information propagates deeper inside the network . The proof of Prop4 is informative in this respect . * * \u201c Proposition 5 : The conditions of prop 5 are somewhat simpler than those of prop 4 , but since we can not eliminate the complicated condition ( ii ) from prop 4 , it does n't help much. \u201d : In Proposition 7 in the Appendix , we show how one replace condition ( ii ) of Prop4 by a simpler condition on phi . We will move this proposition to the main paper ."}, {"review_id": "H1lJws05K7-1", "review_text": "Studying properties of random networks in the infinite width limit, this work suggests guidance for choosing initialization and activation function. In my opinion, novel contribution comes for guidance for choosing activation functions and theoretical grounds for superior performance of \"`'swish\u2019 activation function. I have two main concerns : In terms of selection on initialization, the findings seem to be mostly discussed already in Schoenholz et al (2017) [1]. In their work, Edge of Chaos is critical line separating different phases and was already shown to have power-law decay rather than exponential decay. As far as I can tell, analysis on EOC on ReLU-like activations are different from Schoenholz et al (2017) [1]. Some of the results for ReLU are already appeared in the literature e.g. Lee et al (2018) [2]. Another main concern is in the author\u2019s experimental setup. It is hard to draw conclusions when comparison experiments were done with a fixed learning rate. As we know learning rate is one of the most critical hyperparameter for determining performance and optimal learning rate is often sensitive to architecture choice. Especially for different non-linearity and different depth/width optimal learning rate can change. Pros: - Clearly written and easy to understand what authors are trying to say - Interesting theoretical support for activation function which recently got attention due to boosting performance in neural networks - Nice suggestion of choosing activation function for deep networks (Proposition 4) -- ELU/SELU/Softplus/Swish all satisfy this suggestion Cons: - Novelty may be not strong enough as the standard analysis tool from [1] was mostly used - Experimental setup may suffer from some critical flaw Few comments/questions: - P3: Is M_{ReLU} = 2 correct, from ReLU EOC, shouldn\u2019t it be \u00bd? - For all the works using activation functions satisfying Proposition 4 (ELU/SELU/Softplus/Swish), the initialization scheme close to EOC? Does this work\u2019s analysis actually explain performance boost over ReLU for these activation functions? [1] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017. [2] J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. 6th International Conference on Learning Representations, 2018. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "- Major concerns : * \u201c In terms of selection on initialization , the findings seem to be mostly discussed already in Schoenholz et al ( 2017 ) \u201d . We respectfully disagree with the reviewer . There are several original results in our manuscript . Among others , our main result -Proposition 4- provides sufficient conditions to ensure deep information propagation . This provides some theoretical grounding explaining the excellent empirical performance of Swish observed in several recent papers . We are not aware of any similar result in the literature . * We agree with the reviewer that more experiments could be done with different learning rates , which we are working on right now . However , note that extensive experiments were already carried out in Ramachandran et al . ( 2017 ) with different datasets and different learning rates as indicated in our paper . We thus chose not to pursue more simulations in this paper and focused on theoretical properties enlightening the empirical findings of Ramachandran et al . ( 2017 ) .- Other concerns : Few comments/questions : - P3 : Is M_ { ReLU } = 2 correct , from ReLU EOC , shouldn \u2019 t it be \u00bd ? : Yes indeed , that was a typo and it has been fixed , thank you for pointing this to us . - `` For all the works using activation functions satisfying Proposition 4 ( ELU/SELU/Softplus/Swish ) , the initialization scheme close to EOC ? Does this work \u2019 s analysis actually explain performance boost over ReLU for these activation functions ? `` : We believe that the fact that these activation functions satisfy Prop4 partially explain the performance boost , since it leads to a better initialization scheme which is known to let the information propagate deeper through the network ."}, {"review_id": "H1lJws05K7-2", "review_text": "Good results; providing some insights on the selection of activation function. This paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.) The two principal results of this paper are 1. Initializing the network critically on the edge of chaos. 2. Identifying some conditions on the activation functions which allow good \"information flow\" through the network. The first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. The second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better. cons: 1. I don't think the experimental results are convincing enough for the reasons below: 1.1. All experiments are conducted over MNIST with testing accuracy around 96%. The authors should consider using large datasets (at least Cifar10). 1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine. 1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" 2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. 3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. 4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically. In sum, the paper has some interesting theoretical results but the empirical results are not convincing. Other comments: 1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. 2. Consider replacing \"Proposition 4\" by \"Theorem\", since it is the main result of the paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "The reviewer suggests that \u2018 However , I do n't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better. \u2019 In this paper , we only say that information propagation could partly explain why an activation function performs empirically better than another one . Of course , there are many other properties that make one activation better than another ( e.g.vanishing gradient ) . As far as empirical evidence is concerned we refer the reader to Ramachandran et al . ( 2017 ) .The present paper focuses on understanding/explaining/designing good initialization scheme for the algorithm , which is known to be crucial . 1 ) \u201c All experiments are conducted over MNIST with testing accuracy around 96 % . The authors should consider using large datasets ( at least Cifar10 ) . \u201d : We are currently running more experiments right now . Note that an extensive set of simulations has already been performed in Ramachandran et al . ( 2017 ) . \u201c Figure 6 ( b ) seems unconvincing . ReLU network should be trainable with depth > =200 \u201d . We do not claim that ReLU is not trainable in such scenarios . Figure 6-b only shows the first steps ( 40 epochs ) of the training , because the information propagation analysis is only valid at the initial step , we are comparing the final accuracies here . 2 ) We only say that having an activation function that avoids the vanishing/exploding gradient problem is always better than having an activation function that has gradient strictly less than 1 ( in absolute value ) in deep neural networks . If a Tanh-network can be trained for 1000 layers , it does not mean those networks are not trainable with Swish or Relu . 4 ) We are currently running more experiments with different learning rates . We note that extensive experiments were already done in in Ramachandran et al . ( 2017 ) with different datasets and different learning rates ."}], "0": {"review_id": "H1lJws05K7-0", "review_text": "The authors prove some theoretical results under the mean field regime and support their conclusions with a small number of experiments. Their central argument is that a correlation curve that leads to sub-exponential correlation convergence (edge of chaos) can still lead to rapid convergence if the rate is e.g. quadratic. They show that this is the case for ReLU and argue that we must ensure not only sub-exponential convergence, but also have a correlation curve that is close to the identity everywhere. They suggest activation functions that attain conditions as laid out in propositions 4/5 as an alternative. The paper has many flaws: - the value of the theoretical results is unclear - the paper contains many statements that are either incorrect or overly sweeping - the experimental setup and results are questionnable Theoretical results: **Proposition 1: pretty trivial, not much value in itself **Proposition 2: Pretty obvious to the experienced reader, but nonetheless a valuable if narrow result. **Proposition 3: Interesting if narrow result. Unfortunately, it is not clear what the ultimate takeaway is. Is quadratic correlation convergence \"fast\"? Is it \"slow\"? Are you implying that we should find activation function where at EOC convergence is slower than quadratic? Do those activation functions exist? It would be good to compare this result against similar results for other activation functions. For example, do swish / SeLU etc. have a convergence rate that is less than quadratic? **Proposition 4: The conditions of proposition 4 are highly technical. It is not clear how one should go about verifying these conditions for an arbitrary activation function, let alone how one could generate new activation functions that satisfy these conditions. In fact, for an arbitrary nonlinearity, verifying the conditions of proposition 4 seems harder than verifying f(x) - x \\approx 0 directly. Hence, proposition 4 has little to no value. Further, it is not even clear whether f(x) - x \\approx 0 is actually desirable. For example, the activation function phi(x)=x achieves f(x) = x. But does that mean the identity is a good activation function for deep networks? Clearly not. **Proposition 5: The conditions of prop 5 are somewhat simpler than those of prop 4, but since we cannot eliminate the complicated condition (ii) from prop 4, it doesn't help much. **Proposition 6: True, but the fact that we have f(x) - x \\approx 0 for swish when q is small is kind of obvious. When q is small, \\phi_swish(x) \\approx 0.5x, and so swish is approximately linear and so its correlation curve is approximately the identity. We don't need to take a detour via propposition 4 to realize this. Presentation issues: - While I understand the point figures 1, 2 and 4b are trying to make, I don't understand what those figures actually depict. They are insufficiently labeled. For example, what does each axis represent? - You claim that for ReLU, EOC = {(0,\\sqrt{2})}. But this is not true. By definition 2, EOC is a subset of D_\\phi,var. But {(0,\\sqrt{2})} is not in D_\\phi,var, because it simply leaves all variances unchanged and does not cause them to converge to a single value. You acknowledge this by saying \"For this class of activation functions, we see (Proposition 2) that the variance is unchanged (qal = qa1) on the EOC, so that q does not formally exist in the sense that the limit of qal depends on a. However,this does not impact the analysis of the correlations.\" Section 2 is full of complicated definitions and technical results. If you expect the reader to plow through them all, then you should really stick to those definitions from then on. Declaring that it's fine to ignore your own definitions at the beginning of the very next section is bad presentation. This problem becomes even worse in section 3.2, where it is not clear which definition is actually used for EOC in your main result (prop 4), making prop 4 even harder to parse than it already is. Correctness issues: - \"In this chaotic regime, it has been observed in Schoenholz et al. (2017) that the correlations converge to some random value c < 1\" Actually, the correlation converges deterministically, so c is not random. - \"This means that very close inputs (in terms of correlation) lead to very different outputs. Therefore, in the chaotic phase, the output function of the neural network is non-continuous everywhere.\" Actually, the function computed by a plain tanh network is continuous everywhere. I think you mean something like \"the output can change drastically under small changes to the input\". But this concept is not the same as discontinuity, which has an established formal definition. - \"In unreported experiments, we observed that numerical convergence towards 1 for l \u2265 50 on the EOC.\" Covergence of a sequence is a property of the limit of the sequence, and not of the 50th element. This statement makes no sense. Also if you give a subjective interpretation of those experimental results, you should present the actual results first. - \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions.\" This statement is very vague and sweeping. Also, one could argue that the fact that ReLU is much more popular and tends to give better results than tanh in practice disproves the statement outright. - \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions. However, these functions suffer from the vanishing gradient problem during back-propagation\" At the edge of chaos, vanishing gradients are impossible! As Schoenholz showed, at the edge of chaos, \\chi_1=1, but \\chi_1 is also the rate of growth of the gradient. Pascanu et al (2013) discussed vanishing gradients in RNNs, which is a different story. - \"Other activation functions that have been shown to outperform empirically ReLU such as ELU (Clevert et al. (2016)), SELU (Klambauer et al. (2017)) and Softplus also satisfy the conditions of Proposition 4 (see Supplementary Material for ELU).\" Firstly, SeLU does not satisfy proposition 4. f(x) \\approx x requires \\phi to be close to a linear function in the range where the pre-activations occur. Since SeLU has a kink at 0, it cannot be close to a linear function no matter how small the pre-activations are. Secondly, softplus also doesn't satisfy proposition 4, as \\phi(0) = 0 does not hold. Thirdly, this statement is too sweeping. If ELU / SELU / Softplus \"outperform\" ReLU, why is ReLU still used in practice? At best, those nonlinearities have been shown to outperform in a few scenarios. - \"We proved in Section 3.2 that the Tanh activation guarantees better information propagation through the network when initialized on the EOC.\" Prop 4 only applies in the limit as \\sigma_b converges to 0. So you can't claim that you showed tanh as \"better information propagation\" in general. - \"However, for deeper networks (L \u2265 40), Tanh is stuck at a very low test accuracy, this is due to the fact that a lot of parameters remain essentially unchanged because the gradient is very small.\" But in figure 6b the accuracy for tanh is decreasing rapidly, so therefore the parameters are not remaining \"essentially unchanged\", as this would also cause the accuracy to remain essentially unchanged. Also, if the parameter changes are too small ... why not increase the learning rate? - \"To obtain much richer priors, our results indicate that we need to select not only parameters (\u03c3b , \u03c3w ) on the EOC but also an activation function satisfying Proposition 4.\" Prop 4 only applies when \\sigma_b is small, so you additionally need to make sure \\sigma_b small. - \"In the ordered phase, we know that the output converges exponentially to a fixed value (same value for all Xi), thus a small change in w and b will not change significantly the value of the loss function, therefore the gradient is approximately zero and the gradient descent algorithm will be stuck around the initial value.\" But you are using Adam, not gradient descent! Adam explicitly corrects for this kind of gradient vanishing, so a small gradient can't be the reason for the lack of training success. Experimental issues: - \"We use the Adam optimizer with learning rate lr = 0.001.\" You must tune the learning rate independently for each architecture for an ubiased comparison. - In figure 6b, why does tanh start with a high accuracy and end up with a low accuracy? I've never seen a training curve like this ... This suggests something is wrong with your setup. - You should run more experiments with a larger variety of activation functions. Minor comments: - \"Therefore, it is easy to see that for any (\u03c3b , \u03c3w ) such that F is increasing and admits at least one fixed point,wehaveK\u03c6,corr(\u03c3b,\u03c3w) \u2265 qwhereqistheminimalfixedpoint;i.e. q := min{x : F(x) = x}.\" I believe this statement is true, but I also think it requires more justification. - At the end of page 3, I think \\epsilon_r should be \\epsilon_q There are some good ideas here, but they need to be developed/refined/polished much further before publication. The above (non-exhaustive) list of issues will hopefully be helpful for this. ### Addendum ### After an in-depth discussion with the authors (see below), my opinion on the paper has not changed. All of my major criticisms remain: (1) There are far easier ways of achieving f(x) ~ x than propositions 4/5/7, i.e. we simply have to choose \\phi(x) approximately linear. (2) The experiments are too narrow, and learning rates are badly chosen. (3) The authors do not discuss the fact that as f(x) gets too close to x, performance actually degrades as \\phi(x) gets too close to a linear function. (Many other criticisms also remain.) The one criticism that the authors disputed until the end of the discussion is criticism (1). Their argument seems to hinge on the fact that their paper provides a path to construct activation function that avoid \"structural vanishing gradients\", which they claim 'tanh' suffers from. While they acknowledge that tanh does not necessarily suffer from \"regular\" vanishing gradients (as shown by \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" and \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks\"), they claim it suffers from structural vanishing gradients. I do not believe that there is such a thing as structural vanishing gradients. However, even if such a concept did exist, it falls on the the authors to provide a clear definition / explanation, which they neither do in the paper nor the rebuttal.", "rating": "3: Clear rejection", "reply_text": "- Reviewer \u2019 s comments on theoretical results : * * \u201c Proposition 1 : pretty trivial , not much value in itself \u201d We agree that the proposition is trivial and this is stated explicitly after the Proposition . However , we think that after defining the Domains of Convergence , it is logical to give some sufficient conditions for ( sigma_b , sigma_w ) to be in those domains of convergence . * * \u201c Proposition 2 : Pretty obvious to the experienced reader , but nonetheless a valuable if narrow result \u201d . This Proposition gives explicitly the edge of chaos for an important set of activation functions . This result is indeed useful since it provides the parameters ( sigma_b , sigma_w ) that one should use for ReLU , Leaky-RELU etc . * * \u201c Proposition 3 : Interesting if narrow result . Unfortunately , it is not clear what the ultimate takeaway is . Is quadratic correlation convergence `` fast '' ? Is it `` slow '' ? Are you implying that we should find activation function where at EOC convergence is slower than quadratic ? \u201d It is suggested that that \u2018 it is not clear what the ultimate takeaway is \u2019 . In the text just before Proposition 3 , we explain that this proposition establishes that the convergence rate of the correlation of a ReLU network is 1/l^2 instead of the exponential rate exp ( -b l ) . Hence compared to the rate outside the edge of chaos , this is a slow rate , which in practice means that the correlation ( the information ) propagates deeper inside the network and allows for use of many more layers . This proposition indeed only deals with RELU , which makes it seem narrow , RELU activation functions are however widely used in practice . It is expected that similar phenomena hold under RELU-like activation functions . We did not pursue this here since we propose other activation functions which we believe are better behaved \u2013 as explained by our theory ( prop 4 and 5 ) and the empirical evidence provided in Ramachandran et al ( 2017 ) . * * Proposition 4 : The reviewer suggests that \u201c The conditions of proposition 4 are highly technical . It is not clear how one should go about verifying these conditions for an arbitrary activation function \u201d . We agree that the conditions can be difficult to verify theoretically , but in many cases , they can be verified numerically ; see the Appendix for the ELU activation function . Proposition 5 provides additionally a simpler set of assumptions , which has been verified for important activation functions . * * \u201c \u2026for an arbitrary nonlinearity , verifying the conditions of Proposition 4 seems harder than verifying f ( x ) - x \\approx 0 directly . Hence , Proposition 4 has little to no value\u2026 \u201d We disagree with this comment . First , it is difficult to see how one could verify that f ( x ) - x \\approx 0 as sigma_b goes to zero without having any condition on the limit of q . Recall that f depends on q , and q may diverge , hence the use of condition ( iii ) in Prop4 . Moreover , condition ( iv ) is necessary for uniform convergence of the correlation function f to the identity function . We could have stated a weaker condition , but it be would more complicated to verify numerically . * * \u201c it is not even clear whether f ( x ) - x \\approx 0 is actually desirable . For example , the activation function phi ( x ) =x achieves f ( x ) = x . But does that mean the identity is a good activation function for deep networks ? Clearly not. \u201d : You are quite right the identity function , like any polynomial functions , is not a good activation function . We have explicitly mentioned in the introduction that for phi to be a ` good \u2019 activation function then it needs to be non-polynomial , phi should not suffer from the gradient vanishing problem and it should have a good information propagation , this last condition being the focus of our paper . Having f ( x ) - x \\approx 0 is a desirable property to obtain a good information propagation , since c^ { l+1 } = f ( c^ { l } ) . Therefore , having f close to the identity slows down the convergence of the correlation to 1 , which means the information propagates deeper inside the network . The proof of Prop4 is informative in this respect . * * \u201c Proposition 5 : The conditions of prop 5 are somewhat simpler than those of prop 4 , but since we can not eliminate the complicated condition ( ii ) from prop 4 , it does n't help much. \u201d : In Proposition 7 in the Appendix , we show how one replace condition ( ii ) of Prop4 by a simpler condition on phi . We will move this proposition to the main paper ."}, "1": {"review_id": "H1lJws05K7-1", "review_text": "Studying properties of random networks in the infinite width limit, this work suggests guidance for choosing initialization and activation function. In my opinion, novel contribution comes for guidance for choosing activation functions and theoretical grounds for superior performance of \"`'swish\u2019 activation function. I have two main concerns : In terms of selection on initialization, the findings seem to be mostly discussed already in Schoenholz et al (2017) [1]. In their work, Edge of Chaos is critical line separating different phases and was already shown to have power-law decay rather than exponential decay. As far as I can tell, analysis on EOC on ReLU-like activations are different from Schoenholz et al (2017) [1]. Some of the results for ReLU are already appeared in the literature e.g. Lee et al (2018) [2]. Another main concern is in the author\u2019s experimental setup. It is hard to draw conclusions when comparison experiments were done with a fixed learning rate. As we know learning rate is one of the most critical hyperparameter for determining performance and optimal learning rate is often sensitive to architecture choice. Especially for different non-linearity and different depth/width optimal learning rate can change. Pros: - Clearly written and easy to understand what authors are trying to say - Interesting theoretical support for activation function which recently got attention due to boosting performance in neural networks - Nice suggestion of choosing activation function for deep networks (Proposition 4) -- ELU/SELU/Softplus/Swish all satisfy this suggestion Cons: - Novelty may be not strong enough as the standard analysis tool from [1] was mostly used - Experimental setup may suffer from some critical flaw Few comments/questions: - P3: Is M_{ReLU} = 2 correct, from ReLU EOC, shouldn\u2019t it be \u00bd? - For all the works using activation functions satisfying Proposition 4 (ELU/SELU/Softplus/Swish), the initialization scheme close to EOC? Does this work\u2019s analysis actually explain performance boost over ReLU for these activation functions? [1] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017. [2] J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. 6th International Conference on Learning Representations, 2018. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "- Major concerns : * \u201c In terms of selection on initialization , the findings seem to be mostly discussed already in Schoenholz et al ( 2017 ) \u201d . We respectfully disagree with the reviewer . There are several original results in our manuscript . Among others , our main result -Proposition 4- provides sufficient conditions to ensure deep information propagation . This provides some theoretical grounding explaining the excellent empirical performance of Swish observed in several recent papers . We are not aware of any similar result in the literature . * We agree with the reviewer that more experiments could be done with different learning rates , which we are working on right now . However , note that extensive experiments were already carried out in Ramachandran et al . ( 2017 ) with different datasets and different learning rates as indicated in our paper . We thus chose not to pursue more simulations in this paper and focused on theoretical properties enlightening the empirical findings of Ramachandran et al . ( 2017 ) .- Other concerns : Few comments/questions : - P3 : Is M_ { ReLU } = 2 correct , from ReLU EOC , shouldn \u2019 t it be \u00bd ? : Yes indeed , that was a typo and it has been fixed , thank you for pointing this to us . - `` For all the works using activation functions satisfying Proposition 4 ( ELU/SELU/Softplus/Swish ) , the initialization scheme close to EOC ? Does this work \u2019 s analysis actually explain performance boost over ReLU for these activation functions ? `` : We believe that the fact that these activation functions satisfy Prop4 partially explain the performance boost , since it leads to a better initialization scheme which is known to let the information propagate deeper through the network ."}, "2": {"review_id": "H1lJws05K7-2", "review_text": "Good results; providing some insights on the selection of activation function. This paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.) The two principal results of this paper are 1. Initializing the network critically on the edge of chaos. 2. Identifying some conditions on the activation functions which allow good \"information flow\" through the network. The first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. The second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better. cons: 1. I don't think the experimental results are convincing enough for the reasons below: 1.1. All experiments are conducted over MNIST with testing accuracy around 96%. The authors should consider using large datasets (at least Cifar10). 1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine. 1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" 2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. 3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. 4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically. In sum, the paper has some interesting theoretical results but the empirical results are not convincing. Other comments: 1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. 2. Consider replacing \"Proposition 4\" by \"Theorem\", since it is the main result of the paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "The reviewer suggests that \u2018 However , I do n't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better. \u2019 In this paper , we only say that information propagation could partly explain why an activation function performs empirically better than another one . Of course , there are many other properties that make one activation better than another ( e.g.vanishing gradient ) . As far as empirical evidence is concerned we refer the reader to Ramachandran et al . ( 2017 ) .The present paper focuses on understanding/explaining/designing good initialization scheme for the algorithm , which is known to be crucial . 1 ) \u201c All experiments are conducted over MNIST with testing accuracy around 96 % . The authors should consider using large datasets ( at least Cifar10 ) . \u201d : We are currently running more experiments right now . Note that an extensive set of simulations has already been performed in Ramachandran et al . ( 2017 ) . \u201c Figure 6 ( b ) seems unconvincing . ReLU network should be trainable with depth > =200 \u201d . We do not claim that ReLU is not trainable in such scenarios . Figure 6-b only shows the first steps ( 40 epochs ) of the training , because the information propagation analysis is only valid at the initial step , we are comparing the final accuracies here . 2 ) We only say that having an activation function that avoids the vanishing/exploding gradient problem is always better than having an activation function that has gradient strictly less than 1 ( in absolute value ) in deep neural networks . If a Tanh-network can be trained for 1000 layers , it does not mean those networks are not trainable with Swish or Relu . 4 ) We are currently running more experiments with different learning rates . We note that extensive experiments were already done in in Ramachandran et al . ( 2017 ) with different datasets and different learning rates ."}}