{"year": "2019", "forum": "HylsgnCcFQ", "title": "Dynamic Graph Representation Learning via Self-Attention Networks", "decision": "Reject", "meta_review": "This paper proposes a self-attention based approach for learning representations for the vertices of a dynamic graph, where the topology of the edges may change. The attention focuses on representing the interaction of vertices that have connections. Experimental results for the link prediction task on multiple datasets demonstrate the benefits of the approach. The idea of attention or its computation is not novel, however its application for estimating embeddings for dynamic graph vertices is new.\nThe original version of the paper did not have strong baselines as noted by multiple reviewers, but the paper was  revised during the review period. However, some of these suggestions, for example, experiments with larger graph sizes and other related work i.e., similar work on static graphs are left as a future work.", "reviews": [{"review_id": "HylsgnCcFQ-0", "review_text": "This paper describes learning representation for dynamic graphs using structural and temporal self-attention layers. They applied their method for the task of link-prediction. However, I have serious objections to their experimental setup. I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph, however, working with a real-world dynamic graph, you can compute the difference between G_t and G_{t+1} as the changes that occur in G_t+1 1) Why are you not trying to predict these changes? Moreover, 2) why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph? 3) The selected graphs are very small comparing to the dynamic graphs available here http://konect.uni-koblenz.de/networks/. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank you for your review with thoughtful questions on the experiments ! Please refer to our global comment above for a list of all revisions to the paper . We respond to each of your comments below as follows : Q1 : I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph , however , working with a real-world dynamic graph , you can compute the difference between G_t and G_ { t+1 } as the changes that occur in G_ { t+1 } 1 ) Why are you not trying to predict these changes ? We agree with the observation that the differences between graphs G_t and G_ { t+1 } can be computed in real-world dynamic graphs , and we have included additional experiments on new link prediction in Appendix B . First , we would like to clarify our view of dynamic link prediction based on our understanding of existing literature . The goal of dynamic link prediction is to predict the set of future links ( or interactions ) based on historically observed graph snapshots . In practice , this can be realized as predicting future user interactions in email communication networks or user-item ratings in recommender systems . In such scenarios , dynamic link prediction aims to predict the set of \u201c all \u201d future links ( at time step t+1 ) given history until time step t. To the best of our knowledge , this evaluation approach has been widely adopted in our surveyed literature on dynamic link prediction in Section 2 ( Related Work ) of the paper . Our compared dynamic graph embedding baselines DynamicTriad , DynGEM , and Know-Evolve also adopt the same convention , by evaluating the predicted links at ( t+1 ) through classification and ranking metrics . On the other hand , we do agree with your perspective that a dynamic graph representation should be evaluated in its ability to predict \u201c new \u201d links . We have added an additional experiment ( Appendix B ) where evaluation examples comprise \u201c new \u201d links at G_ { t+1 } ( which have not been observed in G_t ) , and an equal number of randomly sampled pairs of unconnected nodes ( non-links ) . We use a similar evaluation methodology to evaluate the performance of dynamic link prediction through AUC scores . This experiment specifically evaluates the ability of different methods to predict new links at ( t+1 ) . Though the overall prediction accuracies are generally lower in comparison to the previous experimental setting , we observe consistent gains of 3-5 % over the best baseline similar to our earlier results . The new results can be found in Table 4 of Appendix B , along with accompanying discussion . We hope that the addition of this experiment further showcases the capability of DySAT for dynamic link prediction . Q2 : Why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph ? Firstly , the training step for all models only utilizes the snapshots up to t to compute the embeddings for all nodes , which can subsequently be used in different downstream tasks such as link prediction , classification , clustering , etc . No data from snapshot t+1 are utilized in training the node embedding model . Since we focus on dynamic link prediction as the primary task for evaluation , the goal to predict future links ( at time step t+1 ) given history until time step t. Thus , the evaluation set consists of examples from snapshot t+1 . Secondly , the examples from time snapshot t+1 are * only * used to train a downstream logistic regression classifier for evaluating link prediction performance . Since the evaluation set comprises the links at t+1 , we choose a small percentage of those examples ( 20 % ) for training , which is consistent with standard evaluation procedures . We follow the same setup for all the compared methods . In case of a different task such as multi-step forecasting to predict links at t+n , we similarly use 20 % of examples at t+n for training the downstream classifier . We have revised the draft to make the experiment setup clearer . Meanwhile , we also describe the reason for using a downstream classifier to evaluate link prediction . Arguably , link prediction can also be evaluated by applying a sigmoid function on the inner product computed on pairs of node embeddings at time step t. However , we instead choose to train a downstream classifier ( as done in node2vec , DynamicTriad etc . ) to provide a fair comparison against baselines ( such as DynamicTriad ) , which use other distance metrics ( L_1 distance , etc . ) for link prediction . We believe this evaluation methodology provides a more flexible framework to fairly evaluate various methods which are trained using different distance/proximity metrics ."}, {"review_id": "HylsgnCcFQ-1", "review_text": "This paper proposes a model for learning node embedding vectors of dynamic graphs, whose edge topology may change. The proposed model, called Dynamic Self-Attention Network (DySAT), uses attention mechanism to represent the interaction of spatial neighbouring nodes, which is closely related to the Graph Attention Network. For the temporal dependency between successive graphs, DySAT also uses attention structure inspired by previous work in machine translation. Experiments on 4 datasets show that DySAT can improve the AUC of link prediction by significant margins, compared to static graph methods and other dynamic graph methods. Though the attention structures in this paper are not original, combining these structures and applying them on dynamic graph embedding is new. Here are some questions: 1. What will happen if a never-seen node appears at t+1? The model design seems to be compatible with this case. The structural attention will still work, however, the temporal attention degenerates to a \u201cstatic\u201d result --- all the attention focus on the representation at t+1. I am curious about the model performance in this situation, since nodes may arise and vanish in real applications. 2. What is the performance of the proposed algorithm for multi-step forecasting? In the experiments, graph at t+1 is evaluated using the model trained up to graph_t. However, in real applications we may don\u2019t have enough time to retrain the model at every time step. If we use the model trained up to graph_t to compute node embedding for the graph_{t+n}, what is the advantage of DySAT over static methods? 3. What is the running time for a single training process? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank you for the insightful questions on our experiments ! Please refer to our global comment above for a list of all revisions to the paper . We respond to each of your comments below as follows : Q1 : What will happen if a never-seen node appears at t+1 ? The model design seems to be compatible with this case . The structural attention will still work , however , the temporal attention degenerates to a \u201c static \u201d result - all the attention focus on the representation at t+1 . I am curious about the model performance in this situation , since nodes may arise and vanish in real applications . We agree with the apt observation on the capability of DySAT to handle new nodes , and have conducted additional experiments to examine model performance in such situations . To compute the representation of a new node v at time step t+1 , the only available information is the local structure around v at t+1 . Although temporal attention will focus on the latest representation due to absence of history , it however does not degenerate to a \u201c static \u201d result . The temporal attention applied on the neighboring nodes of v ( say N_v ) , indirectly contribute towards the embedding of v , through backpropagation updates . Specifically , the structural embedding of v is computed as a function of N_v , whose structural embeddings receive backpropagation signals through the temporal attention layers ( assuming they are not all new nodes ) . Thus , temporal attention indirectly affects the final embedding of node v. As suggested , we empirically examine the performance of DySAT on \u201c new \u201d previously unseen nodes . We report link prediction performance * only * on the new nodes at each time step using the same experimental setup , i.e. , a test example ( node pair ) is reported for evaluation only if it contains at least one new node . Due to the significant variance in the number of new nodes at each step , we report the performance ( AUC ) at each time step , along with a mention on the corresponding number of new nodes . The results are available in Figure 5 of the Appendix D. From Figure 5 , we observe consistent gains of DySAT over other baselines , similar to our main results . Q2 : What is the performance of the proposed algorithm for multi-step forecasting ? In the experiments , graph at t+1 is evaluated using the model trained up to graph_t . However , in real applications we may don \u2019 t have enough time to retrain the model at every time step . If we use the model trained up to graph_t to compute node embedding for the graph_ { t+n } , what is the advantage of DySAT over static methods ? We agree with your view on the importance of not re-training the model at every time step in real-world applications . Multi-step forecasting is typically achieved either by ( a ) designing a model to predict multiple steps into the future , or by ( b ) recursively feeding next predictions as input , for a desired number of future steps . In case of dynamic graphs , events correspond to link occurrences , which renders forecasting different from conventional time-series , due to the occurrence of new nodes in each time step . Due to this key distinction , we list below , two possibilities for forecasting in dynamic graphs : ( a ) Link prediction at future step t+n ( on all nodes ) by incrementally updating the model on new snapshots . ( b ) Link prediction at future step t+n ( among nodes present at t ) based on dynamic embeddings learned at t , followed by a downstream classifier to predict the links at t+n . Note that ( b ) does not involve model re-training or updating while ( a ) requires incremental model updates . In our paper , we have examined ( a ) by proposing an incremental variant named IncSAT and report the performance in Table 5 of Appendix E. We have now added an additional experiment in Appendix C to evaluate forecasting using strategy ( b ) , which enables direct evaluation of DySAT on multi-step link prediction . Here , each method is trained for a fixed number of time steps , and the latest embeddings are used to predict links at multiple future steps . In each dataset , we choose the last 6 snapshots to evaluate multi-step link prediction where we create examples from the links in G_ { t+n } and an equal number of randomly sampled pairs of unconnected nodes ( non-links ) . Our experimental results ( Figure 4 ) indicate significant improvements for DySAT over all baselines and a highly stable link prediction performance over future time steps . Q3 : What is the running time for a single training process ? We have revised Section 5.4 to add the running time information . Specifically , we report the runtime of DySAT on a machine with Nvidia Tesla V100 GPU and 28 CPU cores . The runtime per mini-batch of DySAT with batch size of 256 nodes on the ML-10M dataset , is 0.72 seconds . In comparison , the model variant without the temporal attention ( No Temporal ) takes 0.51 seconds . Thus , structural attention constitutes a major fraction of the overall runtime , while the cost of temporal attention is relatively lower ."}, {"review_id": "HylsgnCcFQ-2", "review_text": "This is a well-written paper studying the important problem of dynamic network embedding. Please find below some pros and cons of this paper. Pros: * Studies the important problem of network embedding under a more realistic setting (i.e., nodes & edges evolve over time). * Introduces an interesting architecture that uses two forms of attention: structural and temporal. * Demonstrated the effectiveness of the temporal layers through additional experiments (in appendix) and also introduced a variant of their proposed approach which can be trained incrementally using only the last snapshot. Cons: * The authors compared against several dynamic & static graph embedding approaches. If we disregard the proposed approach (DySAT), the static methods seem to match and even, in some cases, beat the dynamic approaches on the compared temporal graph datasets. The authors should compare against stronger baselines for static node embedding, particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful. Please see [1] for an easy way to train GCN/GAT for link prediction. * There are actually quite a number of work done on network embedding on dynamic graphs including [2-4]. In particular, [2-3] support node attributes as well as the addition/deletion of nodes & edges. The author should also compare against these work. * The concept of temporal attention is quite interesting. However, the authors do not provide more analysis on this. For one, I am interested to see how the temporal attention weights are distributed. Are they focused on the more recent snapshots? If so, can we simply retain the more relevant recent information and train a static network embedding approach? Or are the attention weights distributed differently? [1] Modeling Polypharmacy Side Effects with Graph Convolutional Networks. Zitnik et. al. BioInformatics 2018. [2] Attributed Network Embedding for Learning in a Dynamic Environment. Li et. al. In Proc. CIKM '17. [3] Streaming Link Prediction on Dynamic Attributed Networks. Li et. al. In Proc. WSDM '18. [4] Continuous-Time Dynamic Network Embeddings. Nguyen et. al. In Comp. Proc. WWW '18. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank you for the in-depth questions on our experimental results ! Please refer to our global comment above for a list of all revisions to the paper -- we hope they have appropriately addressed your comments . We respond to each of your comments below as follows : Q1 : The authors compared against several dynamic & static graph embedding approaches . If we disregard the proposed approach ( DySAT ) , the static methods seem to match and even , in some cases , beat the dynamic approaches on the compared temporal graph datasets . The authors should compare against stronger baselines for static node embedding , particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful . Please see [ 1 ] for an easy way to train GCN/GAT for link prediction . We agree with you on the observation that static methods often match or beat existing dynamic embedding methods . Our initial experiments contain comparison to GraphSAGE - an unsupervised representation learning framework that supports various neighborhood aggregation functions , including GCN and GAT aggregators . We thank you for the valuable pointer [ 1 ] which trains GCN/GAT as a graph autoencoder directly for link prediction . While conceptually similar to GCN/GAT variants of GraphSAGE , two key differences include ( a ) lack of neighborhood sampling , and ( b ) link prediction objective instead of random walk samples . To examine the effect of these differences on link prediction performance , we used the aforementioned implementation [ 1 ] to train autoencoder models of GCN and GAT , denoted by GCN-AE and GAT-AE in our experiments . Our experimental results have been updated to include these as static embedding methods for comparison . From the results , we find the performance of these methods to be mostly similar to their corresponding GraphSAGE variants , which is consistent with our expectation ."}], "0": {"review_id": "HylsgnCcFQ-0", "review_text": "This paper describes learning representation for dynamic graphs using structural and temporal self-attention layers. They applied their method for the task of link-prediction. However, I have serious objections to their experimental setup. I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph, however, working with a real-world dynamic graph, you can compute the difference between G_t and G_{t+1} as the changes that occur in G_t+1 1) Why are you not trying to predict these changes? Moreover, 2) why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph? 3) The selected graphs are very small comparing to the dynamic graphs available here http://konect.uni-koblenz.de/networks/. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank you for your review with thoughtful questions on the experiments ! Please refer to our global comment above for a list of all revisions to the paper . We respond to each of your comments below as follows : Q1 : I have seen people used sets of edges and pairs of vertices without an edge for creating examples for link-prediction on a static graph , however , working with a real-world dynamic graph , you can compute the difference between G_t and G_ { t+1 } as the changes that occur in G_ { t+1 } 1 ) Why are you not trying to predict these changes ? We agree with the observation that the differences between graphs G_t and G_ { t+1 } can be computed in real-world dynamic graphs , and we have included additional experiments on new link prediction in Appendix B . First , we would like to clarify our view of dynamic link prediction based on our understanding of existing literature . The goal of dynamic link prediction is to predict the set of future links ( or interactions ) based on historically observed graph snapshots . In practice , this can be realized as predicting future user interactions in email communication networks or user-item ratings in recommender systems . In such scenarios , dynamic link prediction aims to predict the set of \u201c all \u201d future links ( at time step t+1 ) given history until time step t. To the best of our knowledge , this evaluation approach has been widely adopted in our surveyed literature on dynamic link prediction in Section 2 ( Related Work ) of the paper . Our compared dynamic graph embedding baselines DynamicTriad , DynGEM , and Know-Evolve also adopt the same convention , by evaluating the predicted links at ( t+1 ) through classification and ranking metrics . On the other hand , we do agree with your perspective that a dynamic graph representation should be evaluated in its ability to predict \u201c new \u201d links . We have added an additional experiment ( Appendix B ) where evaluation examples comprise \u201c new \u201d links at G_ { t+1 } ( which have not been observed in G_t ) , and an equal number of randomly sampled pairs of unconnected nodes ( non-links ) . We use a similar evaluation methodology to evaluate the performance of dynamic link prediction through AUC scores . This experiment specifically evaluates the ability of different methods to predict new links at ( t+1 ) . Though the overall prediction accuracies are generally lower in comparison to the previous experimental setting , we observe consistent gains of 3-5 % over the best baseline similar to our earlier results . The new results can be found in Table 4 of Appendix B , along with accompanying discussion . We hope that the addition of this experiment further showcases the capability of DySAT for dynamic link prediction . Q2 : Why do you need examples from snapshot t+1 for training when you have already observed t snapshots of the graph ? Firstly , the training step for all models only utilizes the snapshots up to t to compute the embeddings for all nodes , which can subsequently be used in different downstream tasks such as link prediction , classification , clustering , etc . No data from snapshot t+1 are utilized in training the node embedding model . Since we focus on dynamic link prediction as the primary task for evaluation , the goal to predict future links ( at time step t+1 ) given history until time step t. Thus , the evaluation set consists of examples from snapshot t+1 . Secondly , the examples from time snapshot t+1 are * only * used to train a downstream logistic regression classifier for evaluating link prediction performance . Since the evaluation set comprises the links at t+1 , we choose a small percentage of those examples ( 20 % ) for training , which is consistent with standard evaluation procedures . We follow the same setup for all the compared methods . In case of a different task such as multi-step forecasting to predict links at t+n , we similarly use 20 % of examples at t+n for training the downstream classifier . We have revised the draft to make the experiment setup clearer . Meanwhile , we also describe the reason for using a downstream classifier to evaluate link prediction . Arguably , link prediction can also be evaluated by applying a sigmoid function on the inner product computed on pairs of node embeddings at time step t. However , we instead choose to train a downstream classifier ( as done in node2vec , DynamicTriad etc . ) to provide a fair comparison against baselines ( such as DynamicTriad ) , which use other distance metrics ( L_1 distance , etc . ) for link prediction . We believe this evaluation methodology provides a more flexible framework to fairly evaluate various methods which are trained using different distance/proximity metrics ."}, "1": {"review_id": "HylsgnCcFQ-1", "review_text": "This paper proposes a model for learning node embedding vectors of dynamic graphs, whose edge topology may change. The proposed model, called Dynamic Self-Attention Network (DySAT), uses attention mechanism to represent the interaction of spatial neighbouring nodes, which is closely related to the Graph Attention Network. For the temporal dependency between successive graphs, DySAT also uses attention structure inspired by previous work in machine translation. Experiments on 4 datasets show that DySAT can improve the AUC of link prediction by significant margins, compared to static graph methods and other dynamic graph methods. Though the attention structures in this paper are not original, combining these structures and applying them on dynamic graph embedding is new. Here are some questions: 1. What will happen if a never-seen node appears at t+1? The model design seems to be compatible with this case. The structural attention will still work, however, the temporal attention degenerates to a \u201cstatic\u201d result --- all the attention focus on the representation at t+1. I am curious about the model performance in this situation, since nodes may arise and vanish in real applications. 2. What is the performance of the proposed algorithm for multi-step forecasting? In the experiments, graph at t+1 is evaluated using the model trained up to graph_t. However, in real applications we may don\u2019t have enough time to retrain the model at every time step. If we use the model trained up to graph_t to compute node embedding for the graph_{t+n}, what is the advantage of DySAT over static methods? 3. What is the running time for a single training process? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank you for the insightful questions on our experiments ! Please refer to our global comment above for a list of all revisions to the paper . We respond to each of your comments below as follows : Q1 : What will happen if a never-seen node appears at t+1 ? The model design seems to be compatible with this case . The structural attention will still work , however , the temporal attention degenerates to a \u201c static \u201d result - all the attention focus on the representation at t+1 . I am curious about the model performance in this situation , since nodes may arise and vanish in real applications . We agree with the apt observation on the capability of DySAT to handle new nodes , and have conducted additional experiments to examine model performance in such situations . To compute the representation of a new node v at time step t+1 , the only available information is the local structure around v at t+1 . Although temporal attention will focus on the latest representation due to absence of history , it however does not degenerate to a \u201c static \u201d result . The temporal attention applied on the neighboring nodes of v ( say N_v ) , indirectly contribute towards the embedding of v , through backpropagation updates . Specifically , the structural embedding of v is computed as a function of N_v , whose structural embeddings receive backpropagation signals through the temporal attention layers ( assuming they are not all new nodes ) . Thus , temporal attention indirectly affects the final embedding of node v. As suggested , we empirically examine the performance of DySAT on \u201c new \u201d previously unseen nodes . We report link prediction performance * only * on the new nodes at each time step using the same experimental setup , i.e. , a test example ( node pair ) is reported for evaluation only if it contains at least one new node . Due to the significant variance in the number of new nodes at each step , we report the performance ( AUC ) at each time step , along with a mention on the corresponding number of new nodes . The results are available in Figure 5 of the Appendix D. From Figure 5 , we observe consistent gains of DySAT over other baselines , similar to our main results . Q2 : What is the performance of the proposed algorithm for multi-step forecasting ? In the experiments , graph at t+1 is evaluated using the model trained up to graph_t . However , in real applications we may don \u2019 t have enough time to retrain the model at every time step . If we use the model trained up to graph_t to compute node embedding for the graph_ { t+n } , what is the advantage of DySAT over static methods ? We agree with your view on the importance of not re-training the model at every time step in real-world applications . Multi-step forecasting is typically achieved either by ( a ) designing a model to predict multiple steps into the future , or by ( b ) recursively feeding next predictions as input , for a desired number of future steps . In case of dynamic graphs , events correspond to link occurrences , which renders forecasting different from conventional time-series , due to the occurrence of new nodes in each time step . Due to this key distinction , we list below , two possibilities for forecasting in dynamic graphs : ( a ) Link prediction at future step t+n ( on all nodes ) by incrementally updating the model on new snapshots . ( b ) Link prediction at future step t+n ( among nodes present at t ) based on dynamic embeddings learned at t , followed by a downstream classifier to predict the links at t+n . Note that ( b ) does not involve model re-training or updating while ( a ) requires incremental model updates . In our paper , we have examined ( a ) by proposing an incremental variant named IncSAT and report the performance in Table 5 of Appendix E. We have now added an additional experiment in Appendix C to evaluate forecasting using strategy ( b ) , which enables direct evaluation of DySAT on multi-step link prediction . Here , each method is trained for a fixed number of time steps , and the latest embeddings are used to predict links at multiple future steps . In each dataset , we choose the last 6 snapshots to evaluate multi-step link prediction where we create examples from the links in G_ { t+n } and an equal number of randomly sampled pairs of unconnected nodes ( non-links ) . Our experimental results ( Figure 4 ) indicate significant improvements for DySAT over all baselines and a highly stable link prediction performance over future time steps . Q3 : What is the running time for a single training process ? We have revised Section 5.4 to add the running time information . Specifically , we report the runtime of DySAT on a machine with Nvidia Tesla V100 GPU and 28 CPU cores . The runtime per mini-batch of DySAT with batch size of 256 nodes on the ML-10M dataset , is 0.72 seconds . In comparison , the model variant without the temporal attention ( No Temporal ) takes 0.51 seconds . Thus , structural attention constitutes a major fraction of the overall runtime , while the cost of temporal attention is relatively lower ."}, "2": {"review_id": "HylsgnCcFQ-2", "review_text": "This is a well-written paper studying the important problem of dynamic network embedding. Please find below some pros and cons of this paper. Pros: * Studies the important problem of network embedding under a more realistic setting (i.e., nodes & edges evolve over time). * Introduces an interesting architecture that uses two forms of attention: structural and temporal. * Demonstrated the effectiveness of the temporal layers through additional experiments (in appendix) and also introduced a variant of their proposed approach which can be trained incrementally using only the last snapshot. Cons: * The authors compared against several dynamic & static graph embedding approaches. If we disregard the proposed approach (DySAT), the static methods seem to match and even, in some cases, beat the dynamic approaches on the compared temporal graph datasets. The authors should compare against stronger baselines for static node embedding, particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful. Please see [1] for an easy way to train GCN/GAT for link prediction. * There are actually quite a number of work done on network embedding on dynamic graphs including [2-4]. In particular, [2-3] support node attributes as well as the addition/deletion of nodes & edges. The author should also compare against these work. * The concept of temporal attention is quite interesting. However, the authors do not provide more analysis on this. For one, I am interested to see how the temporal attention weights are distributed. Are they focused on the more recent snapshots? If so, can we simply retain the more relevant recent information and train a static network embedding approach? Or are the attention weights distributed differently? [1] Modeling Polypharmacy Side Effects with Graph Convolutional Networks. Zitnik et. al. BioInformatics 2018. [2] Attributed Network Embedding for Learning in a Dynamic Environment. Li et. al. In Proc. CIKM '17. [3] Streaming Link Prediction on Dynamic Attributed Networks. Li et. al. In Proc. WSDM '18. [4] Continuous-Time Dynamic Network Embeddings. Nguyen et. al. In Comp. Proc. WWW '18. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank you for the in-depth questions on our experimental results ! Please refer to our global comment above for a list of all revisions to the paper -- we hope they have appropriately addressed your comments . We respond to each of your comments below as follows : Q1 : The authors compared against several dynamic & static graph embedding approaches . If we disregard the proposed approach ( DySAT ) , the static methods seem to match and even , in some cases , beat the dynamic approaches on the compared temporal graph datasets . The authors should compare against stronger baselines for static node embedding , particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful . Please see [ 1 ] for an easy way to train GCN/GAT for link prediction . We agree with you on the observation that static methods often match or beat existing dynamic embedding methods . Our initial experiments contain comparison to GraphSAGE - an unsupervised representation learning framework that supports various neighborhood aggregation functions , including GCN and GAT aggregators . We thank you for the valuable pointer [ 1 ] which trains GCN/GAT as a graph autoencoder directly for link prediction . While conceptually similar to GCN/GAT variants of GraphSAGE , two key differences include ( a ) lack of neighborhood sampling , and ( b ) link prediction objective instead of random walk samples . To examine the effect of these differences on link prediction performance , we used the aforementioned implementation [ 1 ] to train autoencoder models of GCN and GAT , denoted by GCN-AE and GAT-AE in our experiments . Our experimental results have been updated to include these as static embedding methods for comparison . From the results , we find the performance of these methods to be mostly similar to their corresponding GraphSAGE variants , which is consistent with our expectation ."}}