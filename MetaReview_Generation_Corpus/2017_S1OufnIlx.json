{"year": "2017", "forum": "S1OufnIlx", "title": "Adversarial examples in the physical world", "decision": "Invite to Workshop Track", "meta_review": "This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device. \n \n The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions. \n \n In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track.", "reviews": [{"review_id": "S1OufnIlx-0", "review_text": "The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. Pros: 1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge. 2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. Cons: 1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small. 2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy. 3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review . We would like to address some of the cons you mentions . We acknowledge that there are existing papers which describe various adversarial attacks ( including white-box and black box attacks ) on various machine learning systems ( including neural networks ) . However we would like to emphasize that the main difference of our work and prior work is the fact that the attacker in prior work has direct access to inputs of the classifier thus can fine-tune all input values , while in our work we have shown that adversarial attacks are possible even when data is perceived through the sensors ( like a camera ) . Overall this observation is important for practical machine learning systems operating in real world . As we mentioned in the paper , the closest work to ours is Sharif et al.However their work has somewhat different scope and also became publicly available after our paper . Regarding whether the size of perturbation is small or not . We agree that there is no good quantitative threshold assessing whether a perturbation is small or not and this is a very subjective measure . In this sense our primary criterion was whether the perturbation was large enough to interfere with human recognition of the image . Also we have run additional experiments with eps=2 and 4 ( results are added to Tables 1 , 2 and 3 in the paper ) . These experiments are showing the same trend as experiments with higher epsilon - adversarial images may remain misclassified after \u201c photo transformation \u201d . In particular , for eps=2 and 4 about half of \u201c Fast \u201d adversarial examples remain misclassified after \u201c photo transformation \u201d ( see destruction rate in Table 3 ) . Arguably adversarial examples with eps=4 and 2 are small enough to be hard to notice ( examples of adversarial image with eps=4 can be found in Figure 5 ) . So hopefully the fact that physical adversarial examples exist even for such small epsilon should address your concern . Regarding \u201c Some hypotheses proposed in the paper based on one-shot experiments seems too rushy , \u201d we \u2019 d be happy to provide further support for any particular hypothesis you think is invalid . Regarding \u201c the results of this paper seems not really improving the understanding of the adversarial example phenomenon \u201d : this paper is an empirical paper and not a theoretical paper , but we \u2019 ve found important empirical observations that theory should aim to explain , such as the reduced transferability of iterative adversarial examples . In addition the paper helps to advance research on adversarial examples by showing that they also exist in the physical world ."}, {"review_id": "S1OufnIlx-1", "review_text": "In some sense, the Sharif et al. work \"scooped\" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . It 's worth noting that the two papers mutually `` scooped '' each other . While Sharif et al submitted to a conference earlier , our work has been publicly available longer and has more citations . According to Google scholar on Jan 17 , 2017 , our article has been cited 9 times ( https : //scholar.google.com/scholar ? cites=263531058904899909 ) , while that of Sharif et al has been cited 4 times ( https : //scholar.google.com/scholar ? cites=10006479087737690195 ) . One of these 4 citations to Sharif et al is the citation we added in November ."}, {"review_id": "S1OufnIlx-2", "review_text": "Description. The paper investigates whether adversarial examples survive different geometric and photometric image transformations, including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera. The paper considers three different methods to generate adversarial examples \u2014 images with added small amount of noise that changes the output of a classification neural network. In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network. Strong points. - adversarial examples are an interesting phenomenon that is worth detailed investigation. - the paper is well written and presented. - Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting. - Experiments are well done and solid. Weak points: - Probably the main negative point is the amount of novelty and contribution. The paper essentially presents a set of experiments evaluating whether adversarial examples survive different image transformations. Apart from that there is no other main contribution / novelty. While the experiments are solid and well-done, this seems borderline. Detailed evaluation. Originality: - the main contribution of this work is the experimental evaluation showing (and quantifying) how adversarial examples behave under various image transformations. Quality: - The shown experiments are solid and well done. Clarity: - The paper is well written and clear. Significance: - The findings and shown experiments are interesting, but I not sure if the scale and amount of contribution is significant enough for the main conference track. Overall: Experimental paper. Well written. Solid experiments. Not sure if contribution is significant enough.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . We would like to address the significance of the paper . We agree that we did not propose radically novel methods or explanations of the adversarial example problem . At the same time the observations made in this paper are significant for practical ML applications because they show how real world machine learning systems are susceptible to adversarial attacks similar to \u201c digital-only \u201d machine learning systems . Thus the novelty and significance of this paper is in uncovering a new risk for practical real-world machine learning systems . Or in other words it could be considered as a security paper with a new vulnerability disclosed ."}], "0": {"review_id": "S1OufnIlx-0", "review_text": "The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. Pros: 1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge. 2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. Cons: 1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small. 2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy. 3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review . We would like to address some of the cons you mentions . We acknowledge that there are existing papers which describe various adversarial attacks ( including white-box and black box attacks ) on various machine learning systems ( including neural networks ) . However we would like to emphasize that the main difference of our work and prior work is the fact that the attacker in prior work has direct access to inputs of the classifier thus can fine-tune all input values , while in our work we have shown that adversarial attacks are possible even when data is perceived through the sensors ( like a camera ) . Overall this observation is important for practical machine learning systems operating in real world . As we mentioned in the paper , the closest work to ours is Sharif et al.However their work has somewhat different scope and also became publicly available after our paper . Regarding whether the size of perturbation is small or not . We agree that there is no good quantitative threshold assessing whether a perturbation is small or not and this is a very subjective measure . In this sense our primary criterion was whether the perturbation was large enough to interfere with human recognition of the image . Also we have run additional experiments with eps=2 and 4 ( results are added to Tables 1 , 2 and 3 in the paper ) . These experiments are showing the same trend as experiments with higher epsilon - adversarial images may remain misclassified after \u201c photo transformation \u201d . In particular , for eps=2 and 4 about half of \u201c Fast \u201d adversarial examples remain misclassified after \u201c photo transformation \u201d ( see destruction rate in Table 3 ) . Arguably adversarial examples with eps=4 and 2 are small enough to be hard to notice ( examples of adversarial image with eps=4 can be found in Figure 5 ) . So hopefully the fact that physical adversarial examples exist even for such small epsilon should address your concern . Regarding \u201c Some hypotheses proposed in the paper based on one-shot experiments seems too rushy , \u201d we \u2019 d be happy to provide further support for any particular hypothesis you think is invalid . Regarding \u201c the results of this paper seems not really improving the understanding of the adversarial example phenomenon \u201d : this paper is an empirical paper and not a theoretical paper , but we \u2019 ve found important empirical observations that theory should aim to explain , such as the reduced transferability of iterative adversarial examples . In addition the paper helps to advance research on adversarial examples by showing that they also exist in the physical world ."}, "1": {"review_id": "S1OufnIlx-1", "review_text": "In some sense, the Sharif et al. work \"scooped\" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . It 's worth noting that the two papers mutually `` scooped '' each other . While Sharif et al submitted to a conference earlier , our work has been publicly available longer and has more citations . According to Google scholar on Jan 17 , 2017 , our article has been cited 9 times ( https : //scholar.google.com/scholar ? cites=263531058904899909 ) , while that of Sharif et al has been cited 4 times ( https : //scholar.google.com/scholar ? cites=10006479087737690195 ) . One of these 4 citations to Sharif et al is the citation we added in November ."}, "2": {"review_id": "S1OufnIlx-2", "review_text": "Description. The paper investigates whether adversarial examples survive different geometric and photometric image transformations, including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera. The paper considers three different methods to generate adversarial examples \u2014 images with added small amount of noise that changes the output of a classification neural network. In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network. Strong points. - adversarial examples are an interesting phenomenon that is worth detailed investigation. - the paper is well written and presented. - Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting. - Experiments are well done and solid. Weak points: - Probably the main negative point is the amount of novelty and contribution. The paper essentially presents a set of experiments evaluating whether adversarial examples survive different image transformations. Apart from that there is no other main contribution / novelty. While the experiments are solid and well-done, this seems borderline. Detailed evaluation. Originality: - the main contribution of this work is the experimental evaluation showing (and quantifying) how adversarial examples behave under various image transformations. Quality: - The shown experiments are solid and well done. Clarity: - The paper is well written and clear. Significance: - The findings and shown experiments are interesting, but I not sure if the scale and amount of contribution is significant enough for the main conference track. Overall: Experimental paper. Well written. Solid experiments. Not sure if contribution is significant enough.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . We would like to address the significance of the paper . We agree that we did not propose radically novel methods or explanations of the adversarial example problem . At the same time the observations made in this paper are significant for practical ML applications because they show how real world machine learning systems are susceptible to adversarial attacks similar to \u201c digital-only \u201d machine learning systems . Thus the novelty and significance of this paper is in uncovering a new risk for practical real-world machine learning systems . Or in other words it could be considered as a security paper with a new vulnerability disclosed ."}}