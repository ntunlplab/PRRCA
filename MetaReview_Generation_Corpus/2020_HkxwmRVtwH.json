{"year": "2020", "forum": "HkxwmRVtwH", "title": "Gaussian Process Meta-Representations Of Neural Networks", "decision": "Reject", "meta_review": "The authors propose an approach to Bayesian deep learning, by representing neural network weights as latent variables mapped through a Kronecker factored Gaussian process. The ideas have merit and are well-motivated. Reviewers were primarily concerned by the experimental validation, and lack of discussion and comparisons with related work. After the rebuttal, reviewers still expressed concern regarding both points, with no reviewer championing the work.\n\nOne reviewer writes: \"I have read the authors' rebuttal. I still have reservation regarding the gain of a GP over an NN in my original review and I do not think the authors have addressed this very convincingly -- while I agree that in general, sparse GP can match the performance of GP with a sufficiently large number of inducing inputs, the proposed method also incurs extra approximations so arguing for the advantage of the proposed method in term of the accurate approximate inference of sparse GP seems problematic.\"\n\nAnother reviewer points out that the comment in the author rebuttal about Kronecker factored methods (Saatci, 2011) for non-Gaussian likelihoods and with variational inference being an open question is not accurate: SV-DKL (https://arxiv.org/abs/1611.00336) and other approaches (http://proceedings.mlr.press/v37/flaxman15.pdf) were specifically designed to address this question, and are implemented in popular packages. Moreover, there is highly relevant additional work on latent variable representations for neural network weights, inducing priors on p(w) through p(z), which is not discussed or compared against (https://arxiv.org/abs/1811.07006, https://arxiv.org/abs/1907.07504). The revision only includes a minor consideration of DKL in the appendix. \n\nWhile the ideas in the paper are promising, and the generally thoughtful exchanges were appreciated, there is clearly related work that should be discussed in the main text, with appropriate comparisons. With reviewers expressing additional reservations after rebuttal, and the lack of a clear champion, the paper would benefit from significant revisions in these directions. \n\nNote: In the text, it says:\n\"However, obtaining p(w|D) and p(D) exactly is intractable when N is large or when the network is large and as such, approximation methods are often required.\"\nOne cannot exactly obtain p(D), or the predictive distribution, regardless of N or the size of the network; exact inference is intractable because the relevant integrals cannot be expressed in closed form, since the parameters are mapped through non-linearities, in addition to typically non-Gaussian likelihoods.", "reviews": [{"review_id": "HkxwmRVtwH-0", "review_text": "**Summary**: This paper proposes a hierarchical Bayesian approach to hyper-networks by placing a Gaussian process prior over the latent representation for each weight. A stochastic variational inference scheme is then proposed to infer the posterior over both the Gaussian process and the weights themselves. Experiments are performed on toy regression, classification, (edit: post rebuttal) and transfer learning tasks, as well as an uncertainty quantification experiment on MNIST. post rebuttal (noticed recently): Many apologies for updating the review the day before the deadline; however, I recently remembered that Kronecker inference is often used in variational methods - particularly within the vein of literature of deep kernel learning. Indeed, structure exploiting SVI was proposed in Stochastic Variational Deep Kernel Learning, https://arxiv.org/pdf/1611.00336.pdf, and this method is currently the default in Gpytorch: https://github.com/cornellius-gp/gpytorch/tree/master/examples/08_Deep_Kernel_Learning . Furthermore, Kronecker inference for non-Gaussian likelihoods for Laplace approximations was proposed back in 2015: http://proceedings.mlr.press/v37/flaxman15.pdf. I am not updating my score because it would be unfair; however, the record should be set somewhat straight here. post rebuttal: Thank you for the many clarifications and detailed responses. I'm now satisfied with their many changes and and tend to accept this paper despite the experimental results being somewhat limited. I would really encourage the authors to fix the color schemes (please less black and more brighter colors) on their decision boundaries plots however. **tldr**: While I appreciate the concept of this paper, I tend to reject this paper because I find the experimental results to be on too small scale of datasets. Specifically, I would like to see either a larger scale problem being solved with this kind of approach or a tough to model applied problem that is solved with this approach. **Originality**: As far as I can tell, this seems to be a novel approach to hyper-networks. Neural processes (Garnelo et al; 2018) propose a somewhat similar approach to training \u2013 with a latent process over some stored weight space. However, even that is quite distinct from the method proposed in this paper, and I tend to prefer this approach. **Quality**: I really appreciate the merging of neural network and Gaussian process methods; however, tragically, I do wonder if the proposed approach combines the worst of both worlds \u2013 the necessity of architecture search for neural networks with the choice of kernel function (as illustrated in Figure 5). If the method is truly kernel dependent, is it also architecture dependent? That is, is it robust to different settings of nonlinearities and depths? Active learning experiment: While I appreciate the comparison here, it seems like here standard HMC should be trainable over well-designed priors on these architectures. So why not include a comparison instead of just MFVI? **Significance**: Unfortunately, I think that the experiments section is just a bit too limited to warrant acceptance right now. This is despite the fact that I really do appreciate the thoroughness and thoughtfulness of the experiments as they are. Specifically, in Section 6.2 why is the metaGP prior only applied to the last layer of the network? If as I suspect, it is due to the complexity and difficulty of inference, that makes the method doubly tough to use in practice. With that being said, to only have experiments on the last layer implies that one should compare to Bayesian logistic regression and linear regression on the last layer of neural networks (e.g Perrone et al, 2018 and Riquelme et al, 2018). Experiments with other methods that combine Gaussian processes with representations on the final layer (e.g. Wilson et al, 2015) are also probably worth running. Figure 4 is a very well-done experiment, if a bit tough to read. I\u2019d suggest that the out of distribution examples get their own figure, with the in distribution examples going into the appendix. I\u2019d also suggest computing the expected calibration error (Naeini et al, 2015) for in and out of distribution examples on the test sets for both MNIST and K-MNIST in order to have quantitative results on the entire test set. To recommend acceptance, I\u2019d really have to see experiments on either a CIFAR sized dataset for classification or a larger scale regression experiment. A larger dataset on either transfer learning (after all you do have a meta-representation over functions that the NN can learn), a larger active learning experiment, or semi-supervised learning. **Clarity**: Overall, the paper is well-written and mostly easy to follow. The meat of the paper is found in Section 4, which I found a bit difficult to follow. (edit: post rebuttal. This concern is somewhat resolved due to the field not being well developed in this area, although it is a useful place to possibly extend the method in the future.) My primary concern here is that the prior ends up becoming Kronecker structured (after Eq. 7), so it isn\u2019t clear to my why dense matrices and dense variational bounds have to be derived in this setting. Can one not follow the lead of the Gaussian process literature (e.g. Saatci 2012, Wilson & Nickisch, 2015) to exploit the Kronecker structure here to make computation of the log likelihoods fast? (edit: post rebuttal. This concern is somewhat resolved.) As a result, it\u2019s not immediately clear to me why a diagonal approximation (Eq. 10) is even necessary? Furthermore, this may be a setting where iterative methods (e.g conjugate gradients and Lanczos decompositions as in Pleiss et al, 2018) for the predictive means and variances may shine and be fast. I do agree that the approximation in Figure 2 does seem to be relatively accurate, although I would ask the authors to compute a relative error for that plot if possible. Additionally, what is the strange high off diagonal correlations in the marginal covariances? (edit: post rebuttal. Thank you for the clarifications here.) Finally, I was a bit confused by the effect of adding the input dependent kernel in Section 3; this seems to make the weights much more complicated to model \u2013 now each data point has its own set of weights and therefore, we might have to store considerably more weight matrices over time. Could the authors perform a set of experiments showing the necessity of this kernel matrix in the rebuttal? **Minor Comments**: - Above Eq. 9, \u201csplitted\u201d should be split. - Figure 3: could the data points be plotted in a brighter fashion? On a dark background, they are quick tough to see. Additionally, what is the difference between the two levels of classification plots? References: Naeini, et al. Obtaining Well Calibrated Probabilities by Bayesian Binning, AAAI, 2015. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410090/ Perrone, V, et al. Scalable Hyperparameter Transfer Learning, NeurIPS, 2018. http://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning Pleiss, G, et al. Constant Time Predictive Distributions for Gaussian Processes, ICML, 2018. https://arxiv.org/abs/1803.06058 Riquelme, C, Tucker, G, Snoek, J. Deep Bayesian Bandits Showdown, ICLR, 2018. https://arxiv.org/abs/1802.09127 Saatci, Y. Scalable Inference for Structured Gaussian process models, PhD Thesis, U. of Cambridge, 2011. http://mlg.eng.cam.ac.uk/pub/pdf/Saa11.pdf Wilson, AG and Nickisch, H. Kernel Interpolation for Scalable Structured Gaussian Processes, ICML, 2015. http://proceedings.mlr.press/v37/wilson15.pdf Wilson, AG, et al. Deep Kernel Learning, AISTATS, 2015. https://arxiv.org/abs/1511.02222 ", "rating": "6: Weak Accept", "reply_text": "Thank you for your long and detailed review . We have addressed your concerns in both the statement to all reviewers and will address your questions here individually : Re quality , combining neural networks and GPs : In this work , we are changing the prior over the network parameters , while the underlying mapping from inputs to outputs is still a neural network . This prior , driven by a Gaussian process and judiciously chosen latent variables as well as an optional input-dependent kernel , provides a structured , yet flexible way to encode the modelling assumptions and prior knowledge to the network parameters . It does , however , also facilitate black-box modeling by simply utilizing an RBF or Matern kernel for the kernel on Z ( we always use an RBF kernel here ) and similarly generic kernels for the input-dependent kernel ( we use both and RBF kernel when quantifying uncertainty but also explored periodic kernels when wanting to model temporal/periodic models ) . One can dig much deeper into the GP/kernel literature to capture more interesting combinations of kernels or perform kernel learning in future work , but we believe as presented the kernels we use are generic and useful and did not require hard manual tuning whatsoever . We also note that a big part of the prior lies in the latent variable structure Z , which we take as given , but could potentially be enriched with more prior knowledge . Our contributions are orthogonal to that aspect . We believe our model presents clear advantages in terms of representation over other priors or hyper-network architectures which require hard architecture choices and are not interpretable whatsoever . Re quality , active learning + HMC : we have focussed on approximate inference here . We agree that a comparison to HMC for networks with a Gaussian prior over weights or a Gaussian prior with a hierarchical prior over the prior variance would be interesting . We note that we could also apply HMC for our own model for a fair comparison . Re significance , last layer : No , scalability when using metaGP over all layers is not an issue , the proposed algorithm can handle that . Please note the diagonal approximation and our result demonstrate its empirically strong performance in Sec.A.1 and that it is cheap to scale to arbitrary networks . We will include such result for all layers for sec 6.2 in the next iteration . In all other experiments , metaGP is applied to all layers . Re significance , compare to other models : Thanks for the suggestion . We will compare to Bayesian LinReg/LogReg or GP on the last layer . We would like to note that the network architecture remains unchanged in our settings , just the prior on weights is changed , compared to deep kernel learning ( Wilson et al , 2015 ) in which the last layer is replaced by GPs . Re significance , figure 4 : Thanks for the compliment , we indeed tried to provide a thoughtful experiment demonstrating the ability of the model to say \u201c I don \u2019 t know \u201d when appropriate . Our result is extremely difficult for Bayesian Neural Networks to achieve , but intuitively what modelers are hoping for when using them . We will compute the calibration error as suggested and make the figures clearer to read . Re significance , additional experiments : We have added some preliminary experiments on transfer learning and multi-task learning in the appendix . We will expand these in the next iteration . We will also provide a comparison of an HMC network to our local model on a classification task in a next revision . Re clarity , Kronecker : We would like to note that the literature on Kronecker structure and conjugate-gradient + Lanczos + stochastic trace estimation methods has mostly focussed on GP regression in which the log marginal likelihood , as well as the predictive distribution can be computed analytically . Extending these methods beyond GP regression to models with latent variables and non-Gaussian likelihoods is an active research area and beyond the scope of this paper . In particular , Kronecker-factorized models with inducing points present some additional challenges . In addition , our experiments with fully correlated GPs vs. diagonal approximations revealed that diagonal approximation in our model tend to outperform the fully correlated one , perhaps due to the fact that off-diagonal correlations are marginally captured to a large extent by the latent variables Z and learning becomes easier . The fully correlated model is a superset of the Kronecker-approximated one . Re clarity , figure 2 and diagonal approximation : These figures show the covariance structures after training using various approximations . We could include figures showing the difference covariance structures with all other parameters fixed . Importantly , the diagonal approximation here is not a model that tries to minimize the error compared to the fully correlated one , but has a different learning trace altogether ."}, {"review_id": "HkxwmRVtwH-1", "review_text": "This paper proposes an improvement over Probabilistic meta-representations of neural networks by replacing the NN parameterization of the network weights given latent variables by a probabilistic distribution whose mean is distributed by GP. Inference of the induced hierarchical model is achieved by variational inference and various approximations when needed. Comments: 1. The authors claim that the proposed method aims to increase the robustness in the small data settings and improve its out-of-sample uncertainty estimates. The second part is well justified. Could the author elaborate further on the high level intuition behind the first part? 2. What exactly is the gain obtained by replacing a NN parameterization with a GP parameterization? It seems like the proposed method gains the ability to model uncertainty, but potentially incurs performance trade-off from a series of approximation. 3. Following from comment #2, I am a bit surprised by the lack of comparison against the work of Karaletsos although this work was built on top of it. I am interested to understand the mentioned trade-off in practice. 4. Regarding the practical consideration in Eq.(11), I think it kind of defeats the purpose of setting up the latent variables so that \"weights in a layer and across layers are explicitly correlated in the modelling stage\" (Section 2.2). Do all experiments presented in the paper employ this practical consideration? 5. For the text before Eq. (8) should the inducing inputs be xu instead of zu? It seems like a systematic typo here because in the formulation for the lower bound it becomes p(u|xu) again instead of p(u|zu) 6. Is Kuu in Eq.(9) computed by taking Kronecker product of K_in, K_out and K(xu,xu) or just K(xu,xu). I am under the impression that it is the former, in which case taking the inversion is costly (cubic in the number of latent variables). This would not permit large NN anyway, which kind of defeats the purpose. It explains the choice of small architecture in your experiments as well. Overall comment: I think the paper presents an interesting idea but I have questions regarding its practical significance as highlighted in my specific comments above. I hope the authors would clarify these so I can converge on a final rating. ", "rating": "6: Weak Accept", "reply_text": "We thank you for your thoughtful review . Please note the general responses we have added . We also address your individual questions in the following : 1 . Re robustness in the small data setting : Thanks for the suggestion . We have included a qualitative analysis of the performance across different number of training points in appendix A.1 . 2.Re gain obtained by using a GP over a NN : The main advantage is to have a Bayesian non-parametric component in the loop that has fewer parameters and for which we can leverage a rich literature on accurate sparse approximation inference , rather than a parametric one with the inference challenge remained largely unsolved . The GP mapping also allows convenient deployment of the input-dependent kernel which shows additional benefits in many practical settings . Re performance trade-off : In general , sparse GP approximations based on inducing points can match the performance of exact GPs if a sufficient number of inducing points is used ( see e.g.Bui et al ( 2017 ) , Burt et al ( 2019 ) ) . We also provide an empirical analysis of the diagonal approximation in appendix A.1 . 3.Re compare to Karaletsos et al ( 2018 ) : Thanks for the suggestion . We have compared to this work in several toy examples in section 6.1 and appendix A.2 , and observed that GP-metarep tends to perform similarly to NN-metarep , but Local-GP-metarep has many advantages as shown and actually also converges faster than either GP-Metarep or NN-Metarep . We will provide a more thorough comparison in the next version of the paper . 4.Re equation 11 : We would like to note that this diagonal approximation is conditioned on the latent meta-representation variables z and , as such , the parameters will become correlated when the variables z are integrated out . Indeed , a main advantage of this model is that correlations are captured through the hierarchical dependence on individual Z \u2019 s by multiple weights rather than through the output of the mapping . In addition , we can use a non-diagonal approximation ( see appendix A.1 ) without hurting the computational complexity . However , we observed that the diagonal approximation performs as well as other structured and potentially more expensive approximations . In light of this analysis , all experiments considered in the paper employ the diagonal approximation . 5.Re xu vs zu : Thanks for pointing this out . It is indeed a typo . We have fixed this . 6.Re Kuu in eq 9 : It is just K ( zu , zu ) , that is , Kuu is the covariance matrix evaluated at zu . Let M be the number of inducing points then Kuu is a MxM matrix . As with typically done in the sparse GP literature , M is much smaller than the number of weights in the network or the number of data points , which admits tractable computation regardless of the network size . Thanks again for your review . Please let us know if you have any further comments . References : Bui , Yan , and Turner , A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation , JMLR 2017 Burt , Rasmussen and van der Wilk , Rates of Convergence for Sparse Variational Gaussian Process Regression , ICML 2019"}, {"review_id": "HkxwmRVtwH-2", "review_text": "The paper presents two models extended from a meta-presentation of neural networks (Karaletsos et al. 2018) in which neural network weights are constructed hierarchically from latent variables associated with each layer. The mappings from latent space to weight space are used Gaussian Processes instead of neural networks. The proposed models include (1) MetaGP which directly replaces neural network assumption by Gaussian process prior and (2) MetaGP with contextual information which further takes into account input information via performing the multiplication between the kernel function over latent space and kernel function over input space. Variational inference is followed by the pseudo-inducing point approach. Experiments are conducted in both toy data sets and benchmark data sets, demonstrating several points i.e. uncertainty quantification, inductive bias. Pros: The paper is clearly written. Introducing inductive bias or functional regularization for neural network Interesting capability of measuring the uncertainty of out-of-sample data. This can be one of the reasons that the method performed well in active learning. Cons: The approach is incremental or not-so-novel in terms of meta-representation for neural networks. Comments and questions: The prior distribution for latent variable $z$ is not specified. I assume the prior is independent Gaussian. $z$ is unknown beforehand. How are inducing locations for latent variable $z$ initialized? Do you think that there is a connection between contextual metaGP and residual nets? Can skip connections from the input layer to certain layers be considered to be similar to the idea of incorporate input kernel in the paper? Can you comment on the convergence of the estimation of the last term in the variational bound? The MCMC takes two stages of stochasticity: (1) sample $z_k$ and $V_k$ and (2) then estimate $F_k$ using reparameterization. This can make the convergence slow (https://arxiv.org/abs/1709.06181). Minor: a missing period in Sec. 6.3 \u201cquadratic kernel In this example\u201d", "rating": "6: Weak Accept", "reply_text": "Thank you for your thoughtful review , we would ask you to kindly take the general comments into account and will also respond individually to your comments in the following : Re incremental contribution : we would like to note that the proposed model and method allows the prior over the network parameters that can be flexibly modelled by a GP and can be further enriched by the input-dependent kernel . We demonstrate the utility of the proposed approach on a suite of tasks showing accurate prediction , rich inductive biases via kernels and calibrated predictive uncertainty . This principled merger of the worlds of Neural Networks and Gaussian Processes has to our knowledge not been executed as performed here and clearly demonstrates empirical value in addition to conceptual elegance . Re prior over the latent variable : yes , we use a diagonal Gaussian prior , as typically done in many other latent variable models . This is noted in our paper in Sec.2.2 above Eq.2.Re inducing point initialisation : we randomly initialize the inducing inputs and make sure that these initial values are similar in value to the initial mean of the latent variable z . Note that this init strategy is also used in the GP latent variable model ( Lawrence 2006 , Titsias and Lawrence 2010 ) . Re connection to residual nets : Thanks for the suggestion . We agree this connection is a very appealing idea to investigate in future work . We would like to note one clear difference : skip connections directly add the activations of a layer to the output-features of the next layer while in our case , inputs are used to adapt the weight prior only . Re convergence : we observed that the proposed inference scheme converges after a reasonable number of epochs , comparable to that of the standard mean-field Gaussian variational inference . We will include the full training curves in the next iteration . Re missing period : Thanks , we have fixed this . Thanks again for your review . Please let us know if you have any further comments ."}], "0": {"review_id": "HkxwmRVtwH-0", "review_text": "**Summary**: This paper proposes a hierarchical Bayesian approach to hyper-networks by placing a Gaussian process prior over the latent representation for each weight. A stochastic variational inference scheme is then proposed to infer the posterior over both the Gaussian process and the weights themselves. Experiments are performed on toy regression, classification, (edit: post rebuttal) and transfer learning tasks, as well as an uncertainty quantification experiment on MNIST. post rebuttal (noticed recently): Many apologies for updating the review the day before the deadline; however, I recently remembered that Kronecker inference is often used in variational methods - particularly within the vein of literature of deep kernel learning. Indeed, structure exploiting SVI was proposed in Stochastic Variational Deep Kernel Learning, https://arxiv.org/pdf/1611.00336.pdf, and this method is currently the default in Gpytorch: https://github.com/cornellius-gp/gpytorch/tree/master/examples/08_Deep_Kernel_Learning . Furthermore, Kronecker inference for non-Gaussian likelihoods for Laplace approximations was proposed back in 2015: http://proceedings.mlr.press/v37/flaxman15.pdf. I am not updating my score because it would be unfair; however, the record should be set somewhat straight here. post rebuttal: Thank you for the many clarifications and detailed responses. I'm now satisfied with their many changes and and tend to accept this paper despite the experimental results being somewhat limited. I would really encourage the authors to fix the color schemes (please less black and more brighter colors) on their decision boundaries plots however. **tldr**: While I appreciate the concept of this paper, I tend to reject this paper because I find the experimental results to be on too small scale of datasets. Specifically, I would like to see either a larger scale problem being solved with this kind of approach or a tough to model applied problem that is solved with this approach. **Originality**: As far as I can tell, this seems to be a novel approach to hyper-networks. Neural processes (Garnelo et al; 2018) propose a somewhat similar approach to training \u2013 with a latent process over some stored weight space. However, even that is quite distinct from the method proposed in this paper, and I tend to prefer this approach. **Quality**: I really appreciate the merging of neural network and Gaussian process methods; however, tragically, I do wonder if the proposed approach combines the worst of both worlds \u2013 the necessity of architecture search for neural networks with the choice of kernel function (as illustrated in Figure 5). If the method is truly kernel dependent, is it also architecture dependent? That is, is it robust to different settings of nonlinearities and depths? Active learning experiment: While I appreciate the comparison here, it seems like here standard HMC should be trainable over well-designed priors on these architectures. So why not include a comparison instead of just MFVI? **Significance**: Unfortunately, I think that the experiments section is just a bit too limited to warrant acceptance right now. This is despite the fact that I really do appreciate the thoroughness and thoughtfulness of the experiments as they are. Specifically, in Section 6.2 why is the metaGP prior only applied to the last layer of the network? If as I suspect, it is due to the complexity and difficulty of inference, that makes the method doubly tough to use in practice. With that being said, to only have experiments on the last layer implies that one should compare to Bayesian logistic regression and linear regression on the last layer of neural networks (e.g Perrone et al, 2018 and Riquelme et al, 2018). Experiments with other methods that combine Gaussian processes with representations on the final layer (e.g. Wilson et al, 2015) are also probably worth running. Figure 4 is a very well-done experiment, if a bit tough to read. I\u2019d suggest that the out of distribution examples get their own figure, with the in distribution examples going into the appendix. I\u2019d also suggest computing the expected calibration error (Naeini et al, 2015) for in and out of distribution examples on the test sets for both MNIST and K-MNIST in order to have quantitative results on the entire test set. To recommend acceptance, I\u2019d really have to see experiments on either a CIFAR sized dataset for classification or a larger scale regression experiment. A larger dataset on either transfer learning (after all you do have a meta-representation over functions that the NN can learn), a larger active learning experiment, or semi-supervised learning. **Clarity**: Overall, the paper is well-written and mostly easy to follow. The meat of the paper is found in Section 4, which I found a bit difficult to follow. (edit: post rebuttal. This concern is somewhat resolved due to the field not being well developed in this area, although it is a useful place to possibly extend the method in the future.) My primary concern here is that the prior ends up becoming Kronecker structured (after Eq. 7), so it isn\u2019t clear to my why dense matrices and dense variational bounds have to be derived in this setting. Can one not follow the lead of the Gaussian process literature (e.g. Saatci 2012, Wilson & Nickisch, 2015) to exploit the Kronecker structure here to make computation of the log likelihoods fast? (edit: post rebuttal. This concern is somewhat resolved.) As a result, it\u2019s not immediately clear to me why a diagonal approximation (Eq. 10) is even necessary? Furthermore, this may be a setting where iterative methods (e.g conjugate gradients and Lanczos decompositions as in Pleiss et al, 2018) for the predictive means and variances may shine and be fast. I do agree that the approximation in Figure 2 does seem to be relatively accurate, although I would ask the authors to compute a relative error for that plot if possible. Additionally, what is the strange high off diagonal correlations in the marginal covariances? (edit: post rebuttal. Thank you for the clarifications here.) Finally, I was a bit confused by the effect of adding the input dependent kernel in Section 3; this seems to make the weights much more complicated to model \u2013 now each data point has its own set of weights and therefore, we might have to store considerably more weight matrices over time. Could the authors perform a set of experiments showing the necessity of this kernel matrix in the rebuttal? **Minor Comments**: - Above Eq. 9, \u201csplitted\u201d should be split. - Figure 3: could the data points be plotted in a brighter fashion? On a dark background, they are quick tough to see. Additionally, what is the difference between the two levels of classification plots? References: Naeini, et al. Obtaining Well Calibrated Probabilities by Bayesian Binning, AAAI, 2015. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410090/ Perrone, V, et al. Scalable Hyperparameter Transfer Learning, NeurIPS, 2018. http://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning Pleiss, G, et al. Constant Time Predictive Distributions for Gaussian Processes, ICML, 2018. https://arxiv.org/abs/1803.06058 Riquelme, C, Tucker, G, Snoek, J. Deep Bayesian Bandits Showdown, ICLR, 2018. https://arxiv.org/abs/1802.09127 Saatci, Y. Scalable Inference for Structured Gaussian process models, PhD Thesis, U. of Cambridge, 2011. http://mlg.eng.cam.ac.uk/pub/pdf/Saa11.pdf Wilson, AG and Nickisch, H. Kernel Interpolation for Scalable Structured Gaussian Processes, ICML, 2015. http://proceedings.mlr.press/v37/wilson15.pdf Wilson, AG, et al. Deep Kernel Learning, AISTATS, 2015. https://arxiv.org/abs/1511.02222 ", "rating": "6: Weak Accept", "reply_text": "Thank you for your long and detailed review . We have addressed your concerns in both the statement to all reviewers and will address your questions here individually : Re quality , combining neural networks and GPs : In this work , we are changing the prior over the network parameters , while the underlying mapping from inputs to outputs is still a neural network . This prior , driven by a Gaussian process and judiciously chosen latent variables as well as an optional input-dependent kernel , provides a structured , yet flexible way to encode the modelling assumptions and prior knowledge to the network parameters . It does , however , also facilitate black-box modeling by simply utilizing an RBF or Matern kernel for the kernel on Z ( we always use an RBF kernel here ) and similarly generic kernels for the input-dependent kernel ( we use both and RBF kernel when quantifying uncertainty but also explored periodic kernels when wanting to model temporal/periodic models ) . One can dig much deeper into the GP/kernel literature to capture more interesting combinations of kernels or perform kernel learning in future work , but we believe as presented the kernels we use are generic and useful and did not require hard manual tuning whatsoever . We also note that a big part of the prior lies in the latent variable structure Z , which we take as given , but could potentially be enriched with more prior knowledge . Our contributions are orthogonal to that aspect . We believe our model presents clear advantages in terms of representation over other priors or hyper-network architectures which require hard architecture choices and are not interpretable whatsoever . Re quality , active learning + HMC : we have focussed on approximate inference here . We agree that a comparison to HMC for networks with a Gaussian prior over weights or a Gaussian prior with a hierarchical prior over the prior variance would be interesting . We note that we could also apply HMC for our own model for a fair comparison . Re significance , last layer : No , scalability when using metaGP over all layers is not an issue , the proposed algorithm can handle that . Please note the diagonal approximation and our result demonstrate its empirically strong performance in Sec.A.1 and that it is cheap to scale to arbitrary networks . We will include such result for all layers for sec 6.2 in the next iteration . In all other experiments , metaGP is applied to all layers . Re significance , compare to other models : Thanks for the suggestion . We will compare to Bayesian LinReg/LogReg or GP on the last layer . We would like to note that the network architecture remains unchanged in our settings , just the prior on weights is changed , compared to deep kernel learning ( Wilson et al , 2015 ) in which the last layer is replaced by GPs . Re significance , figure 4 : Thanks for the compliment , we indeed tried to provide a thoughtful experiment demonstrating the ability of the model to say \u201c I don \u2019 t know \u201d when appropriate . Our result is extremely difficult for Bayesian Neural Networks to achieve , but intuitively what modelers are hoping for when using them . We will compute the calibration error as suggested and make the figures clearer to read . Re significance , additional experiments : We have added some preliminary experiments on transfer learning and multi-task learning in the appendix . We will expand these in the next iteration . We will also provide a comparison of an HMC network to our local model on a classification task in a next revision . Re clarity , Kronecker : We would like to note that the literature on Kronecker structure and conjugate-gradient + Lanczos + stochastic trace estimation methods has mostly focussed on GP regression in which the log marginal likelihood , as well as the predictive distribution can be computed analytically . Extending these methods beyond GP regression to models with latent variables and non-Gaussian likelihoods is an active research area and beyond the scope of this paper . In particular , Kronecker-factorized models with inducing points present some additional challenges . In addition , our experiments with fully correlated GPs vs. diagonal approximations revealed that diagonal approximation in our model tend to outperform the fully correlated one , perhaps due to the fact that off-diagonal correlations are marginally captured to a large extent by the latent variables Z and learning becomes easier . The fully correlated model is a superset of the Kronecker-approximated one . Re clarity , figure 2 and diagonal approximation : These figures show the covariance structures after training using various approximations . We could include figures showing the difference covariance structures with all other parameters fixed . Importantly , the diagonal approximation here is not a model that tries to minimize the error compared to the fully correlated one , but has a different learning trace altogether ."}, "1": {"review_id": "HkxwmRVtwH-1", "review_text": "This paper proposes an improvement over Probabilistic meta-representations of neural networks by replacing the NN parameterization of the network weights given latent variables by a probabilistic distribution whose mean is distributed by GP. Inference of the induced hierarchical model is achieved by variational inference and various approximations when needed. Comments: 1. The authors claim that the proposed method aims to increase the robustness in the small data settings and improve its out-of-sample uncertainty estimates. The second part is well justified. Could the author elaborate further on the high level intuition behind the first part? 2. What exactly is the gain obtained by replacing a NN parameterization with a GP parameterization? It seems like the proposed method gains the ability to model uncertainty, but potentially incurs performance trade-off from a series of approximation. 3. Following from comment #2, I am a bit surprised by the lack of comparison against the work of Karaletsos although this work was built on top of it. I am interested to understand the mentioned trade-off in practice. 4. Regarding the practical consideration in Eq.(11), I think it kind of defeats the purpose of setting up the latent variables so that \"weights in a layer and across layers are explicitly correlated in the modelling stage\" (Section 2.2). Do all experiments presented in the paper employ this practical consideration? 5. For the text before Eq. (8) should the inducing inputs be xu instead of zu? It seems like a systematic typo here because in the formulation for the lower bound it becomes p(u|xu) again instead of p(u|zu) 6. Is Kuu in Eq.(9) computed by taking Kronecker product of K_in, K_out and K(xu,xu) or just K(xu,xu). I am under the impression that it is the former, in which case taking the inversion is costly (cubic in the number of latent variables). This would not permit large NN anyway, which kind of defeats the purpose. It explains the choice of small architecture in your experiments as well. Overall comment: I think the paper presents an interesting idea but I have questions regarding its practical significance as highlighted in my specific comments above. I hope the authors would clarify these so I can converge on a final rating. ", "rating": "6: Weak Accept", "reply_text": "We thank you for your thoughtful review . Please note the general responses we have added . We also address your individual questions in the following : 1 . Re robustness in the small data setting : Thanks for the suggestion . We have included a qualitative analysis of the performance across different number of training points in appendix A.1 . 2.Re gain obtained by using a GP over a NN : The main advantage is to have a Bayesian non-parametric component in the loop that has fewer parameters and for which we can leverage a rich literature on accurate sparse approximation inference , rather than a parametric one with the inference challenge remained largely unsolved . The GP mapping also allows convenient deployment of the input-dependent kernel which shows additional benefits in many practical settings . Re performance trade-off : In general , sparse GP approximations based on inducing points can match the performance of exact GPs if a sufficient number of inducing points is used ( see e.g.Bui et al ( 2017 ) , Burt et al ( 2019 ) ) . We also provide an empirical analysis of the diagonal approximation in appendix A.1 . 3.Re compare to Karaletsos et al ( 2018 ) : Thanks for the suggestion . We have compared to this work in several toy examples in section 6.1 and appendix A.2 , and observed that GP-metarep tends to perform similarly to NN-metarep , but Local-GP-metarep has many advantages as shown and actually also converges faster than either GP-Metarep or NN-Metarep . We will provide a more thorough comparison in the next version of the paper . 4.Re equation 11 : We would like to note that this diagonal approximation is conditioned on the latent meta-representation variables z and , as such , the parameters will become correlated when the variables z are integrated out . Indeed , a main advantage of this model is that correlations are captured through the hierarchical dependence on individual Z \u2019 s by multiple weights rather than through the output of the mapping . In addition , we can use a non-diagonal approximation ( see appendix A.1 ) without hurting the computational complexity . However , we observed that the diagonal approximation performs as well as other structured and potentially more expensive approximations . In light of this analysis , all experiments considered in the paper employ the diagonal approximation . 5.Re xu vs zu : Thanks for pointing this out . It is indeed a typo . We have fixed this . 6.Re Kuu in eq 9 : It is just K ( zu , zu ) , that is , Kuu is the covariance matrix evaluated at zu . Let M be the number of inducing points then Kuu is a MxM matrix . As with typically done in the sparse GP literature , M is much smaller than the number of weights in the network or the number of data points , which admits tractable computation regardless of the network size . Thanks again for your review . Please let us know if you have any further comments . References : Bui , Yan , and Turner , A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation , JMLR 2017 Burt , Rasmussen and van der Wilk , Rates of Convergence for Sparse Variational Gaussian Process Regression , ICML 2019"}, "2": {"review_id": "HkxwmRVtwH-2", "review_text": "The paper presents two models extended from a meta-presentation of neural networks (Karaletsos et al. 2018) in which neural network weights are constructed hierarchically from latent variables associated with each layer. The mappings from latent space to weight space are used Gaussian Processes instead of neural networks. The proposed models include (1) MetaGP which directly replaces neural network assumption by Gaussian process prior and (2) MetaGP with contextual information which further takes into account input information via performing the multiplication between the kernel function over latent space and kernel function over input space. Variational inference is followed by the pseudo-inducing point approach. Experiments are conducted in both toy data sets and benchmark data sets, demonstrating several points i.e. uncertainty quantification, inductive bias. Pros: The paper is clearly written. Introducing inductive bias or functional regularization for neural network Interesting capability of measuring the uncertainty of out-of-sample data. This can be one of the reasons that the method performed well in active learning. Cons: The approach is incremental or not-so-novel in terms of meta-representation for neural networks. Comments and questions: The prior distribution for latent variable $z$ is not specified. I assume the prior is independent Gaussian. $z$ is unknown beforehand. How are inducing locations for latent variable $z$ initialized? Do you think that there is a connection between contextual metaGP and residual nets? Can skip connections from the input layer to certain layers be considered to be similar to the idea of incorporate input kernel in the paper? Can you comment on the convergence of the estimation of the last term in the variational bound? The MCMC takes two stages of stochasticity: (1) sample $z_k$ and $V_k$ and (2) then estimate $F_k$ using reparameterization. This can make the convergence slow (https://arxiv.org/abs/1709.06181). Minor: a missing period in Sec. 6.3 \u201cquadratic kernel In this example\u201d", "rating": "6: Weak Accept", "reply_text": "Thank you for your thoughtful review , we would ask you to kindly take the general comments into account and will also respond individually to your comments in the following : Re incremental contribution : we would like to note that the proposed model and method allows the prior over the network parameters that can be flexibly modelled by a GP and can be further enriched by the input-dependent kernel . We demonstrate the utility of the proposed approach on a suite of tasks showing accurate prediction , rich inductive biases via kernels and calibrated predictive uncertainty . This principled merger of the worlds of Neural Networks and Gaussian Processes has to our knowledge not been executed as performed here and clearly demonstrates empirical value in addition to conceptual elegance . Re prior over the latent variable : yes , we use a diagonal Gaussian prior , as typically done in many other latent variable models . This is noted in our paper in Sec.2.2 above Eq.2.Re inducing point initialisation : we randomly initialize the inducing inputs and make sure that these initial values are similar in value to the initial mean of the latent variable z . Note that this init strategy is also used in the GP latent variable model ( Lawrence 2006 , Titsias and Lawrence 2010 ) . Re connection to residual nets : Thanks for the suggestion . We agree this connection is a very appealing idea to investigate in future work . We would like to note one clear difference : skip connections directly add the activations of a layer to the output-features of the next layer while in our case , inputs are used to adapt the weight prior only . Re convergence : we observed that the proposed inference scheme converges after a reasonable number of epochs , comparable to that of the standard mean-field Gaussian variational inference . We will include the full training curves in the next iteration . Re missing period : Thanks , we have fixed this . Thanks again for your review . Please let us know if you have any further comments ."}}