{"year": "2017", "forum": "ByG4hz5le", "title": "Adaptive Feature Abstraction for Translating Video to Language", "decision": "Invite to Workshop Track", "meta_review": "Reviewers feel the work is well executed and that the model makes sense, but two of the reviewers were not convinced that the proposed method contains enough novelty in light of prior work. The comparison of the soft vs hard attention model variations is perhaps one of the more novel aspects of the work; however, the degree of novelty within these formulations and the insights obtained from their comparison were not perceived as being enough to warrant higher ratings. We would like to invite the authors to submit this paper to the workshop track.", "reviews": [{"review_id": "ByG4hz5le-0", "review_text": "This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder. Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT. While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN. The work is well presented and the experiments clearly show the benefit of attention over multiple layers. However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR. Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Your result is super , but sorry , I just do n't like your paper . Q.E.D . `` : - )"}, {"review_id": "ByG4hz5le-1", "review_text": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time. I think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. If the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks a lot for your review . Employing an efficient 3D convolution to achieve spatiotemporal alignment for the attention mechanism is our major technical innovation and the key to achieve impressive performance , which involves precisely calibrating the receptive fields of different convnet layers . This efficient approach informs the representation learning community of a new way of considering multiple-level CNN representations other than hypercolumn representations or our failed MLP attempt . Before applying hard attention even with enhanced multi-sample Monte Carlo objective , we do not know how well it performs on video captioning , a completely different task from image captioning . Reporting a different performance observation from previous work about hard attention vs. soft attention is valuable . All these findings are important to know by the community . Simply extending Xu et al . 's architecture to video captioning without considering the nature of video data has already been explored by a Master thesis ( http : //shikharsharma.com/publications/pdfs/msc-thesis.pdf ) , but their performance is much worse than previously established methods , and far from the state-of-the-art result that we achieved . We will cite this work in the revision for comparison to show the significance of our attention even with a similarly simple encoder-decoder framework . Considering that the three datasets are very challenging , our performance with a simple model without complexities such as hierarchical RNN or a 3D/2D combination is very impressive compared to previous results . Exploring feature representation across layers on a standard CNN for classification independent of applications is a good suggestion , but it leads to a completely different paper . The current paper aims for video captioning and achieved the state-of-the-art performance ."}, {"review_id": "ByG4hz5le-2", "review_text": "1) Summary This paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels. 2) Contributions + Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions). + Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms. + Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case. 3) Suggestions for improvement Hypercolumns comparison: As mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels. Minor clarifications in the text and figures as agreed with the authors in our pre-review discussions. 4) Conclusion Although the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review . We will investigate and compare with the simple hypercolumn representation in the revision ."}], "0": {"review_id": "ByG4hz5le-0", "review_text": "This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder. Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT. While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN. The work is well presented and the experiments clearly show the benefit of attention over multiple layers. However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR. Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Your result is super , but sorry , I just do n't like your paper . Q.E.D . `` : - )"}, "1": {"review_id": "ByG4hz5le-1", "review_text": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time. I think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. If the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks a lot for your review . Employing an efficient 3D convolution to achieve spatiotemporal alignment for the attention mechanism is our major technical innovation and the key to achieve impressive performance , which involves precisely calibrating the receptive fields of different convnet layers . This efficient approach informs the representation learning community of a new way of considering multiple-level CNN representations other than hypercolumn representations or our failed MLP attempt . Before applying hard attention even with enhanced multi-sample Monte Carlo objective , we do not know how well it performs on video captioning , a completely different task from image captioning . Reporting a different performance observation from previous work about hard attention vs. soft attention is valuable . All these findings are important to know by the community . Simply extending Xu et al . 's architecture to video captioning without considering the nature of video data has already been explored by a Master thesis ( http : //shikharsharma.com/publications/pdfs/msc-thesis.pdf ) , but their performance is much worse than previously established methods , and far from the state-of-the-art result that we achieved . We will cite this work in the revision for comparison to show the significance of our attention even with a similarly simple encoder-decoder framework . Considering that the three datasets are very challenging , our performance with a simple model without complexities such as hierarchical RNN or a 3D/2D combination is very impressive compared to previous results . Exploring feature representation across layers on a standard CNN for classification independent of applications is a good suggestion , but it leads to a completely different paper . The current paper aims for video captioning and achieved the state-of-the-art performance ."}, "2": {"review_id": "ByG4hz5le-2", "review_text": "1) Summary This paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels. 2) Contributions + Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions). + Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms. + Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case. 3) Suggestions for improvement Hypercolumns comparison: As mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels. Minor clarifications in the text and figures as agreed with the authors in our pre-review discussions. 4) Conclusion Although the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review . We will investigate and compare with the simple hypercolumn representation in the revision ."}}