{"year": "2020", "forum": "ryxjnREFwH", "title": "Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension", "decision": "Accept (Spotlight)", "meta_review": "Main content:\n\nBlind review #1 summarizes it well:\n\nThis paper presents a semantic parser that operates over passages of text instead of a structured data source.  This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different).  The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations.  This is excellent work, and it should definitely be accepted.  I have a ton of questions about this method, but they are good questions.  \n\n--\n\nDiscussion:\n\nThe reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems.\n\n--\n\nRecommendation and justification:\n\nThis paper should be accepted. Even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction (especially in the theme of ICLR).", "reviews": [{"review_id": "ryxjnREFwH-0", "review_text": "This paper presents a semantic parser that operates over passages of text instead of a structured data source. This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different). The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations. This is excellent work, and it should definitely be accepted. I have a ton of questions about this method, but they are good questions. The rest of this review focuses on things that I thought could be more clear, or that raise new questions, and might sound negative. Please understand them, however, in terms of my overall score and what I said above. The three claimed contributions are (1) better numbers, (2) better compositionality / domain applicability, and (3) better interpretability. (2) and (3) sound a bit like overclaiming in the introduction to me, as there isn't a whole lot of nested composition in the language used by NeRd, and the BERT calculator in principle is almost as compositional and interpretable (also, e.g., NAQANet can add and subtract an arbitrary number of numbers, also, and it tells you which ones they are, just as NeRd does). Later in the paper the specifics of those claims are made more clear, and while they are justified, they are very narrow claims. To me, someone who is intimately familiar with this research area, the key contributions (the things that I learned) are (1) using passage-span and key-value predicates actually works, (2) how much difference hard EM and thresholding make, and (3) the data augmentation in this work is pretty clever. (2) was intuitively clear to me after seeing Dasigi's iterative search paper and Min's hard EM paper, but the difference in results presented here is pretty striking. Compositionality: The authors claim that their method is compositional and domain agnostic, while all previous methods had hand-crafted modules for specific question types. However, I see little reason to believe there's much of a difference here. You also defined operations that are tailored to the dataset, and are basically identical to the operations that others have used. I see no evidence that NeRd actually generalizes to program types that are beyond what is captured by other methods. It's possible that this happens, but there is no evaluation that discusses this, and from all of the examples I'm led to believe that this is basically also just learning a few program templates, the same ones learned by previous methods. With the weak supervision that you have, are you actually able to find more complex programs during your search? Some kind of demonstration of actual compositionality on the more complex questions in DROP would make a very strong argument for your claims; without that, they ring a little hollow. Interpretability: The use of passage-span as a predicate is really interesting, and it raises a lot of questions. This predicate lets the model shortcut any interpretable reasoning and do operations entirely inside the encoder/parser. For example, your first example in table 2 ostensibly requires filtering the numbers in the passage to those that are percentages associated with groups, then filtering them again to those where the percentage is larger than 16, then returning the associated groups. But your method jumps straight to returning a set of passage spans. This is hardly interpretable. (In fairness, no prior method provides interpretable reasoning for this kind of operation either.) But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP). But how does the network decide which to do? Any argmax or max question, and many count questions, could be answered by passage-span alone. With only weak supervision, and with the parser having the ability to shortcut these more interpretable operations, how often are you actually getting the interpretable one, and what's causing the model to choose it? Similarly, how often does an argmax or a max operation actually operate on the full set that you would expect it to? Or does it just do the argmax internal to the network, and output only one item as an argument to the argmax? If the later, this again hurts your claims of better interpretability over prior methods, as the logic is just as opaque as before. This also seems like a really hard search problem in how you've set up your DSL - what would make your search over programs actually select all of the correct arguments? Because you're selecting passage spans directly instead of performing some kind of matching operation, you have to have your search select all of the appropriate spans for this to be \"interpretable\", and not just hiding the logic inside of the network. But that seems like a totally intractable search. You found a clever way to get around this for count questions (even though that still implicitly hides a bunch of filtering logic, as noted above), but I don't know how to make it work for maxes and argmaxes. Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality. At one extreme, you end up with something like NABERT (or even less compositional), where basically everything is inside the network. At the other extreme, where you don't have passage-span, you are left with a crippled semantic parser that can't handle most of the questions. But using the predicate introduces tension in the model between interpretability and flexibility. How do we resolve this tension? (This isn't something I expect your paper to address, it's just a really interesting and important question raised by your work.) Parser: Prior work has found benefit in using runtime constraints on parser outputs, or grammar-based decoding. It looks like you are doing neither of those, yet you're able to output specific token indices and number indices in your programs. Are you really not doing anything special to handle those? How does the decoder know token indices? I feel like something must be missing here, or a simple LSTM decoder is more magical than I thought. Evaluation: Why only show results on DROP dev, and not on the test set? It's possible that your higher numbers are because you were better able to overfit to the dev set, which you presumably used during training. (I don't think that that's likely, but it's a concern that would be easily avoided by evaluating on test.)", "rating": "8: Accept", "reply_text": "Thanks for appreciating our work and your insightful comments ! We respond to your questions below in terms of Contributions , Compositionality , Interpretability , Parser details and Evaluation . Clarifications on contributions ( 1 ) We would like to clarify that we did not claim the the model is fully interpretable , solves all the compositional questions in DROP , or requires no domain-specific languages . Instead , we are arguing that NeRd is * better * than previous methods in these dimensions , so we scoped all the claims in comparison with previous methods in the paper . ( 2 ) We would like to emphasize that the claims , especially the ones on compositionality and easier domain adaptation , are supported by considering * both the experiments on DROP and MathQA * . We have demonstrated that the semantic parsing approach can be applied over text to achieve good performance on DROP , but due to the challenges of searching for highly compositional programs on DROP ( more on this in `` Compositionality '' discussion ) , the ability of NeRd to generate highly compositional programs are better demonstrated on MathQA . And easier domain adaptation is shown by the fact that we applied the same architecture to MathQA using the DSL released by the author of MathQA without any further efforts in DSL or architecture design . ( 3 ) We would like to point out some important differences with previous methods : 1 . NeRd , which operates over text , is similar to other semantic parsers in that it has the expressive power to generate compositional programs by recursively calling the operators , and uses a single program decoder , e.g. , LSTM in our evaluation , to answer different types of questions . In contrast , previous methods on DROP rely on designing specialized neural modules to answer different types of questions , and more importantly , those modules can not be applied recursively or compositionally . Let \u2019 s take arithmetic as an example , and show how previous approaches would require specialized neural modules to handle it . BERT with calculator has to introduce an operator `` Sum3 '' that selects three arguments to support summing up 3 numbers , and extending that approach to handle numerical operations with an arbitrary number of arguments would require creating an exponential number of operators like `` Sum4 '' , `` Sum2Diff1 '' , etc ; on the other hand , these operations can be naturally handled by recursively calling `` Sum '' and/or `` Diff '' a few times with a model that supports generating compositional programs such as NeRd . MTMSN designs a specialized \u201c negation \u201d module , which predicts whether a number should be subtracted from 100 to handle negation questions in percentage calculation , e.g. , \u201c How many percent were not German \u201d , because this type of questions appear frequently in the DROP dataset . As you pointed out , NAQANet can add or subtract arbitrary number of numbers by performing a 3-class classification for each number in the passage , representing plus , minus or zero ; this design choice is also applied in several subsequent approaches on DROP , e.g. , NABERT and MTMSN . However , if the questions require more math operators , like in MathQA , this approach can not easily support complex computations interleaving different types of operators , e.g. , ( ( 1+2 ) * ( 3-5 ) +5^2/6 ) , while such questions can be naturally supported by the compositional programs generated by NeRd in our MathQA experiments . In a word , we agree that if we solely focus on the performance , all these different approaches work reasonably well for arithmetic questions in DROP since they are specially designed for it ; however , none of them can be directly adapted to MathQA , which requires highly compositional arithmetic reasoning and a much larger set of operators . By providing the model with the capability of generating compositional programs , it is easier to adapt NeRd to other domains than other methods compared in our work . These compositional programs are already commonly used in semantic parsing to naturally support complex reasoning without designing specialized neural modules for different types of questions . Our contribution , as you noted , is to show that we can apply them to unstructured input as well , i.e. , text , to achieve the same adaptability and compositionality ."}, {"review_id": "ryxjnREFwH-1", "review_text": "This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. They show interesting results on two datasets requiring symbolic reasoning for answering the questions. Overall, I like the paper and I think it contains simple extensions to previous methods referenced in the paper enabling them to work well on these datasets. A few comments: 1 - Would be interesting to see the performance of the weak supervision on these datasets. In other words, if the heuristics are designed to provide noisy instruction sets for training, we need to see the performance of those on these datasets to determine if the models are generalising beyond those heuristics or they perform at the same level which then may mean we don't need the model. 2 - From Tab 4, it seems the largest improvements are due to span-selection cases as a result of adding span operators to the DSL. A deep dive on this would be a great insight (in addition to performance improvement statements on page 7). 3 - Since the span and value operators require indices to the text location, could you please clarify in the text how that is done? Do LSTMs output the indices or are you selecting from a preselection spans as part of preprocessing? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your valuable comments , and we are glad that you like our paper ! About your comments : 1 . The heuristics used to obtain training data augmentations from weak supervision all rely on access to the ground truth answer , so they can not be directly applied for prediction given just questions and passages . In other words , the heuristics are only applicable during training , so at the inference time , the model has to generalize to answer unseen questions . 2.The span selection operators are key components in the DSL because ( 1 ) they can be used to answer the single span and multiple span questions ; ( 2 ) they can support higher level reasoning such as counting and sorting . For the significant improvement on multiple-span questions , we hypothesize that it is because the same PASSAGE_SPAN operators are used in both single-span , multiple-span as well as counting and sorting questions ; i.e. , multiple-span questions are answered by calling PASSAGE_SPAN a few times , and counting also calls PASSAGE_SPAN to find the spans to count . Therefore , the model obtains more training signals to learn how to use such operators . 3.We extended Appendix C to provide the full details of how the programs are generated . Specifically , the program generation process is similar to the pointer network [ Vinyals et al , 2015 ] , where we compute the attention weights over all valid possible program tokens at each timestep , and select the one with the highest prediction probability . Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . `` Pointer networks . '' Advances in Neural Information Processing Systems . 2015 ."}, {"review_id": "ryxjnREFwH-2", "review_text": "-- define cascade errors when you first use the phrase -- basic english grammar could be fixed but is not interfering with understanding -- what is the early stopping criterion in Alg 1? -- did you try any other values for the initial threshold and decay factor? -- At the end of Section 4.1 DROP: \", but not the same.\" - I can't parse what this last clause is supposed to mean. --the diff sum example in table 2 is confusing; the program appears to sum up the numbers but the result is a subtraction without a sum operation in it. Would be clearer to show the sum in the result line as well rather than distribute the subtraction. Also, shouldn't it be diff(9, sum(10, 12))? -- I think you should pull at least some commentary about the constant used in Table 3 from Appendix B and include it in the main paper (or at least mention Appendix B is the place to look). Can you add a table in an appendix showing the complete list of operators used? -- Nice results in Table 4 on the dev set. Are there Test set results as well? -- The organization of the Baselines 4.3 section and the Results 4.4 is confusing. For example, you mention that you test different variants of NeRd, operator variants, and mathqa, but then the results are not mentioned for these experiments until the next page. I found myself immediately looking for the numbers/results when you introduce the experiment. I would pair your experiment description with the results rather than grouping all experiment descriptions and then grouping all results, especially when the order of the experiment descriptions does not match the order of the results presented. For example, in baselines you discuss training variants and then operators. Then in Results you discuss operators before variants. It is too disconnected and makes the reader jump around a bunch. Same goes for the drop baselines where you mention a bunch of models, and I would prefer the Results/discussion paired with each one, rather than having to wait for it down below. -- Overall it seems like a solid work; good empirical results showing improvements of each purported contribution. The model itself is a relatively simple construction of basic component, but the combination with the DSL is intuitive and makes sense. I don't think the novelty in model here is the main selling point anyways; the training variants and the demonstration of how well a DSL approach can do combined with previously introduced methods. -- I find the model description to be slightly unclear. In Fig 1 for example there is an arrow that connects passage to compositional programs. What does that arrow represent? I think you should elaborate on how the attention over the encoded text interacts with the attention over previously generated tokens. Equations would make this far more explicit as is I am left with a lot of questions on how to implement your model. Maybe you can add to your appendix? Or release your code? That is mentioned either.", "rating": "6: Weak Accept", "reply_text": "Thanks for your constructive feedback ! We have incorporated your comments in our revision , and we respond to your questions below : -- define cascade errors when you first use the phrase We use `` cascade error '' to refer to errors caused by the previous phase in a pipeline . In our case , we use this term to refer to the errors caused by data preprocessing for semantic parsing approaches , which parses the text into structured representations using tools such as SRL . We revised the paper to add a description in the introduction , where we first use this phrase . -- basic english grammar could be fixed but is not interfering with understanding We have done another round of proof-reading and updated the paper accordingly . -- what is the early stopping criterion in Alg 1 ? We perform early stopping when both exact match and F1 scores on the development set do not improve for two consecutive training iterations . We have updated Appendix D.2 to make this point clearer . -- did you try any other values for the initial threshold and decay factor ? We have also tried other values for the initial threshold and decay factor , and we found that the specific values do not matter much for the final results as long as they are in a good range . Specifically , values in [ 0.2 , 0.5 ] work well for both the initial threshold and decay factor ( we tried 1/5 , 1/4 , 1/3 , 1/2 within this range ) . Values larger than 0.5 will slow down training , and values smaller than 0.2 will decrease the quality of the final model . -- At the end of Section 4.1 DROP : `` , but not the same . '' - I ca n't parse what this last clause is supposed to mean . We have rephrased the sentence to be `` F1 score , which gives partial credits to a prediction that is not exactly the same as the ground truth , but overlaps with it \u201d . For example , if the ground truth is `` Barrack Obama '' , and the prediction is `` Obama '' , the exact match score will be zero , while the F1 score is larger than zero . -- the diff sum example in table 2 is confusing . Thanks for catching the problem ! It is indeed a typo . We have fixed it in the revision , and modified the explanation accordingly to make it clearer . -- Discussion of the complete operator and constant lists . For DROP , we have included the complete constant list in Appendix B . Note that MathQA covers a wide range of mathematical questions , and in their public dataset , they released the complete lists with 58 operators and 23 constants , which could be too long to include in the paper . Therefore , we added a description in Appendix B , and refer to their paper and public dataset for complete details . -- Nice results in Table 4 on the dev set . Are there Test set results as well ? Due to time constraint , we only evaluated locally on dev set at the time of submission . We submitted to the DROP server later , and the test result is better ( +1.18 % on F1 and +1.37 % on Exact Match ) than other baselines . -- The organization of the Baselines 4.3 section and the Results 4.4 is confusing ... Thanks for the suggestions ! We swapped the descriptions of operator variants and training variants in Section 4.3 , so that the order in Section 4.3 matches the order in Section 4.4 now . The main reason why we did not pair the descriptions with results is for paper space arrangement . Given that our paper has several tables of different sizes , separating them out is sometimes sub-optimal in terms of space usage . Therefore , we tentatively keep the section organization as it is in this revision ; however , in our camera ready version , we will try to re-organize these two sections if it looks better . -- Overall it seems like a solid work ; good empirical results showing improvements of each purported contribution ... Thanks for appreciating our work ! We agree that the neural architecture of each component in our model , i.e. , BERT as the encoder and an LSTM with attention as the program decoder , is not our main contribution . Instead , as pointed out by Reviewer 2 , the key novelty is `` This paper presents a semantic parser that operates over passages of text instead of a structured data source . This is the first time anyone has demonstrated such a semantic parser '' . To achieve this , we made several technical contributions to address the challenges in designing such a model , for example , the introduction of span selection operators so that compositional reasoning can be applied over text . -- I find the model description to be slightly unclear ... The arrow means that the program can directly select spans from the passage by calling the PASSAGE_SPAN operator with the indices of start and end token ; on the contrary , existing neural semantic parsers require an additional structured parser . We have written the detailed equations in Appendix C , including the attention mechanism . We will open source our code with our camera ready version ."}], "0": {"review_id": "ryxjnREFwH-0", "review_text": "This paper presents a semantic parser that operates over passages of text instead of a structured data source. This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different). The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations. This is excellent work, and it should definitely be accepted. I have a ton of questions about this method, but they are good questions. The rest of this review focuses on things that I thought could be more clear, or that raise new questions, and might sound negative. Please understand them, however, in terms of my overall score and what I said above. The three claimed contributions are (1) better numbers, (2) better compositionality / domain applicability, and (3) better interpretability. (2) and (3) sound a bit like overclaiming in the introduction to me, as there isn't a whole lot of nested composition in the language used by NeRd, and the BERT calculator in principle is almost as compositional and interpretable (also, e.g., NAQANet can add and subtract an arbitrary number of numbers, also, and it tells you which ones they are, just as NeRd does). Later in the paper the specifics of those claims are made more clear, and while they are justified, they are very narrow claims. To me, someone who is intimately familiar with this research area, the key contributions (the things that I learned) are (1) using passage-span and key-value predicates actually works, (2) how much difference hard EM and thresholding make, and (3) the data augmentation in this work is pretty clever. (2) was intuitively clear to me after seeing Dasigi's iterative search paper and Min's hard EM paper, but the difference in results presented here is pretty striking. Compositionality: The authors claim that their method is compositional and domain agnostic, while all previous methods had hand-crafted modules for specific question types. However, I see little reason to believe there's much of a difference here. You also defined operations that are tailored to the dataset, and are basically identical to the operations that others have used. I see no evidence that NeRd actually generalizes to program types that are beyond what is captured by other methods. It's possible that this happens, but there is no evaluation that discusses this, and from all of the examples I'm led to believe that this is basically also just learning a few program templates, the same ones learned by previous methods. With the weak supervision that you have, are you actually able to find more complex programs during your search? Some kind of demonstration of actual compositionality on the more complex questions in DROP would make a very strong argument for your claims; without that, they ring a little hollow. Interpretability: The use of passage-span as a predicate is really interesting, and it raises a lot of questions. This predicate lets the model shortcut any interpretable reasoning and do operations entirely inside the encoder/parser. For example, your first example in table 2 ostensibly requires filtering the numbers in the passage to those that are percentages associated with groups, then filtering them again to those where the percentage is larger than 16, then returning the associated groups. But your method jumps straight to returning a set of passage spans. This is hardly interpretable. (In fairness, no prior method provides interpretable reasoning for this kind of operation either.) But the fact that you have this predicate lets the model do these filters and greater-than comparisons inside the network in an opaque way, while also getting interpretable operations for some questions (table 5 is further confirmation of this, and of the fact that you probably are not capturing many of more the complex, compositional questions in DROP). But how does the network decide which to do? Any argmax or max question, and many count questions, could be answered by passage-span alone. With only weak supervision, and with the parser having the ability to shortcut these more interpretable operations, how often are you actually getting the interpretable one, and what's causing the model to choose it? Similarly, how often does an argmax or a max operation actually operate on the full set that you would expect it to? Or does it just do the argmax internal to the network, and output only one item as an argument to the argmax? If the later, this again hurts your claims of better interpretability over prior methods, as the logic is just as opaque as before. This also seems like a really hard search problem in how you've set up your DSL - what would make your search over programs actually select all of the correct arguments? Because you're selecting passage spans directly instead of performing some kind of matching operation, you have to have your search select all of the appropriate spans for this to be \"interpretable\", and not just hiding the logic inside of the network. But that seems like a totally intractable search. You found a clever way to get around this for count questions (even though that still implicitly hides a bunch of filtering logic, as noted above), but I don't know how to make it work for maxes and argmaxes. Another question raised by the passage-span predicate: the more you use bare passage-span programs for training, the more the network learns to put all of its compositional reasoning inside, in an opaque way, instead of giving you interpretable compositionality. At one extreme, you end up with something like NABERT (or even less compositional), where basically everything is inside the network. At the other extreme, where you don't have passage-span, you are left with a crippled semantic parser that can't handle most of the questions. But using the predicate introduces tension in the model between interpretability and flexibility. How do we resolve this tension? (This isn't something I expect your paper to address, it's just a really interesting and important question raised by your work.) Parser: Prior work has found benefit in using runtime constraints on parser outputs, or grammar-based decoding. It looks like you are doing neither of those, yet you're able to output specific token indices and number indices in your programs. Are you really not doing anything special to handle those? How does the decoder know token indices? I feel like something must be missing here, or a simple LSTM decoder is more magical than I thought. Evaluation: Why only show results on DROP dev, and not on the test set? It's possible that your higher numbers are because you were better able to overfit to the dev set, which you presumably used during training. (I don't think that that's likely, but it's a concern that would be easily avoided by evaluating on test.)", "rating": "8: Accept", "reply_text": "Thanks for appreciating our work and your insightful comments ! We respond to your questions below in terms of Contributions , Compositionality , Interpretability , Parser details and Evaluation . Clarifications on contributions ( 1 ) We would like to clarify that we did not claim the the model is fully interpretable , solves all the compositional questions in DROP , or requires no domain-specific languages . Instead , we are arguing that NeRd is * better * than previous methods in these dimensions , so we scoped all the claims in comparison with previous methods in the paper . ( 2 ) We would like to emphasize that the claims , especially the ones on compositionality and easier domain adaptation , are supported by considering * both the experiments on DROP and MathQA * . We have demonstrated that the semantic parsing approach can be applied over text to achieve good performance on DROP , but due to the challenges of searching for highly compositional programs on DROP ( more on this in `` Compositionality '' discussion ) , the ability of NeRd to generate highly compositional programs are better demonstrated on MathQA . And easier domain adaptation is shown by the fact that we applied the same architecture to MathQA using the DSL released by the author of MathQA without any further efforts in DSL or architecture design . ( 3 ) We would like to point out some important differences with previous methods : 1 . NeRd , which operates over text , is similar to other semantic parsers in that it has the expressive power to generate compositional programs by recursively calling the operators , and uses a single program decoder , e.g. , LSTM in our evaluation , to answer different types of questions . In contrast , previous methods on DROP rely on designing specialized neural modules to answer different types of questions , and more importantly , those modules can not be applied recursively or compositionally . Let \u2019 s take arithmetic as an example , and show how previous approaches would require specialized neural modules to handle it . BERT with calculator has to introduce an operator `` Sum3 '' that selects three arguments to support summing up 3 numbers , and extending that approach to handle numerical operations with an arbitrary number of arguments would require creating an exponential number of operators like `` Sum4 '' , `` Sum2Diff1 '' , etc ; on the other hand , these operations can be naturally handled by recursively calling `` Sum '' and/or `` Diff '' a few times with a model that supports generating compositional programs such as NeRd . MTMSN designs a specialized \u201c negation \u201d module , which predicts whether a number should be subtracted from 100 to handle negation questions in percentage calculation , e.g. , \u201c How many percent were not German \u201d , because this type of questions appear frequently in the DROP dataset . As you pointed out , NAQANet can add or subtract arbitrary number of numbers by performing a 3-class classification for each number in the passage , representing plus , minus or zero ; this design choice is also applied in several subsequent approaches on DROP , e.g. , NABERT and MTMSN . However , if the questions require more math operators , like in MathQA , this approach can not easily support complex computations interleaving different types of operators , e.g. , ( ( 1+2 ) * ( 3-5 ) +5^2/6 ) , while such questions can be naturally supported by the compositional programs generated by NeRd in our MathQA experiments . In a word , we agree that if we solely focus on the performance , all these different approaches work reasonably well for arithmetic questions in DROP since they are specially designed for it ; however , none of them can be directly adapted to MathQA , which requires highly compositional arithmetic reasoning and a much larger set of operators . By providing the model with the capability of generating compositional programs , it is easier to adapt NeRd to other domains than other methods compared in our work . These compositional programs are already commonly used in semantic parsing to naturally support complex reasoning without designing specialized neural modules for different types of questions . Our contribution , as you noted , is to show that we can apply them to unstructured input as well , i.e. , text , to achieve the same adaptability and compositionality ."}, "1": {"review_id": "ryxjnREFwH-1", "review_text": "This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. They show interesting results on two datasets requiring symbolic reasoning for answering the questions. Overall, I like the paper and I think it contains simple extensions to previous methods referenced in the paper enabling them to work well on these datasets. A few comments: 1 - Would be interesting to see the performance of the weak supervision on these datasets. In other words, if the heuristics are designed to provide noisy instruction sets for training, we need to see the performance of those on these datasets to determine if the models are generalising beyond those heuristics or they perform at the same level which then may mean we don't need the model. 2 - From Tab 4, it seems the largest improvements are due to span-selection cases as a result of adding span operators to the DSL. A deep dive on this would be a great insight (in addition to performance improvement statements on page 7). 3 - Since the span and value operators require indices to the text location, could you please clarify in the text how that is done? Do LSTMs output the indices or are you selecting from a preselection spans as part of preprocessing? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your valuable comments , and we are glad that you like our paper ! About your comments : 1 . The heuristics used to obtain training data augmentations from weak supervision all rely on access to the ground truth answer , so they can not be directly applied for prediction given just questions and passages . In other words , the heuristics are only applicable during training , so at the inference time , the model has to generalize to answer unseen questions . 2.The span selection operators are key components in the DSL because ( 1 ) they can be used to answer the single span and multiple span questions ; ( 2 ) they can support higher level reasoning such as counting and sorting . For the significant improvement on multiple-span questions , we hypothesize that it is because the same PASSAGE_SPAN operators are used in both single-span , multiple-span as well as counting and sorting questions ; i.e. , multiple-span questions are answered by calling PASSAGE_SPAN a few times , and counting also calls PASSAGE_SPAN to find the spans to count . Therefore , the model obtains more training signals to learn how to use such operators . 3.We extended Appendix C to provide the full details of how the programs are generated . Specifically , the program generation process is similar to the pointer network [ Vinyals et al , 2015 ] , where we compute the attention weights over all valid possible program tokens at each timestep , and select the one with the highest prediction probability . Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . `` Pointer networks . '' Advances in Neural Information Processing Systems . 2015 ."}, "2": {"review_id": "ryxjnREFwH-2", "review_text": "-- define cascade errors when you first use the phrase -- basic english grammar could be fixed but is not interfering with understanding -- what is the early stopping criterion in Alg 1? -- did you try any other values for the initial threshold and decay factor? -- At the end of Section 4.1 DROP: \", but not the same.\" - I can't parse what this last clause is supposed to mean. --the diff sum example in table 2 is confusing; the program appears to sum up the numbers but the result is a subtraction without a sum operation in it. Would be clearer to show the sum in the result line as well rather than distribute the subtraction. Also, shouldn't it be diff(9, sum(10, 12))? -- I think you should pull at least some commentary about the constant used in Table 3 from Appendix B and include it in the main paper (or at least mention Appendix B is the place to look). Can you add a table in an appendix showing the complete list of operators used? -- Nice results in Table 4 on the dev set. Are there Test set results as well? -- The organization of the Baselines 4.3 section and the Results 4.4 is confusing. For example, you mention that you test different variants of NeRd, operator variants, and mathqa, but then the results are not mentioned for these experiments until the next page. I found myself immediately looking for the numbers/results when you introduce the experiment. I would pair your experiment description with the results rather than grouping all experiment descriptions and then grouping all results, especially when the order of the experiment descriptions does not match the order of the results presented. For example, in baselines you discuss training variants and then operators. Then in Results you discuss operators before variants. It is too disconnected and makes the reader jump around a bunch. Same goes for the drop baselines where you mention a bunch of models, and I would prefer the Results/discussion paired with each one, rather than having to wait for it down below. -- Overall it seems like a solid work; good empirical results showing improvements of each purported contribution. The model itself is a relatively simple construction of basic component, but the combination with the DSL is intuitive and makes sense. I don't think the novelty in model here is the main selling point anyways; the training variants and the demonstration of how well a DSL approach can do combined with previously introduced methods. -- I find the model description to be slightly unclear. In Fig 1 for example there is an arrow that connects passage to compositional programs. What does that arrow represent? I think you should elaborate on how the attention over the encoded text interacts with the attention over previously generated tokens. Equations would make this far more explicit as is I am left with a lot of questions on how to implement your model. Maybe you can add to your appendix? Or release your code? That is mentioned either.", "rating": "6: Weak Accept", "reply_text": "Thanks for your constructive feedback ! We have incorporated your comments in our revision , and we respond to your questions below : -- define cascade errors when you first use the phrase We use `` cascade error '' to refer to errors caused by the previous phase in a pipeline . In our case , we use this term to refer to the errors caused by data preprocessing for semantic parsing approaches , which parses the text into structured representations using tools such as SRL . We revised the paper to add a description in the introduction , where we first use this phrase . -- basic english grammar could be fixed but is not interfering with understanding We have done another round of proof-reading and updated the paper accordingly . -- what is the early stopping criterion in Alg 1 ? We perform early stopping when both exact match and F1 scores on the development set do not improve for two consecutive training iterations . We have updated Appendix D.2 to make this point clearer . -- did you try any other values for the initial threshold and decay factor ? We have also tried other values for the initial threshold and decay factor , and we found that the specific values do not matter much for the final results as long as they are in a good range . Specifically , values in [ 0.2 , 0.5 ] work well for both the initial threshold and decay factor ( we tried 1/5 , 1/4 , 1/3 , 1/2 within this range ) . Values larger than 0.5 will slow down training , and values smaller than 0.2 will decrease the quality of the final model . -- At the end of Section 4.1 DROP : `` , but not the same . '' - I ca n't parse what this last clause is supposed to mean . We have rephrased the sentence to be `` F1 score , which gives partial credits to a prediction that is not exactly the same as the ground truth , but overlaps with it \u201d . For example , if the ground truth is `` Barrack Obama '' , and the prediction is `` Obama '' , the exact match score will be zero , while the F1 score is larger than zero . -- the diff sum example in table 2 is confusing . Thanks for catching the problem ! It is indeed a typo . We have fixed it in the revision , and modified the explanation accordingly to make it clearer . -- Discussion of the complete operator and constant lists . For DROP , we have included the complete constant list in Appendix B . Note that MathQA covers a wide range of mathematical questions , and in their public dataset , they released the complete lists with 58 operators and 23 constants , which could be too long to include in the paper . Therefore , we added a description in Appendix B , and refer to their paper and public dataset for complete details . -- Nice results in Table 4 on the dev set . Are there Test set results as well ? Due to time constraint , we only evaluated locally on dev set at the time of submission . We submitted to the DROP server later , and the test result is better ( +1.18 % on F1 and +1.37 % on Exact Match ) than other baselines . -- The organization of the Baselines 4.3 section and the Results 4.4 is confusing ... Thanks for the suggestions ! We swapped the descriptions of operator variants and training variants in Section 4.3 , so that the order in Section 4.3 matches the order in Section 4.4 now . The main reason why we did not pair the descriptions with results is for paper space arrangement . Given that our paper has several tables of different sizes , separating them out is sometimes sub-optimal in terms of space usage . Therefore , we tentatively keep the section organization as it is in this revision ; however , in our camera ready version , we will try to re-organize these two sections if it looks better . -- Overall it seems like a solid work ; good empirical results showing improvements of each purported contribution ... Thanks for appreciating our work ! We agree that the neural architecture of each component in our model , i.e. , BERT as the encoder and an LSTM with attention as the program decoder , is not our main contribution . Instead , as pointed out by Reviewer 2 , the key novelty is `` This paper presents a semantic parser that operates over passages of text instead of a structured data source . This is the first time anyone has demonstrated such a semantic parser '' . To achieve this , we made several technical contributions to address the challenges in designing such a model , for example , the introduction of span selection operators so that compositional reasoning can be applied over text . -- I find the model description to be slightly unclear ... The arrow means that the program can directly select spans from the passage by calling the PASSAGE_SPAN operator with the indices of start and end token ; on the contrary , existing neural semantic parsers require an additional structured parser . We have written the detailed equations in Appendix C , including the attention mechanism . We will open source our code with our camera ready version ."}}