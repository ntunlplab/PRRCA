{"year": "2021", "forum": "OCm0rwa1lx1", "title": "Addressing Some Limitations of Transformers with Feedback Memory", "decision": "Reject", "meta_review": "This paper focuses on the limitations of the transformer architecture as an autoregressive model. The paper is relatively easy to follow. Though most reviewers find the paper interesting, the idea is not very novel. The introduction of sequential-ness to Transformer is good, though it also slow things down especially as the sequence gets longer.\n\nAn extensive set of experiments are performed, though the results are not entirely convincing. The authors are encouraged to add more ablative experiments, efficiency analysis, and large-scale results.", "reviews": [{"review_id": "OCm0rwa1lx1-0", "review_text": "The authors try to identify several problems in the Transformer model and modify the model architecture . Major : 1.The authors argue that for any position k and layer l , the standard Transformer can only access previous positions ( < k ) and lower layers ( < l ) . Instead , the authors propose to leverage > l layers for < k positions . First , apparently , compared to standard Transformer , the training of this model ( teacher forcing setting ) is much slower as the computation of any positions requires the whole forward outputs of all previous positions ( Standard Transformer run positions together ( in parallel ) ) . The author should demonstrate the training efficiency of the proposed model . 2.As far as I know , for other architectures , such as deep RNN ( LSTM ) or convseq , as the same as Transformer , the computations of any position k and layer l only access to previous positions ( < k ) and lower layers ( < l ) . Therefore , I think the authors should discuss the problems in those model architectures and test their proposal in more settings . 3.The strong requirement of a belief state in a model architecture is not convincing evidence to me . Or I can also view the ffn outputs at any ( position , layer ) as a virtual belief state . I also have difficulty understanding the arguments in the 'Maintaining a Belief State ' paragraph for concrete reasons . I hope the author could pay more attention to describing the motivation behind . 4.The authors argue that the proposed model is memory efficient than the standard Transformer , but this seems to be not a fair comparison . They share the key and values across different layers as in Albert , Universal Transformer , and DEQ , but fail to connect to these previous works . This trick can not be viewed as a contribution to the paper . 5.Regarding experiments and comparisons . a.If you need to highlight long memory tasks ( table 1 ) , please include the Transformer-XL , sparse Transformer into the comparison , which are very typical baselines in this scenario . b.For the experiment in 4.2.1 , the single decoding layer setting , what is the difference between your model and Transformer ? In such a setting , both model access to all previous states . Where does the benefit come from ? c. In section 4.2.1 , in the main body , you write the performance of your model ( 12-layer encoder + 12-layer decoder ) is 29.0 . But in Table 2 , you write the performance of your model is 29.5 , but the baseline models are only ( 6-layer encoder + 6-layer decoder ) d. If you need to highlight the fast decoding , please include the non-autoregressive models and linear transformers as baselines . Minor : 1.In the 2nd paragraph , the 'feedforward nature ' is not clear . Overall : The general problem that the authors want to solve is not very clear or very well-motivated . The experimental comparisons and baselines are not adequate . There is much room for better paper writing and presentation .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review ! We respectfully disagree with several points raised in the review , such as : - * \u201c For the experiment in 4.2.1 , the single decoding layer setting , what is the difference between your model and Transformer ? In such a setting , both model access to all previous states. \u201d * \u2192 A standard transformer would only have access to token embeddings when computing output representations of layer 1 . - * \u201c such as deep RNN ( LSTM ) or convseq , as the same as Transformer , the computations of any position k and layer l only access to previous positions ( < k ) and lower layers ( < l ) \u201d * \u2192 Actually , RNNs can only access their own layer ( let \u2019 s call it L ) and the layer immediately below ( L-1 ) . - * \u201c They share the key and values across different layers as in Albert , Universal Transformer , and DEQ , but fail to connect to these previous works. \u201d * \u2192 This is incorrect . We share keys and values ( * * activations * * ) , while these listed models share * * parameters * * across layers . - * \u201c the part about feedforward nature is not clear \u201d * \u2192 Transformers are Feedforward networks , and RNNs are not . Respectfully , we would like to clarify a few points that could help make our contributions more clear . * * re : How Feedforward Networks and RNNs operate- * * A feedforward neural network ( re : \u201c the part about feedforward nature is not clear \u201d ) is a neural network where the connections between neurons only move in one direction . MLPs , convolutions , and Transformers are Feedforward , but RNNs and LSTMs are not . Your statement about RNNs accessing all previous positions and all lower layers is sadly not true . RNNs can only access their own layer ( let \u2019 s call it L ) and the layer immediately below ( L-1 ) . Thus , the Feedback Transformer architecture we propose is distinct from RNNs . In Figure 5 , you can see an ablation where if Feedback Transformer was basically an RNN , it does not work very well . So it \u2019 s important to be able to leverage computation across all layers . * * re : Comparisons to Other Transformers regarding key/value sharing - * * This is incorrect . In our work , we propose to share keys and values which are activations of the model , while ALBERT or Universal Transformers propose to share parameters across layers . For many ( most ? ) use cases , activations actually use more memory and compute than parameters . * * re : what is Belief States- * * We want to clarify that we don \u2019 t mean \u201c belief state \u201d as an abstract concept , but belief state as in - what is the model \u2019 s current internal representation . Having a belief state is absolutely critical . For example , in a task about moving around a maze , the model needs to understand where it is in the maze to decide where to move next . This is also true in our new algorithmic task - without this ability , models can not update information about the world . You can see the clear performance difference between standard Transformers and Feedback Transformers . * * Detailed questions about Experiments and Comparisons : * * * * re : compare to Transformer XL and sparse Transformer * * Note that we compare to Transformer XL in real tasks , and compare to models with better results compared to both Transformer-XL and Sparse Transformer in Tables 3 and 4 . Tables 3 and 4 evaluate on models with much longer sequences than Table 1 , so we feel we have included the comparisons where it is most relevant . .Further , Transformers and Transformer-XL are equivalent when feeding the whole sequence at once ( up to the position encoding , though we also include relative position embeddings as used in Transformer-XL ) . Recent analysis on various Long-Range Transformers ( https : //arxiv.org/pdf/2011.04006.pdf ) indicates that our Transformer baseline is very strong . * * re : What is the difference between Feedback and Transformer in single layer setting ? * * This is also incorrect . In a one layer model , our architecture would access the output representation of layer 1 of positions < k when computing the output representation at position k of layer 1 . On the other hand , a standard transformer would only have access to token embeddings when computing output representations of layer 1 . * * re : fast decoding compared to non-autoregressive and linear Transformers * * - Kasai et al . ( 2020 ) show that a deep encoder with shallow decoder is a very strong baseline for efficient NMT systems , competitive with current non-autoregressive models . This is why we used this as a baseline , and not NAT models . We show that our architecture can even improve over this , leading to a strong model for efficient NMT . J. Kasai , N. Pappas , H. Peng , J . Cross , N. A. Smith . Deep Encoder , Shallow Decoder : Reevaluating the Speed-Quality Tradeoff in Machine Translation ."}, {"review_id": "OCm0rwa1lx1-1", "review_text": "The main topic of this paper is modification and enhancement of Transformers originally proposed in Vaswani \u2019 17 . As we all know , Transformers are now used as a core technology in a wide range of research communities such as natural language , vision , and speech . Many researchers aim to improve such core technology since it might provide a high impact to the communities . Thus , tons of papers propose a wide variety of modifications for Transformers in recent years . In this perspective , this paper can be categorized as one of such papers . Therefore , the audience and influence of this paper could be significantly broader . This paper focuses on the limitations of the Transformer architecture as an autoregressive model . This paper points out two drawbacks . One is the original Transformers do not handle the higher layer representations of the past states that have already been computed as a viewpoint of the autoregressive model . The other is that the model depth bounds the number of transformations possible on the input . This paper then proposes a method called \u201c Feedback transformers \u201c that can effectively overcome such drawbacks by explicitly incorporating all the past state representations , including higher-layer representations , when using transformers as an auto-regressive sequential generator . The top-level concept is rather straightforward and can be easily noticeable by many researchers in some sense , nothing innovative or unique . From this perspective , it seems that this paper is incremental study rather than an innovative one . However , the idea of injecting auto-regressive computation is the somewhat totally counter concept for the original Transformers since Transformers try to significantly reduce the computational cost on the specialized computational environment like GPUs by ignoring the auto-regressive nature . Although the proposed method does not obey the original concept of Transformers , the findings from this paper 's experiments are very impressive . I think the findings in this paper can help many researchers as a new insight into the community . In my feeling , this is basically an insightful paper . The following are the questions/concerns of this paper . 1 , The implicit explanation for the target situation The discussion in this paper only focuses on the auto-regressive generation or sequentially predicting tokens one-by-one . However , the Transformer architectures are also popular to be used in many other situations , such as masked language models like BERT . Unfortunately , the current version does not explicitly distinguish how the Transformer architectures are used for . Some readers might misunderstand that the discussion of this paper could include such a situation . Even if not so much , the authors should clearly state the target of their claims and discussions at the very beginning of this paper . 2 , Calculation cost The calculation cost is one of the main discussion points in the proposed method . However , there is no clear experimental results shown about this part . This is a clear disadvantage of this paper . 3 , Intuition of additional parameters w^l appeared in Eq.1 . ( Ablation study ) The proposed method suggests using the weighted sum of all the hidden vectors in all the layers . However , there is no reasonable explanation about intuition for this introduction . We have many other possible choices . This paper does not discuss such a possible variant at all . To better understand the proposed method , the authors should provide a certain amount of ablation studies . For example , what would happen if we used all the hidden vectors independently , not just weighted summing ups them . Moreover , what would happen if just average them ( without weighting factor ) , etc . 4 , I am not totally convinced that MT and LM 's results are really significant improvements from the original Transformers or the comparative previous methods . Regarding the WMT experiments , the one-point BLEU difference can be often observed by just changing random seeds in the identical method . The authors should somehow provide additional evidence that the proposed method significantly differs from the baseline methods . I am willing to change my score if I got reasonable answers for all the questions and concerns written in the above reviews .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your detailed review ! We have responded to each point below and included several additional experimental results to answer your questions quantitatively . Let us know if you have additional questions or suggestions , thanks ! * * re : The implicit explanation for the target situation * * In the Introduction , we state that we focus on these limitations only for Transformers as autoregressive models , which does not apply to BERT-style masked language model architectures . We added the sentence `` These limitations and our proposed solution target sequential token prediction tasks , such as language modeling or other auto-regressive generative tasks . '' We also edited the abstract to change the first sentence to `` Transformers have been successfully applied to sequential , auto-regressive tasks '' and clarify this point . * * re : Calculation cost * * We include a table here about the training and validation speed of the Feedback Transformer compared to the standard Transformer for language modeling , translation , and reinforcement learning . For language modeling , we measure on Wikitext-103 ( we compare a 8-layer Transformer against a 4-layer Feedback model of the same size.Both models have a fixed attention span of 512 , and trained on 32GPUs . With the Feedback model , we \u2019 re able to fit 2x larger batches in GPU memory . The inference is done with 1GPU ) . For translation , we measure on WMT En-De with 6 layer encoder and 2 layer decoder ( reporting training WPS on 8 GPU and inference WPS on 1 GPU ) . For RL , we report the training frame-per-second ( FPS ) on the maze navigation task ( using 20 CPU cores and 1 GPU ) . We will add this table into the main paper . |Task |Model | Training WPS | Inference WPS ` | | | | | | |LM ( wiki103 ) |Transformer | 296K | 592 | |LM ( wiki103 ) |Feedback |84.K | 2176 | |Translation |Transformer | 280K | 3190 | |Translation | Feedback |126K | 5410 | |RL Maze | Transformer | 22.3K | | |RL Maze |Feedback | 22.3K | | For Encoder-Decoder tasks , the Feedback Transformer is slower than the standard Transformer , but is faster at Inference as it uses less memory and can thus generate translations with larger batch sizes . For Language modeling , the Feedback Transformer is about 3x slower to train , but much faster at inference due to reduced memory cost ( from sharing key-values ) and reduced depth . For reinforcement learning tasks , the training must be online as well , so the Transformer and Feedback Transformer have the same speed . * * re : Intuition of additional parameters * * We investigated different ways to compose the memory in Figure 5 , which indicates that the Feedback Memory form is the strongest performing one . To answer your question about averaging , we added an additional experiment in Section 6.4 ( see Figure 8 ) . Representations from higher layers work better as memory , confirming our assumption of the importance of higher level representations in the Feedback Transformer . Simply averaging all layers together works reasonably well , but the weighted sum approach matches the best performance because it can adopt to select any of the layers . * * re : Significant Improvements * * For Wikitext-103 , we report 18.3 PPL for the Feedback Transformer . If we use our codebase to run a standard Transformer , the performance is only 19.9 PPL , which is substantially worse . This baseline is new , and we have updated Table 3 . We have conducted additional experiments to understand the variance on translation to answer your specific point about the importance of a 1-BLEU difference . On WMT En-De , we train three models with different seeds and see that the standard deviation in BLEU is around * * 0.15 * * for the baseline standard Transformer and around * * 0.12 * * for the Feedback Transformer . This standard deviation is fairly stable across models with varying decoder layers . We will display standard deviation in Figure 4 ( left ) in the main paper . We conclude based on this investigation that the improvement from Feedback Transformer much stronger performance with shallow models is statistically significant . For example , with 1 decoder layer , the Feedback Transformer has about 1 BLEU improvement . At full model size , the Feedback Transformer outperforms the standard Transformer , but marginally ( 0.2 BLEU improvement ) ."}, {"review_id": "OCm0rwa1lx1-2", "review_text": "> Summary : This paper proposes some changes to the classical Transformer architecture to address its major limitations , such as limited access to higher-level representations . It specifically introduces recurrence to the Transformer architecture by feeding the activations of all previous time steps to a later time step ( in the form of self-attention ) . Empirical results on language modeling and small-scale RL tasks seem to suggest the usefulness of doing do . -- Post-rebuttal thoughts : See the comment block below . -- Overall : I found this paper interesting and relatively easy to follow . The idea is simple , and seems useful , although I do find some arguments handwavy and not quite convincing ( e.g. , the `` maintaining a belief state '' one ) . It is unclear to me how exactly the efficiency compares , though the authors did report the # of days on WikiText-103 ( see my detailed question below ) . I overall think that this could be a good architectural improvement on the condition that the authors provide more details . Pros : 1.Simple idea and the flow of the paper is easy to follow . 2.An extensive set of experiments to verify both the usefulness of the Feedback Transformer and the limitations that the authors hypothesize to be true for transformers . Cons : 1.The introduction of sequential-ness to Transformer is good but obviously would slow things down especially as the sequence gets longer . The authors reported on this very briefly , but I think it is an important enough aspect to warrant more analysis . 2.Lack of certain ablative settings in the experiments ( which is unavoidable in a certain sense , given that the paper proposes various changes to the architecture ) . - Additional comments and questions : 1 . The core of the hypothesis on the value of high-level representation feedback is the autoregressiveness , is this correct ? As the paper claims , typical Transformers are restricted from `` taking full advantage of the input 's sequential property '' because they ca n't access the higher-level representations of previous time steps . I have two questions in this respect , and wonder if the authors have verified this ( if not , I think you probably should ( ? ) ) : 1 ) Would you expect a `` feedback LSTM '' to work better than an LSTM as well ? In other words , an LSTM that when computing $ h_t^ { ( l ) } $ of time $ t $ at layer $ l $ , uses $ h_ { < t } ^ { ( L ) } $ just like in the Feedback Transformer ? 2 ) In pixel sequences like CIFAR-10 density modeling ( e.g. , see Sparse Transformer by Child et al.2019 ) , where the autoregressiveness is not rather obvious ( e.g. , you can do column-based or row-based , or even Hilbert curves ) , does Feedback Transformer still help ? If so , then it means higher-level representation is not exactly a `` temporal '' phenomenon , because there 's nothing in pixels that 's temporal ... 2 . Regarding the sharing of keys and values in a Feedback Transformer is the motivation for this just to speed up the architecture ? How well does Feedback Transformer perform without this sharing , and how slow would it be ? 3.I 'm confused about the `` maintaining a belief state '' paragraph . The authors claim that Transformers are limited by `` only a fixed number of transformations can be applied to its internal states '' . But are n't those internal states already aggregated by lower levels ? Why might more transformations be better ? Ca n't one simply increase the number of layers of a Transformer ? I also do n't see the logical connection between this claim and the end of this paragraph : `` This means Transformers can not maintain an internal state for a long time if it has to be frequently updated '' . Can the authors clarify on this part ? 4.In cases especially like NMT , where decoders are trained in parallel ( because at training time , the decoder is trained just like an LM ) and used for inference in sequence ( at test time , it generates tokens one by one ) , would n't it make more sense to pre-train a classical Transformer ( with no feedback memory ) and then directly use , or probably with slight fine-tuning , the feedback version of it at inference ? Is this possible ? 5.One of the most important thing that I believe the current version is missing is a more comprehensive analysis of the efficiency , which seems to be an important drawback ( if any ) . I noticed that the authors claim that key-value sharing compensated for the loss on parallelism but by how much exactly ? Specifically , I 'd appreciate if the authors can provide an analysis of at least some of the following : 1 ) How many GPUs ( and what sort of GPU ) did you use to train your models , e.g. , for WikiText-103 and for char-PTB ? Did you use the same setting for the classical Transformer ? ( The 1.2 vs. 3.5 days on WikiText-103 is still a large gap , almost 3x slower ... ) 2 ) How does the efficiency of the training ( not in terms of days of training , but ms per batch ) scale as you use longer and longer sequences ? I 'm asking because I noticed in Table 7 that these sequence lengths are still pretty short ; e.g. , I believe SOTA char-PTB uses length > 256 and WikiText-103 uses length > 1024 at inference . Does Feedback Transformer further improve when you use longer sequences ? 3 ) If the `` high-level representation '' of Transformers is indeed a major limitation , does a deeper ( but still the same # of parameters , so probably smaller hidden dimensionality ) Transformer perform better , because it can have `` more updates '' to its internal state ? Or maybe a weight-sharing Transformer ? How does the efficiency vs. performance compare in these cases ? 6.Did you train all of the Feedback Transformers from scratch ( i.e. , ` train_step ` =0 ) , or did you warm-up/pretrain the models ? 7.Have you ever tried non-toy-scale RL tasks ? I think this proposed architecture would be very useful in these very sequential settings ( e.g. , in robotics , where the data stream actually has temporal dimensions ) , and these large-scale RL tasks ( e.g. , Doom FPS game , etc . ) could make the paper even stronger . 8.I 'd suggest expanding Tables 3 and 4 there are plenty of prior works that evaluated on these two datasets and it 'd be worth it to cite them to compare . In addition , for Table 3 , does increasing parameters further improve the performance ? It is impressive that you can achieve the same level of result as Transformer-XL with only half of the parameters , which seems to suggest there 's still room for improvement ? 9.Section 4.3 : `` we first '' > `` first '' Overall , I think this paper presents relatively solid results , but there are some key ablative settings , efficiency details & analysis , and large-scale RL results that are missing . I 'm putting a 5 on this paper for now , but I look forward to the authors ' response and am happy to adjust my score positively once my questions are further clarified .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comprehensive questions and detailed reading of the paper ! Below , we \u2019 ve responded to each of your questions and included several new experiments . * * re : Speed as Sequence gets Longer * * For long sequences , we do not process from start to end . Instead , for both standard Transformer and Feedback Transformer , we process the sequence in blocks of fixed size in the same way as TransformerXL , so the speed difference between the two remains constant , regardless of the data length . With regards to longer attention spans , it will not slow down Feedback Transformer more than standard Transformer because both process attention spans in parallel . At inference , both decode with the same speed as decoding proceeds token-by-token no matter what . * * re : Ablations * * We have an ablation study in Sec 4.3.2 . In addition , we also have added a number of new experiments in our review response . Let us know if you have specific suggestions ! * * re : feedback LSTM and temporal phenomenon * * For Feedback LSTM , there is an existing work exploring it and shown improvements [ 1 ] ( we mentioned this in the related work section , but we will make it clearer ) . For your second question , yes , it does n't have to be temporal only . As long as computation is autoregressive in a certain dimension ( e.g. , temporal or spatial ) , the Feedback mechanism can be applied . [ 1 ] Chung et.al. , Gated Feedback Recurrent Neural Networks , 2015 https : //arxiv.org/abs/1502.02367 * * re : motivation to share keys and values * * The motivation is exactly as you state , for efficiency . Note that the standard Transformer can not share keys and values ( because they are computed from different representations at different layers ) , so this is only possible for the Feedback Transformer . Sharing improves the speed around 3x for training and also reduces the memory required by the model , which contributes to faster inference speed as well ( the memory can instead be used to increase batch size ) . In our ablation experiments , we did not find performance difference between sharing and not sharing in the Feedback Transformer . * * re : maintaining belief state and model depth * * Good question . One way to think about it is - how many nonlinear functions can a model apply on any state ? The Feedback Transformer has recursive computation , so it can continuously iterate . The standard Transformer can use each layer to change its internal state . Yes , a deeper Transformer can manipulate its internal state more , but this has clear limitations . One limitation is with regard to sequence length . Can we really scale model depth with sequence length ? What if the model needs to change internal state a large number of times to carefully track something ? This outstrips the rate at which we can grow the # of layers and train deep models stably . This is illustrated in the existing maze task , but we have added an algorithmic task involving code execution to further illustrate this idea . It 's added in the paper in Section 4.1.2 . The model gets some variables and the state of those variables is constantly being updated . You can see that the deeper Transformer is better than the shallow Transformer , but LSTM does much better than both , and Feedback easily does the best . This is because the Transformer needs to constantly track the variable values , and quickly runs out of capacity . We will put this data online for others to try as well . * * re : finetuning from standard transformer * * We have explored previously training a standard Transformer model and then finetuning into the Feedback architecture - it is definitely possible . We add a table below indicating the performance of such a finetuning strategy . |Model |Performance| | | | |Transformer Baseline | 21.2 | |Feedback Transformer | 19.7 | |Transformer Finetuned to Feedback | 19.8 | However , since the Feedback Transformer substantially changes the way the model works ( key and value vectors feeding to self-attention sublayers completely change ) , you can not train with one architecture and apply another at inference time - thus , it takes time to do the finetuning and create the Feedback architecture . With the speedup from sharing keys and values , we found no real benefit from finetuning compared to training a Feedback Transformer fully from scratch . Other ways to speed up convergence still apply in the translation setting , for example , initializing with pretrained embeddings is totally possible , but would improve the convergence speed of the standard Transformer as well , so we do not assess ."}, {"review_id": "OCm0rwa1lx1-3", "review_text": "# # # Summary This paper modifies transformers with feedback memory . Specifically , for each timestep , it merges hidden representations of all layers into a high-level single vector and stores it in memory . For the current timestep , it attends past memory vectors . The authors claim that in this way , low layers of the current timestep can utilize high-level representations of past timesteps . The authors show that the proposed models with shallow layers can achieve stronger performance than comparable transformers . However , it seems that the models need a much longer time to train . # # # Strengths * With feedback memory , the modified can speedup decoding ( autoregressive generation ) as shown in Figure 4 . * Since the proposed model can directly utilize previous high-level representations , it just needs a small size and shallow layers to achieve comparable performance as shown in Table 3 and Table 4 . # # # Weaknesses and Questions * Training time and inference speed are important for such practical models . It is better to complement these to Table 1/2/3/4 . It seems that the proposed needs to take a much longer time to train as the authors mentioned it in a sentence on page 8 . The authors can give more results and discussions so that future users can know whether to choose transformers with feedback memory according to their situations . * ( optional ) In Table 3/4 , how about feedback transformer that keeps the same number of layers and similar parameters as Trans-XL . Transformers usually can achieve better performance when the number of layers increases . It is just an optional discussion as feedback transformers seem to need much time to train and the rebuttal time is limited .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review ! We \u2019 ve included additional analysis to respond to your questions , and included several additional experiments overall in the paper to strengthen our work . Please let us know if you have additional questions . * * re : training and inference time analysis * * We include a table here about the training and validation speed of the Feedback Transformer compared to the standard Transformer for language modeling , translation , and reinforcement learning . For language modeling , we measure on Wikitext-103 ( we compare a 8-layer Transformer against a 4-layer Feedback model of the same size.Both models have a fixed attention span of 512 , and trained on 32GPUs . With the Feedback model , we \u2019 re able to fit 2x larger batches in GPU memory . The inference is done with 1GPU ) . For translation , we measure on WMT En-De with 6 layer encoder and 2 layer decoder ( reporting training WPS on 8 GPU and inference WPS on 1 GPU ) . For RL , we report the training frame-per-second ( FPS ) on the maze navigation task ( using 20 CPU cores and 1 GPU ) . We will add this table into the main paper . |Task |Model | Training WPS | Inference WPS ` | | | | | | |LM ( wiki103 ) |Transformer | 296K | 592 | |LM ( wiki103 ) |Feedback |84.K | 2176 | |Translation |Transformer | 280K | 3190 | |Translation | Feedback |126K | 5410 | |RL Maze | Transformer | 22.3K | | |RL Maze |Feedback | 22.3K | | For Encoder-Decoder tasks , the Feedback Transformer is slower than the standard Transformer , but is faster at Inference as it uses less memory and can thus generate translations with larger batch sizes . For Language modeling , the Feedback Transformer is about 3x slower to train , much faster at inference due to reduced memory cost ( from sharing key-values ) and reduced depth . For reinforcement learning tasks , the training must be online as well , so the Transformer and Feedback Transformer have the same speed . * * re : larger feedback models * * We will explore training larger Feedback Transformer models . For datasets like Wikitext-103 and WMT En-De , it is mainly a challenge of regularization , so we anticipate needing to tune the dropout parameters . We 'll add additional results in the final version of the paper based on this exploration ( as you mention , the training time is a challenge in the short-ish rebuttal period ) . However , we want to emphasize that the main contribution of the paper is a straightforward solution to two major limitations of the Transformer architecture ( limited access to higher level representations and inability to maintain state ) . By resolving these limitations , it 's possible to have a model that is smaller and shallower perform just as well . We 're happy to conduct additional experiments , but the point is n't really chasing state of the art performance by sweeping more , but a simple way to not have these limitations in a Transformer ."}], "0": {"review_id": "OCm0rwa1lx1-0", "review_text": "The authors try to identify several problems in the Transformer model and modify the model architecture . Major : 1.The authors argue that for any position k and layer l , the standard Transformer can only access previous positions ( < k ) and lower layers ( < l ) . Instead , the authors propose to leverage > l layers for < k positions . First , apparently , compared to standard Transformer , the training of this model ( teacher forcing setting ) is much slower as the computation of any positions requires the whole forward outputs of all previous positions ( Standard Transformer run positions together ( in parallel ) ) . The author should demonstrate the training efficiency of the proposed model . 2.As far as I know , for other architectures , such as deep RNN ( LSTM ) or convseq , as the same as Transformer , the computations of any position k and layer l only access to previous positions ( < k ) and lower layers ( < l ) . Therefore , I think the authors should discuss the problems in those model architectures and test their proposal in more settings . 3.The strong requirement of a belief state in a model architecture is not convincing evidence to me . Or I can also view the ffn outputs at any ( position , layer ) as a virtual belief state . I also have difficulty understanding the arguments in the 'Maintaining a Belief State ' paragraph for concrete reasons . I hope the author could pay more attention to describing the motivation behind . 4.The authors argue that the proposed model is memory efficient than the standard Transformer , but this seems to be not a fair comparison . They share the key and values across different layers as in Albert , Universal Transformer , and DEQ , but fail to connect to these previous works . This trick can not be viewed as a contribution to the paper . 5.Regarding experiments and comparisons . a.If you need to highlight long memory tasks ( table 1 ) , please include the Transformer-XL , sparse Transformer into the comparison , which are very typical baselines in this scenario . b.For the experiment in 4.2.1 , the single decoding layer setting , what is the difference between your model and Transformer ? In such a setting , both model access to all previous states . Where does the benefit come from ? c. In section 4.2.1 , in the main body , you write the performance of your model ( 12-layer encoder + 12-layer decoder ) is 29.0 . But in Table 2 , you write the performance of your model is 29.5 , but the baseline models are only ( 6-layer encoder + 6-layer decoder ) d. If you need to highlight the fast decoding , please include the non-autoregressive models and linear transformers as baselines . Minor : 1.In the 2nd paragraph , the 'feedforward nature ' is not clear . Overall : The general problem that the authors want to solve is not very clear or very well-motivated . The experimental comparisons and baselines are not adequate . There is much room for better paper writing and presentation .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review ! We respectfully disagree with several points raised in the review , such as : - * \u201c For the experiment in 4.2.1 , the single decoding layer setting , what is the difference between your model and Transformer ? In such a setting , both model access to all previous states. \u201d * \u2192 A standard transformer would only have access to token embeddings when computing output representations of layer 1 . - * \u201c such as deep RNN ( LSTM ) or convseq , as the same as Transformer , the computations of any position k and layer l only access to previous positions ( < k ) and lower layers ( < l ) \u201d * \u2192 Actually , RNNs can only access their own layer ( let \u2019 s call it L ) and the layer immediately below ( L-1 ) . - * \u201c They share the key and values across different layers as in Albert , Universal Transformer , and DEQ , but fail to connect to these previous works. \u201d * \u2192 This is incorrect . We share keys and values ( * * activations * * ) , while these listed models share * * parameters * * across layers . - * \u201c the part about feedforward nature is not clear \u201d * \u2192 Transformers are Feedforward networks , and RNNs are not . Respectfully , we would like to clarify a few points that could help make our contributions more clear . * * re : How Feedforward Networks and RNNs operate- * * A feedforward neural network ( re : \u201c the part about feedforward nature is not clear \u201d ) is a neural network where the connections between neurons only move in one direction . MLPs , convolutions , and Transformers are Feedforward , but RNNs and LSTMs are not . Your statement about RNNs accessing all previous positions and all lower layers is sadly not true . RNNs can only access their own layer ( let \u2019 s call it L ) and the layer immediately below ( L-1 ) . Thus , the Feedback Transformer architecture we propose is distinct from RNNs . In Figure 5 , you can see an ablation where if Feedback Transformer was basically an RNN , it does not work very well . So it \u2019 s important to be able to leverage computation across all layers . * * re : Comparisons to Other Transformers regarding key/value sharing - * * This is incorrect . In our work , we propose to share keys and values which are activations of the model , while ALBERT or Universal Transformers propose to share parameters across layers . For many ( most ? ) use cases , activations actually use more memory and compute than parameters . * * re : what is Belief States- * * We want to clarify that we don \u2019 t mean \u201c belief state \u201d as an abstract concept , but belief state as in - what is the model \u2019 s current internal representation . Having a belief state is absolutely critical . For example , in a task about moving around a maze , the model needs to understand where it is in the maze to decide where to move next . This is also true in our new algorithmic task - without this ability , models can not update information about the world . You can see the clear performance difference between standard Transformers and Feedback Transformers . * * Detailed questions about Experiments and Comparisons : * * * * re : compare to Transformer XL and sparse Transformer * * Note that we compare to Transformer XL in real tasks , and compare to models with better results compared to both Transformer-XL and Sparse Transformer in Tables 3 and 4 . Tables 3 and 4 evaluate on models with much longer sequences than Table 1 , so we feel we have included the comparisons where it is most relevant . .Further , Transformers and Transformer-XL are equivalent when feeding the whole sequence at once ( up to the position encoding , though we also include relative position embeddings as used in Transformer-XL ) . Recent analysis on various Long-Range Transformers ( https : //arxiv.org/pdf/2011.04006.pdf ) indicates that our Transformer baseline is very strong . * * re : What is the difference between Feedback and Transformer in single layer setting ? * * This is also incorrect . In a one layer model , our architecture would access the output representation of layer 1 of positions < k when computing the output representation at position k of layer 1 . On the other hand , a standard transformer would only have access to token embeddings when computing output representations of layer 1 . * * re : fast decoding compared to non-autoregressive and linear Transformers * * - Kasai et al . ( 2020 ) show that a deep encoder with shallow decoder is a very strong baseline for efficient NMT systems , competitive with current non-autoregressive models . This is why we used this as a baseline , and not NAT models . We show that our architecture can even improve over this , leading to a strong model for efficient NMT . J. Kasai , N. Pappas , H. Peng , J . Cross , N. A. Smith . Deep Encoder , Shallow Decoder : Reevaluating the Speed-Quality Tradeoff in Machine Translation ."}, "1": {"review_id": "OCm0rwa1lx1-1", "review_text": "The main topic of this paper is modification and enhancement of Transformers originally proposed in Vaswani \u2019 17 . As we all know , Transformers are now used as a core technology in a wide range of research communities such as natural language , vision , and speech . Many researchers aim to improve such core technology since it might provide a high impact to the communities . Thus , tons of papers propose a wide variety of modifications for Transformers in recent years . In this perspective , this paper can be categorized as one of such papers . Therefore , the audience and influence of this paper could be significantly broader . This paper focuses on the limitations of the Transformer architecture as an autoregressive model . This paper points out two drawbacks . One is the original Transformers do not handle the higher layer representations of the past states that have already been computed as a viewpoint of the autoregressive model . The other is that the model depth bounds the number of transformations possible on the input . This paper then proposes a method called \u201c Feedback transformers \u201c that can effectively overcome such drawbacks by explicitly incorporating all the past state representations , including higher-layer representations , when using transformers as an auto-regressive sequential generator . The top-level concept is rather straightforward and can be easily noticeable by many researchers in some sense , nothing innovative or unique . From this perspective , it seems that this paper is incremental study rather than an innovative one . However , the idea of injecting auto-regressive computation is the somewhat totally counter concept for the original Transformers since Transformers try to significantly reduce the computational cost on the specialized computational environment like GPUs by ignoring the auto-regressive nature . Although the proposed method does not obey the original concept of Transformers , the findings from this paper 's experiments are very impressive . I think the findings in this paper can help many researchers as a new insight into the community . In my feeling , this is basically an insightful paper . The following are the questions/concerns of this paper . 1 , The implicit explanation for the target situation The discussion in this paper only focuses on the auto-regressive generation or sequentially predicting tokens one-by-one . However , the Transformer architectures are also popular to be used in many other situations , such as masked language models like BERT . Unfortunately , the current version does not explicitly distinguish how the Transformer architectures are used for . Some readers might misunderstand that the discussion of this paper could include such a situation . Even if not so much , the authors should clearly state the target of their claims and discussions at the very beginning of this paper . 2 , Calculation cost The calculation cost is one of the main discussion points in the proposed method . However , there is no clear experimental results shown about this part . This is a clear disadvantage of this paper . 3 , Intuition of additional parameters w^l appeared in Eq.1 . ( Ablation study ) The proposed method suggests using the weighted sum of all the hidden vectors in all the layers . However , there is no reasonable explanation about intuition for this introduction . We have many other possible choices . This paper does not discuss such a possible variant at all . To better understand the proposed method , the authors should provide a certain amount of ablation studies . For example , what would happen if we used all the hidden vectors independently , not just weighted summing ups them . Moreover , what would happen if just average them ( without weighting factor ) , etc . 4 , I am not totally convinced that MT and LM 's results are really significant improvements from the original Transformers or the comparative previous methods . Regarding the WMT experiments , the one-point BLEU difference can be often observed by just changing random seeds in the identical method . The authors should somehow provide additional evidence that the proposed method significantly differs from the baseline methods . I am willing to change my score if I got reasonable answers for all the questions and concerns written in the above reviews .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your detailed review ! We have responded to each point below and included several additional experimental results to answer your questions quantitatively . Let us know if you have additional questions or suggestions , thanks ! * * re : The implicit explanation for the target situation * * In the Introduction , we state that we focus on these limitations only for Transformers as autoregressive models , which does not apply to BERT-style masked language model architectures . We added the sentence `` These limitations and our proposed solution target sequential token prediction tasks , such as language modeling or other auto-regressive generative tasks . '' We also edited the abstract to change the first sentence to `` Transformers have been successfully applied to sequential , auto-regressive tasks '' and clarify this point . * * re : Calculation cost * * We include a table here about the training and validation speed of the Feedback Transformer compared to the standard Transformer for language modeling , translation , and reinforcement learning . For language modeling , we measure on Wikitext-103 ( we compare a 8-layer Transformer against a 4-layer Feedback model of the same size.Both models have a fixed attention span of 512 , and trained on 32GPUs . With the Feedback model , we \u2019 re able to fit 2x larger batches in GPU memory . The inference is done with 1GPU ) . For translation , we measure on WMT En-De with 6 layer encoder and 2 layer decoder ( reporting training WPS on 8 GPU and inference WPS on 1 GPU ) . For RL , we report the training frame-per-second ( FPS ) on the maze navigation task ( using 20 CPU cores and 1 GPU ) . We will add this table into the main paper . |Task |Model | Training WPS | Inference WPS ` | | | | | | |LM ( wiki103 ) |Transformer | 296K | 592 | |LM ( wiki103 ) |Feedback |84.K | 2176 | |Translation |Transformer | 280K | 3190 | |Translation | Feedback |126K | 5410 | |RL Maze | Transformer | 22.3K | | |RL Maze |Feedback | 22.3K | | For Encoder-Decoder tasks , the Feedback Transformer is slower than the standard Transformer , but is faster at Inference as it uses less memory and can thus generate translations with larger batch sizes . For Language modeling , the Feedback Transformer is about 3x slower to train , but much faster at inference due to reduced memory cost ( from sharing key-values ) and reduced depth . For reinforcement learning tasks , the training must be online as well , so the Transformer and Feedback Transformer have the same speed . * * re : Intuition of additional parameters * * We investigated different ways to compose the memory in Figure 5 , which indicates that the Feedback Memory form is the strongest performing one . To answer your question about averaging , we added an additional experiment in Section 6.4 ( see Figure 8 ) . Representations from higher layers work better as memory , confirming our assumption of the importance of higher level representations in the Feedback Transformer . Simply averaging all layers together works reasonably well , but the weighted sum approach matches the best performance because it can adopt to select any of the layers . * * re : Significant Improvements * * For Wikitext-103 , we report 18.3 PPL for the Feedback Transformer . If we use our codebase to run a standard Transformer , the performance is only 19.9 PPL , which is substantially worse . This baseline is new , and we have updated Table 3 . We have conducted additional experiments to understand the variance on translation to answer your specific point about the importance of a 1-BLEU difference . On WMT En-De , we train three models with different seeds and see that the standard deviation in BLEU is around * * 0.15 * * for the baseline standard Transformer and around * * 0.12 * * for the Feedback Transformer . This standard deviation is fairly stable across models with varying decoder layers . We will display standard deviation in Figure 4 ( left ) in the main paper . We conclude based on this investigation that the improvement from Feedback Transformer much stronger performance with shallow models is statistically significant . For example , with 1 decoder layer , the Feedback Transformer has about 1 BLEU improvement . At full model size , the Feedback Transformer outperforms the standard Transformer , but marginally ( 0.2 BLEU improvement ) ."}, "2": {"review_id": "OCm0rwa1lx1-2", "review_text": "> Summary : This paper proposes some changes to the classical Transformer architecture to address its major limitations , such as limited access to higher-level representations . It specifically introduces recurrence to the Transformer architecture by feeding the activations of all previous time steps to a later time step ( in the form of self-attention ) . Empirical results on language modeling and small-scale RL tasks seem to suggest the usefulness of doing do . -- Post-rebuttal thoughts : See the comment block below . -- Overall : I found this paper interesting and relatively easy to follow . The idea is simple , and seems useful , although I do find some arguments handwavy and not quite convincing ( e.g. , the `` maintaining a belief state '' one ) . It is unclear to me how exactly the efficiency compares , though the authors did report the # of days on WikiText-103 ( see my detailed question below ) . I overall think that this could be a good architectural improvement on the condition that the authors provide more details . Pros : 1.Simple idea and the flow of the paper is easy to follow . 2.An extensive set of experiments to verify both the usefulness of the Feedback Transformer and the limitations that the authors hypothesize to be true for transformers . Cons : 1.The introduction of sequential-ness to Transformer is good but obviously would slow things down especially as the sequence gets longer . The authors reported on this very briefly , but I think it is an important enough aspect to warrant more analysis . 2.Lack of certain ablative settings in the experiments ( which is unavoidable in a certain sense , given that the paper proposes various changes to the architecture ) . - Additional comments and questions : 1 . The core of the hypothesis on the value of high-level representation feedback is the autoregressiveness , is this correct ? As the paper claims , typical Transformers are restricted from `` taking full advantage of the input 's sequential property '' because they ca n't access the higher-level representations of previous time steps . I have two questions in this respect , and wonder if the authors have verified this ( if not , I think you probably should ( ? ) ) : 1 ) Would you expect a `` feedback LSTM '' to work better than an LSTM as well ? In other words , an LSTM that when computing $ h_t^ { ( l ) } $ of time $ t $ at layer $ l $ , uses $ h_ { < t } ^ { ( L ) } $ just like in the Feedback Transformer ? 2 ) In pixel sequences like CIFAR-10 density modeling ( e.g. , see Sparse Transformer by Child et al.2019 ) , where the autoregressiveness is not rather obvious ( e.g. , you can do column-based or row-based , or even Hilbert curves ) , does Feedback Transformer still help ? If so , then it means higher-level representation is not exactly a `` temporal '' phenomenon , because there 's nothing in pixels that 's temporal ... 2 . Regarding the sharing of keys and values in a Feedback Transformer is the motivation for this just to speed up the architecture ? How well does Feedback Transformer perform without this sharing , and how slow would it be ? 3.I 'm confused about the `` maintaining a belief state '' paragraph . The authors claim that Transformers are limited by `` only a fixed number of transformations can be applied to its internal states '' . But are n't those internal states already aggregated by lower levels ? Why might more transformations be better ? Ca n't one simply increase the number of layers of a Transformer ? I also do n't see the logical connection between this claim and the end of this paragraph : `` This means Transformers can not maintain an internal state for a long time if it has to be frequently updated '' . Can the authors clarify on this part ? 4.In cases especially like NMT , where decoders are trained in parallel ( because at training time , the decoder is trained just like an LM ) and used for inference in sequence ( at test time , it generates tokens one by one ) , would n't it make more sense to pre-train a classical Transformer ( with no feedback memory ) and then directly use , or probably with slight fine-tuning , the feedback version of it at inference ? Is this possible ? 5.One of the most important thing that I believe the current version is missing is a more comprehensive analysis of the efficiency , which seems to be an important drawback ( if any ) . I noticed that the authors claim that key-value sharing compensated for the loss on parallelism but by how much exactly ? Specifically , I 'd appreciate if the authors can provide an analysis of at least some of the following : 1 ) How many GPUs ( and what sort of GPU ) did you use to train your models , e.g. , for WikiText-103 and for char-PTB ? Did you use the same setting for the classical Transformer ? ( The 1.2 vs. 3.5 days on WikiText-103 is still a large gap , almost 3x slower ... ) 2 ) How does the efficiency of the training ( not in terms of days of training , but ms per batch ) scale as you use longer and longer sequences ? I 'm asking because I noticed in Table 7 that these sequence lengths are still pretty short ; e.g. , I believe SOTA char-PTB uses length > 256 and WikiText-103 uses length > 1024 at inference . Does Feedback Transformer further improve when you use longer sequences ? 3 ) If the `` high-level representation '' of Transformers is indeed a major limitation , does a deeper ( but still the same # of parameters , so probably smaller hidden dimensionality ) Transformer perform better , because it can have `` more updates '' to its internal state ? Or maybe a weight-sharing Transformer ? How does the efficiency vs. performance compare in these cases ? 6.Did you train all of the Feedback Transformers from scratch ( i.e. , ` train_step ` =0 ) , or did you warm-up/pretrain the models ? 7.Have you ever tried non-toy-scale RL tasks ? I think this proposed architecture would be very useful in these very sequential settings ( e.g. , in robotics , where the data stream actually has temporal dimensions ) , and these large-scale RL tasks ( e.g. , Doom FPS game , etc . ) could make the paper even stronger . 8.I 'd suggest expanding Tables 3 and 4 there are plenty of prior works that evaluated on these two datasets and it 'd be worth it to cite them to compare . In addition , for Table 3 , does increasing parameters further improve the performance ? It is impressive that you can achieve the same level of result as Transformer-XL with only half of the parameters , which seems to suggest there 's still room for improvement ? 9.Section 4.3 : `` we first '' > `` first '' Overall , I think this paper presents relatively solid results , but there are some key ablative settings , efficiency details & analysis , and large-scale RL results that are missing . I 'm putting a 5 on this paper for now , but I look forward to the authors ' response and am happy to adjust my score positively once my questions are further clarified .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comprehensive questions and detailed reading of the paper ! Below , we \u2019 ve responded to each of your questions and included several new experiments . * * re : Speed as Sequence gets Longer * * For long sequences , we do not process from start to end . Instead , for both standard Transformer and Feedback Transformer , we process the sequence in blocks of fixed size in the same way as TransformerXL , so the speed difference between the two remains constant , regardless of the data length . With regards to longer attention spans , it will not slow down Feedback Transformer more than standard Transformer because both process attention spans in parallel . At inference , both decode with the same speed as decoding proceeds token-by-token no matter what . * * re : Ablations * * We have an ablation study in Sec 4.3.2 . In addition , we also have added a number of new experiments in our review response . Let us know if you have specific suggestions ! * * re : feedback LSTM and temporal phenomenon * * For Feedback LSTM , there is an existing work exploring it and shown improvements [ 1 ] ( we mentioned this in the related work section , but we will make it clearer ) . For your second question , yes , it does n't have to be temporal only . As long as computation is autoregressive in a certain dimension ( e.g. , temporal or spatial ) , the Feedback mechanism can be applied . [ 1 ] Chung et.al. , Gated Feedback Recurrent Neural Networks , 2015 https : //arxiv.org/abs/1502.02367 * * re : motivation to share keys and values * * The motivation is exactly as you state , for efficiency . Note that the standard Transformer can not share keys and values ( because they are computed from different representations at different layers ) , so this is only possible for the Feedback Transformer . Sharing improves the speed around 3x for training and also reduces the memory required by the model , which contributes to faster inference speed as well ( the memory can instead be used to increase batch size ) . In our ablation experiments , we did not find performance difference between sharing and not sharing in the Feedback Transformer . * * re : maintaining belief state and model depth * * Good question . One way to think about it is - how many nonlinear functions can a model apply on any state ? The Feedback Transformer has recursive computation , so it can continuously iterate . The standard Transformer can use each layer to change its internal state . Yes , a deeper Transformer can manipulate its internal state more , but this has clear limitations . One limitation is with regard to sequence length . Can we really scale model depth with sequence length ? What if the model needs to change internal state a large number of times to carefully track something ? This outstrips the rate at which we can grow the # of layers and train deep models stably . This is illustrated in the existing maze task , but we have added an algorithmic task involving code execution to further illustrate this idea . It 's added in the paper in Section 4.1.2 . The model gets some variables and the state of those variables is constantly being updated . You can see that the deeper Transformer is better than the shallow Transformer , but LSTM does much better than both , and Feedback easily does the best . This is because the Transformer needs to constantly track the variable values , and quickly runs out of capacity . We will put this data online for others to try as well . * * re : finetuning from standard transformer * * We have explored previously training a standard Transformer model and then finetuning into the Feedback architecture - it is definitely possible . We add a table below indicating the performance of such a finetuning strategy . |Model |Performance| | | | |Transformer Baseline | 21.2 | |Feedback Transformer | 19.7 | |Transformer Finetuned to Feedback | 19.8 | However , since the Feedback Transformer substantially changes the way the model works ( key and value vectors feeding to self-attention sublayers completely change ) , you can not train with one architecture and apply another at inference time - thus , it takes time to do the finetuning and create the Feedback architecture . With the speedup from sharing keys and values , we found no real benefit from finetuning compared to training a Feedback Transformer fully from scratch . Other ways to speed up convergence still apply in the translation setting , for example , initializing with pretrained embeddings is totally possible , but would improve the convergence speed of the standard Transformer as well , so we do not assess ."}, "3": {"review_id": "OCm0rwa1lx1-3", "review_text": "# # # Summary This paper modifies transformers with feedback memory . Specifically , for each timestep , it merges hidden representations of all layers into a high-level single vector and stores it in memory . For the current timestep , it attends past memory vectors . The authors claim that in this way , low layers of the current timestep can utilize high-level representations of past timesteps . The authors show that the proposed models with shallow layers can achieve stronger performance than comparable transformers . However , it seems that the models need a much longer time to train . # # # Strengths * With feedback memory , the modified can speedup decoding ( autoregressive generation ) as shown in Figure 4 . * Since the proposed model can directly utilize previous high-level representations , it just needs a small size and shallow layers to achieve comparable performance as shown in Table 3 and Table 4 . # # # Weaknesses and Questions * Training time and inference speed are important for such practical models . It is better to complement these to Table 1/2/3/4 . It seems that the proposed needs to take a much longer time to train as the authors mentioned it in a sentence on page 8 . The authors can give more results and discussions so that future users can know whether to choose transformers with feedback memory according to their situations . * ( optional ) In Table 3/4 , how about feedback transformer that keeps the same number of layers and similar parameters as Trans-XL . Transformers usually can achieve better performance when the number of layers increases . It is just an optional discussion as feedback transformers seem to need much time to train and the rebuttal time is limited .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review ! We \u2019 ve included additional analysis to respond to your questions , and included several additional experiments overall in the paper to strengthen our work . Please let us know if you have additional questions . * * re : training and inference time analysis * * We include a table here about the training and validation speed of the Feedback Transformer compared to the standard Transformer for language modeling , translation , and reinforcement learning . For language modeling , we measure on Wikitext-103 ( we compare a 8-layer Transformer against a 4-layer Feedback model of the same size.Both models have a fixed attention span of 512 , and trained on 32GPUs . With the Feedback model , we \u2019 re able to fit 2x larger batches in GPU memory . The inference is done with 1GPU ) . For translation , we measure on WMT En-De with 6 layer encoder and 2 layer decoder ( reporting training WPS on 8 GPU and inference WPS on 1 GPU ) . For RL , we report the training frame-per-second ( FPS ) on the maze navigation task ( using 20 CPU cores and 1 GPU ) . We will add this table into the main paper . |Task |Model | Training WPS | Inference WPS ` | | | | | | |LM ( wiki103 ) |Transformer | 296K | 592 | |LM ( wiki103 ) |Feedback |84.K | 2176 | |Translation |Transformer | 280K | 3190 | |Translation | Feedback |126K | 5410 | |RL Maze | Transformer | 22.3K | | |RL Maze |Feedback | 22.3K | | For Encoder-Decoder tasks , the Feedback Transformer is slower than the standard Transformer , but is faster at Inference as it uses less memory and can thus generate translations with larger batch sizes . For Language modeling , the Feedback Transformer is about 3x slower to train , much faster at inference due to reduced memory cost ( from sharing key-values ) and reduced depth . For reinforcement learning tasks , the training must be online as well , so the Transformer and Feedback Transformer have the same speed . * * re : larger feedback models * * We will explore training larger Feedback Transformer models . For datasets like Wikitext-103 and WMT En-De , it is mainly a challenge of regularization , so we anticipate needing to tune the dropout parameters . We 'll add additional results in the final version of the paper based on this exploration ( as you mention , the training time is a challenge in the short-ish rebuttal period ) . However , we want to emphasize that the main contribution of the paper is a straightforward solution to two major limitations of the Transformer architecture ( limited access to higher level representations and inability to maintain state ) . By resolving these limitations , it 's possible to have a model that is smaller and shallower perform just as well . We 're happy to conduct additional experiments , but the point is n't really chasing state of the art performance by sweeping more , but a simple way to not have these limitations in a Transformer ."}}