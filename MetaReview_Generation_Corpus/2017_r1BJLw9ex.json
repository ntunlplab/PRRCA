{"year": "2017", "forum": "r1BJLw9ex", "title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "decision": "Reject", "meta_review": "This was a borderline paper. However, no reviewers were willing to champion the acceptance of the paper during the deliberation period. Furthermore, in practice, initialization itself is a hyperparameter that gets tuned automatically. To be a compelling empirical result, it would be useful for the paper to include a comparison between the proposed initialization and a tuned arbitrary initialization scale with various tuning budgets. Additionally, other issues with the empirical evaluation brought up by the reviewers were only partially resolved in the revisions. For these reasons, the paper has been recommended for rejection.", "reviews": [{"review_id": "r1BJLw9ex-0", "review_text": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results. I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear. The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations? Also only mnist does not have to generalize to other benchmarks. Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update. There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly. The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization. In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures. None of the empirical results have data augmentation. It is not clear if the initialization or batch normalization update will help or make it worse for that case. Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures. This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your analysis of our paper . We have added more detail to our synthetic experiment set-up in the new paper . Following your suggestion about more random hyperparameter search , we re-ran the MNIST experiments tuning over { 10^-3 , 10^-4 , 10^-5 } and randomly drew 7 learning rates from 10^Unif [ -1 , -5 ] . We then selected the best curves based on validation results . The plots have been updated to include test log loss curves as well . Your suggestion about tuning the learning rate with random hyperparameter search was indeed executed for the Adam and Nesterov experiments . Other parameters ( depth , number of filters , etc . ) were inherited from the popular VGG Net implementation . Note that He et al . [ 1 ] demonstrate that small weight initialization scalars leave some networks untrainable when there is no batch normalization ( Figure 3 ) , so to us it is not unreasonable to expect that different initializations in our Nesterov experiment have noticeably distinct convergence curves . > However there is no result for 'original with BN update ' therefore it is not clear whether the BN update helps in general or not . SVHN also does not have result for original with BN update . This is because the `` original with BN update '' values are from the original DenseNet paper [ 2 ] , and we do not have their model saved at the midpoint of training . For that reason we re-ran their experiments with and without the Batch Normalization update all while using the setup described in their paper . The `` Ours '' rows show the consequence of using Batch Normalization re-estimation method and the test error without the re-estimation . Note that this lets us see the exact effects of Batch Normalization variance re-estimation because we show how the model performs on the test set and how that _same_ model performs if we only modify the model 's Batch Normalization variance parameters keeping all other parameters the same . > The CIFAR100 difference is not significant without including batch normalization variance re-estimation . The CIFAR-100 experiment is designed to show the effect of batch normalization variance re-estimation . Without the batch normalization variance re-estimation , there are no appreciable differences between the original setup and our experiments . We are not testing our weight initialization in this section , only the batch normalization variance re-estimation . Thank you for helping the paper become stronger . Update : Anonymous Reviewer 2 edited their initial review on Jan 23rd to include new concerns . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > compare the initialization to batch-normalization We compare our initialization to other initializations involving simple scalars , as many still use `` Xavier '' and `` He '' initializations . Batch Normalization significantly reduces the need for a good initialization , so we give the initialization and batch normalization dropout correction methods separate sections . It is still worthwhile to consider the simple scalar initializations because people may opt not to use Batch Normalization if they want to save computational time , do online learning , or train a ConvNet for reinforcement learning ( since Batch Normalization introduces much noise to training in RL ) . This is why we show how to correct for dropout 's influence against other initializations with simple scalars ( Xavier , He ) , and then we show how to correct for dropout 's influence in Batch Normalization separately . > Dense Net scale to many depths , ... it is not clear if there is going to be any empirical gain for that We trained on a 100-layer DenseNet with growth rate k=12 , shown in Table 2 , and this has 7 million parameters . Larger DenseNets are almost never more than twice this depth . Their larger DenseNet required 26GB of memory and to be trained on 4 Titan X 's , according to a correspondence with Zhuang Liu , the DenseNet author . This was simply too bloated for us to train , and we were still able to surpass its performance with batch normalization variance re-estimation . > Table 2 is not significant ( paraphrase ) We respectfully disagree . Going from 5.77 % to 5.38 % ( note we are approaching an accuracy ceiling ) and 23.79 % to 22.17 % ( both state of the art ) with a simple , quick , and highly general trick is significant to us . [ 1 ] Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification . https : //arxiv.org/abs/1502.01852 [ 2 ] Densely Connected Convolutional Networks . https : //arxiv.org/pdf/1608.06993v1.pdf"}, {"review_id": "r1BJLw9ex-1", "review_text": "The main observation made in the paper is that the use of dropout increases the variance of neurons. Correcting for this increase in variance, in the parameter initialization, and in the test-time statistics of batch normalization, improves performance, as is shown reasonably convincingly in the experiments. This observation is important, as it applies to many of the models used in the literature. It's not extremely novel (it's been observed in the literature before that our simple dropout approximations at test time do not achieve the accuracy obtained by full Monte Carlo dropout) The paper could use more experimental validation. Specifically: - I'm guessing the correction for dropout variance at test time is not only specific to batch normalization: Standard dropout, in networks without batch normalization, corrects only for the mean at test time (by dividing activations by one minus the dropout probability). This work suggests it would be beneficial to also correct for the variance. Has this been tested? - How does the dropout variance correction compare to using Monte Carlo dropout at test time? (i.e. just averaging over a large number of random dropout masks)", "rating": "7: Good paper, accept", "reply_text": "Thank you for your analysis of our paper . In our experiments , we do not enable dropout during test time since , as we all know , dropout at test time hurts performance . Then we do not need to correct for variance _during_ testing . We correct for the variance of dropout applied in training either by weight initialization used during training , or batch normalization parameter re-estimation after normal training but before testing . These corrections allow for better training ( weight initialization correction ) or fixes the trained parameters ( batch normalization correction ) , which affect test time performance . If we are misunderstanding your last two points , let us know ."}, {"review_id": "r1BJLw9ex-2", "review_text": "The paper presents an approach for compensating the input/activation variance introduced by dropout in a network. Additionally, a practical inference trick of re-estimating the batch normalization parameters with dropout turned off before testing. The authors very well show how dropout influences the input/activation variance and then scale the initial weights accordingly to achieve unit variance which helps in avoiding activation outputs exploding or vanishing. It is shown that the presented approach serves as a good initialization technique for deep networks and results in performances op par or slightly better than the existing approaches. The limited experimental validation and only small difference in accuracies compared to existing methods makes it difficult to judge the effectiveness of presented approach. Perhaps observing the statistics of output activations and gradients over training epochs in multiple experiments can better support the argument of stability of the network using proposed approach. Authors might consider adding some validation for considering the backpropagation variance. On multiple occasions comparison is drawn against batch normalization which I believe does much more than a weight initialization technique. The presented approach is a good initialization technique just not sure if its better than existing ones. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your analysis of our paper . In light of your review , we added a figure showing the differences in gradient norms using distinct initializations ."}], "0": {"review_id": "r1BJLw9ex-0", "review_text": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results. I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear. The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations? Also only mnist does not have to generalize to other benchmarks. Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update. There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly. The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization. In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures. None of the empirical results have data augmentation. It is not clear if the initialization or batch normalization update will help or make it worse for that case. Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures. This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your analysis of our paper . We have added more detail to our synthetic experiment set-up in the new paper . Following your suggestion about more random hyperparameter search , we re-ran the MNIST experiments tuning over { 10^-3 , 10^-4 , 10^-5 } and randomly drew 7 learning rates from 10^Unif [ -1 , -5 ] . We then selected the best curves based on validation results . The plots have been updated to include test log loss curves as well . Your suggestion about tuning the learning rate with random hyperparameter search was indeed executed for the Adam and Nesterov experiments . Other parameters ( depth , number of filters , etc . ) were inherited from the popular VGG Net implementation . Note that He et al . [ 1 ] demonstrate that small weight initialization scalars leave some networks untrainable when there is no batch normalization ( Figure 3 ) , so to us it is not unreasonable to expect that different initializations in our Nesterov experiment have noticeably distinct convergence curves . > However there is no result for 'original with BN update ' therefore it is not clear whether the BN update helps in general or not . SVHN also does not have result for original with BN update . This is because the `` original with BN update '' values are from the original DenseNet paper [ 2 ] , and we do not have their model saved at the midpoint of training . For that reason we re-ran their experiments with and without the Batch Normalization update all while using the setup described in their paper . The `` Ours '' rows show the consequence of using Batch Normalization re-estimation method and the test error without the re-estimation . Note that this lets us see the exact effects of Batch Normalization variance re-estimation because we show how the model performs on the test set and how that _same_ model performs if we only modify the model 's Batch Normalization variance parameters keeping all other parameters the same . > The CIFAR100 difference is not significant without including batch normalization variance re-estimation . The CIFAR-100 experiment is designed to show the effect of batch normalization variance re-estimation . Without the batch normalization variance re-estimation , there are no appreciable differences between the original setup and our experiments . We are not testing our weight initialization in this section , only the batch normalization variance re-estimation . Thank you for helping the paper become stronger . Update : Anonymous Reviewer 2 edited their initial review on Jan 23rd to include new concerns . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > compare the initialization to batch-normalization We compare our initialization to other initializations involving simple scalars , as many still use `` Xavier '' and `` He '' initializations . Batch Normalization significantly reduces the need for a good initialization , so we give the initialization and batch normalization dropout correction methods separate sections . It is still worthwhile to consider the simple scalar initializations because people may opt not to use Batch Normalization if they want to save computational time , do online learning , or train a ConvNet for reinforcement learning ( since Batch Normalization introduces much noise to training in RL ) . This is why we show how to correct for dropout 's influence against other initializations with simple scalars ( Xavier , He ) , and then we show how to correct for dropout 's influence in Batch Normalization separately . > Dense Net scale to many depths , ... it is not clear if there is going to be any empirical gain for that We trained on a 100-layer DenseNet with growth rate k=12 , shown in Table 2 , and this has 7 million parameters . Larger DenseNets are almost never more than twice this depth . Their larger DenseNet required 26GB of memory and to be trained on 4 Titan X 's , according to a correspondence with Zhuang Liu , the DenseNet author . This was simply too bloated for us to train , and we were still able to surpass its performance with batch normalization variance re-estimation . > Table 2 is not significant ( paraphrase ) We respectfully disagree . Going from 5.77 % to 5.38 % ( note we are approaching an accuracy ceiling ) and 23.79 % to 22.17 % ( both state of the art ) with a simple , quick , and highly general trick is significant to us . [ 1 ] Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification . https : //arxiv.org/abs/1502.01852 [ 2 ] Densely Connected Convolutional Networks . https : //arxiv.org/pdf/1608.06993v1.pdf"}, "1": {"review_id": "r1BJLw9ex-1", "review_text": "The main observation made in the paper is that the use of dropout increases the variance of neurons. Correcting for this increase in variance, in the parameter initialization, and in the test-time statistics of batch normalization, improves performance, as is shown reasonably convincingly in the experiments. This observation is important, as it applies to many of the models used in the literature. It's not extremely novel (it's been observed in the literature before that our simple dropout approximations at test time do not achieve the accuracy obtained by full Monte Carlo dropout) The paper could use more experimental validation. Specifically: - I'm guessing the correction for dropout variance at test time is not only specific to batch normalization: Standard dropout, in networks without batch normalization, corrects only for the mean at test time (by dividing activations by one minus the dropout probability). This work suggests it would be beneficial to also correct for the variance. Has this been tested? - How does the dropout variance correction compare to using Monte Carlo dropout at test time? (i.e. just averaging over a large number of random dropout masks)", "rating": "7: Good paper, accept", "reply_text": "Thank you for your analysis of our paper . In our experiments , we do not enable dropout during test time since , as we all know , dropout at test time hurts performance . Then we do not need to correct for variance _during_ testing . We correct for the variance of dropout applied in training either by weight initialization used during training , or batch normalization parameter re-estimation after normal training but before testing . These corrections allow for better training ( weight initialization correction ) or fixes the trained parameters ( batch normalization correction ) , which affect test time performance . If we are misunderstanding your last two points , let us know ."}, "2": {"review_id": "r1BJLw9ex-2", "review_text": "The paper presents an approach for compensating the input/activation variance introduced by dropout in a network. Additionally, a practical inference trick of re-estimating the batch normalization parameters with dropout turned off before testing. The authors very well show how dropout influences the input/activation variance and then scale the initial weights accordingly to achieve unit variance which helps in avoiding activation outputs exploding or vanishing. It is shown that the presented approach serves as a good initialization technique for deep networks and results in performances op par or slightly better than the existing approaches. The limited experimental validation and only small difference in accuracies compared to existing methods makes it difficult to judge the effectiveness of presented approach. Perhaps observing the statistics of output activations and gradients over training epochs in multiple experiments can better support the argument of stability of the network using proposed approach. Authors might consider adding some validation for considering the backpropagation variance. On multiple occasions comparison is drawn against batch normalization which I believe does much more than a weight initialization technique. The presented approach is a good initialization technique just not sure if its better than existing ones. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your analysis of our paper . In light of your review , we added a figure showing the differences in gradient norms using distinct initializations ."}}