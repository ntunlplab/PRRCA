{"year": "2021", "forum": "N6SmiyDrkR5", "title": "What's in the Box? Exploring the Inner Life of Neural Networks with Robust Rules", "decision": "Reject", "meta_review": "This paper proposes a method to explore neuron interactions within a neural network by deriving rules for the activations of units at different layers. The rules can presumably help interpret the inner workings of the neural network. \nThe reviewers have very different opinions on the paper and the views did not converge.  However, there is a common concern on the lack of quantitative evaluation on the faithfulness of the rules to the models. I therefore do not recommend accept. \n\nR1[5]: On a related note, I felt the evaluation presented by authors while extensive is rather qualitative in nature.\nR2[3]: Given that I could provide you with a couple of references that you admit is relevant, and this was just off the top of my head, would you care to comment on a quantitative comparison with the referenced approaches?\nR3[8]: The examples look very impressive, but my main concern is with whether the examples could have been cherry-picked, in the sense that most of the thousands of rules produced may not be useful.\n\n\n", "reviews": [{"review_id": "N6SmiyDrkR5-0", "review_text": "The paper proposes an approach to explainable supervised learning by extracting sets of rules for two individual layers within a neural network . The authors build their work on recent published work for patttern-based rule mining [ 0 ] to efficently find so-called robust rules . The authors evaluate the approach for image processing tasks with convolutional neural networks on MNIST , ImageNet and Oxford Flower by comparing generated rules against activation maps and prototypes . The proposed approach is interesting , but I am left with several concerns and open questions with respect to contributions , available related works and the conducted evaluation . While the paper discusses quite nicely summarized several relevant related works in the introduction section , I am still wondering how the approach is related to OpenAI 's Circuits [ 1 ] , where already individual neurons and connections between neurons are studied with respect to interpretability/explainability of neural nets ? This is not to say that prior work already learned sets of rules among layers , but - if relevant - it should be evaluated to what extent the proposed method is superior for explaining neural nets to end-users . A smaller comment to the related work is that there might be missing references for model destillation , such as [ 2 ] . The taken approach to rule generation follows recent work on association rule mining , which is sensible . To this end , the paper misses to clearly address the difference to GRAB , i.e.the referenced algorithm used for learning the rules using model description length ( MDL ) . Could you therefore elaborate why GRAB can not be applied to the rule mining task out-of-the-box or clearer state if this what you have actually done ? I feel like the presentation of the paper could benefit from answering this question , one could establish a background section and/or focus on novel aspects of the proposed algorithm . In addition , there might be more space for the evaluation , which should be a major part of the paper 's contribution , especially explainable ML . The idea of exploring neural connections among two layers is generally intriguing . The paper could benefit from better motivating this design choice , i.e.why use `` only '' two layers ? Would it make sense to use more than two convolutional layers for deeper networks ? Would this be computationally tractable ? Are two layers better to interpret for humans ? While the evaluation is insightful , I am not convinced that single-neuron-prototypes and activation maps are representative for all ongoing works on explainable and interpretable ML . More specifically with respect to activation maps , is it possible to rule out all activation map approaches for CNNs at once ? What about perturbation-based approaches ( e.g . [ 3 ] ) ? To this end , I am wondering why you only compare to single neuron prototypes and not more complex prototypes of the individual class ( for Sec.3.1 and the MNIST experiment ) ? Lastly , I am also wondering if a user study would be helpful to confirm that the proposed explanations provide added value for end-users . A minor comment is that it is not helpful for the reader that numerous references figures are in the appendix ( e.g.the comparison of generated MNIST rules in Sec.3.1 to the prototype in the appendix ) . A minor question would be why you define `` prototype '' as on a single-neuron-basis ( in the introduction ) ? Is there a reference for the approach ? References : [ 0 ] Fischer , J. and Vreeken , J. , 2019 , September . Sets of robust rules , and how to find them . In Joint European Conference on Machine Learning and Knowledge Discovery in Databases ( pp.38-54 ) .Springer , Cham . [ 1 ] Olah , C. , Cammarata , N. , Schubert , L. , Goh , G. , Petrov , M. and Carter , S. , 2020 . Zoom In : An Introduction to Circuits . Distill , 5 ( 3 ) , pp.e00024-001 . [ 2 ] Bastani , O. , Kim , C. and Bastani , H. , 2017 . Interpreting blackbox models via model extraction . arXiv preprint arXiv:1705.08504 . [ 3 ] Fong , R.C . and Vedaldi , A. , 2017 . Interpretable explanations of black boxes by meaningful perturbation . In Proceedings of the IEEE International Conference on Computer Vision ( pp.3429-3437 ) . # # # Update after author response # # # Thanks again for the clarifications - After reading the author responses , the other reviewers ' comments and the new version of the manuscript , I increase my score for the paper , as the authors now better state the relationship to Circuits and GRAB , and provide a significantly improved evaluation . The enhanced experimental section is now adequate for the paper 's claims and offers additional insights into the usability of the generated rules .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for this extensive review . In the following , we will adress all your concerns paragraph wise . The work by Olah et al.is actually one of the main motivations for our work . Their approach is best summarized in their own words : `` What if we were willing to spend thousands of hours tracing through every neuron and its connections ? `` , as they propose to painstakingly analyze prototypes of individual neurons , sort them , and see how information flows between them . With ExplaiNN we offer a method to instead do this automatically : in our GoogLeNet experiments we use their prototyping on the results of ExplaiNN -- without needing to spend thousand of hours to find the right connections . Furthermore , in their work they limit themselves to prototypes of single neurons . Groups of neurons that together encode useful information are not considered at all . Clearly , as the task for individual neurons already takes `` thousands of hours '' , the combinatorial explosion that occurs when we consider multiple neurons makes investigation by hand impossible . With ExplaiNN , however , we can automate this search , discover complex interactions between sets of neurons , and combine these with their ( and those proposed by others ) prototyping mechanisms -- which is what we did in our experiments . And thank you for the reference , it indeed slipped our attention . We add it to the upcoming revision . The second raised concern is how different ExplaiNN is to GRAB . First , why is GRAB insufficient here ? There are multiple issues , which lead to the same consequence . GRAB 's pattern language , considering only conjunctions , is too restrictive . Output neuron activations , for example , are often mutually exclusive ; and hence GRAB would not be able to discover trends shared between classes . Hidden neuron activations , for example , are often noisy ( see App.Fig.7 ) ; and hence GRAB would only be able to model the noise-free parts of the neural dependencies . To alleviate this problem , we had to extend the GRAB pattern language to allow for disjunctions , which in turn required new optimization algorithms . Moreover , while GRAB scales well in n ( samples ) it does not scale well enough in m ( neurons ) and hence we had to additionally develop a new candidate generation scheme . We will clarify these aspects in the main body of the upcoming revision -- some of these were already included in the appendix for reasons of space . Regarding the third concern , on `` restricting '' to two layers , we first note that both method and theory are defined for two arbitrary sets of neurons , without specifying what or where these neurons are . In principle , we can mine rules between multiple layers without any restriction . While this might be of interest to some , we are particularly interested in how information flows through the network , and hence considered rules between consecutive layers , as this is how NNs pass on information . For ResNet-like architectures , however , we agree it would be interesting to examine not only the previous , but the previous two layers . Currently , ExplaiNN scales well enough to discover rules over many thousands of neurons within only a few hours . While this is generally sufficient to consider multiple layers at once , considering a full network at once might be a stretch : speeding up ExplaiNN further , which will likely include engineering tricks as well as clever new heuristics will make for interesting future research . On the fourth concern , we agree that the work of Fong et al is interesting , but note that it neither explains how neurons within the network interact nor how they are related to classes ; they focus on saliency maps , where the goal is to retrieve those parts of a specific given input image that drives the classification . Activation maps on the other hand are applicable to hidden neurons , rather than input images , and are used in analysis of neural networks . We compare only to single neuron prototypes because this is the state of the art ( see also Olah et al ) , and figuring out which neurons actually interact is exactly the task that ExplaiNN solves . We strongly agree with the final comment and will adapt the revision accordingly \u2013 we have many more interesting results than fit within the page limit . We plan to use the additional page to resolve this in upcoming revision ."}, {"review_id": "N6SmiyDrkR5-1", "review_text": "This work makes a convincing case that we need to trace information flow ( or at least , to find clusters of neurons working together ) within a deep neural network in order to get the clearest picture of how the network is working . The approach is quite simple but also quite novel in that it uses concepts from coding theory which are not widely known in the deep learning community . The paper is well written and presents examples from deep CNNs ( ~20 layers ) trained on ImageNet . Quality : The method is well thought-out and explained , and two different evaluations are used ( MNIST and ImageNet ) . It would be more impressive if the authors had included a problem from a different data modality since in principle the method is quite general . The examples look very impressive , but my main concern is with whether the examples could have been cherry-picked , in the sense that most of the thousands of rules produced may not be useful . Relatedly , I would like to know the reproducibility of the result . If you train twice with different seeds , how similar are the results ? Or if you fit ExplainNN with different seeds on the same network ? And what is the danger of false positive findings ? Clarity : The paper is very clear with regards to the problem setting , previous work , and the methods . I am somewhat familiar with coding theory , having read much of MacKay 's Information Theory , Inference , and Learning Algorithms , but I am by no means an expert in coding theory . Still , I am confident that I understand the principles of the method . However , I did not exactly follow how the authors carried out the tracing ( pg 7 second paragraph . ) . Do you just apply ExplainN as usual and then filter for rules that ( strictly/non-strictly ) include Y ? This seems important to explain since the tracing , in my view , is the main contribution , given that there already exist tools for understanding the similarities of classes such as representational similarity ( https : //roberttlange.github.io/posts/2019/06/blog-post-3/ ) . Originality : I have not previously seen the idea of mining association rules for deep neural networks , although it is a simple enough idea and I would not be surprised if the idea has appeared before . However , the application of MDL to solving the problem for binarized activations is likely to be novel . Significance : The work is promising , but to me it is not conclusive that it will make a lasting impact . There are a number of important practical questions to be addressed that could make or break the method as a tool for the field , such as the reproducibility and usability of the method ( whether most rules produced are meaningful or significant manual filtering is required ) . On the other hand , even if the method does not meet practical needs , it could still be of great utility for NN methods researchers interested in investigating , say , the redundancy of specific architectures . the true degree of similarity between two trained models , convergent dynamics between alternative architectures , and the value of overparameterization . The paper could be even more significant if the authors could comment on the generalizability to other architectures such as RNNs , GNNs or transformers . The method itself is interesting enough and the examples sufficiently compelling ( even if cherry-picked ) that I would recommend the paper to almost anyone interested in neural network interpretability . Note on rating : In the face of limited details , I am willing to give the authors the benefit of the doubt that they did not cherry-pick overly aggressively and that the examples are representative of typical outputs . If it turns out that most rules do not look like the examples , then my rating would decrease . On the other hand , if the authors can address my concerns about cherry-picking in the response , it is possible that I would raise my rating . Pros : * important application * method is quite general * method is simple and intuitive Cons : * evaluation of method performance limited to selected examples * reproducibility not addressed * control of false positives not addressed * method for tracking across layers not well-explained", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for this extensive and positive feedback . - If you train twice with different seeds , how similar are the results ? This is a very interesting question . We have not yet considered this , but indeed , it should be possible to use ExplaiNN as a tool to see if two networks ( e.g.trained on the same data , but with different seed ) have learned the same main concepts ! Essentially , if the networks have learned the same concepts , have approximatey the same generalization error , then between the networks the neurons are merely `` permuted '' and hence ExplaiNN would discover the same rule sets for both networks ( up to neuron ids ) . The moment that ExplaiNN discovers strongly diverging rule sets , this gives evidence that the networks learned intrinsically different concepts . We could even consider how well the rule set for Network A can be used to compress ( after neuron-id optimization ) the activations of Network B ( and vice-versa ) . Far beyond the scope of this paper , but fascinating ! - If you fit ExplaiNN with different seeds on the same network ? Maybe we misunderstood the question , but ExplaiNN is deterministic , for the same data iw will always return the exact same result . - What is the danger of false positive findings ? If we interpret this question correctly , a false positive finding corresponds to a rule that does not reflect information that is important for the data . If that is the case , it is also very unlikely that these neurons are sufficiently often co-activated for a rule over them to improve compression compared to the base-line model , which assumes independence between neurons . In a nutshell , MDL ensures that only rules for which sufficient evidences exists in the data are added to the model , as otherwise the number of bits needed to encode data and model would unnecessarily increase . This question will also be implicitly covered by the new synthetic data experiments in response to Reviewer 1 . - Question about tracing . That is a good point , we will add a more extensive description in the revised manuscript . Suppose we have three consecutive layers L1 - > L2 - > L3 . We first discover all rules X- > Y , X \\subset L3 , Y \\subset L2 . In the next step , for each such Y , we run ExplaiNN to discover rules Y- > Z , where Z \\subset L1 . There also exist other approaches that would work , for example mining rules between each consecutive layer pair , and then combine those that `` overlap '' -- which we think is what your comment described . Our iterative approach has the advantage that we can force the model to focus on what is necessary to build traces and thus get cleaner results , with the general pairwise application we have to do the filtering by overlap that you mentioned to get the clean results . - Concern about cherry-picking We could not agree more with this concern -- not so much for ExplaiNN , but for explanatory approaches for CNNs in general . A key advantage of our MDL-based approach is that we discover succinct and non-redundant sets of rules , here resulting in hundreds to at most a few thousand rules , which we can analyze in its entirety . For the VGG-S network , the existing highly optimized prototyping allowed us to generate prototypes for all discovered rules . For the large majority the result looks meaningful , it is clear what information they encode , and how it relates to the output . As common with prototyping , there are also few that are hard to interpret , or reveal insights only on second glance , because they e.g.encode abstract information ( geometric shapes , small subparts of an object ) . For rules with larger heads ( spanning many classes ) the prototypes often reflect some of the classes well , while the other classes seem to utilize only certain information from the neuron ( e.g.a pattern , a red leg ) . Such classes are classified by information of multiple neuron groups - found in several rules - which together provide the whole picture . Overall we picked a representative subset that reflects the information content of most of the prototypes . We will make the code available once the paper is published , such that the results can be reproduced and everybody can take a look on what the network sees . We now also added two additional panels containing 20 additional prototypes to the appendix , which we think are larger groups of representatives of our findings ."}, {"review_id": "N6SmiyDrkR5-2", "review_text": "This paper proposes to extract interpretable rules from a learned neural network . The authors claim that they are the first to propose rules connecting 1 ) multiple neurons together , and 2 ) do this at a dataset level . Their approach relies on using minimum description length and well known principles from the data mining community ( e.g. , downward closure lemma of apriori algorithm ) . The authors claim that experiments conducted on image data shows that their approach leads to more faithful , interpretable rules than other approaches such as prototyping or model distillation . Compiling rules from neural networks has been proposed before ( e.g. , see `` Deep Logic Networks : Inserting and Extracting Knowledge from Deep Belief Networks '' by Tran and d'Avila Garcez , 2018 ) . I think the authors need to compare against such previous approaches to quantitatively show how their work extracts better rules . Otherwise , its difficult to appreciate the value of ExplaiNN . Also , the paper does n't say anything about how faithful the rules are to the learned neural network . I mean , it seems possible that for some input the rules could produce a different output from the neural networks . There exist other works that also try to interpret neural networks consisting of affine layers with Relu activation that guarantee consistency ( see `` Exact and Consistent Interpretation for Piecewise Linear Neural Networks : A Closed Form Solution '' by Chu et al in KDD'18 ) . The authors should at the very least compare and contrast with such works to highlight the pros and cons . Another undesirable property of ExplainNN seems to be that it relies on a dataset to derive its rules . Is it possible that when run with a different dataset and the same learned network , ExplaiNN would produce a different set of rules ? Then how much faith do we place on ExplaiNN 's output ? Writing wise , the paper is presented well enough . There 's a few paragraphs in the Experiments section where the authors point repeatedly to the Appendices . In the best case this makes reading a chore . I would advise the authors to refrain from using the main body of the paper as a listing of contents and simply pointing to the appendices . The pictures in the experiments section were difficult to make out . I could n't figure out from the image whether the husky 's pointed snout had been identified as a defining feature . I would hope the authors find more compelling ways to make their point .", "rating": "3: Clear rejection", "reply_text": "Thank you for your comments . Both references are relevant related work , and we will add them to the manuscript . Unlike our approach , both are tailored to specific types of neural networks and do not generalize to state-of-the-art architectures such as e.g.CNNs , and use explanations that are hard to interpret - either by the type of information they retrieve or the sheer size of results returned . Concretely , Tran et al.is limited to Deep Belief networks ( stacked RBMs ) and optimizes for high confidence rules . Mining high confidence rules has serious practical issues \u2013 millions of highly redundant and often spurious rules are mined from small data already , even when the data contains no structure . Modern pattern ( set ) mining methods avoid this issue by relying on statistical tests and/or information theoretic notions . The work by Chu et al.does not suffer from the pattern explosion , but is limited to very small ( < 10 neurons per layer ) piecewise linear neural networks ( see Table 3 in their paper ) as it is based on ( theoretically interesting ) polytope analysis . Moreover , rather than characterizing what neurons inside the network encode , it aims to explain neural activity in terms of network input ( see Figure 1,5,6 in their paper ) . Moreover , it is much harder to interpret what the polytope actually expresses , compared to how accessible rules are . Regarding your last question , we indeed propose to analyze network behaviour based on neuron activations for a given dataset . We consider this an advantage rather than a disadvantage , as we want to learn how the network perceives the world ( the input ) and how and which information it composes and combines from the input to arrive at a decision . Analyzing the network without an input is like analyzing the human brain without stimulus . It also opens the possibility to explore how a network \u2019 s behavior changes when we change either the input distribution ( train/test , ood ) or when we re-train it in a slightly different context , like in the transfer learning task ."}, {"review_id": "N6SmiyDrkR5-3", "review_text": "Paper Summary The authors propose a method to explore how neurons interact within a neural network and derive rules of interactions that can help interpret the inner workings of the neural network and open up the black box . The algorithm , EXPLAINN , identifies rules between successive layers where each rule represents a set of neurons that are activate simultaneously and conditionally based on the previous layer . Minimum Description Length principle is used to derive an objective that minimizes the number of bits used to encode the rules . The rule sets are identified using a greedy heuristic and improved until convergence of the objective . The algorithm is then evaluated to demonstrate the interpretation of images with MNIST , GoogLeNet and VGG-S. Positives * The problem at hand is clearly important and can help with interpretation of the neural networks . This is particularly important in fields such as biology ( genomics ) where better interpretation is often times desired at a slight cost in performance * The formulation using Minimum Description Length Principle is definitely an interesting idea - particularly formulating the objective as set of independent and robust rules * The framework is flexible and allows for discovery of rules relevant for subsets / combinations of classes and not just individual examples or global rules across examples * The paper in general is well written and I particularly liked how the authors guide the readers through the different aspects of the algorithm evaluation . I particularly liked Appendix A since the concrete example made the objective function clear . The notations however can be better laid out and clarified . Concerns * My primary concern is the use of greedy heuristic to identify the rule set . This necessitates that each interaction individually carry some degree of information for a robust composite rule . While the rules identified through the heuristic could be informative , it is not clear whether they are the best set of rules since there are no bounds or guarantees of how the heuristic relates to the global optimum . * On a related note , I felt the evaluation presented by authors while extensive is rather qualitative in nature . While the prototypes of identified rules across different datasets look relevant and interesting , it is not clear whether they are the best set of rules . So , I believe this manuscript needs benchmarking in datasets constructed with known rules ( possibly through simulation ) to alleviate these two major concerns * The quantitative outputs of the ReLU activations are binarized for rule learning - the authors have not addressed how this impacts the accuracy of the rules since that quantitative information is used by the subsequent layers * The impact of the threshold parameters theta and mu ( Algorithms 1 & 2 ) are also not addressed - this seems critical to me since these thresholds define the initial set of rule candidates .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the constructive critique , this certainly helps to improve the manuscript . Regarding your first two concerns : We agree that quality guarantees or even optimal search are desirable , however , as our MDL objective is neither monotone nor sub-modular , and as the search space is exponential , the only available option is to employ heuristics . Our bottom-up strategy is based on the idea that a good large pattern , i.e.one that improves compression when we add it to our model , is composed of smaller parts that all help to compress the data . This is a commonly employed , succesful approach in pattern set mining , see e.g.GRAB [ 1 ] , SLIM [ 2 ] , SQS [ 3 ] , kGIST [ 4 ] . One of the intuitions we exploit is that a larger rule will only lead to better compression if the ` tails ' of its generating rules co-occur under the same conditions , i.e.when the head of the rule is present . As if that is not the case , the new larger rule would leave many rows unexplained , and would hence likely not yield any gain . Before starting with the NN experiments , we validated this approach via initial experiments on synthetic data . We will update the manuscript with an extended version of these experiments as soon as they are finished . Regarding the third concern : It is correct that binarizing ReLU activations may result in information loss \u2013 activation strength might , for example , be used by subsequent layers \u2013 but by doing so we do gain a huge advantage in terms of interpretability : binarizing neuron activations allows us to derive crisp symbolic , and directly interpretable statements on how neurons interact . Moreover , the on/off state not only reflects how biological neurons function , but also closely matches tanh and sigmoid activation functions . Extending ExplaiNN to an easily interpretable pattern language that allows for continuous , ReLU-like , activations is an interesting line for future work . Regarding the fourth concern : Both these easily interpretable parameters are merely simple , yet effective runtime optimizations . From an MDL-perspective , the best results will always be obtained with the largest search space : i.e.with $ \\theta $ and $ \\mu $ set to 0 , respectively $ |X_1| + |X_2| $ , as this corresponds to considering all combinations of rules . Besides impacting run-time , many of those rules may be uninteresting from a user-perspective . Mu and theta allow users to directly instruct ExplaiNN to ignore such rules . Here , we consider a good rule as one for which the tail items frequently occur when the head items are present . In other words , for a given rule $ X \\rightarrow Y $ , if neurons $ X $ are active , the neurons $ Y $ should also likely to be active . This is reflected by the confidence parameter $ \\theta $ , which is the chance that $ Y $ is active when $ X $ is active . In our experiments we found that rules of theta below 0.4 are too weak to reveal valuable insights , but this may be different for different users or domains . For the parameter $ \\mu $ , we want to account for early rule merging decisions that hinder us to see a more general trend . For example it might be that we learned $ ABC \\rightarrow Y $ and $ BCD \\rightarrow Z $ , which both individually make sense and give gain , yet the rule $ BC \\rightarrow YZ $ might be more general and yield even higher gain , but was not possible to obtain when the individual rules were considered . We used $ \\mu=6 $ , which allows to merge fairly large patterns with huge differences . [ 1 ] Fischer , J. , Vreeken , J . Sets of Robust Rules , and How to Find Them . Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Data ( ECMLPKDD ) , Springer , 2019 . [ 2 ] Smets , K. , Vreeken , J . SLIM : Directly mining descriptive patterns . Proceedings of the SIAM International Conference on Data Mining ( SDM ) , SIAM , 2012 . [ 3 ] Tatti , N. , Vreeken , J . The Long and the Short of It : Summarising Event Sequences with Serial Episodes . Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , ACM , 2012 [ 4 ] Belth , C. , Zheng , X. , Vreeken , J. , Koutra , D. What is Normal , What is Strange , and What is Missing in a Knowledge Graph . Proceedings of the Web Conference ( WWW ) , ACM , 2020 ."}], "0": {"review_id": "N6SmiyDrkR5-0", "review_text": "The paper proposes an approach to explainable supervised learning by extracting sets of rules for two individual layers within a neural network . The authors build their work on recent published work for patttern-based rule mining [ 0 ] to efficently find so-called robust rules . The authors evaluate the approach for image processing tasks with convolutional neural networks on MNIST , ImageNet and Oxford Flower by comparing generated rules against activation maps and prototypes . The proposed approach is interesting , but I am left with several concerns and open questions with respect to contributions , available related works and the conducted evaluation . While the paper discusses quite nicely summarized several relevant related works in the introduction section , I am still wondering how the approach is related to OpenAI 's Circuits [ 1 ] , where already individual neurons and connections between neurons are studied with respect to interpretability/explainability of neural nets ? This is not to say that prior work already learned sets of rules among layers , but - if relevant - it should be evaluated to what extent the proposed method is superior for explaining neural nets to end-users . A smaller comment to the related work is that there might be missing references for model destillation , such as [ 2 ] . The taken approach to rule generation follows recent work on association rule mining , which is sensible . To this end , the paper misses to clearly address the difference to GRAB , i.e.the referenced algorithm used for learning the rules using model description length ( MDL ) . Could you therefore elaborate why GRAB can not be applied to the rule mining task out-of-the-box or clearer state if this what you have actually done ? I feel like the presentation of the paper could benefit from answering this question , one could establish a background section and/or focus on novel aspects of the proposed algorithm . In addition , there might be more space for the evaluation , which should be a major part of the paper 's contribution , especially explainable ML . The idea of exploring neural connections among two layers is generally intriguing . The paper could benefit from better motivating this design choice , i.e.why use `` only '' two layers ? Would it make sense to use more than two convolutional layers for deeper networks ? Would this be computationally tractable ? Are two layers better to interpret for humans ? While the evaluation is insightful , I am not convinced that single-neuron-prototypes and activation maps are representative for all ongoing works on explainable and interpretable ML . More specifically with respect to activation maps , is it possible to rule out all activation map approaches for CNNs at once ? What about perturbation-based approaches ( e.g . [ 3 ] ) ? To this end , I am wondering why you only compare to single neuron prototypes and not more complex prototypes of the individual class ( for Sec.3.1 and the MNIST experiment ) ? Lastly , I am also wondering if a user study would be helpful to confirm that the proposed explanations provide added value for end-users . A minor comment is that it is not helpful for the reader that numerous references figures are in the appendix ( e.g.the comparison of generated MNIST rules in Sec.3.1 to the prototype in the appendix ) . A minor question would be why you define `` prototype '' as on a single-neuron-basis ( in the introduction ) ? Is there a reference for the approach ? References : [ 0 ] Fischer , J. and Vreeken , J. , 2019 , September . Sets of robust rules , and how to find them . In Joint European Conference on Machine Learning and Knowledge Discovery in Databases ( pp.38-54 ) .Springer , Cham . [ 1 ] Olah , C. , Cammarata , N. , Schubert , L. , Goh , G. , Petrov , M. and Carter , S. , 2020 . Zoom In : An Introduction to Circuits . Distill , 5 ( 3 ) , pp.e00024-001 . [ 2 ] Bastani , O. , Kim , C. and Bastani , H. , 2017 . Interpreting blackbox models via model extraction . arXiv preprint arXiv:1705.08504 . [ 3 ] Fong , R.C . and Vedaldi , A. , 2017 . Interpretable explanations of black boxes by meaningful perturbation . In Proceedings of the IEEE International Conference on Computer Vision ( pp.3429-3437 ) . # # # Update after author response # # # Thanks again for the clarifications - After reading the author responses , the other reviewers ' comments and the new version of the manuscript , I increase my score for the paper , as the authors now better state the relationship to Circuits and GRAB , and provide a significantly improved evaluation . The enhanced experimental section is now adequate for the paper 's claims and offers additional insights into the usability of the generated rules .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for this extensive review . In the following , we will adress all your concerns paragraph wise . The work by Olah et al.is actually one of the main motivations for our work . Their approach is best summarized in their own words : `` What if we were willing to spend thousands of hours tracing through every neuron and its connections ? `` , as they propose to painstakingly analyze prototypes of individual neurons , sort them , and see how information flows between them . With ExplaiNN we offer a method to instead do this automatically : in our GoogLeNet experiments we use their prototyping on the results of ExplaiNN -- without needing to spend thousand of hours to find the right connections . Furthermore , in their work they limit themselves to prototypes of single neurons . Groups of neurons that together encode useful information are not considered at all . Clearly , as the task for individual neurons already takes `` thousands of hours '' , the combinatorial explosion that occurs when we consider multiple neurons makes investigation by hand impossible . With ExplaiNN , however , we can automate this search , discover complex interactions between sets of neurons , and combine these with their ( and those proposed by others ) prototyping mechanisms -- which is what we did in our experiments . And thank you for the reference , it indeed slipped our attention . We add it to the upcoming revision . The second raised concern is how different ExplaiNN is to GRAB . First , why is GRAB insufficient here ? There are multiple issues , which lead to the same consequence . GRAB 's pattern language , considering only conjunctions , is too restrictive . Output neuron activations , for example , are often mutually exclusive ; and hence GRAB would not be able to discover trends shared between classes . Hidden neuron activations , for example , are often noisy ( see App.Fig.7 ) ; and hence GRAB would only be able to model the noise-free parts of the neural dependencies . To alleviate this problem , we had to extend the GRAB pattern language to allow for disjunctions , which in turn required new optimization algorithms . Moreover , while GRAB scales well in n ( samples ) it does not scale well enough in m ( neurons ) and hence we had to additionally develop a new candidate generation scheme . We will clarify these aspects in the main body of the upcoming revision -- some of these were already included in the appendix for reasons of space . Regarding the third concern , on `` restricting '' to two layers , we first note that both method and theory are defined for two arbitrary sets of neurons , without specifying what or where these neurons are . In principle , we can mine rules between multiple layers without any restriction . While this might be of interest to some , we are particularly interested in how information flows through the network , and hence considered rules between consecutive layers , as this is how NNs pass on information . For ResNet-like architectures , however , we agree it would be interesting to examine not only the previous , but the previous two layers . Currently , ExplaiNN scales well enough to discover rules over many thousands of neurons within only a few hours . While this is generally sufficient to consider multiple layers at once , considering a full network at once might be a stretch : speeding up ExplaiNN further , which will likely include engineering tricks as well as clever new heuristics will make for interesting future research . On the fourth concern , we agree that the work of Fong et al is interesting , but note that it neither explains how neurons within the network interact nor how they are related to classes ; they focus on saliency maps , where the goal is to retrieve those parts of a specific given input image that drives the classification . Activation maps on the other hand are applicable to hidden neurons , rather than input images , and are used in analysis of neural networks . We compare only to single neuron prototypes because this is the state of the art ( see also Olah et al ) , and figuring out which neurons actually interact is exactly the task that ExplaiNN solves . We strongly agree with the final comment and will adapt the revision accordingly \u2013 we have many more interesting results than fit within the page limit . We plan to use the additional page to resolve this in upcoming revision ."}, "1": {"review_id": "N6SmiyDrkR5-1", "review_text": "This work makes a convincing case that we need to trace information flow ( or at least , to find clusters of neurons working together ) within a deep neural network in order to get the clearest picture of how the network is working . The approach is quite simple but also quite novel in that it uses concepts from coding theory which are not widely known in the deep learning community . The paper is well written and presents examples from deep CNNs ( ~20 layers ) trained on ImageNet . Quality : The method is well thought-out and explained , and two different evaluations are used ( MNIST and ImageNet ) . It would be more impressive if the authors had included a problem from a different data modality since in principle the method is quite general . The examples look very impressive , but my main concern is with whether the examples could have been cherry-picked , in the sense that most of the thousands of rules produced may not be useful . Relatedly , I would like to know the reproducibility of the result . If you train twice with different seeds , how similar are the results ? Or if you fit ExplainNN with different seeds on the same network ? And what is the danger of false positive findings ? Clarity : The paper is very clear with regards to the problem setting , previous work , and the methods . I am somewhat familiar with coding theory , having read much of MacKay 's Information Theory , Inference , and Learning Algorithms , but I am by no means an expert in coding theory . Still , I am confident that I understand the principles of the method . However , I did not exactly follow how the authors carried out the tracing ( pg 7 second paragraph . ) . Do you just apply ExplainN as usual and then filter for rules that ( strictly/non-strictly ) include Y ? This seems important to explain since the tracing , in my view , is the main contribution , given that there already exist tools for understanding the similarities of classes such as representational similarity ( https : //roberttlange.github.io/posts/2019/06/blog-post-3/ ) . Originality : I have not previously seen the idea of mining association rules for deep neural networks , although it is a simple enough idea and I would not be surprised if the idea has appeared before . However , the application of MDL to solving the problem for binarized activations is likely to be novel . Significance : The work is promising , but to me it is not conclusive that it will make a lasting impact . There are a number of important practical questions to be addressed that could make or break the method as a tool for the field , such as the reproducibility and usability of the method ( whether most rules produced are meaningful or significant manual filtering is required ) . On the other hand , even if the method does not meet practical needs , it could still be of great utility for NN methods researchers interested in investigating , say , the redundancy of specific architectures . the true degree of similarity between two trained models , convergent dynamics between alternative architectures , and the value of overparameterization . The paper could be even more significant if the authors could comment on the generalizability to other architectures such as RNNs , GNNs or transformers . The method itself is interesting enough and the examples sufficiently compelling ( even if cherry-picked ) that I would recommend the paper to almost anyone interested in neural network interpretability . Note on rating : In the face of limited details , I am willing to give the authors the benefit of the doubt that they did not cherry-pick overly aggressively and that the examples are representative of typical outputs . If it turns out that most rules do not look like the examples , then my rating would decrease . On the other hand , if the authors can address my concerns about cherry-picking in the response , it is possible that I would raise my rating . Pros : * important application * method is quite general * method is simple and intuitive Cons : * evaluation of method performance limited to selected examples * reproducibility not addressed * control of false positives not addressed * method for tracking across layers not well-explained", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for this extensive and positive feedback . - If you train twice with different seeds , how similar are the results ? This is a very interesting question . We have not yet considered this , but indeed , it should be possible to use ExplaiNN as a tool to see if two networks ( e.g.trained on the same data , but with different seed ) have learned the same main concepts ! Essentially , if the networks have learned the same concepts , have approximatey the same generalization error , then between the networks the neurons are merely `` permuted '' and hence ExplaiNN would discover the same rule sets for both networks ( up to neuron ids ) . The moment that ExplaiNN discovers strongly diverging rule sets , this gives evidence that the networks learned intrinsically different concepts . We could even consider how well the rule set for Network A can be used to compress ( after neuron-id optimization ) the activations of Network B ( and vice-versa ) . Far beyond the scope of this paper , but fascinating ! - If you fit ExplaiNN with different seeds on the same network ? Maybe we misunderstood the question , but ExplaiNN is deterministic , for the same data iw will always return the exact same result . - What is the danger of false positive findings ? If we interpret this question correctly , a false positive finding corresponds to a rule that does not reflect information that is important for the data . If that is the case , it is also very unlikely that these neurons are sufficiently often co-activated for a rule over them to improve compression compared to the base-line model , which assumes independence between neurons . In a nutshell , MDL ensures that only rules for which sufficient evidences exists in the data are added to the model , as otherwise the number of bits needed to encode data and model would unnecessarily increase . This question will also be implicitly covered by the new synthetic data experiments in response to Reviewer 1 . - Question about tracing . That is a good point , we will add a more extensive description in the revised manuscript . Suppose we have three consecutive layers L1 - > L2 - > L3 . We first discover all rules X- > Y , X \\subset L3 , Y \\subset L2 . In the next step , for each such Y , we run ExplaiNN to discover rules Y- > Z , where Z \\subset L1 . There also exist other approaches that would work , for example mining rules between each consecutive layer pair , and then combine those that `` overlap '' -- which we think is what your comment described . Our iterative approach has the advantage that we can force the model to focus on what is necessary to build traces and thus get cleaner results , with the general pairwise application we have to do the filtering by overlap that you mentioned to get the clean results . - Concern about cherry-picking We could not agree more with this concern -- not so much for ExplaiNN , but for explanatory approaches for CNNs in general . A key advantage of our MDL-based approach is that we discover succinct and non-redundant sets of rules , here resulting in hundreds to at most a few thousand rules , which we can analyze in its entirety . For the VGG-S network , the existing highly optimized prototyping allowed us to generate prototypes for all discovered rules . For the large majority the result looks meaningful , it is clear what information they encode , and how it relates to the output . As common with prototyping , there are also few that are hard to interpret , or reveal insights only on second glance , because they e.g.encode abstract information ( geometric shapes , small subparts of an object ) . For rules with larger heads ( spanning many classes ) the prototypes often reflect some of the classes well , while the other classes seem to utilize only certain information from the neuron ( e.g.a pattern , a red leg ) . Such classes are classified by information of multiple neuron groups - found in several rules - which together provide the whole picture . Overall we picked a representative subset that reflects the information content of most of the prototypes . We will make the code available once the paper is published , such that the results can be reproduced and everybody can take a look on what the network sees . We now also added two additional panels containing 20 additional prototypes to the appendix , which we think are larger groups of representatives of our findings ."}, "2": {"review_id": "N6SmiyDrkR5-2", "review_text": "This paper proposes to extract interpretable rules from a learned neural network . The authors claim that they are the first to propose rules connecting 1 ) multiple neurons together , and 2 ) do this at a dataset level . Their approach relies on using minimum description length and well known principles from the data mining community ( e.g. , downward closure lemma of apriori algorithm ) . The authors claim that experiments conducted on image data shows that their approach leads to more faithful , interpretable rules than other approaches such as prototyping or model distillation . Compiling rules from neural networks has been proposed before ( e.g. , see `` Deep Logic Networks : Inserting and Extracting Knowledge from Deep Belief Networks '' by Tran and d'Avila Garcez , 2018 ) . I think the authors need to compare against such previous approaches to quantitatively show how their work extracts better rules . Otherwise , its difficult to appreciate the value of ExplaiNN . Also , the paper does n't say anything about how faithful the rules are to the learned neural network . I mean , it seems possible that for some input the rules could produce a different output from the neural networks . There exist other works that also try to interpret neural networks consisting of affine layers with Relu activation that guarantee consistency ( see `` Exact and Consistent Interpretation for Piecewise Linear Neural Networks : A Closed Form Solution '' by Chu et al in KDD'18 ) . The authors should at the very least compare and contrast with such works to highlight the pros and cons . Another undesirable property of ExplainNN seems to be that it relies on a dataset to derive its rules . Is it possible that when run with a different dataset and the same learned network , ExplaiNN would produce a different set of rules ? Then how much faith do we place on ExplaiNN 's output ? Writing wise , the paper is presented well enough . There 's a few paragraphs in the Experiments section where the authors point repeatedly to the Appendices . In the best case this makes reading a chore . I would advise the authors to refrain from using the main body of the paper as a listing of contents and simply pointing to the appendices . The pictures in the experiments section were difficult to make out . I could n't figure out from the image whether the husky 's pointed snout had been identified as a defining feature . I would hope the authors find more compelling ways to make their point .", "rating": "3: Clear rejection", "reply_text": "Thank you for your comments . Both references are relevant related work , and we will add them to the manuscript . Unlike our approach , both are tailored to specific types of neural networks and do not generalize to state-of-the-art architectures such as e.g.CNNs , and use explanations that are hard to interpret - either by the type of information they retrieve or the sheer size of results returned . Concretely , Tran et al.is limited to Deep Belief networks ( stacked RBMs ) and optimizes for high confidence rules . Mining high confidence rules has serious practical issues \u2013 millions of highly redundant and often spurious rules are mined from small data already , even when the data contains no structure . Modern pattern ( set ) mining methods avoid this issue by relying on statistical tests and/or information theoretic notions . The work by Chu et al.does not suffer from the pattern explosion , but is limited to very small ( < 10 neurons per layer ) piecewise linear neural networks ( see Table 3 in their paper ) as it is based on ( theoretically interesting ) polytope analysis . Moreover , rather than characterizing what neurons inside the network encode , it aims to explain neural activity in terms of network input ( see Figure 1,5,6 in their paper ) . Moreover , it is much harder to interpret what the polytope actually expresses , compared to how accessible rules are . Regarding your last question , we indeed propose to analyze network behaviour based on neuron activations for a given dataset . We consider this an advantage rather than a disadvantage , as we want to learn how the network perceives the world ( the input ) and how and which information it composes and combines from the input to arrive at a decision . Analyzing the network without an input is like analyzing the human brain without stimulus . It also opens the possibility to explore how a network \u2019 s behavior changes when we change either the input distribution ( train/test , ood ) or when we re-train it in a slightly different context , like in the transfer learning task ."}, "3": {"review_id": "N6SmiyDrkR5-3", "review_text": "Paper Summary The authors propose a method to explore how neurons interact within a neural network and derive rules of interactions that can help interpret the inner workings of the neural network and open up the black box . The algorithm , EXPLAINN , identifies rules between successive layers where each rule represents a set of neurons that are activate simultaneously and conditionally based on the previous layer . Minimum Description Length principle is used to derive an objective that minimizes the number of bits used to encode the rules . The rule sets are identified using a greedy heuristic and improved until convergence of the objective . The algorithm is then evaluated to demonstrate the interpretation of images with MNIST , GoogLeNet and VGG-S. Positives * The problem at hand is clearly important and can help with interpretation of the neural networks . This is particularly important in fields such as biology ( genomics ) where better interpretation is often times desired at a slight cost in performance * The formulation using Minimum Description Length Principle is definitely an interesting idea - particularly formulating the objective as set of independent and robust rules * The framework is flexible and allows for discovery of rules relevant for subsets / combinations of classes and not just individual examples or global rules across examples * The paper in general is well written and I particularly liked how the authors guide the readers through the different aspects of the algorithm evaluation . I particularly liked Appendix A since the concrete example made the objective function clear . The notations however can be better laid out and clarified . Concerns * My primary concern is the use of greedy heuristic to identify the rule set . This necessitates that each interaction individually carry some degree of information for a robust composite rule . While the rules identified through the heuristic could be informative , it is not clear whether they are the best set of rules since there are no bounds or guarantees of how the heuristic relates to the global optimum . * On a related note , I felt the evaluation presented by authors while extensive is rather qualitative in nature . While the prototypes of identified rules across different datasets look relevant and interesting , it is not clear whether they are the best set of rules . So , I believe this manuscript needs benchmarking in datasets constructed with known rules ( possibly through simulation ) to alleviate these two major concerns * The quantitative outputs of the ReLU activations are binarized for rule learning - the authors have not addressed how this impacts the accuracy of the rules since that quantitative information is used by the subsequent layers * The impact of the threshold parameters theta and mu ( Algorithms 1 & 2 ) are also not addressed - this seems critical to me since these thresholds define the initial set of rule candidates .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the constructive critique , this certainly helps to improve the manuscript . Regarding your first two concerns : We agree that quality guarantees or even optimal search are desirable , however , as our MDL objective is neither monotone nor sub-modular , and as the search space is exponential , the only available option is to employ heuristics . Our bottom-up strategy is based on the idea that a good large pattern , i.e.one that improves compression when we add it to our model , is composed of smaller parts that all help to compress the data . This is a commonly employed , succesful approach in pattern set mining , see e.g.GRAB [ 1 ] , SLIM [ 2 ] , SQS [ 3 ] , kGIST [ 4 ] . One of the intuitions we exploit is that a larger rule will only lead to better compression if the ` tails ' of its generating rules co-occur under the same conditions , i.e.when the head of the rule is present . As if that is not the case , the new larger rule would leave many rows unexplained , and would hence likely not yield any gain . Before starting with the NN experiments , we validated this approach via initial experiments on synthetic data . We will update the manuscript with an extended version of these experiments as soon as they are finished . Regarding the third concern : It is correct that binarizing ReLU activations may result in information loss \u2013 activation strength might , for example , be used by subsequent layers \u2013 but by doing so we do gain a huge advantage in terms of interpretability : binarizing neuron activations allows us to derive crisp symbolic , and directly interpretable statements on how neurons interact . Moreover , the on/off state not only reflects how biological neurons function , but also closely matches tanh and sigmoid activation functions . Extending ExplaiNN to an easily interpretable pattern language that allows for continuous , ReLU-like , activations is an interesting line for future work . Regarding the fourth concern : Both these easily interpretable parameters are merely simple , yet effective runtime optimizations . From an MDL-perspective , the best results will always be obtained with the largest search space : i.e.with $ \\theta $ and $ \\mu $ set to 0 , respectively $ |X_1| + |X_2| $ , as this corresponds to considering all combinations of rules . Besides impacting run-time , many of those rules may be uninteresting from a user-perspective . Mu and theta allow users to directly instruct ExplaiNN to ignore such rules . Here , we consider a good rule as one for which the tail items frequently occur when the head items are present . In other words , for a given rule $ X \\rightarrow Y $ , if neurons $ X $ are active , the neurons $ Y $ should also likely to be active . This is reflected by the confidence parameter $ \\theta $ , which is the chance that $ Y $ is active when $ X $ is active . In our experiments we found that rules of theta below 0.4 are too weak to reveal valuable insights , but this may be different for different users or domains . For the parameter $ \\mu $ , we want to account for early rule merging decisions that hinder us to see a more general trend . For example it might be that we learned $ ABC \\rightarrow Y $ and $ BCD \\rightarrow Z $ , which both individually make sense and give gain , yet the rule $ BC \\rightarrow YZ $ might be more general and yield even higher gain , but was not possible to obtain when the individual rules were considered . We used $ \\mu=6 $ , which allows to merge fairly large patterns with huge differences . [ 1 ] Fischer , J. , Vreeken , J . Sets of Robust Rules , and How to Find Them . Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Data ( ECMLPKDD ) , Springer , 2019 . [ 2 ] Smets , K. , Vreeken , J . SLIM : Directly mining descriptive patterns . Proceedings of the SIAM International Conference on Data Mining ( SDM ) , SIAM , 2012 . [ 3 ] Tatti , N. , Vreeken , J . The Long and the Short of It : Summarising Event Sequences with Serial Episodes . Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , ACM , 2012 [ 4 ] Belth , C. , Zheng , X. , Vreeken , J. , Koutra , D. What is Normal , What is Strange , and What is Missing in a Knowledge Graph . Proceedings of the Web Conference ( WWW ) , ACM , 2020 ."}}