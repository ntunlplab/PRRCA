{"year": "2017", "forum": "HksioDcxl", "title": "Joint Training of Ratings and Reviews with Recurrent Recommender Networks", "decision": "Reject", "meta_review": "The paper has some nice ideas, but requires a bit to push it over the acceptance threshold. I agree with the reviewers who ask for comparisons with other rating-review methods, and that other evaluation metrics more appropriate to the recommendation tasks should be reported. More analysis of the model, and the factors that contribute to its performance, would greatly improve the paper.", "reviews": [{"review_id": "HksioDcxl-0", "review_text": "This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments: 1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing. 2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature. Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the feedback . Broadly speaking , we are working to expand our experiments overall . As we mention in another comment below , we are working to compare against other rating & review systems , which will give us a better sense of how our model uses review data to improve prediction accuracy . With respect to applicability to ICLR , we believe that our model design is novel and provides insight into how to successfully jointly learn from temporal data , ratings , and reviews . As co-training is a continually growing area of interest in the ICLR community , exploring those ideas and how they work in the recommendation setting , which differs from the CV or NLU settings , is important in furthering our understanding of deep networks ."}, {"review_id": "HksioDcxl-1", "review_text": "This paper proposed a joint model for rate prediction and text generation. The author compared the methods on a more realistic time based split setting, which requires \u201cpredict into the future.\u201d One major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information. A more clear study on impact of each component could make it more clear and benefit the readers. Another potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings. The overall technique is intuitive and novel, but can be improved to give more insights to the reader,. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for for your time and comments . We are working on running more experiments , particularly with respect to additional baselines so as to compare the impact of BOW-style modeling versus RNN based models , and also compare different strategies of linking review model to factors . Additionally , we believe that using an RNN model is a step in the direction toward generating reviews and explaining predictions . Second , we decided to begin with the rating prediction task as it offers a standard intrinsic evaluation that directly measures the model accuracy and is widely used across the academic literature . However , we agree that extending this model to other objective functions is an interesting next step for the work ."}, {"review_id": "HksioDcxl-2", "review_text": "The paper seeks to jointly model ratings and reviews in addition to temporal patterns. Existing papers have captured these aspects previously, but what's novel here is the particular combination of parts and choice of techniques. In particular, the use of LSTMs as opposed to \"bag-of-words\" models that have previously been used when combining the same components. A criticism is made of existing models that use bag-of-words features as being too \"coarse\" to capture the real dynamics of reviews. This seems a valid criticsm, though it's not clear exactly what features those existing models miss. Something missing from this paper is any interpretation of what the model \"learns\" that may explain its better performance. I also don't know about the significance of predicting \"future\" ratings as opposed to random splitting of the data. Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings. Is there a methodological contribution associated with this change? The experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text. Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included. Overall this is a reasonably strong experimental comparison, though could be improved in two dimensions: (a) There's no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.). These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here. Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text. (b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation. In particular, they're extremely dense datasets that support very parameter-rich models. I'd question whether the results would hold on sparser data (though in fact I'd suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger). Otherwise the experiments are fine. Perplexity results are nice but essentially what we'd expect. It is a shame that unlike other papers that use text to inform recommender systems there's no high-level analysis here of what the model has uncovered. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the valuable comments . In real-world recommendation settings , a system needs to predict future ratings instead of interpolating past ratings with the benefit of hindsight . The former is much harder than the latter . For instance , knowing that a user will have developed a liking for Pedro Almod\u00f3var in the future makes it much easier to estimate his opinion about La Mala Educaci\u00f3n . The violation of causality in statistical analysis also makes it impossible to translate the reported accuracy to online performance in practice . We are working on carrying out some comparison to other review-rating models , and the results will be presented in the final version ."}], "0": {"review_id": "HksioDcxl-0", "review_text": "This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments: 1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing. 2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature. Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the feedback . Broadly speaking , we are working to expand our experiments overall . As we mention in another comment below , we are working to compare against other rating & review systems , which will give us a better sense of how our model uses review data to improve prediction accuracy . With respect to applicability to ICLR , we believe that our model design is novel and provides insight into how to successfully jointly learn from temporal data , ratings , and reviews . As co-training is a continually growing area of interest in the ICLR community , exploring those ideas and how they work in the recommendation setting , which differs from the CV or NLU settings , is important in furthering our understanding of deep networks ."}, "1": {"review_id": "HksioDcxl-1", "review_text": "This paper proposed a joint model for rate prediction and text generation. The author compared the methods on a more realistic time based split setting, which requires \u201cpredict into the future.\u201d One major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information. A more clear study on impact of each component could make it more clear and benefit the readers. Another potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings. The overall technique is intuitive and novel, but can be improved to give more insights to the reader,. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for for your time and comments . We are working on running more experiments , particularly with respect to additional baselines so as to compare the impact of BOW-style modeling versus RNN based models , and also compare different strategies of linking review model to factors . Additionally , we believe that using an RNN model is a step in the direction toward generating reviews and explaining predictions . Second , we decided to begin with the rating prediction task as it offers a standard intrinsic evaluation that directly measures the model accuracy and is widely used across the academic literature . However , we agree that extending this model to other objective functions is an interesting next step for the work ."}, "2": {"review_id": "HksioDcxl-2", "review_text": "The paper seeks to jointly model ratings and reviews in addition to temporal patterns. Existing papers have captured these aspects previously, but what's novel here is the particular combination of parts and choice of techniques. In particular, the use of LSTMs as opposed to \"bag-of-words\" models that have previously been used when combining the same components. A criticism is made of existing models that use bag-of-words features as being too \"coarse\" to capture the real dynamics of reviews. This seems a valid criticsm, though it's not clear exactly what features those existing models miss. Something missing from this paper is any interpretation of what the model \"learns\" that may explain its better performance. I also don't know about the significance of predicting \"future\" ratings as opposed to random splitting of the data. Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings. Is there a methodological contribution associated with this change? The experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text. Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included. Overall this is a reasonably strong experimental comparison, though could be improved in two dimensions: (a) There's no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.). These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here. Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text. (b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation. In particular, they're extremely dense datasets that support very parameter-rich models. I'd question whether the results would hold on sparser data (though in fact I'd suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger). Otherwise the experiments are fine. Perplexity results are nice but essentially what we'd expect. It is a shame that unlike other papers that use text to inform recommender systems there's no high-level analysis here of what the model has uncovered. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the valuable comments . In real-world recommendation settings , a system needs to predict future ratings instead of interpolating past ratings with the benefit of hindsight . The former is much harder than the latter . For instance , knowing that a user will have developed a liking for Pedro Almod\u00f3var in the future makes it much easier to estimate his opinion about La Mala Educaci\u00f3n . The violation of causality in statistical analysis also makes it impossible to translate the reported accuracy to online performance in practice . We are working on carrying out some comparison to other review-rating models , and the results will be presented in the final version ."}}