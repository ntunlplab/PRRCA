{"year": "2021", "forum": "1flmvXGGJaa", "title": "NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search", "decision": "Reject", "meta_review": "The contributions of this paper lie in two areas: a new benchmarking dataset and a new way to generate benchmarking datasets. Overall, the reviewers are split in their assessment based on which particular area they are focusing on. Reviewers who focus more on evaluating this work as a new benchmarking dataset, correctly point out that the variation within the search space has been shown to be limited and that the evaluation focuses on an overly-studied and toy (by today\u2019s standards) dataset such a CIFAR-10. In terms of choice of dataset, this work is indeed a step backwards from nasbench201, which includes more datasets, although it is a step forward in terms of size of the search space. As one reviewer correctly points out \u201cThis paper doesn\u2019t present a benchmark. It provides a model that represents computationally-efficient means of getting network accuracies from the DARTS search space\u201d. The authors argue that the combination of the DARTS search space and its evaluation on CIFAR-10 is the de-facto evaluation standard in the NAS community, which is also true. Ultimately, benchmark datasets somewhat direct the attention of the community and this attention would be better directed elsewhere, not on DARTS+CIFAR-10, as pointed out by some of the reviewers.\n\nOn the other hand, this work is as much about a new benchmark as it is about a *protocol* to generate new benchmarks. Specifically, a big part of the appeal and novelty here lie in the idea of training a predictive model from a small subset of architecture evaluations. From this perspective, the authors showed evidence that their approach is sound, economical (in terms of computational cost) and robust to sources of bias in the selection of the architectures to evaluate. The limitation here is that this was only shown in search spaces that where either small or lacking diversity and thus it\u2019s unclear how general its findings are.\n\nOverall, this is very much a paper that could have gone either way in terms of acceptance. It\u2019s a step in the right direction in terms of methodology that can be used to generate reproducible benchmarks in a computationally efficient way. It\u2019s a step backwards in terms renewing focus on measures of performance that (arguably) we have all overfit to. \n", "reviews": [{"review_id": "1flmvXGGJaa-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This work filled an important gap in the NAS benchmarks . The previous benchmarks only contain small search space due to the expensive cost of evaluation of neural architecture . In this search space , random search often becomes competitive in the narrow search space . Thus , to provide meaningful comparison , this work provided a benchmark in a large NAS search space ( same as in DARTS ) , and using surrogate models to predict validation performance of untrained neural architecture . The empirical results suggested using the surrogate benchmarks resulted in similar optimization trajectory as real evaluations and the author also shows one can derive/validate research ideas quickly with the benchmarks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . This work filled an important gap in NAS benchmarks . 2.The empirical results are very solid ; the observation on the noise in the training is very insightful . 3.The work also contains guidelines for using the benchmarks . 4.The paper is very well written and contains enough detail for reproducibility . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : I only have some minor comments : 1 . In Section 3.2 , first paragraph , the author mentioned that validation and test error are highly correlated . This is clear if the poor performing architectures are included . But in practice , we are only interested in the good performing region , say top 5 % , is the correlation still high between validation and test there ? 2.In Section 4.2 , second paragraph , I am not sure I understand `` We use train/val/test splits ( 0.8/0.1/0.1 ) stratified by the NAS methods used for the data collection '' . 3.Given the mean and noise estimation based on the surrogate models , is the assumption there is gaussian and every experiment will draw one value from this gaussian ? If so , could you state it clearly in the paper ? If not , please clarify . ==POST-REBUTTAL COMMENTS== Initially I had only minor comments and the authors addressed all of them . I will keep my score .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer very much for the positive feedback and for the acceptance score . In the following we address his/her comments and questions . 1 . * * In Section 3.2 , first paragraph , the author mentioned that validation and test error are highly correlated . This is clear if the poor performing architectures are included . But in practice , we are only interested in the good performing region , say top 5 % , is the correlation still high between validation and test there ? * * : We calculated the correlation between validation and test accuracy on the top 5 % of architectures ( w.r.t.validation accuracy ) as predicted by our surrogate XGBoost model to be 0.37 ( Kendall tau ) / 0.52 ( Spearman rank corr . ) . As comparison , we also calculated this for NAS-Bench-101 which yielded 0.26 ( Kendall Tau ) / 0.37 ( Spearman rank corr . ) and NAS-Bench-201 which yielded a comparable 0.47 ( kendall tau ) / 0.65 ( Spearman rank corr . ) . 2 . * * In Section 4.2 , second paragraph , I am not sure I understand `` We use train/val/test splits ( 0.8/0.1/0.1 ) stratified by the NAS methods used for the data collection '' . * * : We apologize for not being clear enough . This means that in each of the train/val/test splits the fraction of architectures corresponding to a particular optimizer is the same as in the union of train , val and test . For example , we use RS to sample 50 % of the total architectures we train and evaluate . Each of the train , val and test will thus contain 50 % of the architectures coming from RS . Equivalently , if the train : val : test ratio is 8:1:1 , then 80 % of the architectures collected from every NAS optimizer will be included in the training set . We added a more thorough description of this point in Section 4.2 of the updated paper . 3 . * * Given the mean and noise estimation based on the surrogate models , is the assumption there is gaussian and every experiment will draw one value from this gaussian ? If so , could you state it clearly in the paper ? If not , please clarify . * * : Yes indeed ; we had briefly stated this in Section 4.4 , but we added a more detailed statement now . The performance is queried from the surrogate ensemble by sampling from the predictive distribution which is modelled as a normal distribution ."}, {"review_id": "1flmvXGGJaa-1", "review_text": "In this work , the authors train a model on a subset of architectures ( ~60k ) in the DARTS search space and use this model to predict the performance of architectures outside of that subset in DARTS . They then use this as a surrogate for having to perform network training for evaluating NAS algorithms . NAS benchmarks are a good thing , given that they facilitate NAS research outside of labs with lots of resources . Saying this , I think there are serious issues with this paper . NAS-Bench-301 is a model that predicts the performance of networks in the DARTS search space . We know from https : //arxiv.org/abs/1912.12522v3 that there is a significant lack of variety in the performance of models within this space , and this work compounds that . Additionally , this paper only considers CIFAR-10 . This is a step back from NAS-Bench-201 which despite its small size , did contain multiple datasets . The compute used by the authors ( training 60k networks ) has gone into differentiating a bunch of networks that are all quite good , within a few percentage points of error . I \u2019 m not sure how this is of practical use . A problem with NAS-Bench-101 and 201 is that the search spaces are small ( 423k , 6k ) as the authors point out . NAS-Bench-301 encapsulates the DARTS search space which is much bigger ( 10^18 ) . However , from https : //arxiv.org/abs/1912.12522v3 we see that randomly sampling within this space gives networks between 96.5 % and 97.5 % . I would argue that it doesn \u2019 t matter how large a space is if it is lacking in variety ; Every possible network works well enough . In 101/201 we see networks across a much larger range ( typically between 80-95 % although there are some much lower ) . I believe this is more interesting from a research perspective as we would like to apply NAS to situations where networks can break ( and avoid this happening ) . The DARTS networks do explore a slightly higher accuracy range but it is not at state-of-art levels , so the added value is not clear . The authors are critical of random search , stating ( i ) ` random search stagnates and can not identify one of the best architectures even after tens of thousands of evaluations ` , ( ii ) NAS-Bench-201 ( Dong & Yang , 2020 ) only contains 6466 unique architectures in total , causing the median of random search runs to find the best architecture after only 3233 evaluations . However , ( i ) makes random search sound like it is failing , where on Figure 4 we can see at 10^4 ( s ) random search is doing very well - matching or beating all the other techniques . We know from https : //arxiv.org/abs/1806.09055 that random search works well on the DARTS space ( probably due to the lack of variety ) . ( ii ) Random search does work well on NAS-Bench-201 but the DARTS algorithm fails , even though it works on this space . The statement in the abstract that using previous benchmarks \u201c leads to unrealistic results that do not transfer to larger search spaces. \u201d does not tell the whole story , as there appears to be more going on than just the size of the search space . Some results on this large DARTS search space , do not transfer to the smaller search spaces . It seems that there is more to a search space than the number of architectures within it . In terms of presentation , this paper is well written , although the figures could be larger . I appreciate that this is problem to keep within the page limit . NAS benchmark papers are important and shape the direction of research in the field . This paper doesn \u2019 t present a benchmark . It provides a model that represents computationally-efficient means of getting network accuracies from the DARTS search space . I appreciate this endeavour and it could be of use outside of this space \u2014 being able to map a 10^18 space with 50000 points indicates that the effective size of the space is much smaller , and is highly predictable . This space , although large , has very little variety in terms of network accuracy . I believe NAS-Bench-101 and 201 despite their sizes represent more varied search spaces . 201 also covers multiple datasets , whereas 301 only has CIFAR-10 ( which it feels like we are saturating on ) . I recommend rejection for this paper , as I do not believe it represents a step forward in the way we benchmark our NAS algorithms . We need to develop more interesting search spaces , rather than advocating exploration of uninteresting ones .", "rating": "3: Clear rejection", "reply_text": "We would like to thank the reviewer for taking the time to read our paper and for his/her feedback . We now address the reviewer \u2019 s concerns : 1 . * * We know from https : //arxiv.org/abs/1912.12522v3 that there is a significant lack of variety in the performance of models within this space , and this work compounds that . * * : We note that the distribution of architecture performances in the DARTS search space also depends on the choice of macro architecture and training pipeline [ 1 ] . We would also like to point out that Figure 8 in the appendix shows the ECDF for different optimizers and indicates that there is variety in our setting . We agree with the reviewer that the DARTS search space does not yield the highest variety and that the NAS research community * must * move forward to other interesting spaces . Nevertheless , the DARTS space is still the most widely used non-tabular search space and many NAS papers are burning substantial GPU time on this space . With NAS-Bench-301 , we provide a cheap way to prototype and test NAS algorithms that we hope will be of great help to the community , particularly practitioners with fewer resources . While tabular benchmarks restrict the community to small search spaces that are only feasible to evaluate exhaustively , we believe that NAS-Bench-301 might be a milestone for creating future fast-to-evaluate surrogate benchmarks on large and interesting search spaces , enabling the community to focus its resources and time on these new benchmarks rather than spending them on the DARTS space . 2 . * * Additionally , this paper only considers CIFAR-10 . This is a step back from NAS-Bench-201 which despite its small size , did contain multiple datasets . * * : As mentioned above , we chose our search space & dataset , DARTS + CIFAR-10 , based on the most popular space for which most computational resources are currently used . Unfortunately , our own computational budget did not allow us to cover multiple datasets but our approach trivially applies to other spaces and datasets . We would also like to emphasize that our work is the first to propose surrogate benchmarks for NAS and shows that they are indeed applicable . In doing so , our fundamental work on the methodology for creating fast-to-evaluate NAS benchmarks opens up the door to fast-to-evaluate NAS benchmarks for the large set of varied and exciting NAS applications the community is developing these days . 3 . * * The compute used by the authors ( training 60k networks ) has gone into differentiating a bunch of networks that are all quite good , within a few percentage points of error . I \u2019 m not sure how this is of practical use . * * : We point to Figure 8 in our Appendix which shows ECDFs for different optimizers . We note that there are architectures with performances between 80-95 % validation accuracy . Importantly , NAS-Bench-101 and NAS-Bench-201 report the performance of the architectures in the whole search space , while we focus more on collecting architectures from distinct regions of the search space via several NAS optimizers . We refer the reviewer to the t-SNE plots in Fig.9 ( Appendix C.4 ) to visualize that these optimizers exploit different well-performing regions of the architecture space ( e.g.compare DE and BANANAS ) ."}, {"review_id": "1flmvXGGJaa-2", "review_text": "* * * * Update after rebuttal * * * * I am increasing my rating for the paper as they did all the experiments I had asked for and updated the paper accordingly . * * * * * * * * * * * * * * * * * * * * * * * * * * Summary : While NAS has made tremendous advances in recent past , benchmarking algorithms with respect to each other still remains a challenge . Tabular benchmarks like 101 and 201 , take a search space and train all possible architectures in them . While this is possible to do for relatively small search spaces and datasets , this is impractical to repeat for larger search spaces ( e.g.DARTS ' search space which has 10^18 architectures ) . This work makes the nice observation that a tabular benchmark treats each architecture as an independent random variable and does n't utilize any similarities between them . Due to similarities in architecture space , knowing the train/val/test accuracy of one architecture tells us a lot about other architectures nearby . So a predictive model trained on a sparse subset of architectures can actually outperform an exhaustive tabular method . Lots of careful experiments are reported on the DARTS search space to create predictive models which can accurately predict architecture performance ( accuracy , latency ) and hence can be used as a 'simulator ' by NAS algorithms for rapid research and fair comparison . Comments : - The paper is generally well-written and has thorough clean experiments ! Thanks ! - My main concern is the following : The fact that the benchmark has added architectures encountered on the trajectory of well-known performing optimizers bothers me a bit . In the ideal world a benchmark should have no knowledge of any particular solution to the problem . This is true in the case of tabular benchmarks like 101 and 201 . I understand the position that a surrogate model should be very good at predicting parts of the architecture space which optimizers are most likely to visit . Can we construct surrogates without knowing anything about any particular optimizers the community may invent in the future ? One part of an ablation study answering this question has been presented in Appendix E.2 where a model has been only trained on well-performing architectures ( above 92 % accuracy ) and in Figure 21 has been found adequate for predicting the trajectories of BANANAS and Random Search ( RS ) . This begs the question of what if we only used random sampling to fit surrogate models ? ( Of course coverage methods like adaptive submodularity-based greedy algorithms may result in even better performance ) . But can we do the easy baselines first for which the authors already have the data : 1 . Train surrogate model using only the 23746 architectures via random sampling and plot the same figures as in Fig 4 center and right . How much worse are they from current ones ? 2 . ( if compute allows ) : Use up the entire budget of ~60k architectures in Table 2 only from random sampling and plot Fig 4 center and right . How much worse are they from current ones ? 3 . ( to have a fair comparison of current method to baseline 1 without using much more compute ) : Keep the total budget 23746 but fill them up in the same proportion from each method as currently in Table 2 . For example RS will have 23746/~60k ratio , DE will have 7275/~60k ratio , etc . This will create `` NASBench-301-small '' which will have the same budget as baseline 1 above . Then plot Fig 4 center and right with both this and 1 . These might be competitive baselines because the Once-For-All work from Cai et al.ICLR 2020 uses a regressor trained on 16k architectures sampled from a supergraph as a surrogate model to run evolutionary search against and obtain good performance . Also in this paper itself if more than 21500 architectures on 101 are used to train ( unclear from the paper whether they were randomly sampled and diverged models rejected or some other technique was used to select them , since it says `` subsets of D^ { train } '' but I think they were randomly sampled , right ? ) , then that itself is better than the tabular benchmark . Happy to be convinced if these are fair baselines or not . ( Also possible that these are already included and I missed them.Ther are a lot of experiments : - ) ) -My other worry is that the LOOO ablation may be misleading since reasonable optimizers may be visiting similar parts of the architecture space hence may give a false sense of extrapolation .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for reading the paper thoroughly , the positive feedback , and for the detailed and very useful review . We now reply to the questions : 1 . * * My main concern is the following : The fact that the benchmark has added architectures encountered on the trajectory of well-known performing optimizers bothers me a bit . In the ideal world a benchmark should have no knowledge of any particular solution to the problem . This is true in the case of tabular benchmarks like 101 and 201 . I understand the position that a surrogate model should be very good at predicting parts of the architecture space which optimizers are most likely to visit . Can we construct surrogates without knowing anything about any particular optimizers the community may invent in the future ? * * : We think this is a very interesting question and we agree that a benchmark ideally should have no prior knowledge about any particular solutions of a NAS optimizer . To this end we conducted the Leave One Optimizer Out ( LOOO ) experiment in Section 5.1 to remove some of this bias when evaluating the soundness of NAS-Bench-301 . It is true that most of the NAS optimizers we use after some time find really competitive architectures , but as one can see from the t-SNE plots in Fig.9 ( Appendix C.4 ) these optimizers exploit different good-performing regions of the architecture space ( e.g. , compare DE and BANANAS ) . Finally , we also think that another approach that might yield better \u201c unbiased \u201d benchmarks could be to use more sophisticated space-filling methods like quasi-random sequences , e.g.Sobol sequences [ 1 ] , Latin Hypercubes [ 2 ] or Adaptive Submodularity [ 3 ] . 2 . * * On the reviewer suggestion to : - Train surrogate model using only the 23746 architectures via random sampling and plot the same figures as in Fig 4 center and right . How much worse are they from current ones ? - ( to have a fair comparison of current method to baseline 1 without using much more compute ) : Keep the total budget 23746 but fill them up in the same proportion from each method as currently in Table 2 . For example RS will have 23746/60k ratio , DE will have 7275/60k ratio , etc . This will create `` NASBench-301-small '' which will have the same budget as baseline 1 above . Then plot Fig 4 center and right with both this and 1 . * * : This is a very interesting experiment . Thank you for the suggestion ! We are running this , and we will post a reply when we have these plots ready to update the paper accordingly . 3 . * * ( if compute allows ) : Use up the entire budget of ~60k architectures in Table 2 only from random sampling and plot Fig 4 center and right . How much worse are they from current ones ? * * : Unfortunately , as the reviewer assumed , our compute resources do not allow us to collect this many architectures during this short time frame . However , this is an interesting experiment for the future since anyway we are planning to increase the coverage of the space in the long run . 4 . * * Also in this paper itself if more than 21500 architectures on 101 are used to train ( unclear from the paper whether they were randomly sampled and diverged models rejected or some other technique was used to select them , since it says `` subsets of D^ { train } '' but I think they were randomly sampled , right ? ) , then that itself is better than the tabular benchmark . * * : The reviewer is correct , the architectures in the training subsets were uniformly sampled at random . However , prior to sampling , any architectures in which one of the three evaluations diverged , were removed ( as done in e.g. [ 4 ] ) .5 . * * My other worry is that the LOOO ablation may be misleading since reasonable optimizers may be visiting similar parts of the architecture space hence may give a false sense of extrapolation . * * : This is a fair point , however it might also be the case that there exist many distinct regions in the architecture space that yield architecture with good and similar performance . We think this is the case for the DARTS search space and the t-SNE plots for the individual optimizers in Figure 9 indicate that certain optimizers ( e.g.BANANAS ) visit distinct subspaces and are still predicted accurately in the LOOO ( Figure 5 ) . -- References -- [ 1 ] M. Sobol . On the distribution of points in a cube and the approximate evaluation of integrals . Zhurnal Vychislitel ` noi Matematiki i Matematicheskoi Fiziki . 7 ( 4 ) :784-802 , 1967 . [ 2 ] McKay et al.A comparison of three methods for selecting values of input variables in the analysis of output from a computer code . Technometrics , 2000 . [ 3 ] Golovin and Krause . Adaptive submodularity : Theory and applications in active learning and stochastic optimization . Journal of Artificial Intelligence Research , 42:427-486 , 2001 . [ 4 ] Wen et al.Neural predictor for neural architecture search . In ECCV 2020 ."}, {"review_id": "1flmvXGGJaa-3", "review_text": "Summary : The authors propose a new benchmark for evaluating surrogate functions for architecture search . According to the authors , existing tabular architecture search benchmarks are insufficient for this purpose due to using overly small search spaces . The main difference of this benchmark and other existing architecture search benchmarks such as NAS Bench 101 and NAS Bench 201 is that they do not attempt to evaluate all the architectures in the search space and do so for a much larger search space ( DARTS ) . The authors then use this new dataset to show that surrogate functions are better than tabular estimators ( error wise ; although some lower performance architectures were discarded to make this case ) . Additionally , the authors compare the proposed benchmark ( based on surrogate functions ) with a real benchmark and observe that the results are qualitatively similar . Finally , the authors show that reevaluate the claim that local search is state-of-the-art for architecture search and find that , using their benchmark , that local search is still state-of-the-art provided that enough computational budget is available for the experiment . Pros : + The paper presents a new dataset for architecture search that does not rely on exhaustive evaluation of all architectures in the search space . The experiments conducted that compare different surrogate functions on this benchmark are solid + The paper suggests that building benchmarks for different search spaces can be accomplished through the surrogate function route where first a dataset of architectures is collected and then it is used to train a surrogate model . The surrogate function is then used as an interface between the search space and the search algorithm . This approach for building benchmarks is general and is likely to be useful in the construction of future benchmarks for architecture search . Cons : - The fact that existing benchmarks for architecture search are insufficient for surrogate function evaluation and lead to wrong inferences is perhaps insufficiently supported . It is well known in architecture search work that existing search spaces ( DARTS being one of them ) have limited performance variability and that much of the performance variation ascribed to different architecture search methods can often be ascribed to differences in search spaces . Is a single search space good enough to mitigate these problems ? For example , how do we guarantee that NAS Bench 301 is not just another dataset and guarantee that addresses some of the perceived problems with existing architecture search search spaces and benchmarks ? Comments : - Appendices were n't included in the submission . - It would be interesting to discuss how this benchmark should be placed in the context of other existing benchmarks for architecture search to guide future practice . - The motivation from Section 2 is rather sparse . It would be warranted to show that this trend is consistent with other metrics such as squared error and Kendall Tau ( i.e. , showing the ranks of different architectures are also preserved better ) . - Insufficient information about the feature representation that is used for prediction ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for taking his/her time to review our paper and for the useful feedback and suggestions . We hope that the reviewer will consider increasing his/her score after our response to his concerns which we provide in the following . 1 . * * The fact that existing benchmarks for architecture search are insufficient for surrogate function evaluation and lead to wrong inferences is perhaps insufficiently supported . * * : The main difference between existing benchmarks and realistic search spaces is their size mismatch . This in turn leads to 1 . NAS algorithms like local search to become very inefficient [ 1 ] and 2. conclusions and hypotheses drawn from these benchmarks potentially being distinct . We do not argue that existing tabular benchmarks are not interesting ( maybe easier ) spaces to benchmark surrogate models and we have already seen papers applying GNNs to learn useful representations on these tabular benchmarks [ 4 , 5 ] . What we argue is that surrogate models can effectively represent the NAS search space of these tabular benchmarks by using just a fraction of the architectures in search space ( Figure 1 in the paper ) and provide a better estimate for the test performance compared to the tabular entries ( Table 1 ) . We show that regression models can effectively model the noisy evaluations stemming from the SGD training of the neural networks better than a table which contains only a limited number of point estimates . 2 . * * It is well known in architecture search work that existing search spaces ( DARTS being one of them ) have limited performance variability * * : We agree that many existing search spaces ( including the one of DARTS ) do not yield the largest variety in performances . Before we invested the compute to train the architectures in the DARTS space , we thought hard about whether we should introduce a new search space and then create a surrogate of that , but that would come with all kinds of confounding factors , and we ultimately decided that it is far more useful ( and convincing ) to demonstrate the concept of surrogate NAS benchmarks on a highly used benchmark . This is also because DARTS is still the most widely used non-tabular search space and many current NAS papers are burning very substantial GPU time on this benchmark . We would also like to note that the performance depends on the macro architecture and the choice of training pipeline . Figure 8 in the appendix shows that our search space yields architectures with performances between 80-95 % accuracy . Note our training dataset does not encompass all architectures in the space . Since we focus on good regions the bad performing architectures are underrepresented ."}], "0": {"review_id": "1flmvXGGJaa-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This work filled an important gap in the NAS benchmarks . The previous benchmarks only contain small search space due to the expensive cost of evaluation of neural architecture . In this search space , random search often becomes competitive in the narrow search space . Thus , to provide meaningful comparison , this work provided a benchmark in a large NAS search space ( same as in DARTS ) , and using surrogate models to predict validation performance of untrained neural architecture . The empirical results suggested using the surrogate benchmarks resulted in similar optimization trajectory as real evaluations and the author also shows one can derive/validate research ideas quickly with the benchmarks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . This work filled an important gap in NAS benchmarks . 2.The empirical results are very solid ; the observation on the noise in the training is very insightful . 3.The work also contains guidelines for using the benchmarks . 4.The paper is very well written and contains enough detail for reproducibility . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : I only have some minor comments : 1 . In Section 3.2 , first paragraph , the author mentioned that validation and test error are highly correlated . This is clear if the poor performing architectures are included . But in practice , we are only interested in the good performing region , say top 5 % , is the correlation still high between validation and test there ? 2.In Section 4.2 , second paragraph , I am not sure I understand `` We use train/val/test splits ( 0.8/0.1/0.1 ) stratified by the NAS methods used for the data collection '' . 3.Given the mean and noise estimation based on the surrogate models , is the assumption there is gaussian and every experiment will draw one value from this gaussian ? If so , could you state it clearly in the paper ? If not , please clarify . ==POST-REBUTTAL COMMENTS== Initially I had only minor comments and the authors addressed all of them . I will keep my score .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer very much for the positive feedback and for the acceptance score . In the following we address his/her comments and questions . 1 . * * In Section 3.2 , first paragraph , the author mentioned that validation and test error are highly correlated . This is clear if the poor performing architectures are included . But in practice , we are only interested in the good performing region , say top 5 % , is the correlation still high between validation and test there ? * * : We calculated the correlation between validation and test accuracy on the top 5 % of architectures ( w.r.t.validation accuracy ) as predicted by our surrogate XGBoost model to be 0.37 ( Kendall tau ) / 0.52 ( Spearman rank corr . ) . As comparison , we also calculated this for NAS-Bench-101 which yielded 0.26 ( Kendall Tau ) / 0.37 ( Spearman rank corr . ) and NAS-Bench-201 which yielded a comparable 0.47 ( kendall tau ) / 0.65 ( Spearman rank corr . ) . 2 . * * In Section 4.2 , second paragraph , I am not sure I understand `` We use train/val/test splits ( 0.8/0.1/0.1 ) stratified by the NAS methods used for the data collection '' . * * : We apologize for not being clear enough . This means that in each of the train/val/test splits the fraction of architectures corresponding to a particular optimizer is the same as in the union of train , val and test . For example , we use RS to sample 50 % of the total architectures we train and evaluate . Each of the train , val and test will thus contain 50 % of the architectures coming from RS . Equivalently , if the train : val : test ratio is 8:1:1 , then 80 % of the architectures collected from every NAS optimizer will be included in the training set . We added a more thorough description of this point in Section 4.2 of the updated paper . 3 . * * Given the mean and noise estimation based on the surrogate models , is the assumption there is gaussian and every experiment will draw one value from this gaussian ? If so , could you state it clearly in the paper ? If not , please clarify . * * : Yes indeed ; we had briefly stated this in Section 4.4 , but we added a more detailed statement now . The performance is queried from the surrogate ensemble by sampling from the predictive distribution which is modelled as a normal distribution ."}, "1": {"review_id": "1flmvXGGJaa-1", "review_text": "In this work , the authors train a model on a subset of architectures ( ~60k ) in the DARTS search space and use this model to predict the performance of architectures outside of that subset in DARTS . They then use this as a surrogate for having to perform network training for evaluating NAS algorithms . NAS benchmarks are a good thing , given that they facilitate NAS research outside of labs with lots of resources . Saying this , I think there are serious issues with this paper . NAS-Bench-301 is a model that predicts the performance of networks in the DARTS search space . We know from https : //arxiv.org/abs/1912.12522v3 that there is a significant lack of variety in the performance of models within this space , and this work compounds that . Additionally , this paper only considers CIFAR-10 . This is a step back from NAS-Bench-201 which despite its small size , did contain multiple datasets . The compute used by the authors ( training 60k networks ) has gone into differentiating a bunch of networks that are all quite good , within a few percentage points of error . I \u2019 m not sure how this is of practical use . A problem with NAS-Bench-101 and 201 is that the search spaces are small ( 423k , 6k ) as the authors point out . NAS-Bench-301 encapsulates the DARTS search space which is much bigger ( 10^18 ) . However , from https : //arxiv.org/abs/1912.12522v3 we see that randomly sampling within this space gives networks between 96.5 % and 97.5 % . I would argue that it doesn \u2019 t matter how large a space is if it is lacking in variety ; Every possible network works well enough . In 101/201 we see networks across a much larger range ( typically between 80-95 % although there are some much lower ) . I believe this is more interesting from a research perspective as we would like to apply NAS to situations where networks can break ( and avoid this happening ) . The DARTS networks do explore a slightly higher accuracy range but it is not at state-of-art levels , so the added value is not clear . The authors are critical of random search , stating ( i ) ` random search stagnates and can not identify one of the best architectures even after tens of thousands of evaluations ` , ( ii ) NAS-Bench-201 ( Dong & Yang , 2020 ) only contains 6466 unique architectures in total , causing the median of random search runs to find the best architecture after only 3233 evaluations . However , ( i ) makes random search sound like it is failing , where on Figure 4 we can see at 10^4 ( s ) random search is doing very well - matching or beating all the other techniques . We know from https : //arxiv.org/abs/1806.09055 that random search works well on the DARTS space ( probably due to the lack of variety ) . ( ii ) Random search does work well on NAS-Bench-201 but the DARTS algorithm fails , even though it works on this space . The statement in the abstract that using previous benchmarks \u201c leads to unrealistic results that do not transfer to larger search spaces. \u201d does not tell the whole story , as there appears to be more going on than just the size of the search space . Some results on this large DARTS search space , do not transfer to the smaller search spaces . It seems that there is more to a search space than the number of architectures within it . In terms of presentation , this paper is well written , although the figures could be larger . I appreciate that this is problem to keep within the page limit . NAS benchmark papers are important and shape the direction of research in the field . This paper doesn \u2019 t present a benchmark . It provides a model that represents computationally-efficient means of getting network accuracies from the DARTS search space . I appreciate this endeavour and it could be of use outside of this space \u2014 being able to map a 10^18 space with 50000 points indicates that the effective size of the space is much smaller , and is highly predictable . This space , although large , has very little variety in terms of network accuracy . I believe NAS-Bench-101 and 201 despite their sizes represent more varied search spaces . 201 also covers multiple datasets , whereas 301 only has CIFAR-10 ( which it feels like we are saturating on ) . I recommend rejection for this paper , as I do not believe it represents a step forward in the way we benchmark our NAS algorithms . We need to develop more interesting search spaces , rather than advocating exploration of uninteresting ones .", "rating": "3: Clear rejection", "reply_text": "We would like to thank the reviewer for taking the time to read our paper and for his/her feedback . We now address the reviewer \u2019 s concerns : 1 . * * We know from https : //arxiv.org/abs/1912.12522v3 that there is a significant lack of variety in the performance of models within this space , and this work compounds that . * * : We note that the distribution of architecture performances in the DARTS search space also depends on the choice of macro architecture and training pipeline [ 1 ] . We would also like to point out that Figure 8 in the appendix shows the ECDF for different optimizers and indicates that there is variety in our setting . We agree with the reviewer that the DARTS search space does not yield the highest variety and that the NAS research community * must * move forward to other interesting spaces . Nevertheless , the DARTS space is still the most widely used non-tabular search space and many NAS papers are burning substantial GPU time on this space . With NAS-Bench-301 , we provide a cheap way to prototype and test NAS algorithms that we hope will be of great help to the community , particularly practitioners with fewer resources . While tabular benchmarks restrict the community to small search spaces that are only feasible to evaluate exhaustively , we believe that NAS-Bench-301 might be a milestone for creating future fast-to-evaluate surrogate benchmarks on large and interesting search spaces , enabling the community to focus its resources and time on these new benchmarks rather than spending them on the DARTS space . 2 . * * Additionally , this paper only considers CIFAR-10 . This is a step back from NAS-Bench-201 which despite its small size , did contain multiple datasets . * * : As mentioned above , we chose our search space & dataset , DARTS + CIFAR-10 , based on the most popular space for which most computational resources are currently used . Unfortunately , our own computational budget did not allow us to cover multiple datasets but our approach trivially applies to other spaces and datasets . We would also like to emphasize that our work is the first to propose surrogate benchmarks for NAS and shows that they are indeed applicable . In doing so , our fundamental work on the methodology for creating fast-to-evaluate NAS benchmarks opens up the door to fast-to-evaluate NAS benchmarks for the large set of varied and exciting NAS applications the community is developing these days . 3 . * * The compute used by the authors ( training 60k networks ) has gone into differentiating a bunch of networks that are all quite good , within a few percentage points of error . I \u2019 m not sure how this is of practical use . * * : We point to Figure 8 in our Appendix which shows ECDFs for different optimizers . We note that there are architectures with performances between 80-95 % validation accuracy . Importantly , NAS-Bench-101 and NAS-Bench-201 report the performance of the architectures in the whole search space , while we focus more on collecting architectures from distinct regions of the search space via several NAS optimizers . We refer the reviewer to the t-SNE plots in Fig.9 ( Appendix C.4 ) to visualize that these optimizers exploit different well-performing regions of the architecture space ( e.g.compare DE and BANANAS ) ."}, "2": {"review_id": "1flmvXGGJaa-2", "review_text": "* * * * Update after rebuttal * * * * I am increasing my rating for the paper as they did all the experiments I had asked for and updated the paper accordingly . * * * * * * * * * * * * * * * * * * * * * * * * * * Summary : While NAS has made tremendous advances in recent past , benchmarking algorithms with respect to each other still remains a challenge . Tabular benchmarks like 101 and 201 , take a search space and train all possible architectures in them . While this is possible to do for relatively small search spaces and datasets , this is impractical to repeat for larger search spaces ( e.g.DARTS ' search space which has 10^18 architectures ) . This work makes the nice observation that a tabular benchmark treats each architecture as an independent random variable and does n't utilize any similarities between them . Due to similarities in architecture space , knowing the train/val/test accuracy of one architecture tells us a lot about other architectures nearby . So a predictive model trained on a sparse subset of architectures can actually outperform an exhaustive tabular method . Lots of careful experiments are reported on the DARTS search space to create predictive models which can accurately predict architecture performance ( accuracy , latency ) and hence can be used as a 'simulator ' by NAS algorithms for rapid research and fair comparison . Comments : - The paper is generally well-written and has thorough clean experiments ! Thanks ! - My main concern is the following : The fact that the benchmark has added architectures encountered on the trajectory of well-known performing optimizers bothers me a bit . In the ideal world a benchmark should have no knowledge of any particular solution to the problem . This is true in the case of tabular benchmarks like 101 and 201 . I understand the position that a surrogate model should be very good at predicting parts of the architecture space which optimizers are most likely to visit . Can we construct surrogates without knowing anything about any particular optimizers the community may invent in the future ? One part of an ablation study answering this question has been presented in Appendix E.2 where a model has been only trained on well-performing architectures ( above 92 % accuracy ) and in Figure 21 has been found adequate for predicting the trajectories of BANANAS and Random Search ( RS ) . This begs the question of what if we only used random sampling to fit surrogate models ? ( Of course coverage methods like adaptive submodularity-based greedy algorithms may result in even better performance ) . But can we do the easy baselines first for which the authors already have the data : 1 . Train surrogate model using only the 23746 architectures via random sampling and plot the same figures as in Fig 4 center and right . How much worse are they from current ones ? 2 . ( if compute allows ) : Use up the entire budget of ~60k architectures in Table 2 only from random sampling and plot Fig 4 center and right . How much worse are they from current ones ? 3 . ( to have a fair comparison of current method to baseline 1 without using much more compute ) : Keep the total budget 23746 but fill them up in the same proportion from each method as currently in Table 2 . For example RS will have 23746/~60k ratio , DE will have 7275/~60k ratio , etc . This will create `` NASBench-301-small '' which will have the same budget as baseline 1 above . Then plot Fig 4 center and right with both this and 1 . These might be competitive baselines because the Once-For-All work from Cai et al.ICLR 2020 uses a regressor trained on 16k architectures sampled from a supergraph as a surrogate model to run evolutionary search against and obtain good performance . Also in this paper itself if more than 21500 architectures on 101 are used to train ( unclear from the paper whether they were randomly sampled and diverged models rejected or some other technique was used to select them , since it says `` subsets of D^ { train } '' but I think they were randomly sampled , right ? ) , then that itself is better than the tabular benchmark . Happy to be convinced if these are fair baselines or not . ( Also possible that these are already included and I missed them.Ther are a lot of experiments : - ) ) -My other worry is that the LOOO ablation may be misleading since reasonable optimizers may be visiting similar parts of the architecture space hence may give a false sense of extrapolation .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for reading the paper thoroughly , the positive feedback , and for the detailed and very useful review . We now reply to the questions : 1 . * * My main concern is the following : The fact that the benchmark has added architectures encountered on the trajectory of well-known performing optimizers bothers me a bit . In the ideal world a benchmark should have no knowledge of any particular solution to the problem . This is true in the case of tabular benchmarks like 101 and 201 . I understand the position that a surrogate model should be very good at predicting parts of the architecture space which optimizers are most likely to visit . Can we construct surrogates without knowing anything about any particular optimizers the community may invent in the future ? * * : We think this is a very interesting question and we agree that a benchmark ideally should have no prior knowledge about any particular solutions of a NAS optimizer . To this end we conducted the Leave One Optimizer Out ( LOOO ) experiment in Section 5.1 to remove some of this bias when evaluating the soundness of NAS-Bench-301 . It is true that most of the NAS optimizers we use after some time find really competitive architectures , but as one can see from the t-SNE plots in Fig.9 ( Appendix C.4 ) these optimizers exploit different good-performing regions of the architecture space ( e.g. , compare DE and BANANAS ) . Finally , we also think that another approach that might yield better \u201c unbiased \u201d benchmarks could be to use more sophisticated space-filling methods like quasi-random sequences , e.g.Sobol sequences [ 1 ] , Latin Hypercubes [ 2 ] or Adaptive Submodularity [ 3 ] . 2 . * * On the reviewer suggestion to : - Train surrogate model using only the 23746 architectures via random sampling and plot the same figures as in Fig 4 center and right . How much worse are they from current ones ? - ( to have a fair comparison of current method to baseline 1 without using much more compute ) : Keep the total budget 23746 but fill them up in the same proportion from each method as currently in Table 2 . For example RS will have 23746/60k ratio , DE will have 7275/60k ratio , etc . This will create `` NASBench-301-small '' which will have the same budget as baseline 1 above . Then plot Fig 4 center and right with both this and 1 . * * : This is a very interesting experiment . Thank you for the suggestion ! We are running this , and we will post a reply when we have these plots ready to update the paper accordingly . 3 . * * ( if compute allows ) : Use up the entire budget of ~60k architectures in Table 2 only from random sampling and plot Fig 4 center and right . How much worse are they from current ones ? * * : Unfortunately , as the reviewer assumed , our compute resources do not allow us to collect this many architectures during this short time frame . However , this is an interesting experiment for the future since anyway we are planning to increase the coverage of the space in the long run . 4 . * * Also in this paper itself if more than 21500 architectures on 101 are used to train ( unclear from the paper whether they were randomly sampled and diverged models rejected or some other technique was used to select them , since it says `` subsets of D^ { train } '' but I think they were randomly sampled , right ? ) , then that itself is better than the tabular benchmark . * * : The reviewer is correct , the architectures in the training subsets were uniformly sampled at random . However , prior to sampling , any architectures in which one of the three evaluations diverged , were removed ( as done in e.g. [ 4 ] ) .5 . * * My other worry is that the LOOO ablation may be misleading since reasonable optimizers may be visiting similar parts of the architecture space hence may give a false sense of extrapolation . * * : This is a fair point , however it might also be the case that there exist many distinct regions in the architecture space that yield architecture with good and similar performance . We think this is the case for the DARTS search space and the t-SNE plots for the individual optimizers in Figure 9 indicate that certain optimizers ( e.g.BANANAS ) visit distinct subspaces and are still predicted accurately in the LOOO ( Figure 5 ) . -- References -- [ 1 ] M. Sobol . On the distribution of points in a cube and the approximate evaluation of integrals . Zhurnal Vychislitel ` noi Matematiki i Matematicheskoi Fiziki . 7 ( 4 ) :784-802 , 1967 . [ 2 ] McKay et al.A comparison of three methods for selecting values of input variables in the analysis of output from a computer code . Technometrics , 2000 . [ 3 ] Golovin and Krause . Adaptive submodularity : Theory and applications in active learning and stochastic optimization . Journal of Artificial Intelligence Research , 42:427-486 , 2001 . [ 4 ] Wen et al.Neural predictor for neural architecture search . In ECCV 2020 ."}, "3": {"review_id": "1flmvXGGJaa-3", "review_text": "Summary : The authors propose a new benchmark for evaluating surrogate functions for architecture search . According to the authors , existing tabular architecture search benchmarks are insufficient for this purpose due to using overly small search spaces . The main difference of this benchmark and other existing architecture search benchmarks such as NAS Bench 101 and NAS Bench 201 is that they do not attempt to evaluate all the architectures in the search space and do so for a much larger search space ( DARTS ) . The authors then use this new dataset to show that surrogate functions are better than tabular estimators ( error wise ; although some lower performance architectures were discarded to make this case ) . Additionally , the authors compare the proposed benchmark ( based on surrogate functions ) with a real benchmark and observe that the results are qualitatively similar . Finally , the authors show that reevaluate the claim that local search is state-of-the-art for architecture search and find that , using their benchmark , that local search is still state-of-the-art provided that enough computational budget is available for the experiment . Pros : + The paper presents a new dataset for architecture search that does not rely on exhaustive evaluation of all architectures in the search space . The experiments conducted that compare different surrogate functions on this benchmark are solid + The paper suggests that building benchmarks for different search spaces can be accomplished through the surrogate function route where first a dataset of architectures is collected and then it is used to train a surrogate model . The surrogate function is then used as an interface between the search space and the search algorithm . This approach for building benchmarks is general and is likely to be useful in the construction of future benchmarks for architecture search . Cons : - The fact that existing benchmarks for architecture search are insufficient for surrogate function evaluation and lead to wrong inferences is perhaps insufficiently supported . It is well known in architecture search work that existing search spaces ( DARTS being one of them ) have limited performance variability and that much of the performance variation ascribed to different architecture search methods can often be ascribed to differences in search spaces . Is a single search space good enough to mitigate these problems ? For example , how do we guarantee that NAS Bench 301 is not just another dataset and guarantee that addresses some of the perceived problems with existing architecture search search spaces and benchmarks ? Comments : - Appendices were n't included in the submission . - It would be interesting to discuss how this benchmark should be placed in the context of other existing benchmarks for architecture search to guide future practice . - The motivation from Section 2 is rather sparse . It would be warranted to show that this trend is consistent with other metrics such as squared error and Kendall Tau ( i.e. , showing the ranks of different architectures are also preserved better ) . - Insufficient information about the feature representation that is used for prediction ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for taking his/her time to review our paper and for the useful feedback and suggestions . We hope that the reviewer will consider increasing his/her score after our response to his concerns which we provide in the following . 1 . * * The fact that existing benchmarks for architecture search are insufficient for surrogate function evaluation and lead to wrong inferences is perhaps insufficiently supported . * * : The main difference between existing benchmarks and realistic search spaces is their size mismatch . This in turn leads to 1 . NAS algorithms like local search to become very inefficient [ 1 ] and 2. conclusions and hypotheses drawn from these benchmarks potentially being distinct . We do not argue that existing tabular benchmarks are not interesting ( maybe easier ) spaces to benchmark surrogate models and we have already seen papers applying GNNs to learn useful representations on these tabular benchmarks [ 4 , 5 ] . What we argue is that surrogate models can effectively represent the NAS search space of these tabular benchmarks by using just a fraction of the architectures in search space ( Figure 1 in the paper ) and provide a better estimate for the test performance compared to the tabular entries ( Table 1 ) . We show that regression models can effectively model the noisy evaluations stemming from the SGD training of the neural networks better than a table which contains only a limited number of point estimates . 2 . * * It is well known in architecture search work that existing search spaces ( DARTS being one of them ) have limited performance variability * * : We agree that many existing search spaces ( including the one of DARTS ) do not yield the largest variety in performances . Before we invested the compute to train the architectures in the DARTS space , we thought hard about whether we should introduce a new search space and then create a surrogate of that , but that would come with all kinds of confounding factors , and we ultimately decided that it is far more useful ( and convincing ) to demonstrate the concept of surrogate NAS benchmarks on a highly used benchmark . This is also because DARTS is still the most widely used non-tabular search space and many current NAS papers are burning very substantial GPU time on this benchmark . We would also like to note that the performance depends on the macro architecture and the choice of training pipeline . Figure 8 in the appendix shows that our search space yields architectures with performances between 80-95 % accuracy . Note our training dataset does not encompass all architectures in the space . Since we focus on good regions the bad performing architectures are underrepresented ."}}