{"year": "2019", "forum": "H1ersoRqtm", "title": "Structured Neural Summarization", "decision": "Accept (Poster)", "meta_review": "This paper examines ways of encoding structured input such as source code or parsed natural language into representations that are conducive for summarization. Specifically, the innovation is to not use only a sequence model, nor only a tree model, but both. Empirical evaluation is extensive, and it is exhaustively demonstrated that combining both models provides the best results.\n\nThe major perceived issue of the paper is the lack of methodological novelty, which the authors acknowledge. In addition, there are other existing graph-based architectures that have not been compared to.\n\nHowever, given that the experimental results are informative and convincing, I think that the paper is a reasonable candidate to be accepted to the conference.", "reviews": [{"review_id": "H1ersoRqtm-0", "review_text": "Note: I changed my original score from 4 to 7 based on the new experiments that answer many of the questions I had about the relative performance of each part of the model. The review below is the original one I wrote before the paper changes. # Positive aspects of this submission - The intuition and motivation behind the proposed model are well explained. - The empirical results on the MethodNaming and MethodDoc tasks are very promising. # Criticism - The novelty of the proposed model is limited since it is essentially adding an existing GGNN layer, introduced by Li et al. (2015), on top of an existing LSTM encoder. The most important novelty seems to be the custom graph representation for these sequence inputs to make them compatible with the GGNN, which should then deserve a more in-depth study (i.e. ablation study with different graph representations, etc). - Since you compare your model performance against Alon et al. on Java-small, it should be fair to report the numbers on Java-med and Java-large as well. - The \"GNN -> LSTM+POINTER\" experiment results are reported on the MethodDoc task, but not for MethodNaming. Reporting this number for MethodNaming is essential to show the claimed empirical superiority of the hybrid encoder compared to GNN only. - I have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons: - The comparison of the proposed model for NLSummarization against See et al. is a bit unfair, since it uses additional information through the CoreNLP named entity recognizer and coreference models. With the experiments listed in Table 1, there is no way to know whether the increased performance is due to the hybrid encoder design or due the additional named entity and coreference information. Adding the entity and coreference data in a simpler way (i.e. at the token embedding level with a basic sequence encoder) in the ablation study would very useful to answer that question. - In NLSummarization, connecting sentence nodes using a NEXT edge can be analogous to using a hierarchical encoder, as used by Nallapati et al. (\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\", 2016). Ignoring the other edges of the GNN graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder? - Adding the coverage decoder introduced by See et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else. - How essential is the weighted averaging for graph-level document representation (Gilmer et al. 2017) compared to uniform averaging? - A few minor comments about writing: - In Table 1, please put the highest numbers in bold to improve readability - On page 7, the word \"summaries\" is missing in \"the model produces natural-looking with no noticeable negative impact\" - On page 9, \"cove content\" should be \"core content\" ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your detailed comments , which we will integrate your comments in the next version of our paper . On novelty : We agree that we are not contributing fundamentally new models here \u2013 indeed , we refrained from introducing a more complex architecture to make it easy to adopt this modeling approach . We believe that our work introduces a simple way to fuse state-of-the-art sequence ( not only LSTMs , but /any/ sequence encoder ) learning with reasoning enabled by domain-specific graph constructions . We have not found this idea in prior work , and our experiments show the value across three different tasks from different domains . We hope that other researchers can profit from our work by integrating similar techniques into their own architectures and believe that this deserves publication and wider dissemination . As discussed in our reply to all reviewers , we will run additional experiments on the CNN/DM to analyze the influence of different graph constructions . On GNN- > LSTM+pointer on MethodNaming : We decided to show this ablation experiment only on the MethodDoc task for presentation reasons , but we will rerun the model and provide additional results on the MethodNaming task in our next revision . On comparison with Alon et al.2018 on the Java-Large corpus : We did run these experiments but realized that we could obtain best results by models that \u201c felt \u201d like they had too much capacity . Further analysis of this behavior traced this to a problem with a duplication of samples in the dataset . For example , about 30.7 % of files in the Java-Large are near-duplicates of other files in the corpus ( across all folds ) , indicating that results on these datasets primarily measure overfitting to the data . We managed to train competitive models , but only by choosing very large sizes for the hidden dimensions ( > 1000 ) and removing dropout . In contrast , Java-Small only has 3.0 % duplicates . We will clarify this in the next version of our paper . [ This is similar to our experiences with the Barone & Sennrich dataset discussed in Sect . 4.1.2 . ] On NL Summarization and additional information : We agree that our model uses additional information that is not available to the pure sequence models \u2013 indeed , we believe that the ability to use this information is the core contribution of our work . Indeed , it is unclear how to add information from the CoreNLP parser to a standard sequence model ( how , for example , are coreference connections represented ? ) . As discussed in our reply to all reviews , we will run additional experiments to further elucidate this effect . Primarily , we will run an LSTM baseline that uses additional per-token information in the embedding of words , and additionally will introduce fresh tokens ( \u201c < REF1 > \u201d , \u2026 ) to mark points at which references are made . If you had other comparisons in mind , please do react quickly , as these experiments do take a bit of time ... On comparison with Nallapati et al.2016 : The structure of the \u201c Next \u201d tokens in the graph model resembles that of Nallapati et al . ( 2016 ) .However , the core difference is in how message-passing GNNs work . In Nallapati et al . ( 2016 ) computing the representations this is truly hierarchical , I.e . information flows in one direction : sentence representations are computed , then these are combined into a document representation . In a GNN , messages are passed in both directions , and thus our per-sentence nodes also allow the exchange of information between different tokens in the same sentence . Hence , our model is more comparable to a hierarchical setting in which information can flow both up and down . On using coverage : We wanted to avoid the additional work for this experiment , since we believe that the improvements from adding a coverage mechanism are orthogonal to the ones provided by our model but will now run this and provide the results once the experiments have finished . On weighted averaging : In past experiments on a variety of datasets and tasks , we have found that weighted averaging helps compared to uniform averaging . We believe that this is due to the fact that weighted averaging acts as an attention-like mechanism that allows the model to pick the salient information from the graph while allowing the message-passing to \u201c freely \u201d transfer information . Since this is also the accepted method in the GNN literature ( e.g.Gilmer et al.2017 ) we did not further experiment with this design decision . As our compute resources are limited , we want to avoid rerunning this ablation on the CNN/DM dataset , but will provide additional experiments on the two smaller tasks . Please , let us know if these do not sufficiently address the concerns you raise in your review and what alternative experiments are missing ."}, {"review_id": "H1ersoRqtm-1", "review_text": "This paper presents a structural summarization model with a graph-based encoder extended from RNN. Experiments are conducted on three tasks, including generating names for methods, generating descriptions for a function, and generating text summaries for news articles. Experimental results show that the proposed usage of GNN can improve performance by the models without GNN. I think the method is reasonable and results are promising, but I'd like to see more focused evaluation on the semantics captured by the proposed model (compared to the models without GNN). Here are some questions and suggestions: - Overall, I think additional evaluation should be done to evaluate on the semantic understanding aspects of the methods. Concretely, the Graph-based encoder has access to semantic information, such as entities. In order to better understand how this helps with the overall improvement, the authors should consider automatic evaluation and human evaluation to measure its contribution. Also from fig. 3, we can see that all methods get the \"utf8 string\" part right, but it's hard to say the proposed method generates better description. - In the last table in Tab. 1, why the authors don't have results for adding GNN for the pointer-generator model with coverage? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time and helpful comments . As discussed in our reply to all reviews , we will run four additional experiments covering points raised by the different reviewers . However , while we believe that a human evaluation of generated summaries would be helpful , setting this up during the rebuttal period seems to be impossible . Do let us know if you want us to run more experiments / provide more results ."}, {"review_id": "H1ersoRqtm-2", "review_text": "STRUCTURED NEURAL SUMMARIZATION Summary: This work combines Graph Neural Networks with a sequential approach to abstractive summarization across both natural and programming language datasets. The extension of GNNs is simple, but effective across all datasets in comparison to external baselines for CNN/DailyMail, internal baselines for C#, and a combination of both for Java. The idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide. The chosen examples (which I hope are randomly sampled; are they?) do seem to suggest the efficacy of this approach with that intuition. Comments: Should probably cite CNN/DailyMail when it is first introduced as NLSummarization in Section 2 like you do the other datasets. Can you further elaborate on how your approach is similar to and differs from that in Marcheggiani et al 2017 on Graph CNNs for Semantic Role Labeling, Bastings et al 2017 on Graph Convolutional Encoders for Syntax-aware Machine Translation, and De Cao et al 2018? Why should one elect to go the direction of sequential GNNs over the GCNs of those other works, and how might you compare against them? I would like to see some kind of ablation analysis or direct comparison with similar methods if possible. Why would GNNs hurt SelfAtt performance on MethodDoc C# SelfAtt+GNN / SelfAtt? Why not add the coverage mechanism from See et al 2017 in order to demonstrate that the method does in fact surpass that prior work? I'm left wondering whether the proposed method's returns diminish once coverage is added.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your thoughtful review and your time . As discussed in our reply to all reviews , we will run four additional experiments covering points raised by the different reviewers . On related work in NLP with graphs : Thank you for bringing up additional related work . The cited works handle quite different tasks , and so drawing a direct comparison to our work is hard . Marcheggiani et al . ( 2017 ) uses their model , with a single GCN propagation , for classification not sequence prediction , whereas Bastings et al . ( 2017 ) does sentence-to-sentence translation . Both employ purely syntactic graphs and thus lack the advantages that additional semantic information can provide . Our additional experiments 2 and 3 are designed to show the effect of this . The short paper of De Cao et al . ( 2018 ) uses a GCN over entities in multiple documents . Finally , we want to highlight that we propose to use graphs for longer documents , whereas the approaches above are primarily concerned with single sentences . On average the CNN/DM documents lead to graphs with 900 nodes and 2.5k edges . Regarding the question of SequentialGNN vs GCN , we believe that there are no substantial differences between the use of GCNs and GGNNs . The core contribution proposed in our paper is the idea to fuse information obtained from state-of-the-art sequence models with a form of structured reasoning that can integrate domain knowledge . We will clarify the above in the related work section . On the performance of SelfAtt vs. SelfAtt+GNN on MethodDoc C # : In the paper , we discuss this result explicitly in the third paragraph of 4.1.4 . The core reason for the decrease in ROUGE scores is that the SelfAtt+GNN model produces substantially longer outputs , which tends to impact ROUGE scores . This causes the substantial improvement in the BLEU score . We will extend the appendix to include examples of outputs of the SelfAtt/SelfAtt+GNN models that illustrate how the longer output improves the information content of the results . Overall , we want to note that ROUGE and BLEU are problematic measures for these tasks , but we are not aware of any other metrics that can be computed at scale . On randomness of shown samples : The sample in Figure 2 is one appearing in See et al.For Figure 1 , we had to pick a sample that would fit within the given space , so it \u2019 s not randomly sampled . All other examples are randomly selected ."}], "0": {"review_id": "H1ersoRqtm-0", "review_text": "Note: I changed my original score from 4 to 7 based on the new experiments that answer many of the questions I had about the relative performance of each part of the model. The review below is the original one I wrote before the paper changes. # Positive aspects of this submission - The intuition and motivation behind the proposed model are well explained. - The empirical results on the MethodNaming and MethodDoc tasks are very promising. # Criticism - The novelty of the proposed model is limited since it is essentially adding an existing GGNN layer, introduced by Li et al. (2015), on top of an existing LSTM encoder. The most important novelty seems to be the custom graph representation for these sequence inputs to make them compatible with the GGNN, which should then deserve a more in-depth study (i.e. ablation study with different graph representations, etc). - Since you compare your model performance against Alon et al. on Java-small, it should be fair to report the numbers on Java-med and Java-large as well. - The \"GNN -> LSTM+POINTER\" experiment results are reported on the MethodDoc task, but not for MethodNaming. Reporting this number for MethodNaming is essential to show the claimed empirical superiority of the hybrid encoder compared to GNN only. - I have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons: - The comparison of the proposed model for NLSummarization against See et al. is a bit unfair, since it uses additional information through the CoreNLP named entity recognizer and coreference models. With the experiments listed in Table 1, there is no way to know whether the increased performance is due to the hybrid encoder design or due the additional named entity and coreference information. Adding the entity and coreference data in a simpler way (i.e. at the token embedding level with a basic sequence encoder) in the ablation study would very useful to answer that question. - In NLSummarization, connecting sentence nodes using a NEXT edge can be analogous to using a hierarchical encoder, as used by Nallapati et al. (\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\", 2016). Ignoring the other edges of the GNN graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder? - Adding the coverage decoder introduced by See et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else. - How essential is the weighted averaging for graph-level document representation (Gilmer et al. 2017) compared to uniform averaging? - A few minor comments about writing: - In Table 1, please put the highest numbers in bold to improve readability - On page 7, the word \"summaries\" is missing in \"the model produces natural-looking with no noticeable negative impact\" - On page 9, \"cove content\" should be \"core content\" ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your detailed comments , which we will integrate your comments in the next version of our paper . On novelty : We agree that we are not contributing fundamentally new models here \u2013 indeed , we refrained from introducing a more complex architecture to make it easy to adopt this modeling approach . We believe that our work introduces a simple way to fuse state-of-the-art sequence ( not only LSTMs , but /any/ sequence encoder ) learning with reasoning enabled by domain-specific graph constructions . We have not found this idea in prior work , and our experiments show the value across three different tasks from different domains . We hope that other researchers can profit from our work by integrating similar techniques into their own architectures and believe that this deserves publication and wider dissemination . As discussed in our reply to all reviewers , we will run additional experiments on the CNN/DM to analyze the influence of different graph constructions . On GNN- > LSTM+pointer on MethodNaming : We decided to show this ablation experiment only on the MethodDoc task for presentation reasons , but we will rerun the model and provide additional results on the MethodNaming task in our next revision . On comparison with Alon et al.2018 on the Java-Large corpus : We did run these experiments but realized that we could obtain best results by models that \u201c felt \u201d like they had too much capacity . Further analysis of this behavior traced this to a problem with a duplication of samples in the dataset . For example , about 30.7 % of files in the Java-Large are near-duplicates of other files in the corpus ( across all folds ) , indicating that results on these datasets primarily measure overfitting to the data . We managed to train competitive models , but only by choosing very large sizes for the hidden dimensions ( > 1000 ) and removing dropout . In contrast , Java-Small only has 3.0 % duplicates . We will clarify this in the next version of our paper . [ This is similar to our experiences with the Barone & Sennrich dataset discussed in Sect . 4.1.2 . ] On NL Summarization and additional information : We agree that our model uses additional information that is not available to the pure sequence models \u2013 indeed , we believe that the ability to use this information is the core contribution of our work . Indeed , it is unclear how to add information from the CoreNLP parser to a standard sequence model ( how , for example , are coreference connections represented ? ) . As discussed in our reply to all reviews , we will run additional experiments to further elucidate this effect . Primarily , we will run an LSTM baseline that uses additional per-token information in the embedding of words , and additionally will introduce fresh tokens ( \u201c < REF1 > \u201d , \u2026 ) to mark points at which references are made . If you had other comparisons in mind , please do react quickly , as these experiments do take a bit of time ... On comparison with Nallapati et al.2016 : The structure of the \u201c Next \u201d tokens in the graph model resembles that of Nallapati et al . ( 2016 ) .However , the core difference is in how message-passing GNNs work . In Nallapati et al . ( 2016 ) computing the representations this is truly hierarchical , I.e . information flows in one direction : sentence representations are computed , then these are combined into a document representation . In a GNN , messages are passed in both directions , and thus our per-sentence nodes also allow the exchange of information between different tokens in the same sentence . Hence , our model is more comparable to a hierarchical setting in which information can flow both up and down . On using coverage : We wanted to avoid the additional work for this experiment , since we believe that the improvements from adding a coverage mechanism are orthogonal to the ones provided by our model but will now run this and provide the results once the experiments have finished . On weighted averaging : In past experiments on a variety of datasets and tasks , we have found that weighted averaging helps compared to uniform averaging . We believe that this is due to the fact that weighted averaging acts as an attention-like mechanism that allows the model to pick the salient information from the graph while allowing the message-passing to \u201c freely \u201d transfer information . Since this is also the accepted method in the GNN literature ( e.g.Gilmer et al.2017 ) we did not further experiment with this design decision . As our compute resources are limited , we want to avoid rerunning this ablation on the CNN/DM dataset , but will provide additional experiments on the two smaller tasks . Please , let us know if these do not sufficiently address the concerns you raise in your review and what alternative experiments are missing ."}, "1": {"review_id": "H1ersoRqtm-1", "review_text": "This paper presents a structural summarization model with a graph-based encoder extended from RNN. Experiments are conducted on three tasks, including generating names for methods, generating descriptions for a function, and generating text summaries for news articles. Experimental results show that the proposed usage of GNN can improve performance by the models without GNN. I think the method is reasonable and results are promising, but I'd like to see more focused evaluation on the semantics captured by the proposed model (compared to the models without GNN). Here are some questions and suggestions: - Overall, I think additional evaluation should be done to evaluate on the semantic understanding aspects of the methods. Concretely, the Graph-based encoder has access to semantic information, such as entities. In order to better understand how this helps with the overall improvement, the authors should consider automatic evaluation and human evaluation to measure its contribution. Also from fig. 3, we can see that all methods get the \"utf8 string\" part right, but it's hard to say the proposed method generates better description. - In the last table in Tab. 1, why the authors don't have results for adding GNN for the pointer-generator model with coverage? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time and helpful comments . As discussed in our reply to all reviews , we will run four additional experiments covering points raised by the different reviewers . However , while we believe that a human evaluation of generated summaries would be helpful , setting this up during the rebuttal period seems to be impossible . Do let us know if you want us to run more experiments / provide more results ."}, "2": {"review_id": "H1ersoRqtm-2", "review_text": "STRUCTURED NEURAL SUMMARIZATION Summary: This work combines Graph Neural Networks with a sequential approach to abstractive summarization across both natural and programming language datasets. The extension of GNNs is simple, but effective across all datasets in comparison to external baselines for CNN/DailyMail, internal baselines for C#, and a combination of both for Java. The idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide. The chosen examples (which I hope are randomly sampled; are they?) do seem to suggest the efficacy of this approach with that intuition. Comments: Should probably cite CNN/DailyMail when it is first introduced as NLSummarization in Section 2 like you do the other datasets. Can you further elaborate on how your approach is similar to and differs from that in Marcheggiani et al 2017 on Graph CNNs for Semantic Role Labeling, Bastings et al 2017 on Graph Convolutional Encoders for Syntax-aware Machine Translation, and De Cao et al 2018? Why should one elect to go the direction of sequential GNNs over the GCNs of those other works, and how might you compare against them? I would like to see some kind of ablation analysis or direct comparison with similar methods if possible. Why would GNNs hurt SelfAtt performance on MethodDoc C# SelfAtt+GNN / SelfAtt? Why not add the coverage mechanism from See et al 2017 in order to demonstrate that the method does in fact surpass that prior work? I'm left wondering whether the proposed method's returns diminish once coverage is added.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your thoughtful review and your time . As discussed in our reply to all reviews , we will run four additional experiments covering points raised by the different reviewers . On related work in NLP with graphs : Thank you for bringing up additional related work . The cited works handle quite different tasks , and so drawing a direct comparison to our work is hard . Marcheggiani et al . ( 2017 ) uses their model , with a single GCN propagation , for classification not sequence prediction , whereas Bastings et al . ( 2017 ) does sentence-to-sentence translation . Both employ purely syntactic graphs and thus lack the advantages that additional semantic information can provide . Our additional experiments 2 and 3 are designed to show the effect of this . The short paper of De Cao et al . ( 2018 ) uses a GCN over entities in multiple documents . Finally , we want to highlight that we propose to use graphs for longer documents , whereas the approaches above are primarily concerned with single sentences . On average the CNN/DM documents lead to graphs with 900 nodes and 2.5k edges . Regarding the question of SequentialGNN vs GCN , we believe that there are no substantial differences between the use of GCNs and GGNNs . The core contribution proposed in our paper is the idea to fuse information obtained from state-of-the-art sequence models with a form of structured reasoning that can integrate domain knowledge . We will clarify the above in the related work section . On the performance of SelfAtt vs. SelfAtt+GNN on MethodDoc C # : In the paper , we discuss this result explicitly in the third paragraph of 4.1.4 . The core reason for the decrease in ROUGE scores is that the SelfAtt+GNN model produces substantially longer outputs , which tends to impact ROUGE scores . This causes the substantial improvement in the BLEU score . We will extend the appendix to include examples of outputs of the SelfAtt/SelfAtt+GNN models that illustrate how the longer output improves the information content of the results . Overall , we want to note that ROUGE and BLEU are problematic measures for these tasks , but we are not aware of any other metrics that can be computed at scale . On randomness of shown samples : The sample in Figure 2 is one appearing in See et al.For Figure 1 , we had to pick a sample that would fit within the given space , so it \u2019 s not randomly sampled . All other examples are randomly selected ."}}