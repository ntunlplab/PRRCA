{"year": "2018", "forum": "ByQZjx-0-", "title": "Faster Discovery of Neural Architectures by Searching for Paths in a Large Model", "decision": "Invite to Workshop Track", "meta_review": "First off, this was a difficult paper to decide on. There was some vigorous discussion on the paper centering around the choices that were available to the conv-nets.  The author's strongly emphasized the improvements on the PTB task.\n\nFor my part, I think the method is very compelling -- sharing weights for all the models we are optimizing on seems like a great idea -- and that we can make it work is even more interesting. So from this point of view, I think its a novel contribution and worth accepting.\n\nOn the other hand, I'm likely to agree with some of the motivations behind the questions raised by R3. Are all the choices really necessary ? perhaps the gains came from just a couple of things like number of skip connections and channels, etc. That exploration is useful. On the flip side, I think it may be an irrelevant question -- the model is able to make the correct decisions from a big set.\n\nThe authors emphasize the language modelling part, but for me, this was actually less compelling. The authors use some of the tricks from Merity in their model training (perplexity 52.8), and as a result are already using some techniques that produces better results. Further, PTB is a regularization game -- and that's not really the claim of this paper. Although, one could argue that weight sharing between different models can produce an ensembling / regularization effect and those gains may show up on PTB. A much more compelling story would have been to show that this method works on a large dataset where the impact of the architecture cannot be conflated with controlling overfitting better.\n\nAs a result, this puts the paper on the fence for me; even though I very much like the idea. Polishing the paper and making a more convincing case for both the CNNs and RNNs will make this paper a solid contribution in the future.", "reviews": [{"review_id": "ByQZjx-0--0", "review_text": "In the paper titled \"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model\", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks. The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture. The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback) --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly. Overall, the paper is well-written. Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their review . To address reviewers \u2019 concerns , we have completely rewritten the paper to make it easier to follow and we have also included new experimental results ( all reported in our revision ) . We sincerely hope that the reviewer will update positively on our revised paper . We believe that ENAS delivers extremely non-trivial contributions to architecture search approaches . First , while the idea of ENAS is simple - sharing parameters between architectures so that they don \u2019 t need to be trained again - we believe that this idea is far from incremental . A lot of details are needed to share the parameters appropriately , e.g.the design choice of representing all child models in a large graph ( see Sections 3 and 4 in our revision for more details ) . ENAS leads to two orders of magnitude reduction of computing resource and an order of magnitude reduction of time consumed , all without sacrificing much performance . Second , in our revision , we showed that ENAS actually achieves a better perplexity than NAS on Penn Treebank . We designed a different search space , in which ENAS found a recurrent cell that achieves 57.8 perplexity on Penn Treebank ( compared to NAS \u2019 s 62.4 perplexity ) , and establishes the state-of-the-art performance on automatic model design on Penn Treebank . Furthermore , this result is also achieved without extensive hyper-parameters tuning ( Melis et al. , 2017 ) . Details are in Section 4.1 of our revision . Finally , we find that among other submissions to ICLR 2018 , some papers address the similar problem with ENAS : the computational expense of automatic model designing approaches . For example SMASH [ 1 ] is a method that automatically designs networks for image classification tasks . As of now , despite the fact that SMASH performs worse than ENAS and was only applied to images and not texts , their paper has a much better score than ours ( SMASH has an averaged score of 6 while our paper got an averaged score of 5 ) . We sincerely hope that you will give ENAS another consideration . [ 1 ] Paper 1 : SMASH : One-Shot Model Architecture Search through HyperNetworks ( https : //openreview.net/forum ? id=rydeCEhs- )"}, {"review_id": "ByQZjx-0--1", "review_text": "Summary: The paper presents a method for learning certain aspects of a neural network architecture, specifically the number of output maps in certain connections and the existence of skip connections. The method is relatively efficient, since it searches in a space of similar architectures, and uses weights sharing between the tested models to avoid optimization of each model from scratch. Results are presented for image classification on Cifar 10 and for language modeling. Page 3: \u201cfor each channel, we only predict C/S binary masks\u201d -> this seems to be a mistake. Probably \u201cfor each operation, we only predict C/S binary masks\u201d is the right wording Page 4: Stabilizing Stochastic Skip Connections: it seems that the suggested configuration does not enable an identity path, which was found very beneficial in (He. et al., 2016). Identity path does not exist since layers are concatenated and go through 1*1 conv, which does not enable plain identity unless learned by the 1*1 conv. Page 5: - The last paragraph in section 4.2 is not clear to me. What does a compilation failure mean in this context and why does it occur? And: if each layer is connected to all its previous layers by skip connections, what remains to be learned w.r.t the model structure? Isn\u2019t the pattern of skip connection the thing we would like to learn? - Some details of the policy LSTM network are also not clear to me: o How is the integer mask (output of the B channel steps) encoded? Using 1-hot encoding over 2^{C/S} output neurons? Or maybe C/S output neurons, used for sampling the C/S bits of the mask? this should be reported in some detail. o How is the mask converted to an input embedding for the next step? Is it by linear multiplication with a matrix? Something more complicated? And are there different matrices used/trained for each mask embedding (one for 1*1 conv, one for 3*3 conv, etc..)? o What is the motivation for using equation 5 for the sampling of skip connection flags? What is the motivation for averaging the winning anchors as the average embedding for the next stage (to let it \u2018know\u2019 who is connected to the previous?). Is anchor j also added to the average? o How is equation 5 normalized? That is: the probability is stated to be proportional to an exponent of an inner product, but it is not clear what the constant is and how sampling is done. Page 6: - Section 4.4: what is the fixed policy used for generating models in the stage of training the shared W parameters? (this is answered at page 7 Experiments: - The accuracy figures obtained are impressive, but I\u2019m not convinced the ENAS learning is the important ingredient in obtaining them (rather than a very good baseline) - Specifically, in the Cifar -10 example it does not seem that the networks chooses the number of maps in a way which is diverse or different from layer to layer. Therefore we do not have any evidence that the LSTM controller has learnt any interesting rule regarding block type, or relation between block type width and layer index. All we see is that the model does not chose too many maps, thus avoid significant overfit. The relevant baseline here is a model with 64 or 96 maps on each block, each layer.Such a model is likely to do as well as the ENAS model, and can be obtained easily with slight parameter tuning of a single parameter. - Similarly, I\u2019m not convinced the skip connection pattern found for Cifar-10 is superior to standard denseNet or Resnet pattern. The found configuration was not compared to these baselines. So again we do not have evidence showing the merit of keeping and tuning many parameters with the RINFORCE - The experiments with Penn Treebank are described with too less details: for example, what exactly is the task considered (in terms on input-output mapping), what is the dataset size, etc.. - Also, for the Penn treebank experiments no baseline is given, so it is not possible to understand if the structure learning here is beneficial. Comparison of the results to an architecture with all skip connections, and with a single skip connection per layer is required to estimate if useful structure is being learnt. Overall: - Pro: the method gives high accuracy results - Cons: o It is not clear if the ENAS search is responsible to the results, or just the strong baseline. The advantage of ENAS over plain hyper parameter choosing was not sufficiently established. o The controller was not presented in a clear enough manner. Many of its details stay obscure. o The method does not seem to be general. It seems to be limited to choosing a specific set of parameters in very specific scenario (scenario which enable parameter sharing between model. The conditions for this to happen seems to be rather strict, and where not elaborated). After revision: The controller is now better presented. However, the main points were not changed: - ENAS seems to be limited to a specific architecture and search space, in which probably the search is already exhausted. For example for the image processing network, it is determining the number of skip connections and structure of a single layer as a combination of several function types. We already know the answers to these search problems (denser skip connection pattern works better, more functions types in a layer in parallel do better, the number of maps should be adjusted to the complexity and data size to avoid overfit). ENAS does not reveal a new surprising architectures, and it seems that instead of searching in the large space it suggests, one can just tune a 1-2 parameters (for the image network, it is the number of maps in a layer). - Results comparing ENAS results to the simple baseline of just tuning 1-2 hyper parameters were not shown. I hence believe the strong empirical results of ENAS are a property of the search space (the architecture used) and not of the search algorithm.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . We were very dismayed by the low score . Subsequently , we have completely rewritten the paper to make it easier to follow .We have also included new experimental results to address the reviewer \u2019 s concerns . All the results are reported in our revision . We hope that our revisions of the paper can clear the reviewer \u2019 s reservation about ENAS \u2019 s ability . In the following , we try to address the reviewer \u2019 s concerns . First , we have completely rewritten the paper for clarity . We focused on delivering the high level ideas , and we moved a lot of implementation details into our appendix . To address the reviewer \u2019 s particular comment about the lack of details on the controller , we refer the reviewer to Section 3 in our revision . As evidenced by the anonymous comment above , our previous presentation is sufficient for independent researchers to to implement our method . Therefore , we believe that our revision , which aims to improve the original presentation , does not obscure ENAS \u2019 s details . After the reviewing cycle , we will also publish our code , which we hope will clear any ambiguity about ENAS . Second , in our revision , we have included new experimental results to address the reviewer \u2019 s concerns that ENAS is not general , and whether ENAS is responsible for the good results . We summarize them below : 1 ) ENAS is indeed responsible for the results . This information was in our original submission . In our revision , it is highlighted in the paragraph \u201c Sanity Check and Ablation Study \u201d at the beginning Section 4.3 . In particular , a model randomly sampled from our search space does not perform as well as a model sampled by the ENAS controller . Also , we if train ENAS without training its controller , performance is much worse . Both observations , as presented in the paper , indicate the importance of ENAS . To further address the reviewer \u2019 s concerns , we have conducted more controlled experiments . Following are their results : 1a ) 64 or 96 maps on CIFAR-10 models . Sure , a model with randomly chosen 64 or 96 maps on each block , each layer may perform similarly to ENAS . However , in this search space , the controller can take up to 256 maps . Without ENAS , a random model designer would select roughly 128 maps at each block , each layer . If you haven \u2019 t seen ENAS \u2019 s decisions to pick 64 or 96 maps , would you think of such a baseline ? We do agree that a slight tune of hyper-parameters may also lead to this model . However , in other search spaces ( e.g.see Section 4.2 in our revision ) , where one needs to figure out the skip connections , the tuning of hyper-parameters won \u2019 t be as \u201c slight \u201d . 1b ) The pattern of skip connections found by ENAS is indeed better than the DenseNet pattern and the ResNet pattern . In our settings , the DenseNet pattern ( connecting every pair of layers ) achieves 5.23 % test error , and the ResNet pattern ( connecting each layer to the next ) achieves 6.01 % test error . We also note that the DenseNet and the ResNet patterns in our settings are not the DenseNet and ResNet in their original papers . The reason for the differences lies in the design choice of our search spaces : we make skip connections go through a conv1x1 instead of concatenation as in the original DenseNet ( Huang et al. , 2016 ) , or identity and addition as in the original ResNet ( He et al. , 2015 ) . Such design choice may be sub-optimal , and we will try the identity skip connections in our next revision . However , our controlled experiment does show that in our search space , the skip connections that ENAS finds do achieve non-trivial improvements compared to standard patterns . 2 ) ENAS is a general method . To see this , note that one way to do programming is to search for a path in a bigger program , where all operations are available at every step . In ENAS , the computations in a neural architecture can be viewed as a program , which is represented as directed acyclic graph ( DAG ) ( see Section 1 of our revision ) . To apply ENAS to any task , e.g.designing a convolutional network , or designing a recurrent cell , one only needs to specify the DAG \u2019 s components ( examples are now in Section 3 of our revision ) . 3 ) We further elaborate point 2 ) above by applying ENAS to different search spaces . First , we use ENAS to search for both skip connections and layer operations ( convolutions with different filter sizes , or average pooling , or max pooling ) . It turns out that in this search space , ENAS could discover a model with CIFAR-10 test error of 4.23 % . This resulting model is comparable to the model found in the restricted search space over convolutions and pooling masks . Therefore , ENAS works in this search space . [ to be continued ]"}, {"review_id": "ByQZjx-0--2", "review_text": "In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing. In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks. In both approaches, reinforcement learning is used to learn a policy that maximizes the expected reward of some validation set metric. Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice. In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time. The authors present two ENAS models: one for CNNs, and another for RNNs. Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections. However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space. This is a limitation, as the model space is not as flexible as one would desire in a discovery task. Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections. Thus, they are essentially learning the skip connections while using a human-selected model. ENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections. Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections. Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. Overall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process. Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea. It is also impressive how much faster their model performs on tasks without sacrificing much performance. The main limitation is that the best architectures as currently described are less about discovery and more about human input -- finding a more efficient search path would be an important next step.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . We were very dismayed by the low score . Subsequently , we have completely rewritten the paper to make it easier to follow . We have also included new experimental results to address the reviewer \u2019 s concerns . All the results are reported in our revision and are summarized below . We hope that our revisions of the paper can clear the reviewer \u2019 s reservation about ENAS \u2019 s ability . Summary of New Results : 1 ) The reviewer is concerned that ENAS can only search small search spaces . This is not the case . We used ENAS to search for both skip connections and layer operations ( convolutions with different filter sizes , or average pooling , or max pooling ) . It turns out that in this large search space , ENAS could also discover a model with CIFAR-10 test error of 4.23 % . This resulting model is comparable to the model found in the restricted search space over convolutions and pooling masks . Therefore , search space size is not a limitation of ENAS . 2 ) The reviewer is concerned that the best ENAS model is \u201c less about discovery and more about human input. \u201d We show that ENAS can do well with less human inputs . In particular , we took the pattern of skip connections discovered by ENAS ( Figure 5-Right in our revision ) , and simply increased the number of output channels at each layer from 256 to 512 . The resulting model achieves 3.87 % test error on CIFAR-10 . In the original paper , the best ENAS result on CIFAR-10 is 3.86 % test error , achieved by using multiple branches at each layer . Therefore , we showed that the model found by ENAS , with minimal human inputs , can achieve a similar performance to models that are designed with more human inputs . We note that ENAS \u2019 s principle , i.e.searching for a path in a big model is general . Under this principle , we can do whatever other NAS approaches can do . We also note that the human input of increasing the number of output channels was also performed by the original NAS paper ( Zoph and Le , 2017 ) . 3 ) We designed a different search space for recurrent cells . In this search space , ENAS finds a novel recurrent cell that achieves 57.8 test perplexity on Penn Treebank . We have reported this new result in our revision ( Sections 4.1 and 5.1 ) . Let \u2019 s call this the ENASCell . ENASCell is very novel compared to recurrent highway network ( see Figure 4 in our revision ) . While the search space for ENASCell still benefits from highway connections , the ENAS controller has discovered several novelties : - the use of the ReLU activation , unlike in recurrent highway network where only the tanh activation is used - the pattern of connections within the ENASCell To our knowledge , ENASCell \u2019 s perplexity of 57.8 is the state-of-the-art among automatic model design approaches on Penn Treebank . ENASCell outperforms NAS ( 62.4 perplexity ) , which uses two orders of magnitude more computations and one order of magnitude more time . ENASCell , with almost no hyper-parameters tuning , also outperformed LSTM with extensive hyper-parameters tuning ( 59.5 perplexity ) ( Melis et al. , 2017 ) . We now compare ENAS to other ICLR 2018 submissions which address similar problems , i.e.computationally inexpensive approaches for automatic model designing . In particular , Paper 1 presents SMASH , a method that automatically designs networks for image classification tasks , and Paper 323 presents a method that automatically designs recurrent cells . As of now , despite the fact that their methods perform worse than ENAS , both papers receive at least one 7 in their reviews . We believe that ENAS \u2019 s contributions are significant , both in the novelty of its idea and in the significance of its results . We sincerely hope that you will give ENAS another consideration . [ 1 ] Paper 1 : SMASH : One-Shot Model Architecture Search through HyperNetworks ( https : //openreview.net/forum ? id=rydeCEhs- ) [ 2 ] Paper 323 : A Flexible Approach to Automated RNN Architecture Generation ( https : //openreview.net/forum ? id=SkOb1Fl0Z )"}], "0": {"review_id": "ByQZjx-0--0", "review_text": "In the paper titled \"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model\", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks. The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture. The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback) --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly. Overall, the paper is well-written. Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their review . To address reviewers \u2019 concerns , we have completely rewritten the paper to make it easier to follow and we have also included new experimental results ( all reported in our revision ) . We sincerely hope that the reviewer will update positively on our revised paper . We believe that ENAS delivers extremely non-trivial contributions to architecture search approaches . First , while the idea of ENAS is simple - sharing parameters between architectures so that they don \u2019 t need to be trained again - we believe that this idea is far from incremental . A lot of details are needed to share the parameters appropriately , e.g.the design choice of representing all child models in a large graph ( see Sections 3 and 4 in our revision for more details ) . ENAS leads to two orders of magnitude reduction of computing resource and an order of magnitude reduction of time consumed , all without sacrificing much performance . Second , in our revision , we showed that ENAS actually achieves a better perplexity than NAS on Penn Treebank . We designed a different search space , in which ENAS found a recurrent cell that achieves 57.8 perplexity on Penn Treebank ( compared to NAS \u2019 s 62.4 perplexity ) , and establishes the state-of-the-art performance on automatic model design on Penn Treebank . Furthermore , this result is also achieved without extensive hyper-parameters tuning ( Melis et al. , 2017 ) . Details are in Section 4.1 of our revision . Finally , we find that among other submissions to ICLR 2018 , some papers address the similar problem with ENAS : the computational expense of automatic model designing approaches . For example SMASH [ 1 ] is a method that automatically designs networks for image classification tasks . As of now , despite the fact that SMASH performs worse than ENAS and was only applied to images and not texts , their paper has a much better score than ours ( SMASH has an averaged score of 6 while our paper got an averaged score of 5 ) . We sincerely hope that you will give ENAS another consideration . [ 1 ] Paper 1 : SMASH : One-Shot Model Architecture Search through HyperNetworks ( https : //openreview.net/forum ? id=rydeCEhs- )"}, "1": {"review_id": "ByQZjx-0--1", "review_text": "Summary: The paper presents a method for learning certain aspects of a neural network architecture, specifically the number of output maps in certain connections and the existence of skip connections. The method is relatively efficient, since it searches in a space of similar architectures, and uses weights sharing between the tested models to avoid optimization of each model from scratch. Results are presented for image classification on Cifar 10 and for language modeling. Page 3: \u201cfor each channel, we only predict C/S binary masks\u201d -> this seems to be a mistake. Probably \u201cfor each operation, we only predict C/S binary masks\u201d is the right wording Page 4: Stabilizing Stochastic Skip Connections: it seems that the suggested configuration does not enable an identity path, which was found very beneficial in (He. et al., 2016). Identity path does not exist since layers are concatenated and go through 1*1 conv, which does not enable plain identity unless learned by the 1*1 conv. Page 5: - The last paragraph in section 4.2 is not clear to me. What does a compilation failure mean in this context and why does it occur? And: if each layer is connected to all its previous layers by skip connections, what remains to be learned w.r.t the model structure? Isn\u2019t the pattern of skip connection the thing we would like to learn? - Some details of the policy LSTM network are also not clear to me: o How is the integer mask (output of the B channel steps) encoded? Using 1-hot encoding over 2^{C/S} output neurons? Or maybe C/S output neurons, used for sampling the C/S bits of the mask? this should be reported in some detail. o How is the mask converted to an input embedding for the next step? Is it by linear multiplication with a matrix? Something more complicated? And are there different matrices used/trained for each mask embedding (one for 1*1 conv, one for 3*3 conv, etc..)? o What is the motivation for using equation 5 for the sampling of skip connection flags? What is the motivation for averaging the winning anchors as the average embedding for the next stage (to let it \u2018know\u2019 who is connected to the previous?). Is anchor j also added to the average? o How is equation 5 normalized? That is: the probability is stated to be proportional to an exponent of an inner product, but it is not clear what the constant is and how sampling is done. Page 6: - Section 4.4: what is the fixed policy used for generating models in the stage of training the shared W parameters? (this is answered at page 7 Experiments: - The accuracy figures obtained are impressive, but I\u2019m not convinced the ENAS learning is the important ingredient in obtaining them (rather than a very good baseline) - Specifically, in the Cifar -10 example it does not seem that the networks chooses the number of maps in a way which is diverse or different from layer to layer. Therefore we do not have any evidence that the LSTM controller has learnt any interesting rule regarding block type, or relation between block type width and layer index. All we see is that the model does not chose too many maps, thus avoid significant overfit. The relevant baseline here is a model with 64 or 96 maps on each block, each layer.Such a model is likely to do as well as the ENAS model, and can be obtained easily with slight parameter tuning of a single parameter. - Similarly, I\u2019m not convinced the skip connection pattern found for Cifar-10 is superior to standard denseNet or Resnet pattern. The found configuration was not compared to these baselines. So again we do not have evidence showing the merit of keeping and tuning many parameters with the RINFORCE - The experiments with Penn Treebank are described with too less details: for example, what exactly is the task considered (in terms on input-output mapping), what is the dataset size, etc.. - Also, for the Penn treebank experiments no baseline is given, so it is not possible to understand if the structure learning here is beneficial. Comparison of the results to an architecture with all skip connections, and with a single skip connection per layer is required to estimate if useful structure is being learnt. Overall: - Pro: the method gives high accuracy results - Cons: o It is not clear if the ENAS search is responsible to the results, or just the strong baseline. The advantage of ENAS over plain hyper parameter choosing was not sufficiently established. o The controller was not presented in a clear enough manner. Many of its details stay obscure. o The method does not seem to be general. It seems to be limited to choosing a specific set of parameters in very specific scenario (scenario which enable parameter sharing between model. The conditions for this to happen seems to be rather strict, and where not elaborated). After revision: The controller is now better presented. However, the main points were not changed: - ENAS seems to be limited to a specific architecture and search space, in which probably the search is already exhausted. For example for the image processing network, it is determining the number of skip connections and structure of a single layer as a combination of several function types. We already know the answers to these search problems (denser skip connection pattern works better, more functions types in a layer in parallel do better, the number of maps should be adjusted to the complexity and data size to avoid overfit). ENAS does not reveal a new surprising architectures, and it seems that instead of searching in the large space it suggests, one can just tune a 1-2 parameters (for the image network, it is the number of maps in a layer). - Results comparing ENAS results to the simple baseline of just tuning 1-2 hyper parameters were not shown. I hence believe the strong empirical results of ENAS are a property of the search space (the architecture used) and not of the search algorithm.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . We were very dismayed by the low score . Subsequently , we have completely rewritten the paper to make it easier to follow .We have also included new experimental results to address the reviewer \u2019 s concerns . All the results are reported in our revision . We hope that our revisions of the paper can clear the reviewer \u2019 s reservation about ENAS \u2019 s ability . In the following , we try to address the reviewer \u2019 s concerns . First , we have completely rewritten the paper for clarity . We focused on delivering the high level ideas , and we moved a lot of implementation details into our appendix . To address the reviewer \u2019 s particular comment about the lack of details on the controller , we refer the reviewer to Section 3 in our revision . As evidenced by the anonymous comment above , our previous presentation is sufficient for independent researchers to to implement our method . Therefore , we believe that our revision , which aims to improve the original presentation , does not obscure ENAS \u2019 s details . After the reviewing cycle , we will also publish our code , which we hope will clear any ambiguity about ENAS . Second , in our revision , we have included new experimental results to address the reviewer \u2019 s concerns that ENAS is not general , and whether ENAS is responsible for the good results . We summarize them below : 1 ) ENAS is indeed responsible for the results . This information was in our original submission . In our revision , it is highlighted in the paragraph \u201c Sanity Check and Ablation Study \u201d at the beginning Section 4.3 . In particular , a model randomly sampled from our search space does not perform as well as a model sampled by the ENAS controller . Also , we if train ENAS without training its controller , performance is much worse . Both observations , as presented in the paper , indicate the importance of ENAS . To further address the reviewer \u2019 s concerns , we have conducted more controlled experiments . Following are their results : 1a ) 64 or 96 maps on CIFAR-10 models . Sure , a model with randomly chosen 64 or 96 maps on each block , each layer may perform similarly to ENAS . However , in this search space , the controller can take up to 256 maps . Without ENAS , a random model designer would select roughly 128 maps at each block , each layer . If you haven \u2019 t seen ENAS \u2019 s decisions to pick 64 or 96 maps , would you think of such a baseline ? We do agree that a slight tune of hyper-parameters may also lead to this model . However , in other search spaces ( e.g.see Section 4.2 in our revision ) , where one needs to figure out the skip connections , the tuning of hyper-parameters won \u2019 t be as \u201c slight \u201d . 1b ) The pattern of skip connections found by ENAS is indeed better than the DenseNet pattern and the ResNet pattern . In our settings , the DenseNet pattern ( connecting every pair of layers ) achieves 5.23 % test error , and the ResNet pattern ( connecting each layer to the next ) achieves 6.01 % test error . We also note that the DenseNet and the ResNet patterns in our settings are not the DenseNet and ResNet in their original papers . The reason for the differences lies in the design choice of our search spaces : we make skip connections go through a conv1x1 instead of concatenation as in the original DenseNet ( Huang et al. , 2016 ) , or identity and addition as in the original ResNet ( He et al. , 2015 ) . Such design choice may be sub-optimal , and we will try the identity skip connections in our next revision . However , our controlled experiment does show that in our search space , the skip connections that ENAS finds do achieve non-trivial improvements compared to standard patterns . 2 ) ENAS is a general method . To see this , note that one way to do programming is to search for a path in a bigger program , where all operations are available at every step . In ENAS , the computations in a neural architecture can be viewed as a program , which is represented as directed acyclic graph ( DAG ) ( see Section 1 of our revision ) . To apply ENAS to any task , e.g.designing a convolutional network , or designing a recurrent cell , one only needs to specify the DAG \u2019 s components ( examples are now in Section 3 of our revision ) . 3 ) We further elaborate point 2 ) above by applying ENAS to different search spaces . First , we use ENAS to search for both skip connections and layer operations ( convolutions with different filter sizes , or average pooling , or max pooling ) . It turns out that in this search space , ENAS could discover a model with CIFAR-10 test error of 4.23 % . This resulting model is comparable to the model found in the restricted search space over convolutions and pooling masks . Therefore , ENAS works in this search space . [ to be continued ]"}, "2": {"review_id": "ByQZjx-0--2", "review_text": "In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing. In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks. In both approaches, reinforcement learning is used to learn a policy that maximizes the expected reward of some validation set metric. Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice. In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time. The authors present two ENAS models: one for CNNs, and another for RNNs. Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections. However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space. This is a limitation, as the model space is not as flexible as one would desire in a discovery task. Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections. Thus, they are essentially learning the skip connections while using a human-selected model. ENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections. Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections. Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. Overall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process. Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea. It is also impressive how much faster their model performs on tasks without sacrificing much performance. The main limitation is that the best architectures as currently described are less about discovery and more about human input -- finding a more efficient search path would be an important next step.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . We were very dismayed by the low score . Subsequently , we have completely rewritten the paper to make it easier to follow . We have also included new experimental results to address the reviewer \u2019 s concerns . All the results are reported in our revision and are summarized below . We hope that our revisions of the paper can clear the reviewer \u2019 s reservation about ENAS \u2019 s ability . Summary of New Results : 1 ) The reviewer is concerned that ENAS can only search small search spaces . This is not the case . We used ENAS to search for both skip connections and layer operations ( convolutions with different filter sizes , or average pooling , or max pooling ) . It turns out that in this large search space , ENAS could also discover a model with CIFAR-10 test error of 4.23 % . This resulting model is comparable to the model found in the restricted search space over convolutions and pooling masks . Therefore , search space size is not a limitation of ENAS . 2 ) The reviewer is concerned that the best ENAS model is \u201c less about discovery and more about human input. \u201d We show that ENAS can do well with less human inputs . In particular , we took the pattern of skip connections discovered by ENAS ( Figure 5-Right in our revision ) , and simply increased the number of output channels at each layer from 256 to 512 . The resulting model achieves 3.87 % test error on CIFAR-10 . In the original paper , the best ENAS result on CIFAR-10 is 3.86 % test error , achieved by using multiple branches at each layer . Therefore , we showed that the model found by ENAS , with minimal human inputs , can achieve a similar performance to models that are designed with more human inputs . We note that ENAS \u2019 s principle , i.e.searching for a path in a big model is general . Under this principle , we can do whatever other NAS approaches can do . We also note that the human input of increasing the number of output channels was also performed by the original NAS paper ( Zoph and Le , 2017 ) . 3 ) We designed a different search space for recurrent cells . In this search space , ENAS finds a novel recurrent cell that achieves 57.8 test perplexity on Penn Treebank . We have reported this new result in our revision ( Sections 4.1 and 5.1 ) . Let \u2019 s call this the ENASCell . ENASCell is very novel compared to recurrent highway network ( see Figure 4 in our revision ) . While the search space for ENASCell still benefits from highway connections , the ENAS controller has discovered several novelties : - the use of the ReLU activation , unlike in recurrent highway network where only the tanh activation is used - the pattern of connections within the ENASCell To our knowledge , ENASCell \u2019 s perplexity of 57.8 is the state-of-the-art among automatic model design approaches on Penn Treebank . ENASCell outperforms NAS ( 62.4 perplexity ) , which uses two orders of magnitude more computations and one order of magnitude more time . ENASCell , with almost no hyper-parameters tuning , also outperformed LSTM with extensive hyper-parameters tuning ( 59.5 perplexity ) ( Melis et al. , 2017 ) . We now compare ENAS to other ICLR 2018 submissions which address similar problems , i.e.computationally inexpensive approaches for automatic model designing . In particular , Paper 1 presents SMASH , a method that automatically designs networks for image classification tasks , and Paper 323 presents a method that automatically designs recurrent cells . As of now , despite the fact that their methods perform worse than ENAS , both papers receive at least one 7 in their reviews . We believe that ENAS \u2019 s contributions are significant , both in the novelty of its idea and in the significance of its results . We sincerely hope that you will give ENAS another consideration . [ 1 ] Paper 1 : SMASH : One-Shot Model Architecture Search through HyperNetworks ( https : //openreview.net/forum ? id=rydeCEhs- ) [ 2 ] Paper 323 : A Flexible Approach to Automated RNN Architecture Generation ( https : //openreview.net/forum ? id=SkOb1Fl0Z )"}}