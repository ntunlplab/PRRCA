{"year": "2020", "forum": "rye5YaEtPr", "title": "SAdam: A Variant of Adam for Strongly Convex Functions", "decision": "Accept (Poster)", "meta_review": "The reviewers all appreciated the results. They expressed doubts regarding the discrepancy between the assumptions made and the reality of the loss of deep networks.\n\nI share these concerns with the reviewers but also believe that, due to the popularity of Adam, a careful analysis of a variant is worthy of publication.", "reviews": [{"review_id": "rye5YaEtPr-0", "review_text": "In the setting of online convex optimization, this paper investigates the question of whether adaptive gradient methods can achieve \u201cdata dependent\u201d logarithmic regret bounds when the class of loss functions is strongly convex. To this end, the authors propose a variant of Adam - called SAdam - which indeed satisfies such a desired bound. Importantly, SAdam is an extension of SC-RMSprop (a variant of RMSprop) for which a \u201cdata independent\u201d logarithmic bound was found. Experiments on optimizing strongly convex functions and training deep networks show that SAdam outperforms other adaptive gradient methods (and SGD). The paper is very well-written, well-motivated and well-positioned with respect to related work. The regret analysis of SAdam is conceptually simple and elegant. The experimental protocol is well-detailed, and the results look promising. In a nutshell, this is an excellent piece of work. I have just a minor comment. In the experiments, SAdam was tested using $\\beta_1 = 0.9$ and $\\beta_{2t} = 1 - \\frac{0.9}{t}$. Since Corollary 2 covers a wide range of admissible values for these parameters, it would be interesting to report (for example in Appendix) a sensitivity analysis of SAdam, using different choices of $\\beta_1$ and $\\beta_{2t}$. ", "rating": "8: Accept", "reply_text": "Thanks for your comments ! Q1 : It would be interesting to report ( for example in Appendix ) a sensitivity analysis of SAdam , using different choices of \\beta_1 and \\beta_ { 2t } . A1 : Thanks for your constructive suggestion and we will provide experimental results about the sensitivity with respect to \\beta_1 and \\beta_ { 2t } in the revised version . From our experience , SAdam performs well in a wide range of hyper-parameter choices ."}, {"review_id": "rye5YaEtPr-1", "review_text": "In this paper, the authors propose a variant of Adam, named as SAdam, and establish a data-dependent O(log T) regret bound. The key idea is using a faster decaying yet under controlled step size to exploit strong convexity. Some experiments are carried out to demonstrate the effectiveness of the proposed algorithm. The idea seems interesting, the writing is well-written, and the analysis seems correct (I did not fully check all steps, but the key steps seems ok to me). Probs: 1. The proposed SAdam is an effective variant of Adam designed for strongly convex functions. The algorithm is a natural extension of Adam, and SC-RMSprop could be regarded as a special case. 2. The authors establish a data-dependent O(log T) regret bound for SAdam, and as a byproduct, they present the first data-dependent logarithmic regret for SC-RMSprop. The authors also fix a small bug in the analysis of AMSgrad. The theoretical result is the key technical contribution of this paper. 3. The experimental results shows that Aadam can be used to minimize strongly convex functions, as well as neural networks, which is believed to be non-convex. Cons: 1. As the authors mentioned in Remark 2, the main limitation of their analysis is that the role of the first-order momentum is unclear. Although the first-order momentum can accelerate the convergence in practice, proving this in theory remains an open problem. Is there some contribution on this aspect? 2. The 4-layer CNN in Section 4.2 is a bit small. It would be better if the authors test their algorithm on larger and more popular neural networks. In summary, this paper contributes the theoretical studies of ADAM-type algorithm, although the algorithm is somehow incremental. To me, a bit surprising result is that the step size originally designed for strongly convex functions also works well for training CNN. ", "rating": "6: Weak Accept", "reply_text": "Thanks for the comments ! Q1 : \u201c The role of the first-order momentum is unclear\u2026is there some contribution on this aspect ? \u201d A1 : While our paper is the first to show that algorithms equipped with first-order momentum can achieve logarithmic regret bound for strongly convex functions , it is still an open problem to explicitly analysis the influence of this procedure . We note that all the regret bounds of Adam-like algorithms ( e.g. , Reddi et al. , 2018 ; Chen et al. , 2018a ) suffer this limitation , and the advantage of first-order momentum is mainly proved by empirical studies . The difficulty is caused by the fact that the regret bound is data-dependent . Specifically , our regret bounds depend on the cumulation of all gradients g_t , and each g_t would be affected by the first-order momentum . We would like to analyze the influence of first-order momentum theoretically in the future . Q2 : The 4-layer CNN in Section 4.2 is a bit small . It would be better if the authors test their algorithm on larger and more popular neural networks . A2 : Thanks for the suggestion . We have applied our algorithm to the training of ResNet18 ( He et al. , 2016 ) in Appendix H. We are sorry that we forget to mention this clearly in the main paper , and will provide more experiments in the full version ."}, {"review_id": "rye5YaEtPr-2", "review_text": "This paper studies Adam and proves that under strong convexity assumption, it obtains the improved regret bound $O(log(T))$. The regret bound is data-dependent, thus as a side-effect it also improves previous known result for strongly convex RMSProp (SC-RMSProp). The paper is clear and well-written and I also think that theoretical results are correct and new. However, I have some concerns on the possible impacts of the results especially in the context of ICLR: - First of all, the assumption to show improved regret is strong convexity of all functions $f_t$. However, this is very restrictive and much stronger than the assumption that the sum of functions is strongly convex. In addition, from what I see, in the proof of Theorem 1, the authors use strong convexity with the $x^\\star$, so they can maybe replace global strong convexity to strong convexity restricted to the path towards the solution. A reference where these restricted strong convexity type assumptions are studied: Necoara, Nesterov, Glineur, \u201cLinear convergence of first order methods for non-strongly convex optimization\u201d, Math. Prog. 2019. - To show improved regret, consistent with previous work SC-Adagrad and SC-RMSProp, the authors modify the algorithm to use $V_t^{-1}$ in page 3 in the step size, instead of $V_t^{-1/2}$ of regular Adam. It is easy to see that this is to make sure step size has a faster decrease, which is needed also to show standard SGD gets $1/k$ rate for strongly convex problems. However, given that one might not now if the problem has strong convexity (there might exist cases where this property only exists locally), it is not clear if one should apply Adam or SAdam. - Another remark related to the previous one is the following. Standard SGD uses step size $\\alpha_0/\\sqrt{k}$ for convex optimization without strong convexity and $\\alpha_0/k$ for strongly convex optimization. If one uses $alpha_0/k $for convex optimization without strong convexity, one gets a very bad rate $1/log(k)$ and very bad practical performance. So, given SAdam gives step sizes suited for strongly convex optimization (similar to SGD for strongly convex optimization), I would expect SAdam's step sizes to be not very suitable when there is no strong convexity. - An additional point is that the step size of SAdam depends on the global strong convexity parameter $\\lambda$ which further restricts the applicability of the method. For the theoretical results to hold, the step size should be set according to $\\lambda$, and when the step size is not selected that way, one loses the fast convergence rate. - In the experiments, the authors show the performance of SAdam for neural network training and related to my previous remarks, I have the following concerns. First of all, how do the authors pick step sizes now since it depends on strong convexity constant as in eq. (7). In addition, given that neural networks are certainly non-strongly convex, I would expect that the fast decreasing step size caused by using $V_t^{-1}$ might also hurt the performance considerably, which happens as I discussed above even for convex but non-strongly convex losses. I would suspect that much worse effects can be seen for non-convex optimization. Of course, the authors can argue that if the loss landscape of neural network has some local strong convexity parameters, SAdam would adapt and get faster convergence. But unfortunately, I would not agree with such a statement, because the analysis is not made to adapt to local strong convexity and a dependence to strong convexity constant is present due to eq. (7), so if one does not know the constant, the theoretical guarantees would not apply. In addition, the provided experiments for neural network training is not extensive enough to convince practitioners to use SAdam instead of Adam which has been used for years. Overall, I think that it is interesting to see that a variant of Adam can be shown to obtain improved regret under strong convexity, I find the assumptions strong and the impact for neural network training, therefore for ICLR, quite questionable.", "rating": "3: Weak Reject", "reply_text": "Thanks for the comments ! Q1 : \u201c In the proof of Theorem 1 , the authors use strong convexity with the x_ * , so they can maybe replace global strong convexity to strong convexity restricted to the path towards the solution. \u201d A1 : Thanks for the suggestion . We agree that it could be possible to replace the global strong convexity to restricted strong convexity . Then , we probably need to study the stochastic setting , and bound the excess risk instead of the regret . It is difficult to exploit restricted strong convexity in the analysis of regret of OCO . Because in the online setting , the optimal solution of each f_t is different and may not be x_ * . Therefore , if we replace global strong convexity of each f_t by restricted strong convexity ( with respect to its own optimal solution ) , the inequality about x_ * ( eq . ( 13 ) ) can not be obtained , unless all f_t share the same optimal solution . We will study the restricted strong convexity as a future work . Q2 : \u201c Given that one might not know if the problem has strong convexity , \u2026 it is not clear if one should apply Adam or SAdam\u2026I would expect SAdam 's step sizes to be not very suitable when there is no strong convexity\u2026the step size of SAdam depends on the global strong convexity parameter lambda. \u201d A2 : First , we would like to emphasize that optimization under lambda-strong convexity is a classic problem which has been widely studied in both OCO and stochastic optimization ( e.g. , Hazan et al. , 2007 , Hazan & Kyle , 2014 , Mukkamala & Hein , 2017 , Chen et al.2018 ) .The problem is important by its own right . Second , if the type of loss functions or the value of lambda is unknown to the learner , it is possible to combine the theoretical guarantees of Adam and SAdam by applying the universal algorithm framework ( van Erven et al. , 2016 , Wang et al. , 2019 ) . The key idea is to simultaneously run multiple copies of each algorithm with different learning rates in every round , and adaptively learn the best one on the fly . In this way , the algorithm can handle both convex and strongly convex functions , and does not need to know any prior knowledge of lambda . It is an interesting problem and will be investigated in the future . Q3 : How do the authors pick step sizes now since it depends on strong convexity constant as in eq . ( 7 ) .A3 : Following previous work ( Mukkamala & Hein , 2017 ) , for all optimization algorithms , we pick the step sizes in the set { 10^ { -1 } ,10^ { -2 } ,10^ { -3 } , 10^ { -4 } } and report the best results . Q4 : \u201c Given that neural networks are certainly non-strongly convex\u2026I would suspect that much worse effects can be seen for non-convex optimization. \u201d A4 : We agree that currently there still exists a gap between the theoretical analysis of SAdam and its applications to training networks . However , we note that , initially , most of the popular algorithms such as Adagrad , Adam , AMSgrad and SC-RMSprop , are analyzed under the convex assumption or strongly convex assumption . Although these assumptions are violated in training networks , these algorithms have exhibited outstanding results in the experiments . Moreover , the analysis in convex setting lays the foundations of many follow-up works that investigate the non-convex problems ( e.g. , Basu et al. , 2018 , Chen et al. , 2019 , Staib et al. , 2019 ) . In this paper , we prove that our proposed SAdam is able to attain tighter regret bounds under strongly convex condition , and empirically show that it achieves better performance for training some networks . We believe our results are meaningful and could inspire the analysis of Adam-type algorithms under non-convex settings . T. van Erven , and W. M. Koolen . Metagrad : Multiple learning rates in online learning . In NIPS , pages 3666\u20133674 , 2016 . G. Wang , S. Lu , and L. Zhang . Adaptivity and optimality : A universal algorithm for online convex optimization . In UAI , 2019 . M. Staib , S. J. Reddi , S. Kale , S. Kumar , & S. Sra . Escaping saddle points with adaptive gradient methods . arXiv preprint arXiv:1901.09149 , 2019 ."}], "0": {"review_id": "rye5YaEtPr-0", "review_text": "In the setting of online convex optimization, this paper investigates the question of whether adaptive gradient methods can achieve \u201cdata dependent\u201d logarithmic regret bounds when the class of loss functions is strongly convex. To this end, the authors propose a variant of Adam - called SAdam - which indeed satisfies such a desired bound. Importantly, SAdam is an extension of SC-RMSprop (a variant of RMSprop) for which a \u201cdata independent\u201d logarithmic bound was found. Experiments on optimizing strongly convex functions and training deep networks show that SAdam outperforms other adaptive gradient methods (and SGD). The paper is very well-written, well-motivated and well-positioned with respect to related work. The regret analysis of SAdam is conceptually simple and elegant. The experimental protocol is well-detailed, and the results look promising. In a nutshell, this is an excellent piece of work. I have just a minor comment. In the experiments, SAdam was tested using $\\beta_1 = 0.9$ and $\\beta_{2t} = 1 - \\frac{0.9}{t}$. Since Corollary 2 covers a wide range of admissible values for these parameters, it would be interesting to report (for example in Appendix) a sensitivity analysis of SAdam, using different choices of $\\beta_1$ and $\\beta_{2t}$. ", "rating": "8: Accept", "reply_text": "Thanks for your comments ! Q1 : It would be interesting to report ( for example in Appendix ) a sensitivity analysis of SAdam , using different choices of \\beta_1 and \\beta_ { 2t } . A1 : Thanks for your constructive suggestion and we will provide experimental results about the sensitivity with respect to \\beta_1 and \\beta_ { 2t } in the revised version . From our experience , SAdam performs well in a wide range of hyper-parameter choices ."}, "1": {"review_id": "rye5YaEtPr-1", "review_text": "In this paper, the authors propose a variant of Adam, named as SAdam, and establish a data-dependent O(log T) regret bound. The key idea is using a faster decaying yet under controlled step size to exploit strong convexity. Some experiments are carried out to demonstrate the effectiveness of the proposed algorithm. The idea seems interesting, the writing is well-written, and the analysis seems correct (I did not fully check all steps, but the key steps seems ok to me). Probs: 1. The proposed SAdam is an effective variant of Adam designed for strongly convex functions. The algorithm is a natural extension of Adam, and SC-RMSprop could be regarded as a special case. 2. The authors establish a data-dependent O(log T) regret bound for SAdam, and as a byproduct, they present the first data-dependent logarithmic regret for SC-RMSprop. The authors also fix a small bug in the analysis of AMSgrad. The theoretical result is the key technical contribution of this paper. 3. The experimental results shows that Aadam can be used to minimize strongly convex functions, as well as neural networks, which is believed to be non-convex. Cons: 1. As the authors mentioned in Remark 2, the main limitation of their analysis is that the role of the first-order momentum is unclear. Although the first-order momentum can accelerate the convergence in practice, proving this in theory remains an open problem. Is there some contribution on this aspect? 2. The 4-layer CNN in Section 4.2 is a bit small. It would be better if the authors test their algorithm on larger and more popular neural networks. In summary, this paper contributes the theoretical studies of ADAM-type algorithm, although the algorithm is somehow incremental. To me, a bit surprising result is that the step size originally designed for strongly convex functions also works well for training CNN. ", "rating": "6: Weak Accept", "reply_text": "Thanks for the comments ! Q1 : \u201c The role of the first-order momentum is unclear\u2026is there some contribution on this aspect ? \u201d A1 : While our paper is the first to show that algorithms equipped with first-order momentum can achieve logarithmic regret bound for strongly convex functions , it is still an open problem to explicitly analysis the influence of this procedure . We note that all the regret bounds of Adam-like algorithms ( e.g. , Reddi et al. , 2018 ; Chen et al. , 2018a ) suffer this limitation , and the advantage of first-order momentum is mainly proved by empirical studies . The difficulty is caused by the fact that the regret bound is data-dependent . Specifically , our regret bounds depend on the cumulation of all gradients g_t , and each g_t would be affected by the first-order momentum . We would like to analyze the influence of first-order momentum theoretically in the future . Q2 : The 4-layer CNN in Section 4.2 is a bit small . It would be better if the authors test their algorithm on larger and more popular neural networks . A2 : Thanks for the suggestion . We have applied our algorithm to the training of ResNet18 ( He et al. , 2016 ) in Appendix H. We are sorry that we forget to mention this clearly in the main paper , and will provide more experiments in the full version ."}, "2": {"review_id": "rye5YaEtPr-2", "review_text": "This paper studies Adam and proves that under strong convexity assumption, it obtains the improved regret bound $O(log(T))$. The regret bound is data-dependent, thus as a side-effect it also improves previous known result for strongly convex RMSProp (SC-RMSProp). The paper is clear and well-written and I also think that theoretical results are correct and new. However, I have some concerns on the possible impacts of the results especially in the context of ICLR: - First of all, the assumption to show improved regret is strong convexity of all functions $f_t$. However, this is very restrictive and much stronger than the assumption that the sum of functions is strongly convex. In addition, from what I see, in the proof of Theorem 1, the authors use strong convexity with the $x^\\star$, so they can maybe replace global strong convexity to strong convexity restricted to the path towards the solution. A reference where these restricted strong convexity type assumptions are studied: Necoara, Nesterov, Glineur, \u201cLinear convergence of first order methods for non-strongly convex optimization\u201d, Math. Prog. 2019. - To show improved regret, consistent with previous work SC-Adagrad and SC-RMSProp, the authors modify the algorithm to use $V_t^{-1}$ in page 3 in the step size, instead of $V_t^{-1/2}$ of regular Adam. It is easy to see that this is to make sure step size has a faster decrease, which is needed also to show standard SGD gets $1/k$ rate for strongly convex problems. However, given that one might not now if the problem has strong convexity (there might exist cases where this property only exists locally), it is not clear if one should apply Adam or SAdam. - Another remark related to the previous one is the following. Standard SGD uses step size $\\alpha_0/\\sqrt{k}$ for convex optimization without strong convexity and $\\alpha_0/k$ for strongly convex optimization. If one uses $alpha_0/k $for convex optimization without strong convexity, one gets a very bad rate $1/log(k)$ and very bad practical performance. So, given SAdam gives step sizes suited for strongly convex optimization (similar to SGD for strongly convex optimization), I would expect SAdam's step sizes to be not very suitable when there is no strong convexity. - An additional point is that the step size of SAdam depends on the global strong convexity parameter $\\lambda$ which further restricts the applicability of the method. For the theoretical results to hold, the step size should be set according to $\\lambda$, and when the step size is not selected that way, one loses the fast convergence rate. - In the experiments, the authors show the performance of SAdam for neural network training and related to my previous remarks, I have the following concerns. First of all, how do the authors pick step sizes now since it depends on strong convexity constant as in eq. (7). In addition, given that neural networks are certainly non-strongly convex, I would expect that the fast decreasing step size caused by using $V_t^{-1}$ might also hurt the performance considerably, which happens as I discussed above even for convex but non-strongly convex losses. I would suspect that much worse effects can be seen for non-convex optimization. Of course, the authors can argue that if the loss landscape of neural network has some local strong convexity parameters, SAdam would adapt and get faster convergence. But unfortunately, I would not agree with such a statement, because the analysis is not made to adapt to local strong convexity and a dependence to strong convexity constant is present due to eq. (7), so if one does not know the constant, the theoretical guarantees would not apply. In addition, the provided experiments for neural network training is not extensive enough to convince practitioners to use SAdam instead of Adam which has been used for years. Overall, I think that it is interesting to see that a variant of Adam can be shown to obtain improved regret under strong convexity, I find the assumptions strong and the impact for neural network training, therefore for ICLR, quite questionable.", "rating": "3: Weak Reject", "reply_text": "Thanks for the comments ! Q1 : \u201c In the proof of Theorem 1 , the authors use strong convexity with the x_ * , so they can maybe replace global strong convexity to strong convexity restricted to the path towards the solution. \u201d A1 : Thanks for the suggestion . We agree that it could be possible to replace the global strong convexity to restricted strong convexity . Then , we probably need to study the stochastic setting , and bound the excess risk instead of the regret . It is difficult to exploit restricted strong convexity in the analysis of regret of OCO . Because in the online setting , the optimal solution of each f_t is different and may not be x_ * . Therefore , if we replace global strong convexity of each f_t by restricted strong convexity ( with respect to its own optimal solution ) , the inequality about x_ * ( eq . ( 13 ) ) can not be obtained , unless all f_t share the same optimal solution . We will study the restricted strong convexity as a future work . Q2 : \u201c Given that one might not know if the problem has strong convexity , \u2026 it is not clear if one should apply Adam or SAdam\u2026I would expect SAdam 's step sizes to be not very suitable when there is no strong convexity\u2026the step size of SAdam depends on the global strong convexity parameter lambda. \u201d A2 : First , we would like to emphasize that optimization under lambda-strong convexity is a classic problem which has been widely studied in both OCO and stochastic optimization ( e.g. , Hazan et al. , 2007 , Hazan & Kyle , 2014 , Mukkamala & Hein , 2017 , Chen et al.2018 ) .The problem is important by its own right . Second , if the type of loss functions or the value of lambda is unknown to the learner , it is possible to combine the theoretical guarantees of Adam and SAdam by applying the universal algorithm framework ( van Erven et al. , 2016 , Wang et al. , 2019 ) . The key idea is to simultaneously run multiple copies of each algorithm with different learning rates in every round , and adaptively learn the best one on the fly . In this way , the algorithm can handle both convex and strongly convex functions , and does not need to know any prior knowledge of lambda . It is an interesting problem and will be investigated in the future . Q3 : How do the authors pick step sizes now since it depends on strong convexity constant as in eq . ( 7 ) .A3 : Following previous work ( Mukkamala & Hein , 2017 ) , for all optimization algorithms , we pick the step sizes in the set { 10^ { -1 } ,10^ { -2 } ,10^ { -3 } , 10^ { -4 } } and report the best results . Q4 : \u201c Given that neural networks are certainly non-strongly convex\u2026I would suspect that much worse effects can be seen for non-convex optimization. \u201d A4 : We agree that currently there still exists a gap between the theoretical analysis of SAdam and its applications to training networks . However , we note that , initially , most of the popular algorithms such as Adagrad , Adam , AMSgrad and SC-RMSprop , are analyzed under the convex assumption or strongly convex assumption . Although these assumptions are violated in training networks , these algorithms have exhibited outstanding results in the experiments . Moreover , the analysis in convex setting lays the foundations of many follow-up works that investigate the non-convex problems ( e.g. , Basu et al. , 2018 , Chen et al. , 2019 , Staib et al. , 2019 ) . In this paper , we prove that our proposed SAdam is able to attain tighter regret bounds under strongly convex condition , and empirically show that it achieves better performance for training some networks . We believe our results are meaningful and could inspire the analysis of Adam-type algorithms under non-convex settings . T. van Erven , and W. M. Koolen . Metagrad : Multiple learning rates in online learning . In NIPS , pages 3666\u20133674 , 2016 . G. Wang , S. Lu , and L. Zhang . Adaptivity and optimality : A universal algorithm for online convex optimization . In UAI , 2019 . M. Staib , S. J. Reddi , S. Kale , S. Kumar , & S. Sra . Escaping saddle points with adaptive gradient methods . arXiv preprint arXiv:1901.09149 , 2019 ."}}