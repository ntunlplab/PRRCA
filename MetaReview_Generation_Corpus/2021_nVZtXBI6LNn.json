{"year": "2021", "forum": "nVZtXBI6LNn", "title": "Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers", "decision": "Accept (Poster)", "meta_review": "Thank you for your submission to ICLR.  As noted, several of the reviewers had fairly low confidence in evaluating this submission.  However, based upon the reviewers and commenters who were familiar with this line of work, as well as my own evaluation of the paper, I believe it is clearly worth publishing at ICLR.  The proposed method pushes the boundary in methods for exact branch and bound-based verification of neural networks, using clever tricks from existing relaxations.  And while the method is still likely to be relegated to relatively small networks for the time being, pushing forward the state of the art in exact verification is still a worthy goal suitable for publication at ICLR.  I thus think that the paper is quite clearly above the bar, and should be accepted for publication.", "reviews": [{"review_id": "nVZtXBI6LNn-0", "review_text": "The authors demonstrate that using a modification of the LiRPA method during the branch-and-bound process for solving the neural network verification problem can lead to significant speed-ups . The experimental results are strong . The authors convincingly show that the their method outperforms the existing state-of-the-art method by Lu & Kumar ( 2020 ) on an experimental setup similar to that work . The application of LiRPA to branch-and-bound is straightforward ( since any incomplete verifier can be used ) , as is the use of gradient descent to improve the bound given by LiRPA ( a standard technique applied to improve the bounds of certain verifiers ) . Despite the fairly straightforward approach , the strength of the empirical results deserves attention . Overall , a solid contribution to the literature , and proof that research on incomplete verifiers leads to better complete verifiers . Some questions/requests : - The experimental setup details should be provided in the final version . - How dependent is the performance of LiRPA on GPUs ? For example , if we do a CPU-only comparison between the different methods , would other methods now outperform LiRPA ? And if so by how much ? What if we use multiple cores ? I would understand if a detailed comparison is too computationally intensive , but I would like some sense of this . Update after author response : I thank the authors of the paper for significantly improving the prose of the paper , and I agree that the changes make the paper more self-contained and approachable . I have kept my ratings as my score was for primarily for the strong experiment results ( and the score was also conditional on the paper being more polished ) . I am happy to support this paper for acceptance , but I am a little concerned about the degree of changes in the final version versus the initial submission , given the number of concerns the other reviewers had .", "rating": "7: Good paper, accept", "reply_text": "Thank you so much for correctly recognizing the main contributions of our paper . We greatly appreciate your encouraging comments , and we are glad to answer your questions below . 1.Additional experimental setup : we have updated our paper and provide experimental setup details in Appendix B . We will release our full source code once accepted . 2.As suggested by the reviewer , we provided results on using single and multi-core CPU based LiRPA computation for our algorithm in Appendix C. Existing baselines such as BaBSR require an LP solver , which is hard to accelerate on GPU or even on multi-core CPU . The basic computation of LiRPA is just matrix multiplication ( like NN training ) , so it naturally enjoys the parallelization in existing deep learning software libraries such as Pytorch . Figure 5 in Appendix shows that our LiRPA based method on a single CPU core is still competitive when compared to BaBSR+LP on a single CPU , and we enjoy a speedup on multi-core CPUs . Additionally , we have greatly improved our paper in our revision , added examples to illustrate our problem under study and make the formulation and correctness of our algorithm more clear . We hope the reviewer can discuss our main contributions with other reviewers during the second and third stages of discussion . Thank you ."}, {"review_id": "nVZtXBI6LNn-1", "review_text": "The work proposes a new algorithm that can be used for the complete verification of neural networks ( NNs ) . Unfortunately , the authors do not define the verification problem they study : Based on the second paragraph of the introduction , one is given a neural network ( NN ) on the input , and the task is to determine whether the NN has a specific formally defined property - but which kind of properties are verified is never explained . Intuitively , one would expect verification to focus on determining whether the NN gives a `` correct '' output for certain inputs , but that does not really match the general description given in the paper , and I did not find a place where the verification problem is formalized further . Without knowing what `` verification '' means in the context of this paper , it 's difficult to follow the reasoning provided in the paper without having some rough idea of what kind of properties one wishes to verify ( for instance , the discussion about using LP bounds assumes that the property that is being verified can be expressed using LP ) . I believe this issue could have been avoided by formalizing the precise task of verification ( the verification problem ) . On that note , many parts of the paper seemed rather confusing and hard to follow , either due to inconsistencies or due to language issues . For instance , the sentence `` Input domain split is shown effective in verifying the properties with low input dimensions while performs as poorly as incomplete verifiers on higher dimension properties '' on page 2 seems to contradict the definitions given for complete and incomplete verification . How can a complete verifier perform `` as poorly as incomplete verifiers '' if , by the definition given in the paper , complete verifiers must always correctly determine whether the NN has the given `` property '' or not ? ( Section 2 : `` Complete verifiers guarantee to terminate either the property is proved or a violation is located . '' ) In terms of presentation , the submission contains an incredibly large number of minor language issues ( roughly 1 per 2-3 lines on average , ranging from minor article issues to malformed sentences ; see also the quote in the previous paragraph ) , and I strongly encourage authors to fix these as they have a rather disruptive effect when trying to read and understand the paper . A very small number of examples is provided below : Page 1 - '' cause the changes of NN predictions '' - > `` cause changes of NN predictions '' - '' Recently , a framework of Branch and Bound ( BaB ) ( Bunel et al. , 2018 ) is widely used for efficiently verifying NNs '' - can not combine `` recently '' and `` is '' . - '' adopts Linear Program ( LP ) bounding procedure '' - > `` adopts a Linear Program ( LP ) bounding procedure '' Page 2 - '' for construct LPs '' - > `` for constructing LPs '' The main contribution of the paper is the use of incomplete verifiers for complete verification , and the authors propose an algorithm for doing that using LIRPA bounds . However , I found no proof ( or anything resembling a proof ) showing that the resulting algorithm is correct , i.e. , that it performs complete verification for neural networks . In fact , the problem is not even properly and formally defined in the paper . Hence , regardless of the experimental results , I do not think that the submission is ready for publication at this stage . Post-Rebuttal Comment : I thank the authors for responding to my comments . The updated version fixes most of the criticisms raised in the review , and I have raised the score accordingly . My new score is `` 5 '' , partly because I believe that after performing such a large-scale and comprehensive overhaul of the paper ( which was certainly necessary ) , the paper should go through a full new reviewing process .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We greatly appreciate the very helpful comments from the reviewer . We have added necessary proofs , reorganized and revised many parts of our paper to make it easier to understand . We hope the reviewer can check out the updated version of our paper . We provide detailed answers below : 1 . Formalization and definition of the task of verification : To make the verification problem very clear , we now have added a definition at the beginning of introduction , and also give an example after the definition . In section 2 , we also give a more formal definition with discussions . We forgot to clearly define the properties under verification because the benchmarks used in our paper are fairly standard in this field , but now it has also been added everywhere , including in the experiment section . We hope it is now easier to understand the precise task of verification . 2.Proofs : We include more discussions on soundness and completeness in Section 2 and 3 . In Section 2 , the soundness of LiRPA has been proved in previous work ( Xu et al. , 2020 ) and we add a discussion on page 4 . In Section 3.2 , we have discussed the completeness in Theorem 3.1 and 3.2 . Theorem 3.1 shows that feasibility checking is important when using LiRPA as the bounding procedure in branch and bound , and Theorem 3.2 shows that with feasibility checking from LP , completeness is obtained , just like other works using branch and bound [ 1 ] [ 2 ] [ 3 ] . It is worth noting that many existing important works on complete verification such as [ 1 ] [ 2 ] [ 3 ] do not have a completeness proof , and it seems the completeness of the BaB process is implied , so most papers did not give proofs explicitly , and such a proof can look almost the same in every work . However , we do agree with the reviewer that we need to discuss more on the completeness of our algorithm and add explicit theorems for completeness , and we have done so in our revision . 3.Paper hard to follow , confusing sentences , language issues , typos We have fixed all confusing sentences and typos you mentioned and also greatly improve the writing and language of this paper . We added a clear definition of the verification problem , and also used more formal language to introduce the background and our algorithm in section 2 and 3 . We also added figures and examples to illustrate our ideas more clearly . We rewrote those sentences that were hard to understand . The overall flow and clarity of our paper have greatly improved now , and we hope you can take a look again . Conclusions : We would like to thank the reviewer again for your constructive feedback . We hope you can read our revised paper once again where we have made great effort to improve writing and readability . We hope our answers address all your concerns and hope you can re-evaluate our revised paper , because the main issue was mainly language and representation problems rather than technical ones . Please kindly let us know if you have any additional comments . Thank you . [ 1 ] Rudy Bunel , Jingyue Lu , Ilker Turkaslan , P Kohli , P Torr , and P Mudigonda . Branch and bound for piecewise linear neural network verification . Journal of Machine Learning Research , 21 ( 2020 ) [ 2 ] Jingyue Lu and M Pawan Kumar . Neural network branching for neural network verification . International Conference on Learning Representation ( ICLR ) , 2020 . [ 2 ] Rudy R Bunel , Ilker Turkaslan , Philip Torr , Pushmeet Kohli , and Pawan K Mudigonda . A unified view of piecewise linear neural network verification . In Advances in Neural Information Processing Systems , pp . 4790\u20134799 , 2018 ."}, {"review_id": "nVZtXBI6LNn-2", "review_text": "# # # Summary This paper describes a branch-and-bound ( BaB ) process for neural network verification that uses linear relaxation based perturbation analysis ( LiRPA ) . It gives a way to tighten the bounds obtained via LiRPA . Overall , this results is a complete verification procedure , which is an order of magnitude faster than existing linear programming ( LP ) procedures . # # # Strengths The biggest strength of the paper is the impressive experimental results in section 5 : the method described in the paper is several times faster than previous methods . # # # Concerns My main concern is that the paper is very difficult to understand . It seems to require a lot of background knowledge about the problem and the related literature , which is not clearly provided in the paper . I had trouble understanding the problem , the setup , and the proposed algorithm . Another concern is that the paper claims that the proposed verifier is complete , but there is no proof of that . It does not seem like that 's something too difficult to prove ( given that BaB + LP is complete ) , but it should still be clearly stated . Finally , the claim that the proposed framework outperforms previous methods by `` at least 10X and up to 50X '' is unsupported . Based on the results in section 5 , a fairer statement regarding the speed would be `` at least 3X and up to 15X '' faster . # # # Reasons for score It is very difficult to judge this contribution , as the paper is hard to understand . The two main reasons for the score I give are 1 ) some of the claims of the paper are unsupported ( see above ) and 2 ) I believe this paper will have a much better chance of conveying the idea and making a contribution , if more background knowledge and intuition is provided throughout . # # # Suggestions for improvement that have not affected the score I gave to the paper One way to significantly improve the paper is to introduce more examples . An example consisting of a simple neural network to refer to throughout the explanation of LiRPA and BaB , and also in section 4 , would make the paper much easier to read . As a reader , I felt I could n't appreciate the related work section so early in the paper . I encourage you to either move it later in the paper , or even better : introduce more background/examples in the introduction , as well as the notion of completeness , so that the related work is easier to understand . Some typos : * Abstract : `` we demonstrate over a magnitude speedup ... '' - > `` we demonstrate speedup of an order of magnitude ... '' * First paragraph of section 2 : `` guarantee to terminate either ... '' - > `` guarantee to terminate when either ... '' * First paragraph of page 3 : `` used in state-of-the-art verifier ( Lu & Kumar , 2020 ) '' - > `` used in the state-of-the-art verifier by Lu & Kumar ( 2020 ) '' . * Second paragraph of page 3 : `` Our paper firstly leverage ... '' - > `` Our paper firstly leverages ... '' * First paragraph of section 3.1 : `` linear functions in the form of ... '' - > `` linear functions of the form ... '' * Start of section 4.1. : `` As we have introduced ... '' - > `` As we discussed ... '' * Mid page 5 : `` greatly limited ... '' - > `` greatly limits ... '' * Bottom of page 5 : `` We follow the most challenge experimental setup ... '' - > `` We follow the most challenging experimental setup ... '' * Mid page 6 : `` we quickly reaches ... '' - > `` we quickly reach ... '' * Mid page 6 : `` with only two hidden node ... '' - > `` with only two hidden nodes ... '' * Section 4.3 : `` Benefited from our design ... '' - > `` Benefitting from our design ... '' * Conclusion : capitalisation of the first sentence # # # Post rebuttal Thank you to the authors for their detailed response and their effort in improving the presentation of the paper . I was impressed with how much the paper improved in this second version . In particular , I very much appreciate that the introduction starts with a simple one sentence explanation of the problem of neural network verification . This can be further improved if it included ( almost ) no maths , which can be deferred to the Background section . The figures in the updated paper are very good and a huge improvement of presentation . Finally , the paper now includes two clearly stated theorems , which also make the presentation and contribution much clearer . I have increased the score I gave to the paper . Regardless of what the outcome for ICLR will be , I would like to encourage the authors to re-iterate on the presentation to really crystallize the problem , definitions and the suggested approach the paper is already so much better than the first version , and even just a little more work can make it even better .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We really thank the reviewer for all the suggestions on improving our paper , and help us find many typos . We address your concerns below : # # # Concern 1 : Hard to understand , require a lot of background knowledge Thank you for pointing out this concern . We have greatly improved our paper in terms of introducing sufficient background and motivations especially for researchers outside of this field . In our revised Introduction , we have followed your suggestions and made the definition of verification problem very clear ( in Introduction ) , and given a detailed walkthrough of background ( section 2 ) , and examples and figures to illustrate our main idea ( Figure 1 and 2 ) . # # # Concern 2 : Proof for completeness : We include more discussions on soundness and completeness in Section 2 and 3 . In Section 2 , the soundness of LiRPA has been proved in previous work ( Xu et al. , 2020 ) and we add a discussion on page 4 . In Section 3.2 , we have discussed the completeness in Theorem 3.1 and 3.2 . Theorem 3.1 shows that feasibility checking is important when using LiRPA as the bounding procedure in branch and bound , and Theorem 3.2 shows that with feasibility checking from LP , completeness is obtained , just like other works using branch and bound [ 1 ] [ 2 ] [ 3 ] . It is worth noting that many existing important works on complete verification such as [ 1 ] [ 2 ] [ 3 ] do not have a completeness proof , and it seems the completeness of the BaB process is implied , so most papers did not give proofs explicitly , and such a proof can look almost the same in every work . However , we do agree with the reviewer that we need to discuss more on the completeness of our algorithm and add explicit theorems for completeness , and we have done so in our revision . # # # Concern 3 : Speedup claim : Thank you for pointing this out ! We forgot to update these numbers in introduction when our experiment results were updated . We have fixed the speedup claims in our paper . Our speedup is 30X compared to basic BaB baselines , and up to 5X compared to the state-of-the-art verifier . In our revision , we also added a very recent baseline ( proximal BaBSR ) and our method still performs best . # # # Suggestion 1 : Introduce more examples Following the reviewer \u2019 s suggestion , we have added Figure 2 to explain LiRPA bounds and the BaB procedure . Additionally , we also introduce more examples in the text : for example , in the introduction , we give the definition of the verification problem and show the example after the definition . We hope these updates will make our paper much easier to understand . # # # Suggestion 2 : Related work section As suggested by the reviewer , we have moved the related work section to the end of the paper , and enhanced the background section . We introduce the basic definitions of verification problems and the notation of completeness as early as in Introduction , and also give more former notations in section 2 . # # # Typos : Thank you for pointing out these typos . We have fixed them and also greatly improved writing and clarity in our new revision . We added the definition of verification , completeness and incompleteness in Introduction . # # # Conclusion : We have significantly improved the writing of our paper and provide sufficient background and intuitions in our updated paper . We have also formally discussed and proven the completeness of our proposed algorithm . Since the most concerns on our paper are about writing and representation , we hope these changes address the concerns of the reviewer , and hope the reviewer can reconsider the score based on our revision . Feel free to let us know if you have any further questions regarding our revision . Thank you . [ 1 ] Rudy Bunel , Jingyue Lu , Ilker Turkaslan , P Kohli , P Torr , and P Mudigonda . Branch and bound for piecewise linear neural network verification . Journal of Machine Learning Research , 21 ( 2020 ) [ 2 ] Rudy R Bunel , Ilker Turkaslan , Philip Torr , Pushmeet Kohli , and Pawan K Mudigonda . A unified view of piecewise linear neural network verification . In Advances in Neural Information Processing Systems , pp . 4790\u20134799 , 2018 . [ 3 ] Jingyue Lu and M Pawan Kumar . Neural network branching for neural network verification . International Conference on Learning Representation ( ICLR ) , 2020 ."}, {"review_id": "nVZtXBI6LNn-3", "review_text": "The paper focuses on verifying simple properties of neural networks on accelerator hardware . Instead of using linear programming , Lirpa is considered as an alternative and minimal amounts of LP is added around to allow to use the same class of properties . In the considered examples the approach becomes much faster than the LP approach . My main problem with the paper , is that is claims a complete verification procedure without proper proofs . Yes , the new algorithm is presented and some general discussion of things that are done with it and how they work is provided . However , to claim complete verification a general soundness of the procedure should be proved . In particular I would like a theorem for the correctness of each of the various components and a combined theorem for the whole procedure . On the other hand , the experiments show that the approach is fast and as such makes it more feasible for verification and the paper mostly reads well . An interesting alternative to discuss in related work , could be proof assistants . See for example the work of Bentkamp , Blanchette JAR 2019 . Using proof assistants based on more complex logics , one can verify properties much more efficiently . The authors only mention Katz 's work on SMT , but if you consider higher order logic and the logics of interactive theorem provers , the `` NP-hard properties '' can be checked without considering all the cases . Minor : Conclusion starts with lowercase `` we '' . Based on the problems found by the other reviews and having read the rebuttals I have modified my score .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the helpful reviews from you , and we would like to address your concerns below : 1 . Proofs : In our revised paper , we have greatly improved every section . In section 2 , we discussed the soundness of LiRPA method . In section 3 , we gave two Theorems to show the completeness of our full branch and bound algorithm based on the property of LiRPA method . In fact , the proof for correctness of branch and bound is relatively straightforward , and many previous works in complete verification [ 1 ] [ 2 ] [ 3 ] did not give an explicit proof and just imply it is correct ( the proof would be very similar in every work ) . However , we have added proofs in our paper to ensure clarity . Specifically , in Section 3.2 , we have discussed the completeness in Theorem 3.1 and 3.2 . Theorem 3.1 shows that feasibility checking is important when using LiRPA as the bounding procedure in branch and bound , and Theorem 3.2 shows that with feasibility checking from LP , completeness is obtained , just like other works using branch and bound [ 1 ] [ 2 ] [ 3 ] . 2.Additional reference : Thank you for providing this insightful connection ! We have cited [ Bentkamp , Blanchette JAR 2019 ] on proof assistant based higher order logic provers . Lastly , we hope the reviewer can check out our revised paper . We gave more clear formal definitions of the verification problem , as well as more intuition , background and examples . We also include detailed discussions on soundness and completeness as well as proofs for correctness . We hope the reviewer can re-evaluate our paper based on our revision and update the rating . Thank you . [ 1 ] Rudy Bunel , Jingyue Lu , Ilker Turkaslan , P Kohli , P Torr , and P Mudigonda . Branch and bound for piecewise linear neural network verification . Journal of Machine Learning Research , 21 ( 2020 ) [ 2 ] Rudy R Bunel , Ilker Turkaslan , Philip Torr , Pushmeet Kohli , and Pawan K Mudigonda . A unified view of piecewise linear neural network verification . In Advances in Neural Information Processing Systems , pp . 4790\u20134799 , 2018 . [ 3 ] Jingyue Lu and M Pawan Kumar . Neural network branching for neural network verification . International Conference on Learning Representation ( ICLR ) , 2020 ."}], "0": {"review_id": "nVZtXBI6LNn-0", "review_text": "The authors demonstrate that using a modification of the LiRPA method during the branch-and-bound process for solving the neural network verification problem can lead to significant speed-ups . The experimental results are strong . The authors convincingly show that the their method outperforms the existing state-of-the-art method by Lu & Kumar ( 2020 ) on an experimental setup similar to that work . The application of LiRPA to branch-and-bound is straightforward ( since any incomplete verifier can be used ) , as is the use of gradient descent to improve the bound given by LiRPA ( a standard technique applied to improve the bounds of certain verifiers ) . Despite the fairly straightforward approach , the strength of the empirical results deserves attention . Overall , a solid contribution to the literature , and proof that research on incomplete verifiers leads to better complete verifiers . Some questions/requests : - The experimental setup details should be provided in the final version . - How dependent is the performance of LiRPA on GPUs ? For example , if we do a CPU-only comparison between the different methods , would other methods now outperform LiRPA ? And if so by how much ? What if we use multiple cores ? I would understand if a detailed comparison is too computationally intensive , but I would like some sense of this . Update after author response : I thank the authors of the paper for significantly improving the prose of the paper , and I agree that the changes make the paper more self-contained and approachable . I have kept my ratings as my score was for primarily for the strong experiment results ( and the score was also conditional on the paper being more polished ) . I am happy to support this paper for acceptance , but I am a little concerned about the degree of changes in the final version versus the initial submission , given the number of concerns the other reviewers had .", "rating": "7: Good paper, accept", "reply_text": "Thank you so much for correctly recognizing the main contributions of our paper . We greatly appreciate your encouraging comments , and we are glad to answer your questions below . 1.Additional experimental setup : we have updated our paper and provide experimental setup details in Appendix B . We will release our full source code once accepted . 2.As suggested by the reviewer , we provided results on using single and multi-core CPU based LiRPA computation for our algorithm in Appendix C. Existing baselines such as BaBSR require an LP solver , which is hard to accelerate on GPU or even on multi-core CPU . The basic computation of LiRPA is just matrix multiplication ( like NN training ) , so it naturally enjoys the parallelization in existing deep learning software libraries such as Pytorch . Figure 5 in Appendix shows that our LiRPA based method on a single CPU core is still competitive when compared to BaBSR+LP on a single CPU , and we enjoy a speedup on multi-core CPUs . Additionally , we have greatly improved our paper in our revision , added examples to illustrate our problem under study and make the formulation and correctness of our algorithm more clear . We hope the reviewer can discuss our main contributions with other reviewers during the second and third stages of discussion . Thank you ."}, "1": {"review_id": "nVZtXBI6LNn-1", "review_text": "The work proposes a new algorithm that can be used for the complete verification of neural networks ( NNs ) . Unfortunately , the authors do not define the verification problem they study : Based on the second paragraph of the introduction , one is given a neural network ( NN ) on the input , and the task is to determine whether the NN has a specific formally defined property - but which kind of properties are verified is never explained . Intuitively , one would expect verification to focus on determining whether the NN gives a `` correct '' output for certain inputs , but that does not really match the general description given in the paper , and I did not find a place where the verification problem is formalized further . Without knowing what `` verification '' means in the context of this paper , it 's difficult to follow the reasoning provided in the paper without having some rough idea of what kind of properties one wishes to verify ( for instance , the discussion about using LP bounds assumes that the property that is being verified can be expressed using LP ) . I believe this issue could have been avoided by formalizing the precise task of verification ( the verification problem ) . On that note , many parts of the paper seemed rather confusing and hard to follow , either due to inconsistencies or due to language issues . For instance , the sentence `` Input domain split is shown effective in verifying the properties with low input dimensions while performs as poorly as incomplete verifiers on higher dimension properties '' on page 2 seems to contradict the definitions given for complete and incomplete verification . How can a complete verifier perform `` as poorly as incomplete verifiers '' if , by the definition given in the paper , complete verifiers must always correctly determine whether the NN has the given `` property '' or not ? ( Section 2 : `` Complete verifiers guarantee to terminate either the property is proved or a violation is located . '' ) In terms of presentation , the submission contains an incredibly large number of minor language issues ( roughly 1 per 2-3 lines on average , ranging from minor article issues to malformed sentences ; see also the quote in the previous paragraph ) , and I strongly encourage authors to fix these as they have a rather disruptive effect when trying to read and understand the paper . A very small number of examples is provided below : Page 1 - '' cause the changes of NN predictions '' - > `` cause changes of NN predictions '' - '' Recently , a framework of Branch and Bound ( BaB ) ( Bunel et al. , 2018 ) is widely used for efficiently verifying NNs '' - can not combine `` recently '' and `` is '' . - '' adopts Linear Program ( LP ) bounding procedure '' - > `` adopts a Linear Program ( LP ) bounding procedure '' Page 2 - '' for construct LPs '' - > `` for constructing LPs '' The main contribution of the paper is the use of incomplete verifiers for complete verification , and the authors propose an algorithm for doing that using LIRPA bounds . However , I found no proof ( or anything resembling a proof ) showing that the resulting algorithm is correct , i.e. , that it performs complete verification for neural networks . In fact , the problem is not even properly and formally defined in the paper . Hence , regardless of the experimental results , I do not think that the submission is ready for publication at this stage . Post-Rebuttal Comment : I thank the authors for responding to my comments . The updated version fixes most of the criticisms raised in the review , and I have raised the score accordingly . My new score is `` 5 '' , partly because I believe that after performing such a large-scale and comprehensive overhaul of the paper ( which was certainly necessary ) , the paper should go through a full new reviewing process .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We greatly appreciate the very helpful comments from the reviewer . We have added necessary proofs , reorganized and revised many parts of our paper to make it easier to understand . We hope the reviewer can check out the updated version of our paper . We provide detailed answers below : 1 . Formalization and definition of the task of verification : To make the verification problem very clear , we now have added a definition at the beginning of introduction , and also give an example after the definition . In section 2 , we also give a more formal definition with discussions . We forgot to clearly define the properties under verification because the benchmarks used in our paper are fairly standard in this field , but now it has also been added everywhere , including in the experiment section . We hope it is now easier to understand the precise task of verification . 2.Proofs : We include more discussions on soundness and completeness in Section 2 and 3 . In Section 2 , the soundness of LiRPA has been proved in previous work ( Xu et al. , 2020 ) and we add a discussion on page 4 . In Section 3.2 , we have discussed the completeness in Theorem 3.1 and 3.2 . Theorem 3.1 shows that feasibility checking is important when using LiRPA as the bounding procedure in branch and bound , and Theorem 3.2 shows that with feasibility checking from LP , completeness is obtained , just like other works using branch and bound [ 1 ] [ 2 ] [ 3 ] . It is worth noting that many existing important works on complete verification such as [ 1 ] [ 2 ] [ 3 ] do not have a completeness proof , and it seems the completeness of the BaB process is implied , so most papers did not give proofs explicitly , and such a proof can look almost the same in every work . However , we do agree with the reviewer that we need to discuss more on the completeness of our algorithm and add explicit theorems for completeness , and we have done so in our revision . 3.Paper hard to follow , confusing sentences , language issues , typos We have fixed all confusing sentences and typos you mentioned and also greatly improve the writing and language of this paper . We added a clear definition of the verification problem , and also used more formal language to introduce the background and our algorithm in section 2 and 3 . We also added figures and examples to illustrate our ideas more clearly . We rewrote those sentences that were hard to understand . The overall flow and clarity of our paper have greatly improved now , and we hope you can take a look again . Conclusions : We would like to thank the reviewer again for your constructive feedback . We hope you can read our revised paper once again where we have made great effort to improve writing and readability . We hope our answers address all your concerns and hope you can re-evaluate our revised paper , because the main issue was mainly language and representation problems rather than technical ones . Please kindly let us know if you have any additional comments . Thank you . [ 1 ] Rudy Bunel , Jingyue Lu , Ilker Turkaslan , P Kohli , P Torr , and P Mudigonda . Branch and bound for piecewise linear neural network verification . Journal of Machine Learning Research , 21 ( 2020 ) [ 2 ] Jingyue Lu and M Pawan Kumar . Neural network branching for neural network verification . International Conference on Learning Representation ( ICLR ) , 2020 . [ 2 ] Rudy R Bunel , Ilker Turkaslan , Philip Torr , Pushmeet Kohli , and Pawan K Mudigonda . A unified view of piecewise linear neural network verification . In Advances in Neural Information Processing Systems , pp . 4790\u20134799 , 2018 ."}, "2": {"review_id": "nVZtXBI6LNn-2", "review_text": "# # # Summary This paper describes a branch-and-bound ( BaB ) process for neural network verification that uses linear relaxation based perturbation analysis ( LiRPA ) . It gives a way to tighten the bounds obtained via LiRPA . Overall , this results is a complete verification procedure , which is an order of magnitude faster than existing linear programming ( LP ) procedures . # # # Strengths The biggest strength of the paper is the impressive experimental results in section 5 : the method described in the paper is several times faster than previous methods . # # # Concerns My main concern is that the paper is very difficult to understand . It seems to require a lot of background knowledge about the problem and the related literature , which is not clearly provided in the paper . I had trouble understanding the problem , the setup , and the proposed algorithm . Another concern is that the paper claims that the proposed verifier is complete , but there is no proof of that . It does not seem like that 's something too difficult to prove ( given that BaB + LP is complete ) , but it should still be clearly stated . Finally , the claim that the proposed framework outperforms previous methods by `` at least 10X and up to 50X '' is unsupported . Based on the results in section 5 , a fairer statement regarding the speed would be `` at least 3X and up to 15X '' faster . # # # Reasons for score It is very difficult to judge this contribution , as the paper is hard to understand . The two main reasons for the score I give are 1 ) some of the claims of the paper are unsupported ( see above ) and 2 ) I believe this paper will have a much better chance of conveying the idea and making a contribution , if more background knowledge and intuition is provided throughout . # # # Suggestions for improvement that have not affected the score I gave to the paper One way to significantly improve the paper is to introduce more examples . An example consisting of a simple neural network to refer to throughout the explanation of LiRPA and BaB , and also in section 4 , would make the paper much easier to read . As a reader , I felt I could n't appreciate the related work section so early in the paper . I encourage you to either move it later in the paper , or even better : introduce more background/examples in the introduction , as well as the notion of completeness , so that the related work is easier to understand . Some typos : * Abstract : `` we demonstrate over a magnitude speedup ... '' - > `` we demonstrate speedup of an order of magnitude ... '' * First paragraph of section 2 : `` guarantee to terminate either ... '' - > `` guarantee to terminate when either ... '' * First paragraph of page 3 : `` used in state-of-the-art verifier ( Lu & Kumar , 2020 ) '' - > `` used in the state-of-the-art verifier by Lu & Kumar ( 2020 ) '' . * Second paragraph of page 3 : `` Our paper firstly leverage ... '' - > `` Our paper firstly leverages ... '' * First paragraph of section 3.1 : `` linear functions in the form of ... '' - > `` linear functions of the form ... '' * Start of section 4.1. : `` As we have introduced ... '' - > `` As we discussed ... '' * Mid page 5 : `` greatly limited ... '' - > `` greatly limits ... '' * Bottom of page 5 : `` We follow the most challenge experimental setup ... '' - > `` We follow the most challenging experimental setup ... '' * Mid page 6 : `` we quickly reaches ... '' - > `` we quickly reach ... '' * Mid page 6 : `` with only two hidden node ... '' - > `` with only two hidden nodes ... '' * Section 4.3 : `` Benefited from our design ... '' - > `` Benefitting from our design ... '' * Conclusion : capitalisation of the first sentence # # # Post rebuttal Thank you to the authors for their detailed response and their effort in improving the presentation of the paper . I was impressed with how much the paper improved in this second version . In particular , I very much appreciate that the introduction starts with a simple one sentence explanation of the problem of neural network verification . This can be further improved if it included ( almost ) no maths , which can be deferred to the Background section . The figures in the updated paper are very good and a huge improvement of presentation . Finally , the paper now includes two clearly stated theorems , which also make the presentation and contribution much clearer . I have increased the score I gave to the paper . Regardless of what the outcome for ICLR will be , I would like to encourage the authors to re-iterate on the presentation to really crystallize the problem , definitions and the suggested approach the paper is already so much better than the first version , and even just a little more work can make it even better .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We really thank the reviewer for all the suggestions on improving our paper , and help us find many typos . We address your concerns below : # # # Concern 1 : Hard to understand , require a lot of background knowledge Thank you for pointing out this concern . We have greatly improved our paper in terms of introducing sufficient background and motivations especially for researchers outside of this field . In our revised Introduction , we have followed your suggestions and made the definition of verification problem very clear ( in Introduction ) , and given a detailed walkthrough of background ( section 2 ) , and examples and figures to illustrate our main idea ( Figure 1 and 2 ) . # # # Concern 2 : Proof for completeness : We include more discussions on soundness and completeness in Section 2 and 3 . In Section 2 , the soundness of LiRPA has been proved in previous work ( Xu et al. , 2020 ) and we add a discussion on page 4 . In Section 3.2 , we have discussed the completeness in Theorem 3.1 and 3.2 . Theorem 3.1 shows that feasibility checking is important when using LiRPA as the bounding procedure in branch and bound , and Theorem 3.2 shows that with feasibility checking from LP , completeness is obtained , just like other works using branch and bound [ 1 ] [ 2 ] [ 3 ] . It is worth noting that many existing important works on complete verification such as [ 1 ] [ 2 ] [ 3 ] do not have a completeness proof , and it seems the completeness of the BaB process is implied , so most papers did not give proofs explicitly , and such a proof can look almost the same in every work . However , we do agree with the reviewer that we need to discuss more on the completeness of our algorithm and add explicit theorems for completeness , and we have done so in our revision . # # # Concern 3 : Speedup claim : Thank you for pointing this out ! We forgot to update these numbers in introduction when our experiment results were updated . We have fixed the speedup claims in our paper . Our speedup is 30X compared to basic BaB baselines , and up to 5X compared to the state-of-the-art verifier . In our revision , we also added a very recent baseline ( proximal BaBSR ) and our method still performs best . # # # Suggestion 1 : Introduce more examples Following the reviewer \u2019 s suggestion , we have added Figure 2 to explain LiRPA bounds and the BaB procedure . Additionally , we also introduce more examples in the text : for example , in the introduction , we give the definition of the verification problem and show the example after the definition . We hope these updates will make our paper much easier to understand . # # # Suggestion 2 : Related work section As suggested by the reviewer , we have moved the related work section to the end of the paper , and enhanced the background section . We introduce the basic definitions of verification problems and the notation of completeness as early as in Introduction , and also give more former notations in section 2 . # # # Typos : Thank you for pointing out these typos . We have fixed them and also greatly improved writing and clarity in our new revision . We added the definition of verification , completeness and incompleteness in Introduction . # # # Conclusion : We have significantly improved the writing of our paper and provide sufficient background and intuitions in our updated paper . We have also formally discussed and proven the completeness of our proposed algorithm . Since the most concerns on our paper are about writing and representation , we hope these changes address the concerns of the reviewer , and hope the reviewer can reconsider the score based on our revision . Feel free to let us know if you have any further questions regarding our revision . Thank you . [ 1 ] Rudy Bunel , Jingyue Lu , Ilker Turkaslan , P Kohli , P Torr , and P Mudigonda . Branch and bound for piecewise linear neural network verification . Journal of Machine Learning Research , 21 ( 2020 ) [ 2 ] Rudy R Bunel , Ilker Turkaslan , Philip Torr , Pushmeet Kohli , and Pawan K Mudigonda . A unified view of piecewise linear neural network verification . In Advances in Neural Information Processing Systems , pp . 4790\u20134799 , 2018 . [ 3 ] Jingyue Lu and M Pawan Kumar . Neural network branching for neural network verification . International Conference on Learning Representation ( ICLR ) , 2020 ."}, "3": {"review_id": "nVZtXBI6LNn-3", "review_text": "The paper focuses on verifying simple properties of neural networks on accelerator hardware . Instead of using linear programming , Lirpa is considered as an alternative and minimal amounts of LP is added around to allow to use the same class of properties . In the considered examples the approach becomes much faster than the LP approach . My main problem with the paper , is that is claims a complete verification procedure without proper proofs . Yes , the new algorithm is presented and some general discussion of things that are done with it and how they work is provided . However , to claim complete verification a general soundness of the procedure should be proved . In particular I would like a theorem for the correctness of each of the various components and a combined theorem for the whole procedure . On the other hand , the experiments show that the approach is fast and as such makes it more feasible for verification and the paper mostly reads well . An interesting alternative to discuss in related work , could be proof assistants . See for example the work of Bentkamp , Blanchette JAR 2019 . Using proof assistants based on more complex logics , one can verify properties much more efficiently . The authors only mention Katz 's work on SMT , but if you consider higher order logic and the logics of interactive theorem provers , the `` NP-hard properties '' can be checked without considering all the cases . Minor : Conclusion starts with lowercase `` we '' . Based on the problems found by the other reviews and having read the rebuttals I have modified my score .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the helpful reviews from you , and we would like to address your concerns below : 1 . Proofs : In our revised paper , we have greatly improved every section . In section 2 , we discussed the soundness of LiRPA method . In section 3 , we gave two Theorems to show the completeness of our full branch and bound algorithm based on the property of LiRPA method . In fact , the proof for correctness of branch and bound is relatively straightforward , and many previous works in complete verification [ 1 ] [ 2 ] [ 3 ] did not give an explicit proof and just imply it is correct ( the proof would be very similar in every work ) . However , we have added proofs in our paper to ensure clarity . Specifically , in Section 3.2 , we have discussed the completeness in Theorem 3.1 and 3.2 . Theorem 3.1 shows that feasibility checking is important when using LiRPA as the bounding procedure in branch and bound , and Theorem 3.2 shows that with feasibility checking from LP , completeness is obtained , just like other works using branch and bound [ 1 ] [ 2 ] [ 3 ] . 2.Additional reference : Thank you for providing this insightful connection ! We have cited [ Bentkamp , Blanchette JAR 2019 ] on proof assistant based higher order logic provers . Lastly , we hope the reviewer can check out our revised paper . We gave more clear formal definitions of the verification problem , as well as more intuition , background and examples . We also include detailed discussions on soundness and completeness as well as proofs for correctness . We hope the reviewer can re-evaluate our paper based on our revision and update the rating . Thank you . [ 1 ] Rudy Bunel , Jingyue Lu , Ilker Turkaslan , P Kohli , P Torr , and P Mudigonda . Branch and bound for piecewise linear neural network verification . Journal of Machine Learning Research , 21 ( 2020 ) [ 2 ] Rudy R Bunel , Ilker Turkaslan , Philip Torr , Pushmeet Kohli , and Pawan K Mudigonda . A unified view of piecewise linear neural network verification . In Advances in Neural Information Processing Systems , pp . 4790\u20134799 , 2018 . [ 3 ] Jingyue Lu and M Pawan Kumar . Neural network branching for neural network verification . International Conference on Learning Representation ( ICLR ) , 2020 ."}}