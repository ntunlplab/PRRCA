{"year": "2020", "forum": "Hkxzx0NtDB", "title": "Your classifier is secretly an energy based model and you should treat it like one", "decision": "Accept (Talk)", "meta_review": "This paper uses energy based model to interpret standard discriminative classifier and demonstrates that energy based model training of the joint distribution improves calibration, robustness, and out-of-distribution detection while generating samples with better quality than GAN-based approaches. The reviewers are very excited about this work, and the energy-based perspective of generative and discriminative learning. There is a unanimous agreement to strongly accept this paper after author response.", "reviews": [{"review_id": "Hkxzx0NtDB-0", "review_text": "This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects. The idea of reinterpreting the traditional classifier is very interesting, and the experiments show some good results of the proposed method. Here are my main concerns of the current paper: 1. The training procedure seems to be very sensitive, and the SGLD may take a long time at each iteration to converge. This may be a big limitation of the proposed method. 2. According to equation (8), the proposed method is having a trade-off between classification and generation, and this seems to be the key to improve the performance of the model in generation by sacrificing some classification accuracy. I think author should emphasize this instead of energy based model. 3. The presentation is not very clear in section 5. What is the task of calibration, and what is the definition of ECE? 4. The robustness guarantee seems too good to be true. Although the authors claim that they allow the attacker to have access to the gradient of SGLD, the SGLD will add noise during the forward process, this will obfuscate the gradient. In this sense, I don\u2019t think the proposed method will have the strong robustness as they claimed. ---------------- Post-Rebuttal Comments: Thanks for addressing my concerns. Although I think the proposed method is not comprehensive to check obfuscated gradients, I do think the current version is a good fit for ICLR, and I decide to increase my score. ", "rating": "6: Weak Accept", "reply_text": "( PART 1 OF 2 ) We thank you for your time reviewing our work . We will address your concerns in order and we have updated the manuscript accordingly . We hope these changes will encourage you to change your score : 1 ) Your concerns on the sensitivity and speed of SGLD training of EBMs . Regarding your concern about sensitivity : While we agree that SGLD training of EBMs can be sensitive to hyper-parameter settings , we note that throughout our work we used the exact same hyper-parameters for every model and every dataset . We also found these settings transferred well to datasets such as MNIST which we did not present in our paper . Further , we found these same settings worked well across a variety of model architectures such as MLPs , non-resnet convnets , and resnets . This was stated in Appendix G.2 of our paper , but we have added it to the main body of our paper for clarity . This hyper-parameter transferability behavior has also been reported in prior work on EBM training such as [ 1 , 2 ] . Regarding your other concern about the convergence time : In our work we put a great deal of focus into being able to train as quickly as possible with minimal hardware requirements . We have been able to train EBMs with far fewer SGLD steps per training iteration than in previous work and found that at these settings stable training can still take place . All of our models were trained on a single GPU and each training run took $ < 36 $ hours . While this is slower than training a standard classifier on these datasets , our training speed falls comfortably in the range of other popular classes of generative models such as flows [ 3 ] and GANs [ 4 ] . We admit that we did not put enough emphasis on these two facts in our original draft and we have added this information in section 5 . We hope this clarifies your concerns regarding training sensitivity and run-time . Overall we feel that training and sampling are the biggest challenges when working with EBMs . Developing improved methods for this is important further work but we also feel it is outside of the scope of our current work . The main point of our work was to demonstrate that despite the challenges which currently exist in training EBMs , they can be used to achieve a very interesting and diverse set of results on problems which other classes of generative models have not been able to achieve at this scale . These results provide a strong motivation for more work in the space of EBM training methods . 2 ) We are slightly unsure of what you mean with this point . We train our model using the factorized likelihood of Equation ( 8 ) . As we explain in the following sentence , this was done to reduce bias in our training procedure , not because there is a need to weight these terms differently . We are aware that this is common practice in other hybrid-models [ 4 , 5 ] , but we do not do this in our model . Each term in this objective is weighted equally . While different results could possibly be achieved if we did weight each term in ( 8 ) we feel that our model 's ability to weight the terms equally and still perform well at both tasks is actually a benefit of our approach over competing methods . We hope this clarifies your concerns . ( CONTINUED BELOW ) [ 1 ] `` Implicit Generation and Generalization in Energy-Based Models '' Yilun Du , Igor Mordatch . https : //arxiv.org/abs/1903.08689 [ 2 ] `` On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models '' Erik Nijkamp , Mitch Hill , Tian Han , Song-Chun Zhu , Ying Nian Wu . https : //arxiv.org/abs/1903.12370 [ 3 ] `` Large Scale GAN Training for High Fidelity Natural Image Synthesis '' Andrew Brock , Jess Donahue , Karen Simonyan . https : //arxiv.org/abs/1809.11096 [ 4 ] `` Glow : Generative Flow with Invertible 1x1 Convolutions '' Diederik P. Kingma , Prafulla Dhariwal . https : //arxiv.org/abs/1807.03039 [ 5 ] `` Residual Flows for Invertible Generative Modeling '' Ricky T. Q. Chen , Jens Behrmann , David Duvenaud , J\u00f6rn-Henrik Jacobsen . https : //arxiv.org/abs/1906.02735 [ 6 ] `` On Calibration of Modern Neural Networks '' Chuan Guo , Geoff Pleiss , Yu Sun , Kilian Q. Weinberger . https : //arxiv.org/abs/1706.04599"}, {"review_id": "Hkxzx0NtDB-1", "review_text": "This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold-out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc. While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). The advantage is an additional degree of freedom in the scale of the logit vector, which is would have been otherwise normalized by the softmax layer and now can now model the data distribution. The downside is the loss in ease of training. Whereas (discriminative) deep networks can be easily trained by gradient descent on a cross-entropy objective, the partition function in the energy model makes this un tractable. This is addressed through sampling, similar to (Welling & Teh, 2011). One of the biggest achievements reported by the authors is that the performance on discriminative tasks is not hurt (much) by adding the generative model. There is only a 3 point gap between Wide-ResNet and the proposed model (92.9% vs. 95.8%) \u2026 but on what dataset? 3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported. My guess is that this is a mean or mixture, since GEM performances of 96.7% and 72.2% are reported for SVHN and CIFAR10, respectively, but this should be made clearer. On out of distribution detection, could the authors comment on the histograms in table 2, in particular the difference between the new measure (AM JEM) compared to JEM log p(x) on CelebA? The proposed measure does not seem to fare well here. Although the method does not outperform the gold standard of adversarial training, I found the models robustness to adversarial examples quite appealing, given that it was not trained for this objective (which also means that it does not require an adaptation to a norm). I was very impressed by Figure 6 showing distal adversarial initialized from random images, showing pretty clear images of the modelled class. The modelled variations require more investigation to verify whether we have a collapse for each class, but the results look very promising. The paper is well written and easy to understand. A couple of details on the training procedure are missing in the experimental part. It is stated that, both, p(y|x) and the generative part p(x), are optimized, but how are these exactly integrated? Given the difficult in training this model reported in the paper, this seems to be particularly important. I also appreciated the description of the limitations of the algorithm, and the details in the appendix (ICLR should go back to unlimited paper lengths, btw.). More information on complexity (training times etc.) should also be helpful. ", "rating": "8: Accept", "reply_text": "We thank you for your time reviewing our work . You are correct , the results presented in table 1 are CIFAR10 . We forgot to add this to the caption . We have updated the caption to fix this . Results on other datasets can be found in the text of Section 5.1 . Regarding the performance of our approximate-mass measure for OOD detection on CelebA , we refer you to Table 3 , bottom row , right-most column . This metric gets AUROC = .79 on this dataset , higher actually than unnormalized logp ( x ) which gets AUROC = .75 . The metric may appear to perform worse than the likelihood in the histogram but the low-valued tails are larger and thus the AUROC is higher . Regarding the training procedure , we optimize a training objective which is log p ( y|x ) + log p ( x ) . This is equivalent to optimizing log p ( x , y ) . These two terms are combined by adding their gradients exactly . This is equivalent to an equal weighting of the two terms . We choose this way to factor the learning objective since p ( y|x ) is a normalized distribution and we can train with maximum likelihood exactly , avoiding a biased and tricky gradient estimation problem . We are aware that other hybrid models typically downweight the log p ( x ) term . We do not do that here . Regarding the training time and hardware requirements , we note that all models were trained on a single GPU in approximately 36 hours . We have added a few sentences to section 5 in the text to clarify this ."}, {"review_id": "Hkxzx0NtDB-2", "review_text": "The paper uses energy-based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out-of-distribution detection, etc. Although there has been previous work attempting to bridge discriminative classifiers with generative modeling, this work proves to be competitive with both specialized models on discriminative/generative tasks as well as in many downstream tasks such as out-of-distribution detection, calibration, and adversarial robustness. The paper provides a clear exposition of the method, succeeds to discuss related work it bases on, conducts a thorough experimental study providing convincing explanations for results and does not hide the limitations of the work (high computational requirements, optimization difficulties connected with training energy-based model and the method used, limited approximation of the true energy). Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative - generative setting. However, there are points I would like the paper to address for better exposition. 1. It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood. 2. On CIFAR100 the accuracy drop from the reference value is larger than for datasets with 10 classes, could it be due the logits dimension is higher and challenges optimization? 3. It would also be helpful to clarify whether application of the proposed method is primarily restricted by the computational complexity or is there any property inherent to energy-based models that makes treating high-dimensional data challenging? Minor remark - Although the paper doesn't state on which dataset results shown in Table 1 were obtained, I suspect its CIFAR10, please specify this.", "rating": "8: Accept", "reply_text": "We thank you for your time reviewing our work . We will address your concerns in order : 1 ) Visual quality is difficult to quantify . Of the known metrics like IS and FID , using samples that have higher p ( y|x ) values results in higher scores , but not necessary if we use samples with higher p ( x ) . However , this is likely because of the downfalls of the evaluation metrics themselves rather than reflecting true sample quality . Based on our analysis of CIFAR10 ( below ) , we find -Our p ( x ) model assigns values that cluster around different means for different classes . The class automobiles has the highest p ( x ) . Of all generated samples , all top 100 samples are of this class . -Given the class , the samples that have higher p ( x ) values all have white background and centered object , and lower p ( x ) samples have colorful ( e.g. , forest-like ) background . -Of all samples , higher p ( y|x ) values means clearly centered objects , and lower p ( y|x ) otherwise . We completely agree with you that adding these analyese will strengthen the paper , and we have added this discussion with their corresponding images in the revised appendix . 2 ) On CIFAR10 we see our accuracy drop from 95.2 % to 92.9 % and on CIFAR100 we see accuracy drop from 74.2 % to 72.2 % . This is a 2-3 % drop on both datasets . These numbers are from the exact same model with and without JEM training . In these settings the decrease in accuracy is relatively consistent . Perhaps you are referring to the accuracy of our JEM models compared to state-of-the-art discriminative classifiers on these datasets ? Yes , in this setting we have a 2-3 % drop from the best wide-resnet classifier with all forms of regularization added . On CIFAR100 we have approximately a 8 % drop compared to the best wide-resnet . Our best guess to explain this phenomenon is that competitive accuracy on CIFAR100 is much lower than competitive accuracy on CIFAR10 meaning that much more overfitting is happening on CIFAR100 than CIFAR10 ( since all models achieve a training accuracy of 100 % at the end of training ) . In our JEM models we remove two important forms of regularization , batch norm and dropout , which we found to have negligible impact on CIFAR10 but less negligible impact on CIFAR100 . This is backed up by the fact that our baseline classifier with these regularizers removed achieves 74.4 % accuracy , closer to that of our JEM model . We feel that the removal of these regularizers provides an explanation for the decrease in relative performance . 3 ) This is an interesting point . We are very excited about the future of EBMs and we are generally of the belief that the application of EBMs is currently limited by the fragility of the tools we use to train them . So yes , we do believe if one had access to considerable computational resources then one should be able scale these methods presented to larger datasets , but we do believe there would be considerable engineering cost in doing so . We feel the most useful next steps to work on in the EBM-space are more stable and efficient training objectives which will increase the scale of problems to which we can apply these methods . Minor Remark ) Yes you are correct that table presents CIFAR10 results and we did indeed forget to label it as such . This has been changed in our revised version ."}], "0": {"review_id": "Hkxzx0NtDB-0", "review_text": "This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects. The idea of reinterpreting the traditional classifier is very interesting, and the experiments show some good results of the proposed method. Here are my main concerns of the current paper: 1. The training procedure seems to be very sensitive, and the SGLD may take a long time at each iteration to converge. This may be a big limitation of the proposed method. 2. According to equation (8), the proposed method is having a trade-off between classification and generation, and this seems to be the key to improve the performance of the model in generation by sacrificing some classification accuracy. I think author should emphasize this instead of energy based model. 3. The presentation is not very clear in section 5. What is the task of calibration, and what is the definition of ECE? 4. The robustness guarantee seems too good to be true. Although the authors claim that they allow the attacker to have access to the gradient of SGLD, the SGLD will add noise during the forward process, this will obfuscate the gradient. In this sense, I don\u2019t think the proposed method will have the strong robustness as they claimed. ---------------- Post-Rebuttal Comments: Thanks for addressing my concerns. Although I think the proposed method is not comprehensive to check obfuscated gradients, I do think the current version is a good fit for ICLR, and I decide to increase my score. ", "rating": "6: Weak Accept", "reply_text": "( PART 1 OF 2 ) We thank you for your time reviewing our work . We will address your concerns in order and we have updated the manuscript accordingly . We hope these changes will encourage you to change your score : 1 ) Your concerns on the sensitivity and speed of SGLD training of EBMs . Regarding your concern about sensitivity : While we agree that SGLD training of EBMs can be sensitive to hyper-parameter settings , we note that throughout our work we used the exact same hyper-parameters for every model and every dataset . We also found these settings transferred well to datasets such as MNIST which we did not present in our paper . Further , we found these same settings worked well across a variety of model architectures such as MLPs , non-resnet convnets , and resnets . This was stated in Appendix G.2 of our paper , but we have added it to the main body of our paper for clarity . This hyper-parameter transferability behavior has also been reported in prior work on EBM training such as [ 1 , 2 ] . Regarding your other concern about the convergence time : In our work we put a great deal of focus into being able to train as quickly as possible with minimal hardware requirements . We have been able to train EBMs with far fewer SGLD steps per training iteration than in previous work and found that at these settings stable training can still take place . All of our models were trained on a single GPU and each training run took $ < 36 $ hours . While this is slower than training a standard classifier on these datasets , our training speed falls comfortably in the range of other popular classes of generative models such as flows [ 3 ] and GANs [ 4 ] . We admit that we did not put enough emphasis on these two facts in our original draft and we have added this information in section 5 . We hope this clarifies your concerns regarding training sensitivity and run-time . Overall we feel that training and sampling are the biggest challenges when working with EBMs . Developing improved methods for this is important further work but we also feel it is outside of the scope of our current work . The main point of our work was to demonstrate that despite the challenges which currently exist in training EBMs , they can be used to achieve a very interesting and diverse set of results on problems which other classes of generative models have not been able to achieve at this scale . These results provide a strong motivation for more work in the space of EBM training methods . 2 ) We are slightly unsure of what you mean with this point . We train our model using the factorized likelihood of Equation ( 8 ) . As we explain in the following sentence , this was done to reduce bias in our training procedure , not because there is a need to weight these terms differently . We are aware that this is common practice in other hybrid-models [ 4 , 5 ] , but we do not do this in our model . Each term in this objective is weighted equally . While different results could possibly be achieved if we did weight each term in ( 8 ) we feel that our model 's ability to weight the terms equally and still perform well at both tasks is actually a benefit of our approach over competing methods . We hope this clarifies your concerns . ( CONTINUED BELOW ) [ 1 ] `` Implicit Generation and Generalization in Energy-Based Models '' Yilun Du , Igor Mordatch . https : //arxiv.org/abs/1903.08689 [ 2 ] `` On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models '' Erik Nijkamp , Mitch Hill , Tian Han , Song-Chun Zhu , Ying Nian Wu . https : //arxiv.org/abs/1903.12370 [ 3 ] `` Large Scale GAN Training for High Fidelity Natural Image Synthesis '' Andrew Brock , Jess Donahue , Karen Simonyan . https : //arxiv.org/abs/1809.11096 [ 4 ] `` Glow : Generative Flow with Invertible 1x1 Convolutions '' Diederik P. Kingma , Prafulla Dhariwal . https : //arxiv.org/abs/1807.03039 [ 5 ] `` Residual Flows for Invertible Generative Modeling '' Ricky T. Q. Chen , Jens Behrmann , David Duvenaud , J\u00f6rn-Henrik Jacobsen . https : //arxiv.org/abs/1906.02735 [ 6 ] `` On Calibration of Modern Neural Networks '' Chuan Guo , Geoff Pleiss , Yu Sun , Kilian Q. Weinberger . https : //arxiv.org/abs/1706.04599"}, "1": {"review_id": "Hkxzx0NtDB-1", "review_text": "This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold-out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc. While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). The advantage is an additional degree of freedom in the scale of the logit vector, which is would have been otherwise normalized by the softmax layer and now can now model the data distribution. The downside is the loss in ease of training. Whereas (discriminative) deep networks can be easily trained by gradient descent on a cross-entropy objective, the partition function in the energy model makes this un tractable. This is addressed through sampling, similar to (Welling & Teh, 2011). One of the biggest achievements reported by the authors is that the performance on discriminative tasks is not hurt (much) by adding the generative model. There is only a 3 point gap between Wide-ResNet and the proposed model (92.9% vs. 95.8%) \u2026 but on what dataset? 3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported. My guess is that this is a mean or mixture, since GEM performances of 96.7% and 72.2% are reported for SVHN and CIFAR10, respectively, but this should be made clearer. On out of distribution detection, could the authors comment on the histograms in table 2, in particular the difference between the new measure (AM JEM) compared to JEM log p(x) on CelebA? The proposed measure does not seem to fare well here. Although the method does not outperform the gold standard of adversarial training, I found the models robustness to adversarial examples quite appealing, given that it was not trained for this objective (which also means that it does not require an adaptation to a norm). I was very impressed by Figure 6 showing distal adversarial initialized from random images, showing pretty clear images of the modelled class. The modelled variations require more investigation to verify whether we have a collapse for each class, but the results look very promising. The paper is well written and easy to understand. A couple of details on the training procedure are missing in the experimental part. It is stated that, both, p(y|x) and the generative part p(x), are optimized, but how are these exactly integrated? Given the difficult in training this model reported in the paper, this seems to be particularly important. I also appreciated the description of the limitations of the algorithm, and the details in the appendix (ICLR should go back to unlimited paper lengths, btw.). More information on complexity (training times etc.) should also be helpful. ", "rating": "8: Accept", "reply_text": "We thank you for your time reviewing our work . You are correct , the results presented in table 1 are CIFAR10 . We forgot to add this to the caption . We have updated the caption to fix this . Results on other datasets can be found in the text of Section 5.1 . Regarding the performance of our approximate-mass measure for OOD detection on CelebA , we refer you to Table 3 , bottom row , right-most column . This metric gets AUROC = .79 on this dataset , higher actually than unnormalized logp ( x ) which gets AUROC = .75 . The metric may appear to perform worse than the likelihood in the histogram but the low-valued tails are larger and thus the AUROC is higher . Regarding the training procedure , we optimize a training objective which is log p ( y|x ) + log p ( x ) . This is equivalent to optimizing log p ( x , y ) . These two terms are combined by adding their gradients exactly . This is equivalent to an equal weighting of the two terms . We choose this way to factor the learning objective since p ( y|x ) is a normalized distribution and we can train with maximum likelihood exactly , avoiding a biased and tricky gradient estimation problem . We are aware that other hybrid models typically downweight the log p ( x ) term . We do not do that here . Regarding the training time and hardware requirements , we note that all models were trained on a single GPU in approximately 36 hours . We have added a few sentences to section 5 in the text to clarify this ."}, "2": {"review_id": "Hkxzx0NtDB-2", "review_text": "The paper uses energy-based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out-of-distribution detection, etc. Although there has been previous work attempting to bridge discriminative classifiers with generative modeling, this work proves to be competitive with both specialized models on discriminative/generative tasks as well as in many downstream tasks such as out-of-distribution detection, calibration, and adversarial robustness. The paper provides a clear exposition of the method, succeeds to discuss related work it bases on, conducts a thorough experimental study providing convincing explanations for results and does not hide the limitations of the work (high computational requirements, optimization difficulties connected with training energy-based model and the method used, limited approximation of the true energy). Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative - generative setting. However, there are points I would like the paper to address for better exposition. 1. It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood. 2. On CIFAR100 the accuracy drop from the reference value is larger than for datasets with 10 classes, could it be due the logits dimension is higher and challenges optimization? 3. It would also be helpful to clarify whether application of the proposed method is primarily restricted by the computational complexity or is there any property inherent to energy-based models that makes treating high-dimensional data challenging? Minor remark - Although the paper doesn't state on which dataset results shown in Table 1 were obtained, I suspect its CIFAR10, please specify this.", "rating": "8: Accept", "reply_text": "We thank you for your time reviewing our work . We will address your concerns in order : 1 ) Visual quality is difficult to quantify . Of the known metrics like IS and FID , using samples that have higher p ( y|x ) values results in higher scores , but not necessary if we use samples with higher p ( x ) . However , this is likely because of the downfalls of the evaluation metrics themselves rather than reflecting true sample quality . Based on our analysis of CIFAR10 ( below ) , we find -Our p ( x ) model assigns values that cluster around different means for different classes . The class automobiles has the highest p ( x ) . Of all generated samples , all top 100 samples are of this class . -Given the class , the samples that have higher p ( x ) values all have white background and centered object , and lower p ( x ) samples have colorful ( e.g. , forest-like ) background . -Of all samples , higher p ( y|x ) values means clearly centered objects , and lower p ( y|x ) otherwise . We completely agree with you that adding these analyese will strengthen the paper , and we have added this discussion with their corresponding images in the revised appendix . 2 ) On CIFAR10 we see our accuracy drop from 95.2 % to 92.9 % and on CIFAR100 we see accuracy drop from 74.2 % to 72.2 % . This is a 2-3 % drop on both datasets . These numbers are from the exact same model with and without JEM training . In these settings the decrease in accuracy is relatively consistent . Perhaps you are referring to the accuracy of our JEM models compared to state-of-the-art discriminative classifiers on these datasets ? Yes , in this setting we have a 2-3 % drop from the best wide-resnet classifier with all forms of regularization added . On CIFAR100 we have approximately a 8 % drop compared to the best wide-resnet . Our best guess to explain this phenomenon is that competitive accuracy on CIFAR100 is much lower than competitive accuracy on CIFAR10 meaning that much more overfitting is happening on CIFAR100 than CIFAR10 ( since all models achieve a training accuracy of 100 % at the end of training ) . In our JEM models we remove two important forms of regularization , batch norm and dropout , which we found to have negligible impact on CIFAR10 but less negligible impact on CIFAR100 . This is backed up by the fact that our baseline classifier with these regularizers removed achieves 74.4 % accuracy , closer to that of our JEM model . We feel that the removal of these regularizers provides an explanation for the decrease in relative performance . 3 ) This is an interesting point . We are very excited about the future of EBMs and we are generally of the belief that the application of EBMs is currently limited by the fragility of the tools we use to train them . So yes , we do believe if one had access to considerable computational resources then one should be able scale these methods presented to larger datasets , but we do believe there would be considerable engineering cost in doing so . We feel the most useful next steps to work on in the EBM-space are more stable and efficient training objectives which will increase the scale of problems to which we can apply these methods . Minor Remark ) Yes you are correct that table presents CIFAR10 results and we did indeed forget to label it as such . This has been changed in our revised version ."}}