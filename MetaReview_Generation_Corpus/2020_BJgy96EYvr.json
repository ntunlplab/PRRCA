{"year": "2020", "forum": "BJgy96EYvr", "title": "Influence-Based Multi-Agent Exploration", "decision": "Accept (Spotlight)", "meta_review": "The paper presents a new take on exploration in multi-agent reinforcement learning settings, and presents two approaches, one motivated by information theoretic, the other by decision theoretic influence on other agents. Reviewers consider the proposed approach \"pretty elegant, and in a sense seem fundamental\", the experimental section \"thorough\", and expect the work to \"encourage future work to explore more problems in this area\". Several questions were raised, especially regarding related work, comparison to single agent exploration approaches, and several clarifying questions. These were largely addressed by the authors, resulting in a strong submission with valuable contributions.", "reviews": [{"review_id": "BJgy96EYvr-0", "review_text": "Update: I thank the authors for their response and I will maintain my score, my main hesitation being the overall clarity and readability of the paper. Summary: This paper proposes the use of two intrinsic rewards for exploration in MARL settings. The first one is an information-theoretic influence (EITI) bonus and a decision-theoretic influence (EDTI) bonus. EITI uses mutual information to capture the influence of one agent on the transition dynamics of others, while EDTI uses an intrinsic reward called Value of Interaction (VoI) to quantify the influence of one agent\u2019s behavior on expected returns of other agents. Main Comments: Overall, I think this paper would be a good contribution for ICLR 2020 and I lean towards accepting it. The experimental section is thorough, the authors include relevant ablations, baselines and popular algorithms used in MARL settings. The use of the decision-theoretic influence is novel as far as I can tell and it also seems to be quite effective on the tasks used for evaluation. Although the method uses a series of approximations and assumptions, I believe most of them are clearly stated and fairly well-motivated (plus they are not very far from those of other recent work in the deep MARL literature). I also appreciated the fact that the authors explicitly derived the main mathematical results used in the paper. I only have some minor comments and questions regarding some assumptions and notation. I also encourage the authors to proof-read the paper as some parts of it are a bit hard to follow. I would very much like to see a more an edited version of this paper with more precise language. Can you discuss in more detail the difference between EITI and the intrinsic reward based on social influence used in Jacques et al. (2018)? They seem to be quite similar conceptually and the related work part related to this is rather vague. Please clarify the distinction. Minor Questions / Comments: There are a few typos throughout the paper: 1. On page 3 after equation (3), I believe part of the sentence that should describe the I term in the equation is missing. 2. The phrase right after equation (16) which defines the EDTI reward does not seems to not match the above expression. Can you please explain why the transition would be conditioned on the influence term? While reviewing, I\u2019ve been assuming this was just a mistake in writing but please double check and clarify. 3. On page 4, after equation (8), you refer to a_1, a_1 and s_2\u2019 which do not appear in the above equation. Can you please use the same notation or motivate your choice for referring to those variables instead? ", "rating": "6: Weak Accept", "reply_text": "Thanks for your careful reading and thoughtful comments . Here we provide some feedback to your comments . Q1 : A more edited version of this paper with more precise language is expected . A1 : We have proofread the manuscript and made modifications to improve our presentation . We would appreciate it if you have more comments about the readability . Q2 : Difference between EITI and the intrinsic reward based on social influence [ Jaques et al. , ICML 2019 ] . A2 : The differences between EITI and social influence lie in both definition and optimization . ( 1 ) Definition . Social influence is defined to be the influence of one agent on the policies of other agents , while EITI measures the influence of one agent on the transition dynamics of other agents . Accompanying this distinction , EITI considers both states and actions to measure the influence while social influence quantifies the influence of actions without considering state information , which is critical for exploration ; ( 2 ) Optimization . In EITI , to explicitly maximize mutual information , we add it as a regularizer to the learning objective and derive the gradients in the policy gradient framework . In contrast , social influence reward is added to the immediate environmental reward , and is used to train the RL algorithms . We have also updated the relating discussions in Sec.5 of the new version of our manuscript . Q3 : About the typos . A3 : Thank you for pointing out these typos . We have corrected them in the updated version . [ Jaques et al. , ICML 2019 ] Jaques , N. , Lazaridou , A. , Hughes , E. , Gulcehre , C. , Ortega , P. , Strouse , D. , Leibo , J.Z . and De Freitas , N. , 2019 , May . Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning . In International Conference on Machine Learning ( pp.3040-3049 ) ."}, {"review_id": "BJgy96EYvr-1", "review_text": "This paper studies the problem of designing effective exploration strategies in multi-agent domains. The key idea is to define one agent's exploration in terms of its interactions with other agents. This leads to two auxiliary exploration objectives, which measure how one agent's actions affect the dynamics and value of another agent's actions. The paper does an admirable job comparing the proposed method against a number of baselines, where the proposed method performs significantly better. Visualizations and ablation experiments nicely illustrate the contributions of various components of the method. I am leaning towards accepting the paper. To the best of my knowledge, the broad idea of applying information theory to multi-agent exploration, in addition to the specific instantiation described in the paper, is novel. I expect that this paper will encourage future work to explore more problems in this area. The experiments are quite thorough. My main reservation is a lack of comparisons to single agent exploration methods. As noted in Section 3, we can view multi-agent domains as just a special type of single agent domain. How would curiosity-based exploration, such as [Burda 2018, Pathak 17], or mutual information-based exploration, such as [Gregor 16, Eysenbach 18, Achaim 18], compare to the proposed method? I have a few reservations about the clarity of presentation, but I think those are easily addressed. My remaining concern is that the results are on somewhat toy tasks, but I think that is par for this area of research. Overall, I would strongly argue for accepting this paper if comparisons to single-agent exploration methods were added. I would consider decreasing my review if another reviewer found quite similar prior work, or if significant bugs were found in the mathematical derivation (I have not carefully checked all the proofs in the appendix.). Minor comments * \"transition-dependent\" -- what does this mean? * \"while tend\" -- missing a subject * \"struggle in many real-world scenarios with sparse rewards\" -- please add a citation * \"intrinsic value function of agent i, I_{-i|i}^\\pi is \\beta > 0 is a weighting\" -- I think part of this sentence was accidentally deleted. * Eq 5: What is the difference between I and MI? * \"We call \u2026\" -- What is the a_2^V0I^\\pi_{-i|i} term? * Nitpick: Use \" for the start of quotes * Appendix B1: How is Eq 22 obtained from Eq 21? -------------------UPDATE AFTER AUTHOR RESPONSE--------------------- The authors have done a great job address my two concerns (similarity to prior work and empirical comparisons with single-agent exploration). I therefore increase my vote to \"accept.\"", "rating": "8: Accept", "reply_text": "Thanks for your constructive comments . We have added experiments for comparing our methods to single-agent exploration methods and corresponding analyses in the updated version . Here we provide explanations to your questions . Q1 : Comparison to single-agent exploration methods . A1 : As requested , we compare our methods with RND [ Burda et al. , ICLR 2019a ] and EMI ( exploration with mutual information ) [ Kim et al. , ICML 2019 ] . We choose these two methods because they are among the most cutting-edge curiosity-driven and mutual information-based exploration algorithms , respectively . So far , we have carried out a modest grid search over some hyperparameters for both RND ( coefficients of intrinsic and extrinsic reward ) and EMI ( loss weights ) and provided results with their fine-tuned parameters in a new Appendix section ( Appendix E , page 21-23 ) . Briefly speaking , our approaches significantly outperform RND and EMI in problem settings illustrated in this paper . To better understand this observation , we plotted and analyzed agents \u2019 visitation heatmaps over time and found they tended to focus exploration on state-action pairs where the environment transition dynamics is complex , instead of exploring state-action pairs which can lead to interactions potentially with external rewards . Please refer to detailed results and discussions in the updated version . Q2 : About minor comments . Q : What does \u201c transition-dependent \u201d mean ? A : Transition-dependent means that the transition dynamics of one agent is dependent on states and actions of other agents , e.g. , if $ p ( s_1 \u2019 , s_2 \u2019 | s_1 , a_1 , s_2 , a_2 ) \\ne p ( s_1 \u2019 | s_1 , a_1 ) p ( s_2 \u2019 | s_2 , a_2 ) $ , then agent 1 and 2 are transition dependent . Q : `` struggle in many real-world scenarios with sparse rewards '' -- please add a citation A : We added a citation to [ Burda et al. , ICLR 2019b ] , which makes a similar comment and carries out comprehensive experiments on sparse reward tasks . Q : `` intrinsic value function of agent $ i $ , $ I_ { -i|i } ^\\pi $ is $ \\beta > 0 $ is a weighting '' -- I think part of this sentence was accidentally deleted . A : Thank you for pointing out this typo . In the updated version , we have corrected it , which should be \u201c $ I_ { -i|i } ^\\pi $ is the influence value , $ \\beta > 0 $ is a weighting term \u201d . Q : Eq 5 : What is the difference between $ I $ and MI ? A : The notation $ I $ represents the influence value and we propose two methods to instantiate it , i.e. , MI and VoI , respectively . Q : `` We call \u2026 '' -- What is the a_2^VoI^\\pi_ { -i|i } term ? A : Thank you for pointing out this typo and the VoI term should not be included . We have corrected it in the new version . The denominator should be $ p ( s_2^ { t+1 } | s_1^t , s_2^t , a_1^t , a_2^t ) $ . Q : Appendix B1 : How is Eq 22 obtained from Eq 21 ? A : The partial derivative of the numerator in Eq.21 is 0 , because $ p ( s_2 \u2019 | s_1 , s_2 , a_1 , a_2 ) $ is decided by the transition function of the factored multi-agent MDP and is independent of $ \\theta_1 $ ( the policy parameters of agent 1 ) . Therefore , Eq.22 only contains the partial derivative of the denominator of the log term . [ Burda et al. , ICLR 2019a ] Burda , Y. , Edwards , H. , Storkey , A. and Klimov , O. , 2018 . Exploration by random network distillation . In Proceedings of the Seventh International Conference on Learning Representations . [ Kim et al. , ICML 2019 ] Kim , H. , Kim , J. , Jeong , Y. , Levine , S. and Song , H.O. , 2018 . EMI : Exploration with mutual information . In Proceedings of the 36th International Conference on Machine Learning ( Vol.97 , pp.3360-3369 ) . [ Burda et al. , ICLR 2019b ] Burda , Y. , Edwards , H. , Pathak , D. , Storkey , A. , Darrell , T. , & Efros , A . A . ( 2018 ) .Large-scale study of curiosity-driven learning . In Proceedings of the Seventh International Conference on Learning Representations ."}, {"review_id": "BJgy96EYvr-2", "review_text": "This paper proposes methods for incentivizing exploration in multi-agent RL. There are two approaches that are proposed, both framed as influence maximization (of either the state transitions or the decisions of the other agents). The scaling to multiple agents is done via decomposing to pairwise interactions. This influence objective is the appended to the standard intrinsic motivation objective for single agent RL. The proposed approaches are pretty elegant, and in a sense seem fundamental. I'm not an expert in this particular area, so I don't know how novel these ideas are. (See related work comments below.) The empirical results seem quite strong, although (being a a non-expert), I can't tell if they're constructed to be good for the proposed approaches. There isn't much discussion of limitations and/or experiments breaking the proposed approach. I found the related work discussion a bit incomplete. Can the authors comment directly on related MARL work, such as Foerster et al., AAAI 2018? What are the specific points of contrast?", "rating": "6: Weak Accept", "reply_text": "Thanks for your detailed comments . Here we provide clarification for your comments . Q1 : How novel our ideas are ? A1 : To our best knowledge , this paper is the first work that proposes the general idea of introducing influence between agents into multi-agent exploration ( as discussed in the related work part ) . In addition , we present two original instantiations for quantifying influence : information-theoretic measure based on mutual information and decision-theoretic measure based on counterfactual value , and also show how to optimize them in the policy gradient framework . Q2 : Discussions on limitations . A2 : We discuss the main limitation of our methods \u2014 the scalability issue \u2014 in Appendix C. We also propose a variational inference approach there to alleviating this problem and do some experiments to prove the feasibility of this method . As discussed in Section 3.3 , for scenarios with tight-coupled interaction , influences involving more than two agents are approximately decomposed into pairwise interactions , which may be inaccurate and is another potential limitation of our method . Q3 : Comments on related MARL works such as COMA . A3 : COMA [ Foerster et al. , AAAI 2018 ] shares some similarities with EDTI in that both of them use the idea of counterfactual formulations . However , they are quite different in terms of definition and optimization : ( 1 ) conceptually , EDTI is proposed to measure the influence of one agent on the value functions of other agents , while COMA aims at quantifying individual contribution to the team value ; ( 2 ) EDTI is defined on counterfactual Q values over state-action pairs of other agents given its own state-action pair while COMA uses the counterfactual Q value just over its own action without considering state information , which is critical for exploration ; ( 3 ) we explicitly derive the gradients for optimizing EDTI influence for coordinated exploration in the policy gradient framework , which provides more accurate feedback , while COMA uses the counterfactual Q value as a critic . Apart from COMA , our work is also related to influence-based abstraction [ Oliehoek et al. , 2012 , de Castro et al. , 2019 ] and social influence [ Jaques et al. , ICML 2019 ] . We have added discussions in Sec.5 of the updated version of our manuscript . [ Foerster et al. , AAAI 2018 ] Foerster , J.N. , Farquhar , G. , Afouras , T. , Nardelli , N. and Whiteson , S. , 2018 , April . Counterfactual multi-agent policy gradients . In Thirty-Second AAAI Conference on Artificial Intelligence . [ de Castro et al. , 2019 ] de Castro , M.S. , Congeduti , E. , Starre , R.A. , Czechowski , A. and Oliehoek , F.A. , 2019 , May . Influence-based abstraction in deep reinforcement learning . In Adaptive , learning agents workshop ( Vol.34 ) . [ Oliehoek et al. , 2012 ] Oliehoek , F.A. , Witwicki , S.J . and Kaelbling , L.P. , 2012 , July . Influence-based abstraction for multiagent systems . In Twenty-Sixth AAAI Conference on Artificial Intelligence . [ Jaques et al. , ICML 2019 ] Jaques , N. , Lazaridou , A. , Hughes , E. , Gulcehre , C. , Ortega , P. , Strouse , D. , Leibo , J.Z . and De Freitas , N. , 2019 , May . Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning . In International Conference on Machine Learning ( pp.3040-3049 ) ."}], "0": {"review_id": "BJgy96EYvr-0", "review_text": "Update: I thank the authors for their response and I will maintain my score, my main hesitation being the overall clarity and readability of the paper. Summary: This paper proposes the use of two intrinsic rewards for exploration in MARL settings. The first one is an information-theoretic influence (EITI) bonus and a decision-theoretic influence (EDTI) bonus. EITI uses mutual information to capture the influence of one agent on the transition dynamics of others, while EDTI uses an intrinsic reward called Value of Interaction (VoI) to quantify the influence of one agent\u2019s behavior on expected returns of other agents. Main Comments: Overall, I think this paper would be a good contribution for ICLR 2020 and I lean towards accepting it. The experimental section is thorough, the authors include relevant ablations, baselines and popular algorithms used in MARL settings. The use of the decision-theoretic influence is novel as far as I can tell and it also seems to be quite effective on the tasks used for evaluation. Although the method uses a series of approximations and assumptions, I believe most of them are clearly stated and fairly well-motivated (plus they are not very far from those of other recent work in the deep MARL literature). I also appreciated the fact that the authors explicitly derived the main mathematical results used in the paper. I only have some minor comments and questions regarding some assumptions and notation. I also encourage the authors to proof-read the paper as some parts of it are a bit hard to follow. I would very much like to see a more an edited version of this paper with more precise language. Can you discuss in more detail the difference between EITI and the intrinsic reward based on social influence used in Jacques et al. (2018)? They seem to be quite similar conceptually and the related work part related to this is rather vague. Please clarify the distinction. Minor Questions / Comments: There are a few typos throughout the paper: 1. On page 3 after equation (3), I believe part of the sentence that should describe the I term in the equation is missing. 2. The phrase right after equation (16) which defines the EDTI reward does not seems to not match the above expression. Can you please explain why the transition would be conditioned on the influence term? While reviewing, I\u2019ve been assuming this was just a mistake in writing but please double check and clarify. 3. On page 4, after equation (8), you refer to a_1, a_1 and s_2\u2019 which do not appear in the above equation. Can you please use the same notation or motivate your choice for referring to those variables instead? ", "rating": "6: Weak Accept", "reply_text": "Thanks for your careful reading and thoughtful comments . Here we provide some feedback to your comments . Q1 : A more edited version of this paper with more precise language is expected . A1 : We have proofread the manuscript and made modifications to improve our presentation . We would appreciate it if you have more comments about the readability . Q2 : Difference between EITI and the intrinsic reward based on social influence [ Jaques et al. , ICML 2019 ] . A2 : The differences between EITI and social influence lie in both definition and optimization . ( 1 ) Definition . Social influence is defined to be the influence of one agent on the policies of other agents , while EITI measures the influence of one agent on the transition dynamics of other agents . Accompanying this distinction , EITI considers both states and actions to measure the influence while social influence quantifies the influence of actions without considering state information , which is critical for exploration ; ( 2 ) Optimization . In EITI , to explicitly maximize mutual information , we add it as a regularizer to the learning objective and derive the gradients in the policy gradient framework . In contrast , social influence reward is added to the immediate environmental reward , and is used to train the RL algorithms . We have also updated the relating discussions in Sec.5 of the new version of our manuscript . Q3 : About the typos . A3 : Thank you for pointing out these typos . We have corrected them in the updated version . [ Jaques et al. , ICML 2019 ] Jaques , N. , Lazaridou , A. , Hughes , E. , Gulcehre , C. , Ortega , P. , Strouse , D. , Leibo , J.Z . and De Freitas , N. , 2019 , May . Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning . In International Conference on Machine Learning ( pp.3040-3049 ) ."}, "1": {"review_id": "BJgy96EYvr-1", "review_text": "This paper studies the problem of designing effective exploration strategies in multi-agent domains. The key idea is to define one agent's exploration in terms of its interactions with other agents. This leads to two auxiliary exploration objectives, which measure how one agent's actions affect the dynamics and value of another agent's actions. The paper does an admirable job comparing the proposed method against a number of baselines, where the proposed method performs significantly better. Visualizations and ablation experiments nicely illustrate the contributions of various components of the method. I am leaning towards accepting the paper. To the best of my knowledge, the broad idea of applying information theory to multi-agent exploration, in addition to the specific instantiation described in the paper, is novel. I expect that this paper will encourage future work to explore more problems in this area. The experiments are quite thorough. My main reservation is a lack of comparisons to single agent exploration methods. As noted in Section 3, we can view multi-agent domains as just a special type of single agent domain. How would curiosity-based exploration, such as [Burda 2018, Pathak 17], or mutual information-based exploration, such as [Gregor 16, Eysenbach 18, Achaim 18], compare to the proposed method? I have a few reservations about the clarity of presentation, but I think those are easily addressed. My remaining concern is that the results are on somewhat toy tasks, but I think that is par for this area of research. Overall, I would strongly argue for accepting this paper if comparisons to single-agent exploration methods were added. I would consider decreasing my review if another reviewer found quite similar prior work, or if significant bugs were found in the mathematical derivation (I have not carefully checked all the proofs in the appendix.). Minor comments * \"transition-dependent\" -- what does this mean? * \"while tend\" -- missing a subject * \"struggle in many real-world scenarios with sparse rewards\" -- please add a citation * \"intrinsic value function of agent i, I_{-i|i}^\\pi is \\beta > 0 is a weighting\" -- I think part of this sentence was accidentally deleted. * Eq 5: What is the difference between I and MI? * \"We call \u2026\" -- What is the a_2^V0I^\\pi_{-i|i} term? * Nitpick: Use \" for the start of quotes * Appendix B1: How is Eq 22 obtained from Eq 21? -------------------UPDATE AFTER AUTHOR RESPONSE--------------------- The authors have done a great job address my two concerns (similarity to prior work and empirical comparisons with single-agent exploration). I therefore increase my vote to \"accept.\"", "rating": "8: Accept", "reply_text": "Thanks for your constructive comments . We have added experiments for comparing our methods to single-agent exploration methods and corresponding analyses in the updated version . Here we provide explanations to your questions . Q1 : Comparison to single-agent exploration methods . A1 : As requested , we compare our methods with RND [ Burda et al. , ICLR 2019a ] and EMI ( exploration with mutual information ) [ Kim et al. , ICML 2019 ] . We choose these two methods because they are among the most cutting-edge curiosity-driven and mutual information-based exploration algorithms , respectively . So far , we have carried out a modest grid search over some hyperparameters for both RND ( coefficients of intrinsic and extrinsic reward ) and EMI ( loss weights ) and provided results with their fine-tuned parameters in a new Appendix section ( Appendix E , page 21-23 ) . Briefly speaking , our approaches significantly outperform RND and EMI in problem settings illustrated in this paper . To better understand this observation , we plotted and analyzed agents \u2019 visitation heatmaps over time and found they tended to focus exploration on state-action pairs where the environment transition dynamics is complex , instead of exploring state-action pairs which can lead to interactions potentially with external rewards . Please refer to detailed results and discussions in the updated version . Q2 : About minor comments . Q : What does \u201c transition-dependent \u201d mean ? A : Transition-dependent means that the transition dynamics of one agent is dependent on states and actions of other agents , e.g. , if $ p ( s_1 \u2019 , s_2 \u2019 | s_1 , a_1 , s_2 , a_2 ) \\ne p ( s_1 \u2019 | s_1 , a_1 ) p ( s_2 \u2019 | s_2 , a_2 ) $ , then agent 1 and 2 are transition dependent . Q : `` struggle in many real-world scenarios with sparse rewards '' -- please add a citation A : We added a citation to [ Burda et al. , ICLR 2019b ] , which makes a similar comment and carries out comprehensive experiments on sparse reward tasks . Q : `` intrinsic value function of agent $ i $ , $ I_ { -i|i } ^\\pi $ is $ \\beta > 0 $ is a weighting '' -- I think part of this sentence was accidentally deleted . A : Thank you for pointing out this typo . In the updated version , we have corrected it , which should be \u201c $ I_ { -i|i } ^\\pi $ is the influence value , $ \\beta > 0 $ is a weighting term \u201d . Q : Eq 5 : What is the difference between $ I $ and MI ? A : The notation $ I $ represents the influence value and we propose two methods to instantiate it , i.e. , MI and VoI , respectively . Q : `` We call \u2026 '' -- What is the a_2^VoI^\\pi_ { -i|i } term ? A : Thank you for pointing out this typo and the VoI term should not be included . We have corrected it in the new version . The denominator should be $ p ( s_2^ { t+1 } | s_1^t , s_2^t , a_1^t , a_2^t ) $ . Q : Appendix B1 : How is Eq 22 obtained from Eq 21 ? A : The partial derivative of the numerator in Eq.21 is 0 , because $ p ( s_2 \u2019 | s_1 , s_2 , a_1 , a_2 ) $ is decided by the transition function of the factored multi-agent MDP and is independent of $ \\theta_1 $ ( the policy parameters of agent 1 ) . Therefore , Eq.22 only contains the partial derivative of the denominator of the log term . [ Burda et al. , ICLR 2019a ] Burda , Y. , Edwards , H. , Storkey , A. and Klimov , O. , 2018 . Exploration by random network distillation . In Proceedings of the Seventh International Conference on Learning Representations . [ Kim et al. , ICML 2019 ] Kim , H. , Kim , J. , Jeong , Y. , Levine , S. and Song , H.O. , 2018 . EMI : Exploration with mutual information . In Proceedings of the 36th International Conference on Machine Learning ( Vol.97 , pp.3360-3369 ) . [ Burda et al. , ICLR 2019b ] Burda , Y. , Edwards , H. , Pathak , D. , Storkey , A. , Darrell , T. , & Efros , A . A . ( 2018 ) .Large-scale study of curiosity-driven learning . In Proceedings of the Seventh International Conference on Learning Representations ."}, "2": {"review_id": "BJgy96EYvr-2", "review_text": "This paper proposes methods for incentivizing exploration in multi-agent RL. There are two approaches that are proposed, both framed as influence maximization (of either the state transitions or the decisions of the other agents). The scaling to multiple agents is done via decomposing to pairwise interactions. This influence objective is the appended to the standard intrinsic motivation objective for single agent RL. The proposed approaches are pretty elegant, and in a sense seem fundamental. I'm not an expert in this particular area, so I don't know how novel these ideas are. (See related work comments below.) The empirical results seem quite strong, although (being a a non-expert), I can't tell if they're constructed to be good for the proposed approaches. There isn't much discussion of limitations and/or experiments breaking the proposed approach. I found the related work discussion a bit incomplete. Can the authors comment directly on related MARL work, such as Foerster et al., AAAI 2018? What are the specific points of contrast?", "rating": "6: Weak Accept", "reply_text": "Thanks for your detailed comments . Here we provide clarification for your comments . Q1 : How novel our ideas are ? A1 : To our best knowledge , this paper is the first work that proposes the general idea of introducing influence between agents into multi-agent exploration ( as discussed in the related work part ) . In addition , we present two original instantiations for quantifying influence : information-theoretic measure based on mutual information and decision-theoretic measure based on counterfactual value , and also show how to optimize them in the policy gradient framework . Q2 : Discussions on limitations . A2 : We discuss the main limitation of our methods \u2014 the scalability issue \u2014 in Appendix C. We also propose a variational inference approach there to alleviating this problem and do some experiments to prove the feasibility of this method . As discussed in Section 3.3 , for scenarios with tight-coupled interaction , influences involving more than two agents are approximately decomposed into pairwise interactions , which may be inaccurate and is another potential limitation of our method . Q3 : Comments on related MARL works such as COMA . A3 : COMA [ Foerster et al. , AAAI 2018 ] shares some similarities with EDTI in that both of them use the idea of counterfactual formulations . However , they are quite different in terms of definition and optimization : ( 1 ) conceptually , EDTI is proposed to measure the influence of one agent on the value functions of other agents , while COMA aims at quantifying individual contribution to the team value ; ( 2 ) EDTI is defined on counterfactual Q values over state-action pairs of other agents given its own state-action pair while COMA uses the counterfactual Q value just over its own action without considering state information , which is critical for exploration ; ( 3 ) we explicitly derive the gradients for optimizing EDTI influence for coordinated exploration in the policy gradient framework , which provides more accurate feedback , while COMA uses the counterfactual Q value as a critic . Apart from COMA , our work is also related to influence-based abstraction [ Oliehoek et al. , 2012 , de Castro et al. , 2019 ] and social influence [ Jaques et al. , ICML 2019 ] . We have added discussions in Sec.5 of the updated version of our manuscript . [ Foerster et al. , AAAI 2018 ] Foerster , J.N. , Farquhar , G. , Afouras , T. , Nardelli , N. and Whiteson , S. , 2018 , April . Counterfactual multi-agent policy gradients . In Thirty-Second AAAI Conference on Artificial Intelligence . [ de Castro et al. , 2019 ] de Castro , M.S. , Congeduti , E. , Starre , R.A. , Czechowski , A. and Oliehoek , F.A. , 2019 , May . Influence-based abstraction in deep reinforcement learning . In Adaptive , learning agents workshop ( Vol.34 ) . [ Oliehoek et al. , 2012 ] Oliehoek , F.A. , Witwicki , S.J . and Kaelbling , L.P. , 2012 , July . Influence-based abstraction for multiagent systems . In Twenty-Sixth AAAI Conference on Artificial Intelligence . [ Jaques et al. , ICML 2019 ] Jaques , N. , Lazaridou , A. , Hughes , E. , Gulcehre , C. , Ortega , P. , Strouse , D. , Leibo , J.Z . and De Freitas , N. , 2019 , May . Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning . In International Conference on Machine Learning ( pp.3040-3049 ) ."}}