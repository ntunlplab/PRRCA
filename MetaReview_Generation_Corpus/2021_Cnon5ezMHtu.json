{"year": "2021", "forum": "Cnon5ezMHtu", "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective", "decision": "Accept (Poster)", "meta_review": "The authors propose training-free neural architecture search using two theoretically inspired heuristics: the condition number of the Neural Tangent Kernel (to measure \"trainability\" of the architecture), and the number of linear regions in the input space (to measure \"expressivity\"). These two heuristics are negatively and positively correlated with test accuracy, respectively, allowing for fast, training-free Neural Architecture Search. It is certainly not the first training-free NAS proposal, but achieves competitive results with much more expensive NAS methods.\n\nA few reviewers mentioned limited novelty of the method, a claim with which I agree. The contribution of the paper, however, is something different than how it was presented. The core message seems to be that the two proposed heuristics can greatly speed up NAS, and should be a baseline method against which more expensive methods should test.\n\nI feel like this is a borderline paper, but may be of interest to researchers in the field.", "reviews": [{"review_id": "Cnon5ezMHtu-0", "review_text": "# Summary This paper aims to speed up NAS with a training-free performance estimation strategy , i.e. , estimate the performance of an architecture at initialization without training ( not necessarily the absolute performance , but the relative ranking of architectures ) . The proposed strategy estimates an architecture \u2019 s performance from two perspectives : ( 1 ) trainability , and ( 2 ) expressivity . The metrics to measure trainability and expressivity are inspired by recent progress on deep learning theory . Specifically , The trainability of an architecture is evaluated by the spectrum of its NTKs , and the expressivity is evaluated by the number of linear regions in its input space . The authors also propose a pruning-based search algorithm that relies on the proposed performance estimation strategy . Experiments on two search spaces NAS-Bench-210 and DARTS show that the proposed method can find architectures with reasonable performance while being much faster . # Strong points 1 . Training-free NAS is conceptually novel and an interesting direction . This could make NAS more accessible and useful in practice if training-free methods can work . 2.The authors draw inspiration from recent deep learning theory to design the training-free performance estimation metric . 3.The proposed NAS method can complete the search much faster than previous NAS methods . # Weak points 1 . The metrics for trainability and expressivity seem to be the direct application/extension of recent deep learning theories ( NTK , linear region ) . This makes the novelty of this work a bit weak . 2.It \u2019 s unclear how the number of linear regions is computed . The authors gave a definition in Equation 4 but didn \u2019 t mention how exactly the number of linear regions is computed . 3.The Kendall tau correlation between NTK/linear region and the test accuracy is actually weak based on the reported value ( -0.42 / 0.5 ) . Why would this weak correlation lead to an architecture with good performance ? Also , the final method uses a combined relative ranking . What \u2019 s the Kendall tau for this combined relative ranking metric ? 4.The most interesting part in this work is the proposed training-free metric . It is important to compare this training-free metric with the typical metric ( e.g. , validation accuracy after training a limited number of epochs ) , while keeping other factors unchanged . What will the performance/search efficiency be if we combine the proposed metric with existing search algorithms ( e.g. , random search , reinforcement learning ) ? It will be helpful to compare the proposed metric and the typical metric using the same search algorithm and the same number of samples . 5.It \u2019 s unclear how important the pruning mechanism is for the final performance/search efficiency . Having the above analysis/results will also help answer this question . 6.The absolute performance on ImageNet in DARTS search space is not very strong . Also , for experiments on NAS-Bench-201 , the authors only compare to some relatively weak baselines . As shown in Table 5 in the original NAS-Bench-201 paper , random search/REINFORCE perform on par or better than the proposed method , especially on ImageNet-16-120 . # Justification of rating The proposed metric is interesting . My main concern is the lack of analysis as mentioned above . Without those analyses , it \u2019 s hard to convince people that this metric is empirically useful . Also , the novelty is a bit limited as the metric seems to be a direct application/extension of recent deep learning theories . # Additional feedback \u201c Training a supernet till convergence is extremely slow \u201d - This sentence is not very accurate . Training a supernet is usually similar to training one architecture on the dataset . Although it \u2019 s more expensive than training-free , it \u2019 s practically acceptable . The drawback of supernet is more about its inaccurate performance estimation . \u201c What to select \u201d sounds more like the search space definition in NAS . The objective of NAS is always clear ( best accuracy under certain FLOPS/params ) . I suggest the authors rephrase the text . Figure 5.The two figures have some overlappings . # After rebuttal I would like to thank the authors for their extensive efforts during the rebuttal . My main concerns are resolved , so I change the rating to borderline accept . I encourage the authors to update the paper with the results provided in the rebuttal , especially the explanation of the novelty and the results for `` Proposed metric with existing search algorithms '' .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * * Performance on ImageNet * * First , we emphasize that the main pursuit of our work is ultra-fast and low-cost search , at which our work seems to be \u201c unbeatable \u201d . On CIFAR-10 , our search time ( 0.02 GPU day ) outperforms the closest prior work , PC-DARS [ 1 ] , over 5 $ \\times $ . On ImageNet , if we search on ImageNet ( with mark $ \\dagger $ ) , we outperform PC-DARS [ 1 ] over 22.35 $ \\times $ . Second , the latest numbers in the last row of Table 3 , using the standard training recipe , are now updated to 24.5 % top-1 and 7.5 % top-5 , both on par with PC-DART searches on ImageNet , yet using 22.35 $ \\times $ less search time . Our training of ImageNet-searched architecture was not fully completed before the ICLR deadline , and we continued to train it until convergence after our submission . That confirms the effectiveness of our training-free NAS that is competitive to existing NAS algorithms , yet still being extremely efficient . NAS-Bench-201 is a popular testbed where TE-NAS is clearly superior in both performance and efficiency . We also suspect a layout issue in the original draft Table 3 might cause some misinterpretation of our significant results . Table 3 includes results searched on CIFAR-10 and transferred to ImageNet , and directly searched on ImageNet . The two categories are not directly comparable , and the latter will perform better but cost longer . The mark $ \\dagger $ indicates architectures searched on ImageNet , otherwise it is searched on CIFAR-10 or CIFAR-100 . We have revised our Table 3 layout to make the display more clear . We firmly confirm the release of all our codes and searched architectures to ensure the reproducibility of our results . [ 1 ] Xu , Yuhui , et al . `` Pc-darts : Partial channel connections for memory-efficient differentiable architecture search . '' ICLR 2020 . * * * SOTA on NAS-Bench201 * * The \u201c REA/RS/REINFORCE/BOHB \u201d methods used the meta-data ( i.e.ground truth accuracy ) provided by the NAS-Bench201 database . This is the reason why they achieve higher accuracy while only costing a few seconds . Our selected baselines are the state-of-the-art ones in the fair setting . We privately double-confirmed this with the author of NAS-Bench201 . * * * Slow training of supernet * * Thank you for your question . Regarding the slow training of cell-based networks ( each cell is a directed acyclic graph of many operators , e.g.the supernet for DARTs or NAS-Bench-201 search space ) , we have the following two folds of observations : 1 ) On CIFAR-100 , the architecture in NAS-Bench-201 with the highest number of parameters : 1.47MB parameters , and costs 0.2G FLOPs . However , for ResNet18 : 11.1MB parameters , only 35.9MB FLOPs . The reason is that cell-based architecture suffers from repeated merges ( adds ) of feature maps . 2 ) The slowness of supernet training is also discussed in the community : ( a ) https : //github.com/quark0/darts/issues/80 # issuecomment-503075338 ( b ) https : //github.com/quark0/darts/issues/37 # issuecomment-417390963 * * * More language and figure issues * * Thank you for your great suggestion and we happily take your suggestion to improve the clarification . We have rephrased `` what to select '' all with \u201c how to evaluate \u201d in our draft . We have also modified our Figure 5 ."}, {"review_id": "Cnon5ezMHtu-1", "review_text": "This work studied an interesting topic of training-free Neural Architecture Search ( NAS ) . It utilizes two training-free indicators to measure the performance of a network without training it . The experiments show the proposed approach has improvement in searching time . However , as the main contribution , the two measurements of the training-free indicators ( trainability and expressivity ) already be proposed by previous works . The rest contributions of this work are the proposed NAS networks , Instead of using the original loss function to guide the search process , this work simply combined those two measurements as a new ranking identifier of candidate architectures . The contributions not strong enough . Advantages : 1 . The writing quality of the paper is good enough 2 . The paper has good descriptions of the related work . Key weakness : 1 . The major contributions of this work , the two network measurement methods , were proposed by previous works . The first indicator : Arthur Jacot , Franck Gabriel , and Clement Hongler . Neural tangent kernel : Convergence and generalization in neural networks . In Advances in Neural Information Processing Systems 31 . 2018a.Jaehoon Lee , Lechao Xiao , Samuel Schoenholz , Yasaman Bahri , Roman Novak , Jascha SohlDickstein , and Jeffrey Pennington . Wide neural networks of any depth evolve as linear models under gradient descent . In Advances in neural information processing systems , pp . 8572\u20138583 , 2019 . Yeonjong Shin and George Em Karniadakis . Trainability of relu networks and data-dependent initialization . Journal of Machine Learning for Modeling and Computing , 1 ( 1 ) , 2020 . Lenaic Chizat , Edouard Oyallon , and Francis Bach . On lazy training in differentiable programming . 2019.The seconde indicator : Huan Xiong , Lei Huang , Mengyang Yu , Li Liu , Fan Zhu , and Ling Shao . On the number of linear regions of convolutional neural networks . arXiv preprint arXiv:2006.00978 , 2020 . 2.The differences between the proposed pruning-based NAS and previous work are not clear . What is the key novelty of the proposed pruning strategy ? 3.According to Table 3 , it seems that the TE-NAS couldn \u2019 t find the optimal neural architecture like P-DARTS and PC-DARTS , which confirms the limitation of this training-free search framework .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your great question , although we can not agree it \u201c confirms the limitation of this training-free search framework. \u201d First and foremost , we emphasize again that the main pursuit of our work is ultra-fast and low-cost search , at which aspect our work seems to be \u201c unbeatable \u201d . On CIFAR-10 , our search time outperforms the closet prior work , PC-DARS [ 1 ] , over 5 $ \\times $ . On ImageNet , if we search on ImageNet ( with mark `` $ \\dagger $ '' ) , we outperform PC-DARS [ 1 ] over 22.35 $ \\times $ . * We suspect a layout issue in the original draft Table 3 might cause some misinterpretation of our significant results . Table 3 includes results searched on CIFAR-10 and transferred to ImageNet , and directly searched on ImageNet . The two categories are not directly comparable , and the latter will perform better but cost longer . The mark \u201c \u2020 \u201d The architecture is searched on ImageNet , otherwise it is searched on CIFAR-10 or CIFAR-100 . We have revised our Table 3 layout to make the display more clear . Second , our training of ImageNet-searched architecture was not fully completed before the ICLR deadline , and we actually continued to train that until convergence after our submission . The latest numbers in the last row of Table 3 , using the standard training recipe , now become 24.5 % top-1 and 7.5 % top-5 , both on par with PC-DART searches on ImageNet , yet still using 22.35 $ \\times $ less search time . That confirms the non-compromised effectiveness of our training-free NAS , besides being extremely efficient . We firmly confirm the release of all our codes , searched architectures and pre-trained weights to ensure the reproducibility of our results . [ 1 ] Xu , Yuhui , et al . `` Pc-darts : Partial channel connections for memory-efficient differentiable architecture search . '' ICLR 2020 ."}, {"review_id": "Cnon5ezMHtu-2", "review_text": "Summary : Training-free NAS is a promising direction . This work demonstrates that two theoretically inspired indicators : the spectrum of NTK and number of linear regions , strongly correlate with the network \u2019 s performance , and can be leveraged to reduce the search cost and decouple the analysis of the network \u2019 s trainability and expressivity . The authors thus proposed TE-NAS to rank architectures by analyzing the two indicators . Without involving any training , TE-NAS can achieve competitive NAS performance within minimum search time . Pros : + This work bridges between the advances in deep learning theory and the practice of NAS . I think it can become an important starting point towards theoretically inspired NAS and can motivate the discovery of more practically viable deep network performance indicators . + The authors made efforts to add interpretability to their method and to understand the search process , e.g.Figure 4 and section 3.2.1 . It is very interesting to see that the supernet first is pruned by quickly increasing the network \u2019 s trainability , and then fine-tuning the expressivity in a small range . + On NAS-Bench-201 and DARTS search spaces , the propose method \u2019 s results are extremely promising . For the latter space , the proposed method can search within 30 minutes ( on CIFAR-10 ) and 4 hours ( on ImageNet ) , while the search models \u2019 accuracy remain among the most competitive few . Cons : - I remain skeptical how much the initial stage expressiveness/trainability would last and be preserved until end of training . Also , the two do not represent the full spectrum of desirable model characteristics . The ultimate goal of NAS is to seek a model of best generalization ability , which can not be merely or easily from its expressiveness ( fitting ability , i.e. , achievable training error ) and trainability ( optimization difficulty ) . - Although the authors claimed that the two chosen indicators are strongly correlated with the network \u2019 s test accuracies , I \u2019 m not sure how true that actually is . In fact , probing the generalization from a trained model is already challenging , not to say from an untrained one . The authors might need to discuss more or to tone down further . - Figures 1 & 2 leave me with impression that both indicators can filter out some bad outlier architectures ; but for many relatively good accuracy architectures , the correlations between the two and the true accuracy are not necessarily high ( overall Kendall-tau magnitudes are between 0.4 and 0.5 ) . - The authors also didn \u2019 t specify the computational complexity of computing the two indicators ( average per architecture ) , except just saying they \u2019 re very fast . - More NAS works are related to the manuscript , for example [ 1 ] [ 2 ] [ 3 ] . [ 1 ] SGAS : Sequential Greedy Architecture Search [ 2 ] GP-NAS : Gaussian Process Based Neural Architecture Search [ 3 ] HourNAS : Extremely Fast Neural Architecture Search Through an Hourglass Lens - ( minor ) The writing quality is in general good ; some occasional typos : training-free AS framework : \u201c AS \u201d - > \u201c NAS \u201d ; no only - > not only", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We greatly appreciate your questions and suggestions , and we really like them ! Below are our responses . 1 . `` initial stage expressiveness/trainability '' Your question is very much to-the-point and we like it a lot . Actually , we also started investigating similar questions right after this submission . We recently observed some trends of the NTK spectrum during training shared by different networks . For example , in early training ( e.g.within the first epoch ) $ \\kappa $ first increase then drops . We find this observation matches the recent studies on the evolution of the network 's Hessian spectrum during training [ 1 ] , where large negative eigenvalues quickly disappear after the early iterations . We are actively working on this open problem . Meanwhile , in our submission , we demonstrate that $ \\kappa $ and R at initialization can already filter-out bad architectures , which is very effective and helpful for NAS search . Of course , more in-depth analysis of these two indicators at the later stages of NAS are of our vital interest . 2.About Generalization This is really an excellent and insightful comment . In a general case , it is true that only $ \\kappa $ ( trainability ) and $ R $ ( expressiveness ) do not add up to the full picture of generalization . Conceptually , the generalization gap is the difference between a model \u2019 s performance on training data and its performance on unseen data drawn from the same distribution ( e.g. , testing set ) . In comparison , the two indicators of trainability and expressiveness for a network basically determine how well the training set could be fit ( i.e. , training set accuracy ) , and do not directly indicate its generalization gap ( or test set accuracy ) . Indeed , probing the generalization of an untrained network at its initialization is a daunting , open challenge that seems to go beyond the current theory scope . Yet very interestingly , NAS might happen to be such a special case that $ \\kappa $ and $ R $ together indeed work effectively . The goal of NAS , practically , is to find the architecture from the search space with the high test accuracy on the target test set . We show the relationship between test and training accuracies in section E in our new supplement . As we can see from Figure 10 , at least on CIFAR-100 networks achieving high test accuracy also tend to achieve high training accuracy . This seems to be a result of the current standard search space design , which could have implicitly excluded severe overfitting . This explains why in our submission , only focusing on trainability and expressiveness can still achieve good search results of test accuracy . Thank you for pointing out ! We will add the above discussion in the revised draft and will properly tune our tone . 3.About Kendall-tau correlations Since two indicators have different preferences over operators ( Table 5 ) , their combination exhibits more powerful selection capability over the search space . In Figure 9 in section D3 we show that the correlation between test accuracy and the summation of two rankings is much higher ( 0.64 , ranking the lower the better ) . 4.About computation complexity Besides the search time costs reported in Table 1~3 , here we measure the FLOPs of calculating $ \\kappa $ and the number of linear regions on CIFAR-100 . In comparison with the FLOPs of proxy inference ( e.g.train the sampled architecture for one epoch and approximate its validation accuracy ) , our $ \\kappa $ and $ R $ only cost 0.06 % and 0.03 % FLOPs , respectively . However , one-epoch proxy inference only achieves 0.132 kendall-tau correlation by our experiments , much lower than our 0.64 . 5.Typos and missing references Thank you ! We have added the missing references and fixed the typos . [ 1 ] Ghorbani B , Krishnan S , Xiao Y . An investigation into neural net optimization via hessian eigenvalue density . arXiv preprint arXiv:1901.10159 . 2019 Jan 29 ."}, {"review_id": "Cnon5ezMHtu-3", "review_text": "This paper introduces a searching framework of neural architectures ranking the candidates with two different metrics : the spectrum of NTK and the number of linear regions in the input space . These two metrics do not require the training of neural networks lightening the computational burdensome . Authors support their method by providing results from CIFAR-10 , ImageNet , and NAS-Bench-201 benchmark ( Dong & Yang ) . Overall , this paper proposes new metrics that can be used in the NAS search . Despite the paper well written , the performance of their methodology is incremental improvements ( or par ) among various existing NAS algorithms . I would recommend a marginally above acceptance threshold for now . Strength 1 . The author provides two new different metrics : a condition number of NTK and cardinality of linear regions to select good architectures besides the validation loss . 2.Experiments not only on CIFAR10 , ImageNet , but also NAS-Bench 201 3 . Free from the training allows their method to free from computational burdensome allowing to stack an equivalent number of cells for both the search and evaluation phase . Weakness 1 . The experiment results are par ( or worse ) to the existing NAS algorithms . By listing the recently published NAS literatures on CIFAR-10 : P-DARTs : 2.50 % , DATA : 2.59 % , SGAS : 2.66 % , PC-DARTs : 2.57 % , RandomNAS-NSAS : 2.64 % . Especially , SGAS and PC-DARTs require 0.25 and 0.1 GPU days respectively . ( However , NAS-BENCH-201 results are competitive . ) 2.While no training has a benefit in the computational budget , the performance could be worse than the method with training ( less information ) as listed in Weakness 1 . Questions 1 . According to Definition 1 , the number of linear regions depends on the fixed set of parameters $ \\theta $ . How much the number of linear regions affected by the $ \\theta $ ? What happens you use the trained $ \\theta $ or draw the network parameters with other initializing methods such as Ha initialization or Xavier initialization ? 2.Figure 5 shows the pruning trajectory based on the author 's method . Intuitively randomly pruning the operation will also reduce the $ \\kappa $ and the linear regions . Can you support the pruning efficiency based on the proposed metrics by comparing it with the random pruning ? 3.Have you tried weight sum loss such as $ \\alpha \\Delta \\kappa + ( 1-\\alpha ) \\Delta R $ where $ \\alpha \\in ( 0 , 1 ) $ be a hyperparameter ? Table 4 seems $ R $ plays more significant role than $ \\kappa $ . Reference : 1 . Xu , Y et al . ( 2019 , September ) . PC-DARTS : Partial Channel Connections for Memory-Efficient Architecture Search . ICLR2020 2 . Chen , X et al.Progressive differentiable architecture search : Bridging the depth gap between search and evaluation . ICCV2019 3 . Chang , J et al.DATA : Differentiable ArchiTecture Approximation . Neurips2019 4 . Li , G et al.SGAS : Sequential Greedy Architecture Search . CVPR2020 5 . Zhang , M et al.Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization . CVPR2020 6 . Dong & Yang . NAS-Bench-201 : Extending the Scope of Reproducible Neural Architecture Search . ICLR 2020", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.Thank you for your great question ! In summary , different initializations do have some effects on the number of linear regions , and we are also interested in studying this on real-world networks . We chose one architecture from NAS-Bench-201 and compared the number of linear regions with different initializations over four random draws ( on CIFAR-100 ) . Gaussian : 3651.0 ( 37.6 ) ; Kaiming_uniform : 3062.8 ( 318.2 ) ; Xavier_uniform : 2681.3 ( 320.8 ) . We can see the number of linear regions measured under Gaussian initialization benefits from the smallest variance , which contributes to the stability of our NAS search . 2.This is a really great suggestion . We conducted random pruning and recorded the trajectory of the supernet ( each measure is averaged over three times ) : $ R $ ( higher the better ) : 1100.7 $ \\rightarrow $ 1098.3 $ \\rightarrow $ 1022.7 $ \\rightarrow $ 544.7 $ \\kappa $ ( lower the better ) : 246.5 $ \\rightarrow $ 220.1 $ \\rightarrow $ 399.8 $ \\rightarrow $ 305.5 We can see random pruning may potentially drop the number of linear regions , but not necessarily always reduce $ \\kappa $ since it may blindly prune some useful operators while keeping bad ones . This random pruning ends with only 43.35 % test accuracy on CIFAR-100 . 3.As we indicated on page 5 footnote , We tried some weighted summations of the two , and found their equal-weight summation to perform the best . We conducted your suggested convex combination study with $ \\alpha $ from 0 ~ 1 , and the best result was 70.54 ( 0.71 ) achieved with $ \\alpha $ = 0.6 ."}], "0": {"review_id": "Cnon5ezMHtu-0", "review_text": "# Summary This paper aims to speed up NAS with a training-free performance estimation strategy , i.e. , estimate the performance of an architecture at initialization without training ( not necessarily the absolute performance , but the relative ranking of architectures ) . The proposed strategy estimates an architecture \u2019 s performance from two perspectives : ( 1 ) trainability , and ( 2 ) expressivity . The metrics to measure trainability and expressivity are inspired by recent progress on deep learning theory . Specifically , The trainability of an architecture is evaluated by the spectrum of its NTKs , and the expressivity is evaluated by the number of linear regions in its input space . The authors also propose a pruning-based search algorithm that relies on the proposed performance estimation strategy . Experiments on two search spaces NAS-Bench-210 and DARTS show that the proposed method can find architectures with reasonable performance while being much faster . # Strong points 1 . Training-free NAS is conceptually novel and an interesting direction . This could make NAS more accessible and useful in practice if training-free methods can work . 2.The authors draw inspiration from recent deep learning theory to design the training-free performance estimation metric . 3.The proposed NAS method can complete the search much faster than previous NAS methods . # Weak points 1 . The metrics for trainability and expressivity seem to be the direct application/extension of recent deep learning theories ( NTK , linear region ) . This makes the novelty of this work a bit weak . 2.It \u2019 s unclear how the number of linear regions is computed . The authors gave a definition in Equation 4 but didn \u2019 t mention how exactly the number of linear regions is computed . 3.The Kendall tau correlation between NTK/linear region and the test accuracy is actually weak based on the reported value ( -0.42 / 0.5 ) . Why would this weak correlation lead to an architecture with good performance ? Also , the final method uses a combined relative ranking . What \u2019 s the Kendall tau for this combined relative ranking metric ? 4.The most interesting part in this work is the proposed training-free metric . It is important to compare this training-free metric with the typical metric ( e.g. , validation accuracy after training a limited number of epochs ) , while keeping other factors unchanged . What will the performance/search efficiency be if we combine the proposed metric with existing search algorithms ( e.g. , random search , reinforcement learning ) ? It will be helpful to compare the proposed metric and the typical metric using the same search algorithm and the same number of samples . 5.It \u2019 s unclear how important the pruning mechanism is for the final performance/search efficiency . Having the above analysis/results will also help answer this question . 6.The absolute performance on ImageNet in DARTS search space is not very strong . Also , for experiments on NAS-Bench-201 , the authors only compare to some relatively weak baselines . As shown in Table 5 in the original NAS-Bench-201 paper , random search/REINFORCE perform on par or better than the proposed method , especially on ImageNet-16-120 . # Justification of rating The proposed metric is interesting . My main concern is the lack of analysis as mentioned above . Without those analyses , it \u2019 s hard to convince people that this metric is empirically useful . Also , the novelty is a bit limited as the metric seems to be a direct application/extension of recent deep learning theories . # Additional feedback \u201c Training a supernet till convergence is extremely slow \u201d - This sentence is not very accurate . Training a supernet is usually similar to training one architecture on the dataset . Although it \u2019 s more expensive than training-free , it \u2019 s practically acceptable . The drawback of supernet is more about its inaccurate performance estimation . \u201c What to select \u201d sounds more like the search space definition in NAS . The objective of NAS is always clear ( best accuracy under certain FLOPS/params ) . I suggest the authors rephrase the text . Figure 5.The two figures have some overlappings . # After rebuttal I would like to thank the authors for their extensive efforts during the rebuttal . My main concerns are resolved , so I change the rating to borderline accept . I encourage the authors to update the paper with the results provided in the rebuttal , especially the explanation of the novelty and the results for `` Proposed metric with existing search algorithms '' .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * * Performance on ImageNet * * First , we emphasize that the main pursuit of our work is ultra-fast and low-cost search , at which our work seems to be \u201c unbeatable \u201d . On CIFAR-10 , our search time ( 0.02 GPU day ) outperforms the closest prior work , PC-DARS [ 1 ] , over 5 $ \\times $ . On ImageNet , if we search on ImageNet ( with mark $ \\dagger $ ) , we outperform PC-DARS [ 1 ] over 22.35 $ \\times $ . Second , the latest numbers in the last row of Table 3 , using the standard training recipe , are now updated to 24.5 % top-1 and 7.5 % top-5 , both on par with PC-DART searches on ImageNet , yet using 22.35 $ \\times $ less search time . Our training of ImageNet-searched architecture was not fully completed before the ICLR deadline , and we continued to train it until convergence after our submission . That confirms the effectiveness of our training-free NAS that is competitive to existing NAS algorithms , yet still being extremely efficient . NAS-Bench-201 is a popular testbed where TE-NAS is clearly superior in both performance and efficiency . We also suspect a layout issue in the original draft Table 3 might cause some misinterpretation of our significant results . Table 3 includes results searched on CIFAR-10 and transferred to ImageNet , and directly searched on ImageNet . The two categories are not directly comparable , and the latter will perform better but cost longer . The mark $ \\dagger $ indicates architectures searched on ImageNet , otherwise it is searched on CIFAR-10 or CIFAR-100 . We have revised our Table 3 layout to make the display more clear . We firmly confirm the release of all our codes and searched architectures to ensure the reproducibility of our results . [ 1 ] Xu , Yuhui , et al . `` Pc-darts : Partial channel connections for memory-efficient differentiable architecture search . '' ICLR 2020 . * * * SOTA on NAS-Bench201 * * The \u201c REA/RS/REINFORCE/BOHB \u201d methods used the meta-data ( i.e.ground truth accuracy ) provided by the NAS-Bench201 database . This is the reason why they achieve higher accuracy while only costing a few seconds . Our selected baselines are the state-of-the-art ones in the fair setting . We privately double-confirmed this with the author of NAS-Bench201 . * * * Slow training of supernet * * Thank you for your question . Regarding the slow training of cell-based networks ( each cell is a directed acyclic graph of many operators , e.g.the supernet for DARTs or NAS-Bench-201 search space ) , we have the following two folds of observations : 1 ) On CIFAR-100 , the architecture in NAS-Bench-201 with the highest number of parameters : 1.47MB parameters , and costs 0.2G FLOPs . However , for ResNet18 : 11.1MB parameters , only 35.9MB FLOPs . The reason is that cell-based architecture suffers from repeated merges ( adds ) of feature maps . 2 ) The slowness of supernet training is also discussed in the community : ( a ) https : //github.com/quark0/darts/issues/80 # issuecomment-503075338 ( b ) https : //github.com/quark0/darts/issues/37 # issuecomment-417390963 * * * More language and figure issues * * Thank you for your great suggestion and we happily take your suggestion to improve the clarification . We have rephrased `` what to select '' all with \u201c how to evaluate \u201d in our draft . We have also modified our Figure 5 ."}, "1": {"review_id": "Cnon5ezMHtu-1", "review_text": "This work studied an interesting topic of training-free Neural Architecture Search ( NAS ) . It utilizes two training-free indicators to measure the performance of a network without training it . The experiments show the proposed approach has improvement in searching time . However , as the main contribution , the two measurements of the training-free indicators ( trainability and expressivity ) already be proposed by previous works . The rest contributions of this work are the proposed NAS networks , Instead of using the original loss function to guide the search process , this work simply combined those two measurements as a new ranking identifier of candidate architectures . The contributions not strong enough . Advantages : 1 . The writing quality of the paper is good enough 2 . The paper has good descriptions of the related work . Key weakness : 1 . The major contributions of this work , the two network measurement methods , were proposed by previous works . The first indicator : Arthur Jacot , Franck Gabriel , and Clement Hongler . Neural tangent kernel : Convergence and generalization in neural networks . In Advances in Neural Information Processing Systems 31 . 2018a.Jaehoon Lee , Lechao Xiao , Samuel Schoenholz , Yasaman Bahri , Roman Novak , Jascha SohlDickstein , and Jeffrey Pennington . Wide neural networks of any depth evolve as linear models under gradient descent . In Advances in neural information processing systems , pp . 8572\u20138583 , 2019 . Yeonjong Shin and George Em Karniadakis . Trainability of relu networks and data-dependent initialization . Journal of Machine Learning for Modeling and Computing , 1 ( 1 ) , 2020 . Lenaic Chizat , Edouard Oyallon , and Francis Bach . On lazy training in differentiable programming . 2019.The seconde indicator : Huan Xiong , Lei Huang , Mengyang Yu , Li Liu , Fan Zhu , and Ling Shao . On the number of linear regions of convolutional neural networks . arXiv preprint arXiv:2006.00978 , 2020 . 2.The differences between the proposed pruning-based NAS and previous work are not clear . What is the key novelty of the proposed pruning strategy ? 3.According to Table 3 , it seems that the TE-NAS couldn \u2019 t find the optimal neural architecture like P-DARTS and PC-DARTS , which confirms the limitation of this training-free search framework .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your great question , although we can not agree it \u201c confirms the limitation of this training-free search framework. \u201d First and foremost , we emphasize again that the main pursuit of our work is ultra-fast and low-cost search , at which aspect our work seems to be \u201c unbeatable \u201d . On CIFAR-10 , our search time outperforms the closet prior work , PC-DARS [ 1 ] , over 5 $ \\times $ . On ImageNet , if we search on ImageNet ( with mark `` $ \\dagger $ '' ) , we outperform PC-DARS [ 1 ] over 22.35 $ \\times $ . * We suspect a layout issue in the original draft Table 3 might cause some misinterpretation of our significant results . Table 3 includes results searched on CIFAR-10 and transferred to ImageNet , and directly searched on ImageNet . The two categories are not directly comparable , and the latter will perform better but cost longer . The mark \u201c \u2020 \u201d The architecture is searched on ImageNet , otherwise it is searched on CIFAR-10 or CIFAR-100 . We have revised our Table 3 layout to make the display more clear . Second , our training of ImageNet-searched architecture was not fully completed before the ICLR deadline , and we actually continued to train that until convergence after our submission . The latest numbers in the last row of Table 3 , using the standard training recipe , now become 24.5 % top-1 and 7.5 % top-5 , both on par with PC-DART searches on ImageNet , yet still using 22.35 $ \\times $ less search time . That confirms the non-compromised effectiveness of our training-free NAS , besides being extremely efficient . We firmly confirm the release of all our codes , searched architectures and pre-trained weights to ensure the reproducibility of our results . [ 1 ] Xu , Yuhui , et al . `` Pc-darts : Partial channel connections for memory-efficient differentiable architecture search . '' ICLR 2020 ."}, "2": {"review_id": "Cnon5ezMHtu-2", "review_text": "Summary : Training-free NAS is a promising direction . This work demonstrates that two theoretically inspired indicators : the spectrum of NTK and number of linear regions , strongly correlate with the network \u2019 s performance , and can be leveraged to reduce the search cost and decouple the analysis of the network \u2019 s trainability and expressivity . The authors thus proposed TE-NAS to rank architectures by analyzing the two indicators . Without involving any training , TE-NAS can achieve competitive NAS performance within minimum search time . Pros : + This work bridges between the advances in deep learning theory and the practice of NAS . I think it can become an important starting point towards theoretically inspired NAS and can motivate the discovery of more practically viable deep network performance indicators . + The authors made efforts to add interpretability to their method and to understand the search process , e.g.Figure 4 and section 3.2.1 . It is very interesting to see that the supernet first is pruned by quickly increasing the network \u2019 s trainability , and then fine-tuning the expressivity in a small range . + On NAS-Bench-201 and DARTS search spaces , the propose method \u2019 s results are extremely promising . For the latter space , the proposed method can search within 30 minutes ( on CIFAR-10 ) and 4 hours ( on ImageNet ) , while the search models \u2019 accuracy remain among the most competitive few . Cons : - I remain skeptical how much the initial stage expressiveness/trainability would last and be preserved until end of training . Also , the two do not represent the full spectrum of desirable model characteristics . The ultimate goal of NAS is to seek a model of best generalization ability , which can not be merely or easily from its expressiveness ( fitting ability , i.e. , achievable training error ) and trainability ( optimization difficulty ) . - Although the authors claimed that the two chosen indicators are strongly correlated with the network \u2019 s test accuracies , I \u2019 m not sure how true that actually is . In fact , probing the generalization from a trained model is already challenging , not to say from an untrained one . The authors might need to discuss more or to tone down further . - Figures 1 & 2 leave me with impression that both indicators can filter out some bad outlier architectures ; but for many relatively good accuracy architectures , the correlations between the two and the true accuracy are not necessarily high ( overall Kendall-tau magnitudes are between 0.4 and 0.5 ) . - The authors also didn \u2019 t specify the computational complexity of computing the two indicators ( average per architecture ) , except just saying they \u2019 re very fast . - More NAS works are related to the manuscript , for example [ 1 ] [ 2 ] [ 3 ] . [ 1 ] SGAS : Sequential Greedy Architecture Search [ 2 ] GP-NAS : Gaussian Process Based Neural Architecture Search [ 3 ] HourNAS : Extremely Fast Neural Architecture Search Through an Hourglass Lens - ( minor ) The writing quality is in general good ; some occasional typos : training-free AS framework : \u201c AS \u201d - > \u201c NAS \u201d ; no only - > not only", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We greatly appreciate your questions and suggestions , and we really like them ! Below are our responses . 1 . `` initial stage expressiveness/trainability '' Your question is very much to-the-point and we like it a lot . Actually , we also started investigating similar questions right after this submission . We recently observed some trends of the NTK spectrum during training shared by different networks . For example , in early training ( e.g.within the first epoch ) $ \\kappa $ first increase then drops . We find this observation matches the recent studies on the evolution of the network 's Hessian spectrum during training [ 1 ] , where large negative eigenvalues quickly disappear after the early iterations . We are actively working on this open problem . Meanwhile , in our submission , we demonstrate that $ \\kappa $ and R at initialization can already filter-out bad architectures , which is very effective and helpful for NAS search . Of course , more in-depth analysis of these two indicators at the later stages of NAS are of our vital interest . 2.About Generalization This is really an excellent and insightful comment . In a general case , it is true that only $ \\kappa $ ( trainability ) and $ R $ ( expressiveness ) do not add up to the full picture of generalization . Conceptually , the generalization gap is the difference between a model \u2019 s performance on training data and its performance on unseen data drawn from the same distribution ( e.g. , testing set ) . In comparison , the two indicators of trainability and expressiveness for a network basically determine how well the training set could be fit ( i.e. , training set accuracy ) , and do not directly indicate its generalization gap ( or test set accuracy ) . Indeed , probing the generalization of an untrained network at its initialization is a daunting , open challenge that seems to go beyond the current theory scope . Yet very interestingly , NAS might happen to be such a special case that $ \\kappa $ and $ R $ together indeed work effectively . The goal of NAS , practically , is to find the architecture from the search space with the high test accuracy on the target test set . We show the relationship between test and training accuracies in section E in our new supplement . As we can see from Figure 10 , at least on CIFAR-100 networks achieving high test accuracy also tend to achieve high training accuracy . This seems to be a result of the current standard search space design , which could have implicitly excluded severe overfitting . This explains why in our submission , only focusing on trainability and expressiveness can still achieve good search results of test accuracy . Thank you for pointing out ! We will add the above discussion in the revised draft and will properly tune our tone . 3.About Kendall-tau correlations Since two indicators have different preferences over operators ( Table 5 ) , their combination exhibits more powerful selection capability over the search space . In Figure 9 in section D3 we show that the correlation between test accuracy and the summation of two rankings is much higher ( 0.64 , ranking the lower the better ) . 4.About computation complexity Besides the search time costs reported in Table 1~3 , here we measure the FLOPs of calculating $ \\kappa $ and the number of linear regions on CIFAR-100 . In comparison with the FLOPs of proxy inference ( e.g.train the sampled architecture for one epoch and approximate its validation accuracy ) , our $ \\kappa $ and $ R $ only cost 0.06 % and 0.03 % FLOPs , respectively . However , one-epoch proxy inference only achieves 0.132 kendall-tau correlation by our experiments , much lower than our 0.64 . 5.Typos and missing references Thank you ! We have added the missing references and fixed the typos . [ 1 ] Ghorbani B , Krishnan S , Xiao Y . An investigation into neural net optimization via hessian eigenvalue density . arXiv preprint arXiv:1901.10159 . 2019 Jan 29 ."}, "3": {"review_id": "Cnon5ezMHtu-3", "review_text": "This paper introduces a searching framework of neural architectures ranking the candidates with two different metrics : the spectrum of NTK and the number of linear regions in the input space . These two metrics do not require the training of neural networks lightening the computational burdensome . Authors support their method by providing results from CIFAR-10 , ImageNet , and NAS-Bench-201 benchmark ( Dong & Yang ) . Overall , this paper proposes new metrics that can be used in the NAS search . Despite the paper well written , the performance of their methodology is incremental improvements ( or par ) among various existing NAS algorithms . I would recommend a marginally above acceptance threshold for now . Strength 1 . The author provides two new different metrics : a condition number of NTK and cardinality of linear regions to select good architectures besides the validation loss . 2.Experiments not only on CIFAR10 , ImageNet , but also NAS-Bench 201 3 . Free from the training allows their method to free from computational burdensome allowing to stack an equivalent number of cells for both the search and evaluation phase . Weakness 1 . The experiment results are par ( or worse ) to the existing NAS algorithms . By listing the recently published NAS literatures on CIFAR-10 : P-DARTs : 2.50 % , DATA : 2.59 % , SGAS : 2.66 % , PC-DARTs : 2.57 % , RandomNAS-NSAS : 2.64 % . Especially , SGAS and PC-DARTs require 0.25 and 0.1 GPU days respectively . ( However , NAS-BENCH-201 results are competitive . ) 2.While no training has a benefit in the computational budget , the performance could be worse than the method with training ( less information ) as listed in Weakness 1 . Questions 1 . According to Definition 1 , the number of linear regions depends on the fixed set of parameters $ \\theta $ . How much the number of linear regions affected by the $ \\theta $ ? What happens you use the trained $ \\theta $ or draw the network parameters with other initializing methods such as Ha initialization or Xavier initialization ? 2.Figure 5 shows the pruning trajectory based on the author 's method . Intuitively randomly pruning the operation will also reduce the $ \\kappa $ and the linear regions . Can you support the pruning efficiency based on the proposed metrics by comparing it with the random pruning ? 3.Have you tried weight sum loss such as $ \\alpha \\Delta \\kappa + ( 1-\\alpha ) \\Delta R $ where $ \\alpha \\in ( 0 , 1 ) $ be a hyperparameter ? Table 4 seems $ R $ plays more significant role than $ \\kappa $ . Reference : 1 . Xu , Y et al . ( 2019 , September ) . PC-DARTS : Partial Channel Connections for Memory-Efficient Architecture Search . ICLR2020 2 . Chen , X et al.Progressive differentiable architecture search : Bridging the depth gap between search and evaluation . ICCV2019 3 . Chang , J et al.DATA : Differentiable ArchiTecture Approximation . Neurips2019 4 . Li , G et al.SGAS : Sequential Greedy Architecture Search . CVPR2020 5 . Zhang , M et al.Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization . CVPR2020 6 . Dong & Yang . NAS-Bench-201 : Extending the Scope of Reproducible Neural Architecture Search . ICLR 2020", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.Thank you for your great question ! In summary , different initializations do have some effects on the number of linear regions , and we are also interested in studying this on real-world networks . We chose one architecture from NAS-Bench-201 and compared the number of linear regions with different initializations over four random draws ( on CIFAR-100 ) . Gaussian : 3651.0 ( 37.6 ) ; Kaiming_uniform : 3062.8 ( 318.2 ) ; Xavier_uniform : 2681.3 ( 320.8 ) . We can see the number of linear regions measured under Gaussian initialization benefits from the smallest variance , which contributes to the stability of our NAS search . 2.This is a really great suggestion . We conducted random pruning and recorded the trajectory of the supernet ( each measure is averaged over three times ) : $ R $ ( higher the better ) : 1100.7 $ \\rightarrow $ 1098.3 $ \\rightarrow $ 1022.7 $ \\rightarrow $ 544.7 $ \\kappa $ ( lower the better ) : 246.5 $ \\rightarrow $ 220.1 $ \\rightarrow $ 399.8 $ \\rightarrow $ 305.5 We can see random pruning may potentially drop the number of linear regions , but not necessarily always reduce $ \\kappa $ since it may blindly prune some useful operators while keeping bad ones . This random pruning ends with only 43.35 % test accuracy on CIFAR-100 . 3.As we indicated on page 5 footnote , We tried some weighted summations of the two , and found their equal-weight summation to perform the best . We conducted your suggested convex combination study with $ \\alpha $ from 0 ~ 1 , and the best result was 70.54 ( 0.71 ) achieved with $ \\alpha $ = 0.6 ."}}