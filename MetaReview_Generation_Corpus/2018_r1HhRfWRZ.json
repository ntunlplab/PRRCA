{"year": "2018", "forum": "r1HhRfWRZ", "title": "Learning Awareness Models", "decision": "Accept (Poster)", "meta_review": "Since this seems interesting, I suggest to accept this paper at the conference. However, there are still some serious issues with the paper, including missing references. ", "reviews": [{"review_id": "r1HhRfWRZ-0", "review_text": "The paper proposes an architecture for internal model learning of a robotic system and applies it to a simulated and a real robotic hand. The model allows making relatively long-term predictions with uncertainties. The models are used to perform model predictive control to achieve informative actions. It is shown that the hidden state of the learned models contains relevant information about the objects the hand was interacting with. The paper reads well. The method is sufficiently well explained and the results are presented in an illustrative and informative way. update: See critique in my comment below. I have a few minor points: - Sec 2: you may consider to cite the work on maximising predictive information as intrinsic motivation: G. Martius, R. Der, and N. Ay. Information driven self-organization of complex robotic behaviors. PLoS ONE, 8(5):e63400, 2013. - Fig 2: bottom: add labels to axis, and maybe mention that same color code as above - Sec 4 par 3: .... intentionally not autoregressive: w.r.t. to what? to the observations? - Sec 7.1: how is the optimization for the MPC performed? Which algorithm did you use and long does the optimization take? in first Eq: should f not be sampled from GMMpdf, so replace = with \\sim Typos: - Sec1 par2: This pattern has has ... - Sec 2 par2: statistics ofthe - Sec 4 line2: prefix of an episode , where (space before ,) ", "rating": "7: Good paper, accept", "reply_text": "`` - Sec 2 : you may consider to cite the work on maximising predictive information as intrinsic motivation : G. Martius , R. Der , and N. Ay . Information driven self-organization of complex robotic behaviors . PLoS ONE , 8 ( 5 ) : e63400 , 2013 . '' Thank you for the relevant reference . `` - Sec 4 par 3 : .... intentionally not autoregressive : w.r.t.to what ? to the observations ? `` Intentionally not autoregressive with respect to time . We have clarified this in the paper . `` - Sec 7.1 : how is the optimization for the MPC performed ? Which algorithm did you use and long does the optimization take ? '' We take the very naive approach of optimizing the MPC objective for a fixed number of steps by differentiating the cost function with respect to the actions and taking ( projected ) gradient steps with Adam . We project the actions into the constraint set at each step . We initialize the nominal action sequence with a burn in of 1000 Adam steps ( which is fairly time consuming ) and we take 10 additional optimization steps after executing each action and observing a response , warm started from the previous solution . This is acceptably fast for experimentation ( 2-5 steps/second after burn in ) but is substantially slower than real time , primarily due to the cost of evaluating the model. `` in first Eq : should f not be sampled from GMMpdf , so replace = with \\sim '' f is the Gaussian PDF defined by the parameters , not a sample from the PDF . We do n't need to sample from the predictive distribution in order to compute the control objective . One of the reasons we chose the Renyi entropy as the objective is that it is easy to compute analytically for the Mixture of Gaussians predictions that we make at each step ."}, {"review_id": "r1HhRfWRZ-1", "review_text": "Summary: The paper describes a system which creates an internal representation of the scene given observations, being this internal representation advantageous over raw sensory input for object classification and control. The internal representation comes from a recurrent network (more specifically, a sequence2sequence net) trained to maximize the likelihood of the observations from training Positive aspects: The authors suggest an interesting hypothesis: an internal representation of the world which is useful for control could be obtained just by forcing the agent to be able to predict the outcome of its actions in the world. This hypothesis would enable robots to train it in a self-supervised manner, which would be extremely valuable. Negative aspects: Although the premise of the paper is interesting, its execution is not ideal. The formulation of the problem is unclear and difficult to follow, with a number of important terms left undefined. Moreover, the experiment task is too simplistic; from the results, it's not clear whether the representation is anything more than trivial accumulation of sensory input - Lack of clarity: -- what exactly is the \"generic cost\" C in section 7.1? -- why are both f and z parameters of C? f is directly a function of z. Given that the form of C is not explained, seems like f could be directly computing as part of C. -- what is the relation between actions a in section 7.1 and u in section 4? -- How is the minimization problem of u_{1:T} solved? -- Are the authors sure that they perform gathering information through \"maximizing uncertainty\" (section 7.1)? This sounds profoundly counterintuitive. Maximizing the uncertainty in the world state should result in minimum information about the worlds state. I would assume this is a serious typo, but cannot confirm given that the relation between the minimize cost C and the Renyi entropy H is not explicitely stated. -- When the authors state that \"The learner trains the model by maximum likelihood\" in section 7.1, do they refer to the prediction model or the control model? It would seem that it is the control model, but the objective being \"the same as in section 6\" points in the opposite direction -- What is the method for classifying and/or regressing given the features and internal representation? This is important because, if the method was a recurrent net with memory, the differences between the two representations would probably be minimal. - Simplistic experimental task: My main intake from the experiments is that having a recurrent network processing the sensory input provides some \"memory\" to the system which reduces uncertainty when sensory data is ambiguous. This is visible from the fact that the performance from both systems is comparable at the beginning, but degrades for sensory input when the hand is open. This could be achievable in many simple ways, like modeling the classification/regression problem directly with an LSTM for example. Simpler modalities of providing a memory to the system should be used as a baseline. Conclusion: Although the idea of learning an internal representation of the world by being able to predict its state from observations is interesting, the presented paper is a) too simplistic in its experimental evaluation and b) too unclear about its implementation. Consequently, I believe the authors should improve these aspects before the article is valuable to the community ", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Although the premise of the paper is interesting , its execution is not ideal . ... it 's not clear whether the representation is anything more than trivial accumulation of sensory input '' We hope the general reply and videos help with this . `` what exactly is the `` generic cost '' C in section 7.1 ? '' `` why are both f and z parameters of C ? f is directly a function of z . Given that the form of C is not explained , seems like f could be directly computing as part of C. '' In the MPC presentation in section 7.1 we intentionally left the objective generically as C and agree that our current presentation is confusing . For example , to maximize the entropy in the nominal trajectory for exploration , the objective C is the ( negated ) Renyi entropy of the model predictions , which directly uses the GMM PDF f. You are correct that the PDF f and hidden state z need not necessarily be parameters of C. However in some cases it adds notational convenience ; the PDF f is useful for when the goal of the objective is to reach a desired state ( such as the maximum entropy , or to maximize the fingertip pressure ) , and the hidden state z is useful for when the goal of the objective extracts other information from the hidden state ( such as the type of object , or the height of the object ) We have clarified this portion in the paper . `` what is the relation between actions a in section 7.1 and u in section 4 ? '' Minimizing the objective in 7.1 over a obtains u . We agree this is probably not the best notation and we 've updated the description in 7.1 to use u and u^star instead . `` How is the minimization problem of u_ { 1 : T } solved ? '' This problem is solved using Adam with warm starts from the previous step , see our response to R1 for a full description . `` Are the authors sure that they perform gathering information through `` maximizing uncertainty '' ... '' We perform information gathering through maximizing ( and not minimizing ) uncertainty . To understand why this is the case it is important to be clear about exactly which uncertainty is being maximized , and what we are trying to gather information about . It is true that in some settings ( e.g.A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot by R Martinez-Cantin , N de Freitas , E Brochu , J Castellanos , and A Doucet in Autonomous Robots 27 ( 2 ) , 93-103 , and the many references therein ) one might choose to minimize uncertainty of the internal belief state , so as to gather information . This also arises naturally in Bayesian experimental design ( eg the highly cited work of K Chaloner ) . However , here we are concerned with the uncertainty in the predictions of the hand signals ( touch sensors , vestibular info , and proprioception ) . By seeking to maximize this uncertainty , the hand is driven to try behaviours where it is highly uncertain about what sensations it will experience ( eg what its pressure sensors will feel ) . To further emphasize this consistency , we show the outcome of instead acting to minimize the predicted entropy in Video 8 ( Figure 3 ( c ) in the paper ) . Under this objective the hand pulls itself away from the block . `` When the authors state that `` The learner trains the model by maximum likelihood '' in section 7.1 , do they refer to the prediction model or the control model ? '' The only model in this paper is the predictive model . Control is implemented as planning in the predictive model using MPC . The learner trains the predictive model , and the actors plan trajectories using an objective that depends on the predictive model , but there is no separate learned control model involved . `` What is the method for classifying and/or regressing given the features and internal representation ? '' The diagnostic models are MLPs that look at a single time step of the predictive model state . Your supposition is correct that if we use a recurrent net as the diagnostic model then there is no advantage to including the predictive model features . It would be quite surprising if this were not the case . The purpose of the diagnostic model ties back to information partitioning . The role of the diagnostic is to show that although we do not have an explicit representation of any external state in the predictive model , we nonetheless obtain such a representation implicitly ( since if this were not the case the diagnostic task would fail ) . `` Simplistic experimental task : ... '' The motivation behind our setup is that collecting the data to train the `` direct '' model can be difficult or impossible in a real life setting . The most complex part of the apparatus for the shadow hand experiment ( apart from the hand itself ) is the mechanism for measuring the angle of the object being grasped ."}, {"review_id": "r1HhRfWRZ-2", "review_text": "The authors explore how sequence models that look at proprioceptive signals from a simulated or real-world robotic hand can be used to decode properties of objects (which are not directly observed), or produce entropy maximizing or minimizing motions. The overall idea presented in the paper is quite nice: proprioception-based models that inject actions and encoder/pressure observations can be used to measure physical properties of objects that are not directly observed, and can also be used to create information gathering (or avoiding) behaviors. There is some related work that the authors do not cite that is highly relevant here. A few in particular come to mind: Yu, Tan, Liu, Turk. Preparing for the Unknown: uses a sequence model to estimate physical properties of a robot (rather than unobserved objects) Fu, Levine, Abbeel. One-Shot Learning of Manipulation Skills: trains a similar proprioception-only model and uses it for object manipulation, similar idea that object properties can be induced from proprioception But in general the citations to relevant robotic manipulation work are pretty sparse. The biggest issue with the paper though is with the results. There are no comparisons or reasonable baselines of any kind, and the reported results are a bit hard to judge. As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline. Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper! One point about style: I found the somewhat lofty claims in the introduction a bit off-putting. It's great to discuss the greater \"vision\" behind the work, but this paper suffers from a bit too much high-level vision and not enough effort put into explaining what the method actually does.", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Yu , Tan , Liu , Turk . Preparing for the Unknown ... '' `` Fu , Levine , Abbeel . One-Shot Learning of Manipulation Skills ... '' Thank you for these very relevant references . There are similarities , but important differences . The paper of Yu and colleagues uses labels of the world properties ( mu in their notation ) to pre-learn the model in a supervised way . We on the other hand aim to show that properties of the world come to be represented in a model that is only trained with body labels . While relevant the two papers are markedly different . The paper of Lu and colleagues , and in fact may of their subsequent works including guided policy search , use classical control ideas and iterate between fitting trajectories and improving control policies . We have shown that we can learn more complex models and exploration policies jointly . The works are related , but with many differences , and we feel it will be promising to explore this connection further . Thank you for pointing out this connection . `` But in general the citations to relevant robotic manipulation work are pretty sparse . '' We agree.We will address this . `` The biggest issue with the paper though is with the results . '' To demonstrate what we are referring to as \u201c awareness \u201d in this paper , we do include baselines for all of our experiments that perform the same tasks using the raw sensor state instead of the hidden state . Please see also the general response and videos , which include plots and comparisons for reference . In the pose-prediction experiment on the Shadow hand , our baseline model ( in Figure 5 ) is called \u201c sensors \u201d and tries to predict the pose of the object from only sensory information ( without the hidden state ) . This baseline also does not surpass the ~15 degrees of error , indicating that the sensor readings only contain coarse-grained information about the object and that anything better than ~15 degrees is not possible with the current setup . Our approach outperforms this baseline in most cases and shows that the hidden state of the predictive models have learned a useful representation . We will revise our paper and make the baselines more clearly labeled . `` I found the somewhat lofty claims in the introduction a bit off-putting . '' We agree.We 've modified the intro to remove some of the more lofty claims ."}], "0": {"review_id": "r1HhRfWRZ-0", "review_text": "The paper proposes an architecture for internal model learning of a robotic system and applies it to a simulated and a real robotic hand. The model allows making relatively long-term predictions with uncertainties. The models are used to perform model predictive control to achieve informative actions. It is shown that the hidden state of the learned models contains relevant information about the objects the hand was interacting with. The paper reads well. The method is sufficiently well explained and the results are presented in an illustrative and informative way. update: See critique in my comment below. I have a few minor points: - Sec 2: you may consider to cite the work on maximising predictive information as intrinsic motivation: G. Martius, R. Der, and N. Ay. Information driven self-organization of complex robotic behaviors. PLoS ONE, 8(5):e63400, 2013. - Fig 2: bottom: add labels to axis, and maybe mention that same color code as above - Sec 4 par 3: .... intentionally not autoregressive: w.r.t. to what? to the observations? - Sec 7.1: how is the optimization for the MPC performed? Which algorithm did you use and long does the optimization take? in first Eq: should f not be sampled from GMMpdf, so replace = with \\sim Typos: - Sec1 par2: This pattern has has ... - Sec 2 par2: statistics ofthe - Sec 4 line2: prefix of an episode , where (space before ,) ", "rating": "7: Good paper, accept", "reply_text": "`` - Sec 2 : you may consider to cite the work on maximising predictive information as intrinsic motivation : G. Martius , R. Der , and N. Ay . Information driven self-organization of complex robotic behaviors . PLoS ONE , 8 ( 5 ) : e63400 , 2013 . '' Thank you for the relevant reference . `` - Sec 4 par 3 : .... intentionally not autoregressive : w.r.t.to what ? to the observations ? `` Intentionally not autoregressive with respect to time . We have clarified this in the paper . `` - Sec 7.1 : how is the optimization for the MPC performed ? Which algorithm did you use and long does the optimization take ? '' We take the very naive approach of optimizing the MPC objective for a fixed number of steps by differentiating the cost function with respect to the actions and taking ( projected ) gradient steps with Adam . We project the actions into the constraint set at each step . We initialize the nominal action sequence with a burn in of 1000 Adam steps ( which is fairly time consuming ) and we take 10 additional optimization steps after executing each action and observing a response , warm started from the previous solution . This is acceptably fast for experimentation ( 2-5 steps/second after burn in ) but is substantially slower than real time , primarily due to the cost of evaluating the model. `` in first Eq : should f not be sampled from GMMpdf , so replace = with \\sim '' f is the Gaussian PDF defined by the parameters , not a sample from the PDF . We do n't need to sample from the predictive distribution in order to compute the control objective . One of the reasons we chose the Renyi entropy as the objective is that it is easy to compute analytically for the Mixture of Gaussians predictions that we make at each step ."}, "1": {"review_id": "r1HhRfWRZ-1", "review_text": "Summary: The paper describes a system which creates an internal representation of the scene given observations, being this internal representation advantageous over raw sensory input for object classification and control. The internal representation comes from a recurrent network (more specifically, a sequence2sequence net) trained to maximize the likelihood of the observations from training Positive aspects: The authors suggest an interesting hypothesis: an internal representation of the world which is useful for control could be obtained just by forcing the agent to be able to predict the outcome of its actions in the world. This hypothesis would enable robots to train it in a self-supervised manner, which would be extremely valuable. Negative aspects: Although the premise of the paper is interesting, its execution is not ideal. The formulation of the problem is unclear and difficult to follow, with a number of important terms left undefined. Moreover, the experiment task is too simplistic; from the results, it's not clear whether the representation is anything more than trivial accumulation of sensory input - Lack of clarity: -- what exactly is the \"generic cost\" C in section 7.1? -- why are both f and z parameters of C? f is directly a function of z. Given that the form of C is not explained, seems like f could be directly computing as part of C. -- what is the relation between actions a in section 7.1 and u in section 4? -- How is the minimization problem of u_{1:T} solved? -- Are the authors sure that they perform gathering information through \"maximizing uncertainty\" (section 7.1)? This sounds profoundly counterintuitive. Maximizing the uncertainty in the world state should result in minimum information about the worlds state. I would assume this is a serious typo, but cannot confirm given that the relation between the minimize cost C and the Renyi entropy H is not explicitely stated. -- When the authors state that \"The learner trains the model by maximum likelihood\" in section 7.1, do they refer to the prediction model or the control model? It would seem that it is the control model, but the objective being \"the same as in section 6\" points in the opposite direction -- What is the method for classifying and/or regressing given the features and internal representation? This is important because, if the method was a recurrent net with memory, the differences between the two representations would probably be minimal. - Simplistic experimental task: My main intake from the experiments is that having a recurrent network processing the sensory input provides some \"memory\" to the system which reduces uncertainty when sensory data is ambiguous. This is visible from the fact that the performance from both systems is comparable at the beginning, but degrades for sensory input when the hand is open. This could be achievable in many simple ways, like modeling the classification/regression problem directly with an LSTM for example. Simpler modalities of providing a memory to the system should be used as a baseline. Conclusion: Although the idea of learning an internal representation of the world by being able to predict its state from observations is interesting, the presented paper is a) too simplistic in its experimental evaluation and b) too unclear about its implementation. Consequently, I believe the authors should improve these aspects before the article is valuable to the community ", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Although the premise of the paper is interesting , its execution is not ideal . ... it 's not clear whether the representation is anything more than trivial accumulation of sensory input '' We hope the general reply and videos help with this . `` what exactly is the `` generic cost '' C in section 7.1 ? '' `` why are both f and z parameters of C ? f is directly a function of z . Given that the form of C is not explained , seems like f could be directly computing as part of C. '' In the MPC presentation in section 7.1 we intentionally left the objective generically as C and agree that our current presentation is confusing . For example , to maximize the entropy in the nominal trajectory for exploration , the objective C is the ( negated ) Renyi entropy of the model predictions , which directly uses the GMM PDF f. You are correct that the PDF f and hidden state z need not necessarily be parameters of C. However in some cases it adds notational convenience ; the PDF f is useful for when the goal of the objective is to reach a desired state ( such as the maximum entropy , or to maximize the fingertip pressure ) , and the hidden state z is useful for when the goal of the objective extracts other information from the hidden state ( such as the type of object , or the height of the object ) We have clarified this portion in the paper . `` what is the relation between actions a in section 7.1 and u in section 4 ? '' Minimizing the objective in 7.1 over a obtains u . We agree this is probably not the best notation and we 've updated the description in 7.1 to use u and u^star instead . `` How is the minimization problem of u_ { 1 : T } solved ? '' This problem is solved using Adam with warm starts from the previous step , see our response to R1 for a full description . `` Are the authors sure that they perform gathering information through `` maximizing uncertainty '' ... '' We perform information gathering through maximizing ( and not minimizing ) uncertainty . To understand why this is the case it is important to be clear about exactly which uncertainty is being maximized , and what we are trying to gather information about . It is true that in some settings ( e.g.A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot by R Martinez-Cantin , N de Freitas , E Brochu , J Castellanos , and A Doucet in Autonomous Robots 27 ( 2 ) , 93-103 , and the many references therein ) one might choose to minimize uncertainty of the internal belief state , so as to gather information . This also arises naturally in Bayesian experimental design ( eg the highly cited work of K Chaloner ) . However , here we are concerned with the uncertainty in the predictions of the hand signals ( touch sensors , vestibular info , and proprioception ) . By seeking to maximize this uncertainty , the hand is driven to try behaviours where it is highly uncertain about what sensations it will experience ( eg what its pressure sensors will feel ) . To further emphasize this consistency , we show the outcome of instead acting to minimize the predicted entropy in Video 8 ( Figure 3 ( c ) in the paper ) . Under this objective the hand pulls itself away from the block . `` When the authors state that `` The learner trains the model by maximum likelihood '' in section 7.1 , do they refer to the prediction model or the control model ? '' The only model in this paper is the predictive model . Control is implemented as planning in the predictive model using MPC . The learner trains the predictive model , and the actors plan trajectories using an objective that depends on the predictive model , but there is no separate learned control model involved . `` What is the method for classifying and/or regressing given the features and internal representation ? '' The diagnostic models are MLPs that look at a single time step of the predictive model state . Your supposition is correct that if we use a recurrent net as the diagnostic model then there is no advantage to including the predictive model features . It would be quite surprising if this were not the case . The purpose of the diagnostic model ties back to information partitioning . The role of the diagnostic is to show that although we do not have an explicit representation of any external state in the predictive model , we nonetheless obtain such a representation implicitly ( since if this were not the case the diagnostic task would fail ) . `` Simplistic experimental task : ... '' The motivation behind our setup is that collecting the data to train the `` direct '' model can be difficult or impossible in a real life setting . The most complex part of the apparatus for the shadow hand experiment ( apart from the hand itself ) is the mechanism for measuring the angle of the object being grasped ."}, "2": {"review_id": "r1HhRfWRZ-2", "review_text": "The authors explore how sequence models that look at proprioceptive signals from a simulated or real-world robotic hand can be used to decode properties of objects (which are not directly observed), or produce entropy maximizing or minimizing motions. The overall idea presented in the paper is quite nice: proprioception-based models that inject actions and encoder/pressure observations can be used to measure physical properties of objects that are not directly observed, and can also be used to create information gathering (or avoiding) behaviors. There is some related work that the authors do not cite that is highly relevant here. A few in particular come to mind: Yu, Tan, Liu, Turk. Preparing for the Unknown: uses a sequence model to estimate physical properties of a robot (rather than unobserved objects) Fu, Levine, Abbeel. One-Shot Learning of Manipulation Skills: trains a similar proprioception-only model and uses it for object manipulation, similar idea that object properties can be induced from proprioception But in general the citations to relevant robotic manipulation work are pretty sparse. The biggest issue with the paper though is with the results. There are no comparisons or reasonable baselines of any kind, and the reported results are a bit hard to judge. As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline. Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper! One point about style: I found the somewhat lofty claims in the introduction a bit off-putting. It's great to discuss the greater \"vision\" behind the work, but this paper suffers from a bit too much high-level vision and not enough effort put into explaining what the method actually does.", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Yu , Tan , Liu , Turk . Preparing for the Unknown ... '' `` Fu , Levine , Abbeel . One-Shot Learning of Manipulation Skills ... '' Thank you for these very relevant references . There are similarities , but important differences . The paper of Yu and colleagues uses labels of the world properties ( mu in their notation ) to pre-learn the model in a supervised way . We on the other hand aim to show that properties of the world come to be represented in a model that is only trained with body labels . While relevant the two papers are markedly different . The paper of Lu and colleagues , and in fact may of their subsequent works including guided policy search , use classical control ideas and iterate between fitting trajectories and improving control policies . We have shown that we can learn more complex models and exploration policies jointly . The works are related , but with many differences , and we feel it will be promising to explore this connection further . Thank you for pointing out this connection . `` But in general the citations to relevant robotic manipulation work are pretty sparse . '' We agree.We will address this . `` The biggest issue with the paper though is with the results . '' To demonstrate what we are referring to as \u201c awareness \u201d in this paper , we do include baselines for all of our experiments that perform the same tasks using the raw sensor state instead of the hidden state . Please see also the general response and videos , which include plots and comparisons for reference . In the pose-prediction experiment on the Shadow hand , our baseline model ( in Figure 5 ) is called \u201c sensors \u201d and tries to predict the pose of the object from only sensory information ( without the hidden state ) . This baseline also does not surpass the ~15 degrees of error , indicating that the sensor readings only contain coarse-grained information about the object and that anything better than ~15 degrees is not possible with the current setup . Our approach outperforms this baseline in most cases and shows that the hidden state of the predictive models have learned a useful representation . We will revise our paper and make the baselines more clearly labeled . `` I found the somewhat lofty claims in the introduction a bit off-putting . '' We agree.We 've modified the intro to remove some of the more lofty claims ."}}