{"year": "2019", "forum": "H1xwNhCcYm", "title": "Do Deep Generative Models Know What They Don't Know? ", "decision": "Accept (Poster)", "meta_review": "This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, i.e., it assigns higher probability to inputs that are out of the training distribution. This phenomenon is also shown to occur for several other dataset pairs. This finding is surprising and interesting, and the exposition is generally clear. The authors provide empirical and theoretical analysis, although based on rather strong assumptions. Overall, there's consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication.", "reviews": [{"review_id": "H1xwNhCcYm-0", "review_text": "I really enjoyed reading the paper! The exposition is clear with interesting observations, and most importantly, the authors walk the extra mile in doing a theoretical analysis of the observed phenomena. Questions for the authors: 1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled \u201cGenerative Ensembles for Robust Anomaly Detection\u201d makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10. Their criteria also accounts for the variance in model log-likelihoods and is hence slightly different. 2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test. If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram? 3. Why does the constant image (all zeros) in Figure 9 (appendix) have such a high likelihood? It\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable. 4. How much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images. 5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this. Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)", "rating": "7: Good paper, accept", "reply_text": "Thanks again , Reviewer # 1 , for your thoughtful comments . We respond to your other comments below . 1. \u201c It seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram ? \u201d This is an interesting idea , but we are not sure it is applicable . If one looks closely at Figure 2 ( b ) , there are still blue and black histogram bars ( denoting CIFAR-10 train and test instances ) covering the entirety of SVHN \u2019 s support ( red bars ) . 2. \u201c [ The constant input ] \u2019 s mean ( =0 trivially ) is clearly different from the means of the CIFAR-10 images ( Figure 6a ) so the second order analysis of Section 5 doesn \u2019 t seem applicable. \u201d See general response # 2 . 3. \u201c How much of this phenomena do you think is characteristic for images specifically ? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images. \u201d We have not tested non-image data , since images are the primary focus of work on generative models , but this is an interesting area for future work . 4. \u201c Samples from a CIFAR model look nothing like SVHN . This seems to call the validity of the anomalous into question . Curious what the authors have to say about this. \u201d This is a very good point . See our response to Shengyang Sun \u2019 s comment below . We see think this phenomenon has to do with concentration of measure and typical sets , but we do not yet have a rigorous explanation . 5. \u201c There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid : ) \u201d We have fixed the spacing in the latest draft : )"}, {"review_id": "H1xwNhCcYm-1", "review_text": " This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution. Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10. This is an interesting observation because the prevailing assumption is that density models can distinguish inliers from outliers. However, this phenomenon is not encountered when comparing MNIST and NotMNIST. The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1]. Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types. For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers. For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting. The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets. This paper is well written. I think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired. [1] Choi, H. and Jang, E. Generative Ensembles for Robust Anomaly Detection. https://arxiv.org/abs/1810.01392 Pros: - Interesting observation of density modelling shortcoming - Clear presentation Cons: - Lack of a strong explanation for the results or a solution to the problem - Lack of an extensive exploration of datasets ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks again , Reviewer # 2 , for your insightful feedback . We respond to your other comments below . 1. \u201c Why investigate a component specific to just flow-based models ( the volume term ) ? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types. \u201d See general response # 3 . 2. \u201c For instance , the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution , resulting in the simpler dataset ( SVHN ) having higher likelihood. \u201c We do not believe our models are necessarily underfit . In fact , we found that Glow had a tendency to * overfit , * and that one must carefully set Glow \u2019 s l2 penalty and choose its scale parametrization ( exp vs sigmoid , see Appendix D ) in order to prevent it from doing so . We thought this overfitting to the training data could be a reason for the phenomenon and therefore we tuned our implementations to have reasonable generalization . 3. \u201c It would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10 , so that the readers can gain a better feel for when generative models will be able to detect outliers . For instance , a scenario where the data statistics ( pixel means and variances ) are nearly equivalent for both datasets would be interesting. \u201d See general response # 1 in regards to data sets and additional results . Thank you for the suggestion of looking at data sets with similar statistics . We do this , in a way , with our second order analysis and the \u2018 gray-ing \u2019 experiment in Figure 5 ( b ) ( formerly Figure 6 ( b ) in the original draft ) . Gray CIFAR-10 ( blue dotted line ) nearly overlaps with original SVHN ( red solid line ) in terms of their log p ( x ) evaluations . Figure 12 ( formerly Figure 13 ) then shows the latent ( empirical ) distribution of the gray images , and we see that the gray CIFAR-10 latent variables nearly overlap with the SVHN latent variables . This is to be expected though , given the overlapping p ( x ) histograms , since the probability assigned by CV-Glow ( in comparison to other inputs ) is fully determined by the position in latent space . 4. \u201c The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets. \u201d See general response # 2 ."}, {"review_id": "H1xwNhCcYm-2", "review_text": "Pros: - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. - The empirical and theoretical analyses are clear, seem thorough, and make sense. - Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian). Cons: - The premises of the analyses are not very convincing, limiting the significance of the paper. - In particular, Section 4 is a series of empirical analyses, based on one dataset pair. In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain. - It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper. - Some parts of the paper feel long-winded and aimless. [Quality] See above pros and cons. A few less important disagreement I have with the paper: - I don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large. - Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar. [Clarity] In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded. Section 2 background takes too much space. Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment. Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment. A few editorial issues: - On page 4 footnote 2, as far as I know the paper did not define BPD. - There are two lines of text between Fig. 4 and Fig. 5, which is confusing. [Originality] I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel. However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better? ^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried. A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images). [Significance] The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly. However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis. According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that \"lies within\" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test? Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much. ", "rating": "7: Good paper, accept", "reply_text": "Thanks again , Reviewer # 3 , for your thought-provoking critique . We respond to your other comments below . 1. \u201c In particular , Section 4 is a series of empirical analyses , based on one dataset pair\u2026.However , only 1 dataset pair is experimented -- there should be more to ensure the findings generalize , since Sections 3 and 4 rely completely on empirical analysis. \u201d See general responses # 1 and # 3 . 2. \u201c It is good that Section 5 has some theoretical analysis . But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images . At least this limitation should be pointed out in the paper\u2026.Section 5 is based on a 2nd order expansion on the $ log p ( x ) $ given by a deep network -- I should n't be the judge of this , but from a realistic perspective this does not mean much. \u201d See general response # 2 . We emphasize that we are not trying to approximate the density function , only approximate the difference and characterize its sign . Moreover , the special structure of CV-Glow makes these derivative-based approximations better behaved and more tractable than an expansion of a generic deep neural network . 3. \u201c Some parts of the paper feel long-winded and aimless\u2026.In general , the paper is clear and easy to understand given enough reading time , but feels at times long-winded . Section 2 background takes too much space . Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR , and a few variations of the same experiment. \u201d We will attempt to make the writing more concise . But we believe that most , if not all , of Section 2 is necessary in order to make the paper self-contained and accessible to someone who has never before seen invertible generative models . While we are fastidious in our experimental description in Section 3 , we think it is necessary since this is the foundational section of the paper . 4. \u201c I do n't think Glow necessarily is encouraged to increase sensitivity to perturbations . The bijection needs to map training images to a high-density region of the Gaussian , and that aspect would make the model think twice before making the volume term too large. \u201d We are not saying that the model will totally disregard the latent density and attempt to scale the input to very large or infinite values . Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective . The log volume term in the change-of-variable objective is maximizing the very quantity ( the Jacobian \u2019 s diagonal terms ) that the cited work on derivative-based regularization penalties has sought to minimize . The maximization of the derivatives in the objective directly implies increased sensitivity to perturbations . 5. \u201c Figure 6 ( a ) [ Figure 5 ( a ) in revised draft ] clearly suggests that the data mean for SVHN and CIFAR are very different , instead of similar. \u201d We are not sure how you are drawing this conclusion ; perhaps from the scale of the x-axis ? The histogram in Figure 6 ( a ) ( original draft ) has an x-axis covering the interval [ 0.4 , 0.55 ] , meaning the maximal difference between a mean in * any pair of dimensions * is 0.15 . Scaling back to pixel units , 0.15 * 255 = 38.25 , meaning that 38.25 pixels is the maximum difference in means . While this is not a difference of zero , we don \u2019 t see how you could say this \u201c clearly suggests \u201d that the means are \u201c very different. \u201d In the latest draft , this figure -- -now Fig 5 ( a ) -- -has an x-axis that spans from 0-255 . Hopefully the overlap in the means in now conspicuous . 6. \u201c However , there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite : V\u00edt \u0160kv\u00e1ra et al.Are generative deep models for novelty detection truly better ? at first glance , their AUROC is never under 0.5 , indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried. \u201d Thank you for pointing us to this work . We cite it in the revised draft . It looks like they test on UCI data sets of dimensionality less than 200 , and therefore their results speak to a much different data regime than the one we are studying . 7. \u201c A part of the paper 's contribution ( section 5 conclusion ) seem to overlap with others ' work . The section concludes that if the second dataset has small variances , it will get higher likelihood . But this is too similar to the cited findings on page 6 ( models assign high likelihood to constant images ) . \u201d While we do also analyze constant images , we believe that our results for multiple data set pairs ( FashionMNIST-MNIST , CIFAR10-SVHN , CelebA-SVHN , ImageNet-CIFAR10/CIFAR100/SVHN ) and for multiple deep generative models ( flow-based models , VAE , PixelCNN ) is novel . Our conclusions are arrived at through focused experimentation and a novel analytical expression applied to CV-Glow ."}], "0": {"review_id": "H1xwNhCcYm-0", "review_text": "I really enjoyed reading the paper! The exposition is clear with interesting observations, and most importantly, the authors walk the extra mile in doing a theoretical analysis of the observed phenomena. Questions for the authors: 1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled \u201cGenerative Ensembles for Robust Anomaly Detection\u201d makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10. Their criteria also accounts for the variance in model log-likelihoods and is hence slightly different. 2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test. If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram? 3. Why does the constant image (all zeros) in Figure 9 (appendix) have such a high likelihood? It\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable. 4. How much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images. 5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this. Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)", "rating": "7: Good paper, accept", "reply_text": "Thanks again , Reviewer # 1 , for your thoughtful comments . We respond to your other comments below . 1. \u201c It seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram ? \u201d This is an interesting idea , but we are not sure it is applicable . If one looks closely at Figure 2 ( b ) , there are still blue and black histogram bars ( denoting CIFAR-10 train and test instances ) covering the entirety of SVHN \u2019 s support ( red bars ) . 2. \u201c [ The constant input ] \u2019 s mean ( =0 trivially ) is clearly different from the means of the CIFAR-10 images ( Figure 6a ) so the second order analysis of Section 5 doesn \u2019 t seem applicable. \u201d See general response # 2 . 3. \u201c How much of this phenomena do you think is characteristic for images specifically ? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images. \u201d We have not tested non-image data , since images are the primary focus of work on generative models , but this is an interesting area for future work . 4. \u201c Samples from a CIFAR model look nothing like SVHN . This seems to call the validity of the anomalous into question . Curious what the authors have to say about this. \u201d This is a very good point . See our response to Shengyang Sun \u2019 s comment below . We see think this phenomenon has to do with concentration of measure and typical sets , but we do not yet have a rigorous explanation . 5. \u201c There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid : ) \u201d We have fixed the spacing in the latest draft : )"}, "1": {"review_id": "H1xwNhCcYm-1", "review_text": " This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution. Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10. This is an interesting observation because the prevailing assumption is that density models can distinguish inliers from outliers. However, this phenomenon is not encountered when comparing MNIST and NotMNIST. The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1]. Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types. For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers. For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting. The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets. This paper is well written. I think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired. [1] Choi, H. and Jang, E. Generative Ensembles for Robust Anomaly Detection. https://arxiv.org/abs/1810.01392 Pros: - Interesting observation of density modelling shortcoming - Clear presentation Cons: - Lack of a strong explanation for the results or a solution to the problem - Lack of an extensive exploration of datasets ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks again , Reviewer # 2 , for your insightful feedback . We respond to your other comments below . 1. \u201c Why investigate a component specific to just flow-based models ( the volume term ) ? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types. \u201d See general response # 3 . 2. \u201c For instance , the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution , resulting in the simpler dataset ( SVHN ) having higher likelihood. \u201c We do not believe our models are necessarily underfit . In fact , we found that Glow had a tendency to * overfit , * and that one must carefully set Glow \u2019 s l2 penalty and choose its scale parametrization ( exp vs sigmoid , see Appendix D ) in order to prevent it from doing so . We thought this overfitting to the training data could be a reason for the phenomenon and therefore we tuned our implementations to have reasonable generalization . 3. \u201c It would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10 , so that the readers can gain a better feel for when generative models will be able to detect outliers . For instance , a scenario where the data statistics ( pixel means and variances ) are nearly equivalent for both datasets would be interesting. \u201d See general response # 1 in regards to data sets and additional results . Thank you for the suggestion of looking at data sets with similar statistics . We do this , in a way , with our second order analysis and the \u2018 gray-ing \u2019 experiment in Figure 5 ( b ) ( formerly Figure 6 ( b ) in the original draft ) . Gray CIFAR-10 ( blue dotted line ) nearly overlaps with original SVHN ( red solid line ) in terms of their log p ( x ) evaluations . Figure 12 ( formerly Figure 13 ) then shows the latent ( empirical ) distribution of the gray images , and we see that the gray CIFAR-10 latent variables nearly overlap with the SVHN latent variables . This is to be expected though , given the overlapping p ( x ) histograms , since the probability assigned by CV-Glow ( in comparison to other inputs ) is fully determined by the position in latent space . 4. \u201c The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets. \u201d See general response # 2 ."}, "2": {"review_id": "H1xwNhCcYm-2", "review_text": "Pros: - The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. - The empirical and theoretical analyses are clear, seem thorough, and make sense. - Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian). Cons: - The premises of the analyses are not very convincing, limiting the significance of the paper. - In particular, Section 4 is a series of empirical analyses, based on one dataset pair. In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain. - It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper. - Some parts of the paper feel long-winded and aimless. [Quality] See above pros and cons. A few less important disagreement I have with the paper: - I don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large. - Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar. [Clarity] In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded. Section 2 background takes too much space. Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment. Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment. A few editorial issues: - On page 4 footnote 2, as far as I know the paper did not define BPD. - There are two lines of text between Fig. 4 and Fig. 5, which is confusing. [Originality] I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel. However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better? ^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried. A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images). [Significance] The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly. However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis. According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that \"lies within\" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test? Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much. ", "rating": "7: Good paper, accept", "reply_text": "Thanks again , Reviewer # 3 , for your thought-provoking critique . We respond to your other comments below . 1. \u201c In particular , Section 4 is a series of empirical analyses , based on one dataset pair\u2026.However , only 1 dataset pair is experimented -- there should be more to ensure the findings generalize , since Sections 3 and 4 rely completely on empirical analysis. \u201d See general responses # 1 and # 3 . 2. \u201c It is good that Section 5 has some theoretical analysis . But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images . At least this limitation should be pointed out in the paper\u2026.Section 5 is based on a 2nd order expansion on the $ log p ( x ) $ given by a deep network -- I should n't be the judge of this , but from a realistic perspective this does not mean much. \u201d See general response # 2 . We emphasize that we are not trying to approximate the density function , only approximate the difference and characterize its sign . Moreover , the special structure of CV-Glow makes these derivative-based approximations better behaved and more tractable than an expansion of a generic deep neural network . 3. \u201c Some parts of the paper feel long-winded and aimless\u2026.In general , the paper is clear and easy to understand given enough reading time , but feels at times long-winded . Section 2 background takes too much space . Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR , and a few variations of the same experiment. \u201d We will attempt to make the writing more concise . But we believe that most , if not all , of Section 2 is necessary in order to make the paper self-contained and accessible to someone who has never before seen invertible generative models . While we are fastidious in our experimental description in Section 3 , we think it is necessary since this is the foundational section of the paper . 4. \u201c I do n't think Glow necessarily is encouraged to increase sensitivity to perturbations . The bijection needs to map training images to a high-density region of the Gaussian , and that aspect would make the model think twice before making the volume term too large. \u201d We are not saying that the model will totally disregard the latent density and attempt to scale the input to very large or infinite values . Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective . The log volume term in the change-of-variable objective is maximizing the very quantity ( the Jacobian \u2019 s diagonal terms ) that the cited work on derivative-based regularization penalties has sought to minimize . The maximization of the derivatives in the objective directly implies increased sensitivity to perturbations . 5. \u201c Figure 6 ( a ) [ Figure 5 ( a ) in revised draft ] clearly suggests that the data mean for SVHN and CIFAR are very different , instead of similar. \u201d We are not sure how you are drawing this conclusion ; perhaps from the scale of the x-axis ? The histogram in Figure 6 ( a ) ( original draft ) has an x-axis covering the interval [ 0.4 , 0.55 ] , meaning the maximal difference between a mean in * any pair of dimensions * is 0.15 . Scaling back to pixel units , 0.15 * 255 = 38.25 , meaning that 38.25 pixels is the maximum difference in means . While this is not a difference of zero , we don \u2019 t see how you could say this \u201c clearly suggests \u201d that the means are \u201c very different. \u201d In the latest draft , this figure -- -now Fig 5 ( a ) -- -has an x-axis that spans from 0-255 . Hopefully the overlap in the means in now conspicuous . 6. \u201c However , there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite : V\u00edt \u0160kv\u00e1ra et al.Are generative deep models for novelty detection truly better ? at first glance , their AUROC is never under 0.5 , indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried. \u201d Thank you for pointing us to this work . We cite it in the revised draft . It looks like they test on UCI data sets of dimensionality less than 200 , and therefore their results speak to a much different data regime than the one we are studying . 7. \u201c A part of the paper 's contribution ( section 5 conclusion ) seem to overlap with others ' work . The section concludes that if the second dataset has small variances , it will get higher likelihood . But this is too similar to the cited findings on page 6 ( models assign high likelihood to constant images ) . \u201d While we do also analyze constant images , we believe that our results for multiple data set pairs ( FashionMNIST-MNIST , CIFAR10-SVHN , CelebA-SVHN , ImageNet-CIFAR10/CIFAR100/SVHN ) and for multiple deep generative models ( flow-based models , VAE , PixelCNN ) is novel . Our conclusions are arrived at through focused experimentation and a novel analytical expression applied to CV-Glow ."}}