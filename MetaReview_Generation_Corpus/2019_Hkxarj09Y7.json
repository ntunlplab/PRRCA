{"year": "2019", "forum": "Hkxarj09Y7", "title": "Unified recurrent network for many feature types", "decision": "Reject", "meta_review": "This paper presents an algorithm for combining various feature types when training recurrent networks. The features are handled by modifying the update rules and cell states based on the features' type -- dense, sparse, static, w/ decay, etc.\n\nStrengths\n- The model handles each feature according to its type and handles cell state and transitions appropriately. \n- Extends earlier work to handle more feature types, like sparse features.\n\nWeaknesses\n- Limited novelty. Models similar to various aspects of the proposed system have been presented in prior works. For example: TLSTM, which the authors use as a baseline. Although some components are novel, like the treatment of sparse features, contributions, in my opinion, are not sufficient to be accepted at ICLR.\n- Presentation: Confusing and not enough information for reproducing results; multiple reviewers raised concerns about presentation of the feature types and experimental results. There were suggestions to improve, which the authors did consider during revision, but some concerns still remain.\n\nIn the end, the reviewers agreed about the limited novelty of this work, given existing literature. The recommendation, therefore, is to reject the paper. \n", "reviews": [{"review_id": "Hkxarj09Y7-0", "review_text": "This paper proposes a new type of recurrent neural network which takes into account five different features: in addition to the prevalent dense features, the author(s) also consider(s) sparse features, time features, global static and changing features. The differences in feature types are reflected in the cell state or output state update rules. Experiments on a modified UCI dataset and a proprietary dataset show that the proposed model outperforms the time-variant LSTM (TLSTM). Pros: 1. By decomposing the cell state into different components for different feature types (dense vs sparse, short term vs long term) and update them in different manners, the model takes advantage of the feature type information. 2. By updating sparse feature related cell states only when sparse features are present, it could be potentially computationally cheaper than treating everything as dense (although in the paper due to more parameters the proposed model is actually slower). Cons: 1. The contributions are not significant. It seems that TLSTM already \"accounts for asynchronous feature sampling\" (Sec 1) and the novelty here lies most in how sparse features are treated. 2. The presentation is sometimes confusing. For example, in Figure 5 which presents the main results, what's the \"relative change in F1 score\" and what's the unit in the plot? If it's percentage the gains seem to be too small and could be potentially due to the additional parameters. Besides, what does \"group sampling\" mean exactly? Furthermore, legend seems to be missing. 3. Crucial implementation details are missing. The paper mentions that \"the number of features grows\" in the proposed model. Are sparse features and static features used or not in TLSTM? 4. What's the difference between a static decay feature (Sec 3.5) and decay features (Sec 3.2)? Isn't the static decay feature varying with time as well? Minor comments: 1. Figure 1 and 5 are too small and hard to read. 2. Sec 3.3, \"updated based on Equation 1 and Equation 2\", but none of the equations are numbered in this paper. 3. Some discussions on the proprietary dataset seem to be irrelevant. I'd rather see how are sparse features generated for the UCI dataset. 4. The decay function $g= 1 / log (e + \\alpha^T x_t^{\\delta})$, how can we make sure that as time passes it decreases as time passes? Overall, I think explicitly taking into account different feature types in the LSTM cell update rules is interesting, but the contributions of this paper compared to TLSTM are not significant enough for acceptance, and the presentation can be made more clear.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review and helpful comments . We have responded to your concerns below . Con 1 : TLSTM accounts more for irregular time between samples as opposed to asynchronous feature sampling , where different features are sampled at every time step . The modification of TLSTM updates is an expansion on that work , but the inclusion of sparse features tackles a completely different problem than TLSTM and in our opinion is novel . Con 2 : The plots do show the percentage gains in score as you guessed . We can modify the figures to make this clearer . We could also include a legend showing which line corresponds to which feature , but it \u2019 s not clear how much value this would provide . While there is a disparity in the number of parameters between the models , additional experiments showed that the base model did not improve by adding more parameters . We will add this information to the text . All information on sequence processing for the power consumption data set can be found in the Appendix . Con 3 : The quoted part should read \u201c the number of sparse features grows \u201d and will be updated . Sparse features are used in TLSTM but are simply treated as dense features . We can clarify this in the text . In Section 4.1 we mention that static features are present for all experiments , with the exception of the Table 1b which investigates their effectiveness . Con 4 : Static decay and decay features are very similar , however as stated in Section 3.5 \u201c the static decay features apply to the sequence as a whole rather than an individual time step. \u201d A static decay feature does vary with time and could be the time elapsed between the final time step of the sequence and the prediction time being used for that sequence . Minor comment 1 : We can make the figure/fonts bigger as the length restrictions allow . Minor comment 2 : There are labels for groups of equations on Page 4 . Minor comment 3 : More details can be found in the Appendix . Minor comment 4 : We restrict this with \\alpha \\geq 0 so that the function is non-increasing . This restraint will be added to the text ."}, {"review_id": "Hkxarj09Y7-1", "review_text": "The paper addresses limitations of the LTSM method for modeling time series. The work is motivated by applications where multiple time series need to be combined while they may get updated in an asynchronous fashion. Authors mention IoT applications in the Intro and give examples from a power consumption data set and a churn prediction application in their numerical experiments sections. The paper's main contribution is to shrink down time series into 5 different categories which have different sampling schemes: (1) dense (2) sparse (3) decay (4) static decay (3) static standard, and proposing building blocks to incorporate these in a unified recurrent mechanism. The paper's motivation for introducing sparse input to LTSM was rather straightforward and convincing, and the churn prediction application is an excellent motivation for this. I had a harder time following why the 3 other types of features needed to be included as well at this point. Perhaps a more problem oriented explanation with concrete examples could have helped. Or those features should not yet be used as generalizations but kept for future work. Overall I think the paper has several interesting ideas. Even though not all are as convincing, I think the paper is thought provoking and may interest the ICLR community.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments , we are glad that you found the paper thought provoking . In light of your comments about the motivation , we will add explicit examples for each subsection in Section 3.5 to make this clearer , but our churn dataset did in fact support all five data types . Examples of each type for the power consumption dataset can be found in the Appendix . Although some of these features were synthetically created to mirror our churn dataset , they can still give some insight to the need for each feature . A decay feature such as the time between timesteps allows us to take into account the nonuniformity between events . For a static decay feature we used the time between the last event and the prediction time . The intention is to remove short term contributions in the prediction when there is a long-time gap after the final event . Static decay features for the power consumption data set are day of week , day of month , and time of day as these feature are informative but are relevant on a sequence level but not on a time step level ."}, {"review_id": "Hkxarj09Y7-2", "review_text": "Summary ======== The paper addresses the problem of irregular spacing in sequential data with five different irregularity types. The solution is based on modifying LSTM cell. Comment ======== Irregular and multiple spacing presents in many real world applications with time-stamped events. The problem is therefore important to address properly. However, I found the evaluation of the proposed solution is less convincing. The main results in Figure 5 are not informative. There have been related works addressing irregular and multiple spacing (not just missing data as cited in the paper): - Pham, T., Tran, T., Phung, D., & Venkatesh, S. (2016, April). DeepCare: A deep dynamic memory model for predictive medicine. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 30-41). Springer, Cham. - Koutnik, J., Greff, K., Gomez, F., & Schmidhuber, J. (2014). A clockwork RNN. arXiv preprint arXiv:1402.3511. - Chen, C., Kim, S., Bui, H., Rossi, R., Koh, E., Kveton, B., & Bunescu, R. (2018, October). Predictive Analysis by Leveraging Temporal User Behavior and User Embeddings. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 2175-2182). ACM. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would appreciate receiving further comments and details regarding why the proposed solution is not convincing , and why the main results are not informative ? Especially since we see our model performing better under most circumstances ."}, {"review_id": "Hkxarj09Y7-3", "review_text": "[Relevance] Is this paper relevant to the ICLR audience? yes [Significance] Are the results significant? somewhat [Novelty] Are the problems or approaches novel? reasonably [Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations. [Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat [Clarity] Is the paper well-organized and clearly written? yes, except the experiments Confidence: 2/5 Seen submission posted elsewhere: No (but I did find it on arXiv after writing the review) Detailed comments: In this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (\u201cnormal\u201d RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases. === Comments I found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the \u201cnon-decaying time series\u201d observations. However, I had difficult coming up with an example of a \u201cstatic decay feature\u201d. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a \u201cstatic decay\u201d feature rather than just a \u201cstatic\u201d feature.) My main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While \u201cmeaningfulness\u201d may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting. === Typos, etc. The plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and your detailed comments . Below are our responses to some of your concerns . Comment : However , I had difficult coming up with an example of a \u201c static decay feature \u201d . It would be helpful to give a concrete example of one of these in the main text . ( It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a \u201c static decay \u201d feature rather than just a \u201c static \u201d feature . ) It seems that giving examples of each feature type in the subsections of Section 3 would help explain the motivation for incorporating each type of a feature . There is one \u2018 static decay \u2019 feature mentioned in the Appendix , and as you mentioned it is the time between the last event and the prediction time . This is treated differently from \u2018 static \u2019 features for the same reason \u2018 decay \u2019 features are not \u2018 dense \u2019 features , i.e. , they are used to decompose the output of the RNN network ( or the memory state for \u2018 decay \u2019 features ) into short and long-term components . The intention is to remove short term contributions in the prediction when there is a long-time gap . Comment : First , the sparsity mechanism is rather simple . In many domains ( e.g. , the medical domain considered in several of the cited papers ) , missingness is non-uniform and is often meaningful . While \u201c meaningfulness \u201d may be difficult to simulate , burstiness ( non-uniformity ) could be simulated . There are actually two sampling mechanisms for the power consumption dataset , random and group sampling , which are detailed in the appendix . We agree that the random method is rather simple and so we also included group sampling to simulate the \u2018 burstiness \u2019 you mention . While this may still be missing some information that can not be simulated easily , we can say that our churn dataset did exhibit natural non-uniformity . Comment : Second , for the groups , it is not clear whether all combinations of , e.g. , 2 ( informative ) feature were sparsely sampled or if only one group of 2 was chosen . If the former , then some measure of variance should be given to help estimate statistical significance . It was only one subset of the two , determined by which features most improved predictions individually that are treated as sparse . This clarification will be added to the manuscript . Comment : Third , the particular classification task here is , essentially , forecasting one of the input variables . While that is certainly a relevant problem , many other time series classification or regression problems are not tied so directly to observations . It is not clear if these results are relevant in that setting . While we can not share much detail on the churn dataset , we can reveal that the labels are not tied directly to the input features . In churn , the prediction is if a customer is likely to churn while the features are for example customer interactions with the company . So the predictions are completely decoupled from input features . Despite this we still saw performance gains for our model over the base model . The tie mentioned in your comment is present in the power dataset since the data does not include other possible labels ."}], "0": {"review_id": "Hkxarj09Y7-0", "review_text": "This paper proposes a new type of recurrent neural network which takes into account five different features: in addition to the prevalent dense features, the author(s) also consider(s) sparse features, time features, global static and changing features. The differences in feature types are reflected in the cell state or output state update rules. Experiments on a modified UCI dataset and a proprietary dataset show that the proposed model outperforms the time-variant LSTM (TLSTM). Pros: 1. By decomposing the cell state into different components for different feature types (dense vs sparse, short term vs long term) and update them in different manners, the model takes advantage of the feature type information. 2. By updating sparse feature related cell states only when sparse features are present, it could be potentially computationally cheaper than treating everything as dense (although in the paper due to more parameters the proposed model is actually slower). Cons: 1. The contributions are not significant. It seems that TLSTM already \"accounts for asynchronous feature sampling\" (Sec 1) and the novelty here lies most in how sparse features are treated. 2. The presentation is sometimes confusing. For example, in Figure 5 which presents the main results, what's the \"relative change in F1 score\" and what's the unit in the plot? If it's percentage the gains seem to be too small and could be potentially due to the additional parameters. Besides, what does \"group sampling\" mean exactly? Furthermore, legend seems to be missing. 3. Crucial implementation details are missing. The paper mentions that \"the number of features grows\" in the proposed model. Are sparse features and static features used or not in TLSTM? 4. What's the difference between a static decay feature (Sec 3.5) and decay features (Sec 3.2)? Isn't the static decay feature varying with time as well? Minor comments: 1. Figure 1 and 5 are too small and hard to read. 2. Sec 3.3, \"updated based on Equation 1 and Equation 2\", but none of the equations are numbered in this paper. 3. Some discussions on the proprietary dataset seem to be irrelevant. I'd rather see how are sparse features generated for the UCI dataset. 4. The decay function $g= 1 / log (e + \\alpha^T x_t^{\\delta})$, how can we make sure that as time passes it decreases as time passes? Overall, I think explicitly taking into account different feature types in the LSTM cell update rules is interesting, but the contributions of this paper compared to TLSTM are not significant enough for acceptance, and the presentation can be made more clear.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review and helpful comments . We have responded to your concerns below . Con 1 : TLSTM accounts more for irregular time between samples as opposed to asynchronous feature sampling , where different features are sampled at every time step . The modification of TLSTM updates is an expansion on that work , but the inclusion of sparse features tackles a completely different problem than TLSTM and in our opinion is novel . Con 2 : The plots do show the percentage gains in score as you guessed . We can modify the figures to make this clearer . We could also include a legend showing which line corresponds to which feature , but it \u2019 s not clear how much value this would provide . While there is a disparity in the number of parameters between the models , additional experiments showed that the base model did not improve by adding more parameters . We will add this information to the text . All information on sequence processing for the power consumption data set can be found in the Appendix . Con 3 : The quoted part should read \u201c the number of sparse features grows \u201d and will be updated . Sparse features are used in TLSTM but are simply treated as dense features . We can clarify this in the text . In Section 4.1 we mention that static features are present for all experiments , with the exception of the Table 1b which investigates their effectiveness . Con 4 : Static decay and decay features are very similar , however as stated in Section 3.5 \u201c the static decay features apply to the sequence as a whole rather than an individual time step. \u201d A static decay feature does vary with time and could be the time elapsed between the final time step of the sequence and the prediction time being used for that sequence . Minor comment 1 : We can make the figure/fonts bigger as the length restrictions allow . Minor comment 2 : There are labels for groups of equations on Page 4 . Minor comment 3 : More details can be found in the Appendix . Minor comment 4 : We restrict this with \\alpha \\geq 0 so that the function is non-increasing . This restraint will be added to the text ."}, "1": {"review_id": "Hkxarj09Y7-1", "review_text": "The paper addresses limitations of the LTSM method for modeling time series. The work is motivated by applications where multiple time series need to be combined while they may get updated in an asynchronous fashion. Authors mention IoT applications in the Intro and give examples from a power consumption data set and a churn prediction application in their numerical experiments sections. The paper's main contribution is to shrink down time series into 5 different categories which have different sampling schemes: (1) dense (2) sparse (3) decay (4) static decay (3) static standard, and proposing building blocks to incorporate these in a unified recurrent mechanism. The paper's motivation for introducing sparse input to LTSM was rather straightforward and convincing, and the churn prediction application is an excellent motivation for this. I had a harder time following why the 3 other types of features needed to be included as well at this point. Perhaps a more problem oriented explanation with concrete examples could have helped. Or those features should not yet be used as generalizations but kept for future work. Overall I think the paper has several interesting ideas. Even though not all are as convincing, I think the paper is thought provoking and may interest the ICLR community.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments , we are glad that you found the paper thought provoking . In light of your comments about the motivation , we will add explicit examples for each subsection in Section 3.5 to make this clearer , but our churn dataset did in fact support all five data types . Examples of each type for the power consumption dataset can be found in the Appendix . Although some of these features were synthetically created to mirror our churn dataset , they can still give some insight to the need for each feature . A decay feature such as the time between timesteps allows us to take into account the nonuniformity between events . For a static decay feature we used the time between the last event and the prediction time . The intention is to remove short term contributions in the prediction when there is a long-time gap after the final event . Static decay features for the power consumption data set are day of week , day of month , and time of day as these feature are informative but are relevant on a sequence level but not on a time step level ."}, "2": {"review_id": "Hkxarj09Y7-2", "review_text": "Summary ======== The paper addresses the problem of irregular spacing in sequential data with five different irregularity types. The solution is based on modifying LSTM cell. Comment ======== Irregular and multiple spacing presents in many real world applications with time-stamped events. The problem is therefore important to address properly. However, I found the evaluation of the proposed solution is less convincing. The main results in Figure 5 are not informative. There have been related works addressing irregular and multiple spacing (not just missing data as cited in the paper): - Pham, T., Tran, T., Phung, D., & Venkatesh, S. (2016, April). DeepCare: A deep dynamic memory model for predictive medicine. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 30-41). Springer, Cham. - Koutnik, J., Greff, K., Gomez, F., & Schmidhuber, J. (2014). A clockwork RNN. arXiv preprint arXiv:1402.3511. - Chen, C., Kim, S., Bui, H., Rossi, R., Koh, E., Kveton, B., & Bunescu, R. (2018, October). Predictive Analysis by Leveraging Temporal User Behavior and User Embeddings. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 2175-2182). ACM. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would appreciate receiving further comments and details regarding why the proposed solution is not convincing , and why the main results are not informative ? Especially since we see our model performing better under most circumstances ."}, "3": {"review_id": "Hkxarj09Y7-3", "review_text": "[Relevance] Is this paper relevant to the ICLR audience? yes [Significance] Are the results significant? somewhat [Novelty] Are the problems or approaches novel? reasonably [Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations. [Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat [Clarity] Is the paper well-organized and clearly written? yes, except the experiments Confidence: 2/5 Seen submission posted elsewhere: No (but I did find it on arXiv after writing the review) Detailed comments: In this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (\u201cnormal\u201d RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases. === Comments I found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the \u201cnon-decaying time series\u201d observations. However, I had difficult coming up with an example of a \u201cstatic decay feature\u201d. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a \u201cstatic decay\u201d feature rather than just a \u201cstatic\u201d feature.) My main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While \u201cmeaningfulness\u201d may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting. === Typos, etc. The plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and your detailed comments . Below are our responses to some of your concerns . Comment : However , I had difficult coming up with an example of a \u201c static decay feature \u201d . It would be helpful to give a concrete example of one of these in the main text . ( It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a \u201c static decay \u201d feature rather than just a \u201c static \u201d feature . ) It seems that giving examples of each feature type in the subsections of Section 3 would help explain the motivation for incorporating each type of a feature . There is one \u2018 static decay \u2019 feature mentioned in the Appendix , and as you mentioned it is the time between the last event and the prediction time . This is treated differently from \u2018 static \u2019 features for the same reason \u2018 decay \u2019 features are not \u2018 dense \u2019 features , i.e. , they are used to decompose the output of the RNN network ( or the memory state for \u2018 decay \u2019 features ) into short and long-term components . The intention is to remove short term contributions in the prediction when there is a long-time gap . Comment : First , the sparsity mechanism is rather simple . In many domains ( e.g. , the medical domain considered in several of the cited papers ) , missingness is non-uniform and is often meaningful . While \u201c meaningfulness \u201d may be difficult to simulate , burstiness ( non-uniformity ) could be simulated . There are actually two sampling mechanisms for the power consumption dataset , random and group sampling , which are detailed in the appendix . We agree that the random method is rather simple and so we also included group sampling to simulate the \u2018 burstiness \u2019 you mention . While this may still be missing some information that can not be simulated easily , we can say that our churn dataset did exhibit natural non-uniformity . Comment : Second , for the groups , it is not clear whether all combinations of , e.g. , 2 ( informative ) feature were sparsely sampled or if only one group of 2 was chosen . If the former , then some measure of variance should be given to help estimate statistical significance . It was only one subset of the two , determined by which features most improved predictions individually that are treated as sparse . This clarification will be added to the manuscript . Comment : Third , the particular classification task here is , essentially , forecasting one of the input variables . While that is certainly a relevant problem , many other time series classification or regression problems are not tied so directly to observations . It is not clear if these results are relevant in that setting . While we can not share much detail on the churn dataset , we can reveal that the labels are not tied directly to the input features . In churn , the prediction is if a customer is likely to churn while the features are for example customer interactions with the company . So the predictions are completely decoupled from input features . Despite this we still saw performance gains for our model over the base model . The tie mentioned in your comment is present in the power dataset since the data does not include other possible labels ."}}