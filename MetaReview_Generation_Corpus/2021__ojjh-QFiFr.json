{"year": "2021", "forum": "_ojjh-QFiFr", "title": "Language-Mediated, Object-Centric Representation Learning", "decision": "Reject", "meta_review": "While the paper's topic is on a topic of interest and presents an evaluation on three synthetic datasets,  PartNet-Chairs, Shop-VRB-Simple, CLEVR dataset, several concerns and weaknesses remain after the author response.\n\nMain Concern and Weaknesses:\n* The main improvement comes from the additional supervision provided by language, which provides a strong supervision signal as the language is scripted and the parser has nearly \"perfect accuracy (>99.9%) on test questions/captions\", as the authors state.\n* Limited contribution: combination of MONet/Slot-Attention with NS-CL; \n* Experiments limited to synthetic images with no background (relatively easy to segment) and synthetic (templated) language (easy to parse). This is especially concerning when the task is segmentation and the supervision comes from templated language, making it a strong supervision signal.\n* The positive impact of the objectness score on performance was not sufficiently demonstrated\n* Additionally, in the final discussion phase, reviewers were concerned that the with limited visual reasoning training on a subset of 25% of the data, reduces the performance [I note that I did not take this as the decisive reason for rejection as the authors did not have a chance to respond to this concern but the authors should discuss this in any future version of the paper]\n\nThe paper initially received borderline and reject scores and the authors took significant effort to address several of the comments of reviewers. While the paper was improved several of the main concerns remained and reviewers recommended reject after reading the author response and each others comments.\n\nI agree to the concerns and recommend reject.", "reviews": [{"review_id": "_ojjh-QFiFr-0", "review_text": "# # # Summary This paper proposes to combine the neuro-symbolic concept learner for visual reasoning from language ( NS-CL ; Mao et al. , 2019 ) with recent unsupervised approaches to learning object-centric representations such as MONet ( Burgess et al. , 2019 ) and Slot-Attention ( Locatello et al. , 2020 ) . While NS-CL normally relies on pre-trained object-detectors ( in a supervised fashion ) to extract visual representations , the proposed combination ( dubbed LORL ) use MONet or Slot Attention for this . By additionally back-propagating error signals from language-driven visual reasoning tasks obtained via NS-CL into MONet/Slot-Attention , it is shown how LORL is better able at learning object-centric representations and perform instance segmentation . # # # Pro \u2019 s / Con \u2019 s / Justification Overall the paper is well written and easy to follow . Moreover , using other modalities such as language to improve visual perception is an interesting research direction and timely due to recent advances in object-centric representation learning However , the significance of this contribution is rather limited due to two main reasons : LORL is a very straightforward combination of MONet/Slot-Attention with NS-CL and therefore not very novel the encoder part of NS-CL is simply replaced by the MONet/Slot-Attention , while everything else remains pretty much the same . Note that I don \u2019 t consider the \u201c objectness scores \u201d to modulate the NS-CL reasoning process very novel , since it is essentially a straight-forward heuristic to cope with a limitation of MONet/Slot-Attention in that sometimes the object-centric representations may containing background information or are \u201c empty \u201d . Were object-centric representations correctly inferred ( as in the original NS-CL ) , this heuristic would therefore also not be needed The results are not very surprising : it is found that better object-centric representations can be learned by fine-tuning on a visual reasoning task that uses language . However , since this task is learned in a supervised fashion and the dataset contains questions of the form \u201c what is the name of the white object ? \u201d ( parsed by a pre-trained semantic parser using a custom DSL ) , this provides a substantial degree of supervision to the representation learning part . In general , it is then not very surprising that using supervised data for fine-tuning improves representation learning , which limits the significance of this contribution further . Due to these reasons , I can only recommend rejection at this point in time . # # # Potential Improvements I don \u2019 t think that it is straightforward to improve the current submission , although I have some detailed comments that I would like to see addressed below . More generally I would encourage the authors to see if visual reasoning _alone_ ( i.e.running solely the fine-tuning stage , but without the perception loss ) would cause object-centric representations to emerge . Firstly , in that case LORL would not have to rely so much on existing contributions from prior approaches like MONet/Slot-Attention ( a perceptual loss for which we know it already yields reasonably good object-centric representations ) . More importantly , in this case it is less obvious that a supervised language-based approach would work , since there is no guaranteed separation into approximate object-representations due to using a perception loss . This would almost certainly make the contribution more significant . It would also be interesting to analyze in what situations fine-tuning on VQA is helpful and whether there are certain types of questions that are particularly helpful in this case . I could image that questions focusing on individual object properties are more useful than those that focus on more global information ( like the total number of ceramic objects ) . More generally it would be interesting to quantify the amount of supervision given to the representation learning and comparing this to the amount of improvement gained . # # # Detailed comments * Regarding the objectness score an ablation is missing . What is the effect of this when learning object-centric representations ? I expect it to be marginal , but it would still be good to demonstrate this . It would also be good to add the reasoning accuracy when the objectness modulation is not used to Table 4 . * Similarly , is it correct that the baseline approach in Tables 1a and 1b includes the objectness score , and is thus used for the proceeding training stages to arrive at the results for LORL ? If not then please add to these tables the performance of LORL after the perceptual training phase ( and if yes , please add a baseline that does not include the objectness score as per my previous comment ) * The standard deviation in Tables 1b and 2 is generally quite high , which makes it difficult to compare in some cases ( although I agree that LORL generally outperforms the purely perceptual approach as is also expected ) . Therefore I would ask that you add additional seeds at least for Table 2 to allow for a better comparison . * The proposed metrics ( GT Split and Pred Split ) seem rather arbitrary and I don \u2019 t think provide much additional insight . The ARI score is consistently in favor of LORL , and although the magnitude of the difference when comparing is sometimes greater when using GT Split and Pred Split it is unclear to me how to interpret that magnitude . More generally , the choice of assigning a mask to an object if it claims at least 20 % currently seems arbitrary , while this could be an important hyper-parameter for these losses . For example , what do the results look like when using a threshold of 10 % or 30 % ? * I don \u2019 t find the results in Table 3 very surprising , since isn \u2019 t this what LORL is exactly trained for given the kind of questions used for the fine-tuning stage ? What else could explain the improved ARI and the results in Table 4 other than that the representations have become better for semantic retrieval ? Perhaps I am missing some alternative interpretations . * I would appreciate a comparison to NS-CT in Table 4 , even though the latter is supervised . It would help understand how good the score obtained by LORL-SA is . * Section 5.4 essentially feels like an after-thought and details are missing . It would be helpful if the baseline IEP-Ref approach could be explained in some more detail . * Regarding the conclusion , I don \u2019 t see why LORL is a \u201c principled framework \u201d . What is the principle that is being applied here ? And why is this desirable over other approaches ? * I noticed in Appendix B that on PartNet-Chairs the second training phase ( where only the reasoning part is trained ) is skipped . What is the reason for this ? What happens if this is also done for the Shop-VRB-Simple dataset ? # # # Post Rebuttal I have read the other reviews and the rebuttal . I appreciate the extensive revision and response of the authors . Indeed , several of the minor issues/clarifications that I had raised have now been addressed . However , as noted in my initial review , my main concern with this work is the highly limited novelty and the significance of the findings : * LORL is essentially a simple application of unsupervised object-centric representation learners ( like MONet , Slot-attention ) to the language-guided visual reasoning framework proposed in MAO et al ( 2019 ) , where the pre-trained vision module is interchanged . As I have previously argued , the objectness score is a heuristic only needed to overcome a limitation of the considered vision modules , which is not very interesting . Indeed , this score is not needed for segmentation ( which is the primary measure of success that the authors have adopted ) . * The main finding , which is that providing some degree of supervision to purely unsupervised object-centric representation learners improves their performance , is not very surprising . Although one could argue that the visual reasoning task does not provide direct supervision on the object segmentations , the considered type of questions ( and DSL ) provide a sizable amount of supervision I believe ( as is also evident from the observed fluctuations in Table 9 ) . One issue that I noticed in the revision is when comparing the results in Table 9 and Table 10 . It can be seen how when training on the visual reasoning task using only 25 % of the provided data ( i.e.22.5K as opposed to 90K ) actually reduces segmentation performance , i.e.from 83.51 ( image only ) to 81.01 . This is surprising , and perhaps somewhat concerning , since I would have expected any reasonable amount of supervision to be helpful and certainly not degrade performance . The authors do not provide an explanation for this behavior , while it appears to invalidate the main claim regarding the benefit of LORL in the general case . For these reasons I remain in favor of a rejection .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful reviews . * * Q1 : Importance of the objectness score . * * A1 : Please refer to our general response Q2 for additional ablation studies on the objectness score . First , most unsupervised object-centric representation learning methods studied in this paper ( MONET [ 1 ] and Slot Attention [ 2 ] ) output a predetermined number of objects for every image . Objectness score is crucial if we want to evaluate how models discover objects in images . Moreover , we show that the inclusion of objectness scores improves the precision of concept learning . * * Q2 : Object-centric representation in NS-CL . * * A2 : NS-CL [ 3 ] and the proposed LORL work on two different topics : NS-CL focuses on learning concepts by associating linguistic units with object representations . Thus , it assumes access to pre-trained object detectors . By contrast , our framework works on learning object-centric representation that jointly discovers objects from images and learns concepts from language . No annotations of object segmentation or pretrained models are needed . * * Q3 : Language as supervision for representation learning . * * A3 : Please refer to our general response for a clarification of our goal and contribution . Existing methods for unsupervised object representation learning are sensitive to hyperparameter choices and often fail when objects have complex geometry or have translucent parts . Actually , there are often ambiguities in object individuation : should we separate the seat and the back of a chair as individual objects or should they be considered as a single one ? How concept learning from language can improve and mediate models \u2019 individuation of objects in scenes has been largely underexplored , to the best of our knowledge . Moreover , we also conducted an additional baseline model that uses language supervision in a different way . Specifically , based on the Slot Attention model , we use a GRU to directly encode question and answer , and concatenate it with the image feature to obtain the object representation . On Shop-VRB-Simple , this model does not show improvement over the Slot Attention baseline : ARI=76.4 % , GT Split=29.2 % , Pred Split=13.6 % . This further suggests the effectiveness of our proposed integration of NS-CL and the objectness score module . * * Q4 : Experiment with only language supervision . * * A4 : Thanks for the great suggestion . We add a new experiment where we train the LORL + Slot Attention model without perceptual loss on Shop-VRB-Simple . Thus , this baseline is trained with visual reasoning supervision only . We found that QA accuracy gets stuck at a local optima of 52.5 % . Intuitively , without good individuation of objects , it is difficult for the model to ground linguistic concepts onto the object representations . This shows the effectiveness of perceptual losses in unsupervised object-centric representation learning . * * Q5 : Question type ablation and data efficiency study . * * A5 : Thanks for your suggestion . There are three types of questions in our dataset , counting ( e.g. , how many plates are there ? ) , existence ( e.g. , is there a toaster ? ) , and query ( e.g. , what is the color of the mug ? ) . Per request , we train LORL+SA on SHOP-VRB-Simple with only a single type of question ( but the total number of questions is the same ) . Results are summarized in the following table . | | ARI | GT split | Pred Split | | : - : | : :| : -- : | : - : | | SA ( image-only ) | 83.46 % | 16.13 % | 12.95 % | | only count | 85.52 % | 13.86 % | 12.56 % | | only exist | 86.29 % | 15.34 % | 10.33 % | | only query | 87.99 % | 13.37 % | 8.75 % | | all types | 89.23 % | 9.95 % | 10.18 % | In general , training on all three types of questions improves the segmentation accuracy ( compared with the image-only baseline ) . The largest gain comes from the query-typed questions . Interestingly , the best result is achieved when trained on the original dataset , where the ratio of counting , existence , and query-typed questions is 1:1:7 . Note that all these models are trained with the same number of questions and thus they are directly comparable with each other . In addition , we provide another analysis by comparing models trained with a different number of question-answer pairs . The results are shown below . Adding more language data consistently improves the result . All results are based on the LORL+SA trained on the Shop-VRB-Simple dataset . | | ARI | GT Split | Pred Split | | : -- : | : :| : -- : | : - : | | 25 % ( 22.5K ) | 81.01 % | 14.31 % | 15.34 % | | 50 % ( 45K ) | 84.39 % | 14.79 % | 9.37 % | | 75 % ( 67.5K ) | 86.53 % | 11.67 % | 11.52 % | | 100 % ( 90K ) | 89.23 % | 9.95 % | 10.18 % |"}, {"review_id": "_ojjh-QFiFr-1", "review_text": "Focus of the work : The paper tries to tackle the problem of learning object centric representations using language . The authors note that most of the previous work which tries to use language assume pre-trained object detectors to generate object proposals in the visual image . Methodology : The proposed method consists of 4 modules . 1.Object Encoder , which tries to generate a modular representation of the scene . 2.Object Decoder , a module to reconstruct masks corresponding to individual objects given the high level factorial representation from the object encoder . 3.A pre-trained semantic parser : a pre-trained module to parse the input ( for ex.a question ) into semantically meaningful program which can be easily executed . 4.Neural-symbolic Program : This modules takes a factorial representation ( i.e. , the output of the encoder ) , as well as some other intermediate information from module 2 and module 2 and outputs an answer to the question . The paper mentions that this is a `` principled '' framework for object centric learning . By keeping the module 3 , and module 4 fixed , the authors evaluate different methods for learning object centric representations like MONET and SLOT attention . The authors evaluate how inclusion of the language can improve the performance of MONET and SLOT attention . Experiments : The authors evaluate the proposed work on two visual reasoning datasets for image segmentation evaluation : Shop-VRB-Simple , and PartNet , as well as a subset of PartNet called PartNet-Chairs . The paper shows that inclusion of the language improves the performance of both MONET as well as SLOT attention , albeit the increase in performance corresponding to SLOT attention is more as compared to MONET . Interesting points : 1 . I like the problem which the proposed method hopes to achieve i.e. , learning the representation of high level variables ( i.e. , objects ) , which are often associated with language . 2.It 's interesting to note that the gain in performance for Slot attention is more as compared to MONET . This point was also emphasized in the work of RIMs ( https : //arxiv.org/abs/1909.10893 ) , where they used a `` top-down '' representation to learn object centric representations . 3.I like the ablation in table 4 i.e. , fine-tuning the entire model improves the performance of the model , as compared to separately training the different components . Major Points : 1 . I think the paper presents nice preliminary experiments for showing that incorporation of language can help learning object centric representations . Since the contribution of the paper is to actually evaluate different methods for object centric learning ( MONET/SLOT Attention ) and combining with the framework of neuro-symbolic learning . It would be interesting to evaluate the LORL + Slot Attention as well as LORL + MONET for more difficult tasks to see how well the learned representations transfer to new environments which share some common structure . It could be in the form of language conditioned scene generation such that by using language one can generalize in a compositional way to new scenes or in the context of instruction following ( instruction is in the form of language ) where their are objects in the scene , and the goal is to put the objects in a particular spatial configuration or just navigation ( https : //arxiv.org/abs/2003.05161 ) . Minor Comments : `` We have proposed Language-mediated , Object-centric Representation Learning ( LORL ) , a principled framework for learning object-centric representations from vision and language . '' I 'm not sure about the use of the word `` principled framework '' as the proposed method is not really a framework . As authors note in the paper , the goal of the paper is to see how incorporation of the language can be used to learn or improve representations of the high level variables ( i.e. , objects ) . After Rebuttal : I have read the rebuttal , as well as reviews by other reviewers . I very much agree with the authors that the problem is very interesting , but as of now more work needs to be done in terms of `` downstream applications '' .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your thoughtful review . * * Q1 : Evaluation of object-centric representation learning . * * A1 : Object-centric representation learning of images naturally integrates two components : 1 ) the discovery of objects in the image and 2 ) representation learning for individual objects . Thus , our experiment section starts with evaluating object discovery performance , measured by the pixel-level segmentation accuracy . This follows the standard evaluation pipeline in previous works [ 1 , 2 ] . Second , our model is able to learn object representations that are directly associated with concepts in latent embedding spaces . The modularized design of the neuro-symbolic concept learner [ 3 ] has enabled us to make interpretable quantification of object categories , colors , and other properties . As a result , the learned representation can be directly applied to downstream tasks such as visual question answering and referring expression comprehension . Given a specification of an object of interest in language , our model can directly produce the mask of the object . Such ability naturally transfers to other domains such as object manipulation or instruction following if we integrate our model with other planning algorithms , which we leave as future works . `` ` [ 1 ] Klaus Greff , Rapha\u00ebl Lopez Kaufman , Rishabh Kabra , Nick Watters , Christopher Burgess , Daniel Zoran , Lo\u00efc Matthey , M. Botvinick , and Alexander Lerchner . Multi-Object Representation Learning with Iterative Variational Inference . In ICML , 2019 . [ 2 ] Locatello , Francesco , Dirk Weissenborn , Thomas Unterthiner , Aravindh Mahendran , Georg Heigold , Jakob Uszkoreit , A. Dosovitskiy and Thomas Kipf . Object-Centric Learning with Slot Attention . In NeurIPS 2020 . [ 3 ] Jiayuan Mao , Chuang Gan , Pushmeet Kohli , Joshua B. Tenenbaum , and Jiajun Wu . The Neuro-Symbolic Concept Learner : Interpreting Scenes , Words , and Sentences from Natural Supervision . In ICLR , 2019 . `` `"}, {"review_id": "_ojjh-QFiFr-2", "review_text": "=Summary The paper proposes a framework for object-centric representation learning with additional language supervision such as e.g.questions and answers , denoted as Language-mediated , Object-centric Representation Learning ( LORL ) . The authors combine two ideas from prior work , the unsupervised object-centric representation learning and the neural-symbolic concept learning , in one architecture . The model obtains object representations by learning to reconstruct the input image ( as in MONet and Slot Attention ) . The learned representations are used as input to the neural-symbolic program executor , which learns to answer questions about objects . The entire model is trained in three stages : first the reconstruction objective , then the QA objective , and , finally , jointly . Experiments on two datasets demonstrate that the obtained object segmentations have better quality that those of the original unsupervised models . The learned representations are also shown to be effective in several other down-stream tasks . =Strengths The intuitions are clearly conveyed and the method is well motivated : language introduces strong inductive biases , providing concept names which can in turn be grounded in images , helping the model understand what these concepts should look like ( especially for heterogenous objects such as \u201c coffee maker \u201d ) . The authors have collected language descriptions for the PartNet-Chairs dataset . Adding LORL on top of Slot Attention leads to notable improvement in object segmentation performance on two datasets . The learned representations ( using Slot Attention ) allow to retrieve similar objects that belong to the same object category , showing that they capture object semantics much better than the unsupervised models . It is interesting to see that the learned representations can be effectively used for other tasks , e.g.referring expression comprehension w/o additional fine-tuning . =Weaknesses/High-level comments I am not sure how unexpected the main result really is ( the improved segmentation quality ) , considering that LORL receives additional ( language ) supervision . Comparison of vanilla unsupervised models vs. LORL is not completely fair or informative , in my opinion . It would be interesting to compare LORL vs. some other forms of supervision ( or even fully/partially supervised segmentation models ) or show how VQA supervision compares to caption supervision ( more on that below ) , or using attributes instead of language , etc . The authors claim that \u201c LORL can be integrated with various unsupervised segmentation algorithms \u201d ( P1 ) . In practice almost all the experiments are carried out with just Slot Attention . The only reported result with MONet ( Table 2 ) is somewhat weak ( as also stated by the authors , P7 ) . This seems to undermine the authors ' claim . There are no experiments on the popular CLEVR dataset , only on the two recent datasets introduced by the authors ( ShopVRB-Simple is obtained by selecting easier scenarios from ShopVRB ) . Specifically , they could have used CLEVR for MONet , as MONet is not applicable to the ShopVRB-Simple data . It is in unclear what the language task on the PartNet-Chairs actually is , it is only stated that the authors collected descriptive sentences . It is mentioned that the output can be True or False ; does it mean that the task is to predict whether the sentence is true to the image ? No \u201d negative \u201d examples are included in the paper , nor is there any discussion on that . Why have the authors decided against collecting question-answering data for the PartNet-Chairs , similar to the ShopVRB-Simple ? Is there an advantage of using a specific form of supervision ( e.g.question-answering supervision ) ? Some analysis on which form of supervision is more effective would have been interesting . The overall framework mostly relies on existing components , the main technical novelty is bringing them together and an additional objectness score used in both objectives . It is not entirely clear whether the reported Slot attention and MONet performance is obtained with the vanilla implementations or after stage 1 training of the proposed model ( which also includes the objectness scores ) . Overall , would be interesting to see an ablation of the proposed objectness score . Would be great to learn more about the referring expression comprehension experiment ( what the test data looks like , how similar it is to the observed training data , how was the model adapted to the task , etc ) . The authors show that the final training stage significantly improves the model \u2019 s QA abilities , saying that it demonstrates the importance of joint training . I am not entirely sure about the purpose of this experiment . Would be also interesting to see how the obtained performance compares to the vanilla neural-symbolic approach . Here we have synthetic images with no background ( relatively easy to segment ) and synthetic ( templated ) language ( easy to parse ) . I am somewhat skeptical about the generality of the proposed framework ( which also concerns prior works on which this work builds ) , i.e.can we expect that this model will work with real cluttered images and real natural language ? What do the authors believe is the main limitation of their proposed approach ? Nothing was stated about making the code available . =Detailed comments ( P # - page number ) - P1 : perhaps \u201c ( for ) learning disentangled , object-centric scene representations \u201d ? - Throughout the paper the authors say \u201c referential expression interpretation \u201d , while this task is more commonly known as \u201c referring expression comprehension \u201d . - The authors say \u201c object \u201d to refer to both objects ( in ShopVRB-Simple ) and object parts ( in PartNet-Chairs ) . I wonder if there is a better way to explain this , which encapsulates both scenarios . - Fig 1 is misleading as it misrepresents the task posed on the PartNet-Chairs dataset ( it is not question-answering ) . - P2 ShopVRB should be ShopVRB-Simple . - P2 PartNet should be PartNet-Chairs . - P3 : should o_i be z_i ? - P3 white object Fig.2 = > in Fig.2 - P4 : should { z_i } be { z_k } ? - P4 which query the attribute = > which queries the attribute - P4 All concepts appear = > All concepts that appear - P4 \u201c We use the same domain-specific language ( DSL ) for representing programs as CLEVR \u201c : in fact it is not the same but extended , as stated in the appendix . - P5 an filter = > a filter - P5 \u201c we find that it better highlights the quality of learned object-centric representations for various models. \u201d - not clear what this means - P7 \u201c The quantitative results are summarized in Table 1. \u201d - make it clear that this is for the ShopVRB-Simple Post-rebuttal comments : I thank the authors for an extensive response and the other reviewers for brining up many relevant questions ! = Main positive additions : LORL was successfully combined with another unsupervised approach , SPACE , on the CLEVR dataset . LORL+SA outperforms IET , which has the same amount of supervision . It does lose to NS-CL slightly , which has access to pre-trained object detectors . I like the added analysis of the QA types , data efficiency etc . = Some of the remaining issues : The positive impact of the objectness score on performance was not demonstrated . To show its benefit the authors had to propose yet another evaluation scheme ( precision and recall of the reconstructed scene graph ) . The training objective for PartNet-Chairs should be discussed in the main paper , not in the appendix . Also , perhaps I am missing something , but would not one still need some negatives to train it ? Minor : Fig 1 in the revision is still wrong , i.e.the example for PartNet-Chairs dataset still illustrates the QA task . Overall , I like that the approach is intuitive and well motivated but I also think the overall technical novelty is limited . The authors have considerably expanded their evaluation , but at the same time have introduced another confusion ( as pointed out by R3 during post-rebuttal discussion ) : it appears that using 25 % of supervision leads to lower segmentation performance , contradicting the main claim of the paper . I therefore decrease my score to 5 . I hope to see an improved version of the paper ( with more exciting technical contributions ) in a future venue !", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your thoughtful reviews . * * Q1 : Language and other types of supervision . * * A1 : Please refer to our general response for a clarification of the goal and contribution . Existing methods for unsupervised object representation learning are sensitive to hyperparameter choices and often fail when objects have complex geometry or have translucent parts . Actually , there are often ambiguities in object individuation : should we separate the seat and the back of a chair as individual objects or should they be considered as a single one ? How concept learning from language can improve and mediate models \u2019 individuation of objects in scenes has been largely underexplored , to the best of our knowledge . We also agree with the reviewer that other forms of supervision , such as navigation and manipulation in interactive domains are meaningful and interesting supervisions as well . However , in this paper , we focus on how language can mediate object-centric learning . The inclusion of other types of supervisions and their comparisons remain future works . Per request , we have also included additional results to analyze how different types of questions and the number of question-answer pairs contribute to the LORL 's performance . We kindly refer the reviewer to our Response to Reviewer 3 ( Q5 ) for more details . * * Q2 : Integration with other unsupervised object discovery methods . * * A2 : We have included an additional experiment trying to integrate SPACE [ 3 ] with LORL . The results are shown on the CLEVR dataset . As shown in the table below , LORL+SPACE shows a significant advantage over the vanilla SPACE . Additionally , we find that SPACE shows poor segmentation results on Shop-VRB-Simple and ParNet-Chairs , no matter whether it is integrated with LORL . For example , it frequently segments complex objects into too many fragments on Shop-VRB-Simple . We conjecture that this is because SPACE was designed for segmenting objects of similar sizes . | | ARI | GT Split | Pred Split | | : - : | : :| : -- : | : - : | | SPACE | 72.34 % | 29.20 % | 12.38 % | | LORL+SPACE | 97.82 % | 2.04 % | 2.17 % | * * Q3 : MONet on CLEVR . * * A3 : Due to the time limit , we are unable to finish up running experiments on CLEVR . Based on the original paper [ 4 ] , training MONET on CLEVR takes 2 weeks . However , it is worth noting that MONet has already achieved near perfect segmentation results on CLEVR , with an ARI of 96.2 % [ 5 ] . This suggests that there is little room for future improvement by other integration . * * Q4 : Language task on PartNet-Chairs . * * A4 : Your interpretation is absolutely correct . In PartNet-Chairs , all programs for descriptive sentences end with an Equal operator , which evaluates to true iff . both of its arguments are equal to each other . We train LORL with paired images and sentences , by maximizing the probability that the first argument equals to the second argument . We use different text forms ( questions vs. descriptive sentences ) for different datasets to show the generality of the proposed method . Since question-answer pairs can usually be converted into descriptive sentences ( e.g. , what \u2019 s the color of the seat back = > the color of the seat back is\u2026 ) , comparing question-based and descriptive sentence-based supervision is not the main focus of our study . For detailed study on different types of supervision , we kindly refer the reviewer to the \u201c Question Type and Data Efficiency Study \u201d in our response to R3 ( Q5 ) . * * Q5 : Objectness score ablation . * * A5 : We kindly refer the reviewer to general response Q2 for an ablation study on the objectness score . * * Q6 : Referential expression comprehension details . * * A6 : Thanks for the suggestion . We have included the details of the referential expression task in the revision . In this experiment , the data is generated using the code adapted from [ 6 ] . It contains two types of expressions , the first one directly refers to an object by its properties : for example , \u201c the white plate \u201d . The second type of sentences refers to the object by relating it with another object : for example , \u201c the object that is in front of the mug. \u201d The output of the model is the masks of all referred objects . The dataset is composed of the same set of concepts and same DSL as in the Shop-VRB-Simple . For all methods , including ours and the baseline , we assume a pretrained semantic parser . Since the neuro-symbolic program executor outputs a distribution over all objects indicating whether they are selected , we directly multiply its output with the object segmentation masks to get the final output ."}, {"review_id": "_ojjh-QFiFr-3", "review_text": "This paper proposed an interesting idea that uses language to learn the concept and aid downstream tasks such as segmentation and referential expression interpretation . The authors combine the unsupervised segmentation method ( MONet and Slot Attention ) with neural symbolic concept learning ( NS-CL ) . By joint training these two objectives , the authors show improvements in the object segmentation and several downstream tasks . 1 ) What is the language objective for the PartNet-Chairs dataset ? The dataset contains templated captions instead of questions , but the paper did n't mention any of the associate target or loss . 2 ) A lot of the paper needs more clarification , for example , how to calculate the `` Whiteness '' using the concept embedding ? How many programs are there ? I can not find that information even in the supplementary materials . It is hard to understand the exact algorithm of the proposed method . 3 ) The performance of pretrained semantic parser is not reported in the paper . Since there are only a few templates , will the model achieve 100 percent performance ? 4 ) The semantic parser info is necessary since the model will know the exact additional information of the image given the question and captions ( especially captions ) , assuming there is no error in the parsing procedure . This will harm the overall novelty of the paper since the proposed method can not generalized to more complicated questions and captions . 5 ) If the pretrained semantic parser can provide very high accuracies , for a fair comparison , it will be good the authors can show the performance of MONet and Slot Attention with additional supervision . For example , Instead of having a Neuro-symbolic program executor , simply use the concept embedding to filter the object-centric representations and then do image decoding . This will show the effectiveness of the Neuro-Symbolic Program Executor . 6 ) Missing related work , [ 1 ] also learns the visual concept through question answering . The authors should discuss the difference between this work . [ 1 ] Yang et.al Visual Curiosity : Learning to Ask Questions to Learn Visual Recognition ( CORL ) I really like the paper 's idea , but a lot of critical information experiments are missing . I am happy to increase my rating if the authors can resolve my questions .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful review . * * Q1 : Training objective for the PartNet-Chairs dataset . * * A1 : We refer the reviewer to Appendix A and Table 6 for templates and example programs used in the PartNet-Chairs dataset . Specifically , all programs for descriptive sentences end with an Equal operator , which evaluates to true iff . both of its arguments are equal to each other . We train our model with paired images and sentences , by maximizing the probability that the first argument equals the second argument . * * Q2 : Computation of the \u201c Whiteness \u201d score . * * A2 : We use the same concept quantification module as [ 1 ] . Specifically , to compute the \u201c Whiteness \u201d of an object , we first extract the object representation of the target object , project it into the concept space , and compute the cosine similarity between the object representation and a concept embedding for \u201c white \u201d . * * Q3 : The number of programs . * * A3 : On Shop-VRB-Simple , we use 10K images and 90K questions for training , 960 images , and 8.6K questions for testing . We use the same DSL as the original paper and thus there are 19 primitive operations . On PartNet-Chairs , there are 5K images and 40K captions for training , 960 images for testing ; 10 different basic functions are considered . * * Q3 : The performance of the semantic parser . * * A3 : Thank you for the suggestion and we have included an evaluation of the semantic parsing module in our revision . Since the module is trained on paired question-program pairs , it achieves nearly perfect accuracy ( > 99.9 % ) on test questions/captions . * * Q4 : Generalization of the semantic parsing module to more complex questions . * * A4 : We agree with the reviewer that our current way of training semantic parsing modules has its limitations . However , in this paper , our main focus is on joint object discovery ( segmentation ) and concept learning . Parsing more complicated sentences is an orthogonal direction with our contribution . For example , potential future works may consider using semantic parsers trained on general corpus ( e.g. , [ 4 ] ) or joint training of semantic parsing and concept learning [ 2 ] . * * Q5 : The effectiveness of the Neuro-Symbolic Program Executor . * * A5 : The neuro-symbolic program executor indeed executes the program tokens , such as filter [ White ] by comparing the embedding of \u201c white \u201d with individual object representations . Per request , we have included an additional baseline , which uses a GRU to directly encode the questions and the answers associated with each image , and concatenate it with the image feature while extracting the representation for each object . On Shop-VRB-Simple , the modified Slot Attention model does not show improvement : ARI=76.4 % , GT Split=29.2 % , Pred Split=13.6 % . * * Q6 : Related works . * * A6 : Thanks for the suggestion . We have cited and discussed this paper . Specifically , Yang et al.have proposed to learn visual concepts through visual dialog . In contrast , this paper learns visual concepts through question-answering pairs or captions . Most importantly , our model focuses on how concept learning and object discovery/representation learning bootstrap each other . `` ` [ 1 ] Justin Johnson , Bharath Hariharan , Laurens van der Maaten , Li Fei-Fei , C Lawrence Zitnick , and Ross Girshick . CLEVR : A diagnostic dataset for compositional language and elementary visual reasoning . In CVPR , 2017 . [ 2 ] Jiayuan Mao , Chuang Gan , Pushmeet Kohli , Joshua B. Tenenbaum , and Jiajun Wu . The Neuro-Symbolic Concept Learner : Interpreting Scenes , Words , and Sentences from Natural Supervision . In ICLR , 2019 [ 3 ] Kexin Yi , Jiajun Wu , Chuang Gan , Antonio Torralba , Pushmeet Kohli , and Josh Tenenbaum . Neural-Symbolic VQA : Disentangling reasoning from vision and language understanding . In NeurIPS , 2018 [ 4 ] Sebastian Schuster , Ranjay Krishna , Angel Chang , Li Fei-Fei , and Christopher D. Manning . Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval . In Proceedings of the Fourth Workshop on Vision and Language ( VL15 ) , 2015 . `` `"}], "0": {"review_id": "_ojjh-QFiFr-0", "review_text": "# # # Summary This paper proposes to combine the neuro-symbolic concept learner for visual reasoning from language ( NS-CL ; Mao et al. , 2019 ) with recent unsupervised approaches to learning object-centric representations such as MONet ( Burgess et al. , 2019 ) and Slot-Attention ( Locatello et al. , 2020 ) . While NS-CL normally relies on pre-trained object-detectors ( in a supervised fashion ) to extract visual representations , the proposed combination ( dubbed LORL ) use MONet or Slot Attention for this . By additionally back-propagating error signals from language-driven visual reasoning tasks obtained via NS-CL into MONet/Slot-Attention , it is shown how LORL is better able at learning object-centric representations and perform instance segmentation . # # # Pro \u2019 s / Con \u2019 s / Justification Overall the paper is well written and easy to follow . Moreover , using other modalities such as language to improve visual perception is an interesting research direction and timely due to recent advances in object-centric representation learning However , the significance of this contribution is rather limited due to two main reasons : LORL is a very straightforward combination of MONet/Slot-Attention with NS-CL and therefore not very novel the encoder part of NS-CL is simply replaced by the MONet/Slot-Attention , while everything else remains pretty much the same . Note that I don \u2019 t consider the \u201c objectness scores \u201d to modulate the NS-CL reasoning process very novel , since it is essentially a straight-forward heuristic to cope with a limitation of MONet/Slot-Attention in that sometimes the object-centric representations may containing background information or are \u201c empty \u201d . Were object-centric representations correctly inferred ( as in the original NS-CL ) , this heuristic would therefore also not be needed The results are not very surprising : it is found that better object-centric representations can be learned by fine-tuning on a visual reasoning task that uses language . However , since this task is learned in a supervised fashion and the dataset contains questions of the form \u201c what is the name of the white object ? \u201d ( parsed by a pre-trained semantic parser using a custom DSL ) , this provides a substantial degree of supervision to the representation learning part . In general , it is then not very surprising that using supervised data for fine-tuning improves representation learning , which limits the significance of this contribution further . Due to these reasons , I can only recommend rejection at this point in time . # # # Potential Improvements I don \u2019 t think that it is straightforward to improve the current submission , although I have some detailed comments that I would like to see addressed below . More generally I would encourage the authors to see if visual reasoning _alone_ ( i.e.running solely the fine-tuning stage , but without the perception loss ) would cause object-centric representations to emerge . Firstly , in that case LORL would not have to rely so much on existing contributions from prior approaches like MONet/Slot-Attention ( a perceptual loss for which we know it already yields reasonably good object-centric representations ) . More importantly , in this case it is less obvious that a supervised language-based approach would work , since there is no guaranteed separation into approximate object-representations due to using a perception loss . This would almost certainly make the contribution more significant . It would also be interesting to analyze in what situations fine-tuning on VQA is helpful and whether there are certain types of questions that are particularly helpful in this case . I could image that questions focusing on individual object properties are more useful than those that focus on more global information ( like the total number of ceramic objects ) . More generally it would be interesting to quantify the amount of supervision given to the representation learning and comparing this to the amount of improvement gained . # # # Detailed comments * Regarding the objectness score an ablation is missing . What is the effect of this when learning object-centric representations ? I expect it to be marginal , but it would still be good to demonstrate this . It would also be good to add the reasoning accuracy when the objectness modulation is not used to Table 4 . * Similarly , is it correct that the baseline approach in Tables 1a and 1b includes the objectness score , and is thus used for the proceeding training stages to arrive at the results for LORL ? If not then please add to these tables the performance of LORL after the perceptual training phase ( and if yes , please add a baseline that does not include the objectness score as per my previous comment ) * The standard deviation in Tables 1b and 2 is generally quite high , which makes it difficult to compare in some cases ( although I agree that LORL generally outperforms the purely perceptual approach as is also expected ) . Therefore I would ask that you add additional seeds at least for Table 2 to allow for a better comparison . * The proposed metrics ( GT Split and Pred Split ) seem rather arbitrary and I don \u2019 t think provide much additional insight . The ARI score is consistently in favor of LORL , and although the magnitude of the difference when comparing is sometimes greater when using GT Split and Pred Split it is unclear to me how to interpret that magnitude . More generally , the choice of assigning a mask to an object if it claims at least 20 % currently seems arbitrary , while this could be an important hyper-parameter for these losses . For example , what do the results look like when using a threshold of 10 % or 30 % ? * I don \u2019 t find the results in Table 3 very surprising , since isn \u2019 t this what LORL is exactly trained for given the kind of questions used for the fine-tuning stage ? What else could explain the improved ARI and the results in Table 4 other than that the representations have become better for semantic retrieval ? Perhaps I am missing some alternative interpretations . * I would appreciate a comparison to NS-CT in Table 4 , even though the latter is supervised . It would help understand how good the score obtained by LORL-SA is . * Section 5.4 essentially feels like an after-thought and details are missing . It would be helpful if the baseline IEP-Ref approach could be explained in some more detail . * Regarding the conclusion , I don \u2019 t see why LORL is a \u201c principled framework \u201d . What is the principle that is being applied here ? And why is this desirable over other approaches ? * I noticed in Appendix B that on PartNet-Chairs the second training phase ( where only the reasoning part is trained ) is skipped . What is the reason for this ? What happens if this is also done for the Shop-VRB-Simple dataset ? # # # Post Rebuttal I have read the other reviews and the rebuttal . I appreciate the extensive revision and response of the authors . Indeed , several of the minor issues/clarifications that I had raised have now been addressed . However , as noted in my initial review , my main concern with this work is the highly limited novelty and the significance of the findings : * LORL is essentially a simple application of unsupervised object-centric representation learners ( like MONet , Slot-attention ) to the language-guided visual reasoning framework proposed in MAO et al ( 2019 ) , where the pre-trained vision module is interchanged . As I have previously argued , the objectness score is a heuristic only needed to overcome a limitation of the considered vision modules , which is not very interesting . Indeed , this score is not needed for segmentation ( which is the primary measure of success that the authors have adopted ) . * The main finding , which is that providing some degree of supervision to purely unsupervised object-centric representation learners improves their performance , is not very surprising . Although one could argue that the visual reasoning task does not provide direct supervision on the object segmentations , the considered type of questions ( and DSL ) provide a sizable amount of supervision I believe ( as is also evident from the observed fluctuations in Table 9 ) . One issue that I noticed in the revision is when comparing the results in Table 9 and Table 10 . It can be seen how when training on the visual reasoning task using only 25 % of the provided data ( i.e.22.5K as opposed to 90K ) actually reduces segmentation performance , i.e.from 83.51 ( image only ) to 81.01 . This is surprising , and perhaps somewhat concerning , since I would have expected any reasonable amount of supervision to be helpful and certainly not degrade performance . The authors do not provide an explanation for this behavior , while it appears to invalidate the main claim regarding the benefit of LORL in the general case . For these reasons I remain in favor of a rejection .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful reviews . * * Q1 : Importance of the objectness score . * * A1 : Please refer to our general response Q2 for additional ablation studies on the objectness score . First , most unsupervised object-centric representation learning methods studied in this paper ( MONET [ 1 ] and Slot Attention [ 2 ] ) output a predetermined number of objects for every image . Objectness score is crucial if we want to evaluate how models discover objects in images . Moreover , we show that the inclusion of objectness scores improves the precision of concept learning . * * Q2 : Object-centric representation in NS-CL . * * A2 : NS-CL [ 3 ] and the proposed LORL work on two different topics : NS-CL focuses on learning concepts by associating linguistic units with object representations . Thus , it assumes access to pre-trained object detectors . By contrast , our framework works on learning object-centric representation that jointly discovers objects from images and learns concepts from language . No annotations of object segmentation or pretrained models are needed . * * Q3 : Language as supervision for representation learning . * * A3 : Please refer to our general response for a clarification of our goal and contribution . Existing methods for unsupervised object representation learning are sensitive to hyperparameter choices and often fail when objects have complex geometry or have translucent parts . Actually , there are often ambiguities in object individuation : should we separate the seat and the back of a chair as individual objects or should they be considered as a single one ? How concept learning from language can improve and mediate models \u2019 individuation of objects in scenes has been largely underexplored , to the best of our knowledge . Moreover , we also conducted an additional baseline model that uses language supervision in a different way . Specifically , based on the Slot Attention model , we use a GRU to directly encode question and answer , and concatenate it with the image feature to obtain the object representation . On Shop-VRB-Simple , this model does not show improvement over the Slot Attention baseline : ARI=76.4 % , GT Split=29.2 % , Pred Split=13.6 % . This further suggests the effectiveness of our proposed integration of NS-CL and the objectness score module . * * Q4 : Experiment with only language supervision . * * A4 : Thanks for the great suggestion . We add a new experiment where we train the LORL + Slot Attention model without perceptual loss on Shop-VRB-Simple . Thus , this baseline is trained with visual reasoning supervision only . We found that QA accuracy gets stuck at a local optima of 52.5 % . Intuitively , without good individuation of objects , it is difficult for the model to ground linguistic concepts onto the object representations . This shows the effectiveness of perceptual losses in unsupervised object-centric representation learning . * * Q5 : Question type ablation and data efficiency study . * * A5 : Thanks for your suggestion . There are three types of questions in our dataset , counting ( e.g. , how many plates are there ? ) , existence ( e.g. , is there a toaster ? ) , and query ( e.g. , what is the color of the mug ? ) . Per request , we train LORL+SA on SHOP-VRB-Simple with only a single type of question ( but the total number of questions is the same ) . Results are summarized in the following table . | | ARI | GT split | Pred Split | | : - : | : :| : -- : | : - : | | SA ( image-only ) | 83.46 % | 16.13 % | 12.95 % | | only count | 85.52 % | 13.86 % | 12.56 % | | only exist | 86.29 % | 15.34 % | 10.33 % | | only query | 87.99 % | 13.37 % | 8.75 % | | all types | 89.23 % | 9.95 % | 10.18 % | In general , training on all three types of questions improves the segmentation accuracy ( compared with the image-only baseline ) . The largest gain comes from the query-typed questions . Interestingly , the best result is achieved when trained on the original dataset , where the ratio of counting , existence , and query-typed questions is 1:1:7 . Note that all these models are trained with the same number of questions and thus they are directly comparable with each other . In addition , we provide another analysis by comparing models trained with a different number of question-answer pairs . The results are shown below . Adding more language data consistently improves the result . All results are based on the LORL+SA trained on the Shop-VRB-Simple dataset . | | ARI | GT Split | Pred Split | | : -- : | : :| : -- : | : - : | | 25 % ( 22.5K ) | 81.01 % | 14.31 % | 15.34 % | | 50 % ( 45K ) | 84.39 % | 14.79 % | 9.37 % | | 75 % ( 67.5K ) | 86.53 % | 11.67 % | 11.52 % | | 100 % ( 90K ) | 89.23 % | 9.95 % | 10.18 % |"}, "1": {"review_id": "_ojjh-QFiFr-1", "review_text": "Focus of the work : The paper tries to tackle the problem of learning object centric representations using language . The authors note that most of the previous work which tries to use language assume pre-trained object detectors to generate object proposals in the visual image . Methodology : The proposed method consists of 4 modules . 1.Object Encoder , which tries to generate a modular representation of the scene . 2.Object Decoder , a module to reconstruct masks corresponding to individual objects given the high level factorial representation from the object encoder . 3.A pre-trained semantic parser : a pre-trained module to parse the input ( for ex.a question ) into semantically meaningful program which can be easily executed . 4.Neural-symbolic Program : This modules takes a factorial representation ( i.e. , the output of the encoder ) , as well as some other intermediate information from module 2 and module 2 and outputs an answer to the question . The paper mentions that this is a `` principled '' framework for object centric learning . By keeping the module 3 , and module 4 fixed , the authors evaluate different methods for learning object centric representations like MONET and SLOT attention . The authors evaluate how inclusion of the language can improve the performance of MONET and SLOT attention . Experiments : The authors evaluate the proposed work on two visual reasoning datasets for image segmentation evaluation : Shop-VRB-Simple , and PartNet , as well as a subset of PartNet called PartNet-Chairs . The paper shows that inclusion of the language improves the performance of both MONET as well as SLOT attention , albeit the increase in performance corresponding to SLOT attention is more as compared to MONET . Interesting points : 1 . I like the problem which the proposed method hopes to achieve i.e. , learning the representation of high level variables ( i.e. , objects ) , which are often associated with language . 2.It 's interesting to note that the gain in performance for Slot attention is more as compared to MONET . This point was also emphasized in the work of RIMs ( https : //arxiv.org/abs/1909.10893 ) , where they used a `` top-down '' representation to learn object centric representations . 3.I like the ablation in table 4 i.e. , fine-tuning the entire model improves the performance of the model , as compared to separately training the different components . Major Points : 1 . I think the paper presents nice preliminary experiments for showing that incorporation of language can help learning object centric representations . Since the contribution of the paper is to actually evaluate different methods for object centric learning ( MONET/SLOT Attention ) and combining with the framework of neuro-symbolic learning . It would be interesting to evaluate the LORL + Slot Attention as well as LORL + MONET for more difficult tasks to see how well the learned representations transfer to new environments which share some common structure . It could be in the form of language conditioned scene generation such that by using language one can generalize in a compositional way to new scenes or in the context of instruction following ( instruction is in the form of language ) where their are objects in the scene , and the goal is to put the objects in a particular spatial configuration or just navigation ( https : //arxiv.org/abs/2003.05161 ) . Minor Comments : `` We have proposed Language-mediated , Object-centric Representation Learning ( LORL ) , a principled framework for learning object-centric representations from vision and language . '' I 'm not sure about the use of the word `` principled framework '' as the proposed method is not really a framework . As authors note in the paper , the goal of the paper is to see how incorporation of the language can be used to learn or improve representations of the high level variables ( i.e. , objects ) . After Rebuttal : I have read the rebuttal , as well as reviews by other reviewers . I very much agree with the authors that the problem is very interesting , but as of now more work needs to be done in terms of `` downstream applications '' .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your thoughtful review . * * Q1 : Evaluation of object-centric representation learning . * * A1 : Object-centric representation learning of images naturally integrates two components : 1 ) the discovery of objects in the image and 2 ) representation learning for individual objects . Thus , our experiment section starts with evaluating object discovery performance , measured by the pixel-level segmentation accuracy . This follows the standard evaluation pipeline in previous works [ 1 , 2 ] . Second , our model is able to learn object representations that are directly associated with concepts in latent embedding spaces . The modularized design of the neuro-symbolic concept learner [ 3 ] has enabled us to make interpretable quantification of object categories , colors , and other properties . As a result , the learned representation can be directly applied to downstream tasks such as visual question answering and referring expression comprehension . Given a specification of an object of interest in language , our model can directly produce the mask of the object . Such ability naturally transfers to other domains such as object manipulation or instruction following if we integrate our model with other planning algorithms , which we leave as future works . `` ` [ 1 ] Klaus Greff , Rapha\u00ebl Lopez Kaufman , Rishabh Kabra , Nick Watters , Christopher Burgess , Daniel Zoran , Lo\u00efc Matthey , M. Botvinick , and Alexander Lerchner . Multi-Object Representation Learning with Iterative Variational Inference . In ICML , 2019 . [ 2 ] Locatello , Francesco , Dirk Weissenborn , Thomas Unterthiner , Aravindh Mahendran , Georg Heigold , Jakob Uszkoreit , A. Dosovitskiy and Thomas Kipf . Object-Centric Learning with Slot Attention . In NeurIPS 2020 . [ 3 ] Jiayuan Mao , Chuang Gan , Pushmeet Kohli , Joshua B. Tenenbaum , and Jiajun Wu . The Neuro-Symbolic Concept Learner : Interpreting Scenes , Words , and Sentences from Natural Supervision . In ICLR , 2019 . `` `"}, "2": {"review_id": "_ojjh-QFiFr-2", "review_text": "=Summary The paper proposes a framework for object-centric representation learning with additional language supervision such as e.g.questions and answers , denoted as Language-mediated , Object-centric Representation Learning ( LORL ) . The authors combine two ideas from prior work , the unsupervised object-centric representation learning and the neural-symbolic concept learning , in one architecture . The model obtains object representations by learning to reconstruct the input image ( as in MONet and Slot Attention ) . The learned representations are used as input to the neural-symbolic program executor , which learns to answer questions about objects . The entire model is trained in three stages : first the reconstruction objective , then the QA objective , and , finally , jointly . Experiments on two datasets demonstrate that the obtained object segmentations have better quality that those of the original unsupervised models . The learned representations are also shown to be effective in several other down-stream tasks . =Strengths The intuitions are clearly conveyed and the method is well motivated : language introduces strong inductive biases , providing concept names which can in turn be grounded in images , helping the model understand what these concepts should look like ( especially for heterogenous objects such as \u201c coffee maker \u201d ) . The authors have collected language descriptions for the PartNet-Chairs dataset . Adding LORL on top of Slot Attention leads to notable improvement in object segmentation performance on two datasets . The learned representations ( using Slot Attention ) allow to retrieve similar objects that belong to the same object category , showing that they capture object semantics much better than the unsupervised models . It is interesting to see that the learned representations can be effectively used for other tasks , e.g.referring expression comprehension w/o additional fine-tuning . =Weaknesses/High-level comments I am not sure how unexpected the main result really is ( the improved segmentation quality ) , considering that LORL receives additional ( language ) supervision . Comparison of vanilla unsupervised models vs. LORL is not completely fair or informative , in my opinion . It would be interesting to compare LORL vs. some other forms of supervision ( or even fully/partially supervised segmentation models ) or show how VQA supervision compares to caption supervision ( more on that below ) , or using attributes instead of language , etc . The authors claim that \u201c LORL can be integrated with various unsupervised segmentation algorithms \u201d ( P1 ) . In practice almost all the experiments are carried out with just Slot Attention . The only reported result with MONet ( Table 2 ) is somewhat weak ( as also stated by the authors , P7 ) . This seems to undermine the authors ' claim . There are no experiments on the popular CLEVR dataset , only on the two recent datasets introduced by the authors ( ShopVRB-Simple is obtained by selecting easier scenarios from ShopVRB ) . Specifically , they could have used CLEVR for MONet , as MONet is not applicable to the ShopVRB-Simple data . It is in unclear what the language task on the PartNet-Chairs actually is , it is only stated that the authors collected descriptive sentences . It is mentioned that the output can be True or False ; does it mean that the task is to predict whether the sentence is true to the image ? No \u201d negative \u201d examples are included in the paper , nor is there any discussion on that . Why have the authors decided against collecting question-answering data for the PartNet-Chairs , similar to the ShopVRB-Simple ? Is there an advantage of using a specific form of supervision ( e.g.question-answering supervision ) ? Some analysis on which form of supervision is more effective would have been interesting . The overall framework mostly relies on existing components , the main technical novelty is bringing them together and an additional objectness score used in both objectives . It is not entirely clear whether the reported Slot attention and MONet performance is obtained with the vanilla implementations or after stage 1 training of the proposed model ( which also includes the objectness scores ) . Overall , would be interesting to see an ablation of the proposed objectness score . Would be great to learn more about the referring expression comprehension experiment ( what the test data looks like , how similar it is to the observed training data , how was the model adapted to the task , etc ) . The authors show that the final training stage significantly improves the model \u2019 s QA abilities , saying that it demonstrates the importance of joint training . I am not entirely sure about the purpose of this experiment . Would be also interesting to see how the obtained performance compares to the vanilla neural-symbolic approach . Here we have synthetic images with no background ( relatively easy to segment ) and synthetic ( templated ) language ( easy to parse ) . I am somewhat skeptical about the generality of the proposed framework ( which also concerns prior works on which this work builds ) , i.e.can we expect that this model will work with real cluttered images and real natural language ? What do the authors believe is the main limitation of their proposed approach ? Nothing was stated about making the code available . =Detailed comments ( P # - page number ) - P1 : perhaps \u201c ( for ) learning disentangled , object-centric scene representations \u201d ? - Throughout the paper the authors say \u201c referential expression interpretation \u201d , while this task is more commonly known as \u201c referring expression comprehension \u201d . - The authors say \u201c object \u201d to refer to both objects ( in ShopVRB-Simple ) and object parts ( in PartNet-Chairs ) . I wonder if there is a better way to explain this , which encapsulates both scenarios . - Fig 1 is misleading as it misrepresents the task posed on the PartNet-Chairs dataset ( it is not question-answering ) . - P2 ShopVRB should be ShopVRB-Simple . - P2 PartNet should be PartNet-Chairs . - P3 : should o_i be z_i ? - P3 white object Fig.2 = > in Fig.2 - P4 : should { z_i } be { z_k } ? - P4 which query the attribute = > which queries the attribute - P4 All concepts appear = > All concepts that appear - P4 \u201c We use the same domain-specific language ( DSL ) for representing programs as CLEVR \u201c : in fact it is not the same but extended , as stated in the appendix . - P5 an filter = > a filter - P5 \u201c we find that it better highlights the quality of learned object-centric representations for various models. \u201d - not clear what this means - P7 \u201c The quantitative results are summarized in Table 1. \u201d - make it clear that this is for the ShopVRB-Simple Post-rebuttal comments : I thank the authors for an extensive response and the other reviewers for brining up many relevant questions ! = Main positive additions : LORL was successfully combined with another unsupervised approach , SPACE , on the CLEVR dataset . LORL+SA outperforms IET , which has the same amount of supervision . It does lose to NS-CL slightly , which has access to pre-trained object detectors . I like the added analysis of the QA types , data efficiency etc . = Some of the remaining issues : The positive impact of the objectness score on performance was not demonstrated . To show its benefit the authors had to propose yet another evaluation scheme ( precision and recall of the reconstructed scene graph ) . The training objective for PartNet-Chairs should be discussed in the main paper , not in the appendix . Also , perhaps I am missing something , but would not one still need some negatives to train it ? Minor : Fig 1 in the revision is still wrong , i.e.the example for PartNet-Chairs dataset still illustrates the QA task . Overall , I like that the approach is intuitive and well motivated but I also think the overall technical novelty is limited . The authors have considerably expanded their evaluation , but at the same time have introduced another confusion ( as pointed out by R3 during post-rebuttal discussion ) : it appears that using 25 % of supervision leads to lower segmentation performance , contradicting the main claim of the paper . I therefore decrease my score to 5 . I hope to see an improved version of the paper ( with more exciting technical contributions ) in a future venue !", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your thoughtful reviews . * * Q1 : Language and other types of supervision . * * A1 : Please refer to our general response for a clarification of the goal and contribution . Existing methods for unsupervised object representation learning are sensitive to hyperparameter choices and often fail when objects have complex geometry or have translucent parts . Actually , there are often ambiguities in object individuation : should we separate the seat and the back of a chair as individual objects or should they be considered as a single one ? How concept learning from language can improve and mediate models \u2019 individuation of objects in scenes has been largely underexplored , to the best of our knowledge . We also agree with the reviewer that other forms of supervision , such as navigation and manipulation in interactive domains are meaningful and interesting supervisions as well . However , in this paper , we focus on how language can mediate object-centric learning . The inclusion of other types of supervisions and their comparisons remain future works . Per request , we have also included additional results to analyze how different types of questions and the number of question-answer pairs contribute to the LORL 's performance . We kindly refer the reviewer to our Response to Reviewer 3 ( Q5 ) for more details . * * Q2 : Integration with other unsupervised object discovery methods . * * A2 : We have included an additional experiment trying to integrate SPACE [ 3 ] with LORL . The results are shown on the CLEVR dataset . As shown in the table below , LORL+SPACE shows a significant advantage over the vanilla SPACE . Additionally , we find that SPACE shows poor segmentation results on Shop-VRB-Simple and ParNet-Chairs , no matter whether it is integrated with LORL . For example , it frequently segments complex objects into too many fragments on Shop-VRB-Simple . We conjecture that this is because SPACE was designed for segmenting objects of similar sizes . | | ARI | GT Split | Pred Split | | : - : | : :| : -- : | : - : | | SPACE | 72.34 % | 29.20 % | 12.38 % | | LORL+SPACE | 97.82 % | 2.04 % | 2.17 % | * * Q3 : MONet on CLEVR . * * A3 : Due to the time limit , we are unable to finish up running experiments on CLEVR . Based on the original paper [ 4 ] , training MONET on CLEVR takes 2 weeks . However , it is worth noting that MONet has already achieved near perfect segmentation results on CLEVR , with an ARI of 96.2 % [ 5 ] . This suggests that there is little room for future improvement by other integration . * * Q4 : Language task on PartNet-Chairs . * * A4 : Your interpretation is absolutely correct . In PartNet-Chairs , all programs for descriptive sentences end with an Equal operator , which evaluates to true iff . both of its arguments are equal to each other . We train LORL with paired images and sentences , by maximizing the probability that the first argument equals to the second argument . We use different text forms ( questions vs. descriptive sentences ) for different datasets to show the generality of the proposed method . Since question-answer pairs can usually be converted into descriptive sentences ( e.g. , what \u2019 s the color of the seat back = > the color of the seat back is\u2026 ) , comparing question-based and descriptive sentence-based supervision is not the main focus of our study . For detailed study on different types of supervision , we kindly refer the reviewer to the \u201c Question Type and Data Efficiency Study \u201d in our response to R3 ( Q5 ) . * * Q5 : Objectness score ablation . * * A5 : We kindly refer the reviewer to general response Q2 for an ablation study on the objectness score . * * Q6 : Referential expression comprehension details . * * A6 : Thanks for the suggestion . We have included the details of the referential expression task in the revision . In this experiment , the data is generated using the code adapted from [ 6 ] . It contains two types of expressions , the first one directly refers to an object by its properties : for example , \u201c the white plate \u201d . The second type of sentences refers to the object by relating it with another object : for example , \u201c the object that is in front of the mug. \u201d The output of the model is the masks of all referred objects . The dataset is composed of the same set of concepts and same DSL as in the Shop-VRB-Simple . For all methods , including ours and the baseline , we assume a pretrained semantic parser . Since the neuro-symbolic program executor outputs a distribution over all objects indicating whether they are selected , we directly multiply its output with the object segmentation masks to get the final output ."}, "3": {"review_id": "_ojjh-QFiFr-3", "review_text": "This paper proposed an interesting idea that uses language to learn the concept and aid downstream tasks such as segmentation and referential expression interpretation . The authors combine the unsupervised segmentation method ( MONet and Slot Attention ) with neural symbolic concept learning ( NS-CL ) . By joint training these two objectives , the authors show improvements in the object segmentation and several downstream tasks . 1 ) What is the language objective for the PartNet-Chairs dataset ? The dataset contains templated captions instead of questions , but the paper did n't mention any of the associate target or loss . 2 ) A lot of the paper needs more clarification , for example , how to calculate the `` Whiteness '' using the concept embedding ? How many programs are there ? I can not find that information even in the supplementary materials . It is hard to understand the exact algorithm of the proposed method . 3 ) The performance of pretrained semantic parser is not reported in the paper . Since there are only a few templates , will the model achieve 100 percent performance ? 4 ) The semantic parser info is necessary since the model will know the exact additional information of the image given the question and captions ( especially captions ) , assuming there is no error in the parsing procedure . This will harm the overall novelty of the paper since the proposed method can not generalized to more complicated questions and captions . 5 ) If the pretrained semantic parser can provide very high accuracies , for a fair comparison , it will be good the authors can show the performance of MONet and Slot Attention with additional supervision . For example , Instead of having a Neuro-symbolic program executor , simply use the concept embedding to filter the object-centric representations and then do image decoding . This will show the effectiveness of the Neuro-Symbolic Program Executor . 6 ) Missing related work , [ 1 ] also learns the visual concept through question answering . The authors should discuss the difference between this work . [ 1 ] Yang et.al Visual Curiosity : Learning to Ask Questions to Learn Visual Recognition ( CORL ) I really like the paper 's idea , but a lot of critical information experiments are missing . I am happy to increase my rating if the authors can resolve my questions .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful review . * * Q1 : Training objective for the PartNet-Chairs dataset . * * A1 : We refer the reviewer to Appendix A and Table 6 for templates and example programs used in the PartNet-Chairs dataset . Specifically , all programs for descriptive sentences end with an Equal operator , which evaluates to true iff . both of its arguments are equal to each other . We train our model with paired images and sentences , by maximizing the probability that the first argument equals the second argument . * * Q2 : Computation of the \u201c Whiteness \u201d score . * * A2 : We use the same concept quantification module as [ 1 ] . Specifically , to compute the \u201c Whiteness \u201d of an object , we first extract the object representation of the target object , project it into the concept space , and compute the cosine similarity between the object representation and a concept embedding for \u201c white \u201d . * * Q3 : The number of programs . * * A3 : On Shop-VRB-Simple , we use 10K images and 90K questions for training , 960 images , and 8.6K questions for testing . We use the same DSL as the original paper and thus there are 19 primitive operations . On PartNet-Chairs , there are 5K images and 40K captions for training , 960 images for testing ; 10 different basic functions are considered . * * Q3 : The performance of the semantic parser . * * A3 : Thank you for the suggestion and we have included an evaluation of the semantic parsing module in our revision . Since the module is trained on paired question-program pairs , it achieves nearly perfect accuracy ( > 99.9 % ) on test questions/captions . * * Q4 : Generalization of the semantic parsing module to more complex questions . * * A4 : We agree with the reviewer that our current way of training semantic parsing modules has its limitations . However , in this paper , our main focus is on joint object discovery ( segmentation ) and concept learning . Parsing more complicated sentences is an orthogonal direction with our contribution . For example , potential future works may consider using semantic parsers trained on general corpus ( e.g. , [ 4 ] ) or joint training of semantic parsing and concept learning [ 2 ] . * * Q5 : The effectiveness of the Neuro-Symbolic Program Executor . * * A5 : The neuro-symbolic program executor indeed executes the program tokens , such as filter [ White ] by comparing the embedding of \u201c white \u201d with individual object representations . Per request , we have included an additional baseline , which uses a GRU to directly encode the questions and the answers associated with each image , and concatenate it with the image feature while extracting the representation for each object . On Shop-VRB-Simple , the modified Slot Attention model does not show improvement : ARI=76.4 % , GT Split=29.2 % , Pred Split=13.6 % . * * Q6 : Related works . * * A6 : Thanks for the suggestion . We have cited and discussed this paper . Specifically , Yang et al.have proposed to learn visual concepts through visual dialog . In contrast , this paper learns visual concepts through question-answering pairs or captions . Most importantly , our model focuses on how concept learning and object discovery/representation learning bootstrap each other . `` ` [ 1 ] Justin Johnson , Bharath Hariharan , Laurens van der Maaten , Li Fei-Fei , C Lawrence Zitnick , and Ross Girshick . CLEVR : A diagnostic dataset for compositional language and elementary visual reasoning . In CVPR , 2017 . [ 2 ] Jiayuan Mao , Chuang Gan , Pushmeet Kohli , Joshua B. Tenenbaum , and Jiajun Wu . The Neuro-Symbolic Concept Learner : Interpreting Scenes , Words , and Sentences from Natural Supervision . In ICLR , 2019 [ 3 ] Kexin Yi , Jiajun Wu , Chuang Gan , Antonio Torralba , Pushmeet Kohli , and Josh Tenenbaum . Neural-Symbolic VQA : Disentangling reasoning from vision and language understanding . In NeurIPS , 2018 [ 4 ] Sebastian Schuster , Ranjay Krishna , Angel Chang , Li Fei-Fei , and Christopher D. Manning . Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval . In Proceedings of the Fourth Workshop on Vision and Language ( VL15 ) , 2015 . `` `"}}