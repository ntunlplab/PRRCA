{"year": "2020", "forum": "rJxX8T4Kvr", "title": "Learning Efficient Parameter Server Synchronization Policies for Distributed SGD", "decision": "Accept (Poster)", "meta_review": "The authors consider a parameter-server setup where the learner acts a server communicating updated weights to workers and receiving gradient updates from them. A major question then relates in the synchronisation of the gradient updates, for which couple of *fixed* heuristics exists that trade-off accuracy of updates (BSP) for speed (ASP) or even combine the two allowing workers to be at most k steps out-of-sync. Instead, the authors propose to learn a synchronisation policy using RL. The authors perform results on a simulated and real environment. Overall, the RL-based method seems to provide some improvement over the fixed protocols, however the margin between the fixed and the RL get smaller in the real clusters. This is actually the main concern raised by the reviewers as well (especially R2) -- the paper in its initial submission did not include the real cluster results, rather these were added at the rebuttal. I find this to be an interesting real-world application of RL and I think it provides an alternative environment for testing RL algorithms beyond simulated environments.   As such, I\u2019m recommending acceptance. However, I do ask the authors to be upfront with the real cluster results and move them in the main paper.\n", "reviews": [{"review_id": "rJxX8T4Kvr-0", "review_text": "This paper studies how to improve the synchronization policy for parameter server based distributed SGD algorithm. Existing work focus on either synchronous or asynchronous policy, which often results in straggler or staleness problems. Recent research proposes different ways to make the policy design fully adaptive and autonomous. This paper proposes a reinforcement learning (RL) approach and shows promising results to reduce the total training time on various scenario. A major challenge is to design the state and action spaces in the reinforcement learning setting. It requires the design can be generalized to different scenario, while ensuing efficient policy learning. Compared to existing policies such as BSP, ASP and SSP, RL has an advantage to adapt to non-stationary situations over the training process. Compared to other existing approaches, RL could be fully data-driven. The paper formalizes an RL problem by minimizing the total training time to reach a given validation accuracy. To minimize the number of actions, only 3 actions coming from BSP, ASP and SSP are used. The state space is designed to capture similar loss curves, and to be independent of the number of workers (as much as possible). A policy network is used make decisions after trained with exiting methods. Numerical results validate that the RL policy improves the training time compared to BSP, ASP and SSP. Different number of works and models, dataset are also tested to show the RL policy is generalizable to unseen scenario. Although all the results are simulated in a controlled environment, Figure 4 gives a very interesting illustration showing the advantage of using the RL policy. I still have detailed comments (see below), but I find the paper well written, and the author(s) has obtained promising results. Detailed comments: - The validation accuracy 88% on MINST seems pretty low to me to stop the algorithm, in particular when training multiple layer neural networks. What would happen if the accuracy is increased, can the RL approach still find a good policy? What about the validation accuracy on CIFAR-10? - I still have some concern of the computation time obtain the RL state per step. In particular, the time cost to compute the loss L on different weights w. How do you address this issue? Is L computed on the validation set? What is its size? This parameter seems to me highly sensitive when the policy is used for different dataset, in particular the dataset vary. It would be better to have more discussions in the paper or appendix. - What is the final test accuracy on the trained models using different policies? This allows us to see whether the approach has not over-fitted to the training/validation set. Some typo: Page 2 line 2 AP -> ASP Page 5, last line 4: converge -> convergence ", "rating": "6: Weak Accept", "reply_text": "In the following , we list your concerns on the Problems VII and VIII and our detailed responses . Problem VII : What is the final test accuracy on the trained models using different policies ? This allows us to see whether the approach has not over-fitted to the training/validation set . Response to Problem VII : Thank you for your reminder . We put this data in the Table 2 , Appendix A , Page 12 , in the revised paper . We report the test accuracy on the DNN models trained by different policies ( BSP , ASP , SSP and RLP ) . Meanwhile , according to comments , we also report the test accuracy of the models trained with different validation accuracy bounds ( 88 % , 92 % and 95 % ) . We have three observations : 1 ) the final test accuracy is close to the validation accuracy bound , which indicates the approach does not overfit the training/validation set . 2 ) the final test accuracy of BSP is a bit higher than others . This is because there is no staleness effect in BSP so it is likely to be more accurate . However , BSP is the slowest policy of all of them . 3 ) the final test accuracy of RLP is a bit higher than ASP . This is simply because there is no synchronization barriers in ASP while RLP does some synchronizations adaptively to reduce the staleness effects . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Problem VIII : Some typo : Page 2 line 2 AP - > ASP Page 5 , last line 4 : converge - > convergence Response to Problem VIII : Thank you very much for your careful review . We have fixed the typos and proofread the paper several times ."}, {"review_id": "rJxX8T4Kvr-1", "review_text": "This paper proposes to use deep RL to learn a policy for communication in the parameter-server setup of distributed training. From the perspective, the problem formulation is a nice contribution. While it is a reasonable idea and the initial results are promising, the lack of an evaluation on a real cluster, or for training more computationally-demanding models, is limiting. I fully appreciate the need to perform experiments in a controlled environment, such as the ones reported in the paper. These are useful to validate the idea and explore its potential limitations. However, to truly validate such an idea completely it is also necessary to implement it and run it \"in the wild\" on an actual distributed system. From my experience, although performing such experiments is certainly more involved and challenging, there can also be significant differences in the outcomes when one goes to such an implementation. Normally these are due to discrepancies between the assumed/simulated model, and real system behavior. Is it clear that deep RL is needed for this application, as opposed to more traditional RL approaches (either tabular, with suitably quantized actions, or a simpler form of function approximation? And to ask in the other direction, did you consider using a more complex policy architecture, e.g., involving an LSTM or other recurrent unit? ", "rating": "3: Weak Reject", "reply_text": "In the following , we list your concerns on the Problem IV and our detailed responses . Problem IV : Is it clear that deep RL is needed for this application , as opposed to more traditional RL approaches ( either tabular , with suitably quantized actions , or a simpler form of function approximation ? And to ask in the other direction , did you consider using a more complex policy architecture , e.g. , involving an LSTM or other recurrent unit ? Response to Problem IV : Thank for your helpful suggestions . In general , the key challenge for our problem is to design the state and action space , while still balancing cost and benefit . We believe that in our case the design of a deep but not very complex network and a small but powerful action space allows us to get the most benefits out of an RL-based approach , while preserving both training and inference efficiency . The details are as follows . On one side , in terms of benefits , the state space must be able to generalize to different scenarios , such as a different number of workers , datasets and underlying models to be trained . Therefore , we choose some features in the state to characterize the execution process and they are independent of the number of workers , datasets and underlying models ( See the paragraphs in Page 4 entitled with `` State '' for more details ) . The experimental results in Section 4 have verified the generality of our state design ( See the paragraphs in Page 7 entitled with `` Generality of RLP '' and Figure 3 for more details ) . The state space in our problem is very complex and large and may contain dozens to hundreds of features . In our experimental setting , it contains 40 dimensions and some dimensions are continuous ( such as the loss values ) . Therefore , we use a deep neural network to represent the transition function in RL and apply DQN to train the RL policy . The traditional tabular algorithms or simpler form of function approximations have difficulties representing the large and complex transition function in this application . Regarding the action space , it is discrete . Therefore , there is no need to quantize the action space . It would , when designed naively , contain up to 2^n actions for n workers since for each worker it need to be decided whether to run or wait , respectively . For efficiency reasons , we thus choose a small action space with only three valid actions . However , it is powerful enough to generalize all existing policies ( See the paragraphs in Page 5 entitled with `` Action '' for more details ) . On the other side in terms of cost , the design must ensure efficient policy learning and action inference . At the very beginning , we have considered to use LSTM or other recurrent units . Then we have chosen a different approach for two reasons : 1 ) LSTM-based architectures tended to diverge during training . A possible reason may be the non-stationary nature of our RL problem , so more complex models with sequential information have convergence issues . 2 ) An LSTM-based architecture would lead to higher computational costs for the action inference . Using our method , the inference time only takes around 1 % when training a model using our RL-based policy . Therefore , we eventually decided to use a simple two-layer neural network in our setting to approximate the RL transition function . To capture the sequential information of the training steps in our model , we instead encode them in our state representation . That is , each state not only contains the information of the current step , but also contains the historical information of some previous steps . Regarding our choices we were inspired by the following two papers . [ a ] Andrychowicz , Marcin , et al . `` Learning to learn by gradient descent by gradient descent . '' NIPS.2016 . [ b ] Li , Ke and Jitendra , Malik . `` Learn to optimize . '' ICLR.2016.Paper [ a ] applies the LSTM with supervised learning while paper [ b ] encodes historical information into state with reinforcement learning . As claimed by authors of [ b ] ( Available on https : //bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/ ) , the method in [ a ] is hard to generalize to unseen cases and may diverge while the method in [ b ] is more general and easy to train ."}, {"review_id": "rJxX8T4Kvr-2", "review_text": "Background disclaimer: I work in RL research for quite an amount of time, but I do not know much about the domain of distributed systems. For this reason, I may not know the details of technical terms, and I might not be the best person to review this work (when compared with the literature in this field). Nevertheless, below I try to give my evaluation based on reading the paper. ==================== In this work, the authors applied value-based reinforcement learning to learn an optimal policy for global parameter tuning in the parameter server (PS) that trains machine learning models in a distributed way using stochastic gradient descent. Example parameters include SGD hyper-parameters (such as learning rate) and system-level parameters. Immediate cost is to minimize the training time of the SGD algorithm, and i believe the states are the server/worker parameters. From the RL perspective, the algorithm used here is a standard DQN with discrete actions (choices of parameters). But in general I am puzzled why the action space is discrete instead of continuous, if the actions are the hyper-parameters. State transition wise, I am not sure if the states follow an action-dependent MDP transition, and therefore at this point I am not sure if DQN is the best algorithm for this applications (versus bandits/combinatorial bandits). While it is impressive to see that RL beats many of the SOTA baselines for parameter tuning, I also find that instead of using data in the real system to do RL training, the paper proposes generating \"simulation\" data by training a separate DNN. I wonder how the performance would differ if the RL policy is trained on the batch real data. ", "rating": "6: Weak Accept", "reply_text": "In the following , we list your concerns on the Problem II and our detailed responses . Problem II : While it is impressive to see that RL beats many of the SOTA baselines for parameter tuning , I also find that instead of using data in the real system to do RL training , the paper proposes generating `` simulation '' data by training a separate DNN . I wonder how the performance would differ if the RL policy is trained on the batch real data . Response to Problem II : Thank you very much for your very helpful suggestions . In this paper , we mainly concentrated on exhibiting our novel ideas for applying RL to synchronization policy design . Therefore , we implemented a simulated environment which is much easier to configure and to validate the idea and explore the potential limitations . However , this is only the first step of our work . In the second step , we have implemented a prototype distributed system to further examine the method and fix possible problems . In the third step , we plan to implement this method as an component for Tensorflow , so that it may be used in real-world production scenarios . After the submission , we have worked on the second step and implemented a prototype distributed system and further tested our method . This system runs on a real cluster environment , the implementation uses the Tensorflow framework . Based on this real distributed system , we have done more experiments . We have added the new experimental results in the Appendix B , Pages 12-13 , in the revised paper . Some outlines on the results are presented as follows . First , RLP still outperforms existing policies BSP , ASP and SSP on the real cluster . As shown in Figure 6 , RLP is 2.11 and 1.64 times faster than BSP and SSP , respectively , which are higher than the simulation environment ; and RLP is 1.28 times faster than ASP , which is lower than the simulation environment . We conjecture that the key reason underlying this phenomenon is that the straggler effect is more significant than the staleness effect in the real cluster environment . Therefore , ASP runs much faster than BSP and SSP ( almost 1.5 to 2 times ) on real clusters , and our RLP improves more on BSP and SSP and less on ASP . Notice that this result once again verifies the adaptivity of our RLP method , which can find better policies both in the simulation environment and real cluster environment with different levels of straggler and staleness effects . Second , on the generality of the RLP , as shown in Figure 7 , we observe that the trained RLP policy can generalize to clusters with different number of workers , new models and new data . This is due to the fact that in our RL formulation , both the state and action representation have no connection to the number of workers . Therefore RLP can transfer to different number of workers . Meanwhile , we record only the loss values in the state information of RLP . Thus , training models with similar loss curve may also incur a speed up by our RLP policy . Once again these results verify that our state and action design for RLP are reasonable and effective . In this , we do not apply our method on very demanding models such as CNN or BERT-like models due to resource constraints . We are currently implementing RLP in order to deal with these computationally-demanding models more efficiently in a distributed environment . However , these models are much more complex and needs more time to train and tune , so the results are not yet ready to be provided in this paper . We will address this in future work . Regarding an integration into Tensorflow we are currently in discussion regarding a modification of the token queue mechanism underlying TensorFlow 's builtin PS . Ultimately , we want to integrate RLP into the generation and fetching procedures of the token queue in order to implement our synchronization policy beyond the prototype distributed implementation ."}], "0": {"review_id": "rJxX8T4Kvr-0", "review_text": "This paper studies how to improve the synchronization policy for parameter server based distributed SGD algorithm. Existing work focus on either synchronous or asynchronous policy, which often results in straggler or staleness problems. Recent research proposes different ways to make the policy design fully adaptive and autonomous. This paper proposes a reinforcement learning (RL) approach and shows promising results to reduce the total training time on various scenario. A major challenge is to design the state and action spaces in the reinforcement learning setting. It requires the design can be generalized to different scenario, while ensuing efficient policy learning. Compared to existing policies such as BSP, ASP and SSP, RL has an advantage to adapt to non-stationary situations over the training process. Compared to other existing approaches, RL could be fully data-driven. The paper formalizes an RL problem by minimizing the total training time to reach a given validation accuracy. To minimize the number of actions, only 3 actions coming from BSP, ASP and SSP are used. The state space is designed to capture similar loss curves, and to be independent of the number of workers (as much as possible). A policy network is used make decisions after trained with exiting methods. Numerical results validate that the RL policy improves the training time compared to BSP, ASP and SSP. Different number of works and models, dataset are also tested to show the RL policy is generalizable to unseen scenario. Although all the results are simulated in a controlled environment, Figure 4 gives a very interesting illustration showing the advantage of using the RL policy. I still have detailed comments (see below), but I find the paper well written, and the author(s) has obtained promising results. Detailed comments: - The validation accuracy 88% on MINST seems pretty low to me to stop the algorithm, in particular when training multiple layer neural networks. What would happen if the accuracy is increased, can the RL approach still find a good policy? What about the validation accuracy on CIFAR-10? - I still have some concern of the computation time obtain the RL state per step. In particular, the time cost to compute the loss L on different weights w. How do you address this issue? Is L computed on the validation set? What is its size? This parameter seems to me highly sensitive when the policy is used for different dataset, in particular the dataset vary. It would be better to have more discussions in the paper or appendix. - What is the final test accuracy on the trained models using different policies? This allows us to see whether the approach has not over-fitted to the training/validation set. Some typo: Page 2 line 2 AP -> ASP Page 5, last line 4: converge -> convergence ", "rating": "6: Weak Accept", "reply_text": "In the following , we list your concerns on the Problems VII and VIII and our detailed responses . Problem VII : What is the final test accuracy on the trained models using different policies ? This allows us to see whether the approach has not over-fitted to the training/validation set . Response to Problem VII : Thank you for your reminder . We put this data in the Table 2 , Appendix A , Page 12 , in the revised paper . We report the test accuracy on the DNN models trained by different policies ( BSP , ASP , SSP and RLP ) . Meanwhile , according to comments , we also report the test accuracy of the models trained with different validation accuracy bounds ( 88 % , 92 % and 95 % ) . We have three observations : 1 ) the final test accuracy is close to the validation accuracy bound , which indicates the approach does not overfit the training/validation set . 2 ) the final test accuracy of BSP is a bit higher than others . This is because there is no staleness effect in BSP so it is likely to be more accurate . However , BSP is the slowest policy of all of them . 3 ) the final test accuracy of RLP is a bit higher than ASP . This is simply because there is no synchronization barriers in ASP while RLP does some synchronizations adaptively to reduce the staleness effects . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Problem VIII : Some typo : Page 2 line 2 AP - > ASP Page 5 , last line 4 : converge - > convergence Response to Problem VIII : Thank you very much for your careful review . We have fixed the typos and proofread the paper several times ."}, "1": {"review_id": "rJxX8T4Kvr-1", "review_text": "This paper proposes to use deep RL to learn a policy for communication in the parameter-server setup of distributed training. From the perspective, the problem formulation is a nice contribution. While it is a reasonable idea and the initial results are promising, the lack of an evaluation on a real cluster, or for training more computationally-demanding models, is limiting. I fully appreciate the need to perform experiments in a controlled environment, such as the ones reported in the paper. These are useful to validate the idea and explore its potential limitations. However, to truly validate such an idea completely it is also necessary to implement it and run it \"in the wild\" on an actual distributed system. From my experience, although performing such experiments is certainly more involved and challenging, there can also be significant differences in the outcomes when one goes to such an implementation. Normally these are due to discrepancies between the assumed/simulated model, and real system behavior. Is it clear that deep RL is needed for this application, as opposed to more traditional RL approaches (either tabular, with suitably quantized actions, or a simpler form of function approximation? And to ask in the other direction, did you consider using a more complex policy architecture, e.g., involving an LSTM or other recurrent unit? ", "rating": "3: Weak Reject", "reply_text": "In the following , we list your concerns on the Problem IV and our detailed responses . Problem IV : Is it clear that deep RL is needed for this application , as opposed to more traditional RL approaches ( either tabular , with suitably quantized actions , or a simpler form of function approximation ? And to ask in the other direction , did you consider using a more complex policy architecture , e.g. , involving an LSTM or other recurrent unit ? Response to Problem IV : Thank for your helpful suggestions . In general , the key challenge for our problem is to design the state and action space , while still balancing cost and benefit . We believe that in our case the design of a deep but not very complex network and a small but powerful action space allows us to get the most benefits out of an RL-based approach , while preserving both training and inference efficiency . The details are as follows . On one side , in terms of benefits , the state space must be able to generalize to different scenarios , such as a different number of workers , datasets and underlying models to be trained . Therefore , we choose some features in the state to characterize the execution process and they are independent of the number of workers , datasets and underlying models ( See the paragraphs in Page 4 entitled with `` State '' for more details ) . The experimental results in Section 4 have verified the generality of our state design ( See the paragraphs in Page 7 entitled with `` Generality of RLP '' and Figure 3 for more details ) . The state space in our problem is very complex and large and may contain dozens to hundreds of features . In our experimental setting , it contains 40 dimensions and some dimensions are continuous ( such as the loss values ) . Therefore , we use a deep neural network to represent the transition function in RL and apply DQN to train the RL policy . The traditional tabular algorithms or simpler form of function approximations have difficulties representing the large and complex transition function in this application . Regarding the action space , it is discrete . Therefore , there is no need to quantize the action space . It would , when designed naively , contain up to 2^n actions for n workers since for each worker it need to be decided whether to run or wait , respectively . For efficiency reasons , we thus choose a small action space with only three valid actions . However , it is powerful enough to generalize all existing policies ( See the paragraphs in Page 5 entitled with `` Action '' for more details ) . On the other side in terms of cost , the design must ensure efficient policy learning and action inference . At the very beginning , we have considered to use LSTM or other recurrent units . Then we have chosen a different approach for two reasons : 1 ) LSTM-based architectures tended to diverge during training . A possible reason may be the non-stationary nature of our RL problem , so more complex models with sequential information have convergence issues . 2 ) An LSTM-based architecture would lead to higher computational costs for the action inference . Using our method , the inference time only takes around 1 % when training a model using our RL-based policy . Therefore , we eventually decided to use a simple two-layer neural network in our setting to approximate the RL transition function . To capture the sequential information of the training steps in our model , we instead encode them in our state representation . That is , each state not only contains the information of the current step , but also contains the historical information of some previous steps . Regarding our choices we were inspired by the following two papers . [ a ] Andrychowicz , Marcin , et al . `` Learning to learn by gradient descent by gradient descent . '' NIPS.2016 . [ b ] Li , Ke and Jitendra , Malik . `` Learn to optimize . '' ICLR.2016.Paper [ a ] applies the LSTM with supervised learning while paper [ b ] encodes historical information into state with reinforcement learning . As claimed by authors of [ b ] ( Available on https : //bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/ ) , the method in [ a ] is hard to generalize to unseen cases and may diverge while the method in [ b ] is more general and easy to train ."}, "2": {"review_id": "rJxX8T4Kvr-2", "review_text": "Background disclaimer: I work in RL research for quite an amount of time, but I do not know much about the domain of distributed systems. For this reason, I may not know the details of technical terms, and I might not be the best person to review this work (when compared with the literature in this field). Nevertheless, below I try to give my evaluation based on reading the paper. ==================== In this work, the authors applied value-based reinforcement learning to learn an optimal policy for global parameter tuning in the parameter server (PS) that trains machine learning models in a distributed way using stochastic gradient descent. Example parameters include SGD hyper-parameters (such as learning rate) and system-level parameters. Immediate cost is to minimize the training time of the SGD algorithm, and i believe the states are the server/worker parameters. From the RL perspective, the algorithm used here is a standard DQN with discrete actions (choices of parameters). But in general I am puzzled why the action space is discrete instead of continuous, if the actions are the hyper-parameters. State transition wise, I am not sure if the states follow an action-dependent MDP transition, and therefore at this point I am not sure if DQN is the best algorithm for this applications (versus bandits/combinatorial bandits). While it is impressive to see that RL beats many of the SOTA baselines for parameter tuning, I also find that instead of using data in the real system to do RL training, the paper proposes generating \"simulation\" data by training a separate DNN. I wonder how the performance would differ if the RL policy is trained on the batch real data. ", "rating": "6: Weak Accept", "reply_text": "In the following , we list your concerns on the Problem II and our detailed responses . Problem II : While it is impressive to see that RL beats many of the SOTA baselines for parameter tuning , I also find that instead of using data in the real system to do RL training , the paper proposes generating `` simulation '' data by training a separate DNN . I wonder how the performance would differ if the RL policy is trained on the batch real data . Response to Problem II : Thank you very much for your very helpful suggestions . In this paper , we mainly concentrated on exhibiting our novel ideas for applying RL to synchronization policy design . Therefore , we implemented a simulated environment which is much easier to configure and to validate the idea and explore the potential limitations . However , this is only the first step of our work . In the second step , we have implemented a prototype distributed system to further examine the method and fix possible problems . In the third step , we plan to implement this method as an component for Tensorflow , so that it may be used in real-world production scenarios . After the submission , we have worked on the second step and implemented a prototype distributed system and further tested our method . This system runs on a real cluster environment , the implementation uses the Tensorflow framework . Based on this real distributed system , we have done more experiments . We have added the new experimental results in the Appendix B , Pages 12-13 , in the revised paper . Some outlines on the results are presented as follows . First , RLP still outperforms existing policies BSP , ASP and SSP on the real cluster . As shown in Figure 6 , RLP is 2.11 and 1.64 times faster than BSP and SSP , respectively , which are higher than the simulation environment ; and RLP is 1.28 times faster than ASP , which is lower than the simulation environment . We conjecture that the key reason underlying this phenomenon is that the straggler effect is more significant than the staleness effect in the real cluster environment . Therefore , ASP runs much faster than BSP and SSP ( almost 1.5 to 2 times ) on real clusters , and our RLP improves more on BSP and SSP and less on ASP . Notice that this result once again verifies the adaptivity of our RLP method , which can find better policies both in the simulation environment and real cluster environment with different levels of straggler and staleness effects . Second , on the generality of the RLP , as shown in Figure 7 , we observe that the trained RLP policy can generalize to clusters with different number of workers , new models and new data . This is due to the fact that in our RL formulation , both the state and action representation have no connection to the number of workers . Therefore RLP can transfer to different number of workers . Meanwhile , we record only the loss values in the state information of RLP . Thus , training models with similar loss curve may also incur a speed up by our RLP policy . Once again these results verify that our state and action design for RLP are reasonable and effective . In this , we do not apply our method on very demanding models such as CNN or BERT-like models due to resource constraints . We are currently implementing RLP in order to deal with these computationally-demanding models more efficiently in a distributed environment . However , these models are much more complex and needs more time to train and tune , so the results are not yet ready to be provided in this paper . We will address this in future work . Regarding an integration into Tensorflow we are currently in discussion regarding a modification of the token queue mechanism underlying TensorFlow 's builtin PS . Ultimately , we want to integrate RLP into the generation and fetching procedures of the token queue in order to implement our synchronization policy beyond the prototype distributed implementation ."}}