{"year": "2020", "forum": "Bkl5kxrKDr", "title": "A Generalized Training Approach for Multiagent Learning", "decision": "Accept (Talk)", "meta_review": "This paper analyzes and extends learning methods based on Policy-Spaced Response Oracles (PSRO) through the application of alpha-rank.  In doing so, the paper explores connections with Nash equilibria, establishes convergence guarantees in multiple settings, and presents promising empirical results on (among other things) 3-to-5 player poker games.\n\nAlthough this paper originally received mixed scores, after the rebuttal period all reviewers converged to a consensus. A revised version also includes new experiments from the MuJoCo soccer domain, and new poker results as well.  Overall, this paper provides a nice balance of theoretical support and practical relevance that should be of high impact to the RL community. ", "reviews": [{"review_id": "Bkl5kxrKDr-0", "review_text": "The paper studies \u03b1-Rank, a scalable alternative to Nash equilibrium, across a number of areas. Specifically the paper establishes connections between Nash and \u03b1-Rank in specific instances, presents a novel construction of best response that guarantees convergence to the \u03b1-Rank in several games, and demonstrates empirical results in poker and soccer games. The paper is well-written and well-argued. Even without a deep understanding of the subject I was able to follow along across the examples and empirical results. In particular, it was good to see the authors clearly lay out where their novel approach would work and where it would not and to be able to identify why in both cases. My only real concern stems from the empirical results compared to some of the claims made early in the paper. Given the strength of the claims comparing the authors approach and prior approaches, it seems that the empirical results are somewhat weak. The authors make sure to put these results into context, but given the clarity of the results in the toy domains I would have expected clearer takeaways from the empirical results as well. Edit: The authors greatly improved the paper, addressing all major reviewer concerns.", "rating": "8: Accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . Thank you for the suggestion regarding the empirical results . We clarify the takeaways from the paper below , and have worked this commentary into the most recent version of the paper : Validation of the feasibility of the PBR oracle in normal form games ( NFGs ) : the asymmetric nature of these games , in combination with the number of players and strategies involved , makes them inherently , and perhaps surprisingly , large in scale . For example , our largest NFG involves 5 players with 30 strategies each , making for > 24 million strategy profiles in total , which we note is well beyond the scale of canonical NFG domains . Overall , despite their stateless nature , we consider these NFG experiments as key empirical results , in contrast to the toy domains used in our counterexamples . Alpha-PSRO lowering NashConv in 2-player poker experiments : while Alpha-Rank does not seek to find an approximation of Nash , it nonetheless reduces the NashConv yielding extremely competitive results in comparison to an exact-Nash solver in these instances . This result is both non-obvious and quite important , in the sense of establishing Alpha-PSRO as a convenient means of training agents in > 2-player games ( where Nash is not readily computable ) . MuJoCo soccer experiments : Although noted as preliminary , a key observation can be made from these results : upon completion of training , when computing a new play distribution based on a pool of agents trained via the AlphaRank-based training approach vs. a uniform approach , the former attains essentially all of the \u2018 play probability \u2019 . This is evident in the colorbar on the far right of Appendix F , Fig.F.10 , which visualizes the post-training meta-distribution over both training pipelines . Overall , we agree this insight should have been provided more clearly in the text . Based on your feedback , we have updated the revision to integrate changes related to the above discussions . Please let us know if further clarification of any of these points are needed . Finally , on note related to Reviewer 1 and 3 \u2019 s feedback , we are investigating several additional experiments with the aim to include them in the revision before the author discussion period closes . ( We will post an update as soon as applicable regarding any new results . )"}, {"review_id": "Bkl5kxrKDr-1", "review_text": "Review Update (18/11/2019) Thank you for the detailed replies and significant updates to the paper in response to all reviewers. You have comfortably addressed all of my concerns and so I have updated my score. I think the paper has improved significantly through the rebuttal stage and therefore the update in my score is also significant to match the far larger contribution to the community that the paper now represents. -- This paper considers alpha-rank as a solution concept for multi-agent reinforcement learning with a focus on its use as a meta-solver for PSRO. Based on theoretical findings showing shortcomings of using the typical best response oracle, the paper finds a necessity for a new response oracle and proposes preference-based best response. The theoretical contributions help further the community's understanding of alpha-rank but the method remains somewhat disconnected from other recent related literature. Therefore, I think the paper's subsequent impact could be significantly improved by making more direct comparison to recent results. Specifically: 1) In the 2-player games comparisons are currently made to PRD based on its use in Lanctot et al (NeurIPS, 2017) instead of the more recent PSRO Rectified Nash approach proposed by Balduzzi et al. (ICML, 2019). Please make this direct comparison or justify its exclusion. 2) The preliminary MuJoCo soccer results in Appendix G significantly increase the relevance of this work to the ICLR community given the prior publication of this environment at ICLR 2019. However, the results are currently incomplete. In particular, to again strengthen the link to existing work, comparison of the method proposed in this paper to the agents trained by population based training in Liu et al. (ICLR, 2019) would be a more informative comparison than the preliminary results presented in comparison to the na\u00efve uniform meta-solver. 3) Appendix A includes a brief literature survey. This is important material to position the paper in relation to existing work, particularly for readers not familiar with the area that will rely on this to understand the paper as a self contained reference. Please move this section into the main body of the paper and expand to fully credit the work this paper builds upon. Minor Comments: In Appendix C.4 should the reference to Figure C.7 be to Figure C.7a specifically? and the reference to Figure C. 7a be to Figure C. 7b-f inclusive? If so, I believe the available joint strategies in step 4 is missing (1,1,2) as shown in Figure C. 7f. ", "rating": "8: Accept", "reply_text": "We thank the reviewer for the detailed feedback , which we address below . We are currently integrating this feedback into the revision . Main feedback : We are currently running several additional experiments related to those suggested , with the aim to update the paper before the author discussion period closes . We will post an update as soon as new results are available . We completely agree regarding the related works section , and have moved it back to the main body in the latest revision ( Sec.6 ) .Minor comments : Thanks for pointing out the issue with figure references , which have been corrected in the revision as you specified ( please note that the referenced section and figure are now , respectively , Appendix B.4 and Fig.B.7 , due to the related works section being moved out of the appendix ) . We \u2019 ve also updated the subfigure captions to make the correspondence to the counterexample steps clear . Indeed the strategy space in Step 4 should have included ( 1,1,2 ) \u2014 thanks for catching this !"}, {"review_id": "Bkl5kxrKDr-2", "review_text": "This paper extends the original PSRO paper to use an $\\alpha$-Rank based metasolver instead of the projected replicator dynamics and Nash equilibria based metasolvers in the original. To this end, the paper modifies the original idea of Best-Response (BR) oracle since it can ignore some strategies in $\\alpha$-Rank defining SSCC to introduce the idea of _preference-based_ Best-Response (PBR) oracle. The need for a different oracle is well justified especially with the visualization in the Appendix. The main contributions that the paper seems to be going for is a theoretical analysis of $\\alpha$-Rank based PSRO compared to standard PSRO. From the PBR's description (especially in Sec 4.3) it seems the paper is intereseted in expanding the population with novel agents rather than finding the \"best\" single agent which is not well defined for complex games with intransitivities. Nevertheless, it seems that BR is mostly compatible with PBR for symmetric zero-sum two-player games. The paper performs empirical experiments on different versions of poker. First set of experiments compare BR and PBR with $\\alpha$-Rank based metasolver on random games and finds that PBR does better than BR at population expansion as defined. The second set of experiments compare the metasolvers. $\\alpha$-Rank performs similarly to Nash where applicable. Moreover it's faster than Uniform (fictitious self-play) on Kuhn. Then the paper tacks on the MuJoCo soccer experiment as a teaser for ICLR crowd. Overall the paper is quite interesting from the perspective of multiagent learning and I would lean towards accepting. However the paper needs to clarify a lot of details to have any chance of being reproducible. ** Clarifications needed: - Tractability of PBR-Score and PCS-Score It's unclear how tractable these are. Moreover these were only reported for random games. What did these scores look like for the Poker games? Could you clarify how exactly these were computed? - It's somewhat unclear what the lack of convergence without novelty-bound oracle implies. Does this have to do with intransitivities in the game? - Dependence of $\\alpha$? The original $\\alpha$-Rank paper said a lot about the importance of choosing the right value for $\\alpha$. How were these chosen? Do you do the sweep after every iteration of PSRO? - Oracle in experiments? The paper fails to mention the details about the Oracles being used in the experiments. They weren't RL oracles but more details would be useful. - BR not compatible with PBR, albeit not the other way around, meaning one of the solutions you get from PBR might be BR, but can we say which one? - For MuJoCo soccer was it true PSRO or cognitive hierarchy. In general, the original PSRO paper was partly talking about the scalable approach via DCH. This paper doesn't mention that at all. So were the MuJoCo experiments with plain PSRO? What was the exact protocol there? From the appendix it's unclear how the team-vs-team meta game works with individual RL agents. Moreover how are the meta-game evaluation matrices computed in general? How many samples were needed for the Poker games and MuJoCo soccer? - The counterexamples in Appendix B3 are quite interesting. Do you have any hypotheses about the disjoint support from games' correlated equilibria?", "rating": "8: Accept", "reply_text": "We thank the reviewer for the detailed feedback . We agree that clarifying these points is useful for reproducibility and also building reader intuition on the results . Please find our point-by-point responses below , which have been integrated into the latest revision . Tractability of PBR-Score and PCS-Score : This is an important and insightful question regarding the tractability of convergence measures such as PBR- and PCS-Scores . We developed these scores to assess the quality of convergence in our examples , in a manner analogous to NashConv . The computation of these scores is , however , not tractable in general games . Notably , this is also the case for NashConv ( as it requires computation of player-wise best responses , which can be problematic even in moderately-sized games ) . Despite this , these scores remain a useful way to empirically verify the convergence characteristics in small games where they can be tractably computed . We agree that this is a useful remark for readers interested in implementing these scores , and have revised the paper to do so in Section C.3 . Additionally , we now include pseudocode , in the same section , detailing how to compute these scores . Intuition on lack of convergence without novelty-bound oracle : As the reviewer points out , the lack of convergence without a novelty-bound oracle is precisely related to game intransitivities , i.e.cycles in the game can trap the oracle without the novelty-bound constraint . We show an example of this occurring in the revised paper Appendix B.4 ( Figure B.7 ) . Specifically , SSCCs may be hidden by \u201c intermediate \u201d strategies that , while not receiving as high a payoff as current population-pool members , can actually lead to well-performing strategies outside the population . As these \u201c intermediate \u201d strategies are avoided , SSCCs are consequently not found . Note also that this is related to the common problem of action/equilibrium shadowing ( See Matignon et al. , 2012 , \u201c Independent reinforcement learners in cooperative Markov games : a survey regarding coordination problems \u201d ) . Note that per Reviewer 1 and 2 \u2019 s feedback , we have made several improvements to the descriptions of the above example , specifically appending a paragraph following Proposition 4 to better explain this intuition , updating some of the proof text in Section B.4 , and relabeling Fig B.7 \u2019 s captions . We hope these changes make the intuition clearer . Dependence on $ \\alpha $ parameter : Thanks for pointing this out . Indeed , for all alpharank results , we run a sweep over alpha after each PSRO iteration ( as recommended in the original alpharank paper ) . We have updated Section C.1 ( Experimental Procedures ) of the revised paper to clarify this . Overall , relative to the other modules of the training pipeline , we did not find this to be a computational constraint , especially for the larger ( > 2-player ) games and when using a sparse representation and solver for computing the alpharank distribution . On a related note , we have also added more details on the hyperparameters used for the projected replicator dynamics meta-solver to Section C.1 . Oracle in experiments : The oracles used in the experiments were ( exact ) best response oracles , computed by traversing the game tree . Specifically , we used OpenSpiel ( https : //github.com/deepmind/open_spiel ) as the backend for the experiments using the exact best response oracle . Specifics of the implementation can be found in https : //github.com/deepmind/open_spiel/blob/master/open_spiel/python/algorithms/best_response.py ) . We \u2019 ve updated Section C.1 ( Experimental Procedures ) to provide these details . Please let us know if this clarifies things . Many thanks ! BR-PBR compatibility : We thank the reviewer for this question , as the concept of compatibility between objectives benefitted from a clarifying example . In general , BR and PBR optimize different objectives . However , in certain types of games ( e.g. , win-loss and monotonic games , defined respectively in Propositions 5 & 6 ) , the strategy that maximizes value also maximizes the amount of other strategies beaten . In other words , this makes BR compatible with PBR , in the sense that the BR solution space is a subset of the PBR solution space . To make these properties clearer for readers , we have added an example comparing BR and PBR in a monotonic game in Figure B.8 of the appendix . In the case of Win-Loss games , PBR and BR optimize exactly the same objective , and therefore have the same solutions ."}], "0": {"review_id": "Bkl5kxrKDr-0", "review_text": "The paper studies \u03b1-Rank, a scalable alternative to Nash equilibrium, across a number of areas. Specifically the paper establishes connections between Nash and \u03b1-Rank in specific instances, presents a novel construction of best response that guarantees convergence to the \u03b1-Rank in several games, and demonstrates empirical results in poker and soccer games. The paper is well-written and well-argued. Even without a deep understanding of the subject I was able to follow along across the examples and empirical results. In particular, it was good to see the authors clearly lay out where their novel approach would work and where it would not and to be able to identify why in both cases. My only real concern stems from the empirical results compared to some of the claims made early in the paper. Given the strength of the claims comparing the authors approach and prior approaches, it seems that the empirical results are somewhat weak. The authors make sure to put these results into context, but given the clarity of the results in the toy domains I would have expected clearer takeaways from the empirical results as well. Edit: The authors greatly improved the paper, addressing all major reviewer concerns.", "rating": "8: Accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . Thank you for the suggestion regarding the empirical results . We clarify the takeaways from the paper below , and have worked this commentary into the most recent version of the paper : Validation of the feasibility of the PBR oracle in normal form games ( NFGs ) : the asymmetric nature of these games , in combination with the number of players and strategies involved , makes them inherently , and perhaps surprisingly , large in scale . For example , our largest NFG involves 5 players with 30 strategies each , making for > 24 million strategy profiles in total , which we note is well beyond the scale of canonical NFG domains . Overall , despite their stateless nature , we consider these NFG experiments as key empirical results , in contrast to the toy domains used in our counterexamples . Alpha-PSRO lowering NashConv in 2-player poker experiments : while Alpha-Rank does not seek to find an approximation of Nash , it nonetheless reduces the NashConv yielding extremely competitive results in comparison to an exact-Nash solver in these instances . This result is both non-obvious and quite important , in the sense of establishing Alpha-PSRO as a convenient means of training agents in > 2-player games ( where Nash is not readily computable ) . MuJoCo soccer experiments : Although noted as preliminary , a key observation can be made from these results : upon completion of training , when computing a new play distribution based on a pool of agents trained via the AlphaRank-based training approach vs. a uniform approach , the former attains essentially all of the \u2018 play probability \u2019 . This is evident in the colorbar on the far right of Appendix F , Fig.F.10 , which visualizes the post-training meta-distribution over both training pipelines . Overall , we agree this insight should have been provided more clearly in the text . Based on your feedback , we have updated the revision to integrate changes related to the above discussions . Please let us know if further clarification of any of these points are needed . Finally , on note related to Reviewer 1 and 3 \u2019 s feedback , we are investigating several additional experiments with the aim to include them in the revision before the author discussion period closes . ( We will post an update as soon as applicable regarding any new results . )"}, "1": {"review_id": "Bkl5kxrKDr-1", "review_text": "Review Update (18/11/2019) Thank you for the detailed replies and significant updates to the paper in response to all reviewers. You have comfortably addressed all of my concerns and so I have updated my score. I think the paper has improved significantly through the rebuttal stage and therefore the update in my score is also significant to match the far larger contribution to the community that the paper now represents. -- This paper considers alpha-rank as a solution concept for multi-agent reinforcement learning with a focus on its use as a meta-solver for PSRO. Based on theoretical findings showing shortcomings of using the typical best response oracle, the paper finds a necessity for a new response oracle and proposes preference-based best response. The theoretical contributions help further the community's understanding of alpha-rank but the method remains somewhat disconnected from other recent related literature. Therefore, I think the paper's subsequent impact could be significantly improved by making more direct comparison to recent results. Specifically: 1) In the 2-player games comparisons are currently made to PRD based on its use in Lanctot et al (NeurIPS, 2017) instead of the more recent PSRO Rectified Nash approach proposed by Balduzzi et al. (ICML, 2019). Please make this direct comparison or justify its exclusion. 2) The preliminary MuJoCo soccer results in Appendix G significantly increase the relevance of this work to the ICLR community given the prior publication of this environment at ICLR 2019. However, the results are currently incomplete. In particular, to again strengthen the link to existing work, comparison of the method proposed in this paper to the agents trained by population based training in Liu et al. (ICLR, 2019) would be a more informative comparison than the preliminary results presented in comparison to the na\u00efve uniform meta-solver. 3) Appendix A includes a brief literature survey. This is important material to position the paper in relation to existing work, particularly for readers not familiar with the area that will rely on this to understand the paper as a self contained reference. Please move this section into the main body of the paper and expand to fully credit the work this paper builds upon. Minor Comments: In Appendix C.4 should the reference to Figure C.7 be to Figure C.7a specifically? and the reference to Figure C. 7a be to Figure C. 7b-f inclusive? If so, I believe the available joint strategies in step 4 is missing (1,1,2) as shown in Figure C. 7f. ", "rating": "8: Accept", "reply_text": "We thank the reviewer for the detailed feedback , which we address below . We are currently integrating this feedback into the revision . Main feedback : We are currently running several additional experiments related to those suggested , with the aim to update the paper before the author discussion period closes . We will post an update as soon as new results are available . We completely agree regarding the related works section , and have moved it back to the main body in the latest revision ( Sec.6 ) .Minor comments : Thanks for pointing out the issue with figure references , which have been corrected in the revision as you specified ( please note that the referenced section and figure are now , respectively , Appendix B.4 and Fig.B.7 , due to the related works section being moved out of the appendix ) . We \u2019 ve also updated the subfigure captions to make the correspondence to the counterexample steps clear . Indeed the strategy space in Step 4 should have included ( 1,1,2 ) \u2014 thanks for catching this !"}, "2": {"review_id": "Bkl5kxrKDr-2", "review_text": "This paper extends the original PSRO paper to use an $\\alpha$-Rank based metasolver instead of the projected replicator dynamics and Nash equilibria based metasolvers in the original. To this end, the paper modifies the original idea of Best-Response (BR) oracle since it can ignore some strategies in $\\alpha$-Rank defining SSCC to introduce the idea of _preference-based_ Best-Response (PBR) oracle. The need for a different oracle is well justified especially with the visualization in the Appendix. The main contributions that the paper seems to be going for is a theoretical analysis of $\\alpha$-Rank based PSRO compared to standard PSRO. From the PBR's description (especially in Sec 4.3) it seems the paper is intereseted in expanding the population with novel agents rather than finding the \"best\" single agent which is not well defined for complex games with intransitivities. Nevertheless, it seems that BR is mostly compatible with PBR for symmetric zero-sum two-player games. The paper performs empirical experiments on different versions of poker. First set of experiments compare BR and PBR with $\\alpha$-Rank based metasolver on random games and finds that PBR does better than BR at population expansion as defined. The second set of experiments compare the metasolvers. $\\alpha$-Rank performs similarly to Nash where applicable. Moreover it's faster than Uniform (fictitious self-play) on Kuhn. Then the paper tacks on the MuJoCo soccer experiment as a teaser for ICLR crowd. Overall the paper is quite interesting from the perspective of multiagent learning and I would lean towards accepting. However the paper needs to clarify a lot of details to have any chance of being reproducible. ** Clarifications needed: - Tractability of PBR-Score and PCS-Score It's unclear how tractable these are. Moreover these were only reported for random games. What did these scores look like for the Poker games? Could you clarify how exactly these were computed? - It's somewhat unclear what the lack of convergence without novelty-bound oracle implies. Does this have to do with intransitivities in the game? - Dependence of $\\alpha$? The original $\\alpha$-Rank paper said a lot about the importance of choosing the right value for $\\alpha$. How were these chosen? Do you do the sweep after every iteration of PSRO? - Oracle in experiments? The paper fails to mention the details about the Oracles being used in the experiments. They weren't RL oracles but more details would be useful. - BR not compatible with PBR, albeit not the other way around, meaning one of the solutions you get from PBR might be BR, but can we say which one? - For MuJoCo soccer was it true PSRO or cognitive hierarchy. In general, the original PSRO paper was partly talking about the scalable approach via DCH. This paper doesn't mention that at all. So were the MuJoCo experiments with plain PSRO? What was the exact protocol there? From the appendix it's unclear how the team-vs-team meta game works with individual RL agents. Moreover how are the meta-game evaluation matrices computed in general? How many samples were needed for the Poker games and MuJoCo soccer? - The counterexamples in Appendix B3 are quite interesting. Do you have any hypotheses about the disjoint support from games' correlated equilibria?", "rating": "8: Accept", "reply_text": "We thank the reviewer for the detailed feedback . We agree that clarifying these points is useful for reproducibility and also building reader intuition on the results . Please find our point-by-point responses below , which have been integrated into the latest revision . Tractability of PBR-Score and PCS-Score : This is an important and insightful question regarding the tractability of convergence measures such as PBR- and PCS-Scores . We developed these scores to assess the quality of convergence in our examples , in a manner analogous to NashConv . The computation of these scores is , however , not tractable in general games . Notably , this is also the case for NashConv ( as it requires computation of player-wise best responses , which can be problematic even in moderately-sized games ) . Despite this , these scores remain a useful way to empirically verify the convergence characteristics in small games where they can be tractably computed . We agree that this is a useful remark for readers interested in implementing these scores , and have revised the paper to do so in Section C.3 . Additionally , we now include pseudocode , in the same section , detailing how to compute these scores . Intuition on lack of convergence without novelty-bound oracle : As the reviewer points out , the lack of convergence without a novelty-bound oracle is precisely related to game intransitivities , i.e.cycles in the game can trap the oracle without the novelty-bound constraint . We show an example of this occurring in the revised paper Appendix B.4 ( Figure B.7 ) . Specifically , SSCCs may be hidden by \u201c intermediate \u201d strategies that , while not receiving as high a payoff as current population-pool members , can actually lead to well-performing strategies outside the population . As these \u201c intermediate \u201d strategies are avoided , SSCCs are consequently not found . Note also that this is related to the common problem of action/equilibrium shadowing ( See Matignon et al. , 2012 , \u201c Independent reinforcement learners in cooperative Markov games : a survey regarding coordination problems \u201d ) . Note that per Reviewer 1 and 2 \u2019 s feedback , we have made several improvements to the descriptions of the above example , specifically appending a paragraph following Proposition 4 to better explain this intuition , updating some of the proof text in Section B.4 , and relabeling Fig B.7 \u2019 s captions . We hope these changes make the intuition clearer . Dependence on $ \\alpha $ parameter : Thanks for pointing this out . Indeed , for all alpharank results , we run a sweep over alpha after each PSRO iteration ( as recommended in the original alpharank paper ) . We have updated Section C.1 ( Experimental Procedures ) of the revised paper to clarify this . Overall , relative to the other modules of the training pipeline , we did not find this to be a computational constraint , especially for the larger ( > 2-player ) games and when using a sparse representation and solver for computing the alpharank distribution . On a related note , we have also added more details on the hyperparameters used for the projected replicator dynamics meta-solver to Section C.1 . Oracle in experiments : The oracles used in the experiments were ( exact ) best response oracles , computed by traversing the game tree . Specifically , we used OpenSpiel ( https : //github.com/deepmind/open_spiel ) as the backend for the experiments using the exact best response oracle . Specifics of the implementation can be found in https : //github.com/deepmind/open_spiel/blob/master/open_spiel/python/algorithms/best_response.py ) . We \u2019 ve updated Section C.1 ( Experimental Procedures ) to provide these details . Please let us know if this clarifies things . Many thanks ! BR-PBR compatibility : We thank the reviewer for this question , as the concept of compatibility between objectives benefitted from a clarifying example . In general , BR and PBR optimize different objectives . However , in certain types of games ( e.g. , win-loss and monotonic games , defined respectively in Propositions 5 & 6 ) , the strategy that maximizes value also maximizes the amount of other strategies beaten . In other words , this makes BR compatible with PBR , in the sense that the BR solution space is a subset of the PBR solution space . To make these properties clearer for readers , we have added an example comparing BR and PBR in a monotonic game in Figure B.8 of the appendix . In the case of Win-Loss games , PBR and BR optimize exactly the same objective , and therefore have the same solutions ."}}