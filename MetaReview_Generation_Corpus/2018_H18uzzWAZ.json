{"year": "2018", "forum": "H18uzzWAZ", "title": "Correcting Nuisance Variation using Wasserstein Distance", "decision": "Reject", "meta_review": "This is a nice but very narrow study of domain invariance in a microscopic imaging application.  Since the problem is very general, the paper should include much more substantial context, e.g. discussion of various alternative methods (e.g. the ones cited in Sun et al. 2017).  In order to contribute to the broader ICLR community, ideally the paper would also include application to more than just the one task.", "reviews": [{"review_id": "H18uzzWAZ-0", "review_text": "The paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal. A loss function based on the Wasserstein distance is used. The paper is interesting but could certainly do with more explanations. Comments: 1. It is difficult for the reader to understand a) why Wasserstein is used and b) how exactly the nuisance variation is reduced. A dedicated section on motivation is missing. 2. Does the Deep Metric network always return a '64-dim' vector? Have you checked your model using different length vectors? 3. Label the y-axis in Fig 2. 4. The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the comments and suggestions of all the reviewers , which we agreed pointed out important ways in which our work could be improved . We have amended our manuscript with more thorough descriptions and explanations , and added new results . Specific responses to the reviewer is given below : 1 . It is difficult for the reader to understand a ) why Wasserstein is used and b ) how exactly the nuisance variation is reduced . A dedicated section on motivation is missing . We have expanded a section to include a qualitative description of the Wasserstein distance and a motivating concept for our work , the Wasserstein barycenter . The idea of Wasserstein barycenter is two-fold . First , we want to match the distributions after transformation . Second , we want the perturbation as small as possible . These two ideas are reflected in our method . Although other metrics may also be used , the Wasserstein distance and similar related metrics capture relevant geometric information between the distributions . We have extended our description of the Wasserstein distance and its application in our paper . 2.Does the Deep Metric network always return a '64-dim ' vector ? Have you checked your model using different length vectors ? The deep neural network we used always returns a 64-dimensional vector by design , although we do not expect the results to be sensitive to the dimensionality , as long as there are sufficiently many data points compared to the inherent dimension of the data . Additionally , we have experimented with our network on synthetic low-dimensional data as well , obtaining the expected results . 3.Label the y-axis in Fig 2 . Done.4.The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation . We initially used early stopping mostly to show our framework works as a proof-of-concept . Early stopping provides a simpler framework under which fewer parameters required to be optimized ( i.e.the early stopping time instead of the separate regularization weights used to limit the transformation ) . In our revised manuscript we have included experiments carried out with a regularizer . Our results showed that the results from early stopping were comparable , and even better than the experiments we carried out with a regularizer , which did not have their hyperparameters fine-tuned . We agree that a principled regularizer may yield better results once all of its hyperparameters have been optimized . However , tuning the hyperparameters is a potentially difficult problem outside the scope of this work ."}, {"review_id": "H18uzzWAZ-1", "review_text": "The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or \"batches\". A network is trained to learn the transformations that minimize the Wasserstein distance between distributions. The wasserstein distance is also called the \"earth mover distance\" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another. In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions. Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network. Results are examined mainly by looking at the first two PCA components of the data. The paper presents an interesting idea and is fairly well written. However I have a few concerns: 1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017). Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \\lambda and the choice of points for gradient computation. 2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection. This section could be improved by demonstrating the approach on more datasets. 3. There is a lack comparison to other methods such as Shaham et al. (2017). Why is using earth mover distance better than MMD based distance? They only compare it to a method named CORAL and to Typical Variation Normalization (TVN). What about comparison to other batch normalization methods in biology such as SEURAT? 4. Why is the affine transform assumption valid in biology? There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences. 5. Only early stopping seems to constrain their model to be near identity. Doesn't this also prevent optimal results ? How does this compare to the near-identity constraints in resnets in Shaham et al. ? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We appreciate the comments and suggestions of all the reviewers , which we agreed pointed out important ways in which our work could be improved . We have amended our manuscript with more thorough descriptions and explanations , and added new results . Specific responses to the reviewer is given below : 1 . Most of the ideas presented in the paper rely on works by Arjovsky et al . ( 2017 ) , Gulrajani et al . ( 2017 ) , and Gulrajani et al . ( 2017 ) .Some selections , which are presented in the papers are not explained , for example , the gradient penalty , the choice of \\lambda ( now replaced by \\gamma ) and the choice of points for gradient computation . We would like to point out that while we do rely on the methods of Arjovsky et al . ( 2017 ) , Gulrajani et al . ( 2017 ) for estimating the Wasserstein distance , the application of these methods for correcting nuisance variation is novel and independent of these methods . This aspect of our work is a novel way of removing nuisance variation , a significant problem in high-throughput biological experiments . Our approach is based on minimizing the sum of pairwise Wasserstein distances of a transformed set of coordinates . This method is inspired by , but distinct from , finding the Wasserstein barycenter . We hope that our approach demonstrates a novel and general framework for removing nuisance variation . We have added more thorough explanations of our approach for approximating the Wasserstein distance , including the choice of \\lambda and the choice of points for the gradient computation . 2.The experimental results are not fully convincing , they simply compare the first two PC components on this Broad Bioimage benchmark collection . This section could be improved by demonstrating the approach on more datasets . While we use the first two PC components to illustrate the effect of our transformation , we rely on other quantitative metrics for evaluating the performance of our framework . Specifically , we use domain classification accuracy to assess the extent to which nuisance variation has been removed ( discussed in section 3.2.3 and shown in table 2 ) . We also included the k-NN MOA assignment metric to evaluate the quality of the transformed embeddings ( discussed in section 3.2.1 and shown in table 1 ) . In addition , in the revised manuscript we added another quantitative metric , the average silhouette index of the MOAs to better evaluate the effects of our transformation ( see section 3.2.2 and table 3 ) . In our original manuscript the k-NN MOA metric did not show significant differences among the evaluated methods when using cross validation leaving out half the compounds at a time . This occurred because there were not enough compounds remaining in each test/evaluation cross validation folds . However , in our revised manuscript our new analysis using a leave-one-compound-out cross validation showed that the framework can be used to attain a significant improvement in the k-NN MOA metric . Using leave-one-compound-out cross validation has been used in other studies [ Godinez , William J. , et al. \u201c A Multi-Scale Convolutional Neural Network for Phenotyping High-Content Cellular Images. \u201d Bioinformatics ( 2017 ) ] . This method represents a more realistic setting when a compound with unknown MOA takes the role of the held-out compound . The reasons we base our results on specifically the BBBC021 dataset in this paper are : This is an open dataset that has been used as a standard . We would like to produce a direct comparison with existing literature . These include : 1 . Ljosa , Vebjorn , et al . `` Comparison of methods for image-based profiling of cellular morphological responses to small-molecule treatment . '' Journal of biomolecular screening 18.10 ( 2013 ) : 1321-1329 . 2.Ando , D. Michael , Cory McLean , and Marc Berndl . `` Improving Phenotypic Measurements in High-Content Imaging Screens . '' bioRxiv ( 2017 ) : 161422 . 3.Pawlowski , Nick , et al . `` Automating Morphological Profiling with Generic Deep Convolutional Networks . '' bioRxiv ( 2016 ) : 085118 . 4.Singh , S. , et al . `` Pipeline for illumination correction of images for high\u2010throughput microscopy . '' Journal of microscopy 256.3 ( 2014 ) : 231-236 . We have tested our procedure on another dataset with promising results , but unfortunately we are unable to release it at this time . In addition , we have added the dosage response plots before and after the transformation . These plots show qualitatively that our transformation preserves the structural dosage-response . Points 3-5 will be addressed in a separate comment due to the character limit ."}, {"review_id": "H18uzzWAZ-2", "review_text": "This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation. The authors define a Wasserstein Distance Network to find a suitable affine transformation that reduces the nuisance factor. The evaluation on a real dataset yields correct results, this approach is quite general and could be applied to different problems. The contribution of this approach could be better highlighted. The early stopping criteria tend to favor suboptimal solution, indeed relying on the Cramer distance is possible improvement. As a side note, the k-NN MOA is central to for the evaluation of the proposed approach. A possible improvement is to try other means for the embedding instead of the Euclidean one. ", "rating": "7: Good paper, accept", "reply_text": "We appreciate the comments and suggestions of all the reviewers , which we agreed pointed out important ways in which our work could be improved . We have amended our manuscript with more thorough descriptions and explanations , and added new results . Specific responses to the reviewer is given below : 1 . The contribution of this approach could be better highlighted . The early stopping criteria tend to favor suboptimal solution , indeed relying on the Cramer distance is possible improvement . We remark that our main goal was to introduce a general flexible framework , and using the Wasserstein was a demonstration of a specific choice that can yield substantial improvement . In our approach , we do not separate a ` target \u2019 and ` source \u2019 distribution , may include many domains , and can have different treatments across the various domains . We highlight that other distances may be used in our framework , such as the Cramer distance or the MMD distance . This may be preferable since the Cramer distance has unbiased sample gradients ( Bellemare et al.2017 ) .Using the Cramer distance could reduce the number of steps required to adjust the Wasserstein distance approximation for each step of training the embedding transformation . We have added a discussion about early stopping versus a penalty term in our manuscript ( section 3.4.3 ) . To address the issue of potentially overfitting the stopping time , we have included a cross validation procedure based on holding out a single compound at a time . We found that the optimal stopping time was consistent regardless of the choice of the held-out compound . We discuss this in more detail in section 3.3.1 . 2.As a side note , the k-NN MOA is central to for the evaluation of the proposed approach . A possible improvement is to try other means for the embedding instead of the Euclidean one . We remark that one of the main reason we used the k-NN MOA metric is to compare our work with previous approaches in existing literature , and the reviewer is correct to point out there may be better ways to improve this metric both for validation of the quality of embeddings as well as making MOA predictions for unknown compounds . In our dataset , we also expect that the embeddings for each treatment are sufficiently localized ( as vectors , they are close to each other in the sense of having similar length and angle ) , so that the choice of centroid type would not alter them much . In the case when the embeddings are not sufficiently localized , one alternative would be to use an estimator for the Frechet mean [ Salehian , Hesamoddin , et al . `` An efficient recursive estimator of the Fr\u00e9chet mean on a hypersphere with applications to medical image analysis . '' Mathematical Foundations of Computational Anatomy ( 2015 ) ] ."}], "0": {"review_id": "H18uzzWAZ-0", "review_text": "The paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal. A loss function based on the Wasserstein distance is used. The paper is interesting but could certainly do with more explanations. Comments: 1. It is difficult for the reader to understand a) why Wasserstein is used and b) how exactly the nuisance variation is reduced. A dedicated section on motivation is missing. 2. Does the Deep Metric network always return a '64-dim' vector? Have you checked your model using different length vectors? 3. Label the y-axis in Fig 2. 4. The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the comments and suggestions of all the reviewers , which we agreed pointed out important ways in which our work could be improved . We have amended our manuscript with more thorough descriptions and explanations , and added new results . Specific responses to the reviewer is given below : 1 . It is difficult for the reader to understand a ) why Wasserstein is used and b ) how exactly the nuisance variation is reduced . A dedicated section on motivation is missing . We have expanded a section to include a qualitative description of the Wasserstein distance and a motivating concept for our work , the Wasserstein barycenter . The idea of Wasserstein barycenter is two-fold . First , we want to match the distributions after transformation . Second , we want the perturbation as small as possible . These two ideas are reflected in our method . Although other metrics may also be used , the Wasserstein distance and similar related metrics capture relevant geometric information between the distributions . We have extended our description of the Wasserstein distance and its application in our paper . 2.Does the Deep Metric network always return a '64-dim ' vector ? Have you checked your model using different length vectors ? The deep neural network we used always returns a 64-dimensional vector by design , although we do not expect the results to be sensitive to the dimensionality , as long as there are sufficiently many data points compared to the inherent dimension of the data . Additionally , we have experimented with our network on synthetic low-dimensional data as well , obtaining the expected results . 3.Label the y-axis in Fig 2 . Done.4.The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation . We initially used early stopping mostly to show our framework works as a proof-of-concept . Early stopping provides a simpler framework under which fewer parameters required to be optimized ( i.e.the early stopping time instead of the separate regularization weights used to limit the transformation ) . In our revised manuscript we have included experiments carried out with a regularizer . Our results showed that the results from early stopping were comparable , and even better than the experiments we carried out with a regularizer , which did not have their hyperparameters fine-tuned . We agree that a principled regularizer may yield better results once all of its hyperparameters have been optimized . However , tuning the hyperparameters is a potentially difficult problem outside the scope of this work ."}, "1": {"review_id": "H18uzzWAZ-1", "review_text": "The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or \"batches\". A network is trained to learn the transformations that minimize the Wasserstein distance between distributions. The wasserstein distance is also called the \"earth mover distance\" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another. In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions. Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network. Results are examined mainly by looking at the first two PCA components of the data. The paper presents an interesting idea and is fairly well written. However I have a few concerns: 1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017). Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \\lambda and the choice of points for gradient computation. 2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection. This section could be improved by demonstrating the approach on more datasets. 3. There is a lack comparison to other methods such as Shaham et al. (2017). Why is using earth mover distance better than MMD based distance? They only compare it to a method named CORAL and to Typical Variation Normalization (TVN). What about comparison to other batch normalization methods in biology such as SEURAT? 4. Why is the affine transform assumption valid in biology? There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences. 5. Only early stopping seems to constrain their model to be near identity. Doesn't this also prevent optimal results ? How does this compare to the near-identity constraints in resnets in Shaham et al. ? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We appreciate the comments and suggestions of all the reviewers , which we agreed pointed out important ways in which our work could be improved . We have amended our manuscript with more thorough descriptions and explanations , and added new results . Specific responses to the reviewer is given below : 1 . Most of the ideas presented in the paper rely on works by Arjovsky et al . ( 2017 ) , Gulrajani et al . ( 2017 ) , and Gulrajani et al . ( 2017 ) .Some selections , which are presented in the papers are not explained , for example , the gradient penalty , the choice of \\lambda ( now replaced by \\gamma ) and the choice of points for gradient computation . We would like to point out that while we do rely on the methods of Arjovsky et al . ( 2017 ) , Gulrajani et al . ( 2017 ) for estimating the Wasserstein distance , the application of these methods for correcting nuisance variation is novel and independent of these methods . This aspect of our work is a novel way of removing nuisance variation , a significant problem in high-throughput biological experiments . Our approach is based on minimizing the sum of pairwise Wasserstein distances of a transformed set of coordinates . This method is inspired by , but distinct from , finding the Wasserstein barycenter . We hope that our approach demonstrates a novel and general framework for removing nuisance variation . We have added more thorough explanations of our approach for approximating the Wasserstein distance , including the choice of \\lambda and the choice of points for the gradient computation . 2.The experimental results are not fully convincing , they simply compare the first two PC components on this Broad Bioimage benchmark collection . This section could be improved by demonstrating the approach on more datasets . While we use the first two PC components to illustrate the effect of our transformation , we rely on other quantitative metrics for evaluating the performance of our framework . Specifically , we use domain classification accuracy to assess the extent to which nuisance variation has been removed ( discussed in section 3.2.3 and shown in table 2 ) . We also included the k-NN MOA assignment metric to evaluate the quality of the transformed embeddings ( discussed in section 3.2.1 and shown in table 1 ) . In addition , in the revised manuscript we added another quantitative metric , the average silhouette index of the MOAs to better evaluate the effects of our transformation ( see section 3.2.2 and table 3 ) . In our original manuscript the k-NN MOA metric did not show significant differences among the evaluated methods when using cross validation leaving out half the compounds at a time . This occurred because there were not enough compounds remaining in each test/evaluation cross validation folds . However , in our revised manuscript our new analysis using a leave-one-compound-out cross validation showed that the framework can be used to attain a significant improvement in the k-NN MOA metric . Using leave-one-compound-out cross validation has been used in other studies [ Godinez , William J. , et al. \u201c A Multi-Scale Convolutional Neural Network for Phenotyping High-Content Cellular Images. \u201d Bioinformatics ( 2017 ) ] . This method represents a more realistic setting when a compound with unknown MOA takes the role of the held-out compound . The reasons we base our results on specifically the BBBC021 dataset in this paper are : This is an open dataset that has been used as a standard . We would like to produce a direct comparison with existing literature . These include : 1 . Ljosa , Vebjorn , et al . `` Comparison of methods for image-based profiling of cellular morphological responses to small-molecule treatment . '' Journal of biomolecular screening 18.10 ( 2013 ) : 1321-1329 . 2.Ando , D. Michael , Cory McLean , and Marc Berndl . `` Improving Phenotypic Measurements in High-Content Imaging Screens . '' bioRxiv ( 2017 ) : 161422 . 3.Pawlowski , Nick , et al . `` Automating Morphological Profiling with Generic Deep Convolutional Networks . '' bioRxiv ( 2016 ) : 085118 . 4.Singh , S. , et al . `` Pipeline for illumination correction of images for high\u2010throughput microscopy . '' Journal of microscopy 256.3 ( 2014 ) : 231-236 . We have tested our procedure on another dataset with promising results , but unfortunately we are unable to release it at this time . In addition , we have added the dosage response plots before and after the transformation . These plots show qualitatively that our transformation preserves the structural dosage-response . Points 3-5 will be addressed in a separate comment due to the character limit ."}, "2": {"review_id": "H18uzzWAZ-2", "review_text": "This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation. The authors define a Wasserstein Distance Network to find a suitable affine transformation that reduces the nuisance factor. The evaluation on a real dataset yields correct results, this approach is quite general and could be applied to different problems. The contribution of this approach could be better highlighted. The early stopping criteria tend to favor suboptimal solution, indeed relying on the Cramer distance is possible improvement. As a side note, the k-NN MOA is central to for the evaluation of the proposed approach. A possible improvement is to try other means for the embedding instead of the Euclidean one. ", "rating": "7: Good paper, accept", "reply_text": "We appreciate the comments and suggestions of all the reviewers , which we agreed pointed out important ways in which our work could be improved . We have amended our manuscript with more thorough descriptions and explanations , and added new results . Specific responses to the reviewer is given below : 1 . The contribution of this approach could be better highlighted . The early stopping criteria tend to favor suboptimal solution , indeed relying on the Cramer distance is possible improvement . We remark that our main goal was to introduce a general flexible framework , and using the Wasserstein was a demonstration of a specific choice that can yield substantial improvement . In our approach , we do not separate a ` target \u2019 and ` source \u2019 distribution , may include many domains , and can have different treatments across the various domains . We highlight that other distances may be used in our framework , such as the Cramer distance or the MMD distance . This may be preferable since the Cramer distance has unbiased sample gradients ( Bellemare et al.2017 ) .Using the Cramer distance could reduce the number of steps required to adjust the Wasserstein distance approximation for each step of training the embedding transformation . We have added a discussion about early stopping versus a penalty term in our manuscript ( section 3.4.3 ) . To address the issue of potentially overfitting the stopping time , we have included a cross validation procedure based on holding out a single compound at a time . We found that the optimal stopping time was consistent regardless of the choice of the held-out compound . We discuss this in more detail in section 3.3.1 . 2.As a side note , the k-NN MOA is central to for the evaluation of the proposed approach . A possible improvement is to try other means for the embedding instead of the Euclidean one . We remark that one of the main reason we used the k-NN MOA metric is to compare our work with previous approaches in existing literature , and the reviewer is correct to point out there may be better ways to improve this metric both for validation of the quality of embeddings as well as making MOA predictions for unknown compounds . In our dataset , we also expect that the embeddings for each treatment are sufficiently localized ( as vectors , they are close to each other in the sense of having similar length and angle ) , so that the choice of centroid type would not alter them much . In the case when the embeddings are not sufficiently localized , one alternative would be to use an estimator for the Frechet mean [ Salehian , Hesamoddin , et al . `` An efficient recursive estimator of the Fr\u00e9chet mean on a hypersphere with applications to medical image analysis . '' Mathematical Foundations of Computational Anatomy ( 2015 ) ] ."}}