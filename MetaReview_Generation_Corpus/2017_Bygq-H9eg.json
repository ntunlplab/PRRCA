{"year": "2017", "forum": "Bygq-H9eg", "title": "An Analysis of Deep Neural Network Models for Practical Applications", "decision": "Reject", "meta_review": "The paper presents an evaluation of off-the-shelf image classification architectures. The findings are not too surprising and don't provide much new insight.", "reviews": [{"review_id": "Bygq-H9eg-0", "review_text": "The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important. However, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances. Main concerns: 1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance! 2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks. 3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well. Minor concern: 1) Why did the authors choose to use batch normalization in NiN and AlexNet?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and for pointing out well identified improvement points . This is how we keep advancing in Science ! I definitely share your opinion and I have to say that these topics have not been included in this publication due to the allotted amount of time . Nevertheless , I could cover them in the next article , given also the current wide usage of this paper and its previous version , released on arXiv . Batch-normalised NiN and AlexNet have been included due to their availability as re-trained networks by the community . Now some briefs replies to your points . 1 ) The paper addresses the accuracy gain vs. performance tradeoff while training different architectures . Indeed , fine-tuning * is * the norm , but private data sets , used in realistic scenarios , are not available to the public . Therefore , it would have made little sense to publish the results . Furthermore , VGG can be compressed , because it 's huge to begin with , but I guess this was obvious . 2 ) Agreed . Still , the focus was on architecture choice for training from scratch . 3 ) Agreed . We used this analysis to design ENet , best performing architecture . The paper and blog explain in detail the design choices . Once more , I agree on everything but on the rating ; ) Thank you for your constructive criticism ."}, {"review_id": "Bygq-H9eg-1", "review_text": "The authors did solid work in collecting all the reported data. However, most findings don't seem to be too surprising to me: - Finding #1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage). - Regarding Finding #2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship. However, for this finding to be relevant, it has to hold especially for the latest generations of models. These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour. Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner. - Finding #3 seems to be a simple conclusion from finding #1: As long as slower models are better and faster models do draw the same power, finding #3 holds. - Finding #4 is again similar to finding #1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations. Maybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies. However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power. Additionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . I 'm slightly confused by the contradiction in your review : `` solid work '' and `` rejected '' . If the work is solid and therefore it 's valuable , then it should get visibility and get accepted , even though the findings are not surprising . After all , we did not know this before this work . Correct ? I mean , Newton described how an apple falls on Earth . Well , it was not that surprising an object without support will fall on the ground , still his findings have been widely used . About hardware utilisation , there was a substantial difference for architectures using kernels 3x3 with the previous cuDNN library . 1/3 third of the power was used and 3x slower inference time was obtained . Updating the system gave us uniform performance for all architectures . So , please , let me know what you think it should be improved in order to change your opinion on point 4 . Or , if it 's already flawless , please try to reconsider your verdict . Thank you ."}, {"review_id": "Bygq-H9eg-2", "review_text": "A few issues with this paper: 1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions. 2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative. 3- Paper is unfriendly to colorblind readers (or those with B/W printers) Overall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . 1- I have to say that the most unexpected results ( and therefore the one I was not absolutely confident about ) is actually finding # 2 , which comes from Fig # 9 . As reported in the paper , all these architectures lined up ( when sufficiently trained or efficiently designed ) on the grey-white boundary . 2- The power measurements have not been trivial to obtain , for which I 've redesigned the acquisition protocol half a dozen times . Nevertheless , when I initially used the cuDNN v4 , you could see a strong inverse dependance with the inference time ( https : //arxiv.org/pdf/1605.07678v2.pdf ) . Now , such strong dependency is no longer there . 3- I willingly decided to compromise B & W compatibility for intelligibility . Otherwise it would have been unfriendly for everyone . Finally , after analysing all models , we propose a novel architecture that stands out among all the other SOTAs , in terms of information density ( accuracy per tuneable parameter ) . Yet , the main objective was to provide a fair comparison among SOTAs and understand what design choices should be observed in order to create usable models ."}], "0": {"review_id": "Bygq-H9eg-0", "review_text": "The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important. However, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances. Main concerns: 1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance! 2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks. 3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well. Minor concern: 1) Why did the authors choose to use batch normalization in NiN and AlexNet?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and for pointing out well identified improvement points . This is how we keep advancing in Science ! I definitely share your opinion and I have to say that these topics have not been included in this publication due to the allotted amount of time . Nevertheless , I could cover them in the next article , given also the current wide usage of this paper and its previous version , released on arXiv . Batch-normalised NiN and AlexNet have been included due to their availability as re-trained networks by the community . Now some briefs replies to your points . 1 ) The paper addresses the accuracy gain vs. performance tradeoff while training different architectures . Indeed , fine-tuning * is * the norm , but private data sets , used in realistic scenarios , are not available to the public . Therefore , it would have made little sense to publish the results . Furthermore , VGG can be compressed , because it 's huge to begin with , but I guess this was obvious . 2 ) Agreed . Still , the focus was on architecture choice for training from scratch . 3 ) Agreed . We used this analysis to design ENet , best performing architecture . The paper and blog explain in detail the design choices . Once more , I agree on everything but on the rating ; ) Thank you for your constructive criticism ."}, "1": {"review_id": "Bygq-H9eg-1", "review_text": "The authors did solid work in collecting all the reported data. However, most findings don't seem to be too surprising to me: - Finding #1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage). - Regarding Finding #2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship. However, for this finding to be relevant, it has to hold especially for the latest generations of models. These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour. Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner. - Finding #3 seems to be a simple conclusion from finding #1: As long as slower models are better and faster models do draw the same power, finding #3 holds. - Finding #4 is again similar to finding #1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations. Maybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies. However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power. Additionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . I 'm slightly confused by the contradiction in your review : `` solid work '' and `` rejected '' . If the work is solid and therefore it 's valuable , then it should get visibility and get accepted , even though the findings are not surprising . After all , we did not know this before this work . Correct ? I mean , Newton described how an apple falls on Earth . Well , it was not that surprising an object without support will fall on the ground , still his findings have been widely used . About hardware utilisation , there was a substantial difference for architectures using kernels 3x3 with the previous cuDNN library . 1/3 third of the power was used and 3x slower inference time was obtained . Updating the system gave us uniform performance for all architectures . So , please , let me know what you think it should be improved in order to change your opinion on point 4 . Or , if it 's already flawless , please try to reconsider your verdict . Thank you ."}, "2": {"review_id": "Bygq-H9eg-2", "review_text": "A few issues with this paper: 1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions. 2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative. 3- Paper is unfriendly to colorblind readers (or those with B/W printers) Overall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . 1- I have to say that the most unexpected results ( and therefore the one I was not absolutely confident about ) is actually finding # 2 , which comes from Fig # 9 . As reported in the paper , all these architectures lined up ( when sufficiently trained or efficiently designed ) on the grey-white boundary . 2- The power measurements have not been trivial to obtain , for which I 've redesigned the acquisition protocol half a dozen times . Nevertheless , when I initially used the cuDNN v4 , you could see a strong inverse dependance with the inference time ( https : //arxiv.org/pdf/1605.07678v2.pdf ) . Now , such strong dependency is no longer there . 3- I willingly decided to compromise B & W compatibility for intelligibility . Otherwise it would have been unfriendly for everyone . Finally , after analysing all models , we propose a novel architecture that stands out among all the other SOTAs , in terms of information density ( accuracy per tuneable parameter ) . Yet , the main objective was to provide a fair comparison among SOTAs and understand what design choices should be observed in order to create usable models ."}}