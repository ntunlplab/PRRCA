{"year": "2021", "forum": "L8BElg6Qldb", "title": "Nonvacuous Loss Bounds with Fast Rates for Neural Networks via Conditional Information Measures", "decision": "Reject", "meta_review": "The paper extends results from the recent work of Steinke and Zakynthinou (SZ) for the test loss of randomized learning algorithms. They provide bounds in the single draw as well as PAC-Bayes setting. The main result is about fast rates the proof of which follows with minor modifications from the corresponding result in SZ. It is unclear to me the contribution over existing work is sufficient to merit acceptance.", "reviews": [{"review_id": "L8BElg6Qldb-0", "review_text": "The paper tackles an important concern when it comes to PAC-Bayes bounds , i.e. , the fact that such bounds apply to the risk of a stochastic predictor rather than a deterministic one . The authors propose bounds on a `` single draw '' of a predictor according to the PAC-Bayesian predictor . However , I would like the author to discuss to which extent the derivation of this `` single draw '' bound differs from the result of Hellstr\u00f5m & Durisi ( 2020 ) they refer to . Indeed , the latter is a `` slow rate '' $ O ( 1/\\sqrt { n ) } $ bound , while the proposed result is a `` fast rate '' $ O ( 1/n ) $ one . However , such bounds for randomized predictors already exist in the PAC-Bayes literature for randomized predictors . As a matter of fact , the PAC-Bayes theorem of Seeger ( 2002 ) , involving the KL divergence between two Bernoulli distributions , is tighter than both the slow rate result of Eq . ( 1 ) and the fast rate result of Eq . ( 2 ) ; the Seeger 's bound achieving fast rate when the empirical loss is zero ( see Letarte et al. , 2019 ( Thm 3 ) , for an explicit connection between Eq ( 2 ) and Seeger 's bound ) . In other words , I wonder if the proposed `` fast rate '' bound is a particular case of a general analysis obtainable from a slight generalization of Hellstr\u00f5m & Durisi ( 2020 ) . The experiments show promising results but would benefit by being extended . Figure 1 shows the bound values according to the training epoch up to 30 epochs . The training and test loss are still decreasing at this point . In order to have the complete picture , it would be important to compare the results for fully trained models . Moreover , I would like to see the bounds values in the overfitting regime , when the training error is close to zero , as the experiments are performed in Dziugate & Roy ( 2017 ) . Note that the proposed fast rate bounds only converge in this setting . Finally , a nice addition would be to comment on the possibly to learn a predictor by directly minimizing a single draw bound , as it is frequently done in the PAC-Bayes works ( e.g. , Dziugate & Roy ( 2017 ) . # # # Minor comments : - I do not understand the subscript $ i + S_i n $ of $ \\tilde Z $ ( middle of page 3 ) - It is strange that the authors cite an unpublished paper ( Guedj and Pujol 2019 ) for the canonical PAC-Bayes bound stated in the early PAC-Bayes works of McAllester . # # # References : Letarte et al. , Dichotomize and Generalize : PAC-Bayesian Binary Activated Deep Neural Networks . NeurIPS 2019 Seeger , PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification . JMLR 2002", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their positive words and insightful comments . # # # Comments regarding novelty of contributions and relation to previous literature The contributions of this paper are concerned with the * random-subset setting * introduced in ( Steinke \\ & Zakynthinou , 2020 ) , wherein the $ n $ training samples in $ \\boldsymbol { Z } ( \\boldsymbol { S } ) $ are chosen from a set $ \\widetilde { \\boldsymbol { Z } } $ of $ 2n $ samples through a random variable $ \\boldsymbol { S } $ . In our literature survey , we also review bounds obtained in the * standard setting * , where no such structure is assumed , and we simply have $ n $ training samples $ \\boldsymbol { Z } $ . As the reviewer correctly pointed out , the fast-rate PAC-Bayesian bound for the standard setting that we refer to in ( 4 ) below ( Eq . ( 2 ) in the original submission ) is not the tightest available bound . As shown in ( Letarte et al. , 2019 ) , the bound in ( Seeger , 2002 ) , which uses the KL divergence between two Bernoulli distributions , is generally tighter . A slight improvement of this bound can be found in ( Eq . ( 21 ) , McAllester , 2013 ) . By relaxing the bound in ( Eq . ( 21 ) , McAllester , 2013 ) , McAllester derives the fast-rate bound that we report in ( 4 ) below ( Eq.2 in the original submission ) . In the revised version of the paper , we have expanded the discussion around fast-rate PAC-Bayesian bounds in the standard setting to accommodate for these observations , and clarified what the tightest known PAC-Bayesian bound is for the standard setting . The relevant excerpt from the revised version of the paper is given below . * * * In the vein of ( Catoni , 2007 , Thm.1.2.6 ) and ( Seeger , 2002 , Thm.1 ) , McAllester derived the following bound ( McAllester , 2013 , Eq . ( 21 ) ) : for all $ \\gamma\\in\\mathbb { R } $ , and with probability at least $ 1-\\delta $ under $ P_ { \\boldsymbol { Z } } $ , $ $ d_\\gamma ( \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_Z ( W ) ] || \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_\\ { P_Z\\ } ( W ) ] ) \\leq \\frac { 1 } { n } \\left ( D ( P_\\ { W\\vert\\boldsymbol { Z } \\ } ||Q_W ) + \\log \\frac { 1 } { \\delta } \\right ) .\\qquad ( 2 ) $ $ Here , $ d_\\gamma ( q ||p ) =\\gamma q -\\log ( 1-p+p e^\\gamma ) $ , and one can show that $ \\sup_\\gamma d_\\gamma ( q ||p ) =d ( q ||p ) $ , where $ d ( q||p ) $ indicates the KL divergence between two Bernoulli distributions with parameters $ p $ and $ q $ respectively . Let $ q=\\mathbb { E } \\_ { P\\_ { W\\vert \\boldsymbol { Z } } } [ L_Z ( W ) ] $ and let $ c $ denote the right-hand side of ( 2 ) . To use this inequality to bound the population loss , we need to find $ $ p^ * ( q , c ) =\\sup\\\\ { p : p\\in [ 0,1 ] , d ( q ||p ) \\leq c \\\\ } .\\qquad ( 3 ) $ $ This is the largest population loss that satisfies the inequality ( 2 ) . For small $ q $ and $ c $ , we have $ p^ * ( q , c ) \\approx c $ , which gives us a fast-rate bound . More generally , for any permissible values of $ q $ and $ c $ , the bound in ( 2 ) can be weakened to obtain the fast-rate bound reported in ( McAllester , 2013 , Thm.2 ) : for all $ \\lambda\\in ( 0,1 ) $ , and with probability at least $ 1-\\delta $ under $ P_ { \\boldsymbol { Z } } $ $ $ \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_\\ { P_Z\\ } ( W ) ] \\leq\\frac { 1 } { \\lambda } \\left [ \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_Z ( W ) ] +\\frac { D ( P_ { W\\vert \\boldsymbol { Z } } ||Q_W ) +\\log\\frac { 1 } { \\delta } } { 2 ( 1-\\lambda ) n } \\right ] .\\qquad ( 4 ) $ $ Note that the faster decay in $ n $ of this bound comes at the price of a multiplication of the training loss and the KL term by a constant that is larger than $ 1 $ . As a consequence , if the training loss or the KL term are large , this multiplicative constant may make the fast-rate bound quantitatively worse than the slow-rate bound for a fixed $ n $ . Additional insights on the tightness of these bounds are provided in ( Letarte et al. , 2019 , Thm.3 ) . * * *"}, {"review_id": "L8BElg6Qldb-1", "review_text": "This paper extends results of prior work by Steinke and Zakynthinou , by providing generalization bounds in the PAC-Bayesian and single-draw settings that depend on the conditional mutual information . The emphasis in this work is on obtaining fast rates ( $ 1/n $ vs. $ 1/\\sqrt { n } $ ) . The authors also conduct empirical experiments showing how the fast rate bounds they propose can be useful for obtaining non-vacuous generalization bounds in the context of over-parameterized neural networks . I think Theorem 1 and its corollaries are a nice contribution . The paper is very well written and clear . The authors do an excellent job in explaining the relevant related work , and how their results expand upon the results of earlier work . It might be useful if the authors could indicate or explain whether their bounds suggest or motivate improved learning algorithms . For example , how should one choose a prior distribution Q over the hypothesis based on knowledge of the full training examples ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for these kind words . The approach with conditional information measures indicates that a good prior $ Q_ { W\\vert \\widetilde { \\boldsymbol { Z } } } $ can be obtained by marginalizing out the subset choice variable $ \\boldsymbol { S } $ . However , for neural networks , this is much too computationally expensive , since it would involve training the network $ 2^n $ times . Still , this observation leads to a principled approach to choose a good prior , by averaging over a couple of instances of $ \\boldsymbol { S } $ . We altered the text in the conclusion section to highlight this point . Unfortunately , predictors can not be selected by directly minimizing the bounds we present , since , for the bounds to hold , $ W $ has to be conditionally independent of $ \\widetilde { \\boldsymbol { Z } } $ and $ \\boldsymbol { S } $ given $ \\boldsymbol { Z } ( \\boldsymbol { S } ) $ . In other words , the Markov chain $ ( \\widetilde { \\boldsymbol { Z } } , \\boldsymbol { S } ) -\\boldsymbol { Z } ( \\boldsymbol { S } ) -W $ has to hold . Since the bounds depend on $ ( \\widetilde { \\boldsymbol { Z } } , \\boldsymbol { S } ) $ through more than just $ \\boldsymbol { Z } ( \\boldsymbol { S } ) $ , computing $ W $ by directly minimizing the bounds would result in a violation of this Markov property . The modified text reads as follows : * * * In particular , the random-subset setting provides a natural way to select data-dependent priors , namely by marginalizing the learning algorithm $ P_ { W\\vert\\widetilde { \\boldsymbol { Z } } \\boldsymbol { S } } $ over $ \\boldsymbol { S } $ , either exactly or approximately . Such data-dependent priors are a key element in obtaining tight information-theoretic generalization bounds ( Dziugaite et al. , 2020 ) . * * *"}, {"review_id": "L8BElg6Qldb-2", "review_text": "This paper derives bounds on the test loss under the random-subset setting , which are expressed with conditional information measures , and have fast rates with respect to the sample size n. The derived bounds are compared with the practical performance of deep neural networks ( DNNs ) trained on the MNIST and Fashion-MNIST data sets . Although the derived bounds can be somewhat loose practically as it can be larger than those of slow-rates , the paper provides a steady theoretical progress on the evaluation of loss bounds using conditional information measures . Although the experiments with NNs and practical data sets are interesting , they are a little unsatisfactory for demonstrating the significance of the derived fast-rate bounds . p.5 , main theorem : It would be nicer to explain what novel important proof techniques are required , if any , to derive the fast-rate bounds in Section 3. p.6 , computation of \\mu_2 : Since NNs have hierarchical structures , computing the average of their weights does not seem so good . I wonder if computed \\mu_2 are close to zero and if there is any other possible ways to compute \\mu_2 . p.7 , experiments on MNISTs : Although the experiments demonstrate that the fast-rate bound can in fact be tighter than the slow-rate bound in practical cases , I wonder if some experiments should be designed with changing the sample size n in order to compare fast and slow rate bounds . As the training and test errors reported in Fig.1 are close to each other , isn \u2019 t it better to produce any overfitting situation , even for synthetic data ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive feedback and relevant comments . 1.The main contribution , in terms of the proof technique , is that we show how to combine two recent developments to harness the benefits from each . Specifically , we combine the fast-rate derivation of ( Steinke \\ & Zakynthinou , 2020 ) , which itself builds upon ( McAllester , 2013 ) , with the exponential-inequality approach of ( Hellstr\u00f6m \\ & Durisi , 2020b ) , which allows one to derive bounds on the average , PAC-Bayes , and single-draw loss . We added the following sentence after Theorem 1 to better highlight these aspects . * * * We now present an exponential inequality from which several test loss bounds can be derived , in a similar manner as was done in ( Hellstr\u00f6m & Durisi , 2020b ) . The derivation , which echos part of the proof of ( Steinke & Zakynthinou , 2020 , Thm.2 . ( 3 ) ) , is provided in Appendix A.2 . This result and its proof illustrate how to combine the exponential-inequality approach from ( Hellstr\u00f6m & Durisi , 2020b ) with fast-rate derivations , like those employed in ( Steinke & Zakynthinou , 2020 , Thm.2 . ( 3 ) ) and ( McAllester , 2013 , Thm.2 ) for the standard setting . * * * 2.When constructing the prior , we build 10 subsets from the $ 2n $ elements of $ \\widetilde { \\boldsymbol { Z } } $ , each of size $ n $ , and train a neural network on each of these . Thus , we get 10 different parameter vectors $ \\boldsymbol U_1 , \\dots , \\boldsymbol { U } _ { 10 } $ , each containing a list of the weights of the trained networks . To compute $ \\boldsymbol\\mu_2 $ , we average over these vectors : $ \\boldsymbol\\mu_2 = \\frac { 1 } { 10 } ( \\boldsymbol U_1+\\dots+\\boldsymbol { U } _ { 10 } ) $ . Thus , $ \\boldsymbol { \\mu } _2 $ is a vector , the size of which equals the number of parameters in the network , and when performing the averaging , we only sum weights that correspond to the same unit in the network architecture . Thus , this method of computing $ \\boldsymbol { \\mu } _2 $ respects the structure of the network , including its hierarchical nature . The parameter $ \\boldsymbol { \\mu } _2 $ is typically not close to the zero vector . 3.In response to these comments , we are currently extending our numerical results . For all experiments , in order to achieve smaller training errors , we changed our optimization procedure to SGD with momentum . We will update the figure in the main body of the paper to show the dependence of the bounds on the sample size $ n $ rather than the number of epochs . For a given $ n $ , we will show results for networks that are trained until a certain threshold is reached . In line with the results in ( Dziugaite et al. , 2020 , Fig.4 ) , we will use a threshold based on a training error of $ 0.03 $ for MNIST and $ 0.125 $ for Fashion-MNIST . Furthermore , in order to show a situation where the training and test errors differ significantly , we are running experiments with partially corrupted labels . Specifically , in order to make training easier , we consider a binarized version of MNIST where the digits $ 0 , \\dots,4 $ are combined into one class and $ 5 , \\dots,9 $ into another . We then set the labels of a fixed proportion of both the training and test sets randomly . In a new section in the appendices , we will add a plot to show how the bounds behave for different proportions of randomized labels . Finally , we are running experiments for more epochs than what was presented in the original version of this paper . In the aforementioned new appendix , we will also add a plot of this to illustrate the bounds for more fully trained networks , where the training error is close to zero . All of these results will be included in a revised version of the paper that we intend to upload in the coming days ."}], "0": {"review_id": "L8BElg6Qldb-0", "review_text": "The paper tackles an important concern when it comes to PAC-Bayes bounds , i.e. , the fact that such bounds apply to the risk of a stochastic predictor rather than a deterministic one . The authors propose bounds on a `` single draw '' of a predictor according to the PAC-Bayesian predictor . However , I would like the author to discuss to which extent the derivation of this `` single draw '' bound differs from the result of Hellstr\u00f5m & Durisi ( 2020 ) they refer to . Indeed , the latter is a `` slow rate '' $ O ( 1/\\sqrt { n ) } $ bound , while the proposed result is a `` fast rate '' $ O ( 1/n ) $ one . However , such bounds for randomized predictors already exist in the PAC-Bayes literature for randomized predictors . As a matter of fact , the PAC-Bayes theorem of Seeger ( 2002 ) , involving the KL divergence between two Bernoulli distributions , is tighter than both the slow rate result of Eq . ( 1 ) and the fast rate result of Eq . ( 2 ) ; the Seeger 's bound achieving fast rate when the empirical loss is zero ( see Letarte et al. , 2019 ( Thm 3 ) , for an explicit connection between Eq ( 2 ) and Seeger 's bound ) . In other words , I wonder if the proposed `` fast rate '' bound is a particular case of a general analysis obtainable from a slight generalization of Hellstr\u00f5m & Durisi ( 2020 ) . The experiments show promising results but would benefit by being extended . Figure 1 shows the bound values according to the training epoch up to 30 epochs . The training and test loss are still decreasing at this point . In order to have the complete picture , it would be important to compare the results for fully trained models . Moreover , I would like to see the bounds values in the overfitting regime , when the training error is close to zero , as the experiments are performed in Dziugate & Roy ( 2017 ) . Note that the proposed fast rate bounds only converge in this setting . Finally , a nice addition would be to comment on the possibly to learn a predictor by directly minimizing a single draw bound , as it is frequently done in the PAC-Bayes works ( e.g. , Dziugate & Roy ( 2017 ) . # # # Minor comments : - I do not understand the subscript $ i + S_i n $ of $ \\tilde Z $ ( middle of page 3 ) - It is strange that the authors cite an unpublished paper ( Guedj and Pujol 2019 ) for the canonical PAC-Bayes bound stated in the early PAC-Bayes works of McAllester . # # # References : Letarte et al. , Dichotomize and Generalize : PAC-Bayesian Binary Activated Deep Neural Networks . NeurIPS 2019 Seeger , PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification . JMLR 2002", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their positive words and insightful comments . # # # Comments regarding novelty of contributions and relation to previous literature The contributions of this paper are concerned with the * random-subset setting * introduced in ( Steinke \\ & Zakynthinou , 2020 ) , wherein the $ n $ training samples in $ \\boldsymbol { Z } ( \\boldsymbol { S } ) $ are chosen from a set $ \\widetilde { \\boldsymbol { Z } } $ of $ 2n $ samples through a random variable $ \\boldsymbol { S } $ . In our literature survey , we also review bounds obtained in the * standard setting * , where no such structure is assumed , and we simply have $ n $ training samples $ \\boldsymbol { Z } $ . As the reviewer correctly pointed out , the fast-rate PAC-Bayesian bound for the standard setting that we refer to in ( 4 ) below ( Eq . ( 2 ) in the original submission ) is not the tightest available bound . As shown in ( Letarte et al. , 2019 ) , the bound in ( Seeger , 2002 ) , which uses the KL divergence between two Bernoulli distributions , is generally tighter . A slight improvement of this bound can be found in ( Eq . ( 21 ) , McAllester , 2013 ) . By relaxing the bound in ( Eq . ( 21 ) , McAllester , 2013 ) , McAllester derives the fast-rate bound that we report in ( 4 ) below ( Eq.2 in the original submission ) . In the revised version of the paper , we have expanded the discussion around fast-rate PAC-Bayesian bounds in the standard setting to accommodate for these observations , and clarified what the tightest known PAC-Bayesian bound is for the standard setting . The relevant excerpt from the revised version of the paper is given below . * * * In the vein of ( Catoni , 2007 , Thm.1.2.6 ) and ( Seeger , 2002 , Thm.1 ) , McAllester derived the following bound ( McAllester , 2013 , Eq . ( 21 ) ) : for all $ \\gamma\\in\\mathbb { R } $ , and with probability at least $ 1-\\delta $ under $ P_ { \\boldsymbol { Z } } $ , $ $ d_\\gamma ( \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_Z ( W ) ] || \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_\\ { P_Z\\ } ( W ) ] ) \\leq \\frac { 1 } { n } \\left ( D ( P_\\ { W\\vert\\boldsymbol { Z } \\ } ||Q_W ) + \\log \\frac { 1 } { \\delta } \\right ) .\\qquad ( 2 ) $ $ Here , $ d_\\gamma ( q ||p ) =\\gamma q -\\log ( 1-p+p e^\\gamma ) $ , and one can show that $ \\sup_\\gamma d_\\gamma ( q ||p ) =d ( q ||p ) $ , where $ d ( q||p ) $ indicates the KL divergence between two Bernoulli distributions with parameters $ p $ and $ q $ respectively . Let $ q=\\mathbb { E } \\_ { P\\_ { W\\vert \\boldsymbol { Z } } } [ L_Z ( W ) ] $ and let $ c $ denote the right-hand side of ( 2 ) . To use this inequality to bound the population loss , we need to find $ $ p^ * ( q , c ) =\\sup\\\\ { p : p\\in [ 0,1 ] , d ( q ||p ) \\leq c \\\\ } .\\qquad ( 3 ) $ $ This is the largest population loss that satisfies the inequality ( 2 ) . For small $ q $ and $ c $ , we have $ p^ * ( q , c ) \\approx c $ , which gives us a fast-rate bound . More generally , for any permissible values of $ q $ and $ c $ , the bound in ( 2 ) can be weakened to obtain the fast-rate bound reported in ( McAllester , 2013 , Thm.2 ) : for all $ \\lambda\\in ( 0,1 ) $ , and with probability at least $ 1-\\delta $ under $ P_ { \\boldsymbol { Z } } $ $ $ \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_\\ { P_Z\\ } ( W ) ] \\leq\\frac { 1 } { \\lambda } \\left [ \\mathbb { E } \\_\\ { P_\\ { W\\vert Z\\ } \\ } [ L_Z ( W ) ] +\\frac { D ( P_ { W\\vert \\boldsymbol { Z } } ||Q_W ) +\\log\\frac { 1 } { \\delta } } { 2 ( 1-\\lambda ) n } \\right ] .\\qquad ( 4 ) $ $ Note that the faster decay in $ n $ of this bound comes at the price of a multiplication of the training loss and the KL term by a constant that is larger than $ 1 $ . As a consequence , if the training loss or the KL term are large , this multiplicative constant may make the fast-rate bound quantitatively worse than the slow-rate bound for a fixed $ n $ . Additional insights on the tightness of these bounds are provided in ( Letarte et al. , 2019 , Thm.3 ) . * * *"}, "1": {"review_id": "L8BElg6Qldb-1", "review_text": "This paper extends results of prior work by Steinke and Zakynthinou , by providing generalization bounds in the PAC-Bayesian and single-draw settings that depend on the conditional mutual information . The emphasis in this work is on obtaining fast rates ( $ 1/n $ vs. $ 1/\\sqrt { n } $ ) . The authors also conduct empirical experiments showing how the fast rate bounds they propose can be useful for obtaining non-vacuous generalization bounds in the context of over-parameterized neural networks . I think Theorem 1 and its corollaries are a nice contribution . The paper is very well written and clear . The authors do an excellent job in explaining the relevant related work , and how their results expand upon the results of earlier work . It might be useful if the authors could indicate or explain whether their bounds suggest or motivate improved learning algorithms . For example , how should one choose a prior distribution Q over the hypothesis based on knowledge of the full training examples ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for these kind words . The approach with conditional information measures indicates that a good prior $ Q_ { W\\vert \\widetilde { \\boldsymbol { Z } } } $ can be obtained by marginalizing out the subset choice variable $ \\boldsymbol { S } $ . However , for neural networks , this is much too computationally expensive , since it would involve training the network $ 2^n $ times . Still , this observation leads to a principled approach to choose a good prior , by averaging over a couple of instances of $ \\boldsymbol { S } $ . We altered the text in the conclusion section to highlight this point . Unfortunately , predictors can not be selected by directly minimizing the bounds we present , since , for the bounds to hold , $ W $ has to be conditionally independent of $ \\widetilde { \\boldsymbol { Z } } $ and $ \\boldsymbol { S } $ given $ \\boldsymbol { Z } ( \\boldsymbol { S } ) $ . In other words , the Markov chain $ ( \\widetilde { \\boldsymbol { Z } } , \\boldsymbol { S } ) -\\boldsymbol { Z } ( \\boldsymbol { S } ) -W $ has to hold . Since the bounds depend on $ ( \\widetilde { \\boldsymbol { Z } } , \\boldsymbol { S } ) $ through more than just $ \\boldsymbol { Z } ( \\boldsymbol { S } ) $ , computing $ W $ by directly minimizing the bounds would result in a violation of this Markov property . The modified text reads as follows : * * * In particular , the random-subset setting provides a natural way to select data-dependent priors , namely by marginalizing the learning algorithm $ P_ { W\\vert\\widetilde { \\boldsymbol { Z } } \\boldsymbol { S } } $ over $ \\boldsymbol { S } $ , either exactly or approximately . Such data-dependent priors are a key element in obtaining tight information-theoretic generalization bounds ( Dziugaite et al. , 2020 ) . * * *"}, "2": {"review_id": "L8BElg6Qldb-2", "review_text": "This paper derives bounds on the test loss under the random-subset setting , which are expressed with conditional information measures , and have fast rates with respect to the sample size n. The derived bounds are compared with the practical performance of deep neural networks ( DNNs ) trained on the MNIST and Fashion-MNIST data sets . Although the derived bounds can be somewhat loose practically as it can be larger than those of slow-rates , the paper provides a steady theoretical progress on the evaluation of loss bounds using conditional information measures . Although the experiments with NNs and practical data sets are interesting , they are a little unsatisfactory for demonstrating the significance of the derived fast-rate bounds . p.5 , main theorem : It would be nicer to explain what novel important proof techniques are required , if any , to derive the fast-rate bounds in Section 3. p.6 , computation of \\mu_2 : Since NNs have hierarchical structures , computing the average of their weights does not seem so good . I wonder if computed \\mu_2 are close to zero and if there is any other possible ways to compute \\mu_2 . p.7 , experiments on MNISTs : Although the experiments demonstrate that the fast-rate bound can in fact be tighter than the slow-rate bound in practical cases , I wonder if some experiments should be designed with changing the sample size n in order to compare fast and slow rate bounds . As the training and test errors reported in Fig.1 are close to each other , isn \u2019 t it better to produce any overfitting situation , even for synthetic data ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive feedback and relevant comments . 1.The main contribution , in terms of the proof technique , is that we show how to combine two recent developments to harness the benefits from each . Specifically , we combine the fast-rate derivation of ( Steinke \\ & Zakynthinou , 2020 ) , which itself builds upon ( McAllester , 2013 ) , with the exponential-inequality approach of ( Hellstr\u00f6m \\ & Durisi , 2020b ) , which allows one to derive bounds on the average , PAC-Bayes , and single-draw loss . We added the following sentence after Theorem 1 to better highlight these aspects . * * * We now present an exponential inequality from which several test loss bounds can be derived , in a similar manner as was done in ( Hellstr\u00f6m & Durisi , 2020b ) . The derivation , which echos part of the proof of ( Steinke & Zakynthinou , 2020 , Thm.2 . ( 3 ) ) , is provided in Appendix A.2 . This result and its proof illustrate how to combine the exponential-inequality approach from ( Hellstr\u00f6m & Durisi , 2020b ) with fast-rate derivations , like those employed in ( Steinke & Zakynthinou , 2020 , Thm.2 . ( 3 ) ) and ( McAllester , 2013 , Thm.2 ) for the standard setting . * * * 2.When constructing the prior , we build 10 subsets from the $ 2n $ elements of $ \\widetilde { \\boldsymbol { Z } } $ , each of size $ n $ , and train a neural network on each of these . Thus , we get 10 different parameter vectors $ \\boldsymbol U_1 , \\dots , \\boldsymbol { U } _ { 10 } $ , each containing a list of the weights of the trained networks . To compute $ \\boldsymbol\\mu_2 $ , we average over these vectors : $ \\boldsymbol\\mu_2 = \\frac { 1 } { 10 } ( \\boldsymbol U_1+\\dots+\\boldsymbol { U } _ { 10 } ) $ . Thus , $ \\boldsymbol { \\mu } _2 $ is a vector , the size of which equals the number of parameters in the network , and when performing the averaging , we only sum weights that correspond to the same unit in the network architecture . Thus , this method of computing $ \\boldsymbol { \\mu } _2 $ respects the structure of the network , including its hierarchical nature . The parameter $ \\boldsymbol { \\mu } _2 $ is typically not close to the zero vector . 3.In response to these comments , we are currently extending our numerical results . For all experiments , in order to achieve smaller training errors , we changed our optimization procedure to SGD with momentum . We will update the figure in the main body of the paper to show the dependence of the bounds on the sample size $ n $ rather than the number of epochs . For a given $ n $ , we will show results for networks that are trained until a certain threshold is reached . In line with the results in ( Dziugaite et al. , 2020 , Fig.4 ) , we will use a threshold based on a training error of $ 0.03 $ for MNIST and $ 0.125 $ for Fashion-MNIST . Furthermore , in order to show a situation where the training and test errors differ significantly , we are running experiments with partially corrupted labels . Specifically , in order to make training easier , we consider a binarized version of MNIST where the digits $ 0 , \\dots,4 $ are combined into one class and $ 5 , \\dots,9 $ into another . We then set the labels of a fixed proportion of both the training and test sets randomly . In a new section in the appendices , we will add a plot to show how the bounds behave for different proportions of randomized labels . Finally , we are running experiments for more epochs than what was presented in the original version of this paper . In the aforementioned new appendix , we will also add a plot of this to illustrate the bounds for more fully trained networks , where the training error is close to zero . All of these results will be included in a revised version of the paper that we intend to upload in the coming days ."}}