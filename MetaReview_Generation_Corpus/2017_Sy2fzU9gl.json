{"year": "2017", "forum": "Sy2fzU9gl", "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework", "decision": "Accept (Poster)", "meta_review": "This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions.", "reviews": [{"review_id": "Sy2fzU9gl-0", "review_text": "The paper proposes beta-VAE which strengthen the KL divergence between the recognition model and the prior to limit the capacity of latent variables while sacrificing the reconstruction error. This allows the VAE model to learn more disentangled representation. The main concern is that the paper didn't present any quantitative result on log likelihood estimation. On the quality of generated samples, although the beta-VAE learns disentangled representation, the generated samples are not as realistic as those based on generative adversarial network, e.g., InfoGAN. Beta-VAE learns some interpretable factors of variation, but it still remains unclear why it is a better (or more efficient) representation than that of standard VAE. In experiment, what is the criteria for cross-validation on hyperparameter \\beta? There also exists other ways to limit the capacity of the model. The simplest way is to reduce the latent variable dimension. I am wondering how the proposed beta-VAE is a better model than the VAE with reduced, or optimal latent variable dimension. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his work , but given their review we are afraid they may have misunderstood the point of our paper and did not provide a fair assessment of our contribution . We hope our responses below and the comments of the other reviewers may help clarify the scope of our work and its significance . 1.We do not present any quantitative result on log-likelihood estimation , because it is not the main point of our paper ( as also pointed out by AnonReviewer3 ) . This also relates to the reviewer \u2019 s comment about the quality of beta-VAE samples and how they compare to those by InfoGAN . The quality of reconstructions of beta-VAE and InfoGAN are in line with the base methods the two approaches are built upon - the VAE and GAN . Hence our reconstructions are blurrier than those of InfoGAN due to the limitations of VAEs . Saying this , InfoGAN suffers from the shortcomings of GANS , such as training instability and mode jumping . We stress again : we do not strive in this paper for good quality reconstructions ; the main point of our paper is the quality of the learned representations i.e.the disentangling capability of our approach , and in this regard we clearly demonstrate that our method outperforms InfoGAN , which is the current state of the art fully unsupervised method for disentangled factor learning . In addition , we address the other main shortcoming of the InfoGAN paper ( and indeed , most other unsupervised representation-learning papers ) : a lack of justification for the conclusion apart from weak empirical evidence , which we address by our quantitative disentanglement metric . 2.The benefits of the representations learnt by beta-VAE over those learnt by a standard VAE come from their disentangled property . The benefits of disentangled representations have been argued for by many , including Bengio ( 2013 ) and most recently in a review article on representation learning that has just come out on arxiv ( https : //arxiv.org/abs/1612.05299 ) . These benefits include novel example generation through interpretable factor recombination ( problems that arise from recombining entangled factors can be seen in the reduced quality of samples generated by an entangled VAE vs a disentangled beta-VAE as shown in Figure 9 , which we have now added to the appendix ) , compression ( according to the minimum description length principle ) , and performance in zero-shot inference , transfer and domain adaptation scenarios . We have amended our introduction to present these benefits more clearly . Our current manuscript describes a new state of the art framework for disentangled factor learning . We leave the demonstration of benefits that arise from learning such disentangled representations to future work . We have , however , added a new section in Appendix A.5 , as well as Table 2 to describe the performance of a simple classifier trying to predict the data generative factor values using different latent representations . We find that such a classifier performs significantly better using the disentangled representation of beta-VAE compared to the entangled representations of VAE . 3.We did not cross-validate the parameter \\beta , as this doesn \u2019 t really make sense in an unsupervised learning setting . Indeed , as is usually the case in the literature ( see InfoGAN paper for example ) , we train on the full dataset and assess characteristics of the representation obtained . As we mention in the text , in this current work we chose \\beta using visual assessments of the latent traversals . Alternatively , we also mention that our disentanglement metric score could be used for this purpose . This is validated by the fact that \\beta values found to be optimal through the visual assessment heuristic on the dataset of 2D shapes were also found to be within the optimal range of \\beta \u2019 s as assessed with the disentanglement metric . We have now also included Appendix A.5 as further validation ; in this Appendix , we use the \\beta-VAE representation to directly predict the values of the ground truth factors on the 2D shapes dataset with a linear classifier . Although this does not measure disentangling directly ( and hence is not used as our preferred disentangling metric ) , it further backs up the efficacy of \\beta-VAE relative to other models in the literature . Additionally , we always train multiple networks with random initialisations of weights for every value of \\beta we tried ( as can be seen in Figure 6 ) . We did not see any strong variations in their behaviour , and hence we believe that \\beta is not a particularly sensitive parameter . 4.We have trained VAEs with different latent sizes ( z_size \\in [ 2 , 10 ] ) and found that reducing z_size was linearly * reducing * the disentangling metric score . In particular , a VAE with the \u201c optimal \u201d z_size ( e.g.z_size = 5 for our 2D shape dataset , which has 5 ground truth factors of variation ) was not able to learn latents corresponding to the ground truth generative factors , in contrast with the beta-VAE ( as shown in Fig 7 and by the disentangling metric score ) . Hence beta-VAE is indeed different from a VAE with a reduced/optimal latent channel capacity . We hope this addresses the reviewer \u2019 s concerns and welcome any further feedback ."}, {"review_id": "Sy2fzU9gl-1", "review_text": "Summary === This paper presents Beta-VAE, an augmented Variational Auto-Encoder which learns disentangled representations. The VAE objective is derived as an approximate relaxation of a constrained optimization problem where the constraint matches the latent code of the encoder to a prior. When KKT multiplier beta on this constraint is set to 1 the result is the original VAE objective, but when beta > 1 we obtain Beta-VAE, which simply increases the penalty on the KL divergence term. This encourages the model to learn a more efficient representation because the capacity of the latent representation is more limited by beta. The distribution of the latent representation is rewarded more when factors are independent because the prior (an isotropic Gaussian) encourages independent factors, so the representation should also be disentangled. A new metric is proposed to evaluate the degree of disentanglement. Given a setting in which some disentangled latent factors are known, many examples are generated which differ in all of these factors except one. These examples are encoded into the learned latent representation and a simple classifier is used to predict which latent factor was kept constant. If the learned representation does not disentangle the constant factor then the classifier will more easily confuse factors and its accuracy will be lower. This accuracy is the final number reported. A synthetic dataset of 2D shapes with known latent factors is created to test the proposed metric and Beta-VAE outperforms a number of baselines (notably InfoGAN and the semi-supervised DC-IGN). Qualitative results show that Beta-VAE learns disentangled factors on the 3D chairs dataset, a dataset of 3D faces, and the celebA dataset of face images. The effect of varying Beta is also evaluated using the proposed metric and the latent factors learned on the 2D shapes dataset are explored in detail. Strengths === * Beta-VAE is simple and effective. * The proposed metric is a novel way of testing whether ground truth factors of variation have been identified. * There is extensive comparison to relevant baselines. Weaknesses === * Section 3 describes the proposed disentanglement metric, however I feel I need to read the caption of the associated figure (I thank for adding that) and Appendix 4 to understand the metric intuitively or in detail. It would be easier to read this section if a clear intuition preceeded a detailed description and I think more space should be devoted to this in the paper. * Appendix 4: Why was the bottom 50% of the resulting scores discarded? * As indicated in pre-review comments, the disentanglement metric is similar to a measure of correlation between latent features. Could the proposed metric be compared to a direct measure of cross-correlation between latent factors estimated over the 2D shapes dataset? * The end of section 4.2 observes that high beta values result in low disentanglement, which suggests the most efficient representation is not disentangled. This seems to disagree with the intuition from the approach section that more efficient representations should be disentangled. It would be nice to see discussion of potential reasons for this disagreement. * The writing is somewhat dense. Overall Evaluation === The core idea is novel, simple and extensive tests show that it is effective. The proposed evaluation metric is novel might come into broader use. The main downside to the current version of this paper is the presentation, which provides sufficient detail but could be more clear.", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to thank the reviewer for their thorough review and helpful suggestions . 1.We acknowledge the importance of clearly explaining our disentangled metric and we agree that our initial attempt can be improved . In order to address that , we added three new paragraphs at the beginning of Section 3 to provide an intuitive explanation for our disentanglement metric . We also rewrote the end of that section to simplify the formal presentation of the metric . We hope this provides enough background and support for why we devised it that way and what exactly it is assessing . 2.With respect to the bottom 50 % question : we have added a sentence at the end of the second paragraph of Appendix A.4 to address it . The bottom 50 % of the resulting scores were discarded to control for the outlier results from the few experiments that diverged during training . 3.Thank you for the suggestion . The cross-correlation fails to fully capture the properties we would look for in a disentanglement metric , as we now discuss in the new first 3 paragraphs of Section 3 . We did calculate it though ( average absolute values of upper triangular terms in correlation matrix , estimated over the entire dataset ) , but unfortunately found that the cross-correlation does not seem to provide an appropriate measure ( we \u2019 re also concerned with the large standard deviations ) . Here are the values we obtained : PCA : 4.7e-9 ( std 5e-9 ) // ICA : 1.15e-6 ( std 9e-7 ) // DC-IGN : 0.15 ( std 0.16 ) // InfoGAN : 0.09 ( std 0.08 ) // VAE 0.039 ( std 0.04 ) // beta-VAE : 0.057 ( std : 0.053 ) As expected , PCA and ICA produce the least correlated latents , since these approaches directly optimise for reducing cross-correlation between latents . The other approaches , including the semi-supervised DC-IGN , produce more correlated latents . However , these are nevertheless disentangled according to a human interpretation of what disentangled factors should look like . This suggests that humanly interpretable disentangled factors of variation are still somewhat correlated in the real world , which suggests that PCA and ICA may be optimising the wrong objective for interpretable disentangling . Hence we think that our metric provides a better test for interpretable disentangled representations , which a simple cross-correlation fails to capture . 4.We have added a sentence in the relevant paragraph in Section 4.2 . If $ \\beta $ is too high and the resulting capacity of the latent channel is lower than the number of data generative factors , then the learnt representation necessarily has to be entangled ( as we are compressing more than what a factorial representation would require ) . We hope this addresses the reviewer \u2019 s concerns and welcome any further feedback . [ EDIT : sorry , we initially entered wrong cross-correlation values for VAE and beta-VAE , but have now corrected them ]"}, {"review_id": "Sy2fzU9gl-2", "review_text": " This paper proposes the beta-VAE, which is a reasonable but also straightforward generalization of the standard VAE. In particular, a weighting factor beta is added for the KL-divergence term to balance the likelihood and KL-divergence. Experimental results show that tuning this weighting factor is important for learning disentangled representations. A linear-classifier based protocol is proposed for measuring the quality of disentanglement. Impressive illustrations on manipulating latent variables are shown in the paper. Learning disentangled representations without supervision is an important topic. Showing the effectiveness of VAE for this task is interesting. Generalizing VAE with a weighting factor is straightforward (though reformulating VAE is also interesting), the main contribution of this paper is on the empirical side. The proposed protocol for measuring disentangling quality is reasonable. Establishing protocol is one important methodology contribution of this paper, but the presentation of Section 3 is still not good. Little motivation is provided at the beginning of Section 3. Figure 2 is a summary of the algorithm, which is helpful, but it still necessary to intuitively explain the motivation at the first place (e.g., what you expect if a factor is disentangled, and why the performance of a classifier can reflect such an expectation). Moreover, 1) z_diff appeared without any definition in the main text. 2) Use \u201cdecoding\u201d for x~Sim(v,w) may make people confuse the ground truth sampling procedure w ith the trained decoder. The illustrative figures on traversing the disentangled factor are impressive, though image generation quality is not as good as InfoGAN (not the main point of this paper). However, 1) it will be helpful to discuss if the good disentangling quality only attribute to the beta factor and VAE framework. For example, the training data in this paper seems to be densely sampled for the visualized factors. Does the sampling density play a critical role? 2) Not too many qualitative results are provided for each experiment? Adding more figures (e.g., in appendix) to cover more factors and seeding images can strength the conclusions drawn in this paper. 3) Another detailed question related to the generalizability of the model: are the seeding image for visualizing faces from unseen subjects or subjects in the training set? (maybe I missed something here.) Quantitative results are only presented for the synthesized 2D shape. What hinders this paper from reporting quantitative numbers on real data (e.g., the 2D and 3D face data)? One possible reason is that not all factors can be disentangled for real data, but it is still feasible to pick up some well-defined factor to measure the quantitative performance. Quantitative performance is only measured by the proposed protocol. Since the effectiveness of the protocol is something the paper need to justify, reporting quantitative results using simpler protocol is helpful both for demonstrating the disentangling quality and for justifying the proposed protocol (consistency with other measurement). A simple experiment is facial identity recognition and pose estimation using disentangled features on a standard test set (like in Reed et al, ICML 2014). In Figure 6 (left), why ICA is worse than PCA for disentanglement? Is it due to the limitation of the ICA algorithm or some other reasons? In Figure 6 (right), what is \u201cfactor change accuracy\u201d? According to Appendix A.4 (which is not referred to in the main text), it is the \u201cDisentanglement metric score\u201d. Is that right? If so Figure 6 (right) shows the reconstruction results for the best disentanglement metric score. Then, 1) how about random generation or traversing along a disentangled factor? 2) more importantly, how is the reconstruction/generation results when the disentanglement metric score is suboptimal. Overall, the results presented in this paper are very interesting, but there are many details to be clarified. Moreover, more quantitative results are also needed. I hope at least some of the above concerns can be addressed. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are very grateful to the reviewer for their very thorough assessment of our work and the numerous suggestions . We have made a number of changes to fully address them . 1.About our presentation of our disentanglement metric , this was a concern shared with Reviewer 3 . We have now addressed this by introducing three new paragraphs at the start of Section 3 to provide an intuitive explanation of our disentanglement metric . We also rewrote the end of Section 3 to simplify the presentation . 2.Z_diff is now properly defined , sorry for this oversight . 3.We changed \u2018 decode \u2019 to \u2018 simulate \u2019 and \u2018 re-encode \u2019 to \u2018 infer \u2019 in order to avoid confusion between our disentanglement metric and the trained decoder , thank you for mentioning it . 4.We have added a sentence in Conclusions with a reference to Section A.8 in the Appendix . This section discusses the effects of data generative factor sample density on the learnt representations of beta-VAE , with empirical results . 5.We have added extra traversal plots in the appendix , with more seeding images . These include all latents that learnt to be informative when presented with 3D chairs or CelebA . 6.About the generalizability of our model : currently all seeding images are from the same full training set . As done in this literature [ e.g.InfoGAN ] , we did not hold out any test set . This would involve handling a domain adaptation scenario , which we are interested in tackling but decided to leave for future work . 7.On the topic of running the disentanglement metric on real datasets : we explored the possibility on the CelebA dataset , but did not report a score for two main reasons . First , as the reviewer suggested , many of the labels in the dataset are not those that could be reasonably expected to be disentangled , and indeed may not correspond to ground truth factors of variation . For example , \u2018 attractiveness \u2019 is such a label . Secondly , even though there were a ( small ) subset of labels that could potentially disentangle well , these labels do not necessarily correspond to factors that control for large visual changes in the subject \u2019 s faces . Visually , we feel that the primary factors of variation are \u201c background colour \u201d , \u201c azimuth/rotation \u201d , \u201c facial identity \u201d , and \u201c hair colour \u201d . Of these , only hair colour was present as an available label ( albeit with quite poor labelling , e.g.many subjects did not have a particular hair colour specified ) . In future work we would hope to demonstrate the efficacy of the metric on better labeled datasets , such as Caltech-UCSD Birds ( http : //www.vision.caltech.edu/visipedia/CUB-200.html ) or Oxford Flowers ( http : //www.robots.ox.ac.uk/~vgg/data/flowers/ ) . 8.The reviewer mentions using another simpler protocol to assess our disentanglement metric , this is indeed a great suggestion . We have added a new section in Appendix A.5 , with a new Table 2 , to describe the performance of a simple classifier trying to predict the data generative factor values of the 2D shapes dataset using different latent representations . We find that the results of this classification test match those of our proposed disentanglement metric , thus providing further validation for it as the reviewer was suggesting . 9.We are not entirely sure why PCA representations are more disentangled than ICA representations according to our metric . Both approaches score similarly well in terms of cross correlations between the latents , as would be expected ( as mentioned to AnonReviewer1 ) . According to our new Table 2 in Appendix A.5 , the independent components learnt by ICA tend to be very well suited for object identity identification , while those learnt by PCA are more representative of the object location . 10.Thank you for finding the mention of the \u201c factor change accuracy \u201d term , this indeed was a typo . We updated Fig.6 to use \u201c Disentanglement Metric Score \u201d as expected . 11.We also now added a reference to Appendix A.4 in the main text , sorry for the oversight . 12.Finally about your points following the mention of Figure 6 ( right ) : We have included random samples from both disentangled and entangled models in Appendix A.9 , Figure 9 . As for latent traversal visualisations for both conditions , this is actually what can already be seen in Fig.7 , panels A and B , if we understood the question correctly ? We hope this addresses the reviewer \u2019 s concerns and welcome any further feedback ."}], "0": {"review_id": "Sy2fzU9gl-0", "review_text": "The paper proposes beta-VAE which strengthen the KL divergence between the recognition model and the prior to limit the capacity of latent variables while sacrificing the reconstruction error. This allows the VAE model to learn more disentangled representation. The main concern is that the paper didn't present any quantitative result on log likelihood estimation. On the quality of generated samples, although the beta-VAE learns disentangled representation, the generated samples are not as realistic as those based on generative adversarial network, e.g., InfoGAN. Beta-VAE learns some interpretable factors of variation, but it still remains unclear why it is a better (or more efficient) representation than that of standard VAE. In experiment, what is the criteria for cross-validation on hyperparameter \\beta? There also exists other ways to limit the capacity of the model. The simplest way is to reduce the latent variable dimension. I am wondering how the proposed beta-VAE is a better model than the VAE with reduced, or optimal latent variable dimension. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his work , but given their review we are afraid they may have misunderstood the point of our paper and did not provide a fair assessment of our contribution . We hope our responses below and the comments of the other reviewers may help clarify the scope of our work and its significance . 1.We do not present any quantitative result on log-likelihood estimation , because it is not the main point of our paper ( as also pointed out by AnonReviewer3 ) . This also relates to the reviewer \u2019 s comment about the quality of beta-VAE samples and how they compare to those by InfoGAN . The quality of reconstructions of beta-VAE and InfoGAN are in line with the base methods the two approaches are built upon - the VAE and GAN . Hence our reconstructions are blurrier than those of InfoGAN due to the limitations of VAEs . Saying this , InfoGAN suffers from the shortcomings of GANS , such as training instability and mode jumping . We stress again : we do not strive in this paper for good quality reconstructions ; the main point of our paper is the quality of the learned representations i.e.the disentangling capability of our approach , and in this regard we clearly demonstrate that our method outperforms InfoGAN , which is the current state of the art fully unsupervised method for disentangled factor learning . In addition , we address the other main shortcoming of the InfoGAN paper ( and indeed , most other unsupervised representation-learning papers ) : a lack of justification for the conclusion apart from weak empirical evidence , which we address by our quantitative disentanglement metric . 2.The benefits of the representations learnt by beta-VAE over those learnt by a standard VAE come from their disentangled property . The benefits of disentangled representations have been argued for by many , including Bengio ( 2013 ) and most recently in a review article on representation learning that has just come out on arxiv ( https : //arxiv.org/abs/1612.05299 ) . These benefits include novel example generation through interpretable factor recombination ( problems that arise from recombining entangled factors can be seen in the reduced quality of samples generated by an entangled VAE vs a disentangled beta-VAE as shown in Figure 9 , which we have now added to the appendix ) , compression ( according to the minimum description length principle ) , and performance in zero-shot inference , transfer and domain adaptation scenarios . We have amended our introduction to present these benefits more clearly . Our current manuscript describes a new state of the art framework for disentangled factor learning . We leave the demonstration of benefits that arise from learning such disentangled representations to future work . We have , however , added a new section in Appendix A.5 , as well as Table 2 to describe the performance of a simple classifier trying to predict the data generative factor values using different latent representations . We find that such a classifier performs significantly better using the disentangled representation of beta-VAE compared to the entangled representations of VAE . 3.We did not cross-validate the parameter \\beta , as this doesn \u2019 t really make sense in an unsupervised learning setting . Indeed , as is usually the case in the literature ( see InfoGAN paper for example ) , we train on the full dataset and assess characteristics of the representation obtained . As we mention in the text , in this current work we chose \\beta using visual assessments of the latent traversals . Alternatively , we also mention that our disentanglement metric score could be used for this purpose . This is validated by the fact that \\beta values found to be optimal through the visual assessment heuristic on the dataset of 2D shapes were also found to be within the optimal range of \\beta \u2019 s as assessed with the disentanglement metric . We have now also included Appendix A.5 as further validation ; in this Appendix , we use the \\beta-VAE representation to directly predict the values of the ground truth factors on the 2D shapes dataset with a linear classifier . Although this does not measure disentangling directly ( and hence is not used as our preferred disentangling metric ) , it further backs up the efficacy of \\beta-VAE relative to other models in the literature . Additionally , we always train multiple networks with random initialisations of weights for every value of \\beta we tried ( as can be seen in Figure 6 ) . We did not see any strong variations in their behaviour , and hence we believe that \\beta is not a particularly sensitive parameter . 4.We have trained VAEs with different latent sizes ( z_size \\in [ 2 , 10 ] ) and found that reducing z_size was linearly * reducing * the disentangling metric score . In particular , a VAE with the \u201c optimal \u201d z_size ( e.g.z_size = 5 for our 2D shape dataset , which has 5 ground truth factors of variation ) was not able to learn latents corresponding to the ground truth generative factors , in contrast with the beta-VAE ( as shown in Fig 7 and by the disentangling metric score ) . Hence beta-VAE is indeed different from a VAE with a reduced/optimal latent channel capacity . We hope this addresses the reviewer \u2019 s concerns and welcome any further feedback ."}, "1": {"review_id": "Sy2fzU9gl-1", "review_text": "Summary === This paper presents Beta-VAE, an augmented Variational Auto-Encoder which learns disentangled representations. The VAE objective is derived as an approximate relaxation of a constrained optimization problem where the constraint matches the latent code of the encoder to a prior. When KKT multiplier beta on this constraint is set to 1 the result is the original VAE objective, but when beta > 1 we obtain Beta-VAE, which simply increases the penalty on the KL divergence term. This encourages the model to learn a more efficient representation because the capacity of the latent representation is more limited by beta. The distribution of the latent representation is rewarded more when factors are independent because the prior (an isotropic Gaussian) encourages independent factors, so the representation should also be disentangled. A new metric is proposed to evaluate the degree of disentanglement. Given a setting in which some disentangled latent factors are known, many examples are generated which differ in all of these factors except one. These examples are encoded into the learned latent representation and a simple classifier is used to predict which latent factor was kept constant. If the learned representation does not disentangle the constant factor then the classifier will more easily confuse factors and its accuracy will be lower. This accuracy is the final number reported. A synthetic dataset of 2D shapes with known latent factors is created to test the proposed metric and Beta-VAE outperforms a number of baselines (notably InfoGAN and the semi-supervised DC-IGN). Qualitative results show that Beta-VAE learns disentangled factors on the 3D chairs dataset, a dataset of 3D faces, and the celebA dataset of face images. The effect of varying Beta is also evaluated using the proposed metric and the latent factors learned on the 2D shapes dataset are explored in detail. Strengths === * Beta-VAE is simple and effective. * The proposed metric is a novel way of testing whether ground truth factors of variation have been identified. * There is extensive comparison to relevant baselines. Weaknesses === * Section 3 describes the proposed disentanglement metric, however I feel I need to read the caption of the associated figure (I thank for adding that) and Appendix 4 to understand the metric intuitively or in detail. It would be easier to read this section if a clear intuition preceeded a detailed description and I think more space should be devoted to this in the paper. * Appendix 4: Why was the bottom 50% of the resulting scores discarded? * As indicated in pre-review comments, the disentanglement metric is similar to a measure of correlation between latent features. Could the proposed metric be compared to a direct measure of cross-correlation between latent factors estimated over the 2D shapes dataset? * The end of section 4.2 observes that high beta values result in low disentanglement, which suggests the most efficient representation is not disentangled. This seems to disagree with the intuition from the approach section that more efficient representations should be disentangled. It would be nice to see discussion of potential reasons for this disagreement. * The writing is somewhat dense. Overall Evaluation === The core idea is novel, simple and extensive tests show that it is effective. The proposed evaluation metric is novel might come into broader use. The main downside to the current version of this paper is the presentation, which provides sufficient detail but could be more clear.", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to thank the reviewer for their thorough review and helpful suggestions . 1.We acknowledge the importance of clearly explaining our disentangled metric and we agree that our initial attempt can be improved . In order to address that , we added three new paragraphs at the beginning of Section 3 to provide an intuitive explanation for our disentanglement metric . We also rewrote the end of that section to simplify the formal presentation of the metric . We hope this provides enough background and support for why we devised it that way and what exactly it is assessing . 2.With respect to the bottom 50 % question : we have added a sentence at the end of the second paragraph of Appendix A.4 to address it . The bottom 50 % of the resulting scores were discarded to control for the outlier results from the few experiments that diverged during training . 3.Thank you for the suggestion . The cross-correlation fails to fully capture the properties we would look for in a disentanglement metric , as we now discuss in the new first 3 paragraphs of Section 3 . We did calculate it though ( average absolute values of upper triangular terms in correlation matrix , estimated over the entire dataset ) , but unfortunately found that the cross-correlation does not seem to provide an appropriate measure ( we \u2019 re also concerned with the large standard deviations ) . Here are the values we obtained : PCA : 4.7e-9 ( std 5e-9 ) // ICA : 1.15e-6 ( std 9e-7 ) // DC-IGN : 0.15 ( std 0.16 ) // InfoGAN : 0.09 ( std 0.08 ) // VAE 0.039 ( std 0.04 ) // beta-VAE : 0.057 ( std : 0.053 ) As expected , PCA and ICA produce the least correlated latents , since these approaches directly optimise for reducing cross-correlation between latents . The other approaches , including the semi-supervised DC-IGN , produce more correlated latents . However , these are nevertheless disentangled according to a human interpretation of what disentangled factors should look like . This suggests that humanly interpretable disentangled factors of variation are still somewhat correlated in the real world , which suggests that PCA and ICA may be optimising the wrong objective for interpretable disentangling . Hence we think that our metric provides a better test for interpretable disentangled representations , which a simple cross-correlation fails to capture . 4.We have added a sentence in the relevant paragraph in Section 4.2 . If $ \\beta $ is too high and the resulting capacity of the latent channel is lower than the number of data generative factors , then the learnt representation necessarily has to be entangled ( as we are compressing more than what a factorial representation would require ) . We hope this addresses the reviewer \u2019 s concerns and welcome any further feedback . [ EDIT : sorry , we initially entered wrong cross-correlation values for VAE and beta-VAE , but have now corrected them ]"}, "2": {"review_id": "Sy2fzU9gl-2", "review_text": " This paper proposes the beta-VAE, which is a reasonable but also straightforward generalization of the standard VAE. In particular, a weighting factor beta is added for the KL-divergence term to balance the likelihood and KL-divergence. Experimental results show that tuning this weighting factor is important for learning disentangled representations. A linear-classifier based protocol is proposed for measuring the quality of disentanglement. Impressive illustrations on manipulating latent variables are shown in the paper. Learning disentangled representations without supervision is an important topic. Showing the effectiveness of VAE for this task is interesting. Generalizing VAE with a weighting factor is straightforward (though reformulating VAE is also interesting), the main contribution of this paper is on the empirical side. The proposed protocol for measuring disentangling quality is reasonable. Establishing protocol is one important methodology contribution of this paper, but the presentation of Section 3 is still not good. Little motivation is provided at the beginning of Section 3. Figure 2 is a summary of the algorithm, which is helpful, but it still necessary to intuitively explain the motivation at the first place (e.g., what you expect if a factor is disentangled, and why the performance of a classifier can reflect such an expectation). Moreover, 1) z_diff appeared without any definition in the main text. 2) Use \u201cdecoding\u201d for x~Sim(v,w) may make people confuse the ground truth sampling procedure w ith the trained decoder. The illustrative figures on traversing the disentangled factor are impressive, though image generation quality is not as good as InfoGAN (not the main point of this paper). However, 1) it will be helpful to discuss if the good disentangling quality only attribute to the beta factor and VAE framework. For example, the training data in this paper seems to be densely sampled for the visualized factors. Does the sampling density play a critical role? 2) Not too many qualitative results are provided for each experiment? Adding more figures (e.g., in appendix) to cover more factors and seeding images can strength the conclusions drawn in this paper. 3) Another detailed question related to the generalizability of the model: are the seeding image for visualizing faces from unseen subjects or subjects in the training set? (maybe I missed something here.) Quantitative results are only presented for the synthesized 2D shape. What hinders this paper from reporting quantitative numbers on real data (e.g., the 2D and 3D face data)? One possible reason is that not all factors can be disentangled for real data, but it is still feasible to pick up some well-defined factor to measure the quantitative performance. Quantitative performance is only measured by the proposed protocol. Since the effectiveness of the protocol is something the paper need to justify, reporting quantitative results using simpler protocol is helpful both for demonstrating the disentangling quality and for justifying the proposed protocol (consistency with other measurement). A simple experiment is facial identity recognition and pose estimation using disentangled features on a standard test set (like in Reed et al, ICML 2014). In Figure 6 (left), why ICA is worse than PCA for disentanglement? Is it due to the limitation of the ICA algorithm or some other reasons? In Figure 6 (right), what is \u201cfactor change accuracy\u201d? According to Appendix A.4 (which is not referred to in the main text), it is the \u201cDisentanglement metric score\u201d. Is that right? If so Figure 6 (right) shows the reconstruction results for the best disentanglement metric score. Then, 1) how about random generation or traversing along a disentangled factor? 2) more importantly, how is the reconstruction/generation results when the disentanglement metric score is suboptimal. Overall, the results presented in this paper are very interesting, but there are many details to be clarified. Moreover, more quantitative results are also needed. I hope at least some of the above concerns can be addressed. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are very grateful to the reviewer for their very thorough assessment of our work and the numerous suggestions . We have made a number of changes to fully address them . 1.About our presentation of our disentanglement metric , this was a concern shared with Reviewer 3 . We have now addressed this by introducing three new paragraphs at the start of Section 3 to provide an intuitive explanation of our disentanglement metric . We also rewrote the end of Section 3 to simplify the presentation . 2.Z_diff is now properly defined , sorry for this oversight . 3.We changed \u2018 decode \u2019 to \u2018 simulate \u2019 and \u2018 re-encode \u2019 to \u2018 infer \u2019 in order to avoid confusion between our disentanglement metric and the trained decoder , thank you for mentioning it . 4.We have added a sentence in Conclusions with a reference to Section A.8 in the Appendix . This section discusses the effects of data generative factor sample density on the learnt representations of beta-VAE , with empirical results . 5.We have added extra traversal plots in the appendix , with more seeding images . These include all latents that learnt to be informative when presented with 3D chairs or CelebA . 6.About the generalizability of our model : currently all seeding images are from the same full training set . As done in this literature [ e.g.InfoGAN ] , we did not hold out any test set . This would involve handling a domain adaptation scenario , which we are interested in tackling but decided to leave for future work . 7.On the topic of running the disentanglement metric on real datasets : we explored the possibility on the CelebA dataset , but did not report a score for two main reasons . First , as the reviewer suggested , many of the labels in the dataset are not those that could be reasonably expected to be disentangled , and indeed may not correspond to ground truth factors of variation . For example , \u2018 attractiveness \u2019 is such a label . Secondly , even though there were a ( small ) subset of labels that could potentially disentangle well , these labels do not necessarily correspond to factors that control for large visual changes in the subject \u2019 s faces . Visually , we feel that the primary factors of variation are \u201c background colour \u201d , \u201c azimuth/rotation \u201d , \u201c facial identity \u201d , and \u201c hair colour \u201d . Of these , only hair colour was present as an available label ( albeit with quite poor labelling , e.g.many subjects did not have a particular hair colour specified ) . In future work we would hope to demonstrate the efficacy of the metric on better labeled datasets , such as Caltech-UCSD Birds ( http : //www.vision.caltech.edu/visipedia/CUB-200.html ) or Oxford Flowers ( http : //www.robots.ox.ac.uk/~vgg/data/flowers/ ) . 8.The reviewer mentions using another simpler protocol to assess our disentanglement metric , this is indeed a great suggestion . We have added a new section in Appendix A.5 , with a new Table 2 , to describe the performance of a simple classifier trying to predict the data generative factor values of the 2D shapes dataset using different latent representations . We find that the results of this classification test match those of our proposed disentanglement metric , thus providing further validation for it as the reviewer was suggesting . 9.We are not entirely sure why PCA representations are more disentangled than ICA representations according to our metric . Both approaches score similarly well in terms of cross correlations between the latents , as would be expected ( as mentioned to AnonReviewer1 ) . According to our new Table 2 in Appendix A.5 , the independent components learnt by ICA tend to be very well suited for object identity identification , while those learnt by PCA are more representative of the object location . 10.Thank you for finding the mention of the \u201c factor change accuracy \u201d term , this indeed was a typo . We updated Fig.6 to use \u201c Disentanglement Metric Score \u201d as expected . 11.We also now added a reference to Appendix A.4 in the main text , sorry for the oversight . 12.Finally about your points following the mention of Figure 6 ( right ) : We have included random samples from both disentangled and entangled models in Appendix A.9 , Figure 9 . As for latent traversal visualisations for both conditions , this is actually what can already be seen in Fig.7 , panels A and B , if we understood the question correctly ? We hope this addresses the reviewer \u2019 s concerns and welcome any further feedback ."}}