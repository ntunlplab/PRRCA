{"year": "2019", "forum": "SJg7IsC5KQ", "title": "On the Convergence and Robustness of Batch Normalization", "decision": "Reject", "meta_review": "The reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication. The AC thus proposes \"revise and sesubmit\".", "reviews": [{"review_id": "SJg7IsC5KQ-0", "review_text": "The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal. 1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below. 2) Properties of the minimizer The authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying. 3) Scaling property I find this section confusing. Specifically, a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate? b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this? c) The idea of \u201crestarting\u201d is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don\u2019t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this \u201cproperty\u201d seem to be used to simply rescale the a and w parameters. d) The authors claim that \u201cthe scaling law (Proposition 3.2) should play a significant role\u201d to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models? 4) Convergence rate It seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically, a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn\u2019t a constant step size also yield convergence in that case? b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work. 5) Saddles for neural nets The authors claim they \u201chave not encountered convergence to saddles\u201d for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly? 6) Extension of the analysis to deep neural networks The analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.? 7) Experiments How would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Since most of the reviewer \u2019 s criticisms are based on comparisons with Kohler et al , we believe it is necessary to start with some overall comments on the differences between our independent work and theirs , and how these differences are significant in the attempt to gain insights on BNGD . The most important distinction is that in Kohler et al. , the authors considered a modification of the BNGD on OLS in two ways : 1 ) The rescaling parameter is set to a value at every iteration to satisfy stationarity , instead of performing gradient descent ; 2 ) the learning rate is chosen dynamically every step according to a rule which needs much more knowledge of the system . Thus , the analysis is fundamentally on a different algorithm from BNGD . In contrast , we analyse the original BNGD without these modifications , i.e.we consider a constant learning rate and also perform gradient descent on the rescaling parameters . We believe that this is an important distinction because the very goal of analyzing BNGD algorithm on a simplified model is to gain key insights into the * original * algorithm widely used in machine learning while circumventing the difficulties posed by a complex objective , say from a deep learning model . Consequently , none of the results we derived for the OLS model are weaker than , or can be deduced from the analysis presented in Kohler et al.In fact , we outline in some specific comments below ( in next comment ) that some of our results are stronger than those in Kohler et al if we take the simplifying modifications of the BNGD described above . In particular , the linear convergence of this modified BNGD follows directly from Eq . ( 13 ) of our analysis , which is a consequence of a simple projection argument ( Eq . ( 26 ) ) .However , we must stress that we do not discuss these results explicit at length because we believe it is not relevant to our approach to study the original BNGD algorithm using simplified models , as we described above ."}, {"review_id": "SJg7IsC5KQ-1", "review_text": "This paper provides a theoretical analysis for batch normalization with gradient descent (GDBN) under a simplified scenario, i.e., solving an ordinary least squares problem. The analysis shows that GDBN converges to a stationary point when the learning rate is less than or equal to 1, regardless of the condition number of the problem. Some practical experiments are carried out to justify their theoretical insights. The paper is in general easy to follow. Pros: This paper provides some insights for BN using the simplified model. 1. It shows that the optimal convergence rate of BN can be faster than vanilla GD. 2. It shows that GDBN doesn't diverge even if the learning rate for trainable parameters is very large. Cons: 1. In the main theorem, when the learning rate for the rescaling parameter is less than or equal to 1, the algorithm is only proved to converge to a stationary point for OLS problem rather a global optimal. 2. To show convergence to the global optimal, the learning rate needs to be sufficiently small. But it is not specified how small it is. Overall, I think this paper provides some preliminary analysis for BN, which should shed some lights for understanding BN. However, the model under analysis is very simplified and the theoretical results are still preliminary.", "rating": "6: Marginally above acceptance threshold", "reply_text": "In our revised paper ( Theorem 3.3 ) , we give some cases where the convergence to global minimizer for arbitrary learning rates for the weights is proved . We stress that the small learning rates requirement is to avoid saddles , but it is not required for the stability of the algorithm . The smallness is quantified by Lemma A.14 . Nevertheless , if we take Theorem 3.3 ( 2 ) \u2019 s condition , we do not need this requirement on the learning rates . We agree that we are analyzing BNGD on a specific problem , but we demonstrated through experiments that many insights hold generally . We believe it is appropriate to clearly analyze special and representative cases before attempting the general case , which is much more difficult ."}, {"review_id": "SJg7IsC5KQ-2", "review_text": "The paper presents an analysis of the batch normalization idea on a simple OLS problem. The analysis is interesting as presented but several key questions remain, as described below. It is unclear that these questions are answered to the point where the insight gained can be considered transferable to BN in large Neural Network models. - The reason why the auxiliary variable 'a' is included in the formulation (7) is unclear. The whole reason for using BN is to rescale intermediate outputs to have an expectation of zero and variance of one. The authors claim that BN produces \"order 1\" output and so 'a' is needed. Can you please explain his better? - The scaling proposition 3.2 is claimed to be important, but the authors don't provide a clear explanation of why that is so. Two different settings of algorithms are presented where the iterates should roughly be in the same order if input parameters of the formulation or the algorithm are scaled in a specific way. It is unclear how this leads to the claimed insight that the BN algorithm is yielded to be insensitive to input parameters of step length etc. due to this proposition. also, where is the proof of this proposition? I couldn't find it in the appendix, and I apologize in advance if that's an oversight on my part. - The 'u' referred to in eqn (14) is the optimal solution to the original OLS problem, so has form H^{-1} g for some g that depends on input parameters. Doesn't this simplify the expression in (!4)? Does this lead to some intuition on how the condition number of H^* relates to H? Does this operation knock off the highest or lowest eigenvalue of H to impact the condition number? - Additionally, it is bad notation to use two-letter function names in a mathematical description, such as BN(z). This gets confusing very fast in theorems and proofs, though the CS community seems to be comfortable with this convention. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "In our paper , we have done experiments on some neural networks and discovered that the following insights is transferable ( see Figure 3 ) : ( i ) the BNGD allows large step sizes , ( ii ) the optimal step size is insensitive , ( iii ) the optimal performance is better than the optimal GD . For general large scale neural networks , it is much more difficult to strictly analyse those observations . However , some basic properties remain exactly . In Lemma A.9 , we show the scaling property for a more general case , and in fact , the same can be extended to general neural networks where BNGD is used . Moreover , one can check that the monotonicity of { |w_k| } also holds generally . Hence , in general , if one can establish convergence for |w_0|=1 and small step size \\varepsilon , then one has the convergence of { |w_k| } for arbitrary learning rates , which will serve as a useful starting point to prove the convergence of { w_k } . Below is our comments on the questions that the reviewer addressed . 1 ) The reason why the auxiliary variable ' a ' is included . This is standard in the BN procedure and the reason of introducing ` a ` is explained in the original paper of BN ( Ioffe and Szegedy , 2015 ) . \u201c Note that simply normalizing each input of a layer may change what the layer can represent. \u201d . The variable ` a ` is there to make sure that the approximation power of the model remains the same . 2 ) The scaling property . First , the scaling property ensures that equivalent configurations ( as in Definition 3.1 ) must converge or diverge together , with the same rate up to a constant multiple . Hence , this property is used in the proof of the main convergence result ( Theorem 3.3 ) . The sketch of the proof is as follows : we first prove that the norm of the parameters { |w_k| } is a monotone increasing sequence , thus either converges to a finite limit or diverges . The scaling property is then used to exclude the divergent case -- if { |w_k| } diverges , then at some k the norm |w_k| should be large enough , and by the scaling property , it is equivalent to a case where |w_k|=1 and \\varepsilon is small . But we can separately establish that for sufficiently small \\varepsilon , the iterates converge from arbitrary initial condition |w_0|=1 . This proves that { |w_k| } converges to a finite limit , from which the convergence of w_k and the loss can be established , after some work . The insensitivity of choosing step size is stated in Proposition 3.5 and proved in Appendix A.5 . It is not a direct consequence of the scaling property , but rather is related to the dynamics of the effective learning rate \\hat { \\varepsilon } , defined in ( 11 ) . 3 ) Condition number of H^ * . The intuition here comes from the well-known Cauchy interlacing theorem . H^ * can be regard as an orthogonal projection in the H-norm space . The proof of the eigenvalue property ( Lemma A.1 ) is also similar to the proof of Cauchy interlacing theorem . H * does not knock out any eigenspace , but creates a zero-eigenvalue eigenspace with eigenvector u , thus it knocks out a bit from ( in general ) all eigenspaces and gives rise to the iterlacing property . This is the source of the acceleration . Using $ Hu=g $ , we can express equation ( 14 ) as $ H^ * = H - g^T g/ ( u^T g ) $ which contains more letters . 4 ) We changed the notation \u2018 BN ( z ) \u2019 to having BN on the subscript in revision ."}], "0": {"review_id": "SJg7IsC5KQ-0", "review_text": "The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal. 1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below. 2) Properties of the minimizer The authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying. 3) Scaling property I find this section confusing. Specifically, a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate? b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this? c) The idea of \u201crestarting\u201d is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don\u2019t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this \u201cproperty\u201d seem to be used to simply rescale the a and w parameters. d) The authors claim that \u201cthe scaling law (Proposition 3.2) should play a significant role\u201d to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models? 4) Convergence rate It seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically, a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn\u2019t a constant step size also yield convergence in that case? b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work. 5) Saddles for neural nets The authors claim they \u201chave not encountered convergence to saddles\u201d for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly? 6) Extension of the analysis to deep neural networks The analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.? 7) Experiments How would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Since most of the reviewer \u2019 s criticisms are based on comparisons with Kohler et al , we believe it is necessary to start with some overall comments on the differences between our independent work and theirs , and how these differences are significant in the attempt to gain insights on BNGD . The most important distinction is that in Kohler et al. , the authors considered a modification of the BNGD on OLS in two ways : 1 ) The rescaling parameter is set to a value at every iteration to satisfy stationarity , instead of performing gradient descent ; 2 ) the learning rate is chosen dynamically every step according to a rule which needs much more knowledge of the system . Thus , the analysis is fundamentally on a different algorithm from BNGD . In contrast , we analyse the original BNGD without these modifications , i.e.we consider a constant learning rate and also perform gradient descent on the rescaling parameters . We believe that this is an important distinction because the very goal of analyzing BNGD algorithm on a simplified model is to gain key insights into the * original * algorithm widely used in machine learning while circumventing the difficulties posed by a complex objective , say from a deep learning model . Consequently , none of the results we derived for the OLS model are weaker than , or can be deduced from the analysis presented in Kohler et al.In fact , we outline in some specific comments below ( in next comment ) that some of our results are stronger than those in Kohler et al if we take the simplifying modifications of the BNGD described above . In particular , the linear convergence of this modified BNGD follows directly from Eq . ( 13 ) of our analysis , which is a consequence of a simple projection argument ( Eq . ( 26 ) ) .However , we must stress that we do not discuss these results explicit at length because we believe it is not relevant to our approach to study the original BNGD algorithm using simplified models , as we described above ."}, "1": {"review_id": "SJg7IsC5KQ-1", "review_text": "This paper provides a theoretical analysis for batch normalization with gradient descent (GDBN) under a simplified scenario, i.e., solving an ordinary least squares problem. The analysis shows that GDBN converges to a stationary point when the learning rate is less than or equal to 1, regardless of the condition number of the problem. Some practical experiments are carried out to justify their theoretical insights. The paper is in general easy to follow. Pros: This paper provides some insights for BN using the simplified model. 1. It shows that the optimal convergence rate of BN can be faster than vanilla GD. 2. It shows that GDBN doesn't diverge even if the learning rate for trainable parameters is very large. Cons: 1. In the main theorem, when the learning rate for the rescaling parameter is less than or equal to 1, the algorithm is only proved to converge to a stationary point for OLS problem rather a global optimal. 2. To show convergence to the global optimal, the learning rate needs to be sufficiently small. But it is not specified how small it is. Overall, I think this paper provides some preliminary analysis for BN, which should shed some lights for understanding BN. However, the model under analysis is very simplified and the theoretical results are still preliminary.", "rating": "6: Marginally above acceptance threshold", "reply_text": "In our revised paper ( Theorem 3.3 ) , we give some cases where the convergence to global minimizer for arbitrary learning rates for the weights is proved . We stress that the small learning rates requirement is to avoid saddles , but it is not required for the stability of the algorithm . The smallness is quantified by Lemma A.14 . Nevertheless , if we take Theorem 3.3 ( 2 ) \u2019 s condition , we do not need this requirement on the learning rates . We agree that we are analyzing BNGD on a specific problem , but we demonstrated through experiments that many insights hold generally . We believe it is appropriate to clearly analyze special and representative cases before attempting the general case , which is much more difficult ."}, "2": {"review_id": "SJg7IsC5KQ-2", "review_text": "The paper presents an analysis of the batch normalization idea on a simple OLS problem. The analysis is interesting as presented but several key questions remain, as described below. It is unclear that these questions are answered to the point where the insight gained can be considered transferable to BN in large Neural Network models. - The reason why the auxiliary variable 'a' is included in the formulation (7) is unclear. The whole reason for using BN is to rescale intermediate outputs to have an expectation of zero and variance of one. The authors claim that BN produces \"order 1\" output and so 'a' is needed. Can you please explain his better? - The scaling proposition 3.2 is claimed to be important, but the authors don't provide a clear explanation of why that is so. Two different settings of algorithms are presented where the iterates should roughly be in the same order if input parameters of the formulation or the algorithm are scaled in a specific way. It is unclear how this leads to the claimed insight that the BN algorithm is yielded to be insensitive to input parameters of step length etc. due to this proposition. also, where is the proof of this proposition? I couldn't find it in the appendix, and I apologize in advance if that's an oversight on my part. - The 'u' referred to in eqn (14) is the optimal solution to the original OLS problem, so has form H^{-1} g for some g that depends on input parameters. Doesn't this simplify the expression in (!4)? Does this lead to some intuition on how the condition number of H^* relates to H? Does this operation knock off the highest or lowest eigenvalue of H to impact the condition number? - Additionally, it is bad notation to use two-letter function names in a mathematical description, such as BN(z). This gets confusing very fast in theorems and proofs, though the CS community seems to be comfortable with this convention. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "In our paper , we have done experiments on some neural networks and discovered that the following insights is transferable ( see Figure 3 ) : ( i ) the BNGD allows large step sizes , ( ii ) the optimal step size is insensitive , ( iii ) the optimal performance is better than the optimal GD . For general large scale neural networks , it is much more difficult to strictly analyse those observations . However , some basic properties remain exactly . In Lemma A.9 , we show the scaling property for a more general case , and in fact , the same can be extended to general neural networks where BNGD is used . Moreover , one can check that the monotonicity of { |w_k| } also holds generally . Hence , in general , if one can establish convergence for |w_0|=1 and small step size \\varepsilon , then one has the convergence of { |w_k| } for arbitrary learning rates , which will serve as a useful starting point to prove the convergence of { w_k } . Below is our comments on the questions that the reviewer addressed . 1 ) The reason why the auxiliary variable ' a ' is included . This is standard in the BN procedure and the reason of introducing ` a ` is explained in the original paper of BN ( Ioffe and Szegedy , 2015 ) . \u201c Note that simply normalizing each input of a layer may change what the layer can represent. \u201d . The variable ` a ` is there to make sure that the approximation power of the model remains the same . 2 ) The scaling property . First , the scaling property ensures that equivalent configurations ( as in Definition 3.1 ) must converge or diverge together , with the same rate up to a constant multiple . Hence , this property is used in the proof of the main convergence result ( Theorem 3.3 ) . The sketch of the proof is as follows : we first prove that the norm of the parameters { |w_k| } is a monotone increasing sequence , thus either converges to a finite limit or diverges . The scaling property is then used to exclude the divergent case -- if { |w_k| } diverges , then at some k the norm |w_k| should be large enough , and by the scaling property , it is equivalent to a case where |w_k|=1 and \\varepsilon is small . But we can separately establish that for sufficiently small \\varepsilon , the iterates converge from arbitrary initial condition |w_0|=1 . This proves that { |w_k| } converges to a finite limit , from which the convergence of w_k and the loss can be established , after some work . The insensitivity of choosing step size is stated in Proposition 3.5 and proved in Appendix A.5 . It is not a direct consequence of the scaling property , but rather is related to the dynamics of the effective learning rate \\hat { \\varepsilon } , defined in ( 11 ) . 3 ) Condition number of H^ * . The intuition here comes from the well-known Cauchy interlacing theorem . H^ * can be regard as an orthogonal projection in the H-norm space . The proof of the eigenvalue property ( Lemma A.1 ) is also similar to the proof of Cauchy interlacing theorem . H * does not knock out any eigenspace , but creates a zero-eigenvalue eigenspace with eigenvector u , thus it knocks out a bit from ( in general ) all eigenspaces and gives rise to the iterlacing property . This is the source of the acceleration . Using $ Hu=g $ , we can express equation ( 14 ) as $ H^ * = H - g^T g/ ( u^T g ) $ which contains more letters . 4 ) We changed the notation \u2018 BN ( z ) \u2019 to having BN on the subscript in revision ."}}