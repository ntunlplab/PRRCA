{"year": "2021", "forum": "8q_ca26L1fz", "title": "Revisiting Graph Neural Networks for Link Prediction", "decision": "Reject", "meta_review": "The authors study the expressive power of Graph Neural Network architectures for the link prediction problem and provides theoretical justification for the strong performance of SEAL on link prediction benchmarks. However, The reviewers think the paper needs to improve in several aspects before it can be published: 1. More clearly explain the theoretical analysis and contribution. 2. Extensive and in-depth discussion of the similarities with and difference to the work of Li et al. to show the novelty of current work. ", "reviews": [{"review_id": "8q_ca26L1fz-0", "review_text": "Summary : The paper presents results of two popular classes of methods for graph neural networks . Namely , GAE and SEAL . The paper show results on the Open Graph Benchmark of several existing methods , and concludes that GAE can not learn structural link representations while SEAL can . The paper is descriptive in its presentation , and does a good work on detailing the ideas behind GAE and SEAL . However , I felt the lack of a contribution on the paper . As it stands , it reads more as a tutorial and presents key insights of existing methods . Pros : - Clear explanations of existing work . Cons : - No clear contribution over explaining existing approaches . - Experiments show existing results on OGB . - No results on the mentioned labeling trick are given . Comments : - In Fig.1 you mentioned that $ v_2 $ and $ v_3 $ will get the same representation through a GAE . Did you train one and observed it ? It will be more conclusive if you show the results and the embeddings to validate your observation . - Your labeling trick depends on the set of nodes $ S $ used . However , how do you select such sets to produce the labeling ? Are you sampling all possible subsets $ S $ ? Since this seems to be a form of contribution , it should be clear how to use it in any scenario . - In your experiments , due to the way the paper presents the label trick as the novelty , I was expecting to see the boost of performance of such trick on the existing architectures . However , Section 7 presents results of existing approaches on the OGB . Minor comments : - `` Following ( Srinivasan & Ribeiro , 2020 ; Li et al. , 2020 ) '' should be a textual citation Overall rating : Due to the main problems stated , I can not recommend the paper for publication at ICLR . The manuscript reads more like a tutorial , which may be of interest for readers finding a summary of the advances . However , as a paper advancing the field of graph neural networks I feel it lacking .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for the comments . However , we do not agree that this paper is only a tutorial of existing methods . Our paper discusses the deep differences between GAE and SEAL , and answers why SEAL outperforms GAE on link prediction using extensive theoretical arguments . Also , it seems the reviewer has missed a lot of important details in the paper . Below we try to address the comments . 1 . `` No clear contribution over explaining existing approaches . '' We argue that the main contribution of the paper is to demonstrate why SEAL is able to outperform GAE methods on link prediction , and reveal that the key component is the labeling trick . It defines a general form of the labeling trick ( Definition 5 ) , and shows that with the labeling trick a sufficiently expressive GNN can discriminate all non-isomorphic links ( Theorem 1 ) , which is not possible for GAEs as shown in Figure 1 . We also demonstrate why practical GNNs only need a small local subgraph to achieve a good performance without reaching the theoretical worst bound of WL test through Definition 10 and Theorem 2 . It is unfortunate that the reviewer overlooked these significant theoretical contributions . 2 . `` Experiments show existing results on OGB . '' The results of SEAL on OGB leaderboard are uploaded by us after the deadline of ICLR 2021 , and are used to support this submission . In this paper , we reimplemented SEAL , and fundamentally improved its scalability in order to run on OGB datasets . We are sorry to cause the confusion that it seems we are \u201c copying \u201d existing results on OGB leaderboard without doing experiments in this paper . 3 . `` No results on the mentioned labeling trick are given . '' The entire section 4.2 is trying to convey that SEAL is exactly a GNN enhanced by labeling trick . Our results on SEAL show that a GCN enhanced by labeling trick outperforms baselines significantly . 4 . `` In Fig.1 you mentioned that v2 and v3 will get the same representation through a GAE . Did you train one and observed it ? It will be more conclusive if you show the results and the embeddings to validate your observation . '' No , we didn \u2019 t . And it is not necessary . The reason is as follows . GNN has the same convolution parameters for all nodes . v2 and v3 also have the same neighborhood structure . Applying GNN on v2 and v3 will lead to exactly the same representations . The training of GNN will change the parameters over epochs , but still the parameters are shared across all nodes . So no matter how we train the GNN , v2 and v3 will always have the same representation . 5 . `` Your labeling trick depends on the set of nodes S used . However , how do you select such sets to produce the labeling ? Are you sampling all possible subsets S ? Since this seems to be a form of contribution , it should be clear how to use it in any scenario . '' S is the set of nodes to learn a joint structural representation for . For link prediction , S is exactly the source and target nodes to predict link between . The notation of S is used consistently across the paper to denote the target node set of interest since Definition 3 . There is no sampling needed . 6 . `` In your experiments , due to the way the paper presents the label trick as the novelty , I was expecting to see the boost of performance of such trick on the existing architectures . However , Section 7 presents results of existing approaches on the OGB . '' SEAL is exactly a GCN + labeling trick . By showing SEAL \u2019 s significant performance boost over baselines , the experiments demonstrate the of great value of the labeling trick . 7 . `` Following ( Srinivasan & Ribeiro , 2020 ; Li et al. , 2020 ) '' should be a textual citation Thanks ! We will revise it . Summarization We are again sorry about causing those confusions to the reviewer . To clarify better , SEAL is originally proposed in ( Zhang and Chen NeurIPS 2018 ) . It is a SotA method for link prediction , but people have little understanding of why it outperforms normal GNNs . In this paper , we identify one key component of SEAL is its labeling trick , and analyze theoretically how the labeling trick helps a GNN ( that only discriminates non-isomorphic nodes ) to also discriminate non-isomorphic links . We also reimplemented SEAL using advanced data structures and libraries to achieve ~1000 times speed up , and for the first time got its performance on the large-scale OGB datasets , which again verified its SotA performance . All these works are nontrivial , and beyond just a tutorial of existing methods ."}, {"review_id": "8q_ca26L1fz-1", "review_text": "The paper focuses on the link prediction task for graph neural networks . More specifically , it compares GAE and SEAL by providing theoretical evidence why GAE is not able to learn structural link representations , which as a result leads to suboptimal performance in the link prediction task . The paper also introduces a labeling trick that can help GNNs to learn structural link representations . Strong points : 1 . The paper is clear , easy to understand , and the definitions and theorems provide good reference and proofs . 2.A deeper understanding of the link prediction task is indeed needed as a lot of GNN works are mostly using classification as the downstream task . 3.It provides good argument and clear example why some GNN models would not work well in link prediction by showing that individually learning node representations can not handle the case where two nodes appear in topologically identical neighborhoods in the graph . It also provides insights as to how to mitigate such issues . Weak points : 1 . The comparison is oversimplified and overly generalized . The paper mostly compares GAE and SEAL , but there are much more GNN models that should be compared . 2.The sheer separation of GAE and SEAL model is not convincing . GAE and SEAL are both GNN models which follow the message passing and aggregation approaches . There 's no clear argument or theoretical explanation how these two are fundamentally different . 3.There are a lot of existing work that considers position in encoding nodes ( such as position aware gnn [ 2 ] ) and should be able to learn expressive structural link representations . The paper does not compare with them . 4.The experiment does not cover the most recent models . In addition , more benchmark datasets should be used . For example , FB15K ( this is a widely used knowledge graph link prediction benchmark dataset ) , PPI , WN18 , etc . Since previous work on gnn link prediction uses those dataset , I 'm expecting to see a more comprehensive comparison using those datasets . More dataset can be found here : https : //paperswithcode.com/task/link-prediction 5 . The paper should also consider comparing with knowledge graph embedding models which are focusing on the link prediction task . There are also existing work that combine the idea of knowledge graph embedding with GNN models in link prediction [ 1 ] . Other comments : 1 . In def 5 , gnn does not always have an invariant function mapping , depending on the message passing operations used e.g.in the graph sage paper , there are comparison using lstm message passing operator , which is not permutation invariant . 2.Section 3.1 mentions that the paper uses GAE to denote the general class of GNN-based link prediction regardless of the choice of GNN and aggregation function . However , research has shown that the specific choices of message passing operations and aggregations have significant impact on the expressiveness of GNN [ 3 ] 3 . Section 3.3 mentions that GAE can not learn structural link representations , but from the example in fig1 , it seems like this is caused by the graph isomorphism issue . there are existing work that can mitigate this issue , for example position-aware graph neural network [ 2 ] . Reference : 1 . Schlichtkrull , Michael , et al . `` Modeling relational data with graph convolutional networks . '' European Semantic Web Conference . Springer , Cham , 2018 . 2.You , Jiaxuan , Rex Ying , and Jure Leskovec . `` Position-aware graph neural networks . '' arXiv preprint arXiv:1906.04817 ( 2019 ) . 3.Xu , Keyulu , et al . `` How powerful are graph neural networks ? . '' arXiv preprint arXiv:1810.00826 ( 2018 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . 1 . `` The paper mostly compares GAE and SEAL , but there are much more GNN models that should be compared . '' GAE and SEAL are two main GNN link prediction paradigms . They do not restrict which specific GNN model is used . Any GNN , such as GCN , GraphSage or GIN , can be used in both GAE and SEAL frameworks . In the experiments , GCN , GraphSAGE and GCN+LRGA are used in GAE , and GCN is used in SEAL . Using more recent GNNs may further improve both GAE and SEAL 's performance , but it is beyond the purpose of the paper ( which compares the frameworks of GAE and SEAL ) . 2 . `` The sheer separation of GAE and SEAL model is not convincing . GAE and SEAL are both GNN models which follow the message passing and aggregation approaches . There 's no clear argument or theoretical explanation how these two are fundamentally different . '' We believe the entire section 3 and 4 are trying to discuss the differences between GAE and SEAL and answer why SEAL is more suitable than GAE for link prediction in a theoretical way . In short , SEAL uses GNN + labeling trick , while GAE only uses GNN . 3 . `` There are a lot of existing work that considers position in encoding nodes ( such as position aware gnn [ 2 ] ) and should be able to learn expressive structural link representations . The paper does not compare with them . '' Thanks.We indeed think P-GNN can be a competitive baseline . However , the original P-GNN paper uses datasets with at most 3,000 nodes . It is nontrivial to scale it to OGB datasets with millions of nodes and tens of millions of edges , especially when one need to compute the shortest path distance and communicate between a node and all anchor nodes in a large graph . So are other possible baselines . Therefore , we only compared with the baseline results reported in the [ OGB leaderboard ] ( https : //ogb.stanford.edu/docs/leader_linkprop/ ) . We believe the purpose of such leaderboards is to provide a unified protocol/place for fairly comparing various GNNs , so that the authors can focus on improving their own model instead of spending great time addressing baselines \u2019 different settings . Nevertheless , we will try to make P-GNN work on OGB and compare it in the revision . 4 . `` Why don \u2019 t compare with knowledge graph embedding methods and use knowledge graph datasets ? '' Thanks.Knowledge graphs ( FB15K , WN18 etc . ) are heterogeneous , featureless , and each node has a distinct identity ( type ) . These properties make knowledge graph link prediction very different from the standard link prediction problem in homogeneous networks . The baselines for knowledge graph link prediction and standard link prediction are not interchangeable . We can neither compare GAE/SEAL using knowledge graph datasets , nor compare with knowledge graph link prediction baselines using OGB . 5 . `` In def 5 , gnn does not always have an invariant function mapping , depending on the message passing operations used e.g.in the graph sage paper , there are comparison using lstm message passing operator , which is not permutation invariant . '' Thanks.Our definition of GNN covers a class of most popular permutation invariant message passing networks . The definition does not cover GraphSAGE with LSTM operator . However , such non-permutation-invariant versions are of less practical usefulness as two isomorphic structures can be mapped to different representations . 6 . `` Section 3.1 mentions that the paper uses GAE to denote the general class of GNN-based link prediction regardless of the choice of GNN and aggregation function . However , research has shown that the specific choices of message passing operations and aggregations have significant impact on the expressiveness of GNN [ 3 ] '' We agree with this . However , our Definition 5 covers the most expressive GNN , GIN , discussed in [ 3 ] . As long as the GNN used in GAE satisfies Definition 5 , it is not able to discriminate between link ( v1 , v2 ) and link ( v1 , v3 ) in Figure 1 ( even using GIN ) , while the same GNN + labeling trick can . In other words , regardless the expressiveness of the underlying GNN , our labeling trick can always improve its link prediction ability . 7 . `` Section 3.3 mentions that GAE can not learn structural link representations , but from the example in fig1 , it seems like this is caused by the graph isomorphism issue . there are existing work that can mitigate this issue , for example position-aware graph neural network [ 2 ] . '' Thanks.We agree P-GNN is another way to break the symmetry in Figure 1 . However , P-GNN introduces positional embeddings to GNN , making the GNN model no longer inductive . It also can not guarantee to map two isomorphic links ( with different relative positions to the anchor nodes ) to the same representation , which is what labeling trick means to ressolve . [ 2 ] You , Jiaxuan , Rex Ying , and Jure Leskovec . `` Position-aware graph neural networks . '' arXiv preprint arXiv:1906.04817 ( 2019 ) . [ 3 ] Xu , Keyulu , et al . `` How powerful are graph neural networks ? . '' arXiv preprint arXiv:1810.00826 ( 2018 ) ."}, {"review_id": "8q_ca26L1fz-2", "review_text": "This paper provides theoretical analysis of graph neural networks for link prediction , following a number of recent papers that have developed the theoretical understanding of graph neural networks . For example , the work of Xu et al ( ICLR 2019 ) draws on the Weisfeiler-Lehman graph isomorphism test to develop a theoretical framework in which to analyse the expressive power of Graph Neural Networks . In particular , they show that architectural features in common graph neural network models limit the expressivity with respect to problems of node and graph classification and show how one can construct graph neural networks whose expressivity matches the Weisfeiler-Lehmam test . In more recent work , Li et al ( NeurIPS 2020 ) show that augmenting node features with a `` distance encoding '' enables a graph neural network to distinguish node sets in cases where the Weisfeiler-Lehman test fails . This submission is perhaps most closely related to this recent paper of Li et al.In this work , the authors discuss how GAEs can not always discriminate between links ( node pairs ) , even when a GAE has maximal expressivity at the node level ( it can generate representations which discriminate all non-isomorphic nodes ) . They provide an enhancement they call a `` labelling trick '' which can be used to enable GAEs to discriminate between node sets . With the labelleing trick , ach node in the GAE has an additional feature corresponding to a node set specific label . That is , when learning representations for a given node set S , this feature takes on a value which is dependent on the node set S and the graph structure . The authors show ( Theorem 1 ) that this enhancement enables any maximally node expressive GNN ( a GNN which can discriminate all non-isomorphic nodes ) to discriminate between any pair of non-isomorphic node sets . The strong performance of SEAL , a recent SOTA method for link prediction , can be understood in the context of this result . As a second theoretical contribution , the authors define a `` local-h isomorphism '' concept , which they propose can be used to explain why practical GNN implementations perform well , despite theoretically requiring the number of layers to be proportional to the number of nodes in order to distinguish all nodes that 1-WL can discriminate ( in the worst case ) . * * Positives * * Studying the expressive power of Graph Neural Network architectures is an important topic , as can be seen by the increasing number of papers in machine learning conferences over the past couple of years . The analysis in this paper extends the theory with respect to the link prediction problem and provides theoretical justification for the strong performance of SEAL on link prediction benchmarks . * * Concerns * * However , I have some concerns regarding the novelty of the contribution and the significance of the main theoretical result ( Theorem 1 ) . 1.The main methodological contribution in this paper is the concept of the labelling trick , which can be used to improve the representational power of GNNs and explain the strong performance of SEAL . This looks like a special case of the distance encoding method presented in Li et al ( NeurIPS 2020 ) . The authors cite this work but do not discuss how their labelling trick relates to the distance encoding . Li et al also provide an explanation of the strong performance of SEAL ( as an instance of their distance encoding method ) as well as using their analysis to motivate alternative models which give a small performance improvement over SEAL . 2.The main theoretical result ( Theorem 1 ) assumes the existence of a GNN with maximum node expressivity , in the sense that it can discriminate all non-isomorphic nodes in a graph . By my understanding , this architecture does not exist . Indeed , I believe Theorem 3.7 of Li et al ( NeurIPS 2020 ) shows that the distance encoding method ( which seems to be closely related to the labelling trick ) is not sufficient for this . In my view , a more useful theoretical result would be something along the lines of Theorem 3.3 of Li et al , which illustrates how the distance encoding method improves the representational power of GNNs for a large number of graphs where methods bounded by 1-WL would fail .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the insightful comments . Below we address the main concerns . 1 . `` The main methodological contribution in this paper is the concept of the labelling trick , which can be used to improve the representational power of GNNs and explain the strong performance of SEAL . This looks like a special case of the distance encoding method presented in Li et al ( NeurIPS 2020 ) . The authors cite this work but do not discuss how their labelling trick relates to the distance encoding . '' We indeed discussed the relation between our paper and ( Li et.al.NeurIPS 2020 ) in the Related Work section . To restate , distance encoding is a particular form of the labeling trick . When distance encoding is used for link prediction , the shortest-path-distance-based node labels are equivalent to the DRNL of SEAL ( both label a node according to its SPD to the source and target nodes ) . The difference between our paper and ( Li et.al.NeurIPS 2020 ) is that : ( Li et.al.2020 ) shows that distance encoding can improve 1-WL-GNN \u2019 s expressive power , enabling them to distinguish almost all node sets sampled from * r-regular * graphs . However , our analysis is not restricted to r-regular graphs , but discusses any general graph . We show that a sufficiently expressive GNN can discriminate non-isomorphic node sets in * any graphs * with the labeling trick . Thus , our paper gives a broader applicability of the labeling trick , showing that it is generally useful for link prediction in any graphs . Moreover , we give a more general definition of labeling trick , which incorporates DRNL , distance encoding , and the simplest annotation of S , etc . into the same framework . As long as a node labeling satisfies Definition 8 , it can enable the structural link representation learning ability of GNNs . Thus , it actually does not restrict the node labels to be distance-based , and implies a promising research field of studying more novel ways of node labeling to enhance GNNs . 2 . `` The main theoretical result ( Theorem 1 ) assumes the existence of a GNN with maximum node expressivity , in the sense that it can discriminate all non-isomorphic nodes in a graph . By my understanding , this architecture does not exist . Indeed , I believe Theorem 3.7 of Li et al ( NeurIPS 2020 ) shows that the distance encoding method ( which seems to be closely related to the labelling trick ) is not sufficient for this . In my view , a more useful theoretical result would be something along the lines of Theorem 3.3 of Li et al , which illustrates how the distance encoding method improves the representational power of GNNs for a large number of graphs where methods bounded by 1-WL would fail . '' We actually discussed this issue in the paragraph after Proposition 2 . We emphasized that even without a node-most-expressive GNN , the labeling trick can still benefit most link representation learning tasks . For example , in Figure 2 , as long as a normal GNN can give different embeddings to v2 and v3 in the left and right graphs ( which is easy for most GNNs ) , we can still differentiate link ( v1 , v2 ) from link ( v1 , v3 ) because the neighborhood of v2 and v3 are no longer the same with the node label of v1 . Also , as discussed in the paper , although such a node-most-expressive GNN is not guaranteed to exist , practical GNNs/1-WL are powerful enough to discriminate almost all non-isomorphic nodes . Thus , Theorem 1 also provides insights to practical GNNs . With a practical GNN , Theorem 1 can still guarantee non-isomorphic links to be mapped to different representations as long as the practical GNN can discriminate the non-isomorphic nodes from two links ( such as v2 and v3 in Figure 2 ) . This is true almost surely according to [ 1 ] . The reason why we assume a node-most-expressive GNN is because it allows a clean argument of the benefit of labeling trick ( enables a GNN learning structural node representations to also learn structural link representations ) , and connects the concept of structural representations with GNNs . Our theorems are orthogonal to ( Li et.al.NeurIPS 2020 ) in terms of practical significance . # # # # # Summarization To summarize , distance encoding is a special form of the labeling trick . As our first contribution , we prove that any labeling trick satisfying Definition 8 can enable a sufficiently expressive GNN to distinguish ( almost ) all non-isomorphic links . This is not proved in ( Li et.al.NeurIPS 2020 ) , and is raised as an open problem questioning GNN for link prediction in ( Srinivasan and Ribeiro ICLR 2020 ) . We provided a solution here , which reassures using GNNs for link prediction . Our second contribution is to define local isomorphisms to justify learning from local subgraphs . Our re-implementation of SEAL also significantly improved the state-of-the-art results of OGB link prediction datasets . [ 1 ] Laszl \u00b4 o Babai and Ludik Kucera . Canonical labelling of graphs in linear average time . In \u00b4 20th Annual Symposium on Foundations of Computer Science ( sfcs 1979 ) , pp . 39\u201346.IEEE , 1979 ."}, {"review_id": "8q_ca26L1fz-3", "review_text": "The authors tried to explore the key differences between two link prediction methods . However , some statements are not precise . The example the authors provided in the introduction is not correct . The GAE will also assign them different probabilities . When we use GAE to learn node embedding in the graphs , we usually have two different inputs : ( A ) nodes in the graph have input features ( B ) nodes in the graph do not have features and we assign each a one hot vector . For both two cases , each node in the graph in Figure 1 is unique node . Even node v2 and v3 play the same role in terms of graph structure , they still have different node embeddings considering the node features . It is worthy to discuss that why node embeddings learning from exisiting method can not help SEAL . It can be seen from the experimental results in two SEAL papers [ 1 ] [ 2 ] that the node embedding does not help improve performance . [ 1 ] Inductive Matrix Completion Based on Graph Neural Networks [ 2 ] Link Prediction Based on Graph Neural Networks", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for providing the context of GAE . We indeed have implicitly assumed that the graph does not have node-identifying features . However , this is a standard assumption in theoretical GNN papers . If we use node-identifying features , there is no need for studying the expressive power of GNNs , because we can always discriminate between any two nodes/links/graphs using their discriminative features . However , even there are node-identifying features , there is still a need to study the pure structure representation ability of GNNs . This is because discriminative features mean less generalizability . In the extreme case of using one-hot node features , these unique node identifiers do not generalize at all . Thus , the generalization ability of GNN still has to come from structure learning . This motivates the research in the paper . We want to not only map non-isomorphic links to different representations ( which is also achievable with node-identifying features ) , while mapping isomorphic links to the same representation ( which is not achievable with node-identifying features ) . By the way , the analysis in the paper allows the presentation of node features ( Definition 1 ) . That is , our theorems are applicable to attributed graphs . However , our theorems are not applicable to one-hot node features ( so are not most other theoretical GNN papers ) . The issue of using one-hot node features is that all the discussions on isomorphisms ( Definition 3 , 10 , Theorem 1 , 2 ) will make no sense , as any two nodes/links/graphs will be non-isomorphic directly due to their different node features . We agree with reviewer that it is worthy to study why node embeddings does not help SEAL , yet it is not the focus of this paper . We really hope the reviewer could reevaluate the main theorems of the paper instead of rejecting the paper only based on the example in Figure 1 . We will make our assumptions on node features more clear in the revision ."}], "0": {"review_id": "8q_ca26L1fz-0", "review_text": "Summary : The paper presents results of two popular classes of methods for graph neural networks . Namely , GAE and SEAL . The paper show results on the Open Graph Benchmark of several existing methods , and concludes that GAE can not learn structural link representations while SEAL can . The paper is descriptive in its presentation , and does a good work on detailing the ideas behind GAE and SEAL . However , I felt the lack of a contribution on the paper . As it stands , it reads more as a tutorial and presents key insights of existing methods . Pros : - Clear explanations of existing work . Cons : - No clear contribution over explaining existing approaches . - Experiments show existing results on OGB . - No results on the mentioned labeling trick are given . Comments : - In Fig.1 you mentioned that $ v_2 $ and $ v_3 $ will get the same representation through a GAE . Did you train one and observed it ? It will be more conclusive if you show the results and the embeddings to validate your observation . - Your labeling trick depends on the set of nodes $ S $ used . However , how do you select such sets to produce the labeling ? Are you sampling all possible subsets $ S $ ? Since this seems to be a form of contribution , it should be clear how to use it in any scenario . - In your experiments , due to the way the paper presents the label trick as the novelty , I was expecting to see the boost of performance of such trick on the existing architectures . However , Section 7 presents results of existing approaches on the OGB . Minor comments : - `` Following ( Srinivasan & Ribeiro , 2020 ; Li et al. , 2020 ) '' should be a textual citation Overall rating : Due to the main problems stated , I can not recommend the paper for publication at ICLR . The manuscript reads more like a tutorial , which may be of interest for readers finding a summary of the advances . However , as a paper advancing the field of graph neural networks I feel it lacking .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for the comments . However , we do not agree that this paper is only a tutorial of existing methods . Our paper discusses the deep differences between GAE and SEAL , and answers why SEAL outperforms GAE on link prediction using extensive theoretical arguments . Also , it seems the reviewer has missed a lot of important details in the paper . Below we try to address the comments . 1 . `` No clear contribution over explaining existing approaches . '' We argue that the main contribution of the paper is to demonstrate why SEAL is able to outperform GAE methods on link prediction , and reveal that the key component is the labeling trick . It defines a general form of the labeling trick ( Definition 5 ) , and shows that with the labeling trick a sufficiently expressive GNN can discriminate all non-isomorphic links ( Theorem 1 ) , which is not possible for GAEs as shown in Figure 1 . We also demonstrate why practical GNNs only need a small local subgraph to achieve a good performance without reaching the theoretical worst bound of WL test through Definition 10 and Theorem 2 . It is unfortunate that the reviewer overlooked these significant theoretical contributions . 2 . `` Experiments show existing results on OGB . '' The results of SEAL on OGB leaderboard are uploaded by us after the deadline of ICLR 2021 , and are used to support this submission . In this paper , we reimplemented SEAL , and fundamentally improved its scalability in order to run on OGB datasets . We are sorry to cause the confusion that it seems we are \u201c copying \u201d existing results on OGB leaderboard without doing experiments in this paper . 3 . `` No results on the mentioned labeling trick are given . '' The entire section 4.2 is trying to convey that SEAL is exactly a GNN enhanced by labeling trick . Our results on SEAL show that a GCN enhanced by labeling trick outperforms baselines significantly . 4 . `` In Fig.1 you mentioned that v2 and v3 will get the same representation through a GAE . Did you train one and observed it ? It will be more conclusive if you show the results and the embeddings to validate your observation . '' No , we didn \u2019 t . And it is not necessary . The reason is as follows . GNN has the same convolution parameters for all nodes . v2 and v3 also have the same neighborhood structure . Applying GNN on v2 and v3 will lead to exactly the same representations . The training of GNN will change the parameters over epochs , but still the parameters are shared across all nodes . So no matter how we train the GNN , v2 and v3 will always have the same representation . 5 . `` Your labeling trick depends on the set of nodes S used . However , how do you select such sets to produce the labeling ? Are you sampling all possible subsets S ? Since this seems to be a form of contribution , it should be clear how to use it in any scenario . '' S is the set of nodes to learn a joint structural representation for . For link prediction , S is exactly the source and target nodes to predict link between . The notation of S is used consistently across the paper to denote the target node set of interest since Definition 3 . There is no sampling needed . 6 . `` In your experiments , due to the way the paper presents the label trick as the novelty , I was expecting to see the boost of performance of such trick on the existing architectures . However , Section 7 presents results of existing approaches on the OGB . '' SEAL is exactly a GCN + labeling trick . By showing SEAL \u2019 s significant performance boost over baselines , the experiments demonstrate the of great value of the labeling trick . 7 . `` Following ( Srinivasan & Ribeiro , 2020 ; Li et al. , 2020 ) '' should be a textual citation Thanks ! We will revise it . Summarization We are again sorry about causing those confusions to the reviewer . To clarify better , SEAL is originally proposed in ( Zhang and Chen NeurIPS 2018 ) . It is a SotA method for link prediction , but people have little understanding of why it outperforms normal GNNs . In this paper , we identify one key component of SEAL is its labeling trick , and analyze theoretically how the labeling trick helps a GNN ( that only discriminates non-isomorphic nodes ) to also discriminate non-isomorphic links . We also reimplemented SEAL using advanced data structures and libraries to achieve ~1000 times speed up , and for the first time got its performance on the large-scale OGB datasets , which again verified its SotA performance . All these works are nontrivial , and beyond just a tutorial of existing methods ."}, "1": {"review_id": "8q_ca26L1fz-1", "review_text": "The paper focuses on the link prediction task for graph neural networks . More specifically , it compares GAE and SEAL by providing theoretical evidence why GAE is not able to learn structural link representations , which as a result leads to suboptimal performance in the link prediction task . The paper also introduces a labeling trick that can help GNNs to learn structural link representations . Strong points : 1 . The paper is clear , easy to understand , and the definitions and theorems provide good reference and proofs . 2.A deeper understanding of the link prediction task is indeed needed as a lot of GNN works are mostly using classification as the downstream task . 3.It provides good argument and clear example why some GNN models would not work well in link prediction by showing that individually learning node representations can not handle the case where two nodes appear in topologically identical neighborhoods in the graph . It also provides insights as to how to mitigate such issues . Weak points : 1 . The comparison is oversimplified and overly generalized . The paper mostly compares GAE and SEAL , but there are much more GNN models that should be compared . 2.The sheer separation of GAE and SEAL model is not convincing . GAE and SEAL are both GNN models which follow the message passing and aggregation approaches . There 's no clear argument or theoretical explanation how these two are fundamentally different . 3.There are a lot of existing work that considers position in encoding nodes ( such as position aware gnn [ 2 ] ) and should be able to learn expressive structural link representations . The paper does not compare with them . 4.The experiment does not cover the most recent models . In addition , more benchmark datasets should be used . For example , FB15K ( this is a widely used knowledge graph link prediction benchmark dataset ) , PPI , WN18 , etc . Since previous work on gnn link prediction uses those dataset , I 'm expecting to see a more comprehensive comparison using those datasets . More dataset can be found here : https : //paperswithcode.com/task/link-prediction 5 . The paper should also consider comparing with knowledge graph embedding models which are focusing on the link prediction task . There are also existing work that combine the idea of knowledge graph embedding with GNN models in link prediction [ 1 ] . Other comments : 1 . In def 5 , gnn does not always have an invariant function mapping , depending on the message passing operations used e.g.in the graph sage paper , there are comparison using lstm message passing operator , which is not permutation invariant . 2.Section 3.1 mentions that the paper uses GAE to denote the general class of GNN-based link prediction regardless of the choice of GNN and aggregation function . However , research has shown that the specific choices of message passing operations and aggregations have significant impact on the expressiveness of GNN [ 3 ] 3 . Section 3.3 mentions that GAE can not learn structural link representations , but from the example in fig1 , it seems like this is caused by the graph isomorphism issue . there are existing work that can mitigate this issue , for example position-aware graph neural network [ 2 ] . Reference : 1 . Schlichtkrull , Michael , et al . `` Modeling relational data with graph convolutional networks . '' European Semantic Web Conference . Springer , Cham , 2018 . 2.You , Jiaxuan , Rex Ying , and Jure Leskovec . `` Position-aware graph neural networks . '' arXiv preprint arXiv:1906.04817 ( 2019 ) . 3.Xu , Keyulu , et al . `` How powerful are graph neural networks ? . '' arXiv preprint arXiv:1810.00826 ( 2018 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . 1 . `` The paper mostly compares GAE and SEAL , but there are much more GNN models that should be compared . '' GAE and SEAL are two main GNN link prediction paradigms . They do not restrict which specific GNN model is used . Any GNN , such as GCN , GraphSage or GIN , can be used in both GAE and SEAL frameworks . In the experiments , GCN , GraphSAGE and GCN+LRGA are used in GAE , and GCN is used in SEAL . Using more recent GNNs may further improve both GAE and SEAL 's performance , but it is beyond the purpose of the paper ( which compares the frameworks of GAE and SEAL ) . 2 . `` The sheer separation of GAE and SEAL model is not convincing . GAE and SEAL are both GNN models which follow the message passing and aggregation approaches . There 's no clear argument or theoretical explanation how these two are fundamentally different . '' We believe the entire section 3 and 4 are trying to discuss the differences between GAE and SEAL and answer why SEAL is more suitable than GAE for link prediction in a theoretical way . In short , SEAL uses GNN + labeling trick , while GAE only uses GNN . 3 . `` There are a lot of existing work that considers position in encoding nodes ( such as position aware gnn [ 2 ] ) and should be able to learn expressive structural link representations . The paper does not compare with them . '' Thanks.We indeed think P-GNN can be a competitive baseline . However , the original P-GNN paper uses datasets with at most 3,000 nodes . It is nontrivial to scale it to OGB datasets with millions of nodes and tens of millions of edges , especially when one need to compute the shortest path distance and communicate between a node and all anchor nodes in a large graph . So are other possible baselines . Therefore , we only compared with the baseline results reported in the [ OGB leaderboard ] ( https : //ogb.stanford.edu/docs/leader_linkprop/ ) . We believe the purpose of such leaderboards is to provide a unified protocol/place for fairly comparing various GNNs , so that the authors can focus on improving their own model instead of spending great time addressing baselines \u2019 different settings . Nevertheless , we will try to make P-GNN work on OGB and compare it in the revision . 4 . `` Why don \u2019 t compare with knowledge graph embedding methods and use knowledge graph datasets ? '' Thanks.Knowledge graphs ( FB15K , WN18 etc . ) are heterogeneous , featureless , and each node has a distinct identity ( type ) . These properties make knowledge graph link prediction very different from the standard link prediction problem in homogeneous networks . The baselines for knowledge graph link prediction and standard link prediction are not interchangeable . We can neither compare GAE/SEAL using knowledge graph datasets , nor compare with knowledge graph link prediction baselines using OGB . 5 . `` In def 5 , gnn does not always have an invariant function mapping , depending on the message passing operations used e.g.in the graph sage paper , there are comparison using lstm message passing operator , which is not permutation invariant . '' Thanks.Our definition of GNN covers a class of most popular permutation invariant message passing networks . The definition does not cover GraphSAGE with LSTM operator . However , such non-permutation-invariant versions are of less practical usefulness as two isomorphic structures can be mapped to different representations . 6 . `` Section 3.1 mentions that the paper uses GAE to denote the general class of GNN-based link prediction regardless of the choice of GNN and aggregation function . However , research has shown that the specific choices of message passing operations and aggregations have significant impact on the expressiveness of GNN [ 3 ] '' We agree with this . However , our Definition 5 covers the most expressive GNN , GIN , discussed in [ 3 ] . As long as the GNN used in GAE satisfies Definition 5 , it is not able to discriminate between link ( v1 , v2 ) and link ( v1 , v3 ) in Figure 1 ( even using GIN ) , while the same GNN + labeling trick can . In other words , regardless the expressiveness of the underlying GNN , our labeling trick can always improve its link prediction ability . 7 . `` Section 3.3 mentions that GAE can not learn structural link representations , but from the example in fig1 , it seems like this is caused by the graph isomorphism issue . there are existing work that can mitigate this issue , for example position-aware graph neural network [ 2 ] . '' Thanks.We agree P-GNN is another way to break the symmetry in Figure 1 . However , P-GNN introduces positional embeddings to GNN , making the GNN model no longer inductive . It also can not guarantee to map two isomorphic links ( with different relative positions to the anchor nodes ) to the same representation , which is what labeling trick means to ressolve . [ 2 ] You , Jiaxuan , Rex Ying , and Jure Leskovec . `` Position-aware graph neural networks . '' arXiv preprint arXiv:1906.04817 ( 2019 ) . [ 3 ] Xu , Keyulu , et al . `` How powerful are graph neural networks ? . '' arXiv preprint arXiv:1810.00826 ( 2018 ) ."}, "2": {"review_id": "8q_ca26L1fz-2", "review_text": "This paper provides theoretical analysis of graph neural networks for link prediction , following a number of recent papers that have developed the theoretical understanding of graph neural networks . For example , the work of Xu et al ( ICLR 2019 ) draws on the Weisfeiler-Lehman graph isomorphism test to develop a theoretical framework in which to analyse the expressive power of Graph Neural Networks . In particular , they show that architectural features in common graph neural network models limit the expressivity with respect to problems of node and graph classification and show how one can construct graph neural networks whose expressivity matches the Weisfeiler-Lehmam test . In more recent work , Li et al ( NeurIPS 2020 ) show that augmenting node features with a `` distance encoding '' enables a graph neural network to distinguish node sets in cases where the Weisfeiler-Lehman test fails . This submission is perhaps most closely related to this recent paper of Li et al.In this work , the authors discuss how GAEs can not always discriminate between links ( node pairs ) , even when a GAE has maximal expressivity at the node level ( it can generate representations which discriminate all non-isomorphic nodes ) . They provide an enhancement they call a `` labelling trick '' which can be used to enable GAEs to discriminate between node sets . With the labelleing trick , ach node in the GAE has an additional feature corresponding to a node set specific label . That is , when learning representations for a given node set S , this feature takes on a value which is dependent on the node set S and the graph structure . The authors show ( Theorem 1 ) that this enhancement enables any maximally node expressive GNN ( a GNN which can discriminate all non-isomorphic nodes ) to discriminate between any pair of non-isomorphic node sets . The strong performance of SEAL , a recent SOTA method for link prediction , can be understood in the context of this result . As a second theoretical contribution , the authors define a `` local-h isomorphism '' concept , which they propose can be used to explain why practical GNN implementations perform well , despite theoretically requiring the number of layers to be proportional to the number of nodes in order to distinguish all nodes that 1-WL can discriminate ( in the worst case ) . * * Positives * * Studying the expressive power of Graph Neural Network architectures is an important topic , as can be seen by the increasing number of papers in machine learning conferences over the past couple of years . The analysis in this paper extends the theory with respect to the link prediction problem and provides theoretical justification for the strong performance of SEAL on link prediction benchmarks . * * Concerns * * However , I have some concerns regarding the novelty of the contribution and the significance of the main theoretical result ( Theorem 1 ) . 1.The main methodological contribution in this paper is the concept of the labelling trick , which can be used to improve the representational power of GNNs and explain the strong performance of SEAL . This looks like a special case of the distance encoding method presented in Li et al ( NeurIPS 2020 ) . The authors cite this work but do not discuss how their labelling trick relates to the distance encoding . Li et al also provide an explanation of the strong performance of SEAL ( as an instance of their distance encoding method ) as well as using their analysis to motivate alternative models which give a small performance improvement over SEAL . 2.The main theoretical result ( Theorem 1 ) assumes the existence of a GNN with maximum node expressivity , in the sense that it can discriminate all non-isomorphic nodes in a graph . By my understanding , this architecture does not exist . Indeed , I believe Theorem 3.7 of Li et al ( NeurIPS 2020 ) shows that the distance encoding method ( which seems to be closely related to the labelling trick ) is not sufficient for this . In my view , a more useful theoretical result would be something along the lines of Theorem 3.3 of Li et al , which illustrates how the distance encoding method improves the representational power of GNNs for a large number of graphs where methods bounded by 1-WL would fail .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the insightful comments . Below we address the main concerns . 1 . `` The main methodological contribution in this paper is the concept of the labelling trick , which can be used to improve the representational power of GNNs and explain the strong performance of SEAL . This looks like a special case of the distance encoding method presented in Li et al ( NeurIPS 2020 ) . The authors cite this work but do not discuss how their labelling trick relates to the distance encoding . '' We indeed discussed the relation between our paper and ( Li et.al.NeurIPS 2020 ) in the Related Work section . To restate , distance encoding is a particular form of the labeling trick . When distance encoding is used for link prediction , the shortest-path-distance-based node labels are equivalent to the DRNL of SEAL ( both label a node according to its SPD to the source and target nodes ) . The difference between our paper and ( Li et.al.NeurIPS 2020 ) is that : ( Li et.al.2020 ) shows that distance encoding can improve 1-WL-GNN \u2019 s expressive power , enabling them to distinguish almost all node sets sampled from * r-regular * graphs . However , our analysis is not restricted to r-regular graphs , but discusses any general graph . We show that a sufficiently expressive GNN can discriminate non-isomorphic node sets in * any graphs * with the labeling trick . Thus , our paper gives a broader applicability of the labeling trick , showing that it is generally useful for link prediction in any graphs . Moreover , we give a more general definition of labeling trick , which incorporates DRNL , distance encoding , and the simplest annotation of S , etc . into the same framework . As long as a node labeling satisfies Definition 8 , it can enable the structural link representation learning ability of GNNs . Thus , it actually does not restrict the node labels to be distance-based , and implies a promising research field of studying more novel ways of node labeling to enhance GNNs . 2 . `` The main theoretical result ( Theorem 1 ) assumes the existence of a GNN with maximum node expressivity , in the sense that it can discriminate all non-isomorphic nodes in a graph . By my understanding , this architecture does not exist . Indeed , I believe Theorem 3.7 of Li et al ( NeurIPS 2020 ) shows that the distance encoding method ( which seems to be closely related to the labelling trick ) is not sufficient for this . In my view , a more useful theoretical result would be something along the lines of Theorem 3.3 of Li et al , which illustrates how the distance encoding method improves the representational power of GNNs for a large number of graphs where methods bounded by 1-WL would fail . '' We actually discussed this issue in the paragraph after Proposition 2 . We emphasized that even without a node-most-expressive GNN , the labeling trick can still benefit most link representation learning tasks . For example , in Figure 2 , as long as a normal GNN can give different embeddings to v2 and v3 in the left and right graphs ( which is easy for most GNNs ) , we can still differentiate link ( v1 , v2 ) from link ( v1 , v3 ) because the neighborhood of v2 and v3 are no longer the same with the node label of v1 . Also , as discussed in the paper , although such a node-most-expressive GNN is not guaranteed to exist , practical GNNs/1-WL are powerful enough to discriminate almost all non-isomorphic nodes . Thus , Theorem 1 also provides insights to practical GNNs . With a practical GNN , Theorem 1 can still guarantee non-isomorphic links to be mapped to different representations as long as the practical GNN can discriminate the non-isomorphic nodes from two links ( such as v2 and v3 in Figure 2 ) . This is true almost surely according to [ 1 ] . The reason why we assume a node-most-expressive GNN is because it allows a clean argument of the benefit of labeling trick ( enables a GNN learning structural node representations to also learn structural link representations ) , and connects the concept of structural representations with GNNs . Our theorems are orthogonal to ( Li et.al.NeurIPS 2020 ) in terms of practical significance . # # # # # Summarization To summarize , distance encoding is a special form of the labeling trick . As our first contribution , we prove that any labeling trick satisfying Definition 8 can enable a sufficiently expressive GNN to distinguish ( almost ) all non-isomorphic links . This is not proved in ( Li et.al.NeurIPS 2020 ) , and is raised as an open problem questioning GNN for link prediction in ( Srinivasan and Ribeiro ICLR 2020 ) . We provided a solution here , which reassures using GNNs for link prediction . Our second contribution is to define local isomorphisms to justify learning from local subgraphs . Our re-implementation of SEAL also significantly improved the state-of-the-art results of OGB link prediction datasets . [ 1 ] Laszl \u00b4 o Babai and Ludik Kucera . Canonical labelling of graphs in linear average time . In \u00b4 20th Annual Symposium on Foundations of Computer Science ( sfcs 1979 ) , pp . 39\u201346.IEEE , 1979 ."}, "3": {"review_id": "8q_ca26L1fz-3", "review_text": "The authors tried to explore the key differences between two link prediction methods . However , some statements are not precise . The example the authors provided in the introduction is not correct . The GAE will also assign them different probabilities . When we use GAE to learn node embedding in the graphs , we usually have two different inputs : ( A ) nodes in the graph have input features ( B ) nodes in the graph do not have features and we assign each a one hot vector . For both two cases , each node in the graph in Figure 1 is unique node . Even node v2 and v3 play the same role in terms of graph structure , they still have different node embeddings considering the node features . It is worthy to discuss that why node embeddings learning from exisiting method can not help SEAL . It can be seen from the experimental results in two SEAL papers [ 1 ] [ 2 ] that the node embedding does not help improve performance . [ 1 ] Inductive Matrix Completion Based on Graph Neural Networks [ 2 ] Link Prediction Based on Graph Neural Networks", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for providing the context of GAE . We indeed have implicitly assumed that the graph does not have node-identifying features . However , this is a standard assumption in theoretical GNN papers . If we use node-identifying features , there is no need for studying the expressive power of GNNs , because we can always discriminate between any two nodes/links/graphs using their discriminative features . However , even there are node-identifying features , there is still a need to study the pure structure representation ability of GNNs . This is because discriminative features mean less generalizability . In the extreme case of using one-hot node features , these unique node identifiers do not generalize at all . Thus , the generalization ability of GNN still has to come from structure learning . This motivates the research in the paper . We want to not only map non-isomorphic links to different representations ( which is also achievable with node-identifying features ) , while mapping isomorphic links to the same representation ( which is not achievable with node-identifying features ) . By the way , the analysis in the paper allows the presentation of node features ( Definition 1 ) . That is , our theorems are applicable to attributed graphs . However , our theorems are not applicable to one-hot node features ( so are not most other theoretical GNN papers ) . The issue of using one-hot node features is that all the discussions on isomorphisms ( Definition 3 , 10 , Theorem 1 , 2 ) will make no sense , as any two nodes/links/graphs will be non-isomorphic directly due to their different node features . We agree with reviewer that it is worthy to study why node embeddings does not help SEAL , yet it is not the focus of this paper . We really hope the reviewer could reevaluate the main theorems of the paper instead of rejecting the paper only based on the example in Figure 1 . We will make our assumptions on node features more clear in the revision ."}}