{"year": "2019", "forum": "Bygh9j09KX", "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "decision": "Accept (Oral)", "meta_review": "This paper proposes a hypothesis about the kinds of visual information for which popular neural networks are most selective.  It then proposes a series of empirical experiments on synthetically modified training sets to test this and related hypotheses.  The main conclusions of the paper are contained in the title, and the presentation was consistently rated as very clear.  As such, it is both interesting to a relatively wide audience and accessible.\n\nAlthough the paper is comparatively limited in theoretical or algorithmic contribution, the empirical results and experimental design are of sufficient quality to inform design choices of future neural networks, and to better understand the reasons for their current behavior.\n\nThe reviewers were unanimous in their appreciation of the contributions, and all recommended that the paper be accepted.\n\n", "reviews": [{"review_id": "Bygh9j09KX-0", "review_text": "Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures. If the authors address my major concerns, I would increasing my rating 1-2 points. Major Comments: The results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool. 1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include: --> \"These experiments provide conclusive behavioural evidence in favour of the texture hypothesis\" --> \"we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.\" I would prefer to see language such as \"We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs.\" or \"We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements\". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly. 2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)? Although the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training? 3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. \"the convex outline of object segmentation\", etc.). Minor Comments: - Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together. - Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score. - In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest. - Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization? - Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels? - Please use names of Shape-ResNet, etc. in Table 2. - Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques. - A.2. \"not not used in the experiment\" --> \"not used in the experiment\" ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear Reviewer 1 , Thank you for reviewing our paper , and for your helpful suggestions . We appreciate your assessment of our results as `` very cool '' and `` surprising '' . We are happy to address your detailed suggestions and questions in a point-by-point response below . Writing : claims . We have addressed this concern , which was shared by Reviewer 2 : https : //openreview.net/forum ? id=Bygh9j09KX & noteId=HygXM-Yb07 Robustness and data augmentation . We made sure not to include any image distortions in the training data for any of our networks . Both models displayed in Figure 6 ( ResNet-50 trained on ImageNet and ResNet-50 trained on Stylized-ImageNet ) were trained under identical circumstances with respect to data augmentation ( none apart from random resizing and flipping ) , hyperparameter settings , number of epochs , etc . Hence , any changes in the distortion robustness between these two models can be attributed solely to the changed training data ( inducing different biases ) . We made this more clear in the Introduction as well as in Section 3.3 and the Discussion by writing that the SIN-trained network is more robust `` despite never being trained on any of the distortions '' . Semantics of object shape . We have added the requested definitions in Section 2.2 . We define `` silhouette '' as the bounding contour of an object in 2D ( i.e. , the outline of object segmentation ) . When mentioning `` object shape '' , we use a definition that is broader than just the silhouette of an object : we refer to the set of contours that describe the 3D form of an object , i.e.including those contours that are not part of the silhouette . Quotes in Introduction . In general we agree that it is preferable to summarise ideas in one 's own language . In this particular case , quotes are used with the aim to convince the reader that the `` shape hypothesis '' is not merely a straw man . Response times / forced choice . If human observers are allowed to select `` I do n't know '' , comparing results across participants with different confidence thresholds becomes very difficult . We therefore followed a standard psychophysical paradigm , namely an identification task with `` forced choice '' in the sense that they had to select a category even if unsure . However , observer confidence is typically correlated with reaction times ( rapid responses for confident decisions ) , and we have thus added median reaction times across experiments as a column in Table 3 of the Appendix . This indeed shows that reaction times are longer for experiments with manipulated images . Guard against human fatigue / lack of interest . We have added the following explanation to Section A.4 of the Appendix : `` Overall , we took the following steps to prevent low quality human data : 1. , using a controlled lab environment instead of an online crowdsourcing platform ; 2. the payment motivation scheme as explained above [ i.e. , better payment for better performance in experiments with a unique ground truth category ] ; 3. displaying observer performance on the screen at regular intervals during the practice session ; and 4. splitting longer experiments into five blocks , where participants could take a break in between blocks . '' Reason for fast image stylization . We have added the following explanation to Section 2.3 : `` We used AdaIN fast style transfer rather than iterative stylization ( e.g.Gatys et al. , 2016 ) for two reasons : Firstly , to ensure that training on SIN and testing on cue conflict stimuli is done using different stylization techniques , such that the results do not rely on a single stylization method . Secondly , to enable processing entire ImageNet , which would take prohibitively long with an iterative approach . '' Explanation for large variations in results across labels . One possible explanation for the large variation in CNN results across labels may be that they use different strategies for different categories , i.e.that they sometimes rely solely on the texture ( e.g.for category bear ) , and sometimes more on other cues . This is supported by the fact that there is a negative Spearman correlation between accuracy in our edge experiment , and texture bias in our cue conflict experiment ( AlexNet : -0.582 , GoogLeNet : -0.508 , VGG-16 : -0.238 , ResNet-50 : -0.014 , human observers : -0.621 ) : if a certain category seems hard to recognize from edges and contours , most networks are more likely to show a stronger class-conditional texture bias . Fine-tuning image features for Pascal VOC . We followed standard best practices for using image features in an object detection setting , which includes fine-tuning the image features . Importantly , this was done for all networks equally ( which are trained on ImageNet and Stylized-ImageNet respectively ) , and the networks were trained under identical circumstances w.r.t.data augmentation . Thus , improved object detection performance can be attributed directly to better generic features induced by Stylized-ImageNet . Again , thank you very much for your review and suggestions !"}, {"review_id": "Bygh9j09KX-1", "review_text": "The paper is well written and easy to follow. It was a nice read for me. The paper studies the CNNs like AlexNet, VGG, GoogleNet, ResNet50 and shows that these models are heavily biased towards the texture when trained on ImageNet. The paper shows human evaluations and compares model accuracies when various transformations like cue hypothesis, texture hypothesis (terms coined in the paper) are applied to study texture vs shape importance. The paper shows various results on different models clearly and results are easily interpretable. The paper then proposed a new ImageNet dataset which is called Stylized-ImageNet (SIN) where the texture is replaced with randomly selected painting style. I believe that this is a good empirical study which is needed to understand why the ImageNet features are good (supervised training) and this can inform research in self-supervision, few shot learning domains. The paper is an empirical paper and is presenting a quantitive study of role of texture which others have already presented like Gatys et al. 2017. The paper itself has no novel contributions. The paper notes \"novel Stylized-ImageNet dataset\" and shows that models can learn shape/texture features both but there is not much detail/explanation on why \"Stylized\" is the novel approach and also the methodology of constructing data by replacing with painting from AdaIN style transfer (Huang & Belongie, 2017) is not discussed/explored. More specifically, there is no ablation on other ways this dataset could have been constructed and why style transfer was picked as the choice, why was AdaIN chosen. While the choice is valid, I think these questions need to be answered if we have to consider it \"novel\". Additionally, I would like answers to the following questions: 1. In Figure 4, ResNet50 results are missing. I would be very interested in seeing those results. Can authors show those results? 2. Did authors study deeper networks like RN101/152 and do the observations about texture still hold? 3. Did authors consider inspecting if the models have same texture biases when trained on other datasets like COCO? If yes, can you share your results? 4. In Figure 5, can authors also show the results of training VGG, AlexNet, GoogleNet models on SIN dataset? I believe otherwise the results are incomplete since Fig. 4 shows the biases of these models on IN dataset but doesn't show if these biases are removed by training on SIN. 5. In Section 3.3, Transfer learning, authors show improvement on VOC 2007 Faster R-CNN . Do authors have explanation on why this gain happens? how's the texture learning in pretext task (like image classification training on SIN dataset) tied to the transfer learning no different dataset? 6. What are the results of transfer learning on other datasets like COCO, Faster R-CNN?", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 3 , Thank you very much for reviewing our paper . Please allow us a quick and important clarification regarding your most important criticism ( `` The paper itself has no novel contributions '' ) as we believe this may be due to a misunderstanding : First , we fully agree with you that one of the datasets we created , Stylized-ImageNet , is in itself not a major contribution - we use an existing fast style transfer method to strip ImageNet images of their original texture to replace it with the uninformative texture of a painting . Stylized-ImageNet is , for us , merely a means to an end , enabling us to make three ( novel ) core contributions : 1 . Quantifying existing texture vs. shape biases . Many of the most influential explanations of CNN object recognition [ 1-3 ] describe it as a process of recognizing parts of objects / object shapes ( the shape hypothesis ) . We contrast this with our carefully collected evidence for the texture hypothesis , offering an entirely different explanation . Furthermore , [ e.g.2,4-5 ] argue that CNNs closely mirror human object recognition and human shape perception . We here provide insights into a core difference of human and machine vision by comparing both under fair circumstances . To the best of our knowledge , our work is the first to systematically pitch shape against texture cues to investigate CNN biases and compare them to the human visual system . 2.Overcoming the texture bias in CNNs . Based on our texture hypothesis , we hypothesized that a CNN texture bias might be changed towards a shape bias if trained on a suitable dataset . We demonstrate the effectiveness of this approach , which shows that the texture bias in standard CNNs is not an inherent property of the architecture but rather induced by the training data . To the best of our knowledge , this is a novel finding and has never been attempted before . ( The method of creating a suitable dataset , AdaIN style transfer , is not new . ) 3.Showing emergent benefits of changed CNN biases . We demonstrate substantial advantages of a shape-based over a texture-based representation in CNNs , most importantly better features for transfer learning ( object detection ) and a previously unmatched robustness against a number of image distortions - despite never being trained on any of them . To the best of our knowledge , ShapeResNet is the first network to approach human-level distortion robustness on distortions that were not part of the training data . We believe that describing Stylized-ImageNet as `` novel '' - which we did in our original manuscript , e.g.in our abstract - was misleading since it is , as mentioned above and pointed out by you in your review , not a substantial contribution and , more importantly , merely a means to achieve our main and truly novel contributions . We will thus rephrase our description of Stylized-ImageNet throughout the paper to avoid any misunderstandings by future readers who might get the idea that the dataset itself is being described as our core novel contribution . Apologies for any confusion this issue may have caused in the original submission . We would appreciate it if you could let us know whether this clarifies the issue , and changes your assessment of the novelty of our work . [ 1 ] Goodfellow , I. , Bengio , Y. , Courville , A. , & Bengio , Y . ( 2016 ) .Deep learning ( Vol.1 ) .Cambridge : MIT press . [ 2 ] Kriegeskorte , N. ( 2015 ) . Deep neural networks : a new framework for modeling biological vision and brain information processing . Annual review of vision science , 1 , 417-446 . [ 3 ] LeCun , Y. , Bengio , Y. , & Hinton , G. ( 2015 ) . Deep learning . Nature , 521 ( 7553 ) , 436 . [ 4 ] Kubilius , J. , Bracci , S. , & de Beeck , H. P. O . ( 2016 ) .Deep neural networks as a computational model for human shape sensitivity . PLoS computational biology , 12 ( 4 ) , e1004896 . [ 5 ] Cadieu , C. F. , Hong , H. , Yamins , D. L. , Pinto , N. , Ardila , D. , Solomon , E. A. , ... & DiCarlo , J. J . ( 2014 ) .Deep neural networks rival the representation of primate IT cortex for core visual object recognition . PLoS computational biology , 10 ( 12 ) , e1003963 ."}, {"review_id": "Bygh9j09KX-2", "review_text": "This paper talks about the behavior bias between human and advanced CNN classifier when classifying objects. A clear conclusion is that DNN classifiers lean on texture cues more than human, which is in contrast to empirical evidence. The experimental results are delighting and convincing to some extent. This paper is also inspiring and potentially useful to interpret how CNN works in object classification task. Nevertheless, I have several small issues: - I like the writing of this paper, fluent description and clear topic. Besides, it provides sufficient information about experiment details, thus I think the experiments are fully reproducible. But I want to remind the authors to downplay their claims. Some sentences are not with academic rigor. i.e. \u201cTextures, not object shapes, are the most important cues for CNN object recognition.\u201dI don\u2019t think it a good idea to claim textures as the \u201cmost important\u201dcue. - Although adequate experiments are conducted on ResNet-50 on ImageNet, I miss experiments on a different object classification dataset i.e. PASCAL VOC, and a different network backbone such as very deep ResNet-152 or wider DenseNet. This lies in the concern that a different (deeper or wider) framework may behave quite differently and also the slightly shifted data distribution may induce controversial results. The adopted network ResNet-50, AlexNet, VGG-16 and GoogLeNet are not deep enough or either wide as DenseNet. Although transfer learning experiment is carried out upon PASCAL VOC, it\u2019s not straightforward and not so truly telling. We\u2019re curious about universal conclusions rather than that based on one dataset or network architecture of the same category. As a matter of fact, I\u2019m nearly convinced by the provided results. But I think the demanding experiments will make the conclusions more solid. Besides, I think the constructed dataset is beneficial to further research or fair comparison of future works, and I wonder the authors\u2019 intention to publish such a dataset in the future. I would raise my scores if the aforementioned problems are convincingly checked and solved.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear Reviewer 2 , Thank you very much for your valuable feedback . We appreciate your assessment of our work as an `` inspiring '' paper . We provide a point-by-point response to your three suggestions below . Writing : claims . We have identified a number of sentences where our excitement about the results has biased our writing . We made the following changes : 1 . ) `` the texture hypothesis : object textures , not object shapes as commonly assumed , are the most important cues for CNN object recognition . '' changed to `` the texture hypothesis : in contrast to the common assumption , object textures are more important than global object shapes for CNN object recognition '' . 2 . ) '' conclusive behavioural evidence '' replaced with `` behavioural evidence '' . 3 . ) In the Discussion , `` we found strong evidence '' replaced with `` we provide evidence '' . 4 . ) In the Discussion , `` we conclude the following : Textures , not object shapes , are the most important cues for CNN object recognition '' replaced with `` this highlights the special role that local cues such as textures seem to play in CNN object recognition '' . 5 . ) In the Summary , `` we showed that machine recognition today primarily relies on texture rather than shape cues '' replaced with `` we provided evidence that machine recognition today overly relies on object textures rather than global object shapes as commonly assumed '' . Control experiments with different networks and data sets . You were expressing concern that deeper or wider networks and training on a different object classification data set may lead to different results . To address your concerns we have collected results for the requested control experiments . In Figure 13 of the Appendix , the left plot shows that the texture bias is equally prominent ( if not stronger ) in a network trained on the Open Images classification data set , thus the texture bias is not specific to training on ImageNet . The right plot of Figure 13 shows that ImageNet-trained networks ResNet-152 ( a very deep network ) , DenseNet-121 ( a very wide network ) and for comparison also Squeezenet1_1 ( a highly compressed network ) all have a strong texture bias . Method details are reported in Section A.5 of the Appendix . Furthermore , we have started to train a ResNet-152 architecture on Stylized-ImageNet to use it as an additional `` deep '' backbone for our object detection experiments . Since network training and fine-tuning takes a lot of time , we will not be able to provide results by the end of the rebuttal period but we will make sure to include them afterwards . We have inquired with the ICLR organizers who have assured us it will be possible to make minor changes after the rebuttal period . Release of data sets . We are determined to release our cue conflict images ( such as the cat with elephant skin ) along with our raw data , image manipulation code , data analysis scripts , psychophysical experiment code and links to trained model weights in a github repository at the end of the anonymous review process . Furthermore , we will release code to create Stylized-ImageNet in a separate github repository along with a docker image ; given two directory paths to ImageNet images ( available from the ImageNet website [ 1 ] ) and to the paintings used as a style source ( available from Kaggle 's painter-by-numbers website [ 2 ] ) a shell script then creates Stylized-ImageNet . Again , thank you very much for reviewing our paper and for your valuable suggestions ! [ 1 ] http : //www.image-net.org/ [ 2 ] https : //www.kaggle.com/c/painter-by-numbers/data"}], "0": {"review_id": "Bygh9j09KX-0", "review_text": "Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures. If the authors address my major concerns, I would increasing my rating 1-2 points. Major Comments: The results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool. 1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include: --> \"These experiments provide conclusive behavioural evidence in favour of the texture hypothesis\" --> \"we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.\" I would prefer to see language such as \"We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs.\" or \"We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements\". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly. 2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)? Although the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training? 3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. \"the convex outline of object segmentation\", etc.). Minor Comments: - Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together. - Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score. - In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest. - Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization? - Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels? - Please use names of Shape-ResNet, etc. in Table 2. - Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques. - A.2. \"not not used in the experiment\" --> \"not used in the experiment\" ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear Reviewer 1 , Thank you for reviewing our paper , and for your helpful suggestions . We appreciate your assessment of our results as `` very cool '' and `` surprising '' . We are happy to address your detailed suggestions and questions in a point-by-point response below . Writing : claims . We have addressed this concern , which was shared by Reviewer 2 : https : //openreview.net/forum ? id=Bygh9j09KX & noteId=HygXM-Yb07 Robustness and data augmentation . We made sure not to include any image distortions in the training data for any of our networks . Both models displayed in Figure 6 ( ResNet-50 trained on ImageNet and ResNet-50 trained on Stylized-ImageNet ) were trained under identical circumstances with respect to data augmentation ( none apart from random resizing and flipping ) , hyperparameter settings , number of epochs , etc . Hence , any changes in the distortion robustness between these two models can be attributed solely to the changed training data ( inducing different biases ) . We made this more clear in the Introduction as well as in Section 3.3 and the Discussion by writing that the SIN-trained network is more robust `` despite never being trained on any of the distortions '' . Semantics of object shape . We have added the requested definitions in Section 2.2 . We define `` silhouette '' as the bounding contour of an object in 2D ( i.e. , the outline of object segmentation ) . When mentioning `` object shape '' , we use a definition that is broader than just the silhouette of an object : we refer to the set of contours that describe the 3D form of an object , i.e.including those contours that are not part of the silhouette . Quotes in Introduction . In general we agree that it is preferable to summarise ideas in one 's own language . In this particular case , quotes are used with the aim to convince the reader that the `` shape hypothesis '' is not merely a straw man . Response times / forced choice . If human observers are allowed to select `` I do n't know '' , comparing results across participants with different confidence thresholds becomes very difficult . We therefore followed a standard psychophysical paradigm , namely an identification task with `` forced choice '' in the sense that they had to select a category even if unsure . However , observer confidence is typically correlated with reaction times ( rapid responses for confident decisions ) , and we have thus added median reaction times across experiments as a column in Table 3 of the Appendix . This indeed shows that reaction times are longer for experiments with manipulated images . Guard against human fatigue / lack of interest . We have added the following explanation to Section A.4 of the Appendix : `` Overall , we took the following steps to prevent low quality human data : 1. , using a controlled lab environment instead of an online crowdsourcing platform ; 2. the payment motivation scheme as explained above [ i.e. , better payment for better performance in experiments with a unique ground truth category ] ; 3. displaying observer performance on the screen at regular intervals during the practice session ; and 4. splitting longer experiments into five blocks , where participants could take a break in between blocks . '' Reason for fast image stylization . We have added the following explanation to Section 2.3 : `` We used AdaIN fast style transfer rather than iterative stylization ( e.g.Gatys et al. , 2016 ) for two reasons : Firstly , to ensure that training on SIN and testing on cue conflict stimuli is done using different stylization techniques , such that the results do not rely on a single stylization method . Secondly , to enable processing entire ImageNet , which would take prohibitively long with an iterative approach . '' Explanation for large variations in results across labels . One possible explanation for the large variation in CNN results across labels may be that they use different strategies for different categories , i.e.that they sometimes rely solely on the texture ( e.g.for category bear ) , and sometimes more on other cues . This is supported by the fact that there is a negative Spearman correlation between accuracy in our edge experiment , and texture bias in our cue conflict experiment ( AlexNet : -0.582 , GoogLeNet : -0.508 , VGG-16 : -0.238 , ResNet-50 : -0.014 , human observers : -0.621 ) : if a certain category seems hard to recognize from edges and contours , most networks are more likely to show a stronger class-conditional texture bias . Fine-tuning image features for Pascal VOC . We followed standard best practices for using image features in an object detection setting , which includes fine-tuning the image features . Importantly , this was done for all networks equally ( which are trained on ImageNet and Stylized-ImageNet respectively ) , and the networks were trained under identical circumstances w.r.t.data augmentation . Thus , improved object detection performance can be attributed directly to better generic features induced by Stylized-ImageNet . Again , thank you very much for your review and suggestions !"}, "1": {"review_id": "Bygh9j09KX-1", "review_text": "The paper is well written and easy to follow. It was a nice read for me. The paper studies the CNNs like AlexNet, VGG, GoogleNet, ResNet50 and shows that these models are heavily biased towards the texture when trained on ImageNet. The paper shows human evaluations and compares model accuracies when various transformations like cue hypothesis, texture hypothesis (terms coined in the paper) are applied to study texture vs shape importance. The paper shows various results on different models clearly and results are easily interpretable. The paper then proposed a new ImageNet dataset which is called Stylized-ImageNet (SIN) where the texture is replaced with randomly selected painting style. I believe that this is a good empirical study which is needed to understand why the ImageNet features are good (supervised training) and this can inform research in self-supervision, few shot learning domains. The paper is an empirical paper and is presenting a quantitive study of role of texture which others have already presented like Gatys et al. 2017. The paper itself has no novel contributions. The paper notes \"novel Stylized-ImageNet dataset\" and shows that models can learn shape/texture features both but there is not much detail/explanation on why \"Stylized\" is the novel approach and also the methodology of constructing data by replacing with painting from AdaIN style transfer (Huang & Belongie, 2017) is not discussed/explored. More specifically, there is no ablation on other ways this dataset could have been constructed and why style transfer was picked as the choice, why was AdaIN chosen. While the choice is valid, I think these questions need to be answered if we have to consider it \"novel\". Additionally, I would like answers to the following questions: 1. In Figure 4, ResNet50 results are missing. I would be very interested in seeing those results. Can authors show those results? 2. Did authors study deeper networks like RN101/152 and do the observations about texture still hold? 3. Did authors consider inspecting if the models have same texture biases when trained on other datasets like COCO? If yes, can you share your results? 4. In Figure 5, can authors also show the results of training VGG, AlexNet, GoogleNet models on SIN dataset? I believe otherwise the results are incomplete since Fig. 4 shows the biases of these models on IN dataset but doesn't show if these biases are removed by training on SIN. 5. In Section 3.3, Transfer learning, authors show improvement on VOC 2007 Faster R-CNN . Do authors have explanation on why this gain happens? how's the texture learning in pretext task (like image classification training on SIN dataset) tied to the transfer learning no different dataset? 6. What are the results of transfer learning on other datasets like COCO, Faster R-CNN?", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 3 , Thank you very much for reviewing our paper . Please allow us a quick and important clarification regarding your most important criticism ( `` The paper itself has no novel contributions '' ) as we believe this may be due to a misunderstanding : First , we fully agree with you that one of the datasets we created , Stylized-ImageNet , is in itself not a major contribution - we use an existing fast style transfer method to strip ImageNet images of their original texture to replace it with the uninformative texture of a painting . Stylized-ImageNet is , for us , merely a means to an end , enabling us to make three ( novel ) core contributions : 1 . Quantifying existing texture vs. shape biases . Many of the most influential explanations of CNN object recognition [ 1-3 ] describe it as a process of recognizing parts of objects / object shapes ( the shape hypothesis ) . We contrast this with our carefully collected evidence for the texture hypothesis , offering an entirely different explanation . Furthermore , [ e.g.2,4-5 ] argue that CNNs closely mirror human object recognition and human shape perception . We here provide insights into a core difference of human and machine vision by comparing both under fair circumstances . To the best of our knowledge , our work is the first to systematically pitch shape against texture cues to investigate CNN biases and compare them to the human visual system . 2.Overcoming the texture bias in CNNs . Based on our texture hypothesis , we hypothesized that a CNN texture bias might be changed towards a shape bias if trained on a suitable dataset . We demonstrate the effectiveness of this approach , which shows that the texture bias in standard CNNs is not an inherent property of the architecture but rather induced by the training data . To the best of our knowledge , this is a novel finding and has never been attempted before . ( The method of creating a suitable dataset , AdaIN style transfer , is not new . ) 3.Showing emergent benefits of changed CNN biases . We demonstrate substantial advantages of a shape-based over a texture-based representation in CNNs , most importantly better features for transfer learning ( object detection ) and a previously unmatched robustness against a number of image distortions - despite never being trained on any of them . To the best of our knowledge , ShapeResNet is the first network to approach human-level distortion robustness on distortions that were not part of the training data . We believe that describing Stylized-ImageNet as `` novel '' - which we did in our original manuscript , e.g.in our abstract - was misleading since it is , as mentioned above and pointed out by you in your review , not a substantial contribution and , more importantly , merely a means to achieve our main and truly novel contributions . We will thus rephrase our description of Stylized-ImageNet throughout the paper to avoid any misunderstandings by future readers who might get the idea that the dataset itself is being described as our core novel contribution . Apologies for any confusion this issue may have caused in the original submission . We would appreciate it if you could let us know whether this clarifies the issue , and changes your assessment of the novelty of our work . [ 1 ] Goodfellow , I. , Bengio , Y. , Courville , A. , & Bengio , Y . ( 2016 ) .Deep learning ( Vol.1 ) .Cambridge : MIT press . [ 2 ] Kriegeskorte , N. ( 2015 ) . Deep neural networks : a new framework for modeling biological vision and brain information processing . Annual review of vision science , 1 , 417-446 . [ 3 ] LeCun , Y. , Bengio , Y. , & Hinton , G. ( 2015 ) . Deep learning . Nature , 521 ( 7553 ) , 436 . [ 4 ] Kubilius , J. , Bracci , S. , & de Beeck , H. P. O . ( 2016 ) .Deep neural networks as a computational model for human shape sensitivity . PLoS computational biology , 12 ( 4 ) , e1004896 . [ 5 ] Cadieu , C. F. , Hong , H. , Yamins , D. L. , Pinto , N. , Ardila , D. , Solomon , E. A. , ... & DiCarlo , J. J . ( 2014 ) .Deep neural networks rival the representation of primate IT cortex for core visual object recognition . PLoS computational biology , 10 ( 12 ) , e1003963 ."}, "2": {"review_id": "Bygh9j09KX-2", "review_text": "This paper talks about the behavior bias between human and advanced CNN classifier when classifying objects. A clear conclusion is that DNN classifiers lean on texture cues more than human, which is in contrast to empirical evidence. The experimental results are delighting and convincing to some extent. This paper is also inspiring and potentially useful to interpret how CNN works in object classification task. Nevertheless, I have several small issues: - I like the writing of this paper, fluent description and clear topic. Besides, it provides sufficient information about experiment details, thus I think the experiments are fully reproducible. But I want to remind the authors to downplay their claims. Some sentences are not with academic rigor. i.e. \u201cTextures, not object shapes, are the most important cues for CNN object recognition.\u201dI don\u2019t think it a good idea to claim textures as the \u201cmost important\u201dcue. - Although adequate experiments are conducted on ResNet-50 on ImageNet, I miss experiments on a different object classification dataset i.e. PASCAL VOC, and a different network backbone such as very deep ResNet-152 or wider DenseNet. This lies in the concern that a different (deeper or wider) framework may behave quite differently and also the slightly shifted data distribution may induce controversial results. The adopted network ResNet-50, AlexNet, VGG-16 and GoogLeNet are not deep enough or either wide as DenseNet. Although transfer learning experiment is carried out upon PASCAL VOC, it\u2019s not straightforward and not so truly telling. We\u2019re curious about universal conclusions rather than that based on one dataset or network architecture of the same category. As a matter of fact, I\u2019m nearly convinced by the provided results. But I think the demanding experiments will make the conclusions more solid. Besides, I think the constructed dataset is beneficial to further research or fair comparison of future works, and I wonder the authors\u2019 intention to publish such a dataset in the future. I would raise my scores if the aforementioned problems are convincingly checked and solved.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear Reviewer 2 , Thank you very much for your valuable feedback . We appreciate your assessment of our work as an `` inspiring '' paper . We provide a point-by-point response to your three suggestions below . Writing : claims . We have identified a number of sentences where our excitement about the results has biased our writing . We made the following changes : 1 . ) `` the texture hypothesis : object textures , not object shapes as commonly assumed , are the most important cues for CNN object recognition . '' changed to `` the texture hypothesis : in contrast to the common assumption , object textures are more important than global object shapes for CNN object recognition '' . 2 . ) '' conclusive behavioural evidence '' replaced with `` behavioural evidence '' . 3 . ) In the Discussion , `` we found strong evidence '' replaced with `` we provide evidence '' . 4 . ) In the Discussion , `` we conclude the following : Textures , not object shapes , are the most important cues for CNN object recognition '' replaced with `` this highlights the special role that local cues such as textures seem to play in CNN object recognition '' . 5 . ) In the Summary , `` we showed that machine recognition today primarily relies on texture rather than shape cues '' replaced with `` we provided evidence that machine recognition today overly relies on object textures rather than global object shapes as commonly assumed '' . Control experiments with different networks and data sets . You were expressing concern that deeper or wider networks and training on a different object classification data set may lead to different results . To address your concerns we have collected results for the requested control experiments . In Figure 13 of the Appendix , the left plot shows that the texture bias is equally prominent ( if not stronger ) in a network trained on the Open Images classification data set , thus the texture bias is not specific to training on ImageNet . The right plot of Figure 13 shows that ImageNet-trained networks ResNet-152 ( a very deep network ) , DenseNet-121 ( a very wide network ) and for comparison also Squeezenet1_1 ( a highly compressed network ) all have a strong texture bias . Method details are reported in Section A.5 of the Appendix . Furthermore , we have started to train a ResNet-152 architecture on Stylized-ImageNet to use it as an additional `` deep '' backbone for our object detection experiments . Since network training and fine-tuning takes a lot of time , we will not be able to provide results by the end of the rebuttal period but we will make sure to include them afterwards . We have inquired with the ICLR organizers who have assured us it will be possible to make minor changes after the rebuttal period . Release of data sets . We are determined to release our cue conflict images ( such as the cat with elephant skin ) along with our raw data , image manipulation code , data analysis scripts , psychophysical experiment code and links to trained model weights in a github repository at the end of the anonymous review process . Furthermore , we will release code to create Stylized-ImageNet in a separate github repository along with a docker image ; given two directory paths to ImageNet images ( available from the ImageNet website [ 1 ] ) and to the paintings used as a style source ( available from Kaggle 's painter-by-numbers website [ 2 ] ) a shell script then creates Stylized-ImageNet . Again , thank you very much for reviewing our paper and for your valuable suggestions ! [ 1 ] http : //www.image-net.org/ [ 2 ] https : //www.kaggle.com/c/painter-by-numbers/data"}}