{"year": "2021", "forum": "yr1mzrH3IC", "title": "Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control", "decision": "Accept (Spotlight)", "meta_review": "The reviewers all  appreciated the insights drawn from this study as well at its thoroughness. I want to commend both authors for running additional experiments to strengthen the paper and reviewers for updating the scores accordingly.\n\nCongratulations.", "reviews": [{"review_id": "yr1mzrH3IC-0", "review_text": "This work empirically studies the widely used regularization techniques for training deep neural networks , such as $ L_2 $ / $ L_1 $ regularizer , Batch Normalization ( BN ) , Weight Clip , and Dropout , in policy optimization algorithms ( A2C , SAC , TRPO , PPO ) . The experimental results demonstrate that these Deep Learning ( DL ) regularizations actually can help policy optimization . Pros : 1.The combination of DL regularizations and Reinforcement Learning ( RL ) algorithms seems to be a reasonable and under-explored idea . The motivation is convincing . 2.The authors conducted substantive experiments , which I appreciate . 3.The work is presented clearly , and the paper is well written . Cons : 1.The explanations for why these DL regularizers work or not are hand-waving . As an empirical study paper , I understand theory is not the main focus . But since this paper focused on policy optimization , I expected some insights or explanations from RL perspectives , which are important to guide future research , but they are not provided here ( or I was missing something ) . ( 1a ) The DL regularizers studied in this paper have proved to help training neural networks . As neural networks are used as function approximations in RL , it is as expected sometimes they should have some improvements . ( 1b ) The main reason the authors claimed for why some DL regularizers work is from the generalization perspective , which makes sense in DL . However , for policy optimization , more explanations are needed from the perspectives of learning better agents ( e.g. , exploration vs. exploitation , and better objective landscape ) , which make more sense in RL . An interpretation from an RL perspective is lacking in this paper , which seems necessary since policy optimization is the main topic of this paper . 2.Experimental results are not enough to provide useful conclusions . Since the main focus is on the empirical side , I would expect more on this part , but it seems some conclusions have been made in this paper without sufficient investigations . ( 2a ) The comparison of DL regularizers with entropy regularization actually does not seem reasonable to me . First , the entropy regularization is provable to increase exploration ( see [ 1 ] to the end ) and help convergence in policy optimization ( see [ 2,3 ] ) , which is not claimed to help generalization . Second , DL regularizers help generalization as claimed in the paper . Therefore , they help agent learning in different ways , and I did not see the reason to compare them and what we can conclude from the results . ( 2b ) The conclusion that DL regularizers do not work very well for value function ( comparing with policy optimization ) is lack of support . There is a number of regularizers/tricks of training in value functions ( e.g. , replay buffer , multi-step roll-out , distributional RL , double-Q , etc , see [ 4 ] ) . The authors did not do experiments ( or did not mention ) using those well-known ideas in RL and made this conclusion , which seems hasty to me . ( 2c ) The conclusion and explanation that BN does not work for on-policy methods and works better for off-policy methods seem quite interesting . But also the study here is not enough . There is an amount of RL techniques for off-policy training ( e.g. , corrections , see [ 5 ] ) . I would suggest more investigation and deeper explanation than the discussion of the paper in this direction . Overall , the idea of using DL regularizers in RL seems reasonable and the experimental results look promising . However , the theory part is not solid and insightful , and some of the conclusions are lack support . References : [ 1 ] `` Making sense of reinforcement learning and probabilistic inference '' , O \u2019 Donoghue et al . [ 2 ] `` Understanding the impact of entropy on policy optimization '' , Ahmed et al . [ 3 ] `` On the global convergence rates of softmax policy gradient methods '' , Mei et al . [ 4 ] `` Rainbow : Combining Improvements in Deep Reinforcement Learning '' , Hessel et al . [ 5 ] `` Safe and Efficient Off-Policy Reinforcement Learning '' , Munos et al.Update Thank you for the rebuttal , which resolved most of my concerns . I increased my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> 2a.First , the entropy regularization is provable to increase exploration and help convergence in policy optimization , which is not claimed to help generalization . Second , DL regularizers help generalization as claimed in the paper ... Comparison of DL regularizers with entropy regularization actually does not seem reasonable . We would like to analyze why we believe comparing entropy regularization with DL regularizers is meaningful from the following perspectives : - * * Entropy regularization can arguably be seen as a form of preventing overfitting / helping generalization . * * According to [ 1 ] , entropy regularization \u201c improves exploration by discouraging premature convergence to suboptimal deterministic policies \u201d . Therefore , entropy prevents the policy network from overfitting to certain deterministic actions given certain states , which arguably can be seen as a form of helping generalization . - * * DL regularizers could encourage exploration while helping generalization . * * In Table 5 and Figure 5 , we show that $ L_2 $ regularization not only decreases policy norm , but also increases policy entropy . This suggests that DL regularizers could accomplish the effect of entropy regularization . Since entropy regularization encourages exploration , DL regularizers could also encourage exploration . - * * We empirically observe that entropy regularization and some DL regularizers could have overlapping effects . * * In Appendix N , we observe that using entropy regularization and $ L_2 $ regularization together is at most marginally more effective than using entropy regularization or $ L_2 $ regularization alone , so they might have similar effect throughout the learning process . > 2b.The conclusion that DL regularizers do not work very well for value function ( comparing with policy optimization ) is lack of support . There is a number of regularizers/tricks of training in value functions ( e.g. , replay buffer , multi-step roll-out , distributional RL , double-Q , etc ) . The authors did not do experiments ( or did not mention ) using those well-known ideas in RL and made this conclusion . In our work , we investigate the standard version of A2C , TRPO , PPO , and SAC as described in the original papers and as implemented in OPENAI Baselines [ 2 ] . * * Indeed , SAC adopts the replay buffer and double-Q . A2C , TRPO , and PPO adopt multi-step roll-out , and the sum of discounted rewards is used as the objective for training the value network . * * In Table 4 , we empirically observe that the value function regularization does not perform well on the four algorithms we study , even though they adopted tricks for training value functions . In the updated version , we have clarified this point in Section 5 . Despite that some of these tricks are used in the algorithms we study , analyzing the individual effect of these tricks in training the value function is not the main focus of our current work . We would like to leave the interaction between these individual techniques and value network regularization for future work . > 2c.The conclusion and explanation that BN does not work for on-policy methods and works better for off-policy methods seem quite interesting . But also the study here is not enough . * * In the updated version , we have added experiments on DDPG [ 4 ] , another off-policy algorithm , on MuJoCo . We report the results in Appendix M. * * DDPG uses the technique of `` soft updates '' to encourage training stability when updating the target Q network , rather than directly copying the weights from the current Q network . We obtain similar observations as we did in SAC . Notably , Batch Normalization and Dropout can be useful in DDPG . BN can improve the average $ z $ -score by 0.54 over the baseline , and Dropout can improve the average $ z $ -score by 1.18 . The results are statistically significant . This further supports our hypothesis that they can be helpful on off-policy algorithms . We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions . [ 1 ] Minh et al. , Asynchronous Methods for Deep Reinforcement Learning [ 2 ] https : //github.com/openai/baselines [ 3 ] Bellemare et al. , A Distributional Perspective on Reinforcement Learning [ 4 ] Lillicrap et al. , Continuous Control with Deep Reinforcement Learning"}, {"review_id": "yr1mzrH3IC-1", "review_text": "The paper studies how different regularizations affect common RL algorithms in continuous control tasks . The paper is very well written and organized . The authors clearly state the scope of the paper and its placement with respect to RL literature . Despite having no theoretical novelty , the paper has a good empirical contribution by being the first comprehensive study of the subject . I appreciate the comments and explanations the authors give about the results , and I believe that the findings of this paper can help the RL community in being more aware of the importance of regularization , which is often overlooked . This is crucial especially when it comes to reimplementing existing algorithms without relying too much on handtuned hyperparameters . Overall , I find this paper to be a good empirical study and I am leaning to accept it . However , my major concern is the number of seeds per experiment . As shown by Henderson et al . ( `` Deep Reinforcement Learning that Matters '' ) , 5 seeds are definitely not enough to get accurate statistical results out of RL experiments . I understand that the amount of total experiments is extremely large , given all the different hyperparameters you are testing , but you should have aimed for at least 10 seeds , especially because the paper is based on empirical contributions . As a further suggestion , I invite you to include DDPG , maybe in the final version of the paper . First , because it is another off-policy algorithm ( you have 3 on-policy and 1 off-policy ) . Second , because ( as you also wrote ) DDPG was one of the few algorithms to include BN , while TRPO , PPO and SAC rely on more sophisticated regularizations ( entropy , KL , or surrogate ) . This could lead to very interesting results . Another suggestion ( for future work , as it may be a bit out of scope ) is to test all algorithms on sparse-reward environments . In this case , it is known that common algorithms either prematurely convergence to local optima , or do not learn anything at all . Some of the regularizations ( eg , entropy ) should be beneficial in this case , but an extensive study is still missing . * * EDIT * * The authors have addressed my concernes and I have increased my score .", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank you for your constructive comments . We are encouraged that you found our motivation convincing , our experiments substantive , and our paper well-written . We would like to address the comments and questions below , and we have updated our submission accordingly . > 1.My major concern is the number of seeds per experiment . As shown by Henderson et al . ( `` Deep Reinforcement Learning that Matters '' ) , 5 seeds are definitely not enough to get accurate statistical results out of RL experiments ... you should have aimed for at least 10 seeds * * In our updated version , we increase the number of seeds from 5 to 10 * * and present results for the six MuJoCo environments in Appendix G. We report $ z $ -scores , and we also conduct Welch \u2019 s $ t $ -test . We find that our observations are consistent with those in Section 3 . For example , $ L_2 $ tops the average $ z $ -score most often , and by large margin in total ; entropy regularization is best used with A2C ; Dropout and BN are only useful in the off-policy SAC algorithm ; the improvement over baseline is larger on hard tasks . * * In our updated version , we also provided rigorous justification that , because we test on the entire set of environments instead of on a single environment , our sample size is large enough to satisfy the condition of Welch 's $ t $ -test and provide reliable results . * * We would like to refer to Appendix K and our response ( 1 ) to R2 for more details . > 2.As a further suggestion , I invite you to include DDPG , maybe in the final version of the paper . * * In our updated version , we have conducted experiments on DDPG [ 1 ] , another off-policy algorithm , on MuJoCo . We report the results in Appendix M. * * For baseline experiments , We take away the regularizations originally implemented in DDPG . We obtain similar observations as we did in SAC . Notably , Dropout and Batch Normalization can be useful in DDPG , as indicated by the higher average $ z $ -score than the baseline , which supports our hypothesis that they can be helpful on off-policy algorithms . Also , while the original DDPG paper uses BN , we find that Dropout is even more effective than BN . > 3.Another suggestion ( for future work , as it may be a bit out of scope ) is to test all algorithms on sparse-reward environments . We thank the reviewer for pointing out this interesting research direction , and we look forward to pursuing this goal in future work . For example , experimenting our studied algorithms on the SparseAnt and SparseHumanoid environments . We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions ."}, {"review_id": "yr1mzrH3IC-2", "review_text": "* * Pros * * The paper investigates a worthwhile question . Given the prevalence and success of regularization in deep learning , it is curious why we have n't consistently observed the same in deep RL . * * Cons * * My main concern with the work , especially since it is an empirical study , concerned the lack of statistical significance in the results . Here are the problems I found . 1.Welch 's t-test requires normality assumption , and I 'm not sure why returns would be normally distributed ; in any case , no verification of this assumption was attempted . 2.Even if the normality issue is ignored , low p-values in the t-test suggests that the means are different , but does n't necessarily say anything about the direction or magnitude of difference . It could be that regularisation worsens performance , for all we know . 3.The learning curves are uninformative for determining the direction and magnitude of any performance difference , given the high variance of RL and the number of seeds used ( 5 seeds ) . Indeed , in the Henderson et al.work cited , Figure 5 shows how a small number of random seeds can be misleading ( they even show that for the same algorithm and different seeds , a t-test can give a small p-value ) . 4.Similar concerns apply for ranking metrics , given the small sample size . * * Summary * * Based on the concerns with statistical significance I have highlighted above , I recommend rejection of this paper . The results presented in the paper are certainly suggestive , but do not I think meet the level of rigour required at ICLR . I do very much like the motivation of the study , and would have liked to have seen results of greater statistical significance . I recognize that the results presented in the text already took 57 days on a substantial computational setup , but it worries me that we are using massive computational power to little avail if we can not produce reliable results . It would perhaps be better to focus on a smaller subset of algorithms and environments in order to guarantee reliable results . * * Edit after author response * * I feel that the authors have sufficiently addressed the statistical significance concerns I had in the comments below . I now recommend acceptance because I think the work provides strong evidence that regularization is beneficial in continuous control . Although , as other reviewers have pointed out , further analysis of why regularization is beneficial -- especially from a theoretical standpoint -- would be helpful , I think the empirical contribution of the paper still stands .", "rating": "7: Good paper, accept", "reply_text": "( Continued for 3a ) * * In our updated version , we increase the number of seeds from 5 to 10 * * and present results for the six MuJoCo environments ( easy : Hopper , Walker , HalfCheetah ; hard : Ant , Humanoid , HumanoidStandup ) in Appendix G to further verify our observations . Due to the large computation cost required , we do not include the three hard Roboschool environments . We report $ z $ -scores , and we also conduct Welch \u2019 s $ t $ -test ( for each algorithm , \u201c easy \u201d : 3 envs * 10 seeds = 30 samples ; \u201c hard \u201d : 3 * 10 = 30 samples ; \u201c total \u201d : 30 + 30 = 60 samples ; for the last three columns on statistics over all algorithms , \u201c easy \u201d : 30 * 4 = 120 ; \u201c hard \u201d : 30 * 4 = 120 ; \u201c total \u201d : 120 + 120 = 240 ; thus the sample size is large enough for $ t $ -test ) . * * We find that our observations in Section 3 still hold , i.e.our observations from 5 seeds per environment are consistent with those from 10 seeds per environment . * * For example , $ L_2 $ tops the average $ z $ -score most often , and by large margin in total ; entropy regularization is best used with A2C ; Dropout and BN are only useful in the off-policy SAC algorithm ; the improvement over baseline is larger on hard tasks . > 3b.In the Henderson et al.work cited , Figure 5 shows how a small number of random seeds can be misleading ( they even show that for the same algorithm and different seeds , a t-test can give a small p-value ) . * * The Figure 5 of Henderson et al . [ 4 ] tests a set of 5 random seeds vs another set of 5 seeds on the single environment of HalfCheetah . We would like to clarify that our $ t $ -test does not test whether a regularizer significantly improves over the baseline under one single environment , but under all environments . Thus , our sample size satisfies the condition for statistical tests . More details are presented in our response to the first concern . * * * * We further discuss the HalfCheetah environment used in Figure 5 in Henderson et al . * * , which shows that , under the same hyperparameter configuration , two sets of 5 different runs on the HalfCheetah environment can be significantly different from each other . * * We find that the unique environment property of HalfCheetah contributes to such observation . * * For A2C , PPO , and TRPO on HalfCheetah , there is a certain probability that the policy found is suboptimal , where the half cheetah robot runs upside-down using its head . In this case , the final return never rises above 2200 . In other cases , the half cheetah robot runs using its legs , and the final return is almost always above 4000 . Therefore , it is possible that in a set of 5 runs , 4 of the runs have final returns above 4000 , while for another set of 5 runs , 4 of the runs have final returns below 2200 . This causes a significant performance difference between the two sets of runs . However , for all other environments , the final return is approximately normally distributed with respect to seeds , instead of categorically distributed like HalfCheetah . The variance on the other environments is much smaller than that of HalfCheetah . For example , according to Table 3 of Henderson et al. , PPO Walker 's 95 % confidence interval for the final return has a range of 800 , while HalfCheetah has a range of 2200 . * * Empirically , we found that other environments do not yield as much fluctuations as HalfCheetah . * * > 4.Similar concerns apply for ranking metrics , given the small sample size . * * Since we test on all environments instead of on a single environment , the sample size is large enough to satisfy the condition for statistical significance tests . * * We have conducted $ t $ -test on the ranking metrics in Table 24 and 25 . We would like to note that * * the ranking metric is only one of the metrics we evaluate in our paper * * . We have also evaluated improvement percentage metric ( Table 1,4,19 ) , $ z $ -score ( Table 2 , 3 ) , and scaled returns ( Table 26 , 29 ) . We believe evaluating under a variety of metrics make our conclusions more reliable . > 5.Based on the concerns with statistical significance I have highlighted above ... can not produce reliable results To summarize our response , - * * We believe that our results are reliable , because we evaluate our results on multiple performance metrics , and the condition for significance testing is satisfied . * * - * * To further address your concerns , in our updated version , we report the results on MuJoCo when we increase the number of training seeds from 5 to 10 , and we find that our observations are consistent . * * -- We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions . [ 1 ] Location Test . https : //en.wikipedia.org/wiki/Location_test [ 2 ] Statistics/Testing Data/t-tests . https : //en.wikibooks.org/wiki/Statistics/Testing_Data/t-tests [ 3 ] Slutsky 's Theorem . https : //en.wikipedia.org/wiki/Slutsky % 27s_theorem [ 4 ] Henderson et al. , Deep Reinforcement Learning that Matters"}, {"review_id": "yr1mzrH3IC-3", "review_text": "This paper conducts a comprehensive study on the effect of different regularization on Deep RL algorithms . Regularization has been mostly neglected in RL as most benefits were believed to be in generalization to unseen test environments in supervised learning settings . However , this paper shows that regularization does provide benefit even though training/testing is done on the same environment in deep RL settings . The paper studies L1/L2 regularization , dropout , weight clipping , and batch normalization on four different deep RL policy optimization algorithms . Results show that regularization improves performance especially L2 , that regularization brings more benefit on harder tasks that have higher sample complexity , and that it makes algorithms more robust to training hyperparameter variations . The paper conducts rigorous statistical significance test , and analyzes the benefit of regularization through four ways : sample complexity , reward distribution , weight norm , and training noise robustness . Overall , I think the paper is well-written giving a comprehensive evaluation on a widely neglected area in reinforcement learning . Many different RL algorithm implementations have used regularization with and without acknowledging its use in the paper , and this paper sheds light that using regularization in deep RL algorithms does have significant impact and warrants further study . I wish the paper conducted more than 5 runs ( it is shown that mujoco environments have high variance due to random seeds and that same alg.performs significantly differently based on different groups of random seeds -- see Henderson et al.2018 ( https : //arxiv.org/pdf/1709.06560.pdf ) ) , but the authors also perform significance testing to validate the performance improvements . I think the paper 's analysis section can be further improved . For example , the bar chart in Figure 3 does not indicate how many runs it has done , or show any error bars to show statistical significance . And I think 'return ' would be the correct terminology instead of 'reward ' to indicate cumulative sum of rewards ( For Figure 2 , 3 , and Table 5 ) Also , I think the claim that 'BN and dropout work only with off-policy algorithm ' , or 'BN and dropout can only help in off-policy algorithm ' is quite strong . Only a single off-policy algorithm SAC has been tested , and although BN and dropout helped a lot , its improvement may have been due to algorithm-specific properties of SAC . The authors hypothesize plausible reasons , but I think the findings only show that BN and dropout does not work well for on-policy algorithms . There is a minor typo in page 4 : decompled - > decoupled", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank you for your constructive comments . We are encouraged that you found our statistical significance test rigorous , our evaluation comprehensive , and our paper well-written . We would like to address the comments and questions below , and we have updated our submission accordingly . > 1.I wish the paper conducted more than 5 runs ( it is shown that mujoco environments have high variance due to random seeds and that same alg.performs significantly differently based on different groups of random seeds -- see Henderson et al.2018 . * * In our updated version , we increase the number of seeds from 5 to 10 * * and present results for the six MuJoCo environments in Appendix G to further verify our observations . We report $ z $ -scores , and we also conduct Welch \u2019 s $ t $ -test . We find that our observations in Section 3 still hold . For example , $ L_2 $ tops the average $ z $ -score most often , and by large margin in total ; entropy regularization is best used with A2C ; Dropout and BN are only useful in the off-policy SAC algorithm ; the improvement over baseline is larger on hard tasks . In our updated version , we also provided rigorous justification that , because we test on the entire set of environments instead of on a single environment , our sample size is large enough to satisfy the condition of Welch 's $ t $ -test and provide reliable results . We would like to refer to Appendix K and our response ( 1 ) to R2 for more details . In addition , we address that the unique environment property of HalfCheetah contributes to Henderson et al . 's observation that the same algorithm performs significantly using different sets of random seeds . We would like to refer to Appendix K and our response ( 3b ) to R2 for more details . > 2.The bar chart in Figure 3 does not indicate how many runs it has done , or show any error bars to show statistical significance . * * We have updated Figure 3 to include error bars from our increased 10 random seeds . * * For some environments , we find that adding proper regularization can significantly reduce the error bars ( SAC Ant , PPO Humanoid ) . > 3 . 'Return ' would be the correct terminology instead of 'reward ' to indicate cumulative sum of rewards . We have revised the expressions accordingly . > 4.Only a single off-policy algorithm SAC has been tested , and although BN and dropout helped a lot , its improvement may have been due to algorithm-specific properties of SAC . * * In the updated version , we have added experiments on DDPG [ 1 ] , another off-policy algorithm , on MuJoCo . We report the results in Appendix M. * * We obtain similar observations as we did in SAC . Notably , Dropout and Batch Normalization can be useful in DDPG , as indicated by the higher average $ z $ -score than the baseline , which supports our hypothesis that they can be helpful on off-policy algorithms . > 5.Typo on Page 4 We have corrected the typo accordingly . We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions . [ 1 ] Lillicrap et al. , Continuous Control with Deep Reinforcement Learning"}], "0": {"review_id": "yr1mzrH3IC-0", "review_text": "This work empirically studies the widely used regularization techniques for training deep neural networks , such as $ L_2 $ / $ L_1 $ regularizer , Batch Normalization ( BN ) , Weight Clip , and Dropout , in policy optimization algorithms ( A2C , SAC , TRPO , PPO ) . The experimental results demonstrate that these Deep Learning ( DL ) regularizations actually can help policy optimization . Pros : 1.The combination of DL regularizations and Reinforcement Learning ( RL ) algorithms seems to be a reasonable and under-explored idea . The motivation is convincing . 2.The authors conducted substantive experiments , which I appreciate . 3.The work is presented clearly , and the paper is well written . Cons : 1.The explanations for why these DL regularizers work or not are hand-waving . As an empirical study paper , I understand theory is not the main focus . But since this paper focused on policy optimization , I expected some insights or explanations from RL perspectives , which are important to guide future research , but they are not provided here ( or I was missing something ) . ( 1a ) The DL regularizers studied in this paper have proved to help training neural networks . As neural networks are used as function approximations in RL , it is as expected sometimes they should have some improvements . ( 1b ) The main reason the authors claimed for why some DL regularizers work is from the generalization perspective , which makes sense in DL . However , for policy optimization , more explanations are needed from the perspectives of learning better agents ( e.g. , exploration vs. exploitation , and better objective landscape ) , which make more sense in RL . An interpretation from an RL perspective is lacking in this paper , which seems necessary since policy optimization is the main topic of this paper . 2.Experimental results are not enough to provide useful conclusions . Since the main focus is on the empirical side , I would expect more on this part , but it seems some conclusions have been made in this paper without sufficient investigations . ( 2a ) The comparison of DL regularizers with entropy regularization actually does not seem reasonable to me . First , the entropy regularization is provable to increase exploration ( see [ 1 ] to the end ) and help convergence in policy optimization ( see [ 2,3 ] ) , which is not claimed to help generalization . Second , DL regularizers help generalization as claimed in the paper . Therefore , they help agent learning in different ways , and I did not see the reason to compare them and what we can conclude from the results . ( 2b ) The conclusion that DL regularizers do not work very well for value function ( comparing with policy optimization ) is lack of support . There is a number of regularizers/tricks of training in value functions ( e.g. , replay buffer , multi-step roll-out , distributional RL , double-Q , etc , see [ 4 ] ) . The authors did not do experiments ( or did not mention ) using those well-known ideas in RL and made this conclusion , which seems hasty to me . ( 2c ) The conclusion and explanation that BN does not work for on-policy methods and works better for off-policy methods seem quite interesting . But also the study here is not enough . There is an amount of RL techniques for off-policy training ( e.g. , corrections , see [ 5 ] ) . I would suggest more investigation and deeper explanation than the discussion of the paper in this direction . Overall , the idea of using DL regularizers in RL seems reasonable and the experimental results look promising . However , the theory part is not solid and insightful , and some of the conclusions are lack support . References : [ 1 ] `` Making sense of reinforcement learning and probabilistic inference '' , O \u2019 Donoghue et al . [ 2 ] `` Understanding the impact of entropy on policy optimization '' , Ahmed et al . [ 3 ] `` On the global convergence rates of softmax policy gradient methods '' , Mei et al . [ 4 ] `` Rainbow : Combining Improvements in Deep Reinforcement Learning '' , Hessel et al . [ 5 ] `` Safe and Efficient Off-Policy Reinforcement Learning '' , Munos et al.Update Thank you for the rebuttal , which resolved most of my concerns . I increased my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> 2a.First , the entropy regularization is provable to increase exploration and help convergence in policy optimization , which is not claimed to help generalization . Second , DL regularizers help generalization as claimed in the paper ... Comparison of DL regularizers with entropy regularization actually does not seem reasonable . We would like to analyze why we believe comparing entropy regularization with DL regularizers is meaningful from the following perspectives : - * * Entropy regularization can arguably be seen as a form of preventing overfitting / helping generalization . * * According to [ 1 ] , entropy regularization \u201c improves exploration by discouraging premature convergence to suboptimal deterministic policies \u201d . Therefore , entropy prevents the policy network from overfitting to certain deterministic actions given certain states , which arguably can be seen as a form of helping generalization . - * * DL regularizers could encourage exploration while helping generalization . * * In Table 5 and Figure 5 , we show that $ L_2 $ regularization not only decreases policy norm , but also increases policy entropy . This suggests that DL regularizers could accomplish the effect of entropy regularization . Since entropy regularization encourages exploration , DL regularizers could also encourage exploration . - * * We empirically observe that entropy regularization and some DL regularizers could have overlapping effects . * * In Appendix N , we observe that using entropy regularization and $ L_2 $ regularization together is at most marginally more effective than using entropy regularization or $ L_2 $ regularization alone , so they might have similar effect throughout the learning process . > 2b.The conclusion that DL regularizers do not work very well for value function ( comparing with policy optimization ) is lack of support . There is a number of regularizers/tricks of training in value functions ( e.g. , replay buffer , multi-step roll-out , distributional RL , double-Q , etc ) . The authors did not do experiments ( or did not mention ) using those well-known ideas in RL and made this conclusion . In our work , we investigate the standard version of A2C , TRPO , PPO , and SAC as described in the original papers and as implemented in OPENAI Baselines [ 2 ] . * * Indeed , SAC adopts the replay buffer and double-Q . A2C , TRPO , and PPO adopt multi-step roll-out , and the sum of discounted rewards is used as the objective for training the value network . * * In Table 4 , we empirically observe that the value function regularization does not perform well on the four algorithms we study , even though they adopted tricks for training value functions . In the updated version , we have clarified this point in Section 5 . Despite that some of these tricks are used in the algorithms we study , analyzing the individual effect of these tricks in training the value function is not the main focus of our current work . We would like to leave the interaction between these individual techniques and value network regularization for future work . > 2c.The conclusion and explanation that BN does not work for on-policy methods and works better for off-policy methods seem quite interesting . But also the study here is not enough . * * In the updated version , we have added experiments on DDPG [ 4 ] , another off-policy algorithm , on MuJoCo . We report the results in Appendix M. * * DDPG uses the technique of `` soft updates '' to encourage training stability when updating the target Q network , rather than directly copying the weights from the current Q network . We obtain similar observations as we did in SAC . Notably , Batch Normalization and Dropout can be useful in DDPG . BN can improve the average $ z $ -score by 0.54 over the baseline , and Dropout can improve the average $ z $ -score by 1.18 . The results are statistically significant . This further supports our hypothesis that they can be helpful on off-policy algorithms . We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions . [ 1 ] Minh et al. , Asynchronous Methods for Deep Reinforcement Learning [ 2 ] https : //github.com/openai/baselines [ 3 ] Bellemare et al. , A Distributional Perspective on Reinforcement Learning [ 4 ] Lillicrap et al. , Continuous Control with Deep Reinforcement Learning"}, "1": {"review_id": "yr1mzrH3IC-1", "review_text": "The paper studies how different regularizations affect common RL algorithms in continuous control tasks . The paper is very well written and organized . The authors clearly state the scope of the paper and its placement with respect to RL literature . Despite having no theoretical novelty , the paper has a good empirical contribution by being the first comprehensive study of the subject . I appreciate the comments and explanations the authors give about the results , and I believe that the findings of this paper can help the RL community in being more aware of the importance of regularization , which is often overlooked . This is crucial especially when it comes to reimplementing existing algorithms without relying too much on handtuned hyperparameters . Overall , I find this paper to be a good empirical study and I am leaning to accept it . However , my major concern is the number of seeds per experiment . As shown by Henderson et al . ( `` Deep Reinforcement Learning that Matters '' ) , 5 seeds are definitely not enough to get accurate statistical results out of RL experiments . I understand that the amount of total experiments is extremely large , given all the different hyperparameters you are testing , but you should have aimed for at least 10 seeds , especially because the paper is based on empirical contributions . As a further suggestion , I invite you to include DDPG , maybe in the final version of the paper . First , because it is another off-policy algorithm ( you have 3 on-policy and 1 off-policy ) . Second , because ( as you also wrote ) DDPG was one of the few algorithms to include BN , while TRPO , PPO and SAC rely on more sophisticated regularizations ( entropy , KL , or surrogate ) . This could lead to very interesting results . Another suggestion ( for future work , as it may be a bit out of scope ) is to test all algorithms on sparse-reward environments . In this case , it is known that common algorithms either prematurely convergence to local optima , or do not learn anything at all . Some of the regularizations ( eg , entropy ) should be beneficial in this case , but an extensive study is still missing . * * EDIT * * The authors have addressed my concernes and I have increased my score .", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank you for your constructive comments . We are encouraged that you found our motivation convincing , our experiments substantive , and our paper well-written . We would like to address the comments and questions below , and we have updated our submission accordingly . > 1.My major concern is the number of seeds per experiment . As shown by Henderson et al . ( `` Deep Reinforcement Learning that Matters '' ) , 5 seeds are definitely not enough to get accurate statistical results out of RL experiments ... you should have aimed for at least 10 seeds * * In our updated version , we increase the number of seeds from 5 to 10 * * and present results for the six MuJoCo environments in Appendix G. We report $ z $ -scores , and we also conduct Welch \u2019 s $ t $ -test . We find that our observations are consistent with those in Section 3 . For example , $ L_2 $ tops the average $ z $ -score most often , and by large margin in total ; entropy regularization is best used with A2C ; Dropout and BN are only useful in the off-policy SAC algorithm ; the improvement over baseline is larger on hard tasks . * * In our updated version , we also provided rigorous justification that , because we test on the entire set of environments instead of on a single environment , our sample size is large enough to satisfy the condition of Welch 's $ t $ -test and provide reliable results . * * We would like to refer to Appendix K and our response ( 1 ) to R2 for more details . > 2.As a further suggestion , I invite you to include DDPG , maybe in the final version of the paper . * * In our updated version , we have conducted experiments on DDPG [ 1 ] , another off-policy algorithm , on MuJoCo . We report the results in Appendix M. * * For baseline experiments , We take away the regularizations originally implemented in DDPG . We obtain similar observations as we did in SAC . Notably , Dropout and Batch Normalization can be useful in DDPG , as indicated by the higher average $ z $ -score than the baseline , which supports our hypothesis that they can be helpful on off-policy algorithms . Also , while the original DDPG paper uses BN , we find that Dropout is even more effective than BN . > 3.Another suggestion ( for future work , as it may be a bit out of scope ) is to test all algorithms on sparse-reward environments . We thank the reviewer for pointing out this interesting research direction , and we look forward to pursuing this goal in future work . For example , experimenting our studied algorithms on the SparseAnt and SparseHumanoid environments . We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions ."}, "2": {"review_id": "yr1mzrH3IC-2", "review_text": "* * Pros * * The paper investigates a worthwhile question . Given the prevalence and success of regularization in deep learning , it is curious why we have n't consistently observed the same in deep RL . * * Cons * * My main concern with the work , especially since it is an empirical study , concerned the lack of statistical significance in the results . Here are the problems I found . 1.Welch 's t-test requires normality assumption , and I 'm not sure why returns would be normally distributed ; in any case , no verification of this assumption was attempted . 2.Even if the normality issue is ignored , low p-values in the t-test suggests that the means are different , but does n't necessarily say anything about the direction or magnitude of difference . It could be that regularisation worsens performance , for all we know . 3.The learning curves are uninformative for determining the direction and magnitude of any performance difference , given the high variance of RL and the number of seeds used ( 5 seeds ) . Indeed , in the Henderson et al.work cited , Figure 5 shows how a small number of random seeds can be misleading ( they even show that for the same algorithm and different seeds , a t-test can give a small p-value ) . 4.Similar concerns apply for ranking metrics , given the small sample size . * * Summary * * Based on the concerns with statistical significance I have highlighted above , I recommend rejection of this paper . The results presented in the paper are certainly suggestive , but do not I think meet the level of rigour required at ICLR . I do very much like the motivation of the study , and would have liked to have seen results of greater statistical significance . I recognize that the results presented in the text already took 57 days on a substantial computational setup , but it worries me that we are using massive computational power to little avail if we can not produce reliable results . It would perhaps be better to focus on a smaller subset of algorithms and environments in order to guarantee reliable results . * * Edit after author response * * I feel that the authors have sufficiently addressed the statistical significance concerns I had in the comments below . I now recommend acceptance because I think the work provides strong evidence that regularization is beneficial in continuous control . Although , as other reviewers have pointed out , further analysis of why regularization is beneficial -- especially from a theoretical standpoint -- would be helpful , I think the empirical contribution of the paper still stands .", "rating": "7: Good paper, accept", "reply_text": "( Continued for 3a ) * * In our updated version , we increase the number of seeds from 5 to 10 * * and present results for the six MuJoCo environments ( easy : Hopper , Walker , HalfCheetah ; hard : Ant , Humanoid , HumanoidStandup ) in Appendix G to further verify our observations . Due to the large computation cost required , we do not include the three hard Roboschool environments . We report $ z $ -scores , and we also conduct Welch \u2019 s $ t $ -test ( for each algorithm , \u201c easy \u201d : 3 envs * 10 seeds = 30 samples ; \u201c hard \u201d : 3 * 10 = 30 samples ; \u201c total \u201d : 30 + 30 = 60 samples ; for the last three columns on statistics over all algorithms , \u201c easy \u201d : 30 * 4 = 120 ; \u201c hard \u201d : 30 * 4 = 120 ; \u201c total \u201d : 120 + 120 = 240 ; thus the sample size is large enough for $ t $ -test ) . * * We find that our observations in Section 3 still hold , i.e.our observations from 5 seeds per environment are consistent with those from 10 seeds per environment . * * For example , $ L_2 $ tops the average $ z $ -score most often , and by large margin in total ; entropy regularization is best used with A2C ; Dropout and BN are only useful in the off-policy SAC algorithm ; the improvement over baseline is larger on hard tasks . > 3b.In the Henderson et al.work cited , Figure 5 shows how a small number of random seeds can be misleading ( they even show that for the same algorithm and different seeds , a t-test can give a small p-value ) . * * The Figure 5 of Henderson et al . [ 4 ] tests a set of 5 random seeds vs another set of 5 seeds on the single environment of HalfCheetah . We would like to clarify that our $ t $ -test does not test whether a regularizer significantly improves over the baseline under one single environment , but under all environments . Thus , our sample size satisfies the condition for statistical tests . More details are presented in our response to the first concern . * * * * We further discuss the HalfCheetah environment used in Figure 5 in Henderson et al . * * , which shows that , under the same hyperparameter configuration , two sets of 5 different runs on the HalfCheetah environment can be significantly different from each other . * * We find that the unique environment property of HalfCheetah contributes to such observation . * * For A2C , PPO , and TRPO on HalfCheetah , there is a certain probability that the policy found is suboptimal , where the half cheetah robot runs upside-down using its head . In this case , the final return never rises above 2200 . In other cases , the half cheetah robot runs using its legs , and the final return is almost always above 4000 . Therefore , it is possible that in a set of 5 runs , 4 of the runs have final returns above 4000 , while for another set of 5 runs , 4 of the runs have final returns below 2200 . This causes a significant performance difference between the two sets of runs . However , for all other environments , the final return is approximately normally distributed with respect to seeds , instead of categorically distributed like HalfCheetah . The variance on the other environments is much smaller than that of HalfCheetah . For example , according to Table 3 of Henderson et al. , PPO Walker 's 95 % confidence interval for the final return has a range of 800 , while HalfCheetah has a range of 2200 . * * Empirically , we found that other environments do not yield as much fluctuations as HalfCheetah . * * > 4.Similar concerns apply for ranking metrics , given the small sample size . * * Since we test on all environments instead of on a single environment , the sample size is large enough to satisfy the condition for statistical significance tests . * * We have conducted $ t $ -test on the ranking metrics in Table 24 and 25 . We would like to note that * * the ranking metric is only one of the metrics we evaluate in our paper * * . We have also evaluated improvement percentage metric ( Table 1,4,19 ) , $ z $ -score ( Table 2 , 3 ) , and scaled returns ( Table 26 , 29 ) . We believe evaluating under a variety of metrics make our conclusions more reliable . > 5.Based on the concerns with statistical significance I have highlighted above ... can not produce reliable results To summarize our response , - * * We believe that our results are reliable , because we evaluate our results on multiple performance metrics , and the condition for significance testing is satisfied . * * - * * To further address your concerns , in our updated version , we report the results on MuJoCo when we increase the number of training seeds from 5 to 10 , and we find that our observations are consistent . * * -- We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions . [ 1 ] Location Test . https : //en.wikipedia.org/wiki/Location_test [ 2 ] Statistics/Testing Data/t-tests . https : //en.wikibooks.org/wiki/Statistics/Testing_Data/t-tests [ 3 ] Slutsky 's Theorem . https : //en.wikipedia.org/wiki/Slutsky % 27s_theorem [ 4 ] Henderson et al. , Deep Reinforcement Learning that Matters"}, "3": {"review_id": "yr1mzrH3IC-3", "review_text": "This paper conducts a comprehensive study on the effect of different regularization on Deep RL algorithms . Regularization has been mostly neglected in RL as most benefits were believed to be in generalization to unseen test environments in supervised learning settings . However , this paper shows that regularization does provide benefit even though training/testing is done on the same environment in deep RL settings . The paper studies L1/L2 regularization , dropout , weight clipping , and batch normalization on four different deep RL policy optimization algorithms . Results show that regularization improves performance especially L2 , that regularization brings more benefit on harder tasks that have higher sample complexity , and that it makes algorithms more robust to training hyperparameter variations . The paper conducts rigorous statistical significance test , and analyzes the benefit of regularization through four ways : sample complexity , reward distribution , weight norm , and training noise robustness . Overall , I think the paper is well-written giving a comprehensive evaluation on a widely neglected area in reinforcement learning . Many different RL algorithm implementations have used regularization with and without acknowledging its use in the paper , and this paper sheds light that using regularization in deep RL algorithms does have significant impact and warrants further study . I wish the paper conducted more than 5 runs ( it is shown that mujoco environments have high variance due to random seeds and that same alg.performs significantly differently based on different groups of random seeds -- see Henderson et al.2018 ( https : //arxiv.org/pdf/1709.06560.pdf ) ) , but the authors also perform significance testing to validate the performance improvements . I think the paper 's analysis section can be further improved . For example , the bar chart in Figure 3 does not indicate how many runs it has done , or show any error bars to show statistical significance . And I think 'return ' would be the correct terminology instead of 'reward ' to indicate cumulative sum of rewards ( For Figure 2 , 3 , and Table 5 ) Also , I think the claim that 'BN and dropout work only with off-policy algorithm ' , or 'BN and dropout can only help in off-policy algorithm ' is quite strong . Only a single off-policy algorithm SAC has been tested , and although BN and dropout helped a lot , its improvement may have been due to algorithm-specific properties of SAC . The authors hypothesize plausible reasons , but I think the findings only show that BN and dropout does not work well for on-policy algorithms . There is a minor typo in page 4 : decompled - > decoupled", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank you for your constructive comments . We are encouraged that you found our statistical significance test rigorous , our evaluation comprehensive , and our paper well-written . We would like to address the comments and questions below , and we have updated our submission accordingly . > 1.I wish the paper conducted more than 5 runs ( it is shown that mujoco environments have high variance due to random seeds and that same alg.performs significantly differently based on different groups of random seeds -- see Henderson et al.2018 . * * In our updated version , we increase the number of seeds from 5 to 10 * * and present results for the six MuJoCo environments in Appendix G to further verify our observations . We report $ z $ -scores , and we also conduct Welch \u2019 s $ t $ -test . We find that our observations in Section 3 still hold . For example , $ L_2 $ tops the average $ z $ -score most often , and by large margin in total ; entropy regularization is best used with A2C ; Dropout and BN are only useful in the off-policy SAC algorithm ; the improvement over baseline is larger on hard tasks . In our updated version , we also provided rigorous justification that , because we test on the entire set of environments instead of on a single environment , our sample size is large enough to satisfy the condition of Welch 's $ t $ -test and provide reliable results . We would like to refer to Appendix K and our response ( 1 ) to R2 for more details . In addition , we address that the unique environment property of HalfCheetah contributes to Henderson et al . 's observation that the same algorithm performs significantly using different sets of random seeds . We would like to refer to Appendix K and our response ( 3b ) to R2 for more details . > 2.The bar chart in Figure 3 does not indicate how many runs it has done , or show any error bars to show statistical significance . * * We have updated Figure 3 to include error bars from our increased 10 random seeds . * * For some environments , we find that adding proper regularization can significantly reduce the error bars ( SAC Ant , PPO Humanoid ) . > 3 . 'Return ' would be the correct terminology instead of 'reward ' to indicate cumulative sum of rewards . We have revised the expressions accordingly . > 4.Only a single off-policy algorithm SAC has been tested , and although BN and dropout helped a lot , its improvement may have been due to algorithm-specific properties of SAC . * * In the updated version , we have added experiments on DDPG [ 1 ] , another off-policy algorithm , on MuJoCo . We report the results in Appendix M. * * We obtain similar observations as we did in SAC . Notably , Dropout and Batch Normalization can be useful in DDPG , as indicated by the higher average $ z $ -score than the baseline , which supports our hypothesis that they can be helpful on off-policy algorithms . > 5.Typo on Page 4 We have corrected the typo accordingly . We hope our response could address your concern , and we thank you again for the helpful comments . We are glad to discuss further comments and suggestions . [ 1 ] Lillicrap et al. , Continuous Control with Deep Reinforcement Learning"}}