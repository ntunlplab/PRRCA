{"year": "2020", "forum": "rJxbJeHFPS", "title": "What Can Neural Networks Reason About?", "decision": "Accept (Spotlight)", "meta_review": "This paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks. From this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming.\n\nThis substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance. My view is that this is exactly the sort of work we should be show-casing at the conference, both in terms of focus, and of quality. I am happy to recommend this for acceptance.", "reviews": [{"review_id": "rJxbJeHFPS-0", "review_text": "This paper presents a framework, dubbed algorithmic alignment, based on PAC learning and sample complexity, with the aim to explain generalization on reasoning tasks for different neural architectures. The framework roughly states that in order for the model to be able to learn and successfully generalize on a reasoning task, it needs to be able to easily learn (to approximate) steps of the reasoning tasks. The authors use this framework to propose an increasingly difficult set of tasks, designed to showcase the type of models that would be fit or unfit to solve them. The resulting experiments corroborate the theory, showing the limits of MLPs, Deep Sets, and consequently Graph Neural Networks. The final claim that an NP-hard task needs an enumerative architecture, and then experimental validation of that claim is nice and fits into the theory. The added benefit of the paper is that the authors show as a side-effect that visual question answering and intuitive physics Overall, the paper presents a meaningful contribution to the theory of learning, formalizing the means of quantifying the capabilities of architectures to solve tasks of certain complexity. The paper, though dense, is well well written, and carries an interesting conclusion that better algorithmic alignment brings the sample complexity down, i.e. models with better algorithmic alignment to the task (function they want to approximate) should generalize better. The formalization presented in the paper, though remarkably intuitive, might be difficult to practically use for more elaborate models and it is not clear whether it can be numerically computed. The paper (i.e. the reader) would certainly benefit from more examples of algorithmic alignment comparison of different models, such as one done in Corollary 3.7. Question: - difference to Kolmogorov complexity is that any algorithmic alignment that yields decent sample complexity is good enough - how do you define decent? - You state: \u201cin Section 4, we will show that we can usually derive a near-optimal alignment by avoiding as many \u201cfor loops\u201d in algorithm steps as possible.\u201d yet I did not see that there. Was that effectively shown in Corollary 3.7? Slightly related work: On the Turing Completeness of Modern Neural Network Architectures", "rating": "8: Accept", "reply_text": "Thank you for your helpful feedback . We answer your questions below . - \u201c difference to Kolmogorov complexity is that any algorithmic alignment that yields decent sample complexity is good enough - how do you define decent ? \u201d . Here , \u201c decent \u201d is a loose term we use to refer to a tight enough algorithmic alignment for good generalization performance . We will explain more in the revised version . - \u201c You state : \u2018 in Section 4 , we will show that we can usually derive a near-optimal alignment by avoiding as many \u2018 for loops \u2019 in algorithm steps as possible. \u2019 yet I did not see that there \u201d . One example is Section 4.2 : DeepSets does not algorithmically align well with the relational argmax task . It has to learn the for-loops ( Claim 4.1 ) , which requires many samples . On the other hand , GNN algorithmically aligns well with relational argmax -- - the for-loops are hard-coded in the computation graph . Therefore , GNN achieves better sample efficiency by avoiding learning the for-loop . We will make the connection clearer in the revised version . - We will add the suggested reference and discuss the relation in the revised version ."}, {"review_id": "rJxbJeHFPS-1", "review_text": "The paper proposes a measure of classes of algorithmic alignment that measure how \"close\" neural networks are to known algorithms, e.g. dynamic programming (DP). The measure is based on the number of samples needed such that the expected generalization error is less than epsilon with 1-delta probability, where epsilon and delta are free parameters. The paper proves the link between several classes of known algorithms and neural network architectures by showing how their sample complexity varies. For instance the paper shows that Graph Neural Network (GNN), can approximate any DP algorithm in a sample efficient manner, whereas MLP and deep sets (permutation invariant NN) can't. The paper empirically verifies their claims on 4 toy datasets, each representing an increasingly complex algorithm needed to solve the problem. I recommend this paper be accepted, since I think it's an important direction of research, and it formalizes a lot of intuition about neural network architectures. It would be very interesting if the authors could actually compute the number of samples, M, for different NN architectures on the toy datasets, and show how it matches empirical findings. This could be a powerful tool if it could be made easy to use for the common practitioner.", "rating": "8: Accept", "reply_text": "Thank you for appreciating our work and giving the nice suggestion . It would indeed be very interesting to see sample sizes for different architectures and tasks in practice . However , the number of samples needed for models like MLP to learn the more complex tasks , e.g.DP , would be very high , so the experiments will be prohibitively expensive . We are considering experimenting models on smaller training set and plot accuracy v.s . sample size to showcase the trend . We have included the experimental results in the revised version ( Fig 4 and Sec 4.3 ) ."}, {"review_id": "rJxbJeHFPS-2", "review_text": "This work seeks theoretical and empirical proof of the reasoning capacity of neural networks. The authors build on a body of research that demonstrates the usefulness of different neural network architectures for different reasoning problems. For example, Deep Sets have been proposed to answer questions about sets (e.g., a summary statistic), and GNNs about graph related problems, such as shortest path. I anticipate that readers would be very satisfied with the intuition behind the main result: neural networks that \u201calign\u201d with known algorithmic solutions are better able to learn the solutions. Many architectures have been proposed over the years, often with a high-level justification for the architecture\u2019s form. For example, Relation Networks noted the difficulty with learning n^2 relations using an MLP, which is an observation reflected in this work\u2019s explanation of the difficulty with learning a for loop. Provided here is a justification for these high-level design decisions. The authors provide some theory and experimental results to demonstrate their proposed notion of alignment, and show that NNs that align with known algorithmic solution do well, while those that do not align do not do well. In particular, I appreciate both the positive and negative evidence, since demonstrating lack of alignment (and poor performance) is a necessary condition to show alongside alignment (and good performance). I\u2019d like to caution the authors regarding their main conclusion, which is stated a few times in the paper: \u201cThis perspective suggests that whether a neural network can learn a reasoning task depends on whether there exists an algorithmic solution that the network aligns with\u201d. I think this logic is not precisely correct, and I would modify this to: \u201cIf the structure of a neural network aligns with a known algorithmic solution, then it can more easily learn a reasoning task than a neural network does not align\u201d. This is a subtle but important difference. In particular, the original logic does not capture situations where an algorithmic solution is not known, but a neural network can otherwise still learn a solution (consider object classification). I think even the corrected logic as I\u2019ve spelled it out above might not be quite right either, since it does not consider situations where the algorithmic solution exists, but it obtuse. Would a neural network easily learn such a task? Overall I think the paper is clearly written, and the experiments are adequate. Unfortunately I am not well-versed in the theoretical literature on this topic, so my assessment of the proofs is limited, and I will need to defer to the other reviewers on these matters. My surface level assessment of them is that the logic seems generally sound, but I cannot make any strong statements placing them in the context of previous work, nor can I properly evaluate the nuances. Nonetheless, as a whole, I think this is a strong contribution and a nicely put together piece of work. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive feedback . Reviewer points out an imprecise statement/conclusion in our paper . We have adopted the reviewer 's suggested version in the revised revision . Reviewer asks whether neural networks can learn tasks where the algorithm is not known . Our answer is the algorithm we hope to learn does not need to be known , but knowing the structure of the algorithmic solution can help with designing architectures and theoretical guarantees . For example , our experiments ( Sec 4.1 , 4.3 ) show that different architectures that align to different algorithms can both learn the task well . Reviewer asks to more carefully consider the situation where the algorithmic solution exists but is obtuse . In this paper , we focus on reasoning tasks whose underlying algorithm is exact and has clear structure , and leave the study of approximation algorithms ( do not solve the task exactly ) and unknown structures for future work . We discussed this at the end of Sec 3 at page 4 . This should clarify the the range of problems we address in this paper , and how our results relate to various situations . In the case where we face a problem where we do not have knowledge about the underlying algorithmic structure , in order to still generalize well , we think neural architecture search over the algorithmic structure space could be a promising future direction . We will discuss these in the final version ."}], "0": {"review_id": "rJxbJeHFPS-0", "review_text": "This paper presents a framework, dubbed algorithmic alignment, based on PAC learning and sample complexity, with the aim to explain generalization on reasoning tasks for different neural architectures. The framework roughly states that in order for the model to be able to learn and successfully generalize on a reasoning task, it needs to be able to easily learn (to approximate) steps of the reasoning tasks. The authors use this framework to propose an increasingly difficult set of tasks, designed to showcase the type of models that would be fit or unfit to solve them. The resulting experiments corroborate the theory, showing the limits of MLPs, Deep Sets, and consequently Graph Neural Networks. The final claim that an NP-hard task needs an enumerative architecture, and then experimental validation of that claim is nice and fits into the theory. The added benefit of the paper is that the authors show as a side-effect that visual question answering and intuitive physics Overall, the paper presents a meaningful contribution to the theory of learning, formalizing the means of quantifying the capabilities of architectures to solve tasks of certain complexity. The paper, though dense, is well well written, and carries an interesting conclusion that better algorithmic alignment brings the sample complexity down, i.e. models with better algorithmic alignment to the task (function they want to approximate) should generalize better. The formalization presented in the paper, though remarkably intuitive, might be difficult to practically use for more elaborate models and it is not clear whether it can be numerically computed. The paper (i.e. the reader) would certainly benefit from more examples of algorithmic alignment comparison of different models, such as one done in Corollary 3.7. Question: - difference to Kolmogorov complexity is that any algorithmic alignment that yields decent sample complexity is good enough - how do you define decent? - You state: \u201cin Section 4, we will show that we can usually derive a near-optimal alignment by avoiding as many \u201cfor loops\u201d in algorithm steps as possible.\u201d yet I did not see that there. Was that effectively shown in Corollary 3.7? Slightly related work: On the Turing Completeness of Modern Neural Network Architectures", "rating": "8: Accept", "reply_text": "Thank you for your helpful feedback . We answer your questions below . - \u201c difference to Kolmogorov complexity is that any algorithmic alignment that yields decent sample complexity is good enough - how do you define decent ? \u201d . Here , \u201c decent \u201d is a loose term we use to refer to a tight enough algorithmic alignment for good generalization performance . We will explain more in the revised version . - \u201c You state : \u2018 in Section 4 , we will show that we can usually derive a near-optimal alignment by avoiding as many \u2018 for loops \u2019 in algorithm steps as possible. \u2019 yet I did not see that there \u201d . One example is Section 4.2 : DeepSets does not algorithmically align well with the relational argmax task . It has to learn the for-loops ( Claim 4.1 ) , which requires many samples . On the other hand , GNN algorithmically aligns well with relational argmax -- - the for-loops are hard-coded in the computation graph . Therefore , GNN achieves better sample efficiency by avoiding learning the for-loop . We will make the connection clearer in the revised version . - We will add the suggested reference and discuss the relation in the revised version ."}, "1": {"review_id": "rJxbJeHFPS-1", "review_text": "The paper proposes a measure of classes of algorithmic alignment that measure how \"close\" neural networks are to known algorithms, e.g. dynamic programming (DP). The measure is based on the number of samples needed such that the expected generalization error is less than epsilon with 1-delta probability, where epsilon and delta are free parameters. The paper proves the link between several classes of known algorithms and neural network architectures by showing how their sample complexity varies. For instance the paper shows that Graph Neural Network (GNN), can approximate any DP algorithm in a sample efficient manner, whereas MLP and deep sets (permutation invariant NN) can't. The paper empirically verifies their claims on 4 toy datasets, each representing an increasingly complex algorithm needed to solve the problem. I recommend this paper be accepted, since I think it's an important direction of research, and it formalizes a lot of intuition about neural network architectures. It would be very interesting if the authors could actually compute the number of samples, M, for different NN architectures on the toy datasets, and show how it matches empirical findings. This could be a powerful tool if it could be made easy to use for the common practitioner.", "rating": "8: Accept", "reply_text": "Thank you for appreciating our work and giving the nice suggestion . It would indeed be very interesting to see sample sizes for different architectures and tasks in practice . However , the number of samples needed for models like MLP to learn the more complex tasks , e.g.DP , would be very high , so the experiments will be prohibitively expensive . We are considering experimenting models on smaller training set and plot accuracy v.s . sample size to showcase the trend . We have included the experimental results in the revised version ( Fig 4 and Sec 4.3 ) ."}, "2": {"review_id": "rJxbJeHFPS-2", "review_text": "This work seeks theoretical and empirical proof of the reasoning capacity of neural networks. The authors build on a body of research that demonstrates the usefulness of different neural network architectures for different reasoning problems. For example, Deep Sets have been proposed to answer questions about sets (e.g., a summary statistic), and GNNs about graph related problems, such as shortest path. I anticipate that readers would be very satisfied with the intuition behind the main result: neural networks that \u201calign\u201d with known algorithmic solutions are better able to learn the solutions. Many architectures have been proposed over the years, often with a high-level justification for the architecture\u2019s form. For example, Relation Networks noted the difficulty with learning n^2 relations using an MLP, which is an observation reflected in this work\u2019s explanation of the difficulty with learning a for loop. Provided here is a justification for these high-level design decisions. The authors provide some theory and experimental results to demonstrate their proposed notion of alignment, and show that NNs that align with known algorithmic solution do well, while those that do not align do not do well. In particular, I appreciate both the positive and negative evidence, since demonstrating lack of alignment (and poor performance) is a necessary condition to show alongside alignment (and good performance). I\u2019d like to caution the authors regarding their main conclusion, which is stated a few times in the paper: \u201cThis perspective suggests that whether a neural network can learn a reasoning task depends on whether there exists an algorithmic solution that the network aligns with\u201d. I think this logic is not precisely correct, and I would modify this to: \u201cIf the structure of a neural network aligns with a known algorithmic solution, then it can more easily learn a reasoning task than a neural network does not align\u201d. This is a subtle but important difference. In particular, the original logic does not capture situations where an algorithmic solution is not known, but a neural network can otherwise still learn a solution (consider object classification). I think even the corrected logic as I\u2019ve spelled it out above might not be quite right either, since it does not consider situations where the algorithmic solution exists, but it obtuse. Would a neural network easily learn such a task? Overall I think the paper is clearly written, and the experiments are adequate. Unfortunately I am not well-versed in the theoretical literature on this topic, so my assessment of the proofs is limited, and I will need to defer to the other reviewers on these matters. My surface level assessment of them is that the logic seems generally sound, but I cannot make any strong statements placing them in the context of previous work, nor can I properly evaluate the nuances. Nonetheless, as a whole, I think this is a strong contribution and a nicely put together piece of work. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive feedback . Reviewer points out an imprecise statement/conclusion in our paper . We have adopted the reviewer 's suggested version in the revised revision . Reviewer asks whether neural networks can learn tasks where the algorithm is not known . Our answer is the algorithm we hope to learn does not need to be known , but knowing the structure of the algorithmic solution can help with designing architectures and theoretical guarantees . For example , our experiments ( Sec 4.1 , 4.3 ) show that different architectures that align to different algorithms can both learn the task well . Reviewer asks to more carefully consider the situation where the algorithmic solution exists but is obtuse . In this paper , we focus on reasoning tasks whose underlying algorithm is exact and has clear structure , and leave the study of approximation algorithms ( do not solve the task exactly ) and unknown structures for future work . We discussed this at the end of Sec 3 at page 4 . This should clarify the the range of problems we address in this paper , and how our results relate to various situations . In the case where we face a problem where we do not have knowledge about the underlying algorithmic structure , in order to still generalize well , we think neural architecture search over the algorithmic structure space could be a promising future direction . We will discuss these in the final version ."}}