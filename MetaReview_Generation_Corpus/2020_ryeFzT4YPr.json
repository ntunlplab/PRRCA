{"year": "2020", "forum": "ryeFzT4YPr", "title": "Lift-the-flap: what, where and when for context reasoning", "decision": "Reject", "meta_review": "The authors present the task lift-the-flap where an agent (artificial or human) is presented with a blurred image and a hidden item. The agent can de-blur the parts of the image by clicking on it. The authors introduce a model for this task (ClickNet) and they compare this against others.\nAs reviewers point, this paper presents an interesting set of experiments and analyses. Overall, this type of work can be quite influential as it gives an alternative way to improve our models by unveiling human strategies and using those as inductive biases for our models. That being said, I find the conclusions of this paper quite narrow for the general audience of ICLR (as R2 and R3 also point), as authors look into an artificial task and show ClickNet performs well. But what have we learned beyond that? How do we use these results to improve either our models or our understanding of these models? I believe these are the type of questions that  are missing from the current version of the paper and that if answered would greatly increase its impact and relevance to the ICLR community. At the moment though, I cannot recommend this paper for acceptance.\n", "reviews": [{"review_id": "ryeFzT4YPr-0", "review_text": "The paper is about context reasoning in the visual recognition. They designed a task called 'lift-the-flap', where the human or the model are given an image with one of the object blacked out. The image is provided in a relative low resolution, and the subjects could choose to click on area to reveal a local window of high-res image. The subject then need to answer what is the object in the blacked out region after a few clicks (ranging from 1 to 8 clicks). The authors has collected human data using mTurk, and also trained a model (named ClickNet) to perform the task. ClickNet has a LSTM memory which can carry the information of previous clicks and use spatial attention mechanism to decide the next click location (just use the argmax of the attention weight.) I find the paper is clearly-written and quite easy to follow. I think this is an interesting paper comparing human attentional performance to a trained computation model (without any human-imposed prior), showing that they behave similarly. However, I am not sure it would be of general interest to the ICLR audience. This may be more suitable for cognitive science conference/journal in my opinion. I have a few suggestions below: 1. In formula 2, the attention weight (before softmax) is \" e_ti = A_h * h_{t\u22121} + A_a * a_ti \" I am surprised there is no term that is h_{t-1} * a_ti, which do the content-based reading that is common in memory model like (DNC) or in language processing (transformer.) Can the author comment on this decision? 2. In result 5.1, the authors titled: \"WHAT: IMPORTANCE SAMPLING VIA ATTENTION AND PRIOR INFORMATION\". I am not sure I get where is the importance sampling appear in this paragraph. Importance sampling is a specific term, which refers to estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. I did not see how that is related to this paragraph. Or, I may have missed it, and would like the authors to elaborate. 3. I could not follow well the discussion in section 5.5 WHEN: ROLE OF RECURRENT CONNECTIONS. Based on the title of the section, I would expect to discuss why the recurrence of LSTM is important. Though, the paragraph is mainly talking about other control models like SVM and HMM, so I don't quite follow it. Also, it is not clear to me the word 'When' is a good choice. I am convinced that the LSTM helps the model carrying information across click, but it is not clearly shown the sequential order is critical (say click location 1, then location 2, then location 3 compared to location 2, location 3, then location 1 matters) Maybe it does? But, it is now shown in the results. 4. The comparison of where the human versus the model click (in Figure 6) is impressive. I wonder if the authors could go even one step further that is to see if the temporal order of the clicks are similar in human vs. model. It may not be the case. However, if it is true, that could make an even stronger case that the model has a similar way to decide attentional location compared to human subjects. Minor: 1. It is impressive that the authors have considered a wide range of control models. Though, for a conference paper with limited page limit, I don't think it is necessary to describe all these models in the main text ( for the SVM, HMM, and CRF models.) ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for constructive feedbacks . Answers are enclosed in sqared brackets . Page and figure numbers refer to the revised version of the paper . [ R2 ] Review : The paper is about context reasoning in the visual recognition . They designed a task called \u2019 lift-the-flap \u2019 , where the human or the model are given an image with one of the object blacked out . The image is provided in a relative low resolution , and the subjects could choose to click on area to reveal a local window of high-res image . The subject then need to answer what is the object in the blacked out region after a few clicks ( ranging from 1 to 8 clicks ) . The authors has collected human data using mTurk , and also trained a model ( named ClickNet ) to perform the task . ClickNet has a LSTM memory which can carry the information of previous clicks and use spatial attention mechanism to decide the next click location ( just use the argmax of the attention weight . ) I find the paper is clearly-written and quite easy to follow . I think this is an interesting paper comparing human attentional performance to a trained computation model ( without any human-imposed prior ) , showing that they behave similarly . [ R2.1 ] However , I am not sure it would be of general interest to the ICLR audience . This may be more suitable for cognitive science conference/journal in my opinion . [ Answer : As the reviewer points out , our work combines Cognitive Science and Computer Science . Our work directly concerns the type of computer vision experiments that have been critical to the development of recent AI approaches in pattern recognition and that computer vision has historically drawn on inspiration from Cognitive Science and Neuroscience . We hope that this work will strengthen these connections and bring insights to the computer vision community and more generally to the deep learning community . Several amazing works connecting cognitive neuroscience and computer science have been previously published in ICLR ( e.g. , Linsley et al , Learning what and where to attend , ICLR , 2019 ; Zheng et al , Revealing interpretable object representaitons from human behavior , ICLR , 2019 ) . ] [ R2.2 ] In formula 2 , the attention weight ( before softmax ) is eti= Ah \u00d7 ht .. 1 + Aa \u00d7 ati , I am surprised there is no term that is ht_1 \u00d7 ati , which do the content-based reading that is common in memory model like ( DNC ) or in language processing ( transformer . ) Can the author comment on this decision ? [ Answer : In DNC , attention denotes the similarity between the writing or reading content with the content stored in each memory location . However , in ClickNet , it is not clear how each element in ht corresponds to the content in each channel of feature vector ati . We share the reviewer \u2019 s intuition that ht .. 1 and ati should be combined , but whether this should be an additive term or a multiplicative term is not obvious . To assess this question , we implemented an alternative model : as suggested by the reviewer , instead of addition , we introduced ht_1 \u00d7 ati in equation 2 and reported the result in Figure S4 , and discussed the results on page 5 in the main text and Section A in Supplemenatry Material . Overall , adding the multiplicative term did not improve the model \u2019 s performance . ] [ R2.2 ] In result 5.1 , the authors titled : \u201d WHAT : IMPORTANCE SAMPLING VIA ATTENTION AND PRIOR INFORMATION \u201d . I am not sure I get where is the importance sampling appear in this paragraph . Importance sampling is a specific term , which refers to estimating properties of a particular distribution , while only having samples generated from a different distribution than the distribution of interest . I did not see how that is related to this paragraph . Or , I may have missed it , and would like the authors to elaborate . [ Answer : We thank the reviewer for clarifying the usage of importance sampling in statistics . The reviewer is correct in that we were not using this term correctly . What we intended to convey here is that subjects ( and the model ) sample ( click ) the image ( i.e.choose specific locations ) according to an internal model of what is important or not . To avoid confusion , we have rephrased the title as \u201d WHAT : REGION SELECTION VIA ATTENTION AND PRIOR INFORMATION \u201d in the revised version . ]"}, {"review_id": "ryeFzT4YPr-1", "review_text": "I agree that context is important in some visual recognition tasks. I think this is an ambitious study and find the employed experimental methods interesting. However, it is not clear to me what has been actually revealed by this study. In the last section, there is a statement \u201cthe model adequately predicts human sampling behavior and reaches almost human-level performance in this contextual reasoning task.\u201d I think that at least the first half is overstatement; it is not well validated by the experimental results. For instance, contrary to the authors\u2019 claim, I do not think the effectiveness of the recurrent connections in ClickNet is sufficiently validated. I think a yet another baseline is missing in the experiments, which is a strategy of clicking points in the periphery of the black box while avoiding (or minimizing) overlaps of the regions deblurred by previous clicks. Although ClickNet-RandPrior appears to be close to this strategy, it does not seem to use any constraint of avoiding such region overlap. On the other hand, in the training of ClickNet, a constraint \\sum_t\\alpha_{ti}=1 is used, which seems to play this very role, i.e., making it possible for ClickNet to avoid the region overlap. Isn\u2019t the good performance of ClickNet fully attributable to this constraint? The proposed method is to make subjects (or ClickNet) click a series of points in the input image and then deblur the local regions around the points. I suppose this procedure is accumulative, that is, once a local region is deblurred, it will be kept deblurred in the subsequent clicks. Then, I'm not sure if the order of clicking points really matters, whether it is a human subject or the ClickNet. For instance, is there any evidence that a click is dependent on the previous clicks, other than the above behavior of avoiding overlaps? Additionally, I am somewhat skeptical if a pretrained VGG can really extract useful features from (partially) blurred images, even though it is not trained on blurred images. It is widely known that CNNs for visual recognition tasks are vulnerable to image blur, noise, etc. when they do not exist in the images used for training, which is a kind of domain shift. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for constructive feedbacks . Answers are enclosed in squared brackets . Page and figure numbers refer to the revised version of the paper . [ R3 ] Review : I agree that context is important in some visual recognition tasks . I think this is an ambitious study and find the employed experimental methods interesting . However , it is not clear to me what has been actually revealed by this study . [ R3.1 ] In the last section , there is a statement \u201c the model adequately predicts human sampling behavior and reaches almost human-level performance in this contextual reasoning task. \u201d I think that at least the first half is overstatement ; it is not well validated by the experimental results . [ Answer : We have rewritten this statement to remove the ambiguous term \u201d adequately \u201d . Now we write : \u201d ... the model approximates human sampling behavior ( Click consistency in spatial domains ( Euclidean distance distribution and spatial bias in Figure 6 ) and temporal domains ( click sequence score in Figure S6 ) ) and reaches almost human-level performance in this contextual reasoning task ( contextual reasoning accuracy in Figure 3 and Figure 5b ) . \u201d We note that we are strictly referring to this particular task and we are not claiming that the model generally describes human sampling behavior in other domains . ] [ R3.2 ] For instance , contrary to the authors \u2019 claim , I do not think the effectiveness of the recurrent connections in ClickNet is sufficiently validated . I think a yet another baseline is missing in the experiments , which is a strategy of clicking points in the periphery of the black box while avoiding ( or minimizing ) overlaps of the regions deblurred by previous clicks . Although ClickNet-RandPrior appears to be close to this strategy , it does not seem to use any constraint of avoiding such region overlap . [ Answer : We have removed the regularization constraint in Equation 9 ( see R3.3 ) . There is no difference between ClickNet and ClicNet-RandPrior in whether the clicks can potentially overlap . However , we note that the chance that two random clicks near the bounding box overlap is very small ( p=0.0065 using the same overlap criteria defined for human clicks , page 6 ) . We further note , that two clicks can also overlap in ClickNet . ]"}, {"review_id": "ryeFzT4YPr-2", "review_text": "The paper does a psychological-computational-combined experiments for context reasoning. The experiment is done by \"lift-the-flap\" -- masking out a region of interest in the image and let either the human or a convNet based model to guess what it is by checking the context. Both of them are first shown with a blurred image with masked region, and then start to guess by clicking on surrounding regions and unblur them. A lot of baselines are compared and it is shown that the computational model is working well, and the behavior is highly correlated with humans. + The paper reads very well, the illustrations and tables are very well done. + The experiment itself is interesting that it delves into the context problem directly. It would be interesting to see how it works for objects that are out of context though: http://people.csail.mit.edu/myungjin/outOfContext.html + A like it that a great set of baselines and analysis are done for the paper. It strengthens the paper a lot. - I think the paper needs to have a baseline for the upper bound as well: what is the accuracy if the region is seen? In other words, what is the performance if no context is needed. It would be interesting to see the gap over there. A baseline could be a region-classification model, e.g., from: Chen, Xinlei, et al. \"Iterative visual reasoning beyond convolutions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. Just from the results (e.g. Fig 3) it seems our current models are already doing pretty well! (and it is VGG, not the best model yet), but on the other hand it is not clear how such models can help recognize visible objects better -- maybe a lot of the things that the context can offer has already been incorporated in the object pixels itself. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for constructive feedbacks . Answers are enclosed in squared brackets . Page and figure numbers refer to the revised version of the paper . [ R1 ] [ R1.1 ] The experiment itself is interesting that it delves into the context problem directly . It would be interesting to see how it works for objects that are out of context though : http : //people . csail.mit.edu/myungjin/outOfContext.html [ Answer : This is a great idea . As suggested , in the follow-up work after ICLR submission , we conducted another human behavioral experiment to test recognition accuracy in congruent versus incongruent backgrounds , and we also evaluated ClickNet in this new task . We added this experiment in the supplementary material ( Figure S7 and Section D ) . We also discuss this point in the main text ( page 2 ) and cite this work ( page 2 and 3 ) in the revised version of our paper . ] [ R1.2 ] I think the paper needs to have a baseline for the upper bound as well : what is the accuracy if the region is seen ? In other words , what is the performance if no context is needed . It would be interesting to see the gap over there . [ Answer : As the reviewer suggested , there are two interesting questions here : ( 1 ) Performance for the same image without the black box : We fine-tuned ClickNet on the same training images with the target object revealed ClickNet-ObjRevealed ) , shown in Figure S4 in the revised version . ( 2 ) Performance for the object only condition : We tested ClickNet fine-tuned above in ( 1 ) on the same test set but only showing the object without context and reported performance ( ClickNet-ObjOnly ) in Figure S4 in the revised version . Please see page 7 in the main text and Section C in the supplementary material in the revised paper for analyses and discussion . ] [ R1.3 ] A baseline could be a region-classification model , e.g. , from : Chen , Xinlei , et al. \u201d Iterative visual reasoning beyond convolutions. \u201d Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2018 . [ Answer : This is another great suggestion . Despite our attempts , we were not able to get the publicly available source code of this paper to run in the short time since this suggestion . Due to the limited time , we chose an alternative baseline , YOLO3 ( Redmon J et al , YOLOv3 : An Incremental Improvement ) , which is a state-of-the-art object detection algorithm . We are aware that this baseline is not comparable with the nice work of Chen et al , where there is a local spatial memory module to update belief and a global graph reasoning module , but we hope YOLO3 provides a general comparison to how ClickNet-ObjOnly and ClickNet-ObjRevealed perform on this task . YOLO3 was tested on full resolution test images with either objects revealed or object-only conditions ( R1.2 ) yielding 65 % and 44.2 % performance , respectively . ( Secition C ) . Meanwhile , we cited Chen \u2019 s work in the related work section , and we will continue to work on its implementation ; we agree that this is an interesting comparison . ] [ R1.4 ] Just from the results ( e.g.Fig 3 ) it seems our current models are already doing pretty well ! ( and it is VGG , not the best model yet ) , but on the other hand it is not clear how such models can help recognize visible objects better \u2013 maybe a lot of the things that the context can offer has already been incorporated in the object pixels itself . [ Answer : We agree with the reviewer that in object recognition tasks in computer vision , VGG16 and other feed-forward networks implicitly but heavily rely on contextual information to recognize objects ( e.g. , Linsley et al , LEARNING WHAT AND WHERE TO ATTEND , ICLR , 2019 ) . Some studies ( Beery et al , Recognition in terra incognita , 2018 , ECCV ; Dvornik , Nikita , Modeling visual context is key to augmenting object detection datasets , 2018 , ECCV ) and our work following up on the suggestion in R1.1 also show that these models can fail miserably when objects are placed in an incongruent context . These studies motivate us to systematically examine the role of context in lift-the-flap tasks . Furthermore , in situations of complete occlusion ( lift-the-flap ) , here we provide human and computational benchmarks for object inference . We have added these references and points in the text ( page 2 ) ] ."}], "0": {"review_id": "ryeFzT4YPr-0", "review_text": "The paper is about context reasoning in the visual recognition. They designed a task called 'lift-the-flap', where the human or the model are given an image with one of the object blacked out. The image is provided in a relative low resolution, and the subjects could choose to click on area to reveal a local window of high-res image. The subject then need to answer what is the object in the blacked out region after a few clicks (ranging from 1 to 8 clicks). The authors has collected human data using mTurk, and also trained a model (named ClickNet) to perform the task. ClickNet has a LSTM memory which can carry the information of previous clicks and use spatial attention mechanism to decide the next click location (just use the argmax of the attention weight.) I find the paper is clearly-written and quite easy to follow. I think this is an interesting paper comparing human attentional performance to a trained computation model (without any human-imposed prior), showing that they behave similarly. However, I am not sure it would be of general interest to the ICLR audience. This may be more suitable for cognitive science conference/journal in my opinion. I have a few suggestions below: 1. In formula 2, the attention weight (before softmax) is \" e_ti = A_h * h_{t\u22121} + A_a * a_ti \" I am surprised there is no term that is h_{t-1} * a_ti, which do the content-based reading that is common in memory model like (DNC) or in language processing (transformer.) Can the author comment on this decision? 2. In result 5.1, the authors titled: \"WHAT: IMPORTANCE SAMPLING VIA ATTENTION AND PRIOR INFORMATION\". I am not sure I get where is the importance sampling appear in this paragraph. Importance sampling is a specific term, which refers to estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. I did not see how that is related to this paragraph. Or, I may have missed it, and would like the authors to elaborate. 3. I could not follow well the discussion in section 5.5 WHEN: ROLE OF RECURRENT CONNECTIONS. Based on the title of the section, I would expect to discuss why the recurrence of LSTM is important. Though, the paragraph is mainly talking about other control models like SVM and HMM, so I don't quite follow it. Also, it is not clear to me the word 'When' is a good choice. I am convinced that the LSTM helps the model carrying information across click, but it is not clearly shown the sequential order is critical (say click location 1, then location 2, then location 3 compared to location 2, location 3, then location 1 matters) Maybe it does? But, it is now shown in the results. 4. The comparison of where the human versus the model click (in Figure 6) is impressive. I wonder if the authors could go even one step further that is to see if the temporal order of the clicks are similar in human vs. model. It may not be the case. However, if it is true, that could make an even stronger case that the model has a similar way to decide attentional location compared to human subjects. Minor: 1. It is impressive that the authors have considered a wide range of control models. Though, for a conference paper with limited page limit, I don't think it is necessary to describe all these models in the main text ( for the SVM, HMM, and CRF models.) ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for constructive feedbacks . Answers are enclosed in sqared brackets . Page and figure numbers refer to the revised version of the paper . [ R2 ] Review : The paper is about context reasoning in the visual recognition . They designed a task called \u2019 lift-the-flap \u2019 , where the human or the model are given an image with one of the object blacked out . The image is provided in a relative low resolution , and the subjects could choose to click on area to reveal a local window of high-res image . The subject then need to answer what is the object in the blacked out region after a few clicks ( ranging from 1 to 8 clicks ) . The authors has collected human data using mTurk , and also trained a model ( named ClickNet ) to perform the task . ClickNet has a LSTM memory which can carry the information of previous clicks and use spatial attention mechanism to decide the next click location ( just use the argmax of the attention weight . ) I find the paper is clearly-written and quite easy to follow . I think this is an interesting paper comparing human attentional performance to a trained computation model ( without any human-imposed prior ) , showing that they behave similarly . [ R2.1 ] However , I am not sure it would be of general interest to the ICLR audience . This may be more suitable for cognitive science conference/journal in my opinion . [ Answer : As the reviewer points out , our work combines Cognitive Science and Computer Science . Our work directly concerns the type of computer vision experiments that have been critical to the development of recent AI approaches in pattern recognition and that computer vision has historically drawn on inspiration from Cognitive Science and Neuroscience . We hope that this work will strengthen these connections and bring insights to the computer vision community and more generally to the deep learning community . Several amazing works connecting cognitive neuroscience and computer science have been previously published in ICLR ( e.g. , Linsley et al , Learning what and where to attend , ICLR , 2019 ; Zheng et al , Revealing interpretable object representaitons from human behavior , ICLR , 2019 ) . ] [ R2.2 ] In formula 2 , the attention weight ( before softmax ) is eti= Ah \u00d7 ht .. 1 + Aa \u00d7 ati , I am surprised there is no term that is ht_1 \u00d7 ati , which do the content-based reading that is common in memory model like ( DNC ) or in language processing ( transformer . ) Can the author comment on this decision ? [ Answer : In DNC , attention denotes the similarity between the writing or reading content with the content stored in each memory location . However , in ClickNet , it is not clear how each element in ht corresponds to the content in each channel of feature vector ati . We share the reviewer \u2019 s intuition that ht .. 1 and ati should be combined , but whether this should be an additive term or a multiplicative term is not obvious . To assess this question , we implemented an alternative model : as suggested by the reviewer , instead of addition , we introduced ht_1 \u00d7 ati in equation 2 and reported the result in Figure S4 , and discussed the results on page 5 in the main text and Section A in Supplemenatry Material . Overall , adding the multiplicative term did not improve the model \u2019 s performance . ] [ R2.2 ] In result 5.1 , the authors titled : \u201d WHAT : IMPORTANCE SAMPLING VIA ATTENTION AND PRIOR INFORMATION \u201d . I am not sure I get where is the importance sampling appear in this paragraph . Importance sampling is a specific term , which refers to estimating properties of a particular distribution , while only having samples generated from a different distribution than the distribution of interest . I did not see how that is related to this paragraph . Or , I may have missed it , and would like the authors to elaborate . [ Answer : We thank the reviewer for clarifying the usage of importance sampling in statistics . The reviewer is correct in that we were not using this term correctly . What we intended to convey here is that subjects ( and the model ) sample ( click ) the image ( i.e.choose specific locations ) according to an internal model of what is important or not . To avoid confusion , we have rephrased the title as \u201d WHAT : REGION SELECTION VIA ATTENTION AND PRIOR INFORMATION \u201d in the revised version . ]"}, "1": {"review_id": "ryeFzT4YPr-1", "review_text": "I agree that context is important in some visual recognition tasks. I think this is an ambitious study and find the employed experimental methods interesting. However, it is not clear to me what has been actually revealed by this study. In the last section, there is a statement \u201cthe model adequately predicts human sampling behavior and reaches almost human-level performance in this contextual reasoning task.\u201d I think that at least the first half is overstatement; it is not well validated by the experimental results. For instance, contrary to the authors\u2019 claim, I do not think the effectiveness of the recurrent connections in ClickNet is sufficiently validated. I think a yet another baseline is missing in the experiments, which is a strategy of clicking points in the periphery of the black box while avoiding (or minimizing) overlaps of the regions deblurred by previous clicks. Although ClickNet-RandPrior appears to be close to this strategy, it does not seem to use any constraint of avoiding such region overlap. On the other hand, in the training of ClickNet, a constraint \\sum_t\\alpha_{ti}=1 is used, which seems to play this very role, i.e., making it possible for ClickNet to avoid the region overlap. Isn\u2019t the good performance of ClickNet fully attributable to this constraint? The proposed method is to make subjects (or ClickNet) click a series of points in the input image and then deblur the local regions around the points. I suppose this procedure is accumulative, that is, once a local region is deblurred, it will be kept deblurred in the subsequent clicks. Then, I'm not sure if the order of clicking points really matters, whether it is a human subject or the ClickNet. For instance, is there any evidence that a click is dependent on the previous clicks, other than the above behavior of avoiding overlaps? Additionally, I am somewhat skeptical if a pretrained VGG can really extract useful features from (partially) blurred images, even though it is not trained on blurred images. It is widely known that CNNs for visual recognition tasks are vulnerable to image blur, noise, etc. when they do not exist in the images used for training, which is a kind of domain shift. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for constructive feedbacks . Answers are enclosed in squared brackets . Page and figure numbers refer to the revised version of the paper . [ R3 ] Review : I agree that context is important in some visual recognition tasks . I think this is an ambitious study and find the employed experimental methods interesting . However , it is not clear to me what has been actually revealed by this study . [ R3.1 ] In the last section , there is a statement \u201c the model adequately predicts human sampling behavior and reaches almost human-level performance in this contextual reasoning task. \u201d I think that at least the first half is overstatement ; it is not well validated by the experimental results . [ Answer : We have rewritten this statement to remove the ambiguous term \u201d adequately \u201d . Now we write : \u201d ... the model approximates human sampling behavior ( Click consistency in spatial domains ( Euclidean distance distribution and spatial bias in Figure 6 ) and temporal domains ( click sequence score in Figure S6 ) ) and reaches almost human-level performance in this contextual reasoning task ( contextual reasoning accuracy in Figure 3 and Figure 5b ) . \u201d We note that we are strictly referring to this particular task and we are not claiming that the model generally describes human sampling behavior in other domains . ] [ R3.2 ] For instance , contrary to the authors \u2019 claim , I do not think the effectiveness of the recurrent connections in ClickNet is sufficiently validated . I think a yet another baseline is missing in the experiments , which is a strategy of clicking points in the periphery of the black box while avoiding ( or minimizing ) overlaps of the regions deblurred by previous clicks . Although ClickNet-RandPrior appears to be close to this strategy , it does not seem to use any constraint of avoiding such region overlap . [ Answer : We have removed the regularization constraint in Equation 9 ( see R3.3 ) . There is no difference between ClickNet and ClicNet-RandPrior in whether the clicks can potentially overlap . However , we note that the chance that two random clicks near the bounding box overlap is very small ( p=0.0065 using the same overlap criteria defined for human clicks , page 6 ) . We further note , that two clicks can also overlap in ClickNet . ]"}, "2": {"review_id": "ryeFzT4YPr-2", "review_text": "The paper does a psychological-computational-combined experiments for context reasoning. The experiment is done by \"lift-the-flap\" -- masking out a region of interest in the image and let either the human or a convNet based model to guess what it is by checking the context. Both of them are first shown with a blurred image with masked region, and then start to guess by clicking on surrounding regions and unblur them. A lot of baselines are compared and it is shown that the computational model is working well, and the behavior is highly correlated with humans. + The paper reads very well, the illustrations and tables are very well done. + The experiment itself is interesting that it delves into the context problem directly. It would be interesting to see how it works for objects that are out of context though: http://people.csail.mit.edu/myungjin/outOfContext.html + A like it that a great set of baselines and analysis are done for the paper. It strengthens the paper a lot. - I think the paper needs to have a baseline for the upper bound as well: what is the accuracy if the region is seen? In other words, what is the performance if no context is needed. It would be interesting to see the gap over there. A baseline could be a region-classification model, e.g., from: Chen, Xinlei, et al. \"Iterative visual reasoning beyond convolutions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. Just from the results (e.g. Fig 3) it seems our current models are already doing pretty well! (and it is VGG, not the best model yet), but on the other hand it is not clear how such models can help recognize visible objects better -- maybe a lot of the things that the context can offer has already been incorporated in the object pixels itself. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for constructive feedbacks . Answers are enclosed in squared brackets . Page and figure numbers refer to the revised version of the paper . [ R1 ] [ R1.1 ] The experiment itself is interesting that it delves into the context problem directly . It would be interesting to see how it works for objects that are out of context though : http : //people . csail.mit.edu/myungjin/outOfContext.html [ Answer : This is a great idea . As suggested , in the follow-up work after ICLR submission , we conducted another human behavioral experiment to test recognition accuracy in congruent versus incongruent backgrounds , and we also evaluated ClickNet in this new task . We added this experiment in the supplementary material ( Figure S7 and Section D ) . We also discuss this point in the main text ( page 2 ) and cite this work ( page 2 and 3 ) in the revised version of our paper . ] [ R1.2 ] I think the paper needs to have a baseline for the upper bound as well : what is the accuracy if the region is seen ? In other words , what is the performance if no context is needed . It would be interesting to see the gap over there . [ Answer : As the reviewer suggested , there are two interesting questions here : ( 1 ) Performance for the same image without the black box : We fine-tuned ClickNet on the same training images with the target object revealed ClickNet-ObjRevealed ) , shown in Figure S4 in the revised version . ( 2 ) Performance for the object only condition : We tested ClickNet fine-tuned above in ( 1 ) on the same test set but only showing the object without context and reported performance ( ClickNet-ObjOnly ) in Figure S4 in the revised version . Please see page 7 in the main text and Section C in the supplementary material in the revised paper for analyses and discussion . ] [ R1.3 ] A baseline could be a region-classification model , e.g. , from : Chen , Xinlei , et al. \u201d Iterative visual reasoning beyond convolutions. \u201d Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2018 . [ Answer : This is another great suggestion . Despite our attempts , we were not able to get the publicly available source code of this paper to run in the short time since this suggestion . Due to the limited time , we chose an alternative baseline , YOLO3 ( Redmon J et al , YOLOv3 : An Incremental Improvement ) , which is a state-of-the-art object detection algorithm . We are aware that this baseline is not comparable with the nice work of Chen et al , where there is a local spatial memory module to update belief and a global graph reasoning module , but we hope YOLO3 provides a general comparison to how ClickNet-ObjOnly and ClickNet-ObjRevealed perform on this task . YOLO3 was tested on full resolution test images with either objects revealed or object-only conditions ( R1.2 ) yielding 65 % and 44.2 % performance , respectively . ( Secition C ) . Meanwhile , we cited Chen \u2019 s work in the related work section , and we will continue to work on its implementation ; we agree that this is an interesting comparison . ] [ R1.4 ] Just from the results ( e.g.Fig 3 ) it seems our current models are already doing pretty well ! ( and it is VGG , not the best model yet ) , but on the other hand it is not clear how such models can help recognize visible objects better \u2013 maybe a lot of the things that the context can offer has already been incorporated in the object pixels itself . [ Answer : We agree with the reviewer that in object recognition tasks in computer vision , VGG16 and other feed-forward networks implicitly but heavily rely on contextual information to recognize objects ( e.g. , Linsley et al , LEARNING WHAT AND WHERE TO ATTEND , ICLR , 2019 ) . Some studies ( Beery et al , Recognition in terra incognita , 2018 , ECCV ; Dvornik , Nikita , Modeling visual context is key to augmenting object detection datasets , 2018 , ECCV ) and our work following up on the suggestion in R1.1 also show that these models can fail miserably when objects are placed in an incongruent context . These studies motivate us to systematically examine the role of context in lift-the-flap tasks . Furthermore , in situations of complete occlusion ( lift-the-flap ) , here we provide human and computational benchmarks for object inference . We have added these references and points in the text ( page 2 ) ] ."}}