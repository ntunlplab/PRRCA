{"year": "2017", "forum": "ByEPMj5el", "title": "Out-of-class novelty generation: an experimental foundation", "decision": "Reject", "meta_review": "This paper aims to present an experimental framework for selecting machine learning models that can generate novel objects. As the work is devoted to a relatively subjective area of study, it is not surprising that opinions of the work are mixed.\n \n A large section of the paper is devoted to review, and more detail could be given to the experimental framework. It is not clear whether the framework can actually be useful outside the synthetic setup described. Moreover, I worry it encourages unhealthy directions for the field. Over 1000 models were trained and evaluated. There is no form of separate held-out comparison: the framework encourages people to keep trying random stuff until the chosen measure reports success.", "reviews": [{"review_id": "ByEPMj5el-0", "review_text": "This paper examines computational creativity from a machine learning perspective. Creativity is defined as a model's ability to generate new types of objects unseen during training. The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model's ability to generate new and interesting object types. I am not very familiar with the literature on computational creativity research, so I can't judge on how well this work has been put into the context of existing work. From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking. As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former. The extent to which this is true depends on the bias introduced by model selection. Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation. Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework. I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers.", "rating": "7: Good paper, accept", "reply_text": "We are grateful to the reviewer for qualifying the ideas presented in the paper as \u201c new , interesting and thought-provoking \u201d and judging that our paper deserves its place at ICLR . We also thank him or her for acknowledging the importance of our effort in formalizing creativity from a machine learning perspective . Indeed , as the reviewer states , we strived to make the topic approachable by machine learning researchers , both for enabling a transfer of knowledge across domains and to bring a systematic and rigorous evaluation scheme for computational creativity work ."}, {"review_id": "ByEPMj5el-1", "review_text": "First, the bad: This paper is frustratingly written. The grammar is fine, but: - The first four pages are completely theoretical and difficult to follow without any concrete examples. These sections would benefit greatly from a common example woven through the different aspects of the theoretical discussion. - The ordering of the exposition is also frustrating. I found myself constantly having to refer ahead to figures and back to details that were important but seemingly presented out of order. Perhaps a reordering of some details could fix this. Recommendation: give the most naturally ordered oral presentation of the work and then order the paper similarly. Finally, the description of the experiments is cursory, and I found myself wondering whether the details omitted were important or not. Including experimental details in a supplementary section could help assuage these fears. The good: What the paper does well is to gather together past work on novelty generation and propose a unified framework in which to evaluate past and future models. This is done by repurposing existing generative model evaluation metrics for the task of evaluating novelty. The experiments are basic, but even the basic experiments go beyond previous work in this area (to this reviewer\u2019s knowledge). Overall I recommend the paper be accepted, but I strongly recommend rewriting some components to make it more digestible. As with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to the reviewer : \u201c Paper fights a difficult battle to defend and unify novelty generating models . It fights somewhat well. \u201d \u201c the basic experiments go beyond previous work in this area \u201d We think the following points is especially relevant , for this kind of paper : As with other novelty papers , [ ... ] it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation ; We contend that this is true for many cross-domain , multidisciplinary work . The chance to present the paper in the conference will surely improve future versions and reduce incomprehensions that typically arise in such efforts . \u201c for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible. \u201d Other reviewers have stated that the paper is well-written and easy to understand for machine learning researchers . Nevertheless , we took into account reviewer 3 \u2019 s suggestion : We made some modifications in the paper that will clarify that the readers interested into the metrics and the experiments can jump to the relevant sections ."}, {"review_id": "ByEPMj5el-2", "review_text": "The authors proposed an way to measure the generation of out-of-distribution novelty. Their methods implied, if a model trained on MNIST digits could generate some samples are more like letters judged by anther model trained both on MNIST and letters, the model trained on MNIST could be seen as having the ability to generate novel samples. Some empirical experiments were reported. The novelty is hard to define. The proposed metric is also problematic. A naive combination of MNIST and letters dataset do not represent the natural distribution of handwritten digits and letters. IT means that the model trained on the combination could not properly distinguished digits and letters. The proposed out-of-class count and out-of-class max are thus pointless. For the \"novel\" samples in Fig. 3, they are clearly digits. I guess they quantize the samples to binary. If they would quantize the samples to 8 bit, the resulting images would look even more like digits. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer states : \u201c For the `` novel '' samples in Fig.3 , they are clearly digits. \u201d We kindly ask the reviewer to classify the symbols on the following panel into one of the digit categories 0-9 . They are from Figure 3 . We de-binarized them ( quantized them to 8 bits ) as you suggested . https : //lh3.googleusercontent.com/-Bs646MOG89g/WFalSDVY2JI/AAAAAAAAOfk/cvGQqQQZeTw6PMWdih_LrGyYUfFW0LoTwCL0B/h88/panel2.png Also , we kindly ask the reviewer to comment on why our brains can read the following pangrams as sentences , and not digits : https : //lh3.googleusercontent.com/-zcZv3_saVkQ/WFe294YB9pI/AAAAAAAAFGg/yoODIjOQSBkc_ZGmiG5eI7_6hCJId5uDQCL0B/h282/2016-12-19.jpg Note that the models were never _asked_ to generate letters ; they just do . With respect to the limited knowledge the model has , how is this not novel ? The problem we are dealing with is not whether we can learn or not a supposedly natural distribution of handwritten digits and letters , from digits only . The problem we are dealing with is this : Suppose we ask a thousand children , who only know how to write digits , to write something different - How would you evaluate these children on their ability to come up with coherent , and even meaningful symbols ? Please replace child/children in the above two sentences with \u201c generative models \u201d ."}, {"review_id": "ByEPMj5el-3", "review_text": "This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them. The authors mentioned difficulties in human subject studies, but did not provide details of their own setting. An \"in-house\" annotation tool was used but it's unclear how many subjects were involved, who they are, and how many samples were presented to each subject. I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field. This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters. There is insufficient evidence to prove the generality of the proposed metric. Moreover, defining English letters as \"novel\" compared to Arabic digits is questionable. What if the model generates Arabic or Indian letters? Can a human who has never seen Arabic handwriting tell it from random doodle? What makes English letters more \"novel\" than random doodle? In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings. For example, do you prefer painting A (generated) or B (painted by artist).", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his/her comments . We believe his/her criticism does not apply to our work and we feel that he/she may have misunderstood the problem our paper deals with , as the following quotes demonstrate : \u201c This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. \u201d \u201c This paper aims at proposing a general metric for novelty but the experiments only\u2026 \u201d The paper proposes metrics for model selection , _not_ for novelty selection . Thus , the issues raised about universality of a novelty metric , novelty or artistic value of letters , experimental conditions with human subjects are all out of the scope . One should not confuse the objective of this work : Selecting a good generative model for novelty generation is an open question . Current work on deep generative modeling learns a set of objects ( e.g.digits or bedrooms ) and generate the same _kinds_ of objects ( e.g.digits or bedrooms ) . Clearly , when a generative model is used in this way , it is unlikely to generate novelty . Thus , the scientific question here is what setup would enable us to measure the novelty generation capacity of a deep generative model , where novelty is defined as above - generating objects from categories unknown to the machine . This idea is the basis of the setup introduced in the paper : much as we , as humans , know Roman letters , the generative models we train do not . Albeit this fact , some of the 1000 or so models we trained are able to generate objects that a separate letter-discriminator model classifies as letters . This , we believe , is a good proxy for testing the capacity of a generative model to generate unknown categories of objects , much in the same way , held-out data points enabling testing the generalization capacity of a predictive model . Furthermore , we would like to highlight the following point : if our objective was to generate art or genuine novelty , we would have used some data sets other than roman letters or digits . That 's precisely what we _don't_ want to do , since in more complex systems and datasets , it is unclear whether the computer generates art ( or novelty ) or whether the object generated based on a possibly infinite combinatorics is _perceived_ as such by the humans . In such complicated systems , the frontier of what the system creator has put in and what the machine adds is often blurred . To be able to start investigating what the machine can come up with without any value objective imposed by its creator , starting with a simple and controlled setup is only a best practice in science ."}], "0": {"review_id": "ByEPMj5el-0", "review_text": "This paper examines computational creativity from a machine learning perspective. Creativity is defined as a model's ability to generate new types of objects unseen during training. The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model's ability to generate new and interesting object types. I am not very familiar with the literature on computational creativity research, so I can't judge on how well this work has been put into the context of existing work. From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking. As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former. The extent to which this is true depends on the bias introduced by model selection. Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation. Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework. I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers.", "rating": "7: Good paper, accept", "reply_text": "We are grateful to the reviewer for qualifying the ideas presented in the paper as \u201c new , interesting and thought-provoking \u201d and judging that our paper deserves its place at ICLR . We also thank him or her for acknowledging the importance of our effort in formalizing creativity from a machine learning perspective . Indeed , as the reviewer states , we strived to make the topic approachable by machine learning researchers , both for enabling a transfer of knowledge across domains and to bring a systematic and rigorous evaluation scheme for computational creativity work ."}, "1": {"review_id": "ByEPMj5el-1", "review_text": "First, the bad: This paper is frustratingly written. The grammar is fine, but: - The first four pages are completely theoretical and difficult to follow without any concrete examples. These sections would benefit greatly from a common example woven through the different aspects of the theoretical discussion. - The ordering of the exposition is also frustrating. I found myself constantly having to refer ahead to figures and back to details that were important but seemingly presented out of order. Perhaps a reordering of some details could fix this. Recommendation: give the most naturally ordered oral presentation of the work and then order the paper similarly. Finally, the description of the experiments is cursory, and I found myself wondering whether the details omitted were important or not. Including experimental details in a supplementary section could help assuage these fears. The good: What the paper does well is to gather together past work on novelty generation and propose a unified framework in which to evaluate past and future models. This is done by repurposing existing generative model evaluation metrics for the task of evaluating novelty. The experiments are basic, but even the basic experiments go beyond previous work in this area (to this reviewer\u2019s knowledge). Overall I recommend the paper be accepted, but I strongly recommend rewriting some components to make it more digestible. As with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to the reviewer : \u201c Paper fights a difficult battle to defend and unify novelty generating models . It fights somewhat well. \u201d \u201c the basic experiments go beyond previous work in this area \u201d We think the following points is especially relevant , for this kind of paper : As with other novelty papers , [ ... ] it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation ; We contend that this is true for many cross-domain , multidisciplinary work . The chance to present the paper in the conference will surely improve future versions and reduce incomprehensions that typically arise in such efforts . \u201c for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible. \u201d Other reviewers have stated that the paper is well-written and easy to understand for machine learning researchers . Nevertheless , we took into account reviewer 3 \u2019 s suggestion : We made some modifications in the paper that will clarify that the readers interested into the metrics and the experiments can jump to the relevant sections ."}, "2": {"review_id": "ByEPMj5el-2", "review_text": "The authors proposed an way to measure the generation of out-of-distribution novelty. Their methods implied, if a model trained on MNIST digits could generate some samples are more like letters judged by anther model trained both on MNIST and letters, the model trained on MNIST could be seen as having the ability to generate novel samples. Some empirical experiments were reported. The novelty is hard to define. The proposed metric is also problematic. A naive combination of MNIST and letters dataset do not represent the natural distribution of handwritten digits and letters. IT means that the model trained on the combination could not properly distinguished digits and letters. The proposed out-of-class count and out-of-class max are thus pointless. For the \"novel\" samples in Fig. 3, they are clearly digits. I guess they quantize the samples to binary. If they would quantize the samples to 8 bit, the resulting images would look even more like digits. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer states : \u201c For the `` novel '' samples in Fig.3 , they are clearly digits. \u201d We kindly ask the reviewer to classify the symbols on the following panel into one of the digit categories 0-9 . They are from Figure 3 . We de-binarized them ( quantized them to 8 bits ) as you suggested . https : //lh3.googleusercontent.com/-Bs646MOG89g/WFalSDVY2JI/AAAAAAAAOfk/cvGQqQQZeTw6PMWdih_LrGyYUfFW0LoTwCL0B/h88/panel2.png Also , we kindly ask the reviewer to comment on why our brains can read the following pangrams as sentences , and not digits : https : //lh3.googleusercontent.com/-zcZv3_saVkQ/WFe294YB9pI/AAAAAAAAFGg/yoODIjOQSBkc_ZGmiG5eI7_6hCJId5uDQCL0B/h282/2016-12-19.jpg Note that the models were never _asked_ to generate letters ; they just do . With respect to the limited knowledge the model has , how is this not novel ? The problem we are dealing with is not whether we can learn or not a supposedly natural distribution of handwritten digits and letters , from digits only . The problem we are dealing with is this : Suppose we ask a thousand children , who only know how to write digits , to write something different - How would you evaluate these children on their ability to come up with coherent , and even meaningful symbols ? Please replace child/children in the above two sentences with \u201c generative models \u201d ."}, "3": {"review_id": "ByEPMj5el-3", "review_text": "This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them. The authors mentioned difficulties in human subject studies, but did not provide details of their own setting. An \"in-house\" annotation tool was used but it's unclear how many subjects were involved, who they are, and how many samples were presented to each subject. I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field. This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters. There is insufficient evidence to prove the generality of the proposed metric. Moreover, defining English letters as \"novel\" compared to Arabic digits is questionable. What if the model generates Arabic or Indian letters? Can a human who has never seen Arabic handwriting tell it from random doodle? What makes English letters more \"novel\" than random doodle? In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings. For example, do you prefer painting A (generated) or B (painted by artist).", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his/her comments . We believe his/her criticism does not apply to our work and we feel that he/she may have misunderstood the problem our paper deals with , as the following quotes demonstrate : \u201c This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. \u201d \u201c This paper aims at proposing a general metric for novelty but the experiments only\u2026 \u201d The paper proposes metrics for model selection , _not_ for novelty selection . Thus , the issues raised about universality of a novelty metric , novelty or artistic value of letters , experimental conditions with human subjects are all out of the scope . One should not confuse the objective of this work : Selecting a good generative model for novelty generation is an open question . Current work on deep generative modeling learns a set of objects ( e.g.digits or bedrooms ) and generate the same _kinds_ of objects ( e.g.digits or bedrooms ) . Clearly , when a generative model is used in this way , it is unlikely to generate novelty . Thus , the scientific question here is what setup would enable us to measure the novelty generation capacity of a deep generative model , where novelty is defined as above - generating objects from categories unknown to the machine . This idea is the basis of the setup introduced in the paper : much as we , as humans , know Roman letters , the generative models we train do not . Albeit this fact , some of the 1000 or so models we trained are able to generate objects that a separate letter-discriminator model classifies as letters . This , we believe , is a good proxy for testing the capacity of a generative model to generate unknown categories of objects , much in the same way , held-out data points enabling testing the generalization capacity of a predictive model . Furthermore , we would like to highlight the following point : if our objective was to generate art or genuine novelty , we would have used some data sets other than roman letters or digits . That 's precisely what we _don't_ want to do , since in more complex systems and datasets , it is unclear whether the computer generates art ( or novelty ) or whether the object generated based on a possibly infinite combinatorics is _perceived_ as such by the humans . In such complicated systems , the frontier of what the system creator has put in and what the machine adds is often blurred . To be able to start investigating what the machine can come up with without any value objective imposed by its creator , starting with a simple and controlled setup is only a best practice in science ."}}