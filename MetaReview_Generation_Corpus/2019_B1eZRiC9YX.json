{"year": "2019", "forum": "B1eZRiC9YX", "title": "Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks", "decision": "Reject", "meta_review": "This paper conducts a study of the adversarial robustness of Bayesian Neural Network models. The reviewers all agree that the paper presents an interesting direction, with sound theoretical backing. However, there are important concerns regarding the significance and clarity of the work. In particular, the paper would greatly benefit from more demonstrated empirical significance, and more polished definitions and theoretical results. ", "reviews": [{"review_id": "B1eZRiC9YX-0", "review_text": "The paper studies the adversarial robustness of Bayesian classifiers. The authors state two conditions that they show are provably sufficient for \"idealised models\" on \"idealised datasets\" to not have adversarial examples. (In the context of this paper, adversarial examples can either be nearby points that are classified differently with high confidence or points that are \"far\" from training data and classified with high confidence.) They complement their results with experiments. I believe that studying Bayesian models and their adversarial robustness is an interesting and promising direction. However I find the current paper lacking both in terms of conceptual and technical contributions. They consider \"idealized Bayesian Neural Networks (BNNs)\" to be continuous models with a confidence of 1.0 on the training set. Since these models are continuous, there exists an L2 ball of radius delta_x around each point x, where the classifier has high confidence (say 0.9). This in turn defines a region D' (the training examples plus the L2 balls around them) where the classifier has high confidence. By assuming that an \"idealized BNN\" has low certainty on all points outside D' they argue that these idealized models do not have adversarial examples. In my opinion, this statement follows directly from definitions and assumptions, hence having little technical depth or value. From a conceptual point of view, I don't see how this argument \"explains\" anything. It is fairly clear that classifiers only predicting confidently on points _very_ close to training examples will not have high-confidence adversarial examples. How do these results guide our design of ML models? How do they help us understand the shortcomings of our current models? Moreover, this argument is not directly connected to the accuracy of the model. The idealized models described are essentially only confident in regions very close to the training examples and are thus unlikely to confidently generalize to new, unseen inputs. In order to escape this issue, the authors propose an additional assumption. Namely that idealized models are invariant to a set of transformations T that we expect the model to be also invariant to. Hence by assuming that the \"idealized\" training set contains at least one input from each \"equivalence class\", the model will have good \"coverage\". As far as I understand, this assumption is not connected to the main theorem at all and is mostly a hand-wavy argument. Additionally, I don't see how this assumption is justified. Formally describing the set of invariances we expect natural data to have or even building models that are perfectly encoding these invariances by design is a very challenging problem that is unlikely to have a definite solution. Also, is it natural to expect that for each test input we will have a training input that is close to L2 norm to some transformation of the test input? Another major issue is that the value of delta_x (the L2 distance around training point x where the model assigns high confidence) is never discussed. This value is _very_ small for standard NN classifiers (this is what causes adversarial examples in the first place!). How do we expect models to deal with this issue? The experimental results of the paper are essentially for a toy setting. The dataset considered (\"ManifoldMNIST\") is essentially synthetic with access to the ground-truth probability of each sample. Moreover, the results on real datasets are unreliable. When evaluating the robustness of a model utilizing dropout, using a single gradient estimation query is not enough. Since the model is randomized, it is necessary to estimate the gradient using multiple queries. By using first-order attacks on these more reliable gradient estimates, an adversary can completely bypass a dropout \"defense\" (https://arxiv.org/abs/1802.00420). Overall, I find the contributions of the paper limited both technically and conceptually. I thus recommend rejection. [UPDATE]: Given the discussion with the authors, I agree that the paper outlines a potentially interesting research direction. As such, I have increased my score from 3 to 5 (and updated the review title). I still do not find the contribution of the paper significant enough to cross the ICLR bar. Comments to the authors: -- You cite certain detention methods for adversarial examples (Grosse et al. (2017), Feinman et al. (2017)) that have been shown to be ineffective (that is they can be bypassed by an adaptive attacker) by Carlini and Wagner (https://arxiv.org/abs/1705.07263) -- The organization of the paper could be improved. I didn't understand what the main contribution was until reading Theorem 1 (this is 6 pages into the paper). The introduction is fairly vague about your contributions. You should consider moving related work later in the paper (most of the discussion there is not directly related to your approach) and potentially shortening your background section. -- How is the discussion about Gaussian Processes connected with your results? -- Consider making the two conditions more prominent in the text. -- In Definition 5, the wording is confusing \"We define an idealized BNN to be a Bayesian idealized NN...\"", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the time spent reading our paper . We would like to dispute the reviewer \u2019 s extremely strong claim of \u201c tautological results of questionable significance \u201d . Our proof is by no means tautological ( see below ) , and a proof does not need to be intricate or over-complicated to shed light on a matter of interest . On the contrary - it is often the simplest of proofs that gives the most interesting insights . We would further like to draw the reviewer \u2019 s attention to the fact that this boils down to a matter of presentation . Papers can either over-complicate things to make the results look like an impressive theoretical contribution , or do their best to explain things with the clearest possible presentation . We chose the latter , making an effort to move as much construction outside of the proof into our well-designed premise setup in order to make the the proof clear . We also strongly disagree with the reviewer \u2019 s post-hoc view claiming `` this is obvious '' , a claim which was made with hindsight after reading our proof and problem definition , and which could be made with any proof after understanding its premise . We give many examples of new and previously undiscussed insights ( with many highlighted below ) . These might seem obvious after reading our submission , but these observations are overlooked in the literature , and they have important implications for the way we think about the problem . * * * We would ask the reviewer explicitly where in the literature they saw the result presented in this work , or any ideas following from the results we presented here ? -- The rest of the reviewer 's criticism falls into two parts - a criticism of the theoretical content of the paper , and of the relevance of the experiments to real world adversarial example defence . We will address these in order . * `` Moreover , this argument [ the reviewer 's own argument ] completely ignores the accuracy of the model . [ .. ] In order to escape this issue , the authors propose [ .. ] '' - The reviewer brings up an argument and then criticizes their own argument as lacking ! ( reviewer \u2019 s argument : `` continuous models with a confidence of 1 on the training set [ .. ] exists an L2 ball [ .. ] the classifier has high confidence [ .. ] low certainty on all points outside D \u2019 '' ) . This is exactly the reason why we introduce the concept of T , and which allows us to * * * expose a sufficient set of requirements for a model to be robust to adversarial examples * * * . The reviewer then criticizes the proof as tautological , and says they do not see how the results give anything of value . We mention above that we see Bayesian methods and their robustness as an important direction for future research on adversarial examples , a stance the reviewer agrees with . The purpose of the proof is to illustrate which properties of Bayesian classifiers are necessary for adversarial robustness . Simply being a Bayesian model , for example , is clearly insufficient - linear classifiers will have adversarial examples in high dimensions regardless of whether they are Bayesian or not because the model class is not expressive enough for the uncertainty to increase far away from the data . As mentioned in our reply to the second reviewer , we do not claim that our assumptions are realistic or that our proof is particularly difficult if we assume them . However , the assumptions do * * * shed light on which properties of Bayesian models are important for being robust to adversarial examples * * * . * `` this assumption [ existence of a set of transformations T both model and data are invariant to ] is not connected to the main theorem at all '' - We are not sure why the reviewer claims that the assumption is not connected to the main theorem ? As the reviewer themselves points out , without the assumption of invariance to a class of transformations it is possible for a model which is \u201c uncertain about all inputs not extremely close to a training input \u201d to satisfy our definition of being robust . This is clearly not a very interesting class of models . We introduce the invariance class as a way to avoid this degeneracy . Further , it is , as the reviewer points out , not a very realistic assumption for real models . We do not claim that it is an assumption that holds in practice - we observe that with this assumption we can prove robustness , and from there we proceed to examine the counterpart properties of non-idealised networks ( which we discuss in appendix C ) . [ response continued in a separate message ]"}, {"review_id": "B1eZRiC9YX-1", "review_text": "In this paper, the authors posit a class of discriminative Bayesian classifiers that, under sufficient conditions, do not have any adversarial examples. They distinguish between two sources of uncertainty (epistemic and aleatoric), and show that mutual information is a measure of epistemic uncertainty (essentially uncertainty due to missing regions of the input space). They then define an idealised Bayesian Neural Network (BNN), which is essentially a BNN that 1) outputs the correct class probability (and always with probability 1.0) for each input in the training set (and for each transformation of training inputs that the data distribution is invariant under), and 2) outputs a sufficiently high uncertainty for each input not in the union of delta-balls surrounding the training set points. Similarly, an example is defined to be adversarial if it has two characteristics: it 1) lies far from the training data but is classified with high output probability, and it 2) is classified with high output probability although it lies very close to another example that is classified with high output probability for the other class. Condition 1) of an idealised BNN prevents Definition 2) of an adversarial example using the fact that BNNs are continuous, and Condition 2) prevents Definition 1) of an adversarial example since it will prevent \"garbage\" examples by predicting with high uncertainty (of course I'm glossing over many important technical details, but these are the main ideas if I understand correctly). The authors backed up their theoretical findings with empirical experiments. In the synthetic MNIST examples, the authors show that adversarial attacks are indeed correlated with lower true input probability. They show that training with HMC results in high uncertainty for inputs not near the input-space, a quality certainly not shared with all other deep models (and another reason that Bayesian models should be preferred for preventing adversarial attacks). On the other hand, the out-of-sample uncertainty for training with dropout is not sufficient to prevent adversarial attacks, although the authors posit a form of dropout ensemble training to help prevent these vulnerabilities. The authors are tackling an important issue with theoretical and technical tools that are not used often enough in machine learning research. Much of the literature on adversarial attacks is focused on finding adversarial examples, without trying to find a unifying theory for why they work. They do a very solid exposition of previous work, and one of the strengths of this paper comes in presenting their findings in the context of previously discovered adversarial attacks, in particular that of the spheres data set. Ultimately, I'm not convinced of the usefulness of their theoretical findings. In particular, the assumption that the model is invariant to all transformations that the data distribution is invariant under is an unprovable assumption that can expose many real-world vulnerabilities. This is the case of the spheres data set without a rotation invariance from Gilmer et al. (2018). In the appendix, the authors mention that the data invariance property is key for making the proof non-vacuous, and I would agree. Without the data invariance property, the proof mainly relies on the fact that BNNs are continuous. The experiments are promising in support of the theory, but they do not seem to address this data invariance property. Indeed a model can prevent adversarial examples by predicting high uncertainty for all points that are not near the training examples, which Bayesian models are well equipped to do. I also thought the paper was unclear at times. It is not easy to write such a technical and theoretical paper and to clearly convey all the main points, but I think the paper would've benefited from more clarity. For example, the notation is overloaded in a way that made it difficult to understand the main proofs, such as not clearly explaining what is meant by I(w ; p) and not contrasting between the binary entropy function used for the entropy of a constant epsilon and the more general random variable entropy function. In contrast, I thought the appendices were clearer and helpful for explaining the main ideas. Additionally, Lemma 1 follows trivially from continuity of a BNN. Perhaps removing this and being clearer with notation would've allowed for more room to be clearer for the proof of Theorem 1. A more minor point that I think would be interesting is comparing training with HMC to training with variational inference. Do the holes that come from training with dropout still exist for VI? VI could certainly scale in a way that HMC could not, which perhaps would make the results more applicable. Overall, this is a promising theoretical paper although I'm not currently convinced of the real-world applications beyond the somewhat small examples in the experiments section. PROS -Importance of the issue -Exposition and relation to previous work -Experimental results (although these were for smaller data sets) -Appendices really helped aid the understanding CONS -Real world usefulness -Clarity ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the detailed feedback and for acknowledging that we tackled an important issue with theoretical and technical tools that are not used often enough in machine learning . The reviewer \u2019 s summary of our theoretical development is faithful , but we would like to address the reviewer \u2019 s comments on impact and real-world usefulness . This is followed by detailed answers to the minor comments and suggestions . -- Major points : First , for the first time in the literature ( as far as we know ) , we studied in detail a proposed mechanism to explain why BNNs have been observed to be empirically more robust to adversarial examples ( with such empirical observations given by ( Li & Gal , 2017 ; Feinman et al. , 2017 ; Rawat et al. , 2017 ; Carlini & Wagner , 2017 ) ) . We argue that idealised BNNs can not have adversarial examples , and related these to real-world conditions on BNNs . More specifically , see the sequence of experiments starting with idealised models with idealised data ( section 5.1 ) , going through idealised models with real world data ( table 1 ) , and finishing with real world data and inference ( dropout approx inference , section 5.2 , as well as appendix G - REAL-WORLD CATS VS DOGS CLASSIFICATION ) . Secondly , for the first time in the literature , we proved ( and gave empirical evidence in Fig 3 ) that a connection exists between epistemic uncertainty/input density and adversarial examples , a link which was only * hypothesised * previously ( Nguyen et al. , 2015 ) . Further , our proof highlighted sufficient conditions for robustness which resolved the inconsistency between ( Nguyen et al. , 2015 ; Li , 2018 ) and ( Gilmer et al. , 2018 ) . With regards to the the assumption that \u201c the model is invariant to all transformations that the data distribution is invariant under \u201d not being addressed : this assumption was used in the formal proof to get non-vacuous coverage . In appendix C we give a real-world alternative to this assumption , and in our experiments we demonstrate high coverage empirically ( since this property is mostly impossible to assess otherwise , as the reviewer mentioned ) . Apart from resolving an open dispute in the literature , our observations above also suggest future research directions towards tools which answer ( or approximate ) our conditions better . More specifically , we highlighted ( section 6 ) that the main difficulty with modern BNN robustness is not coverage ( as speculated by some ) , but rather that approximate inference doesn \u2019 t increase the uncertainty fast enough with practical BNN tools ( Fig 7 ) . We believe that our observations set major milestones for the community to tackle , and are not of `` no real world use '' . -- Minor comments : We appreciate the reviewer \u2019 s comments that the appendices were clear and helpful for explaining the main ideas , and acknowledge that our notation might be a bit cumbersome at times . We will improve notation as suggested . * The reviewer mentions in the review the fact that the experiments with HMC included in the paper do not address the data invariance property . This is not the aim of the experiments - rather this is to back up the claim that in the idealised case a BNN increases it \u2019 s uncertainty far from the data , as we assume in our idealised model ( \u201c we show that near-perfect epistemic uncertainty correlates to density under the image manifold \u201c ) . It is not obvious a priori that this property is actually true of neural networks in the same way that it is the case for kernel machines with stationary covariance kernels , for example . Previous work on Bayesian techniques for adversarial example detection has invariably used approximate inference , and these proposed defences have generally increased robustness but not eliminated adversarial examples , so we felt that experimentally demonstrating BNNs have this property in the case of idealised * inference * was important . * \u201c Indeed a model can prevent adversarial examples by predicting high uncertainty for all points that are not near the training examples \u201d : The reviewer correctly points out that a BNN of this kind could fall back to predicting high uncertainty far away from all the data if the invariance property does not hold ( e.g a model that predicts P = \u00bd everywhere is \u2018 robust \u2019 to adversarial examples but not very interestingly ) . The reviewer 's point that this is a very strong assumption is valid , and is explicitly discussed in the paper ( appendix B PROOF CRITIQUE ) . * I ( w , p ) is defined in as equation 1 - we will make this clearer in the text . * We are uncertain what the reviewer means by VI - as argued by ( Gal & Gharimani , 2015 ) dropout is a form of variational approximation . Other flavours of VI such as mean field exist as well , but based on previous results these are unlikely to be significantly better than dropout unless a far more expressive varational distribution is used , leading to scaling issues ( Gal , 2016 ) . We agree that a more scalable method would be highly desirable ( as discussed in section 6 ) ."}, {"review_id": "B1eZRiC9YX-2", "review_text": "This paper extends the definition of adversarial examples to the ones that are \u201cfar\u201d from the training data, and provides two conditions that are sufficient to guarantee the non-existence of adversarial examples. The core idea of the paper is using the epistemic uncertainty, that is the mutual information measuring the reduction of the uncertainty given an observation of the data, to detect such faraway data. The authors provided simulation studies to support their arguments. It is interesting to connect robustness with BNN. Using the mutual information to detect the \u201cfaraway\u201d datapoint is also interesting. But I have some concerns about the significance of the paper: 1. The investigation of this paper seems shallow and vague. (1). Overall, I don\u2019t see the investigation on the \u201ctypical\u201d definition of adversarial examples. The focus of the paper is rather on detecting \u201cfaraway\u201d data points. The nearby perturbation part is taken care by the concept of \u201call possible transformations\u201d which is actually vague. (2). Theorem 1 is basically repeating the definition of adversarial examples. The conditions in the theorem hardly have practical guidance: while they are sufficient conditions, all transformations etc.. seem far from being necessary conditions, which raises the question of why this theory is useful? Also how practical for the notion of \u201cidealized NN\u201d? (3). What about the neighbourhood around the true data manifold? How would the model succeed to generalize to the true data manifold, yet fail to generalize to the neighbourhood of the manifold in the space? Delta ball is not very relevant to the \u201ctypical\u201d definition of adversarial examples, as we have no control on \\delta at all. 2. While the simulations support the concepts in section 4, it is quite far from the real data with the \u201ctypical\u201d adversarial examples. I also find it difficult to follow the exact trend of the paper, maybe due to my lack of background in bayesian models. 1. In the second paragraph of section 3, how is the Gaussian processes and its relation to BNN contributing to the results of this paper? 2. What is the rigorous definition for \\eta in definition 1? 3. What is the role of $\\mathcal{T}$, all the transformations $T$ that introduce no ambiguity, in Theorem 1. Why this condition is important/essential here? 4. What is the D in the paragraph right after Definition 4? What is D\u2019 in Theorem 1? 5. Section references need to be fixed. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for spending time reading our paper . The reviewer seems to have misunderstood large portions of our submission , and we would like to clarify some of these misunderstood points . * The reviewer wrongly characterises the paper as focusing on \u2018 faraway \u2019 or garbage examples only , rather than adversarial examples generated by nearby perturbations ( `` Overall , I don \u2019 t see the investigation on the \u201c typical \u201d definition of adversarial examples '' ; `` The focus of the paper is rather on detecting \u201c faraway \u201d data points . `` ) . This is incorrect ( see Definition 1 and Theorem 1 ) . We think the confusion might have been caused because the reviewer misunderstood our notation x + \u03b7 ( a standard notation in the field for a small perturbation \u03b7 around x , see eg ( Papernot et al. , 2016 ; Goodfellow et al. , 2014 ) , and our Definition 1 which states \u201c small perturbation \u03b7 \u201d ) . Theorem one clearly deals with the case of x + \u03b7 as well as \u201c garbage points \u201d . * The reviewer writes \u2018 the nearby perturbation part is taken care of by the concept of all possible transformations \u2019 . This is not what we intended . Rather , the concept of the invariances of the dataset is included in order to give the idealised classifier non-vacous coverage . Without this condition , it is possible for an idealised model to \u201c reject all points it has never seen before \u201d as adversarial . For example , consider a classifier that keeps the dataset { x_i , y_i } in memory , and classifies new points x by returning y_i with probability 1 if there exists an i such that x_i = x , and returns a uniform probability vector otherwise . This satisfies our definition of \u2018 high uncertainty away from the training data \u2019 but is clearly not very useful . Introducing the class of transformations gives the idealised classifier * in the proof * the ability to generalise in a non-trivial way , though it is clearly a very strong assumption that does not hold in real networks . This assumption , as well as real-world alternatives to high coverage , is discussed in Appendix C. * The reviewers second point is that our conditions are very strong , and it is not clear they are necessary . We do not claim that the conditions are * necessary * - our results are giving * sufficient * conditions for robustness ( as our paper title says ) . Conditions do not need to be necessary for them to be useful . We think that our conditions provide a useful framework to set directions for future work . We would point out that as far as we know there are no other attempts to formalise what an idealised model without adversarial examples would be like , and we think it is fair to say that empirical attempts to find sufficient conditions by producing a model which does not exhibit adversarial examples have been unsuccessful as of yet . * We are unsure what the reviewer means by \u2018 generalising to the neighbourhood of the true data manifold \u2019 . If by the true data manifold they mean the support of P ( x ) , then \u2018 generalising \u2019 outside this region is more or less meaningless . This is mentioned in the related work section about oracles - \u201c can a point outside the true data distribution be assigned a label in a meaningful way ? \u201d . We would also highlight that the delta balls are a subset of the neighbourhood around the true data manifold . * Lastly , the simulations are not designed to resemble real data necessarily , but to show under some idealised conditions ( but less idealised than used in the proof ) that the properties described do indeed approximately hold for realisable networks . Indeed , one of the experiments ( on manifold mnist ) is designed to address the previous point about the data support , by showing that adversarial examples are being \u2018 moved away \u2019 from the data distribution . In Appendix G we give results on real world cats-vs-dogs classification as well . -- To address the questions the reviewer had about the trend of the paper : * GPs are the infinite limit of BNNs , and share many properties with them ( Matthews et al. , 2017 ) . The point of the reference to GPs is that these are an example of a model for which the uncertainty is easy to evaluate . This is then used to illustrate what we hope to obtain in the case of idealised neural networks ( gp-like uncertainty ) . * Eta is a vector in R^dim , the image space , where we follow standard notation as mentioned above . There is no commonly agreed on definition of adversarial examples more rigorous than the one we provide . However , the argument in the paper does not rely on any properties of eta other than there is * some * eta we consider \u2018 small enough \u2019 to be adversarial . * As mentioned above , this condition is essential to avoid a classifier that is uncertain everywhere from satisfying our definition of an idealised model . * D refers to the training set , which is mentioned above equation 1 . D \u2019 refers to the union of delta balls around the training set , which is in definition 5 . * Thanks for pointing this out , we will fix this in our next draft ."}], "0": {"review_id": "B1eZRiC9YX-0", "review_text": "The paper studies the adversarial robustness of Bayesian classifiers. The authors state two conditions that they show are provably sufficient for \"idealised models\" on \"idealised datasets\" to not have adversarial examples. (In the context of this paper, adversarial examples can either be nearby points that are classified differently with high confidence or points that are \"far\" from training data and classified with high confidence.) They complement their results with experiments. I believe that studying Bayesian models and their adversarial robustness is an interesting and promising direction. However I find the current paper lacking both in terms of conceptual and technical contributions. They consider \"idealized Bayesian Neural Networks (BNNs)\" to be continuous models with a confidence of 1.0 on the training set. Since these models are continuous, there exists an L2 ball of radius delta_x around each point x, where the classifier has high confidence (say 0.9). This in turn defines a region D' (the training examples plus the L2 balls around them) where the classifier has high confidence. By assuming that an \"idealized BNN\" has low certainty on all points outside D' they argue that these idealized models do not have adversarial examples. In my opinion, this statement follows directly from definitions and assumptions, hence having little technical depth or value. From a conceptual point of view, I don't see how this argument \"explains\" anything. It is fairly clear that classifiers only predicting confidently on points _very_ close to training examples will not have high-confidence adversarial examples. How do these results guide our design of ML models? How do they help us understand the shortcomings of our current models? Moreover, this argument is not directly connected to the accuracy of the model. The idealized models described are essentially only confident in regions very close to the training examples and are thus unlikely to confidently generalize to new, unseen inputs. In order to escape this issue, the authors propose an additional assumption. Namely that idealized models are invariant to a set of transformations T that we expect the model to be also invariant to. Hence by assuming that the \"idealized\" training set contains at least one input from each \"equivalence class\", the model will have good \"coverage\". As far as I understand, this assumption is not connected to the main theorem at all and is mostly a hand-wavy argument. Additionally, I don't see how this assumption is justified. Formally describing the set of invariances we expect natural data to have or even building models that are perfectly encoding these invariances by design is a very challenging problem that is unlikely to have a definite solution. Also, is it natural to expect that for each test input we will have a training input that is close to L2 norm to some transformation of the test input? Another major issue is that the value of delta_x (the L2 distance around training point x where the model assigns high confidence) is never discussed. This value is _very_ small for standard NN classifiers (this is what causes adversarial examples in the first place!). How do we expect models to deal with this issue? The experimental results of the paper are essentially for a toy setting. The dataset considered (\"ManifoldMNIST\") is essentially synthetic with access to the ground-truth probability of each sample. Moreover, the results on real datasets are unreliable. When evaluating the robustness of a model utilizing dropout, using a single gradient estimation query is not enough. Since the model is randomized, it is necessary to estimate the gradient using multiple queries. By using first-order attacks on these more reliable gradient estimates, an adversary can completely bypass a dropout \"defense\" (https://arxiv.org/abs/1802.00420). Overall, I find the contributions of the paper limited both technically and conceptually. I thus recommend rejection. [UPDATE]: Given the discussion with the authors, I agree that the paper outlines a potentially interesting research direction. As such, I have increased my score from 3 to 5 (and updated the review title). I still do not find the contribution of the paper significant enough to cross the ICLR bar. Comments to the authors: -- You cite certain detention methods for adversarial examples (Grosse et al. (2017), Feinman et al. (2017)) that have been shown to be ineffective (that is they can be bypassed by an adaptive attacker) by Carlini and Wagner (https://arxiv.org/abs/1705.07263) -- The organization of the paper could be improved. I didn't understand what the main contribution was until reading Theorem 1 (this is 6 pages into the paper). The introduction is fairly vague about your contributions. You should consider moving related work later in the paper (most of the discussion there is not directly related to your approach) and potentially shortening your background section. -- How is the discussion about Gaussian Processes connected with your results? -- Consider making the two conditions more prominent in the text. -- In Definition 5, the wording is confusing \"We define an idealized BNN to be a Bayesian idealized NN...\"", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the time spent reading our paper . We would like to dispute the reviewer \u2019 s extremely strong claim of \u201c tautological results of questionable significance \u201d . Our proof is by no means tautological ( see below ) , and a proof does not need to be intricate or over-complicated to shed light on a matter of interest . On the contrary - it is often the simplest of proofs that gives the most interesting insights . We would further like to draw the reviewer \u2019 s attention to the fact that this boils down to a matter of presentation . Papers can either over-complicate things to make the results look like an impressive theoretical contribution , or do their best to explain things with the clearest possible presentation . We chose the latter , making an effort to move as much construction outside of the proof into our well-designed premise setup in order to make the the proof clear . We also strongly disagree with the reviewer \u2019 s post-hoc view claiming `` this is obvious '' , a claim which was made with hindsight after reading our proof and problem definition , and which could be made with any proof after understanding its premise . We give many examples of new and previously undiscussed insights ( with many highlighted below ) . These might seem obvious after reading our submission , but these observations are overlooked in the literature , and they have important implications for the way we think about the problem . * * * We would ask the reviewer explicitly where in the literature they saw the result presented in this work , or any ideas following from the results we presented here ? -- The rest of the reviewer 's criticism falls into two parts - a criticism of the theoretical content of the paper , and of the relevance of the experiments to real world adversarial example defence . We will address these in order . * `` Moreover , this argument [ the reviewer 's own argument ] completely ignores the accuracy of the model . [ .. ] In order to escape this issue , the authors propose [ .. ] '' - The reviewer brings up an argument and then criticizes their own argument as lacking ! ( reviewer \u2019 s argument : `` continuous models with a confidence of 1 on the training set [ .. ] exists an L2 ball [ .. ] the classifier has high confidence [ .. ] low certainty on all points outside D \u2019 '' ) . This is exactly the reason why we introduce the concept of T , and which allows us to * * * expose a sufficient set of requirements for a model to be robust to adversarial examples * * * . The reviewer then criticizes the proof as tautological , and says they do not see how the results give anything of value . We mention above that we see Bayesian methods and their robustness as an important direction for future research on adversarial examples , a stance the reviewer agrees with . The purpose of the proof is to illustrate which properties of Bayesian classifiers are necessary for adversarial robustness . Simply being a Bayesian model , for example , is clearly insufficient - linear classifiers will have adversarial examples in high dimensions regardless of whether they are Bayesian or not because the model class is not expressive enough for the uncertainty to increase far away from the data . As mentioned in our reply to the second reviewer , we do not claim that our assumptions are realistic or that our proof is particularly difficult if we assume them . However , the assumptions do * * * shed light on which properties of Bayesian models are important for being robust to adversarial examples * * * . * `` this assumption [ existence of a set of transformations T both model and data are invariant to ] is not connected to the main theorem at all '' - We are not sure why the reviewer claims that the assumption is not connected to the main theorem ? As the reviewer themselves points out , without the assumption of invariance to a class of transformations it is possible for a model which is \u201c uncertain about all inputs not extremely close to a training input \u201d to satisfy our definition of being robust . This is clearly not a very interesting class of models . We introduce the invariance class as a way to avoid this degeneracy . Further , it is , as the reviewer points out , not a very realistic assumption for real models . We do not claim that it is an assumption that holds in practice - we observe that with this assumption we can prove robustness , and from there we proceed to examine the counterpart properties of non-idealised networks ( which we discuss in appendix C ) . [ response continued in a separate message ]"}, "1": {"review_id": "B1eZRiC9YX-1", "review_text": "In this paper, the authors posit a class of discriminative Bayesian classifiers that, under sufficient conditions, do not have any adversarial examples. They distinguish between two sources of uncertainty (epistemic and aleatoric), and show that mutual information is a measure of epistemic uncertainty (essentially uncertainty due to missing regions of the input space). They then define an idealised Bayesian Neural Network (BNN), which is essentially a BNN that 1) outputs the correct class probability (and always with probability 1.0) for each input in the training set (and for each transformation of training inputs that the data distribution is invariant under), and 2) outputs a sufficiently high uncertainty for each input not in the union of delta-balls surrounding the training set points. Similarly, an example is defined to be adversarial if it has two characteristics: it 1) lies far from the training data but is classified with high output probability, and it 2) is classified with high output probability although it lies very close to another example that is classified with high output probability for the other class. Condition 1) of an idealised BNN prevents Definition 2) of an adversarial example using the fact that BNNs are continuous, and Condition 2) prevents Definition 1) of an adversarial example since it will prevent \"garbage\" examples by predicting with high uncertainty (of course I'm glossing over many important technical details, but these are the main ideas if I understand correctly). The authors backed up their theoretical findings with empirical experiments. In the synthetic MNIST examples, the authors show that adversarial attacks are indeed correlated with lower true input probability. They show that training with HMC results in high uncertainty for inputs not near the input-space, a quality certainly not shared with all other deep models (and another reason that Bayesian models should be preferred for preventing adversarial attacks). On the other hand, the out-of-sample uncertainty for training with dropout is not sufficient to prevent adversarial attacks, although the authors posit a form of dropout ensemble training to help prevent these vulnerabilities. The authors are tackling an important issue with theoretical and technical tools that are not used often enough in machine learning research. Much of the literature on adversarial attacks is focused on finding adversarial examples, without trying to find a unifying theory for why they work. They do a very solid exposition of previous work, and one of the strengths of this paper comes in presenting their findings in the context of previously discovered adversarial attacks, in particular that of the spheres data set. Ultimately, I'm not convinced of the usefulness of their theoretical findings. In particular, the assumption that the model is invariant to all transformations that the data distribution is invariant under is an unprovable assumption that can expose many real-world vulnerabilities. This is the case of the spheres data set without a rotation invariance from Gilmer et al. (2018). In the appendix, the authors mention that the data invariance property is key for making the proof non-vacuous, and I would agree. Without the data invariance property, the proof mainly relies on the fact that BNNs are continuous. The experiments are promising in support of the theory, but they do not seem to address this data invariance property. Indeed a model can prevent adversarial examples by predicting high uncertainty for all points that are not near the training examples, which Bayesian models are well equipped to do. I also thought the paper was unclear at times. It is not easy to write such a technical and theoretical paper and to clearly convey all the main points, but I think the paper would've benefited from more clarity. For example, the notation is overloaded in a way that made it difficult to understand the main proofs, such as not clearly explaining what is meant by I(w ; p) and not contrasting between the binary entropy function used for the entropy of a constant epsilon and the more general random variable entropy function. In contrast, I thought the appendices were clearer and helpful for explaining the main ideas. Additionally, Lemma 1 follows trivially from continuity of a BNN. Perhaps removing this and being clearer with notation would've allowed for more room to be clearer for the proof of Theorem 1. A more minor point that I think would be interesting is comparing training with HMC to training with variational inference. Do the holes that come from training with dropout still exist for VI? VI could certainly scale in a way that HMC could not, which perhaps would make the results more applicable. Overall, this is a promising theoretical paper although I'm not currently convinced of the real-world applications beyond the somewhat small examples in the experiments section. PROS -Importance of the issue -Exposition and relation to previous work -Experimental results (although these were for smaller data sets) -Appendices really helped aid the understanding CONS -Real world usefulness -Clarity ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the detailed feedback and for acknowledging that we tackled an important issue with theoretical and technical tools that are not used often enough in machine learning . The reviewer \u2019 s summary of our theoretical development is faithful , but we would like to address the reviewer \u2019 s comments on impact and real-world usefulness . This is followed by detailed answers to the minor comments and suggestions . -- Major points : First , for the first time in the literature ( as far as we know ) , we studied in detail a proposed mechanism to explain why BNNs have been observed to be empirically more robust to adversarial examples ( with such empirical observations given by ( Li & Gal , 2017 ; Feinman et al. , 2017 ; Rawat et al. , 2017 ; Carlini & Wagner , 2017 ) ) . We argue that idealised BNNs can not have adversarial examples , and related these to real-world conditions on BNNs . More specifically , see the sequence of experiments starting with idealised models with idealised data ( section 5.1 ) , going through idealised models with real world data ( table 1 ) , and finishing with real world data and inference ( dropout approx inference , section 5.2 , as well as appendix G - REAL-WORLD CATS VS DOGS CLASSIFICATION ) . Secondly , for the first time in the literature , we proved ( and gave empirical evidence in Fig 3 ) that a connection exists between epistemic uncertainty/input density and adversarial examples , a link which was only * hypothesised * previously ( Nguyen et al. , 2015 ) . Further , our proof highlighted sufficient conditions for robustness which resolved the inconsistency between ( Nguyen et al. , 2015 ; Li , 2018 ) and ( Gilmer et al. , 2018 ) . With regards to the the assumption that \u201c the model is invariant to all transformations that the data distribution is invariant under \u201d not being addressed : this assumption was used in the formal proof to get non-vacuous coverage . In appendix C we give a real-world alternative to this assumption , and in our experiments we demonstrate high coverage empirically ( since this property is mostly impossible to assess otherwise , as the reviewer mentioned ) . Apart from resolving an open dispute in the literature , our observations above also suggest future research directions towards tools which answer ( or approximate ) our conditions better . More specifically , we highlighted ( section 6 ) that the main difficulty with modern BNN robustness is not coverage ( as speculated by some ) , but rather that approximate inference doesn \u2019 t increase the uncertainty fast enough with practical BNN tools ( Fig 7 ) . We believe that our observations set major milestones for the community to tackle , and are not of `` no real world use '' . -- Minor comments : We appreciate the reviewer \u2019 s comments that the appendices were clear and helpful for explaining the main ideas , and acknowledge that our notation might be a bit cumbersome at times . We will improve notation as suggested . * The reviewer mentions in the review the fact that the experiments with HMC included in the paper do not address the data invariance property . This is not the aim of the experiments - rather this is to back up the claim that in the idealised case a BNN increases it \u2019 s uncertainty far from the data , as we assume in our idealised model ( \u201c we show that near-perfect epistemic uncertainty correlates to density under the image manifold \u201c ) . It is not obvious a priori that this property is actually true of neural networks in the same way that it is the case for kernel machines with stationary covariance kernels , for example . Previous work on Bayesian techniques for adversarial example detection has invariably used approximate inference , and these proposed defences have generally increased robustness but not eliminated adversarial examples , so we felt that experimentally demonstrating BNNs have this property in the case of idealised * inference * was important . * \u201c Indeed a model can prevent adversarial examples by predicting high uncertainty for all points that are not near the training examples \u201d : The reviewer correctly points out that a BNN of this kind could fall back to predicting high uncertainty far away from all the data if the invariance property does not hold ( e.g a model that predicts P = \u00bd everywhere is \u2018 robust \u2019 to adversarial examples but not very interestingly ) . The reviewer 's point that this is a very strong assumption is valid , and is explicitly discussed in the paper ( appendix B PROOF CRITIQUE ) . * I ( w , p ) is defined in as equation 1 - we will make this clearer in the text . * We are uncertain what the reviewer means by VI - as argued by ( Gal & Gharimani , 2015 ) dropout is a form of variational approximation . Other flavours of VI such as mean field exist as well , but based on previous results these are unlikely to be significantly better than dropout unless a far more expressive varational distribution is used , leading to scaling issues ( Gal , 2016 ) . We agree that a more scalable method would be highly desirable ( as discussed in section 6 ) ."}, "2": {"review_id": "B1eZRiC9YX-2", "review_text": "This paper extends the definition of adversarial examples to the ones that are \u201cfar\u201d from the training data, and provides two conditions that are sufficient to guarantee the non-existence of adversarial examples. The core idea of the paper is using the epistemic uncertainty, that is the mutual information measuring the reduction of the uncertainty given an observation of the data, to detect such faraway data. The authors provided simulation studies to support their arguments. It is interesting to connect robustness with BNN. Using the mutual information to detect the \u201cfaraway\u201d datapoint is also interesting. But I have some concerns about the significance of the paper: 1. The investigation of this paper seems shallow and vague. (1). Overall, I don\u2019t see the investigation on the \u201ctypical\u201d definition of adversarial examples. The focus of the paper is rather on detecting \u201cfaraway\u201d data points. The nearby perturbation part is taken care by the concept of \u201call possible transformations\u201d which is actually vague. (2). Theorem 1 is basically repeating the definition of adversarial examples. The conditions in the theorem hardly have practical guidance: while they are sufficient conditions, all transformations etc.. seem far from being necessary conditions, which raises the question of why this theory is useful? Also how practical for the notion of \u201cidealized NN\u201d? (3). What about the neighbourhood around the true data manifold? How would the model succeed to generalize to the true data manifold, yet fail to generalize to the neighbourhood of the manifold in the space? Delta ball is not very relevant to the \u201ctypical\u201d definition of adversarial examples, as we have no control on \\delta at all. 2. While the simulations support the concepts in section 4, it is quite far from the real data with the \u201ctypical\u201d adversarial examples. I also find it difficult to follow the exact trend of the paper, maybe due to my lack of background in bayesian models. 1. In the second paragraph of section 3, how is the Gaussian processes and its relation to BNN contributing to the results of this paper? 2. What is the rigorous definition for \\eta in definition 1? 3. What is the role of $\\mathcal{T}$, all the transformations $T$ that introduce no ambiguity, in Theorem 1. Why this condition is important/essential here? 4. What is the D in the paragraph right after Definition 4? What is D\u2019 in Theorem 1? 5. Section references need to be fixed. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for spending time reading our paper . The reviewer seems to have misunderstood large portions of our submission , and we would like to clarify some of these misunderstood points . * The reviewer wrongly characterises the paper as focusing on \u2018 faraway \u2019 or garbage examples only , rather than adversarial examples generated by nearby perturbations ( `` Overall , I don \u2019 t see the investigation on the \u201c typical \u201d definition of adversarial examples '' ; `` The focus of the paper is rather on detecting \u201c faraway \u201d data points . `` ) . This is incorrect ( see Definition 1 and Theorem 1 ) . We think the confusion might have been caused because the reviewer misunderstood our notation x + \u03b7 ( a standard notation in the field for a small perturbation \u03b7 around x , see eg ( Papernot et al. , 2016 ; Goodfellow et al. , 2014 ) , and our Definition 1 which states \u201c small perturbation \u03b7 \u201d ) . Theorem one clearly deals with the case of x + \u03b7 as well as \u201c garbage points \u201d . * The reviewer writes \u2018 the nearby perturbation part is taken care of by the concept of all possible transformations \u2019 . This is not what we intended . Rather , the concept of the invariances of the dataset is included in order to give the idealised classifier non-vacous coverage . Without this condition , it is possible for an idealised model to \u201c reject all points it has never seen before \u201d as adversarial . For example , consider a classifier that keeps the dataset { x_i , y_i } in memory , and classifies new points x by returning y_i with probability 1 if there exists an i such that x_i = x , and returns a uniform probability vector otherwise . This satisfies our definition of \u2018 high uncertainty away from the training data \u2019 but is clearly not very useful . Introducing the class of transformations gives the idealised classifier * in the proof * the ability to generalise in a non-trivial way , though it is clearly a very strong assumption that does not hold in real networks . This assumption , as well as real-world alternatives to high coverage , is discussed in Appendix C. * The reviewers second point is that our conditions are very strong , and it is not clear they are necessary . We do not claim that the conditions are * necessary * - our results are giving * sufficient * conditions for robustness ( as our paper title says ) . Conditions do not need to be necessary for them to be useful . We think that our conditions provide a useful framework to set directions for future work . We would point out that as far as we know there are no other attempts to formalise what an idealised model without adversarial examples would be like , and we think it is fair to say that empirical attempts to find sufficient conditions by producing a model which does not exhibit adversarial examples have been unsuccessful as of yet . * We are unsure what the reviewer means by \u2018 generalising to the neighbourhood of the true data manifold \u2019 . If by the true data manifold they mean the support of P ( x ) , then \u2018 generalising \u2019 outside this region is more or less meaningless . This is mentioned in the related work section about oracles - \u201c can a point outside the true data distribution be assigned a label in a meaningful way ? \u201d . We would also highlight that the delta balls are a subset of the neighbourhood around the true data manifold . * Lastly , the simulations are not designed to resemble real data necessarily , but to show under some idealised conditions ( but less idealised than used in the proof ) that the properties described do indeed approximately hold for realisable networks . Indeed , one of the experiments ( on manifold mnist ) is designed to address the previous point about the data support , by showing that adversarial examples are being \u2018 moved away \u2019 from the data distribution . In Appendix G we give results on real world cats-vs-dogs classification as well . -- To address the questions the reviewer had about the trend of the paper : * GPs are the infinite limit of BNNs , and share many properties with them ( Matthews et al. , 2017 ) . The point of the reference to GPs is that these are an example of a model for which the uncertainty is easy to evaluate . This is then used to illustrate what we hope to obtain in the case of idealised neural networks ( gp-like uncertainty ) . * Eta is a vector in R^dim , the image space , where we follow standard notation as mentioned above . There is no commonly agreed on definition of adversarial examples more rigorous than the one we provide . However , the argument in the paper does not rely on any properties of eta other than there is * some * eta we consider \u2018 small enough \u2019 to be adversarial . * As mentioned above , this condition is essential to avoid a classifier that is uncertain everywhere from satisfying our definition of an idealised model . * D refers to the training set , which is mentioned above equation 1 . D \u2019 refers to the union of delta balls around the training set , which is in definition 5 . * Thanks for pointing this out , we will fix this in our next draft ."}}