{"year": "2017", "forum": "SJMGPrcle", "title": "Learning to Navigate in Complex Environments", "decision": "Accept (Poster)", "meta_review": "The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:\n \n \n Pros:\n + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel\n + Good results on a challenge task of maze navigation from visual data\n \n Cons:\n - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple \"this is what worked for this domain\"", "reviews": [{"review_id": "SJMGPrcle-0", "review_text": "This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks. The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general. As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.", "rating": "7: Good paper, accept", "reply_text": "I am not an author of this paper . However , out of curiosity , I would like to know from AnonReviewer1 a small clarification about a couple of sentences : i ) `` While the use of auxiliary tasks to improve training of models including RL agents is not new '' , the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks . ii ) `` However , the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks . '' The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation ( e.g.the value of other visual / geometry-based tasks ) , or on auxiliary tasks with RL in general . I would like to point out that the only other Deep RL paper with auxiliary tasks is from Jaderberg et al , and a paper from DeepMind itself , and was a `` parallel '' submission to ICLR 17 ( the same conference ) . So , I do not think the novelty of one paper should be judged based on the existence of the other since they were both submissions made at the same time to the same conference . I would be interested to know if there has been some other work in Deep RL with auxiliary tasks , preceding both these works , so that the comment is still applicable . I do not have any comments on the appropriateness of the rating . I am just curious to know if there have been previous works ."}, {"review_id": "SJMGPrcle-1", "review_text": "I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task! ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding the reviewer \u2019 s comment about supervised vs. unsupervised terminology , we agree that the nomenclature can be confusing . While we do not agree with the reviewer 's views on supervised or unsupervised learning , we have removed the qualifier from the paper , and simply use \u201c auxiliary task \u201d . We emphasise that the fundamental approach and results are independent from the nomenclature used . In order to evaluate the reviewer \u2019 s suggestion that auxiliary tasks accelerate learning through increased training , leading to faster representation learning , we did asymptotic performance analysis on the existing agents , documented in Appendix section C.5 \u201c Asymptotic performance of the agents \u201d . Note that the asymptotic performance ( see Table 3 ) still shows a significant advantage , indicating that the contribution of these tasks does more than simply accelerate learning . As Table 3 shows , the performance and the position accuracy of the baseline agent significantly increase after twice the number of training steps ( going from 57 points to 90 points , and from 33.4 % to 66.5 % ) , but still do not reach the performance and accuracy of the Nav A3C+D2 agent . We also provide new evidence for the specific benefit of the depth prediction task : i.e.that in our experiments , it provides a greater performance benefit than the reward prediction task introduced in [ Jaderberg et al. , 2016 ICLR submission ] ."}, {"review_id": "SJMGPrcle-2", "review_text": "This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics. Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed. Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks. While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence. One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters. Another downside is that the authors dismiss navigation literature as \"not RL\". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . We provided additional analysis , in Appendix section C.4 , to address your comments . For each of the experiments in this paper , 64 replicas were run with hyperparameters ( learning rate , entropy cost ) sampled from the same interval . Figure 12 shows that the Nav architectures with auxiliary tasks achieve higher results for a comparatively larger number of replicas , suggesting that auxiliary tasks make learning more robust to the choice of hyperparameters - in line with the reviewer \u2019 s intuition . This observation is particularly strong for the small static maze ( more than a third of the replicas for FF A3C and LSTM A3C baselines do not even reach the goal , whereas less than 10 Nav agents out of 64 replicas suffer from this ) . In this paper we focused on the potential benefits of auxiliary tasks in enhancing the navigational capacities of agents that use deep RL to map pixels directly to actions - rather than designing a new state-of-the-art navigation system . Our discussion of the literature reflected this focus , but was not intended to be dismissive of other navigation approaches such as SLAM ."}], "0": {"review_id": "SJMGPrcle-0", "review_text": "This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks. The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general. As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.", "rating": "7: Good paper, accept", "reply_text": "I am not an author of this paper . However , out of curiosity , I would like to know from AnonReviewer1 a small clarification about a couple of sentences : i ) `` While the use of auxiliary tasks to improve training of models including RL agents is not new '' , the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks . ii ) `` However , the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks . '' The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation ( e.g.the value of other visual / geometry-based tasks ) , or on auxiliary tasks with RL in general . I would like to point out that the only other Deep RL paper with auxiliary tasks is from Jaderberg et al , and a paper from DeepMind itself , and was a `` parallel '' submission to ICLR 17 ( the same conference ) . So , I do not think the novelty of one paper should be judged based on the existence of the other since they were both submissions made at the same time to the same conference . I would be interested to know if there has been some other work in Deep RL with auxiliary tasks , preceding both these works , so that the comment is still applicable . I do not have any comments on the appropriateness of the rating . I am just curious to know if there have been previous works ."}, "1": {"review_id": "SJMGPrcle-1", "review_text": "I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task! ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding the reviewer \u2019 s comment about supervised vs. unsupervised terminology , we agree that the nomenclature can be confusing . While we do not agree with the reviewer 's views on supervised or unsupervised learning , we have removed the qualifier from the paper , and simply use \u201c auxiliary task \u201d . We emphasise that the fundamental approach and results are independent from the nomenclature used . In order to evaluate the reviewer \u2019 s suggestion that auxiliary tasks accelerate learning through increased training , leading to faster representation learning , we did asymptotic performance analysis on the existing agents , documented in Appendix section C.5 \u201c Asymptotic performance of the agents \u201d . Note that the asymptotic performance ( see Table 3 ) still shows a significant advantage , indicating that the contribution of these tasks does more than simply accelerate learning . As Table 3 shows , the performance and the position accuracy of the baseline agent significantly increase after twice the number of training steps ( going from 57 points to 90 points , and from 33.4 % to 66.5 % ) , but still do not reach the performance and accuracy of the Nav A3C+D2 agent . We also provide new evidence for the specific benefit of the depth prediction task : i.e.that in our experiments , it provides a greater performance benefit than the reward prediction task introduced in [ Jaderberg et al. , 2016 ICLR submission ] ."}, "2": {"review_id": "SJMGPrcle-2", "review_text": "This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics. Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed. Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks. While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence. One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters. Another downside is that the authors dismiss navigation literature as \"not RL\". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . We provided additional analysis , in Appendix section C.4 , to address your comments . For each of the experiments in this paper , 64 replicas were run with hyperparameters ( learning rate , entropy cost ) sampled from the same interval . Figure 12 shows that the Nav architectures with auxiliary tasks achieve higher results for a comparatively larger number of replicas , suggesting that auxiliary tasks make learning more robust to the choice of hyperparameters - in line with the reviewer \u2019 s intuition . This observation is particularly strong for the small static maze ( more than a third of the replicas for FF A3C and LSTM A3C baselines do not even reach the goal , whereas less than 10 Nav agents out of 64 replicas suffer from this ) . In this paper we focused on the potential benefits of auxiliary tasks in enhancing the navigational capacities of agents that use deep RL to map pixels directly to actions - rather than designing a new state-of-the-art navigation system . Our discussion of the literature reflected this focus , but was not intended to be dismissive of other navigation approaches such as SLAM ."}}