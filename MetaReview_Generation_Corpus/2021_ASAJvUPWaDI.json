{"year": "2021", "forum": "ASAJvUPWaDI", "title": "A Near-Optimal Recipe for Debiasing Trained Machine Learning Models", "decision": "Reject", "meta_review": "All reviewers feel this paper addresses and important topic, and has many merits. However, it is difficult to recommend publication at this time. The primary concern is that the paper has its theoretical optimality as an important contribution, but the reviewers and myself (in a non-public thread) were unable to verify the correctness of the proofs. In part unfortunately this is due to edits to the proofs happening late in the revision period, too late for further discussion with the authors. Some of the particular questions in the proof of theorem 1 (appendix B) include: clarifying the value of $\\rho$ which makes the unnumbered equation above equation (6) equivalent to definition 1, and in particular whether the $1/|X_k|$ term should be inside or outside the absolute value; and clarifying various undefined symbols which are introduced in the equation at the top of page 13, but are never defined, including $M$, $b$, and $z_i$. Reviewers also had some concern that the algorithm should be benchmarked against more recent / better performant baselines than Kamiran et al. (2012).", "reviews": [{"review_id": "ASAJvUPWaDI-0", "review_text": "Evaluation and improvement of fairness of machine learning algorithms is a very important issue . To this end , the authors of this paper propose a post-processing algorithm to enforce fairness in a narrowly defined notion of fairness . Unfortunately , I have serious concerns about the validity of the results and conclusions of the paper , and hence I can not recommend the paper to be accepted . [ * * Definition * * ] This entire paper is only applicable to a narrow definition of fairness in Definition 1 . Given that $ a $ and $ b $ are binary , Definition 1 ( page 2 ) is essentially equivalent to assuming that $ P [ a= b =1 |c ] = P [ a= 1 |c ] P [ b= 1 |c ] $ which is weaker than the well-known demographic ( statistical ) parity , and is only applicable to a binary setting . Hence , the applicability of the proposed algorithm is extremely limited . [ * * Theory * * ] The theoretical exposure in this paper is confusing and not rigorous . The trouble begins from the unnumbered equation in page 4 , where the authors define $ \\widetilde { h } $ . It is unclear what $ \\widetilde { h } $ depends on . Also , the output is seemingly binary , but the authors claim this a _randomized prediction rule_ and $ \\widetilde { h } ( \\mathbf { x } ) $ represents the probability of predicting the positive class . * * Proposition 1 * * is an obvious statement and not relevant to the claims of this paper , and hence should be removed . * * Theorem 1 : * * In the proof of Theorem 1 , the authors make an assumption about what the output of $ f $ is . In different places the output is assumed to be in $ [ 0,1 ] $ and $ [ -1 , 1 ] $ ! ! Even worse , they claim that the empirical average of some loss value is equal to its expectation ( _what happened to generalization ? _ ) . That is where I stopped reading . [ * * Experiments * * ] The experiments are very weak and not convincing . The paper only compares with Hardt et al.but in an unconvincing way as detailed here . * * ( a ) * * the comparisons should be made in terms of a tradeoff curve between _fairness_ and _performance_ . In the currently reported results , there are instances where Hard et al.and the proposed method are not comparable , e.g. , kNN . * * ( b ) * * The comparisons are unfair because they are done with respect to Definition 1 , which is a very narrow sense of fairness while Hard et al.impose fairness either with respect to _equalized odds_ or _equal opportiunity_ which require conditional independence of $ \\widehat { Y } $ and $ S $ given $ Y ( =1 ) $ . Hence , the comparison with Hard et al.is unjustified . At the very least the authors should compare with a plethora of baselines designed for demographic parity ( e.g. , Kamiran et al.2012 , Zemel et al.2013 , Feldman et al.2015 , Zafar et al.2017 , Jiang et al.2019 , Baharlouei at al . 2020 ) .Even then , the comparison should be done with respect to the statistical parity violation as defined in Dwork et al.2012 , which is a more established notion of fairness . == after rebuttal == I 'd like to thank the authors for their revisions , which have significantly improved the readability of the paper , and the presentation of the results . The addition of fairness violation/accuracy tradeoffs and also ( Kamiran et al.2012 ) add a lot of value in putting this paper in the right context . After reading the rebuttal , I still remain unconvinced about the contributions of this paper . From a practical point of view , the performance of the proposed algorithm is on a par with ( Kamiran et al.2012 ) , which is a baseline for demographic parity and has been improved on several times in the past 8 years . On the other hand , the main claim of the paper seems to be theoretical optimality . Unfortunately , although several previous mistakes have been corrected in the proofs , I can not still follow the proofs of Theorems 1 and 2 . New notation pops up all over the proof , and it is unclear how to follow some lines from the others . Given this , I am increasing my rating to 4 in acknowledgement of my previous misunderstanding of the definition of the statistical parity , which was cleared by the authors during the discussion period . However , I am still unable to recommend this paper in the current form for publication in the conference proceedings .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . Please see our response below . [ Statistical parity ] We disagree with the assessment that conditional statistical parity is \u201c narrow \u201d as it is a strict generalization to \u201c statistical parity \u201d , which as you mentioned , is a well-established notion of fairness used in the literature . In fact , a recent crowdsourcing study has found that it most closely matches with the human perception of bias [ 1 ] . As such , it has gathered a lot of interest in the community ( e.g . [ 2 ] , [ 3 ] ) . [ Binary vs non-binary ] This binary case is presented in the paper for clarity and it is straightforward to extend the algorithm to non-binary sensitive attributes . We have included an outline of how the algorithm can be extended to non-binary sensitive attributes in Appendix F ( please see the revised version ) . [ Proposition 1 ] The reason we include it is to clarify why the algorithm assumes that the groups X_k are known in advance as this was a common question and the proposition shows that it is indeed necessary . We disagree that the explicit lower bound in Proposition 1 is obvious ! In general , we agree that this is non-central to the rest of the paper and we can defer it to the appendix if needed . [ Theorem 1 ] We use $ f ( x ) $ for the output of the original classifier , which is assumed to be in [ -1 , +1 ] and we used $ \\tilde f ( x ) $ for the new rule learned by the post-processing algorithm whose range is [ 0 , 1 ] . There is no contradiction since these are different functions . We have replaced $ \\tilde f ( x ) $ with $ \\tilde h ( x ) $ to avoid such confusion . Regarding the question about generalization , the claim of Theorem 1 is that the algorithm satisfies the desired fairness guarantee on the training sample . The fairness guarantee on the population ( test data ) is provided by Theorem 2 . We have clarified this in the statement of Theorem 1 ( please see the revised version ) . [ Empirical validation ] We propose a post-processing algorithm and we show that randomization is key ( both in theory and in practice ) . The closest algorithm to ours with a randomized rule is the post-processing algorithm by Hardt et , al . ( 2016 ) , which can be used for statistical parity ( see for instance the implementation available in the FairLearn software package by Dudik et al. , 2020 ) . To clarify , our claim is not that our algorithm subsumes ( Hardt et , al.2016 ) because the latter can be used for equalized odds and equality of opportunity as well . Rather , our claim is that for statistical parity , or more generally conditional statistical parity , our algorithm significantly outperforms it . Regarding kNN , we also report results on 3 other classical algorithms ( logistic regression , random forests , and MLP ) as well as results on modern state-of-the-art neural network architectures ( ResNet50 and MobileNet ) both trained from scratch and pre-trained on ImageNet . Our conclusions hold across all settings , not just in kNN . As for the other algorithms , we propose a post-processing algorithm so we focused on post-processing rules . Kamiran et al.2012 is a preprocessing algorithm , Zemel et al.2013 is a representation learning method , and so on . We do believe post-processing is advantageous , particularly for deep learning , for the reasons mentioned in the paper . [ Typos ] Regarding the unnumbered equation in Page 4 , thanks for catching that typo : The range is [ 0 , 1 ] , not { 0 , 1 } . This has been fixed in the revised version . Please note that we have explicitly mentioned what h depends on right below the equation . [ 1 ] Srivastava , Megha , Hoda Heidari , and Andreas Krause . `` Mathematical notions vs. human perception of fairness : A descriptive approach to fairness for machine learning . '' In SIGKDD , 2019 . [ 2 ] S. Corbett-Davies , E. Pierson , A. Feller , S. Goel , and A. Huq , \u201c Algorithmic decision making and the cost of fairness , \u201d in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017 , pp . 797\u2013806 . [ 3 ] N. Mehrabi , F. Morstatter , N. Saxena , K. Lerman , and A. Galstyan , \u201c A survey on bias and fairness in machine learning , \u201d arXiv preprint arXiv:1908.09635 , 2019"}, {"review_id": "ASAJvUPWaDI-1", "review_text": "The paper considers the fair classification problem with respect to conditional statistical parity . It proposes a near-optimal post-processing algorithm for debiasing trained machine learning models , including deep neural networks . The empirical results show that the algorithm outperforms existing post-processing approaches for fair classification . Overall , I vote for accepting . Existing post-processing algorithms usually lack theoretical guarantees but not this paper . My main concern is the clarity ; see cons . Pros : 1.The paper provides provable guarantees from both the impossible side and the algorithmic side . 2.The post-processing approach does not require retraining the classifiers , and the impact on the test accuracy is limited . Cons : 1.Several symbols or notions lack explanations . E.g. , what $ \\gamma $ means in Eq . ( 1 ) ? What $ \\eta $ means in Eq . ( 2 ) ? 2.Theorems lack explanations . E.g. , in Theorem 2 , it is better to explain each term of the right side , including why these terms exist and when they are small . 3.Equations lack explanations . E.g. , why the update rules should be formulated as ( 2 ) ? It is better to explain the intuition .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We strived to incorporate your feedback and provide as much detail as possible given the limited space . We deferred the remaining discussion to the appendix . Specifically : - $ \\gamma $ is the regularization parameter that controls the width of randomization ( see for example Figure 2 ( a ) ) . It is introduced in Section 3 . More details about it are available in Appendix C.1 ( e.g.the discussion around Eq 13 ) . - We reiterated the definition of $ \\eta $ in Theorem 2 for clarity . Please see the revised version . - Regarding the right-hand side of Theorem 2 , the first term is the Bayes risk which can not be avoided . The second term involving gamma is due to the fact that the algorithm optimizes a regularized loss . The third and fourth term are due to the finite sample size and Lipschitz continuity of the loss . We specifically used the robustness-based framework introduced by Xu and Mannor ( 2010 ) to obtain that bound as mentioned in the paper . The update rules are derived in Appendix C. They correspond to the application of the projected SGD method ."}, {"review_id": "ASAJvUPWaDI-2", "review_text": "In this paper , the authors propose a post-processing method for removing bias from a trained model . The bias is defined as conditional statistical parity \u2014 for a given partitioning of the data , the predicted label should be conditionally uncorrelated with the sensitive ( bias inducing ) attribute for each partition . The authors relax this strong requirement to an epsilon-constraint on the conditional covariance for each partition . As an example , race ( sensitive attribute ) should be conditionally uncorrelated to whether an individual will default on their loan ( predicted target ) for each city ( data partition ) . The authors propose a constrained optimization problem that takes the input data , sensitive attribute , partitioning and a trained model to yield a probabilistic decision rule . Subsequently , they propose an iterative solution to the problem , proving some theoretical properties as well as showing how the method compares to different baselines . Fairness is an important consideration as machine learning models find purchase in sensitive applications like loan approvals , job candidate filtering , compensation decisions , and so on . This clearly written paper summarizes the different ways of removing data bias , and proposes a sensible and fairly general post-processing solution . The paper is easy to follow , and while the main body skims on some details to assist readability ( and adhere to the page limit ) , the appendix , which I only quickly scanned , is well-done . I did have some concerns : 1 . The assumption that \u201c the original classifier outputs a monotone transformation of some approximation to the posterior probability p ( y = 1 | x ) \u201d needs further justification since models often tend to be overconfident of the wrong predictions violating the monotonicity assumption [ Guo , Chuan , et al . `` On calibration of modern neural networks . '' arXiv preprint arXiv:1706.04599 ( 2017 ) ] . How central is this assumption to the analysis ? 2.In section 3 ( Algorithm subsection ) doesn \u2019 t \\tilde { h } represent probability ? If so , the function range should be [ 0,1 ] , and not { 0,1 } . 3.Right below , a bar is missing in the absolute value : \u201c can be written as |.| < \\epsilon \u201d . 4.Theorem 2 notation : h ( x ) \\neq y would be more cleanly written with an indication function : I ( h ( x ) \\neq y ) . 5.In baseline comparison , using only conditional statistical parity might be unfair to other methods which don \u2019 t optimize this notion of bias explicitly . Did you consider evaluating for other metrics ? Overall , this is a well-written paper that tackles an important problem , and it would make for a good addition to the conference .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . Please see our response below . [ Assumptions ] The assumption that \u201c the original classifier outputs a monotone transformation of the Bayes regressor \u201d is there only to motivate the development of the algorithm . It is by no means required -- both in theory ( not necessary for the correctness in Theorem 1 ) and in practice ( validated empirically ) . In addition , the algorithm performs well for a wide range of classifiers . [ Theorem 2 ] We have included the indicator function as you suggested . [ Empirical validation ] Regarding the experiments , the algorithm we propose is specifically designed for conditional statistical parity , including the more commonly used demographic parity . Our experiments in Section 5 focus only on demographic parity in its unconditional form for the reasons you have mentioned , but the algorithm handles conditional statistical parity , in general , as shown in the experiment in Figure 1 . [ Typos ] Thanks for pointing out the typos regarding the range of \\tilde h and the missing bar in the constraint . They have been corrected ( please see the revised version ) ."}], "0": {"review_id": "ASAJvUPWaDI-0", "review_text": "Evaluation and improvement of fairness of machine learning algorithms is a very important issue . To this end , the authors of this paper propose a post-processing algorithm to enforce fairness in a narrowly defined notion of fairness . Unfortunately , I have serious concerns about the validity of the results and conclusions of the paper , and hence I can not recommend the paper to be accepted . [ * * Definition * * ] This entire paper is only applicable to a narrow definition of fairness in Definition 1 . Given that $ a $ and $ b $ are binary , Definition 1 ( page 2 ) is essentially equivalent to assuming that $ P [ a= b =1 |c ] = P [ a= 1 |c ] P [ b= 1 |c ] $ which is weaker than the well-known demographic ( statistical ) parity , and is only applicable to a binary setting . Hence , the applicability of the proposed algorithm is extremely limited . [ * * Theory * * ] The theoretical exposure in this paper is confusing and not rigorous . The trouble begins from the unnumbered equation in page 4 , where the authors define $ \\widetilde { h } $ . It is unclear what $ \\widetilde { h } $ depends on . Also , the output is seemingly binary , but the authors claim this a _randomized prediction rule_ and $ \\widetilde { h } ( \\mathbf { x } ) $ represents the probability of predicting the positive class . * * Proposition 1 * * is an obvious statement and not relevant to the claims of this paper , and hence should be removed . * * Theorem 1 : * * In the proof of Theorem 1 , the authors make an assumption about what the output of $ f $ is . In different places the output is assumed to be in $ [ 0,1 ] $ and $ [ -1 , 1 ] $ ! ! Even worse , they claim that the empirical average of some loss value is equal to its expectation ( _what happened to generalization ? _ ) . That is where I stopped reading . [ * * Experiments * * ] The experiments are very weak and not convincing . The paper only compares with Hardt et al.but in an unconvincing way as detailed here . * * ( a ) * * the comparisons should be made in terms of a tradeoff curve between _fairness_ and _performance_ . In the currently reported results , there are instances where Hard et al.and the proposed method are not comparable , e.g. , kNN . * * ( b ) * * The comparisons are unfair because they are done with respect to Definition 1 , which is a very narrow sense of fairness while Hard et al.impose fairness either with respect to _equalized odds_ or _equal opportiunity_ which require conditional independence of $ \\widehat { Y } $ and $ S $ given $ Y ( =1 ) $ . Hence , the comparison with Hard et al.is unjustified . At the very least the authors should compare with a plethora of baselines designed for demographic parity ( e.g. , Kamiran et al.2012 , Zemel et al.2013 , Feldman et al.2015 , Zafar et al.2017 , Jiang et al.2019 , Baharlouei at al . 2020 ) .Even then , the comparison should be done with respect to the statistical parity violation as defined in Dwork et al.2012 , which is a more established notion of fairness . == after rebuttal == I 'd like to thank the authors for their revisions , which have significantly improved the readability of the paper , and the presentation of the results . The addition of fairness violation/accuracy tradeoffs and also ( Kamiran et al.2012 ) add a lot of value in putting this paper in the right context . After reading the rebuttal , I still remain unconvinced about the contributions of this paper . From a practical point of view , the performance of the proposed algorithm is on a par with ( Kamiran et al.2012 ) , which is a baseline for demographic parity and has been improved on several times in the past 8 years . On the other hand , the main claim of the paper seems to be theoretical optimality . Unfortunately , although several previous mistakes have been corrected in the proofs , I can not still follow the proofs of Theorems 1 and 2 . New notation pops up all over the proof , and it is unclear how to follow some lines from the others . Given this , I am increasing my rating to 4 in acknowledgement of my previous misunderstanding of the definition of the statistical parity , which was cleared by the authors during the discussion period . However , I am still unable to recommend this paper in the current form for publication in the conference proceedings .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . Please see our response below . [ Statistical parity ] We disagree with the assessment that conditional statistical parity is \u201c narrow \u201d as it is a strict generalization to \u201c statistical parity \u201d , which as you mentioned , is a well-established notion of fairness used in the literature . In fact , a recent crowdsourcing study has found that it most closely matches with the human perception of bias [ 1 ] . As such , it has gathered a lot of interest in the community ( e.g . [ 2 ] , [ 3 ] ) . [ Binary vs non-binary ] This binary case is presented in the paper for clarity and it is straightforward to extend the algorithm to non-binary sensitive attributes . We have included an outline of how the algorithm can be extended to non-binary sensitive attributes in Appendix F ( please see the revised version ) . [ Proposition 1 ] The reason we include it is to clarify why the algorithm assumes that the groups X_k are known in advance as this was a common question and the proposition shows that it is indeed necessary . We disagree that the explicit lower bound in Proposition 1 is obvious ! In general , we agree that this is non-central to the rest of the paper and we can defer it to the appendix if needed . [ Theorem 1 ] We use $ f ( x ) $ for the output of the original classifier , which is assumed to be in [ -1 , +1 ] and we used $ \\tilde f ( x ) $ for the new rule learned by the post-processing algorithm whose range is [ 0 , 1 ] . There is no contradiction since these are different functions . We have replaced $ \\tilde f ( x ) $ with $ \\tilde h ( x ) $ to avoid such confusion . Regarding the question about generalization , the claim of Theorem 1 is that the algorithm satisfies the desired fairness guarantee on the training sample . The fairness guarantee on the population ( test data ) is provided by Theorem 2 . We have clarified this in the statement of Theorem 1 ( please see the revised version ) . [ Empirical validation ] We propose a post-processing algorithm and we show that randomization is key ( both in theory and in practice ) . The closest algorithm to ours with a randomized rule is the post-processing algorithm by Hardt et , al . ( 2016 ) , which can be used for statistical parity ( see for instance the implementation available in the FairLearn software package by Dudik et al. , 2020 ) . To clarify , our claim is not that our algorithm subsumes ( Hardt et , al.2016 ) because the latter can be used for equalized odds and equality of opportunity as well . Rather , our claim is that for statistical parity , or more generally conditional statistical parity , our algorithm significantly outperforms it . Regarding kNN , we also report results on 3 other classical algorithms ( logistic regression , random forests , and MLP ) as well as results on modern state-of-the-art neural network architectures ( ResNet50 and MobileNet ) both trained from scratch and pre-trained on ImageNet . Our conclusions hold across all settings , not just in kNN . As for the other algorithms , we propose a post-processing algorithm so we focused on post-processing rules . Kamiran et al.2012 is a preprocessing algorithm , Zemel et al.2013 is a representation learning method , and so on . We do believe post-processing is advantageous , particularly for deep learning , for the reasons mentioned in the paper . [ Typos ] Regarding the unnumbered equation in Page 4 , thanks for catching that typo : The range is [ 0 , 1 ] , not { 0 , 1 } . This has been fixed in the revised version . Please note that we have explicitly mentioned what h depends on right below the equation . [ 1 ] Srivastava , Megha , Hoda Heidari , and Andreas Krause . `` Mathematical notions vs. human perception of fairness : A descriptive approach to fairness for machine learning . '' In SIGKDD , 2019 . [ 2 ] S. Corbett-Davies , E. Pierson , A. Feller , S. Goel , and A. Huq , \u201c Algorithmic decision making and the cost of fairness , \u201d in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017 , pp . 797\u2013806 . [ 3 ] N. Mehrabi , F. Morstatter , N. Saxena , K. Lerman , and A. Galstyan , \u201c A survey on bias and fairness in machine learning , \u201d arXiv preprint arXiv:1908.09635 , 2019"}, "1": {"review_id": "ASAJvUPWaDI-1", "review_text": "The paper considers the fair classification problem with respect to conditional statistical parity . It proposes a near-optimal post-processing algorithm for debiasing trained machine learning models , including deep neural networks . The empirical results show that the algorithm outperforms existing post-processing approaches for fair classification . Overall , I vote for accepting . Existing post-processing algorithms usually lack theoretical guarantees but not this paper . My main concern is the clarity ; see cons . Pros : 1.The paper provides provable guarantees from both the impossible side and the algorithmic side . 2.The post-processing approach does not require retraining the classifiers , and the impact on the test accuracy is limited . Cons : 1.Several symbols or notions lack explanations . E.g. , what $ \\gamma $ means in Eq . ( 1 ) ? What $ \\eta $ means in Eq . ( 2 ) ? 2.Theorems lack explanations . E.g. , in Theorem 2 , it is better to explain each term of the right side , including why these terms exist and when they are small . 3.Equations lack explanations . E.g. , why the update rules should be formulated as ( 2 ) ? It is better to explain the intuition .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We strived to incorporate your feedback and provide as much detail as possible given the limited space . We deferred the remaining discussion to the appendix . Specifically : - $ \\gamma $ is the regularization parameter that controls the width of randomization ( see for example Figure 2 ( a ) ) . It is introduced in Section 3 . More details about it are available in Appendix C.1 ( e.g.the discussion around Eq 13 ) . - We reiterated the definition of $ \\eta $ in Theorem 2 for clarity . Please see the revised version . - Regarding the right-hand side of Theorem 2 , the first term is the Bayes risk which can not be avoided . The second term involving gamma is due to the fact that the algorithm optimizes a regularized loss . The third and fourth term are due to the finite sample size and Lipschitz continuity of the loss . We specifically used the robustness-based framework introduced by Xu and Mannor ( 2010 ) to obtain that bound as mentioned in the paper . The update rules are derived in Appendix C. They correspond to the application of the projected SGD method ."}, "2": {"review_id": "ASAJvUPWaDI-2", "review_text": "In this paper , the authors propose a post-processing method for removing bias from a trained model . The bias is defined as conditional statistical parity \u2014 for a given partitioning of the data , the predicted label should be conditionally uncorrelated with the sensitive ( bias inducing ) attribute for each partition . The authors relax this strong requirement to an epsilon-constraint on the conditional covariance for each partition . As an example , race ( sensitive attribute ) should be conditionally uncorrelated to whether an individual will default on their loan ( predicted target ) for each city ( data partition ) . The authors propose a constrained optimization problem that takes the input data , sensitive attribute , partitioning and a trained model to yield a probabilistic decision rule . Subsequently , they propose an iterative solution to the problem , proving some theoretical properties as well as showing how the method compares to different baselines . Fairness is an important consideration as machine learning models find purchase in sensitive applications like loan approvals , job candidate filtering , compensation decisions , and so on . This clearly written paper summarizes the different ways of removing data bias , and proposes a sensible and fairly general post-processing solution . The paper is easy to follow , and while the main body skims on some details to assist readability ( and adhere to the page limit ) , the appendix , which I only quickly scanned , is well-done . I did have some concerns : 1 . The assumption that \u201c the original classifier outputs a monotone transformation of some approximation to the posterior probability p ( y = 1 | x ) \u201d needs further justification since models often tend to be overconfident of the wrong predictions violating the monotonicity assumption [ Guo , Chuan , et al . `` On calibration of modern neural networks . '' arXiv preprint arXiv:1706.04599 ( 2017 ) ] . How central is this assumption to the analysis ? 2.In section 3 ( Algorithm subsection ) doesn \u2019 t \\tilde { h } represent probability ? If so , the function range should be [ 0,1 ] , and not { 0,1 } . 3.Right below , a bar is missing in the absolute value : \u201c can be written as |.| < \\epsilon \u201d . 4.Theorem 2 notation : h ( x ) \\neq y would be more cleanly written with an indication function : I ( h ( x ) \\neq y ) . 5.In baseline comparison , using only conditional statistical parity might be unfair to other methods which don \u2019 t optimize this notion of bias explicitly . Did you consider evaluating for other metrics ? Overall , this is a well-written paper that tackles an important problem , and it would make for a good addition to the conference .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . Please see our response below . [ Assumptions ] The assumption that \u201c the original classifier outputs a monotone transformation of the Bayes regressor \u201d is there only to motivate the development of the algorithm . It is by no means required -- both in theory ( not necessary for the correctness in Theorem 1 ) and in practice ( validated empirically ) . In addition , the algorithm performs well for a wide range of classifiers . [ Theorem 2 ] We have included the indicator function as you suggested . [ Empirical validation ] Regarding the experiments , the algorithm we propose is specifically designed for conditional statistical parity , including the more commonly used demographic parity . Our experiments in Section 5 focus only on demographic parity in its unconditional form for the reasons you have mentioned , but the algorithm handles conditional statistical parity , in general , as shown in the experiment in Figure 1 . [ Typos ] Thanks for pointing out the typos regarding the range of \\tilde h and the missing bar in the constraint . They have been corrected ( please see the revised version ) ."}}