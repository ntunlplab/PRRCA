{"year": "2017", "forum": "HkljfjFee", "title": "Support Regularized Sparse Coding and Its Fast Encoder", "decision": "Accept (Poster)", "meta_review": "Adding a manifold regularizer to a learning objective function is certainly not a new direction. The paper argues that using a support based regularizer is superior to using a standard graph Laplacian regularizer (which has been explored before), although this argument is not developed particularly rigorously and dominantly has to fall back on empirical evidence. The main contribution of the paper appears to be theoretical justification of an alternating optimization scheme for minimizing the resulting objective function (yet the optimization aspects of dealing with a sparse support regularizer are somewhat orthogonal to the current context). The empirical results are not very convincing since the dictionary size is relatively large compared to the dataset size; the gains with respect to l2 manifold regularizer are not consistent; and the gains using deep architectures to directly predict sparse codes are also modest and somewhat inconsistent. These points aside, the reviewers are overall enthusiastic about the paper and find it to be well written and complete.", "reviews": [{"review_id": "HkljfjFee-0", "review_text": " In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. In the abstract the authors mention \u201cextensive experimental results \u2026\u201d, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in \u201cCUDA C++ with extreme efficiency\u201d (page 10). But more importantly, I find it hard to interpret and compare the results: The paper reports accuracy and and normalized mutual information for a image retrieval / clustering task where the proposed SRSC is used as a feature extractor. The improvements relative to standard Sparse Coding seem very small (often < 1% in terms of NMI; it looks more promising in terms of accuracy) and if I understand the description on page 11 correctly, than the test set was used to select some hyperparameters (the best similarity measure for clustering step)? There is no comparisons to other baselines / state of the art image clustering methods. Besides of providing features for a small scale image clustering system, are there maybe ways to more directly evaluate the properties and qualities of a sparse coding approach? E.g. reconstruction error / sparsity; maybe even denoising performance? In summary, I think in it current form the paper lacks the evaluation and experimental results for an ICLR publication. Intuitively, I agree with the authors that the proposed regularization is an interesting direction, but I don\u2019t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where SRSC is used as a feature extractor are very modest.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments and the raised concerns . 1 Compact dictionary learned by sparse coding methods leads to compelling empirical results , as shown by the extensive study in the machine learning and computer vision literature , so we choose the dictionary size between 100 and 500 . Note that the dictionary size in our paper is comparable to that in the seminal paper \u201c Learning Fast Approximations of Sparse Coding \u201d by Karol Gregor and Yann LeCun which proposes LISTA network for fast approximation of regular sparse coding . We report the best performance among the two similarity measures as the clustering results on the test set of each data set in Section 5 . The same procedure is performed for all sparse coding based methods to produce fair comparison results , and we observe that the clustering results by the two similar measures are usually similar . In our semi-supervised learning experiments ( see more details in the second part of this response ) , a single similarity measure , i.e.the support similarity introduced in Section 5.1 , is used for all the semi-supervised learning methods . Regarding to the clustering results in terms of accuracy and normalized mutual information ( NMI ) , accuracy is in many cases the more preferable measure since it directly reflects the percentage of the data that have correct cluster labels . Our clustering results on various data sets are always promising in terms of accuracy ( thank you for pointing this out ) , while maintaining consistently competitive NMI value . Also , SRSC produces notably better NMI results than the baseline methods on the USPS data ( e.g.for the first c = 6 and 8 clusters of the USPS data in Table 1 ) . More importantly , we conduct more clustering experiments on larger data sets , i.e.MNIST handwritten digit data and CIFAR-10 data , with results shown in Table 7 and Table 8 of the revised paper . Such results again demonstrate the effectiveness of SRSC and Deep-SRSC , and SRSC and Deep-SRSC achieve considerable improvement in terms of both accuracy and NMI on the MNIST data . Since our SRSC method is an improved sparse coding method by exploiting manifold structure of the data , we mainly demonstrate performance improvement over other sparse coding methods , i.e.regular sparse coding and L2-RSC which uses graph Laplacian to smooth the sparse codes based on the local manifold structure of the data . In our experiments of semi-supervised learning by label propagation , we add into our baseline methods the state-of-the-art semi-supervised method that is mostly relevant to our study to the best of our knowledge ( see more details below ) . 2 We have added another important application of the sparse coding methods , i.e.semi-supervised learning by label propagation ( this application was shown on the USPS data in response to the comment of AnonReviewer2 ) , and we conduct semi-supervised learning experiments on the USPS data and the CIFAR-10 data with the results illustrated in Figure 6 and Figure 7 of the revised paper . Note that SRSC and Deep-SRSC achieve significant improvement over other baseline methods based on label propagation in terms of error rate especially when the number of labeled samples for each class is small ( usually with 15 % to 40 % improvement ) , revealing the advantage of support regularization that captures the locally linear manifold structure of the data . The baseline methods include manifold based similarity adaptation ( MBS ) by Karasuyama et al.in NIPS 2013 , one of the state-of-the-art semi-supervised learning methods based on label propagation . Update : We use the deep neural network trained on the ILSVRC 2012 data to extract the $ 4096 $ -dimensional feature vector for each image in the CIFAR-10 data , and all the clustering methods are performed on the extracted features so as to achieve much higher performance . SRSC still outperforms other baseline methods in terms of both accuracy and NMI . Please see more details in Table 8 and the subsection `` Deep-SRSC with the Second Test Setting ( Referring to the Training Data ) '' in the appendix of the revised paper ."}, {"review_id": "HkljfjFee-1", "review_text": "The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation). Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance. Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comment ! We have included a more clear motivation of Deep-SRSC in the beginning of Section 4 in the revised paper , namely \u201c The goal of Deep-SRSC is to approximate the sparse codes of the input data in a fast way by feeding the data through the Deep-SRSC network , instead of running the iterative optimization algorithm for SRSC in Section 2.1 . \u201d In fact , SRSC is efficient from the perspective of the conventional optimization algorithms for the sparse coding methods . Please refer to the complexity analysis of SRSC in the subsection \u201c Time Complexity \u201d in Section 2.1 of the revised paper . Deep-SRSC is proposed as a fast approximation of SRSC with considerable ( around 8.3 times ) speedup for obtaining the approximate support regularized sparse codes of the new data or the test data ( more details in Section 4.1 of the revised paper : Deep-SRSC As Fast Encoder ) . Moreover , we have presented the additional experiments in the revised paper on the MNIST and CIFAR-10 data for the clustering and semi-supervised learning tasks , which further demonstrate the effectiveness of SRSC and Deep-SRSC ."}, {"review_id": "HkljfjFee-2", "review_text": " I'd like to thank the authors for their detailed response to my questions. The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach. The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript. The authors also propose a fast encoding scheme for their proposed method. The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. \"Converting\" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comment ! We have included the intuition of what implies the condition G_ki > =0 in Section 3 . Moreover , we have presented the additional experiments in the revised paper on the MNIST and CIFAR-10 data for the clustering and semi-supervised learning tasks , which further demonstrate the effectiveness of SRSC and Deep-SRSC ."}], "0": {"review_id": "HkljfjFee-0", "review_text": " In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. In the abstract the authors mention \u201cextensive experimental results \u2026\u201d, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in \u201cCUDA C++ with extreme efficiency\u201d (page 10). But more importantly, I find it hard to interpret and compare the results: The paper reports accuracy and and normalized mutual information for a image retrieval / clustering task where the proposed SRSC is used as a feature extractor. The improvements relative to standard Sparse Coding seem very small (often < 1% in terms of NMI; it looks more promising in terms of accuracy) and if I understand the description on page 11 correctly, than the test set was used to select some hyperparameters (the best similarity measure for clustering step)? There is no comparisons to other baselines / state of the art image clustering methods. Besides of providing features for a small scale image clustering system, are there maybe ways to more directly evaluate the properties and qualities of a sparse coding approach? E.g. reconstruction error / sparsity; maybe even denoising performance? In summary, I think in it current form the paper lacks the evaluation and experimental results for an ICLR publication. Intuitively, I agree with the authors that the proposed regularization is an interesting direction, but I don\u2019t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where SRSC is used as a feature extractor are very modest.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments and the raised concerns . 1 Compact dictionary learned by sparse coding methods leads to compelling empirical results , as shown by the extensive study in the machine learning and computer vision literature , so we choose the dictionary size between 100 and 500 . Note that the dictionary size in our paper is comparable to that in the seminal paper \u201c Learning Fast Approximations of Sparse Coding \u201d by Karol Gregor and Yann LeCun which proposes LISTA network for fast approximation of regular sparse coding . We report the best performance among the two similarity measures as the clustering results on the test set of each data set in Section 5 . The same procedure is performed for all sparse coding based methods to produce fair comparison results , and we observe that the clustering results by the two similar measures are usually similar . In our semi-supervised learning experiments ( see more details in the second part of this response ) , a single similarity measure , i.e.the support similarity introduced in Section 5.1 , is used for all the semi-supervised learning methods . Regarding to the clustering results in terms of accuracy and normalized mutual information ( NMI ) , accuracy is in many cases the more preferable measure since it directly reflects the percentage of the data that have correct cluster labels . Our clustering results on various data sets are always promising in terms of accuracy ( thank you for pointing this out ) , while maintaining consistently competitive NMI value . Also , SRSC produces notably better NMI results than the baseline methods on the USPS data ( e.g.for the first c = 6 and 8 clusters of the USPS data in Table 1 ) . More importantly , we conduct more clustering experiments on larger data sets , i.e.MNIST handwritten digit data and CIFAR-10 data , with results shown in Table 7 and Table 8 of the revised paper . Such results again demonstrate the effectiveness of SRSC and Deep-SRSC , and SRSC and Deep-SRSC achieve considerable improvement in terms of both accuracy and NMI on the MNIST data . Since our SRSC method is an improved sparse coding method by exploiting manifold structure of the data , we mainly demonstrate performance improvement over other sparse coding methods , i.e.regular sparse coding and L2-RSC which uses graph Laplacian to smooth the sparse codes based on the local manifold structure of the data . In our experiments of semi-supervised learning by label propagation , we add into our baseline methods the state-of-the-art semi-supervised method that is mostly relevant to our study to the best of our knowledge ( see more details below ) . 2 We have added another important application of the sparse coding methods , i.e.semi-supervised learning by label propagation ( this application was shown on the USPS data in response to the comment of AnonReviewer2 ) , and we conduct semi-supervised learning experiments on the USPS data and the CIFAR-10 data with the results illustrated in Figure 6 and Figure 7 of the revised paper . Note that SRSC and Deep-SRSC achieve significant improvement over other baseline methods based on label propagation in terms of error rate especially when the number of labeled samples for each class is small ( usually with 15 % to 40 % improvement ) , revealing the advantage of support regularization that captures the locally linear manifold structure of the data . The baseline methods include manifold based similarity adaptation ( MBS ) by Karasuyama et al.in NIPS 2013 , one of the state-of-the-art semi-supervised learning methods based on label propagation . Update : We use the deep neural network trained on the ILSVRC 2012 data to extract the $ 4096 $ -dimensional feature vector for each image in the CIFAR-10 data , and all the clustering methods are performed on the extracted features so as to achieve much higher performance . SRSC still outperforms other baseline methods in terms of both accuracy and NMI . Please see more details in Table 8 and the subsection `` Deep-SRSC with the Second Test Setting ( Referring to the Training Data ) '' in the appendix of the revised paper ."}, "1": {"review_id": "HkljfjFee-1", "review_text": "The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation). Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance. Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comment ! We have included a more clear motivation of Deep-SRSC in the beginning of Section 4 in the revised paper , namely \u201c The goal of Deep-SRSC is to approximate the sparse codes of the input data in a fast way by feeding the data through the Deep-SRSC network , instead of running the iterative optimization algorithm for SRSC in Section 2.1 . \u201d In fact , SRSC is efficient from the perspective of the conventional optimization algorithms for the sparse coding methods . Please refer to the complexity analysis of SRSC in the subsection \u201c Time Complexity \u201d in Section 2.1 of the revised paper . Deep-SRSC is proposed as a fast approximation of SRSC with considerable ( around 8.3 times ) speedup for obtaining the approximate support regularized sparse codes of the new data or the test data ( more details in Section 4.1 of the revised paper : Deep-SRSC As Fast Encoder ) . Moreover , we have presented the additional experiments in the revised paper on the MNIST and CIFAR-10 data for the clustering and semi-supervised learning tasks , which further demonstrate the effectiveness of SRSC and Deep-SRSC ."}, "2": {"review_id": "HkljfjFee-2", "review_text": " I'd like to thank the authors for their detailed response to my questions. The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach. The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript. The authors also propose a fast encoding scheme for their proposed method. The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. \"Converting\" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comment ! We have included the intuition of what implies the condition G_ki > =0 in Section 3 . Moreover , we have presented the additional experiments in the revised paper on the MNIST and CIFAR-10 data for the clustering and semi-supervised learning tasks , which further demonstrate the effectiveness of SRSC and Deep-SRSC ."}}