{"year": "2021", "forum": "6M4c3WegNtX", "title": "Neural Ensemble Search for Uncertainty Estimation and Dataset Shift", "decision": "Reject", "meta_review": "This paper proposes a new method to perform uncertainty estimation based on ensembles with diverse network architecture. \n\nThe reviewers raised a few concerns:\n- Although it is ok not to compare with (Tao, 2019), an active analytical comparison with baselines for ensemble diversification should not be overlooked e.g. (Yao et al, 2008), (Olson et al, 2019), (Khurana et al, 2018), etc.\n- The approach presented in this paper is not novel in the general idea of searching for or diversifying ensembles \n- The reviewers agree that diversity methods can be implemented on top of NES, but it is unclear whether NES+diversity methods would give more over just diversity methods; so either measuring NES+diversity methods or a direct comparison of NES and diversity methods is important.\n\nWe encourage the authors address these issues in the next revision.\n", "reviews": [{"review_id": "6M4c3WegNtX-0", "review_text": "The paper suggests a new approach to the construction of ensembles of deep neural networks ( DNN ) . Unlike previous methods which usually deal with multiple DNNs of same structure authors propose to form an ensemble of networks with different architecture . The main claim is that using diverse architectures increases diversity and hence the quality of predictions . To find the best architectures they use methodology inspired by neural architecture search ( NAS ) in particular random search and regularized evolution . The method for neural ensemble search ( NES ) is algorithmically simple although computationally hard . On several experiments the authors show NES outperforms standard deep ensembles formed from networks with same ( even optimal ) structure both in terms of test NLL and in terms of uncertainty estimation under domain shift . Pros.Nice idea Simple algorithm Cons . My main point for the criticism is the lack of experiment which I find to be crucially important namely the comparison aganist deep ensemble of DNNs with same architecture to which ForwardSelect procedure has been applied . Train P DNNs with same architecture then perform ForwardSelect routine to take the best K of them and compare your method with such deep ensemble . Currently the authors only compare their method with deep ensembles to which no special selection procedure was applied . This causes bias and it is not clear whether the improvement in NES is due to the usage of different architectures or due to the selection procedure which encourages diversity in resulting ensemble . P.S.Please correct me if I misunderstood the last point . I have read the corresponding part twice and found no evidence that you 're using ForwardSelection when analysing the performance of ensembles of DNNs with same architecture . =UPDATE My concerns were partly addressed in author 's response so I have raised my score to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . Below we address the reviewer \u2019 s main concern . 1 . * * '' My main point for the criticism is the lack of experiment which I find to be crucially important namely the comparison against deep ensemble of DNNs with same architecture to which ForwardSelect procedure has been applied . Train P DNNs with same architecture then perform ForwardSelect routine to take the best K of them and compare your method with such deep ensemble . Currently the authors only compare their method with deep ensembles to which no special selection procedure was applied . This causes bias and it is not clear whether the improvement in NES is due to the usage of different architectures or due to the selection procedure which encourages diversity in resulting ensemble . `` * * : Thank you for this suggestion ! We agree this is an important study to gain insight into the improvement from NES , and we have added this ablation to our work in Appendix C.3 . As you described , we compare to additional deep ensemble baselines ( called \u201c DeepEns + ES \u201d ) which select the ensemble from a pool of trained random initializations of a fixed , optimized architecture . Our results show that NES algorithms continue to outperform these baselines . Also , note that the cost of \u201c DeepEns + ES \u201d baselines is substantially higher at the ensembling stage than usual deep ensembles , as we now train a pool of random initializations instead of just M random initializations ( M = ensemble size ) . In fact , the total cost ends up becoming larger than NES , since for DeepEns + ES we first need to find a good architecture and then train it multiple times to form a pool ( as in NES ) . Appendix C.3 contains further discussion on these points . We hope the reviewer will consider increasing their score , as we have added the experiment they suggested . We welcome any questions ."}, {"review_id": "6M4c3WegNtX-1", "review_text": "The paper proposes creating diverse ensembles of neural networks using an evolutionary method for finding base learners with high performance as well as mutual diversity . The selected base learners are then aggregated for an ensemble using a known method for ensemble selection . The paper is generally well written and addresses a relevant problem of constructing ensembles while training neural networks instead of building models first , independently , and later constructing ensembles . Having said that , the paper lacks a significant contribution . The second phase ( Ensemble Selection ) of the proposed method is essentially the algorithm from Rich Caruana et al.2004.The first phase of Pool Building suggests either a random generation or alternatively a vaguely described evolutionary method , lacking details or analysis . It is not clear how exactly is the random initialization of architectures performed . Do you randomly select from a set of seed architectures or randomly create ( i.e. , random number of layers , random number of units , random activation functions , random initial weights , etc . ) ? Growing from a random neural architectures to multiple highly performing ( besides being mutually diverse ) through single permutations upon model training and evaluation seems like an expensive process . An evolutionary approach in such a manner seems in efficient . Can you report the time taken for some of the reported cases in the evaluation ? The ensemble methodology of unweighted averaging is fairly naive . What was the reason to select this one particularly ? Contribution # 1 ( page 2 ) is n't really a contribution . It is common knowledge amongst practitioners . The proposed method can lead to overfitting because the search seems to be based on a fixed set for evaluation . Regarding evaluation -- Can you explain how that is addressed ? Have you evaluated your method on a broader variety of datasets ? Can you confirm that the test data used for search/optimization is different than the one used for measuring reported performance ? Did you consider comparing it other methods such as Tao 2019 ( mentioned below ) ? This paper can improve its literature survey by citing more directly relevant work in ensemble search using diversification . Here are few examples of more sophisticated ensemble evolution work , not necessarily for a DL base learner , but relevant nonetheless : -Bhowan , et al.2013.Evolving diverse ensembles using genetic programming for classification with unbalanced data . Trans.Evol.Comp -Khurana et al.2018.Ensembles with Automated Feature Engineering . AutoML at ICML . -Olson et al.2019.TPOT : A Tree-Based Pipeline Optimization Tool for Automating Machine Learning . Automated ML . -Tao , 2019 . Deep Neural Network Ensembles . Machine Learning , Optimization , and Data Science . -Yao et al. , 2008 . Evolving artificial neural network ensembles , in IEEE Computational Intelligence Magazine Overall , it is a good problem , but this paper falls well short of the threshold . Update : I thank the authors for their response . Some justifications are provided and for that I will change my score . Overall , the paper still needs work .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments and the references . We have seen similar remarks previously and a number of them are addressed in our work ; we refer to the appropriate sections . 1 . * * \u201c ensemble selection ... is essentially Caruana et al . ( 2004 ) \u201d * * : We cited Caruana et al . ( 2004 ) in explaining our choice of forward selection on pg . 4 and in the related work section , and we do not claim forward selection to be a contribution . 2 . * * \u201c vaguely described evolutionary method , lacking details or analysis \u201d * * : Can you please specify what you felt is lacking in our description of NES-RE ? We have described NES-RE in Section 4.2 and Figure 2 . We have also provided pseudocode in Algorithm 2 , implementation and scalability details in Appendix B.4 and code in the supplementary material . 3 . * * \u201c Do you randomly select from a set of seed architectures or randomly create \u201d * * : The architecture search spaces and how we randomly sample architectures are described in Appendices B.1 and B.3 . In short , architectures in the DARTS search space are specified by cells which are DAGs where each edge represents an operation ( e.g.max pooling , separable convolution ) . We sample both the structure of the cell and the operations at each edge . 4 . * * \u201c ensemble methodology of unweighted averaging is fairly naive \u201d * * : Despite it being simple and \u201c na\u00efve \u201d , unweighted averaging is used in deep ensembles ( and works well ) , and in order to isolate and evaluate the impact of varying architectures , we also used unweighted averaging . Note that multiple popular neural network ensembling techniques also use unweighted averaging ( e.g.deep ensembles , snapshot ensembles , fast geometric ensembling etc . ) Nonetheless , more sophisticated ensemble combination methods could readily be used with NES . 5 . * * \u201c The proposed method can lead to overfitting \u201d * * : While it is difficult to guarantee no overfitting , in our experiments , we did not experience evidence for overfitting during ensemble selection despite using a fixed validation set . This is evident in Figures 4 , 7 , 17 , since test performance of NES improves with increasing pool size/budget . 6 . * * \u201c Regarding evaluation -- Can you explain how that is addressed ? Have you evaluated your method on a broader variety of datasets ? Can you confirm that the test data used for search/optimization is different than the one used for measuring reported performance ? \u201d * * : Section 5 evaluates NES on 5 datasets ( FMNIST , CIFAR-10 , CIFAR-100 , ImageNet-16-120 and Tiny ImageNet ) and 2 architecture search spaces ( DARTS search space and NAS-Bench-201 ) , using 3 metrics ( NLL , classification error , ECE ) including when there is test-time dataset shift . Regarding the use of test data , yes , \u201c unless stated otherwise , all evaluations are on the test dataset \u201d ( section 5 ) , and Algorithms 1-2 only use D_train , D_val . Using suggestions from reviewers , we have also added comparisons to new baselines in Appendices C.3 and C.4 . Overall , we believe that NES \u2019 empirical performance has been extensively evaluated , as also noted by AnonReviewer4 : * \u201c The authors made comparisons to several reasonable baselines . The improvement of the proposed NES-RE/RS method over fixed architecture ensembles is consistent and significant. \u201d * 7 . * * \u201c Did you consider comparing it to other methods such as Tao 2019 ? \u201d * * : Thank you for the reference . We have not compared to Tao 2019 , because this seems akin to a boosting-based approach which requires training base learners sequentially ( at least , partially ) . First , this is time-consuming for large networks and ensemble sizes ( e.g.size 30 in our experiments ) in contrast with randomization-based approaches for ensembling , such as NES and deep ensembles , which are readily parallelized . We have also been unable to find code implementation for Tao 2019 . Second , in the context of predictive uncertainty and dataset shift , ensemble diversity due to randomization-based approaches reduces overconfident predictions by individual baselearners . It is unclear this benefit would be retained by boosting-based approaches which optimize solely for predictive performance on in-distribution data ."}, {"review_id": "6M4c3WegNtX-2", "review_text": "The authors addressed my concerns in the rebuttal . I have raised my score . Summary : This paper combines AutoML techniques and deep ensembles to improve the ensemble diversity so that it improves the entire ensemble quality in both in- and out-of-distribution dataset . The authors made a study on two possible AutoML methods which can be combined with ensembles : 1 . Random search & 2 . Regularized mutation . The empirical results showed that the proposed NES methods outperform commonly selected baselines . Pros : How to improve ensemble diversity is one of the core topics on the way to better ensemble performance ( in terms of both accuracy and uncertainty metrics ) . Many previous research works focused on 1 . Build efficient ensembles while remaining reasonable diversity ; 2 . Improve ensemble diversity by exploring the hyper-parameter space . In practice , it is standard to ensemble neural networks with different depths or widths to improve diversity and hence the ensemble performance . However , as far as I know , there is no guidance or automatic mechanism in ensembling neural networks with different architectures . Thus , the problem this paper aims to tackle is significant and it will benefit the research community . The authors did a self-contained introduction on ensembles & uncertainty and AutoML . The coverage on mutation AutoML is limited but this is still reasonable due to the page constraint . The motivation of why we want to combine AutoML and deep ensembles is clearly stated in section 3.2 . Figure 1 demonstrates the effectiveness of various architectures in promoting ensemble diversity . The empirical evaluation ranges across CIFAR dataset and ImageNet . It also includes calibration performance under dataset shift ( uncertainty estimation on out-of-distribution dataset ) , which is the common benchmark to evaluate an ensemble 's performance . The authors made comparisons to several reasonable baselines . The improvement of the proposed NES-RE/RS method over fixed architecture ensembles is consistent and significant . Cons : Figure 1 demonstrates the motivation behind this work . To be more convincing , it can be supplemented with the predictive disagreement on the testset or the averaged KL divergence between the predictive distribution among ensemble members . Moreover , the figure compares diversity between ensembles with different architectures and ensembles with random seeds . For a more comprehensive study , the figure can include a study on ensembles with different hyper-parameters . A more interesting baseline I will mention below is an ensemble with different depths . The baselines considered in this paper only include ensembles with a fixed architecture . It would be more convincing if the authors can include other baselines which include ensembles with different architectures ( without neural architecture search ) . For example , one naive baseline would be ensembling DeepEns ( Optimal ) with different depths ( fully trained independently ) . This highlights the need for neural architecture search . It also helps to understand how much diversity in architecture ( among ensemble members ) we need in order to achieve desired diversity in ensemble predictions . Additionally , it is encouraged to compare to hyper-parameter ensembles . This uncovers the question of which axis ( hyper-parameter & architectures ) is more effective in promoting an ensemble \u2019 s performance . Another missing part in this paper is the cost analysis of NES and how much does it increase compared to deep ensembles . Both NES-RS and NES-RE require training after sampling one neural architecture . This leads to a non-trivial computational overhead compared to traditional deep ensembles . In section 4.3 , it mentioned that a proportion of validation data encapsulates the belief about test-time shift . The authors didn \u2019 t mention whether the same protocol is applied to baselines ( DeepEns ) . This leads to a question that is the improvement on out-of-distribution calibration coming from NES or shifted validation set . I consider this is a minor issue because Figure 21 in the appendix also shows that the clear improvement without shifted validation data . Table 1 shows that larger ensemble size leads to worse ensembling performance . This is against our general intuition in deep ensembles where more ensemble members lead to better performance . My guess is more ensemble members leads to optimization difficulties in NES . I expect to see more discussion on this observation . Overall , the authors propose a compelling solution to automatically design the neural network architectures in deep ensembels . However , the cons slightly outweight the pros in this version .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Many thanks for your detailed , helpful review and for appreciating our work . We address your concerns below by incorporating your suggestions into our work and hope that you will consider updating your score : 1 . * * \u201c To be more convincing , [ Figure 1 ] can be supplemented with the predictive disagreement on the test set \u201d * * : Thanks for this suggestion ! We have added a comparison of the predictive disagreement in an ensemble with fixed architecture vs an ensemble with varying architectures in the last paragraph of Section 3.2 . The results ( 11.88 % vs. 10.51 % disagreement for varying vs. fixed architecture ensembles respectively ) show that varying the architecture also yields higher predictive disagreement , i.e.higher diversity . 2 . * * \u201c The baselines considered in this paper only include ensembles with a fixed architecture . It would be more convincing if the authors can include other baselines which include ensembles with different architectures ( without neural architecture search ) . For example , one naive baseline would be ensembling DeepEns ( Optimal ) with different depths ( fully trained independently ) . This highlights the need for neural architecture search . It also helps to understand how much diversity in architecture ( among ensemble members ) we need in order to achieve desired diversity in ensemble predictions . Additionally , it is encouraged to compare to hyper-parameter ensembles . This uncovers the question of which axis ( hyper-parameter & architectures ) is more effective in promoting an ensemble \u2019 s performance. \u201d * * : Thank you for this suggestion ! We agree that both baselines you suggested are reasonable to compare , therefore we have added Appendix C.4 comparing NES to them . Note that hyperparameter ensembles [ 1 ] is concurrent work to ours ( ours posted on arXiv a week earlier ) , and while both papers show that varying particular hyperparameters ( note architecture is a hyperparameter ) is beneficial , the question of which hyperparameters one should vary remains open and is left for future work . Nonetheless , we perform a comparison to two new baselines in Appendix C.4 : 1 . HyperEns : ensembles with a fixed , optimized architecture but varying learning rates and L2 regularization strengths , and 2 . NES-RS ( depth , width ) : ensembles with architectures varying only in terms of width and depth , keeping the cell fixed . The results show that NES tends to improve upon these baselines . Please refer to Appendix C.4 for details . 3 . * * \u201c Another missing part in this paper is the cost analysis of NES \u201d * * : Thanks for this important comment ! We have added a paragraph in Section 5 comparing the computational cost of NES vs. baselines , explaining why NES is not necessarily more costly than DeepEns . In summary , DeepEns baselines have two costs ( which are subsumed into one cost in NES as explained below ) : finding an optimized , fixed base learner architecture and training M initializations of it . While both steps incur a cost , the former step can be extremely costly ( e.g.where it is not clear what base learner architecture is best given a set of choices , requiring the use of random search or a typical NAS algorithm to find an optimized architecture ) . A concrete example from our paper is the deep ensemble made using AmoebaNet , which is an architecture found using a regularized evolution run that required 3150 GPU days . As you mention , \u201c as far as I know , there is no guidance or automatic mechanism in ensembling neural networks with different architectures \u201d ; NES combines the architecture search and ensembling components , with its only cost being the training of K architectures to form the pool . Also , see Table 3 regarding costs in Appendix C.3 ."}, {"review_id": "6M4c3WegNtX-3", "review_text": "The paper explores whether one can use Architecture Search to enhance ensemble diversity . They start with the observation that embeddings generated by different architectures ( for multiple different initialization per architecture ) are well separated from each other . They then try out a couple of architecture search methods to find ensembles with diverse architectures that minimize the loss . While the novelty is incremental , I like the idea in general . My main objection is that critical baselines are not compared with . Ensemble diversity is a well explored topic with multiple easy to implement regularizations to increase diversity [ 1 , 2 , 3 ] and several more that should be compared with . [ 1 ] \u201c Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles \u201d by S Jain , G Liu , DK Gifford ( AAAI 2020 ) [ 2 ] \u201c Ensemble learning via negative correlation \u201d by Y. Liu , X. Yao ( Neural Networks 1999 ) [ 3 ] \u201c Uncertainty in Neural Networks : Approximately Bayesian Ensembling \u201d by Pearce et al ( AISTATS 2020 )", "rating": "4: Ok but not good enough - rejection", "reply_text": "Many thanks for taking the time to read our paper and for your feedback . Below we address the reviewer \u2019 s main concerns : 1 . * * '' While the novelty is incremental , I like the idea in general . `` * * : Thanks for your kind words ! Regarding novelty : to our knowledge , ensembles with varying architectures have not previously been considered in the context of uncertainty calibration and dataset shift , and our work is the first to utilize ideas from NAS to build algorithms for automatically selecting the architectures and to show that this yields improvements in an empirical evaluation using state-of-the-art NAS search spaces . 2 . * * '' My main objection is that critical baselines are not compared with . Ensemble diversity is a well explored topic with multiple easy to implement regularizations to increase diversity [ 1 , 2 , 3 ] and several more that should be compared with . `` * * : Thank you for pointing us to these papers ( added to our related work ! ) . We do believe that we use meaningful baselines , which was also stated by AnonReviewer4 : * '' The authors made comparisons to several reasonable baselines . `` * A central aim of our work is to investigate the impact of varying architectures in an ensemble from the perspective of uncertainty estimation and dataset shift ; we chose our baselines in accordance with this aim . Deep ensembles which keep the architecture fixed are therefore a natural baseline where the architecture is optimized and chosen from the same search space over which NES operates ( e.g.DARTS and AmoebaNet architectures ) . Crucially , the training pipeline is identical for base learners in NES and deep ensembles . Diversity regularizing methods such as [ 1 ] , [ 2 ] and [ 3 ] can be implemented on top of NES ensembles ( i.e.train the selected architectures with diversity regularizing penalties ) . Also , note that work such as [ 4 ] have compared deep ensembles to multiple ensembling techniques ( including snapshot ensembles , fast geometric ensembling , SWA-Gaussian , cyclical SGLD and dropout ) , showing that * \u201c most of the popular ensembling techniques require averaging predictions across dozens of samples ( members of an ensemble ) , yet are essentially equivalent to an ensemble of only few independently trained models. \u201d * Based on the results of that paper , we believe deep ensembles form a difficult-to-beat baseline . -- References -- [ 1 ] \u201c Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles \u201d by S Jain , G Liu , DK Gifford ( AAAI 2020 ) [ 2 ] \u201c Ensemble learning via negative correlation \u201d by Y. Liu , X. Yao ( Neural Networks 1999 ) [ 3 ] \u201c Uncertainty in Neural Networks : Approximately Bayesian Ensembling \u201d by Pearce et al ( AISTATS 2020 ) [ 4 ] \u201c Pitfalls of in-domain uncertainty estimation and ensembling in deep learning \u201d by Ashukha et al . ( ICLR 2020 )"}], "0": {"review_id": "6M4c3WegNtX-0", "review_text": "The paper suggests a new approach to the construction of ensembles of deep neural networks ( DNN ) . Unlike previous methods which usually deal with multiple DNNs of same structure authors propose to form an ensemble of networks with different architecture . The main claim is that using diverse architectures increases diversity and hence the quality of predictions . To find the best architectures they use methodology inspired by neural architecture search ( NAS ) in particular random search and regularized evolution . The method for neural ensemble search ( NES ) is algorithmically simple although computationally hard . On several experiments the authors show NES outperforms standard deep ensembles formed from networks with same ( even optimal ) structure both in terms of test NLL and in terms of uncertainty estimation under domain shift . Pros.Nice idea Simple algorithm Cons . My main point for the criticism is the lack of experiment which I find to be crucially important namely the comparison aganist deep ensemble of DNNs with same architecture to which ForwardSelect procedure has been applied . Train P DNNs with same architecture then perform ForwardSelect routine to take the best K of them and compare your method with such deep ensemble . Currently the authors only compare their method with deep ensembles to which no special selection procedure was applied . This causes bias and it is not clear whether the improvement in NES is due to the usage of different architectures or due to the selection procedure which encourages diversity in resulting ensemble . P.S.Please correct me if I misunderstood the last point . I have read the corresponding part twice and found no evidence that you 're using ForwardSelection when analysing the performance of ensembles of DNNs with same architecture . =UPDATE My concerns were partly addressed in author 's response so I have raised my score to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . Below we address the reviewer \u2019 s main concern . 1 . * * '' My main point for the criticism is the lack of experiment which I find to be crucially important namely the comparison against deep ensemble of DNNs with same architecture to which ForwardSelect procedure has been applied . Train P DNNs with same architecture then perform ForwardSelect routine to take the best K of them and compare your method with such deep ensemble . Currently the authors only compare their method with deep ensembles to which no special selection procedure was applied . This causes bias and it is not clear whether the improvement in NES is due to the usage of different architectures or due to the selection procedure which encourages diversity in resulting ensemble . `` * * : Thank you for this suggestion ! We agree this is an important study to gain insight into the improvement from NES , and we have added this ablation to our work in Appendix C.3 . As you described , we compare to additional deep ensemble baselines ( called \u201c DeepEns + ES \u201d ) which select the ensemble from a pool of trained random initializations of a fixed , optimized architecture . Our results show that NES algorithms continue to outperform these baselines . Also , note that the cost of \u201c DeepEns + ES \u201d baselines is substantially higher at the ensembling stage than usual deep ensembles , as we now train a pool of random initializations instead of just M random initializations ( M = ensemble size ) . In fact , the total cost ends up becoming larger than NES , since for DeepEns + ES we first need to find a good architecture and then train it multiple times to form a pool ( as in NES ) . Appendix C.3 contains further discussion on these points . We hope the reviewer will consider increasing their score , as we have added the experiment they suggested . We welcome any questions ."}, "1": {"review_id": "6M4c3WegNtX-1", "review_text": "The paper proposes creating diverse ensembles of neural networks using an evolutionary method for finding base learners with high performance as well as mutual diversity . The selected base learners are then aggregated for an ensemble using a known method for ensemble selection . The paper is generally well written and addresses a relevant problem of constructing ensembles while training neural networks instead of building models first , independently , and later constructing ensembles . Having said that , the paper lacks a significant contribution . The second phase ( Ensemble Selection ) of the proposed method is essentially the algorithm from Rich Caruana et al.2004.The first phase of Pool Building suggests either a random generation or alternatively a vaguely described evolutionary method , lacking details or analysis . It is not clear how exactly is the random initialization of architectures performed . Do you randomly select from a set of seed architectures or randomly create ( i.e. , random number of layers , random number of units , random activation functions , random initial weights , etc . ) ? Growing from a random neural architectures to multiple highly performing ( besides being mutually diverse ) through single permutations upon model training and evaluation seems like an expensive process . An evolutionary approach in such a manner seems in efficient . Can you report the time taken for some of the reported cases in the evaluation ? The ensemble methodology of unweighted averaging is fairly naive . What was the reason to select this one particularly ? Contribution # 1 ( page 2 ) is n't really a contribution . It is common knowledge amongst practitioners . The proposed method can lead to overfitting because the search seems to be based on a fixed set for evaluation . Regarding evaluation -- Can you explain how that is addressed ? Have you evaluated your method on a broader variety of datasets ? Can you confirm that the test data used for search/optimization is different than the one used for measuring reported performance ? Did you consider comparing it other methods such as Tao 2019 ( mentioned below ) ? This paper can improve its literature survey by citing more directly relevant work in ensemble search using diversification . Here are few examples of more sophisticated ensemble evolution work , not necessarily for a DL base learner , but relevant nonetheless : -Bhowan , et al.2013.Evolving diverse ensembles using genetic programming for classification with unbalanced data . Trans.Evol.Comp -Khurana et al.2018.Ensembles with Automated Feature Engineering . AutoML at ICML . -Olson et al.2019.TPOT : A Tree-Based Pipeline Optimization Tool for Automating Machine Learning . Automated ML . -Tao , 2019 . Deep Neural Network Ensembles . Machine Learning , Optimization , and Data Science . -Yao et al. , 2008 . Evolving artificial neural network ensembles , in IEEE Computational Intelligence Magazine Overall , it is a good problem , but this paper falls well short of the threshold . Update : I thank the authors for their response . Some justifications are provided and for that I will change my score . Overall , the paper still needs work .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments and the references . We have seen similar remarks previously and a number of them are addressed in our work ; we refer to the appropriate sections . 1 . * * \u201c ensemble selection ... is essentially Caruana et al . ( 2004 ) \u201d * * : We cited Caruana et al . ( 2004 ) in explaining our choice of forward selection on pg . 4 and in the related work section , and we do not claim forward selection to be a contribution . 2 . * * \u201c vaguely described evolutionary method , lacking details or analysis \u201d * * : Can you please specify what you felt is lacking in our description of NES-RE ? We have described NES-RE in Section 4.2 and Figure 2 . We have also provided pseudocode in Algorithm 2 , implementation and scalability details in Appendix B.4 and code in the supplementary material . 3 . * * \u201c Do you randomly select from a set of seed architectures or randomly create \u201d * * : The architecture search spaces and how we randomly sample architectures are described in Appendices B.1 and B.3 . In short , architectures in the DARTS search space are specified by cells which are DAGs where each edge represents an operation ( e.g.max pooling , separable convolution ) . We sample both the structure of the cell and the operations at each edge . 4 . * * \u201c ensemble methodology of unweighted averaging is fairly naive \u201d * * : Despite it being simple and \u201c na\u00efve \u201d , unweighted averaging is used in deep ensembles ( and works well ) , and in order to isolate and evaluate the impact of varying architectures , we also used unweighted averaging . Note that multiple popular neural network ensembling techniques also use unweighted averaging ( e.g.deep ensembles , snapshot ensembles , fast geometric ensembling etc . ) Nonetheless , more sophisticated ensemble combination methods could readily be used with NES . 5 . * * \u201c The proposed method can lead to overfitting \u201d * * : While it is difficult to guarantee no overfitting , in our experiments , we did not experience evidence for overfitting during ensemble selection despite using a fixed validation set . This is evident in Figures 4 , 7 , 17 , since test performance of NES improves with increasing pool size/budget . 6 . * * \u201c Regarding evaluation -- Can you explain how that is addressed ? Have you evaluated your method on a broader variety of datasets ? Can you confirm that the test data used for search/optimization is different than the one used for measuring reported performance ? \u201d * * : Section 5 evaluates NES on 5 datasets ( FMNIST , CIFAR-10 , CIFAR-100 , ImageNet-16-120 and Tiny ImageNet ) and 2 architecture search spaces ( DARTS search space and NAS-Bench-201 ) , using 3 metrics ( NLL , classification error , ECE ) including when there is test-time dataset shift . Regarding the use of test data , yes , \u201c unless stated otherwise , all evaluations are on the test dataset \u201d ( section 5 ) , and Algorithms 1-2 only use D_train , D_val . Using suggestions from reviewers , we have also added comparisons to new baselines in Appendices C.3 and C.4 . Overall , we believe that NES \u2019 empirical performance has been extensively evaluated , as also noted by AnonReviewer4 : * \u201c The authors made comparisons to several reasonable baselines . The improvement of the proposed NES-RE/RS method over fixed architecture ensembles is consistent and significant. \u201d * 7 . * * \u201c Did you consider comparing it to other methods such as Tao 2019 ? \u201d * * : Thank you for the reference . We have not compared to Tao 2019 , because this seems akin to a boosting-based approach which requires training base learners sequentially ( at least , partially ) . First , this is time-consuming for large networks and ensemble sizes ( e.g.size 30 in our experiments ) in contrast with randomization-based approaches for ensembling , such as NES and deep ensembles , which are readily parallelized . We have also been unable to find code implementation for Tao 2019 . Second , in the context of predictive uncertainty and dataset shift , ensemble diversity due to randomization-based approaches reduces overconfident predictions by individual baselearners . It is unclear this benefit would be retained by boosting-based approaches which optimize solely for predictive performance on in-distribution data ."}, "2": {"review_id": "6M4c3WegNtX-2", "review_text": "The authors addressed my concerns in the rebuttal . I have raised my score . Summary : This paper combines AutoML techniques and deep ensembles to improve the ensemble diversity so that it improves the entire ensemble quality in both in- and out-of-distribution dataset . The authors made a study on two possible AutoML methods which can be combined with ensembles : 1 . Random search & 2 . Regularized mutation . The empirical results showed that the proposed NES methods outperform commonly selected baselines . Pros : How to improve ensemble diversity is one of the core topics on the way to better ensemble performance ( in terms of both accuracy and uncertainty metrics ) . Many previous research works focused on 1 . Build efficient ensembles while remaining reasonable diversity ; 2 . Improve ensemble diversity by exploring the hyper-parameter space . In practice , it is standard to ensemble neural networks with different depths or widths to improve diversity and hence the ensemble performance . However , as far as I know , there is no guidance or automatic mechanism in ensembling neural networks with different architectures . Thus , the problem this paper aims to tackle is significant and it will benefit the research community . The authors did a self-contained introduction on ensembles & uncertainty and AutoML . The coverage on mutation AutoML is limited but this is still reasonable due to the page constraint . The motivation of why we want to combine AutoML and deep ensembles is clearly stated in section 3.2 . Figure 1 demonstrates the effectiveness of various architectures in promoting ensemble diversity . The empirical evaluation ranges across CIFAR dataset and ImageNet . It also includes calibration performance under dataset shift ( uncertainty estimation on out-of-distribution dataset ) , which is the common benchmark to evaluate an ensemble 's performance . The authors made comparisons to several reasonable baselines . The improvement of the proposed NES-RE/RS method over fixed architecture ensembles is consistent and significant . Cons : Figure 1 demonstrates the motivation behind this work . To be more convincing , it can be supplemented with the predictive disagreement on the testset or the averaged KL divergence between the predictive distribution among ensemble members . Moreover , the figure compares diversity between ensembles with different architectures and ensembles with random seeds . For a more comprehensive study , the figure can include a study on ensembles with different hyper-parameters . A more interesting baseline I will mention below is an ensemble with different depths . The baselines considered in this paper only include ensembles with a fixed architecture . It would be more convincing if the authors can include other baselines which include ensembles with different architectures ( without neural architecture search ) . For example , one naive baseline would be ensembling DeepEns ( Optimal ) with different depths ( fully trained independently ) . This highlights the need for neural architecture search . It also helps to understand how much diversity in architecture ( among ensemble members ) we need in order to achieve desired diversity in ensemble predictions . Additionally , it is encouraged to compare to hyper-parameter ensembles . This uncovers the question of which axis ( hyper-parameter & architectures ) is more effective in promoting an ensemble \u2019 s performance . Another missing part in this paper is the cost analysis of NES and how much does it increase compared to deep ensembles . Both NES-RS and NES-RE require training after sampling one neural architecture . This leads to a non-trivial computational overhead compared to traditional deep ensembles . In section 4.3 , it mentioned that a proportion of validation data encapsulates the belief about test-time shift . The authors didn \u2019 t mention whether the same protocol is applied to baselines ( DeepEns ) . This leads to a question that is the improvement on out-of-distribution calibration coming from NES or shifted validation set . I consider this is a minor issue because Figure 21 in the appendix also shows that the clear improvement without shifted validation data . Table 1 shows that larger ensemble size leads to worse ensembling performance . This is against our general intuition in deep ensembles where more ensemble members lead to better performance . My guess is more ensemble members leads to optimization difficulties in NES . I expect to see more discussion on this observation . Overall , the authors propose a compelling solution to automatically design the neural network architectures in deep ensembels . However , the cons slightly outweight the pros in this version .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Many thanks for your detailed , helpful review and for appreciating our work . We address your concerns below by incorporating your suggestions into our work and hope that you will consider updating your score : 1 . * * \u201c To be more convincing , [ Figure 1 ] can be supplemented with the predictive disagreement on the test set \u201d * * : Thanks for this suggestion ! We have added a comparison of the predictive disagreement in an ensemble with fixed architecture vs an ensemble with varying architectures in the last paragraph of Section 3.2 . The results ( 11.88 % vs. 10.51 % disagreement for varying vs. fixed architecture ensembles respectively ) show that varying the architecture also yields higher predictive disagreement , i.e.higher diversity . 2 . * * \u201c The baselines considered in this paper only include ensembles with a fixed architecture . It would be more convincing if the authors can include other baselines which include ensembles with different architectures ( without neural architecture search ) . For example , one naive baseline would be ensembling DeepEns ( Optimal ) with different depths ( fully trained independently ) . This highlights the need for neural architecture search . It also helps to understand how much diversity in architecture ( among ensemble members ) we need in order to achieve desired diversity in ensemble predictions . Additionally , it is encouraged to compare to hyper-parameter ensembles . This uncovers the question of which axis ( hyper-parameter & architectures ) is more effective in promoting an ensemble \u2019 s performance. \u201d * * : Thank you for this suggestion ! We agree that both baselines you suggested are reasonable to compare , therefore we have added Appendix C.4 comparing NES to them . Note that hyperparameter ensembles [ 1 ] is concurrent work to ours ( ours posted on arXiv a week earlier ) , and while both papers show that varying particular hyperparameters ( note architecture is a hyperparameter ) is beneficial , the question of which hyperparameters one should vary remains open and is left for future work . Nonetheless , we perform a comparison to two new baselines in Appendix C.4 : 1 . HyperEns : ensembles with a fixed , optimized architecture but varying learning rates and L2 regularization strengths , and 2 . NES-RS ( depth , width ) : ensembles with architectures varying only in terms of width and depth , keeping the cell fixed . The results show that NES tends to improve upon these baselines . Please refer to Appendix C.4 for details . 3 . * * \u201c Another missing part in this paper is the cost analysis of NES \u201d * * : Thanks for this important comment ! We have added a paragraph in Section 5 comparing the computational cost of NES vs. baselines , explaining why NES is not necessarily more costly than DeepEns . In summary , DeepEns baselines have two costs ( which are subsumed into one cost in NES as explained below ) : finding an optimized , fixed base learner architecture and training M initializations of it . While both steps incur a cost , the former step can be extremely costly ( e.g.where it is not clear what base learner architecture is best given a set of choices , requiring the use of random search or a typical NAS algorithm to find an optimized architecture ) . A concrete example from our paper is the deep ensemble made using AmoebaNet , which is an architecture found using a regularized evolution run that required 3150 GPU days . As you mention , \u201c as far as I know , there is no guidance or automatic mechanism in ensembling neural networks with different architectures \u201d ; NES combines the architecture search and ensembling components , with its only cost being the training of K architectures to form the pool . Also , see Table 3 regarding costs in Appendix C.3 ."}, "3": {"review_id": "6M4c3WegNtX-3", "review_text": "The paper explores whether one can use Architecture Search to enhance ensemble diversity . They start with the observation that embeddings generated by different architectures ( for multiple different initialization per architecture ) are well separated from each other . They then try out a couple of architecture search methods to find ensembles with diverse architectures that minimize the loss . While the novelty is incremental , I like the idea in general . My main objection is that critical baselines are not compared with . Ensemble diversity is a well explored topic with multiple easy to implement regularizations to increase diversity [ 1 , 2 , 3 ] and several more that should be compared with . [ 1 ] \u201c Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles \u201d by S Jain , G Liu , DK Gifford ( AAAI 2020 ) [ 2 ] \u201c Ensemble learning via negative correlation \u201d by Y. Liu , X. Yao ( Neural Networks 1999 ) [ 3 ] \u201c Uncertainty in Neural Networks : Approximately Bayesian Ensembling \u201d by Pearce et al ( AISTATS 2020 )", "rating": "4: Ok but not good enough - rejection", "reply_text": "Many thanks for taking the time to read our paper and for your feedback . Below we address the reviewer \u2019 s main concerns : 1 . * * '' While the novelty is incremental , I like the idea in general . `` * * : Thanks for your kind words ! Regarding novelty : to our knowledge , ensembles with varying architectures have not previously been considered in the context of uncertainty calibration and dataset shift , and our work is the first to utilize ideas from NAS to build algorithms for automatically selecting the architectures and to show that this yields improvements in an empirical evaluation using state-of-the-art NAS search spaces . 2 . * * '' My main objection is that critical baselines are not compared with . Ensemble diversity is a well explored topic with multiple easy to implement regularizations to increase diversity [ 1 , 2 , 3 ] and several more that should be compared with . `` * * : Thank you for pointing us to these papers ( added to our related work ! ) . We do believe that we use meaningful baselines , which was also stated by AnonReviewer4 : * '' The authors made comparisons to several reasonable baselines . `` * A central aim of our work is to investigate the impact of varying architectures in an ensemble from the perspective of uncertainty estimation and dataset shift ; we chose our baselines in accordance with this aim . Deep ensembles which keep the architecture fixed are therefore a natural baseline where the architecture is optimized and chosen from the same search space over which NES operates ( e.g.DARTS and AmoebaNet architectures ) . Crucially , the training pipeline is identical for base learners in NES and deep ensembles . Diversity regularizing methods such as [ 1 ] , [ 2 ] and [ 3 ] can be implemented on top of NES ensembles ( i.e.train the selected architectures with diversity regularizing penalties ) . Also , note that work such as [ 4 ] have compared deep ensembles to multiple ensembling techniques ( including snapshot ensembles , fast geometric ensembling , SWA-Gaussian , cyclical SGLD and dropout ) , showing that * \u201c most of the popular ensembling techniques require averaging predictions across dozens of samples ( members of an ensemble ) , yet are essentially equivalent to an ensemble of only few independently trained models. \u201d * Based on the results of that paper , we believe deep ensembles form a difficult-to-beat baseline . -- References -- [ 1 ] \u201c Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles \u201d by S Jain , G Liu , DK Gifford ( AAAI 2020 ) [ 2 ] \u201c Ensemble learning via negative correlation \u201d by Y. Liu , X. Yao ( Neural Networks 1999 ) [ 3 ] \u201c Uncertainty in Neural Networks : Approximately Bayesian Ensembling \u201d by Pearce et al ( AISTATS 2020 ) [ 4 ] \u201c Pitfalls of in-domain uncertainty estimation and ensembling in deep learning \u201d by Ashukha et al . ( ICLR 2020 )"}}