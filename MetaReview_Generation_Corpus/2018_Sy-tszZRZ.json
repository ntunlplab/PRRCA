{"year": "2018", "forum": "Sy-tszZRZ", "title": "Bounding and Counting Linear Regions of Deep Neural Networks", "decision": "Reject", "meta_review": "Dear authors,\n\nThe reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets. That said, the improvement over existing work (especially Montufar, 2017) is minor. This, combined with the limited attraction of such work, means that the paper will not be accepted.\n\nI acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to re-review a significantly updated version.", "reviews": [{"review_id": "Sy-tszZRZ-0", "review_text": "This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning \"Notes on the number of linear regions of deep neural networks\" by Montufar. The improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. The improved upper bound for maxout networks follows a similar intuition but appears to be novel. The paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. Here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are currently working on addressing all the comments of the reviewers . However , we would like to provide a brief update on the status and current progress on our part in addressing the concerns . 1 ) `` The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning `` Notes on the number of linear regions of deep neural networks '' by Montufar . '' We were not aware of the paper in our original submission and we searched for it but it is not available online . We have emailed the author for a copy . As soon as we obtain one , we will clarify the relationship between both papers ."}, {"review_id": "Sy-tszZRZ-1", "review_text": "Paper Summary: This paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network. It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions. These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. This paper improves on the upper bound given by [2] and the lower bound given by [1]. They also provide a tight bound for the one dimensional input case. Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST Main Comments: The paper is very well written and clearly states and explains the contributions. However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds, with no other novel interpretations or insights into deep architectures. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better. I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful. Overall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance. [1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio [2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are currently working on addressing all the comments of the reviewers . However , we would like to provide a brief update on the status and current progress on our part in addressing the concerns . 1 ) `` [ ... ] The new bounds proposed ( Theorem 1 , Theorem 6 ) , seem like small improvements over the previously proposed bounds , with no other novel interpretations or insights into deep architectures . '' A novel interpretation derived from Theorem 1 is on the relationship between the number of linear regions and the widths of the layers of the DNN . We emphasize that in Theorem 1 , the summations depend on the minimum width across the previous layers . This yields the insight that the number of regions is affected by a bottleneck-like effect from earlier layers . In other words , the bound from Theorem 1 is smaller if the earlier layers are smaller rather than if the later layers are smaller , fixed the total size of the network . This is reflected in the upper bound plot in Figure 4 ( b ) and further validated by the computational results shown in the same figure . In addition , the insights behind Theorem 1 pave the road to Theorem 5 , which exploits the dimensionality of the regions in order to achieve the exact maximal number of regions for the one-dimensional case . The case of more dimensions has proven to be more challenging , as evidenced by previous papers on the topic , but Theorem 6 nevertheless achieves a modest improvement . It generalizes the insight of Theorem 5 to higher dimensions . We will elaborate on this discussion in the paper . 2 ) `` I would like to see the technique of counting linear regions used even approximately for larger networks , where even though the results are an approximation , the takeaways might be more insightful . '' We agree with the reviewer that more insight could be obtained with larger networks . However , exact counting has never been done before and we are excited about this new capability . While this is not fully scalable in the current form , this serves as a proof-of-concept that already provides insights even at a small scale . Nevertheless , as the reviewer correctly pointed out , moving towards larger networks may require approximations . While we have already been thinking about using approximations , this is a different line of research and may need substantial additional work . 3 ) `` [ ... ] as a result the experiments are on extremely small networks ( width 8 ) , which only achieve 90 % accuracy on MNIST . '' In order to partially address this concern , we are working on counting ( possibly larger ) networks with higher accuracy ."}, {"review_id": "Sy-tszZRZ-2", "review_text": "This is quite an interesting paper. Thank you. Here are a few comments: I think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix. However I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved). I can appreciate though that this a fine line to walk. Also in the appendix, please restate the lemma that is being proven. Otherwise one will have to scroll up and down all the time to understand the proof. I think the paper could also discuss a bit more in detail the results provided. For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice. Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets. **************** I had reduced my score based on the observation made by Reviewer 1 regarding the talk Montufar at SampTA. Could the authors prioritize clarification to that point ! - Thanks for the clarification and adding this citation. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are currently working on addressing all the comments of the reviewers . However , we would like to provide a brief update on the status and current progress on our part in addressing the concerns . 1 ) `` [ ... ] I would have appreciated a little bit more details about the proofs in the main text . [ ... ] I think the paper could also discuss a bit more in detail the results provided . '' We will improve the discussion and move some of the contents from the Appendix to the main section . 2 ) `` [ ... ] observation made by Reviewer 1 regarding the talk Montufar at SampTA . '' Please see our answer to AnonReviewer1 ."}], "0": {"review_id": "Sy-tszZRZ-0", "review_text": "This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning \"Notes on the number of linear regions of deep neural networks\" by Montufar. The improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. The improved upper bound for maxout networks follows a similar intuition but appears to be novel. The paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. Here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are currently working on addressing all the comments of the reviewers . However , we would like to provide a brief update on the status and current progress on our part in addressing the concerns . 1 ) `` The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning `` Notes on the number of linear regions of deep neural networks '' by Montufar . '' We were not aware of the paper in our original submission and we searched for it but it is not available online . We have emailed the author for a copy . As soon as we obtain one , we will clarify the relationship between both papers ."}, "1": {"review_id": "Sy-tszZRZ-1", "review_text": "Paper Summary: This paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network. It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions. These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. This paper improves on the upper bound given by [2] and the lower bound given by [1]. They also provide a tight bound for the one dimensional input case. Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST Main Comments: The paper is very well written and clearly states and explains the contributions. However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds, with no other novel interpretations or insights into deep architectures. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better. I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful. Overall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance. [1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio [2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are currently working on addressing all the comments of the reviewers . However , we would like to provide a brief update on the status and current progress on our part in addressing the concerns . 1 ) `` [ ... ] The new bounds proposed ( Theorem 1 , Theorem 6 ) , seem like small improvements over the previously proposed bounds , with no other novel interpretations or insights into deep architectures . '' A novel interpretation derived from Theorem 1 is on the relationship between the number of linear regions and the widths of the layers of the DNN . We emphasize that in Theorem 1 , the summations depend on the minimum width across the previous layers . This yields the insight that the number of regions is affected by a bottleneck-like effect from earlier layers . In other words , the bound from Theorem 1 is smaller if the earlier layers are smaller rather than if the later layers are smaller , fixed the total size of the network . This is reflected in the upper bound plot in Figure 4 ( b ) and further validated by the computational results shown in the same figure . In addition , the insights behind Theorem 1 pave the road to Theorem 5 , which exploits the dimensionality of the regions in order to achieve the exact maximal number of regions for the one-dimensional case . The case of more dimensions has proven to be more challenging , as evidenced by previous papers on the topic , but Theorem 6 nevertheless achieves a modest improvement . It generalizes the insight of Theorem 5 to higher dimensions . We will elaborate on this discussion in the paper . 2 ) `` I would like to see the technique of counting linear regions used even approximately for larger networks , where even though the results are an approximation , the takeaways might be more insightful . '' We agree with the reviewer that more insight could be obtained with larger networks . However , exact counting has never been done before and we are excited about this new capability . While this is not fully scalable in the current form , this serves as a proof-of-concept that already provides insights even at a small scale . Nevertheless , as the reviewer correctly pointed out , moving towards larger networks may require approximations . While we have already been thinking about using approximations , this is a different line of research and may need substantial additional work . 3 ) `` [ ... ] as a result the experiments are on extremely small networks ( width 8 ) , which only achieve 90 % accuracy on MNIST . '' In order to partially address this concern , we are working on counting ( possibly larger ) networks with higher accuracy ."}, "2": {"review_id": "Sy-tszZRZ-2", "review_text": "This is quite an interesting paper. Thank you. Here are a few comments: I think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix. However I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved). I can appreciate though that this a fine line to walk. Also in the appendix, please restate the lemma that is being proven. Otherwise one will have to scroll up and down all the time to understand the proof. I think the paper could also discuss a bit more in detail the results provided. For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice. Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets. **************** I had reduced my score based on the observation made by Reviewer 1 regarding the talk Montufar at SampTA. Could the authors prioritize clarification to that point ! - Thanks for the clarification and adding this citation. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are currently working on addressing all the comments of the reviewers . However , we would like to provide a brief update on the status and current progress on our part in addressing the concerns . 1 ) `` [ ... ] I would have appreciated a little bit more details about the proofs in the main text . [ ... ] I think the paper could also discuss a bit more in detail the results provided . '' We will improve the discussion and move some of the contents from the Appendix to the main section . 2 ) `` [ ... ] observation made by Reviewer 1 regarding the talk Montufar at SampTA . '' Please see our answer to AnonReviewer1 ."}}