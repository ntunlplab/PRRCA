{"year": "2019", "forum": "H1ldNoC9tX", "title": "Classification from Positive, Unlabeled and Biased Negative Data", "decision": "Reject", "meta_review": "The paper proposes an algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.\n\nThe reviewers and AC commonly note the critical limitation of practical value of the paper and results are rather straightforward.\n\nAC decided the paper might not be ready to publish as other contributions are not enough to compensate the issue.", "reviews": [{"review_id": "H1ldNoC9tX-0", "review_text": "The authors rst present standard binary (positive negative or PN) classi ca- tion, followed by positive unlabeled (PU) classi cation, that they motivate with examples, such as one-class remote sensing classi cation. The new setting that they introduce and study is called positive unlabeled biaised negative (PUbN classi cation) and adds a biaised negative sample to PU learning. They give motivating examples and compare this setting to the existing literature. A con- vincing case is made regarding the di erence between the PUbN problem and the known problems of semi-supervised learning and dataset shift. They start by recalling the notations and nature of standard binary classi - cation, PU classi cation and the nnPU (non-negative PU) strategy, as in the previous PU learning papers. Then, they present the semi-supervised setting under the name PNU learning, which simply studies the minimization of a con- vex combination of the PN risk and the PU risk. As in PU learning, a correction exists to avoid considering the estimate of the negative risk to be negative, re- ferred to as nnPNU. Finally, the authors introduce PUbN learning as the problem in which we only have access to negatives that follow the law p(x\\mid y = -1; s = +1), where s is a latent variable that formalizes the bias. As in PU learning, the authors derive an unbiased estimator of the risk that involves only distributions for which data is available. However, they need to reweight the P and bN distribution by the unknown posterior probability \u001bsigma(x) = p(s = +1\\mid x) of s. Considering s as the label, the problem of learning a probabilistic classi er separating the elements for which s = +1 and s = \udbc0\udc001 can be seen as a PU learning problem, which gives an estimator ^\u001b of sigma, and makes the method practical. They derive estimation error bounds, that depend on the mean squared di fference between \u001bsigma and sigma^\u001b and a term of order n^-1/2 where n is the cardinal of the smallest sample. They considered the function ^\u001b as a xed function in their bounds, which implies that the bounds are only true if some of the data is kept for the estimation of sigma^\u001b. Finally, they present a variant of their algorithm for PU learning, named PUbNnN where unlabeled instances are not all given the same weight, but weighted according to sigma hat\u001b. The experiments use neural networks with stochastic optimization, on the classic datasets MNIST, CIFAR-10 and 20 Newsgroup. They report better per- formance using their technique on all datasets. The authors documented their experiences thoroughly in the appendix. However, I did not find information about the nature of the estimator of the posterior probability sigma^\u001b, which is im- portant for reproducibility. Furthermore, in appendix B, choosing sigma^\u001b = 0 will minimize the criterion . Finally, they proceed to justify the dominance of the variant of their method over usual nnPU learning.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer2 , Thank you very much for taking your time to review our paper . Your concise summary of our paper shall be very helpful to anybody who is interested in our work . Below we address the two concerns raised at the end of the review . * * 1 . [ Nature of the estimator of the posterior probability \u03c3\u0302 ] * * - As for the learning algorithm , as mentioned in your comment , we use s as label and trained a probabilistic classifier to separate s=+1 and s=-1 by leveraging PU learning algorithms . See section 3.1 , Estimating \u03c3 : [ In other words , here we regard X_P and X_bN as P and X_U as U , and attempt to solve a PU learning problem by applying nnPU . ] - As for the model , we use always the same model for \u03c3\u0302 and the main classifier . See section 4.1 : [ For simplicity , in an experiment , \u03c3\u0302 and g always use the same model and are trained for the same number of epochs . ] - By the way , we will put our code on github after the reviewing process to ensure that all the experiments in our paper can be easily reproduced by anybody who is interested . * * 2 . [ In appendix B , choosing \u03c3\u0302=0 will minimize the criterion ] * * - This is not true , the criterion \\hat { J } is minimized when \u03c3\u0302~\u03c3 . The criterion is the empirical approximation of E_ { x~p ( x ) } [ |\u03c3\u0302 ( x ) -\u03c3 ( x ) |^2 ] plus some constant that is independent of \u03c3\u0302 . Due to the presence of this constant term , which is negative , \\hat { J } can be negative and is not minimized when \u03c3\u0302=0 in which case we have \\hat { J } =0 . Finally , we would like to thank you again for your detailed feedback and great summary of our paper ."}, {"review_id": "H1ldNoC9tX-1", "review_text": "This paper studied classification problem, with Positive, Unlabeled and biased Negative labeled data. The paper presents a two-step method, where the first-step is instance weighting and the second-step is standard binary classification. The paper shows theoretical proofs on the error estimation. Experiments on several well-known data sets are conducted and compared. The good things of the paper are clear. 1. Technical sound with statistical foundation 2. Theoretical foundation 3. Problem is general 4. Paper is general well written. Some weak points as well 1. Application value is not so big, as there is no real application problem and the experiments are based on simulation. 2. Although the studied problem is reasonable, the setup is a bit too general and need rather strict condition to have a good method. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your feedback and approving that our paper is well written . To address the two weak points : We agree that better methods can be developed given stricter conditions or if the problem becomes more specific , but this is a trade-off that one always needs to face : a general setup with a good but not perfect algorithm or an ad hoc algorithm that works really well but only for rather restricted situations . Furthermore , we think there are also problems for which our method can be readily applied , namely the ones that are mentioned in our paper and in the replies to other reviewers . At the same time , the problem that we formulate and the approach that we consider can also inspire people who want to pursue in similar directions to design algorithms under stricter conditions or for more specific problems ."}, {"review_id": "H1ldNoC9tX-2", "review_text": "This paper has proposed a new algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework. The paper was written in clarity and easy to follow overall. However, the original motivation for having biased negative data are not explained very clear. The relation to dataset shift was very interesting, but it\u2019s unclear what\u2019s the exact connection between the proposed algorithm and the dataset shift. Maybe the authors can elaborate a little more on their point here in the future revision. The paper has made some assumption about the relation between the latent random variable and the label in section 2.4. In the experiment, data sets are generated following the exact assumption. That\u2019s not surprising to see that the proposed algorithm that fits the assumption will perform better than the previous methods without this assumption. In practice, there\u2019s no way to really verify this assumption. Thus, it\u2019s more interesting to see how the algorithm performs under the more generic semi-supervised learning setting, with unbiased, or biased negatives that don\u2019t really fit the exact assumption in this paper. Moreover, I\u2019d like to see more intuition on why adding biased negative data will further improve upon nnPNU. The author provided some explanation in section 4.3, which seems just observations on the FPR and FNR, rather than the fundamental explanation for the advantage of this algorithm. Choice of baseline methods is also limited. The original paper [1] for PNU has included a bunch of benchmark algorithms for semi-supervised learning. The authors should also include more benchmark algorithms for comparison, e.g. those listed in Section 5.2 in [1]. [1] Sakai, Tomoya, et al. \"Semi-supervised classification based on classification from positive and unlabeled data.\" arXiv preprint arXiv:1605.06955 (2016).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , Thank you very much for your insightful comments . Instead of answering all the questions at once , we will address the issues that you raise point by point , and also revise the paper accordingly ."}], "0": {"review_id": "H1ldNoC9tX-0", "review_text": "The authors rst present standard binary (positive negative or PN) classi ca- tion, followed by positive unlabeled (PU) classi cation, that they motivate with examples, such as one-class remote sensing classi cation. The new setting that they introduce and study is called positive unlabeled biaised negative (PUbN classi cation) and adds a biaised negative sample to PU learning. They give motivating examples and compare this setting to the existing literature. A con- vincing case is made regarding the di erence between the PUbN problem and the known problems of semi-supervised learning and dataset shift. They start by recalling the notations and nature of standard binary classi - cation, PU classi cation and the nnPU (non-negative PU) strategy, as in the previous PU learning papers. Then, they present the semi-supervised setting under the name PNU learning, which simply studies the minimization of a con- vex combination of the PN risk and the PU risk. As in PU learning, a correction exists to avoid considering the estimate of the negative risk to be negative, re- ferred to as nnPNU. Finally, the authors introduce PUbN learning as the problem in which we only have access to negatives that follow the law p(x\\mid y = -1; s = +1), where s is a latent variable that formalizes the bias. As in PU learning, the authors derive an unbiased estimator of the risk that involves only distributions for which data is available. However, they need to reweight the P and bN distribution by the unknown posterior probability \u001bsigma(x) = p(s = +1\\mid x) of s. Considering s as the label, the problem of learning a probabilistic classi er separating the elements for which s = +1 and s = \udbc0\udc001 can be seen as a PU learning problem, which gives an estimator ^\u001b of sigma, and makes the method practical. They derive estimation error bounds, that depend on the mean squared di fference between \u001bsigma and sigma^\u001b and a term of order n^-1/2 where n is the cardinal of the smallest sample. They considered the function ^\u001b as a xed function in their bounds, which implies that the bounds are only true if some of the data is kept for the estimation of sigma^\u001b. Finally, they present a variant of their algorithm for PU learning, named PUbNnN where unlabeled instances are not all given the same weight, but weighted according to sigma hat\u001b. The experiments use neural networks with stochastic optimization, on the classic datasets MNIST, CIFAR-10 and 20 Newsgroup. They report better per- formance using their technique on all datasets. The authors documented their experiences thoroughly in the appendix. However, I did not find information about the nature of the estimator of the posterior probability sigma^\u001b, which is im- portant for reproducibility. Furthermore, in appendix B, choosing sigma^\u001b = 0 will minimize the criterion . Finally, they proceed to justify the dominance of the variant of their method over usual nnPU learning.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer2 , Thank you very much for taking your time to review our paper . Your concise summary of our paper shall be very helpful to anybody who is interested in our work . Below we address the two concerns raised at the end of the review . * * 1 . [ Nature of the estimator of the posterior probability \u03c3\u0302 ] * * - As for the learning algorithm , as mentioned in your comment , we use s as label and trained a probabilistic classifier to separate s=+1 and s=-1 by leveraging PU learning algorithms . See section 3.1 , Estimating \u03c3 : [ In other words , here we regard X_P and X_bN as P and X_U as U , and attempt to solve a PU learning problem by applying nnPU . ] - As for the model , we use always the same model for \u03c3\u0302 and the main classifier . See section 4.1 : [ For simplicity , in an experiment , \u03c3\u0302 and g always use the same model and are trained for the same number of epochs . ] - By the way , we will put our code on github after the reviewing process to ensure that all the experiments in our paper can be easily reproduced by anybody who is interested . * * 2 . [ In appendix B , choosing \u03c3\u0302=0 will minimize the criterion ] * * - This is not true , the criterion \\hat { J } is minimized when \u03c3\u0302~\u03c3 . The criterion is the empirical approximation of E_ { x~p ( x ) } [ |\u03c3\u0302 ( x ) -\u03c3 ( x ) |^2 ] plus some constant that is independent of \u03c3\u0302 . Due to the presence of this constant term , which is negative , \\hat { J } can be negative and is not minimized when \u03c3\u0302=0 in which case we have \\hat { J } =0 . Finally , we would like to thank you again for your detailed feedback and great summary of our paper ."}, "1": {"review_id": "H1ldNoC9tX-1", "review_text": "This paper studied classification problem, with Positive, Unlabeled and biased Negative labeled data. The paper presents a two-step method, where the first-step is instance weighting and the second-step is standard binary classification. The paper shows theoretical proofs on the error estimation. Experiments on several well-known data sets are conducted and compared. The good things of the paper are clear. 1. Technical sound with statistical foundation 2. Theoretical foundation 3. Problem is general 4. Paper is general well written. Some weak points as well 1. Application value is not so big, as there is no real application problem and the experiments are based on simulation. 2. Although the studied problem is reasonable, the setup is a bit too general and need rather strict condition to have a good method. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your feedback and approving that our paper is well written . To address the two weak points : We agree that better methods can be developed given stricter conditions or if the problem becomes more specific , but this is a trade-off that one always needs to face : a general setup with a good but not perfect algorithm or an ad hoc algorithm that works really well but only for rather restricted situations . Furthermore , we think there are also problems for which our method can be readily applied , namely the ones that are mentioned in our paper and in the replies to other reviewers . At the same time , the problem that we formulate and the approach that we consider can also inspire people who want to pursue in similar directions to design algorithms under stricter conditions or for more specific problems ."}, "2": {"review_id": "H1ldNoC9tX-2", "review_text": "This paper has proposed a new algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework. The paper was written in clarity and easy to follow overall. However, the original motivation for having biased negative data are not explained very clear. The relation to dataset shift was very interesting, but it\u2019s unclear what\u2019s the exact connection between the proposed algorithm and the dataset shift. Maybe the authors can elaborate a little more on their point here in the future revision. The paper has made some assumption about the relation between the latent random variable and the label in section 2.4. In the experiment, data sets are generated following the exact assumption. That\u2019s not surprising to see that the proposed algorithm that fits the assumption will perform better than the previous methods without this assumption. In practice, there\u2019s no way to really verify this assumption. Thus, it\u2019s more interesting to see how the algorithm performs under the more generic semi-supervised learning setting, with unbiased, or biased negatives that don\u2019t really fit the exact assumption in this paper. Moreover, I\u2019d like to see more intuition on why adding biased negative data will further improve upon nnPNU. The author provided some explanation in section 4.3, which seems just observations on the FPR and FNR, rather than the fundamental explanation for the advantage of this algorithm. Choice of baseline methods is also limited. The original paper [1] for PNU has included a bunch of benchmark algorithms for semi-supervised learning. The authors should also include more benchmark algorithms for comparison, e.g. those listed in Section 5.2 in [1]. [1] Sakai, Tomoya, et al. \"Semi-supervised classification based on classification from positive and unlabeled data.\" arXiv preprint arXiv:1605.06955 (2016).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , Thank you very much for your insightful comments . Instead of answering all the questions at once , we will address the issues that you raise point by point , and also revise the paper accordingly ."}}