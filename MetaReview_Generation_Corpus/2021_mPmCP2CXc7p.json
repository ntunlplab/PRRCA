{"year": "2021", "forum": "mPmCP2CXc7p", "title": "Dynamic Feature Selection for Efficient and Interpretable Human Activity Recognition", "decision": "Reject", "meta_review": "The authors propose a methodoloy for dynamic feature selection. They use differentiable gates with \nan RNN architecture to select different subsets of features at each time point thus resulting in dynamic selection. \nThe reviewers agree that the idea is interesting and the method could be useful and I share their opinion.\n\nThe majority vote is towards rejection. The overarching mwssage of the reviews is that the manuscript raises confusion in a number of points. I see this work as one with good potential for impact but its current presentation is confusing. The vivid discussion that it raised is also an indication of it. The authors have done a good job replying to the concerns and the questions raised. However, the reviewers were still unsatisfied with the authors response to their concerns.  I recommend rejection at this time, while encouraging the authors to take seriously the reviewers' requests for a clearer presentation of their approach's contribution in order to strengthen their paper for future submission.\n\n", "reviews": [{"review_id": "mPmCP2CXc7p-0", "review_text": "This paper proposes an RNN model for adaptive dynamic feature selection , for efficient and interpretable human activity recognition ( HAR ) . From the intuition that human activity can be predictable by using a small number of sensors , the paper introduces an l0-norm minimization problem with parameter regularization , and provide a logic on formulating a dynamic feature selection model with relaxations . The difficulty of the discrete optimization problem is solved by differentiable relaxation , which is known as Gumbel-Softmax reparameterization techniques . The formulation is naturally led to an RNN model that uses histories as input with an additional sigmoid unit for adaptive feature selection . Empirical studies are performed to show the superiority of the adaptive feature selection network . Results are shown on the task of 1 ) UCI-HAR smartphone dataset with 561 features , 2 ) UCI Opportunity sensor dataset with 242 features , 3 ) ExtraSensory dataset with 225 features for multilabel binary classification . In particular , by using the adaptive feature selection technique , the average number of features necessary for HAR prediction can be very small ( 0.3 % , 15.9 % , 11.3 % among all features ) at any given time . Overall , the paper is well written . In particular , analysis results on three datasets are clear and detailed , so that the reader would be available to understand what sensors were necessary for HAR prediction . The key concern about the paper is that the algorithm lacks practicality . To show the adaptive selection algorithm is efficient , it should be shown that the algorithm drastically reduces features that are not necessary for prediction over time , while maintaining the performance even in the lighter feature space . Although the average number of features selected by the adaptive selection algorithm for each snapshot is small , all features are entered as input , which may not help to speed up the algorithm . To claim that the algorithm is efficient , it is required to show that the computation cost can be saved . Also , based on the current experimental results , it is difficult to say that features that were not used in earlier timestamp will not be used in later timestamp with a different context . Minor comments and questions : - Can you report the running time of each model ? - Is this model working in an online setting without tuning ? If yes , would you like to clarify ? If no , may I think this technique is for maintaining a dashboard that informs important features every time to users by calculating feature importance over time ? - The performance of the adaptive method on the NTU-RGB-D dataset is quite poor . What part of the dataset do you think caused the difficulty in feature selection ? Do all features important ? - The technical novelty seems to be low if the proposed model is an RNN with an additional sigmoid layer . - Figure 2a does not have a ground truth blue line .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive comments and criticisms . Crucially , we clarify that our algorithm DOES NOT need all features to be entered as input . Using the previously observed feature sets , our model is able to select the next feature set to query without observing the entire feature set . As stated in our methodology section ( equations ( 2 ) , ( 3 ) , ( 4 ) , and ( 6 ) ) , the features that are not selected are set to zero and thus need not be observed . In practice , the next feature set to use can be determined without ever observing all of the features . We have shown through extensive experiments that our method drastically reduces the number of features that are not necessary for prediction for certain contexts . We also would like to point out the possible misunderstanding . Mostly importantly , our aim is not to have a compressed/simplified neural network model for activity recognition . Instead , we would like our dynamic feature selection framework to not only have good activity recognition performance , but also ( more importantly ) to inform which sensors should be turned on or off to achieve desired performance-cost tradeoff . Note that here the cost is incurred for sensor feature collection but not the inference for activity recognition based on given features . As we have stated in our manuscript , our claim on efficiency is not regarding computational efficiency , but rather efficiency in maintaining the sensors needed for prediction . Since not all sensors will be used at all times , we can use our gating method to safely TURN OFF sensors to achieve power efficiency . We answer the questions the reviewer listed below : - The running time of our model compared to a standard GRU is quite comparable . On the UCI HAR dataset , our model takes 11.15ms to predict each time point while a standard GRU takes 7.68ms . This is still within acceptable latencies for continuous activity monitoring . - The model learns a feature selection policy offline and uses the learned policy to select features during testing time . The parameters of the model and selection policy are fixed during testing and no online update of the parameters are done in this setting . The dynamic feature selection is achieved because the selection policy is conditioned on the previous observations to determine the next feature set to use . From this perspective , the technique can indeed be thought of as maintaining a dashboard indicating which features are important given the context summarized by the previous observations . - We note that we are still achieving competitive performance on the NTU-RGB-D dataset , with only about a 2.5 % accuracy drop compared to the baseline model which uses the entire sensor set . We attribute this to the fact that the dataset only contains 25 sensors in total . We believe that such a small sensor set is already curated to be the most informative and therefore , any additional sensor selection would slightly degrade the performance of a predictive model . - We note that our model isn \u2019 t simply \u201c an RNN with an additional sigmoid layer \u201d . In fact , adding a simple sigmoid layer would not produce any feature sparsity by itself as shown by our comparisons with an attention-based model . Instead , in our model , the next feature set is sampled using a learned discrete probability distribution conditioned on the previous observations . Such a model isn \u2019 t trivial to optimize , as backpropagation isn \u2019 t trivially done through the discrete probability distribution parameters . Moreover , we also introduce a regularization that is easily evaluated using a stochastic relaxation . The resulting module is then highly applicable to more complex architectures and modeling scenarios . - In Figure 2a , the orange and blue lines both overlap exactly , causing the blue line to be unseen . We will fix this in our revision of our manuscript . We truly appreciate the reviewer to reply to our post and ask more questions to ensure there are no more misunderstandings that would have affected your assessment of our method . At this point , we are very confident that our above clarifications have thoroughly addressed the potential confusions and misunderstandings in the current review ."}, {"review_id": "mPmCP2CXc7p-1", "review_text": "The authors tackle the important problem of feature selection . They propose to use differentiable gates with an RNN architecture to select different subsets of features for each time point . I think the idea and method are interesting , and the method could be useful . However , I have crucial problems with the way the paper is presented . Most importantly , the authors describe the l_0 relaxation of Bernoulli random variables as if it is their own contribution . They describe existing known results under a section titles \u201c Methodology \u201d as if they are the first to present Bernoulli random variables to feature selection or that they are the first to relax them using the Gumbel Softmax trick . They also use the word : \u201c we derive \u201d ( p.3 ) . This is wrong ! And misleading ! The same relaxation appears in [ 1 ] and used for model sparsification , the descriptions are almost identical to what appears in [ 1 ] with almost zero credit to the authors in [ 1 ] ( a citation appears in related work in a different context ) . Bernoulli relaxation was already used for feature selection , in [ 2 ] , and [ 3 ] , these papers were not even mentioned . The reader can think the authors are the first to introduce such relaxation into the problem of feature selection , while this is again , clearly wrong . The authors are well aware of that this relaxation was presented in [ 1 ] , and in the experiment section they describe the baseline which solves ( 4 ) by citing [ 1 ] ( citation [ 18 ] in their paper ) , this is again in contradiction to the way they describe the relaxation as if it is their own contribution . Putting these CRITICAL comments aside , I think the results are misleading . Specifically , comparing the average number of selected features to the ( constant ) number of selected features of the non-adaptive method is misleading . You need to compare the union of selected features by your method to the constant number , otherwise , there is no way to infer if this feature selection method can result in any compression of the model or could lead to training or inference speed up . Given that this is what you measure since you still need all the features to use your model , what are the advantages of the method ? Only interpretability ? The authors do not explain how the method is used in the testing phase , is the randomness removed ? How exactly ? The authors do not explain how training/ testing is performed , this appears in the appendix but should be moved to the main texts . The authors should compare the method to the distribution suggested in [ 1 ] , which seems more suitable for feature selection than the Concrete distribution ( used by the authors ) . Citations are not in the correct ICLR format . Some pros : I like the examples used in the paper as well as the comparison to ARM , ST , ST-ARM . To conclude , I am voting to reject the paper , based on all the reasons mentioned above . [ 1 ] Louizos , Christos , Max Welling , and Diederik P. Kingma . `` Learning Sparse Neural Networks through $ L_0 $ Regularization . '' ICLR , 2018 . [ 2 ] Yamada , Y. , Lindenbaum , O. , Negahban , S. , & Kluger , Y . Feature selection using stochastic gates . ICML , 2020 . [ 3 ] Bal\u0131n , Muhammed Fatih , Abubakar Abid , and James Zou . `` Concrete autoencoders : Differentiable feature selection and reconstruction . '' ICML.2019 .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for the constructive comments and criticisms . We will clarify confusions and improve the presentation of our manuscript . Here , we would like to * * strongly point out the possible misunderstanding by the reviewer * * . Most importantly , our aim is * * NOT * * to have a compressed/simplified neural network model for activity recognition . Instead , we would like our dynamic feature selection framework to not only have good activity recognition performance , but also ( more importantly ) to inform which sensors should be turned on or off to achieve desired performance-cost tradeoff . Here the cost is incurred for sensor feature collection but not the inference for activity recognition based on given features . In mobile sensing tasks , devices are typically equipped with a redundant set of sensors , and turning on/off those sensors accounts for a major portion of on-device energy budgets . As Reviewer 1 has insightfully pointed out : \u201c This has implications in energy consumptions of wearable sensors . but could even generalize to measurement timings in clinical care to make the work of nurses more efficient , and reduce the stress caused by some medical procedures. \u201d We consider dynamic feature selection as sequential context-dependent feature subset selection and cast it into a stochastic optimization formulation , enabling gradient-based solutions . The method we derived in our manuscript is a temporal feature selection method , which is different from the one presented in [ 1 ] , where the goal is neural network architecture sparsification . Unlike in [ 1 ] where the weight masking is done independent of the current input data , our feature selection method is context dependent and adapts to the input data currently being handled . This introduces non-trivial complications when solving the optimization problem , especially when using the ARM optimization strategies . Also different from [ 1 ] , our method selects features across time , while [ 1 ] is concerned with model compression . We note that many other methods in the literature can be derived as a relaxation similar to the one presented in [ 1 ] ( see [ a ] , [ b ] , [ c ] ) . The fact that our method uses a similar relaxation as [ 1 ] should not discredit its novelty . It was never our intention to misrepresent our work and we will modify our write-up to more clearly show its relation to existing work . The other works [ 2 ] and [ 3 ] that the reviewer cited are also unlike the method in our manuscript . Crucially , both deal with static feature selection as opposed to the dynamic feature selection method we are proposing . We beg to differ from the reviewer \u2019 s statement that we should compare the union of the selected features to the constant number of features selected by the non-adaptive method . As we have stated numerous times in our manuscript , the problem we are concerned with is regarding DYNAMIC feature selection for continuous human activity recognition . Here , we are tasked to give a prediction for each time point while achieving sensor power efficiency by dynamically selecting the sensor set used for each time point . Because we are able to select fewer sensors on average , we are able to keep more sensors turned off on average thus achieving greater power efficiency . Comparing the union of the selected sensors would not be representative of the sensor power consumed by our method , as not all sensors in the union would be turned on at all times when using our method . Nevertheless , we have computed the union of features selected . All of them still show our method to be superior to the non adaptive method . We list these numbers below : UCI HAR : 3.56 % . OPPORTUNITY : 19.83 % . ExtraSensory : 26.66 % . We emphasize that our model DOES NOT need all the features observed and to be entered as input . Using the previously observed features , our model is able to select the next feature set to query without observing the entire feature set . As stated in our methodology section ( equations ( 2 ) , ( 3 ) , ( 4 ) , and ( 6 ) ) , the features that are not selected are set to zero and thus need not be observed . In the testing phase , the randomness is not removed , we stated this in section 2.4 . We would place additional details regarding training/testing in the main manuscript if the page limit permits . However , we believe that the training details would detract from our main message of achieving efficiency and interpretability using our model . We are currently revising both the main text and supplement to try our best to make these points clearer . We would truly appreciate the reviewer to reply to our post and ask more questions , if our points are still not clear enough , to ensure there are no misunderstandings regarding our method and motivations to develop dynamic/adaptive feature selection . At this point , we are very confident that our above clarifications have thoroughly addressed the potential confusions and misunderstandings in the current review ."}, {"review_id": "mPmCP2CXc7p-2", "review_text": "This paper presents a learning-based binary sampling mechanism for feature selection . It filters salient feature dimensions by sampling from a Gumbel-softmax distribution , which is differentiable and can be trained with other network parameters . The proposed method is evaluated on several Human Activity Recognition ( HAR ) datasets . The positive and negative points of this paper can be summarized as following : pros : + This paper is well written and is easy to follow . + The experimental evaluations give positive results . cons : - Important previous works are missing . Learning to generate categorical samples for RNNs is not a fresh idea . Actually , [ a ] has already employs Gumbel-softmax to sample scales in order to dynamically control the temporal pattern learning ; More generally , the topic of this paper is connected to a amount of previous works aiming to adaptively decide how/when to memorize/update the inputs/states , such as [ b ] and [ c ] . These works should also been cited by this paper . - With these missing works taking into account , the novelty of this paper becomes incremental and contribution is trivial . Integrating Gumbel-softmax sampling with RNN cells is very straightforward , and the motivation of applying Gumbel-softmax is very similar to [ a ] . While [ a ] is proposed for general sequence tasks , the proposed method seems to work only for HAR with multi-dimensional inputs . - Since \\tau is the only hyper parameter of Gumbel-softmax , evaluations on how the value of \\tau could impact the performance can be important . Yet no such results are reported in the paper . From the original Gumbel-softmax paper we can see a sample can approximate to a one-hot vector when \\tau is small and be closed to a uniform distribution when \\tau goes large . So it is very likely that the performance will become unstable as \\tau changes . Showing such experimental results could be definitely improve the paper quality . I would suggest to report the means and stds of accuracies with different sampling seeds . Summary : Considering the concerns listed above , I believe there are problems that outweighs the strengths of this paper . They should be fixed before acceptance . [ a ] H Hu , et al.Learning to Adaptively Scale Recurrent Neural Networks . AAAI 2019 [ b ] V Campos , et al.Skip RNN : Learning to Skip State Updates in Recurrent Neural Networks . ICLR 2018 [ c ] D Neil , et al.Phased LSTM : Accelerating Recurrent Network Training for Long or Event-based Sequences . NIPS 2016", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive comments and criticisms . We emphasize that our method is a dynamic sensor selection method that infers which * * small subset of sensor features * * to use * * adaptively * * , at any given context or state indicated by the latent representation , * * at any given time point * * . In this way , its aim is to optimize the tradeoff between prediction accuracy and power and/or other incurring cost to maintain the sensor features , It is unlike any of the methods the reviewer has cited . [ a ] selects wavelet scales used for prediction , instead of which features to use at any given context . This is motivated by a need to capture multi-scale patterns to achieve a more expressive model , instead of for energy efficiency or interpretability . [ b ] selects which time steps to skip for updating the recurrent state , and applies this to sequence modeling tasks . This may not be appropriate for continuous monitoring tasks , where we are tasked to give a prediction at every time step . Moreover , it doesn \u2019 t enable interpretability of the model \u2019 s decisions . Indeed , with our model , we can maintain a small adaptive set of sensors for easily discriminable contexts , and we can observe these sensors for interpretation of the model \u2019 s behavior . [ c ] introduces a gating mechanism across time which controls the frequency with which the memory cell is updated . Such a selection is not context based , as the parameters of the gating mechanism , such as the period and the phase shift , are independent of the current state of information given in order to optimize both prediction accuracy and sensor usage . On the other hand , our dynamic selection mechanism is context dependent , it uses the previous observations to infer the next feature set to use in order to optimize both the prediction accuracy and sensor usage . We further note that all the references applied selection or skipping along the temporal direction , which is only loosely related to our work , to say the least . Our proposed dynamic feature selection focuses on which sensor features across time points should be selected to achieve the desired performance-cost tradeoff and the feature selection is adaptive with respect to the underlying states or contexts of the corresponding activities and environment . Our dynamic/adaptive feature selection is orthogonal to temporal selection/skipping and we do believe these are two different research questions . But we will be happy to cite those papers as our broader context , and we can explore the potential of integrating these two directions as our future research . Gumbel-softmax is one strategy to solve our proposed optimization formulation ( the true originality and merit ) : we neither claimed it as our originality , nor consider it the only way to go ; in face , we also state that the Gumbel-softmax along with many other categorical selection mechanisms have been used for many purposes such as attention , model compression , mixture of experts , Neural Turing Machines , etc . Usage of such a selection mechanism alone should not be grounds to say that a method is not novel . We would consider these are potential solution strategies to achieve our proposed dynamic/adaptive feature selection . Indeed , we benchmarked multiple methods to optimize our dynamic feature selection formulation and found that the Gumbel-softmax performed most favorably for our applications . We have tested different hyperparameters \\tau ranging from 0.001 to 5 and did not notice any significant differences in performance . We will shortly update our manuscript to include our comparisons on \\tau in the appendix . We hope the above can clarify several confusions and make our novelty & merit much more clear to the reviewer ."}, {"review_id": "mPmCP2CXc7p-3", "review_text": "The authors provide a novel combination of known architectures to an important use case of reducing the density of required measurements in sensor-fusion based temporal multi-class inference tasks . This has implications in energy consumptions of wearable sensors . but could even generalise to measurement timings in clinical care to make the work of nurses more efficient , and reduce the stress caused by some medical procedures .. The authors represent a way to train consistent policy that predicts the best combination of sensors to estimate the state of the subjects . They have found that a smaller set of features . is more explainable than the full set of features . However , I think that this somewhat of an overpromise . The trained model gives the optimal density of the measurements and can discern also if old values and features measured are till OK for the inference . This does not mean that those measurements are not needed at all in the features . One can only argue that the required features can be estimated from the older measurement . So , the current set of active sensors is not the full set of required measurement values and can not be exclusively used to explain the logic of the system . Even more , the logic of the policy deciding the new measurement is not discussed in an explainability context . The authors provide no data on this . It may be just an estimate the derivative of the signal and ignore a new measurement , if it 's time derivative is small enough . As a summary , I support publication of the manuscript , provided the authors modify the message on the interpretable features .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for the constructive comments and criticisms . We agree that in the current formulation , \u201c one can only argue that the required features can be estimated from the older measurement . So , the current set of active sensors is not the full set of required measurement values and can not be exclusively used to explain the logic of the system. \u201d Indeed , some features can potentially be reliably estimated from older measurements , and our method can decide to turn off these sensors depending on the underlying states or contexts and save energy . Using the previously observed features , our model is able to select the next feature set to query without observing the entire feature set . We tried to explain this by showing that the selected features indeed reflect the actual activities . As the reviewer pointed out , our dynamic/adaptive feature selection is learned using all the training data . Showing the activity class specific features can be considered as a retrospective interpretation of what our method can do . For testing , dynamic feature selection is determined by the derived latent representations based on all the past observations . We will revise to better explain this point ."}], "0": {"review_id": "mPmCP2CXc7p-0", "review_text": "This paper proposes an RNN model for adaptive dynamic feature selection , for efficient and interpretable human activity recognition ( HAR ) . From the intuition that human activity can be predictable by using a small number of sensors , the paper introduces an l0-norm minimization problem with parameter regularization , and provide a logic on formulating a dynamic feature selection model with relaxations . The difficulty of the discrete optimization problem is solved by differentiable relaxation , which is known as Gumbel-Softmax reparameterization techniques . The formulation is naturally led to an RNN model that uses histories as input with an additional sigmoid unit for adaptive feature selection . Empirical studies are performed to show the superiority of the adaptive feature selection network . Results are shown on the task of 1 ) UCI-HAR smartphone dataset with 561 features , 2 ) UCI Opportunity sensor dataset with 242 features , 3 ) ExtraSensory dataset with 225 features for multilabel binary classification . In particular , by using the adaptive feature selection technique , the average number of features necessary for HAR prediction can be very small ( 0.3 % , 15.9 % , 11.3 % among all features ) at any given time . Overall , the paper is well written . In particular , analysis results on three datasets are clear and detailed , so that the reader would be available to understand what sensors were necessary for HAR prediction . The key concern about the paper is that the algorithm lacks practicality . To show the adaptive selection algorithm is efficient , it should be shown that the algorithm drastically reduces features that are not necessary for prediction over time , while maintaining the performance even in the lighter feature space . Although the average number of features selected by the adaptive selection algorithm for each snapshot is small , all features are entered as input , which may not help to speed up the algorithm . To claim that the algorithm is efficient , it is required to show that the computation cost can be saved . Also , based on the current experimental results , it is difficult to say that features that were not used in earlier timestamp will not be used in later timestamp with a different context . Minor comments and questions : - Can you report the running time of each model ? - Is this model working in an online setting without tuning ? If yes , would you like to clarify ? If no , may I think this technique is for maintaining a dashboard that informs important features every time to users by calculating feature importance over time ? - The performance of the adaptive method on the NTU-RGB-D dataset is quite poor . What part of the dataset do you think caused the difficulty in feature selection ? Do all features important ? - The technical novelty seems to be low if the proposed model is an RNN with an additional sigmoid layer . - Figure 2a does not have a ground truth blue line .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive comments and criticisms . Crucially , we clarify that our algorithm DOES NOT need all features to be entered as input . Using the previously observed feature sets , our model is able to select the next feature set to query without observing the entire feature set . As stated in our methodology section ( equations ( 2 ) , ( 3 ) , ( 4 ) , and ( 6 ) ) , the features that are not selected are set to zero and thus need not be observed . In practice , the next feature set to use can be determined without ever observing all of the features . We have shown through extensive experiments that our method drastically reduces the number of features that are not necessary for prediction for certain contexts . We also would like to point out the possible misunderstanding . Mostly importantly , our aim is not to have a compressed/simplified neural network model for activity recognition . Instead , we would like our dynamic feature selection framework to not only have good activity recognition performance , but also ( more importantly ) to inform which sensors should be turned on or off to achieve desired performance-cost tradeoff . Note that here the cost is incurred for sensor feature collection but not the inference for activity recognition based on given features . As we have stated in our manuscript , our claim on efficiency is not regarding computational efficiency , but rather efficiency in maintaining the sensors needed for prediction . Since not all sensors will be used at all times , we can use our gating method to safely TURN OFF sensors to achieve power efficiency . We answer the questions the reviewer listed below : - The running time of our model compared to a standard GRU is quite comparable . On the UCI HAR dataset , our model takes 11.15ms to predict each time point while a standard GRU takes 7.68ms . This is still within acceptable latencies for continuous activity monitoring . - The model learns a feature selection policy offline and uses the learned policy to select features during testing time . The parameters of the model and selection policy are fixed during testing and no online update of the parameters are done in this setting . The dynamic feature selection is achieved because the selection policy is conditioned on the previous observations to determine the next feature set to use . From this perspective , the technique can indeed be thought of as maintaining a dashboard indicating which features are important given the context summarized by the previous observations . - We note that we are still achieving competitive performance on the NTU-RGB-D dataset , with only about a 2.5 % accuracy drop compared to the baseline model which uses the entire sensor set . We attribute this to the fact that the dataset only contains 25 sensors in total . We believe that such a small sensor set is already curated to be the most informative and therefore , any additional sensor selection would slightly degrade the performance of a predictive model . - We note that our model isn \u2019 t simply \u201c an RNN with an additional sigmoid layer \u201d . In fact , adding a simple sigmoid layer would not produce any feature sparsity by itself as shown by our comparisons with an attention-based model . Instead , in our model , the next feature set is sampled using a learned discrete probability distribution conditioned on the previous observations . Such a model isn \u2019 t trivial to optimize , as backpropagation isn \u2019 t trivially done through the discrete probability distribution parameters . Moreover , we also introduce a regularization that is easily evaluated using a stochastic relaxation . The resulting module is then highly applicable to more complex architectures and modeling scenarios . - In Figure 2a , the orange and blue lines both overlap exactly , causing the blue line to be unseen . We will fix this in our revision of our manuscript . We truly appreciate the reviewer to reply to our post and ask more questions to ensure there are no more misunderstandings that would have affected your assessment of our method . At this point , we are very confident that our above clarifications have thoroughly addressed the potential confusions and misunderstandings in the current review ."}, "1": {"review_id": "mPmCP2CXc7p-1", "review_text": "The authors tackle the important problem of feature selection . They propose to use differentiable gates with an RNN architecture to select different subsets of features for each time point . I think the idea and method are interesting , and the method could be useful . However , I have crucial problems with the way the paper is presented . Most importantly , the authors describe the l_0 relaxation of Bernoulli random variables as if it is their own contribution . They describe existing known results under a section titles \u201c Methodology \u201d as if they are the first to present Bernoulli random variables to feature selection or that they are the first to relax them using the Gumbel Softmax trick . They also use the word : \u201c we derive \u201d ( p.3 ) . This is wrong ! And misleading ! The same relaxation appears in [ 1 ] and used for model sparsification , the descriptions are almost identical to what appears in [ 1 ] with almost zero credit to the authors in [ 1 ] ( a citation appears in related work in a different context ) . Bernoulli relaxation was already used for feature selection , in [ 2 ] , and [ 3 ] , these papers were not even mentioned . The reader can think the authors are the first to introduce such relaxation into the problem of feature selection , while this is again , clearly wrong . The authors are well aware of that this relaxation was presented in [ 1 ] , and in the experiment section they describe the baseline which solves ( 4 ) by citing [ 1 ] ( citation [ 18 ] in their paper ) , this is again in contradiction to the way they describe the relaxation as if it is their own contribution . Putting these CRITICAL comments aside , I think the results are misleading . Specifically , comparing the average number of selected features to the ( constant ) number of selected features of the non-adaptive method is misleading . You need to compare the union of selected features by your method to the constant number , otherwise , there is no way to infer if this feature selection method can result in any compression of the model or could lead to training or inference speed up . Given that this is what you measure since you still need all the features to use your model , what are the advantages of the method ? Only interpretability ? The authors do not explain how the method is used in the testing phase , is the randomness removed ? How exactly ? The authors do not explain how training/ testing is performed , this appears in the appendix but should be moved to the main texts . The authors should compare the method to the distribution suggested in [ 1 ] , which seems more suitable for feature selection than the Concrete distribution ( used by the authors ) . Citations are not in the correct ICLR format . Some pros : I like the examples used in the paper as well as the comparison to ARM , ST , ST-ARM . To conclude , I am voting to reject the paper , based on all the reasons mentioned above . [ 1 ] Louizos , Christos , Max Welling , and Diederik P. Kingma . `` Learning Sparse Neural Networks through $ L_0 $ Regularization . '' ICLR , 2018 . [ 2 ] Yamada , Y. , Lindenbaum , O. , Negahban , S. , & Kluger , Y . Feature selection using stochastic gates . ICML , 2020 . [ 3 ] Bal\u0131n , Muhammed Fatih , Abubakar Abid , and James Zou . `` Concrete autoencoders : Differentiable feature selection and reconstruction . '' ICML.2019 .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for the constructive comments and criticisms . We will clarify confusions and improve the presentation of our manuscript . Here , we would like to * * strongly point out the possible misunderstanding by the reviewer * * . Most importantly , our aim is * * NOT * * to have a compressed/simplified neural network model for activity recognition . Instead , we would like our dynamic feature selection framework to not only have good activity recognition performance , but also ( more importantly ) to inform which sensors should be turned on or off to achieve desired performance-cost tradeoff . Here the cost is incurred for sensor feature collection but not the inference for activity recognition based on given features . In mobile sensing tasks , devices are typically equipped with a redundant set of sensors , and turning on/off those sensors accounts for a major portion of on-device energy budgets . As Reviewer 1 has insightfully pointed out : \u201c This has implications in energy consumptions of wearable sensors . but could even generalize to measurement timings in clinical care to make the work of nurses more efficient , and reduce the stress caused by some medical procedures. \u201d We consider dynamic feature selection as sequential context-dependent feature subset selection and cast it into a stochastic optimization formulation , enabling gradient-based solutions . The method we derived in our manuscript is a temporal feature selection method , which is different from the one presented in [ 1 ] , where the goal is neural network architecture sparsification . Unlike in [ 1 ] where the weight masking is done independent of the current input data , our feature selection method is context dependent and adapts to the input data currently being handled . This introduces non-trivial complications when solving the optimization problem , especially when using the ARM optimization strategies . Also different from [ 1 ] , our method selects features across time , while [ 1 ] is concerned with model compression . We note that many other methods in the literature can be derived as a relaxation similar to the one presented in [ 1 ] ( see [ a ] , [ b ] , [ c ] ) . The fact that our method uses a similar relaxation as [ 1 ] should not discredit its novelty . It was never our intention to misrepresent our work and we will modify our write-up to more clearly show its relation to existing work . The other works [ 2 ] and [ 3 ] that the reviewer cited are also unlike the method in our manuscript . Crucially , both deal with static feature selection as opposed to the dynamic feature selection method we are proposing . We beg to differ from the reviewer \u2019 s statement that we should compare the union of the selected features to the constant number of features selected by the non-adaptive method . As we have stated numerous times in our manuscript , the problem we are concerned with is regarding DYNAMIC feature selection for continuous human activity recognition . Here , we are tasked to give a prediction for each time point while achieving sensor power efficiency by dynamically selecting the sensor set used for each time point . Because we are able to select fewer sensors on average , we are able to keep more sensors turned off on average thus achieving greater power efficiency . Comparing the union of the selected sensors would not be representative of the sensor power consumed by our method , as not all sensors in the union would be turned on at all times when using our method . Nevertheless , we have computed the union of features selected . All of them still show our method to be superior to the non adaptive method . We list these numbers below : UCI HAR : 3.56 % . OPPORTUNITY : 19.83 % . ExtraSensory : 26.66 % . We emphasize that our model DOES NOT need all the features observed and to be entered as input . Using the previously observed features , our model is able to select the next feature set to query without observing the entire feature set . As stated in our methodology section ( equations ( 2 ) , ( 3 ) , ( 4 ) , and ( 6 ) ) , the features that are not selected are set to zero and thus need not be observed . In the testing phase , the randomness is not removed , we stated this in section 2.4 . We would place additional details regarding training/testing in the main manuscript if the page limit permits . However , we believe that the training details would detract from our main message of achieving efficiency and interpretability using our model . We are currently revising both the main text and supplement to try our best to make these points clearer . We would truly appreciate the reviewer to reply to our post and ask more questions , if our points are still not clear enough , to ensure there are no misunderstandings regarding our method and motivations to develop dynamic/adaptive feature selection . At this point , we are very confident that our above clarifications have thoroughly addressed the potential confusions and misunderstandings in the current review ."}, "2": {"review_id": "mPmCP2CXc7p-2", "review_text": "This paper presents a learning-based binary sampling mechanism for feature selection . It filters salient feature dimensions by sampling from a Gumbel-softmax distribution , which is differentiable and can be trained with other network parameters . The proposed method is evaluated on several Human Activity Recognition ( HAR ) datasets . The positive and negative points of this paper can be summarized as following : pros : + This paper is well written and is easy to follow . + The experimental evaluations give positive results . cons : - Important previous works are missing . Learning to generate categorical samples for RNNs is not a fresh idea . Actually , [ a ] has already employs Gumbel-softmax to sample scales in order to dynamically control the temporal pattern learning ; More generally , the topic of this paper is connected to a amount of previous works aiming to adaptively decide how/when to memorize/update the inputs/states , such as [ b ] and [ c ] . These works should also been cited by this paper . - With these missing works taking into account , the novelty of this paper becomes incremental and contribution is trivial . Integrating Gumbel-softmax sampling with RNN cells is very straightforward , and the motivation of applying Gumbel-softmax is very similar to [ a ] . While [ a ] is proposed for general sequence tasks , the proposed method seems to work only for HAR with multi-dimensional inputs . - Since \\tau is the only hyper parameter of Gumbel-softmax , evaluations on how the value of \\tau could impact the performance can be important . Yet no such results are reported in the paper . From the original Gumbel-softmax paper we can see a sample can approximate to a one-hot vector when \\tau is small and be closed to a uniform distribution when \\tau goes large . So it is very likely that the performance will become unstable as \\tau changes . Showing such experimental results could be definitely improve the paper quality . I would suggest to report the means and stds of accuracies with different sampling seeds . Summary : Considering the concerns listed above , I believe there are problems that outweighs the strengths of this paper . They should be fixed before acceptance . [ a ] H Hu , et al.Learning to Adaptively Scale Recurrent Neural Networks . AAAI 2019 [ b ] V Campos , et al.Skip RNN : Learning to Skip State Updates in Recurrent Neural Networks . ICLR 2018 [ c ] D Neil , et al.Phased LSTM : Accelerating Recurrent Network Training for Long or Event-based Sequences . NIPS 2016", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive comments and criticisms . We emphasize that our method is a dynamic sensor selection method that infers which * * small subset of sensor features * * to use * * adaptively * * , at any given context or state indicated by the latent representation , * * at any given time point * * . In this way , its aim is to optimize the tradeoff between prediction accuracy and power and/or other incurring cost to maintain the sensor features , It is unlike any of the methods the reviewer has cited . [ a ] selects wavelet scales used for prediction , instead of which features to use at any given context . This is motivated by a need to capture multi-scale patterns to achieve a more expressive model , instead of for energy efficiency or interpretability . [ b ] selects which time steps to skip for updating the recurrent state , and applies this to sequence modeling tasks . This may not be appropriate for continuous monitoring tasks , where we are tasked to give a prediction at every time step . Moreover , it doesn \u2019 t enable interpretability of the model \u2019 s decisions . Indeed , with our model , we can maintain a small adaptive set of sensors for easily discriminable contexts , and we can observe these sensors for interpretation of the model \u2019 s behavior . [ c ] introduces a gating mechanism across time which controls the frequency with which the memory cell is updated . Such a selection is not context based , as the parameters of the gating mechanism , such as the period and the phase shift , are independent of the current state of information given in order to optimize both prediction accuracy and sensor usage . On the other hand , our dynamic selection mechanism is context dependent , it uses the previous observations to infer the next feature set to use in order to optimize both the prediction accuracy and sensor usage . We further note that all the references applied selection or skipping along the temporal direction , which is only loosely related to our work , to say the least . Our proposed dynamic feature selection focuses on which sensor features across time points should be selected to achieve the desired performance-cost tradeoff and the feature selection is adaptive with respect to the underlying states or contexts of the corresponding activities and environment . Our dynamic/adaptive feature selection is orthogonal to temporal selection/skipping and we do believe these are two different research questions . But we will be happy to cite those papers as our broader context , and we can explore the potential of integrating these two directions as our future research . Gumbel-softmax is one strategy to solve our proposed optimization formulation ( the true originality and merit ) : we neither claimed it as our originality , nor consider it the only way to go ; in face , we also state that the Gumbel-softmax along with many other categorical selection mechanisms have been used for many purposes such as attention , model compression , mixture of experts , Neural Turing Machines , etc . Usage of such a selection mechanism alone should not be grounds to say that a method is not novel . We would consider these are potential solution strategies to achieve our proposed dynamic/adaptive feature selection . Indeed , we benchmarked multiple methods to optimize our dynamic feature selection formulation and found that the Gumbel-softmax performed most favorably for our applications . We have tested different hyperparameters \\tau ranging from 0.001 to 5 and did not notice any significant differences in performance . We will shortly update our manuscript to include our comparisons on \\tau in the appendix . We hope the above can clarify several confusions and make our novelty & merit much more clear to the reviewer ."}, "3": {"review_id": "mPmCP2CXc7p-3", "review_text": "The authors provide a novel combination of known architectures to an important use case of reducing the density of required measurements in sensor-fusion based temporal multi-class inference tasks . This has implications in energy consumptions of wearable sensors . but could even generalise to measurement timings in clinical care to make the work of nurses more efficient , and reduce the stress caused by some medical procedures .. The authors represent a way to train consistent policy that predicts the best combination of sensors to estimate the state of the subjects . They have found that a smaller set of features . is more explainable than the full set of features . However , I think that this somewhat of an overpromise . The trained model gives the optimal density of the measurements and can discern also if old values and features measured are till OK for the inference . This does not mean that those measurements are not needed at all in the features . One can only argue that the required features can be estimated from the older measurement . So , the current set of active sensors is not the full set of required measurement values and can not be exclusively used to explain the logic of the system . Even more , the logic of the policy deciding the new measurement is not discussed in an explainability context . The authors provide no data on this . It may be just an estimate the derivative of the signal and ignore a new measurement , if it 's time derivative is small enough . As a summary , I support publication of the manuscript , provided the authors modify the message on the interpretable features .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for the constructive comments and criticisms . We agree that in the current formulation , \u201c one can only argue that the required features can be estimated from the older measurement . So , the current set of active sensors is not the full set of required measurement values and can not be exclusively used to explain the logic of the system. \u201d Indeed , some features can potentially be reliably estimated from older measurements , and our method can decide to turn off these sensors depending on the underlying states or contexts and save energy . Using the previously observed features , our model is able to select the next feature set to query without observing the entire feature set . We tried to explain this by showing that the selected features indeed reflect the actual activities . As the reviewer pointed out , our dynamic/adaptive feature selection is learned using all the training data . Showing the activity class specific features can be considered as a retrospective interpretation of what our method can do . For testing , dynamic feature selection is determined by the derived latent representations based on all the past observations . We will revise to better explain this point ."}}