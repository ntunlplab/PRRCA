{"year": "2019", "forum": "ryGs6iA5Km", "title": "How Powerful are Graph Neural Networks?", "decision": "Accept (Oral)", "meta_review": "Graph neural networks are an increasingly popular topic of research in machine learning, and this paper does a good job of studying the representational power of some newly proposed variants. The framing of the problem in terms of the WL test, and the proposal of the GIN architecture is a valuable contribution. Through the reviews and subsequent discussion, it looks like the issues surrounding Theorem 3 have been resolved, and therefore all of the reviewers now agree that this paper should be accepted. There may be some interesting followup work based on studying depth, as pointed out by reviewer 1, but this may not be an issue in GIN and is regardless a topic for future research.", "reviews": [{"review_id": "ryGs6iA5Km-0", "review_text": "The author study the expressive power of neighborhood aggregation mechanisms used in Graph Neural Networks and relates them to the 1-dimensional Weisfeiler-Lehman heuristic (1-WL) for graph isomorphism testing. The authors show that GCNs with injections acting on the neighborhood features can distinguish the same graphs that can be distinguished by 1-WL. Moreover, they propose a simple GNN layer, namely GIN, that satisfies this property. Moreover, less powerful GNN layers are studied, such as GCN or GraphSage. Their advantages and disadvantages are discussed and it is shown which graph structures they can distinguish. Finally, the paper shows that the GIN layer beats SOTA GNN layers on well-known benchmark datasets from the graph kernel literature. Studying the expressive power of neighborhood aggregation mechanisms is an important contribution to the further development of GCNs. The paper is well-written and easy to follow. The experimental results are well explained and the evaluation is convincing. However, I have some concerns regarding the main result in Theorem 3. A consequence of the theorem is that it makes no differences (w.r.t. expressive power) whether one distinguishes the features of the node itself from those of its neighbors. This is remarkable and counterintuitive, but not discussed in the article. However, it is discussed in the proof of Theorem 3 (Appendix) which suggests that the number of iterations must be increased for some graphs in order to obtain the same expressive power. Unfortunately, at this point, the proof is a bit vague. I would like to see a discussion of this differences in the article. This should be clarified in a revised version. ---- Edit: The counter example posted in a comment ( https://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X&noteId=rkl2Q1Qi6X ) actually shows that my concerns regarding Theorem 3 and its proof were perfectly justified. I agree that the two graphs provide a counterexample to the main result of the paper. Therefore, I have adjusted my rating. I will increase my rating again when the problem can be resolved. However, this appears to be non-trivial. ---- Moreover, the novelty of the results compared to the related work, e.g., mentioned in the comments, should be pointed out. Some further questions and remarks: (Q1) Did you use a validation set for evaluation? If not, what kind of stopping criteria did was use? (Q2) You use the universal approximation theorem to prove Theorem 3. Could you please say something about the needed width of the networks? (R1) Could you please provide standard deviations for all experiments. I suspect that the accuracies on the these small datasets fluctuates quite a bit. (R2) In the comments it was already mentioned, that some important related work, e.g., [1], [2], are not mentioned. You should address how your work is different from theirs. Minor remarks: - The colors in Figure 1 are difficult to distinguish [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4703190 [2] https://people.csail.mit.edu/taolei/papers/icml17.pdf ------------------- Update: Most of the weak points were appropriately addressed by the authors and I have increased my rating accordingly.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed reviews . In the general post , we have addressed your chief concern regarding our original Eqn ( 4.1 ) and part of Theorem 3a ) . We sincerely hope R2 can revisit the rating in light of our revision and response . Answers to R2 \u2019 s other questions : 1 . RE : Standard deviations We added the standard deviations in Table 1 . Note that on many datasets , standard deviations are fairly high for the previous methods as well as our methods due to the small training datasets . Our GINs achieved statistically significant improvement on the two REDDIT datasets where the number of graphs are fairly large . We leave the empirical evaluation on larger datasets to future work , but we believe that more expressive GNN models like our GINs can benefit more from larger training data by better capturing important discriminative structural features . 2.RE : Discussion on related work Following the suggestion , in Section 6 of the revised paper , we discuss the difference of our work to e.g. , [ 1 ] [ 2 ] . In short , the important difference is that [ 1 ] [ 2 ] both focus on the specific GNN architectures , while we provide a general framework for analyzing and characterizing the expressive power of a broad class of GNNs in the literature . 3.RE : Experimental setup and stopping criteria We selected an epoch with the highest cross-validation accuracy ( averaged over 10 folds ) following what previous deep learning papers do , e.g. , [ 3 ] [ 4 ] . This is for fair comparison as most previous papers on graph classification only report cross-validation accuracy . 4.RE : Network width Our proofs focus on existential analysis , i.e. , there exists a way we can represent multisets with unique representations . Thus , the network width necessary for the functions provided in our proofs may only serve as an upper bound . For practical purposes , in our experiments , we found 32 or 64 hidden units are usually sufficient to perfectly fit the training set . [ 3 ] Mathias Niepert , Mohamed Ahmed , and Konstantin Kutzkov . Learning convolutional neural networks for graphs . In International Conference on Machine Learning ( ICML ) , pp . 2014\u20132023 , 2016 . [ 4 ] Sergey Ivanov and Evgeny Burnaev . Anonymous walk embeddings . In International Conference on Machine Learning ( ICML ) , pp . 2191\u20132200 , 2018 ."}, {"review_id": "ryGs6iA5Km-1", "review_text": "This papers presents an interesting take on Weisfeiler-Lehman-type GNNs, where it shows that a WL-GNNs classification power is related to its ability to represent multisets. The authors show a few exemplar networks where the mean and the max aggregators are unable to distinguish different multisets, thus losing classification power. The paper also proposes averaging the node representation with its neighbors (foregoing the \u201cconcatenate\u201d function) and using sum pooling rather than mean pooling as aggregator. All these observations are wrapped up in a GNN, called GIN. The experiments on Table 1 are inconclusive, unfortunately, as the average accuracies of the different methods are often close and there are no confidence intervals and statistical tests to help guide the reader to understand the significance of the results. My chief concern is equating the Weisfeiler-Lehman test (WL-test) with Weisfeiler-Lehman-type GNNs (WL-GNNs). The WL-test relies on countable set inputs and injective hash functions. Here, the paper is oversimplifying the WL-GNN problem. After the first layer, a WL-GNN is operating on uncountable sets. On uncountable sets, saying that a function is injective does not tells us much about it; we need a measure of how closely packed we find the points in the function\u2019s image (a measure in measure theory, a density in probability). On countable sets, saying a function is injective tells us much about the function. Moreover, the WL-test hash function does not even need to operate over sets with total or even partial orders. As a neural network, the WL-GNN \u201chash\u201d ($f$ in the paper) must operate over a totally ordered set (\\mathbb{R}^n, n > 0). Porting the WL-test argument of \u201cconvergence to unique isomorphic fingerprints\u201d to a WL-GNN requires a measure-theoretic analysis of the output of the WL-GNN layers, and careful analysis if the total order of the set does not create attractors when they are applied recursively. To illustrate the above *attractor* point, let\u2019s consider the construct of Theorem 1 of (Xu et al., 2018), where the WL-GNN \u201chash\u201d ($f$) is (roughly) described as the transition probability matrix of a random walk on the input graph. Under well-known conditions, the successive application of this operator (\"hash\" or transition probability matrix P in this case) can go towards an attractor (the steady state). Here, we need a measure-theoretic analysis of the \u201chash\u201d even if it is bijective: random walk mixing. The random walk transition operator can be invertible (bijective), but we still say the random walker will mix, i.e., the walker forgets where it started, even if the transition operation can be perfectly undone by inversion (P^{-1}). In a WL-GNN that only uses the last layer for classification, this would manifest itself as poor performance in a WL-GNN with a large number of layers, and vanishing gradients. Of course, since (Xu et al., 2018) argued to revert back to the framework of (Duvenaud et al., 2015) of using the embeddings of all layers, one can argue that this mixing problem is just a problem of \u201cwasted computation\u201d. The matrix analysis of the last paragraph also points to another potential problem with the sum aggregator. GIN needs to be shallow. With ReLU activations the reason is simple: for an adjacency matrix $A$, the value of $A^j$ grows very quickly with $j$ (diverges). With sigmoid activations, GIN would experience vanishing gradients in graphs with high variance in node degrees. The paper should be careful with oversimplifications. Simplifications are useful for insight but can be dangerous if not prefaced by clear warnings and a good understanding of their limitations. I am not asking for a measure-theoretic analysis revision of the paper (it could be left to a follow-up paper). I am asking for a *relatively long* discussion of the limitations of the analysis. Suggestions to strengthen the paper: \u2022 Please address the above concerns. \u2022 Table 1 should have confidence intervals (a statistical analysis of significance would be a welcome bonus). \u2022 Please mention the classes of graphs where the WL-test cannot distinguish two non-isomorphic graphs. See (Douglas, 2011), (Cai et al., 1992) and (Evdokimov and Ponomarenko, 1999) for the examples. It is important for the WL-GNN literature to keep track of the more fundamental limitations of the method. \u2022 (Hamilton et al, 2017) also uses the LSTM aggregator, besides max aggregator and mean aggregator, which outperforms both max and mean in some tasks. Does the LSTM aggregator also outperforms the sum aggregator in the tasks of Table 1? It is important for the community to know if unusual aggregators (such as the asymmetric LSTM) have some yet-to-be-discovered class-distinguishing power. --------- Update ------- The counter-example in https://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X is indeed a problem for Theorem 3 if \\{h_v^{(k-1)}, h_u^{(k-1)} : u \\in \\mathcal{N}_v\\} is not a typo for a set of tuples \\{(h_v^{(k-1)}, h_u^{(k-1)}) : u \\in \\mathcal{N}_v\\}. Unfortunately, in their proof, the submission states \"difficulty in proving this form of aggregation mainly lies in the fact that it does not immediately distinguish the root or central node from its neighbors\", which means \\{h_v^{(k-1)}, h_u^{(k-1)} : u \\in \\mathcal{N}_v\\} is actually \\{h_v^{(k-1)}\\} \\cup \\{ h_u^{(k-1)} : u \\in \\mathcal{N}_v\\}, which is not as powerful as WL. Concatenating is more powerful than the summing the node's own embedding, but it results in a simpler model and could be easier to learn in practice. And I am still concerned about the countable x uncountable domain/image issue I raised in my review. Still, the reviewers seem to be doing all the discussion among themselves, with no input from the authors. I am now following Reviewer 2. ---- Reverting my score to my original score. The authors have addressed most of my concerns, thank you. The restricted theorems and propositions better describe the contribution. I would like to note that while the proof of (Xu et al., 2018) is limited that does not mean it is not applicable to GIN or GraphSAGE or similar models. The paper uses 5 GNN layers, which in my experience is the maximum I could ever use with GNNs without seeing a degradation in performance. I don't think this should be a topic for this paper, though. Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K., & Jegelka, S. (2018). Representation Learning on Graphs with Jumping Knowledge Networks. In ICML. Cai, J. Y., F\u00fcrer, M., & Immerman, N. (1992). An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4), 389-410. Douglas, B. L. (2011). The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211. Evdokimov, S., & Ponomarenko, I. (1999). Isomorphism of coloured graphs with slowly increasing multiplicity of Jordan blocks. Combinatorica, 19(3), 321-333. ", "rating": "7: Good paper, accept", "reply_text": "We are now working hard for the thorough response and revision to fully address the concern of Reviewer2 and the anonymous reader . Thanks for your patience ."}, {"review_id": "ryGs6iA5Km-2", "review_text": "This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different. The analysis is based on the study of injective representation functions on multisets. This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs. Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis. Experiments were done on node classification benchmarks to support the claims. Overall I quite liked this paper. The study of the expressive capabilities of GNNs is a very important problem. Given the popularity of this class of models recently, theoretical analysis for these models is largely missing. Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing). This paper presents a very different angle focusing on discriminative capabilities. Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights. I do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests. The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all. In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps. If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist. In equation 4.1 describes the GIN update, which is proposed as \u201cthe most powerful GNN\u201d. However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks. Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on. Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary. This isn\u2019t made very clear in the paper. The training set results presented in section 6.1 is not very clear. The plots show only one run for each model variant, which run was it? As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization. Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node. I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up. As mentioned earlier I quite liked the paper despite some restrictions anc things to clarify. I would vote for accepting this paper for publication at ICLR. -------- Considering the counter-example given above, I'm lowering my scores a bit. The proof of theorem 3 is less than clear. The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy. In the worst case, the second half of theorem 3 (a) will be invalid. The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds. The experiments will need to be rerun. -------- Update: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the positive review and constructive feedback ! We are glad that the reviewer likes our paper . First , we completely agree that the ability of GNNs to capture structural similarity of graphs is very important besides their discriminative power , and we believe this is one of the most important benefits of using GNNs over WL kernel . We have now made this point clearer in Section 4 . Furthermore , we emphasized that we do consider node features to lie in R^d so that they can capture the similarity . The subtlety is that ( as R1 nicely pointed out ) , we need a common assumption that node features at each layer are from countable set in R^d ( not from the entire R^d ) . This is satisfied if the input node features are from a countable set , because for a graph neural network , countability propagates across all layers in a GNN . We leave uncountable node input features for future work and add a more detailed discussion in Section 4 of the revised paper . In the following , we respond to R3 \u2019 s other helpful comments and suggestions : 1 . RE : Architecture is similar to , e.g. , Interaction Networks Thank you for the pointers . Some of our GIN \u2019 s building blocks , e.g.sum and MLP indeed appeared in other architectures . We emphasize that while previous work tend to be somewhat ad-hoc in designing GNN architectures , our main emphasis is on deriving our GIN architecture based on the theoretical motivation . In Section 6 of the revised version , we mention related GNN architectures and discuss the differences . 2.RE : Using MLP for mean or max in the initial step is more fair ? We think there might be a slight misunderstanding here : as we discussed with concrete examples in Section 5.2 , mean or max pooling are inherently incapable of capturing the multiset information regardless of the use of MLP . Especially , in our experiments , we use one-hot encodings as input node features , so the use of MLP on top of them does not increase the discriminative power of mean/max pooling . 3.RE : Training set results optimized for test performance ? The results were not actually optimized for test performance . Instead , we used exactly the same configurations for all the datasets : For all the GNNs , the same configurations were used across datasets : 5 GNN layers ( including the input layer ) , hidden units of size 64 , minibatch of size 128 , and 0.5 dropout ratio . For the WL subtree kernel , we set the number of iterations to 4 , which is comparable to the 5 GNN layers . We clarified this in Figure 6 of the revised paper ."}], "0": {"review_id": "ryGs6iA5Km-0", "review_text": "The author study the expressive power of neighborhood aggregation mechanisms used in Graph Neural Networks and relates them to the 1-dimensional Weisfeiler-Lehman heuristic (1-WL) for graph isomorphism testing. The authors show that GCNs with injections acting on the neighborhood features can distinguish the same graphs that can be distinguished by 1-WL. Moreover, they propose a simple GNN layer, namely GIN, that satisfies this property. Moreover, less powerful GNN layers are studied, such as GCN or GraphSage. Their advantages and disadvantages are discussed and it is shown which graph structures they can distinguish. Finally, the paper shows that the GIN layer beats SOTA GNN layers on well-known benchmark datasets from the graph kernel literature. Studying the expressive power of neighborhood aggregation mechanisms is an important contribution to the further development of GCNs. The paper is well-written and easy to follow. The experimental results are well explained and the evaluation is convincing. However, I have some concerns regarding the main result in Theorem 3. A consequence of the theorem is that it makes no differences (w.r.t. expressive power) whether one distinguishes the features of the node itself from those of its neighbors. This is remarkable and counterintuitive, but not discussed in the article. However, it is discussed in the proof of Theorem 3 (Appendix) which suggests that the number of iterations must be increased for some graphs in order to obtain the same expressive power. Unfortunately, at this point, the proof is a bit vague. I would like to see a discussion of this differences in the article. This should be clarified in a revised version. ---- Edit: The counter example posted in a comment ( https://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X&noteId=rkl2Q1Qi6X ) actually shows that my concerns regarding Theorem 3 and its proof were perfectly justified. I agree that the two graphs provide a counterexample to the main result of the paper. Therefore, I have adjusted my rating. I will increase my rating again when the problem can be resolved. However, this appears to be non-trivial. ---- Moreover, the novelty of the results compared to the related work, e.g., mentioned in the comments, should be pointed out. Some further questions and remarks: (Q1) Did you use a validation set for evaluation? If not, what kind of stopping criteria did was use? (Q2) You use the universal approximation theorem to prove Theorem 3. Could you please say something about the needed width of the networks? (R1) Could you please provide standard deviations for all experiments. I suspect that the accuracies on the these small datasets fluctuates quite a bit. (R2) In the comments it was already mentioned, that some important related work, e.g., [1], [2], are not mentioned. You should address how your work is different from theirs. Minor remarks: - The colors in Figure 1 are difficult to distinguish [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4703190 [2] https://people.csail.mit.edu/taolei/papers/icml17.pdf ------------------- Update: Most of the weak points were appropriately addressed by the authors and I have increased my rating accordingly.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed reviews . In the general post , we have addressed your chief concern regarding our original Eqn ( 4.1 ) and part of Theorem 3a ) . We sincerely hope R2 can revisit the rating in light of our revision and response . Answers to R2 \u2019 s other questions : 1 . RE : Standard deviations We added the standard deviations in Table 1 . Note that on many datasets , standard deviations are fairly high for the previous methods as well as our methods due to the small training datasets . Our GINs achieved statistically significant improvement on the two REDDIT datasets where the number of graphs are fairly large . We leave the empirical evaluation on larger datasets to future work , but we believe that more expressive GNN models like our GINs can benefit more from larger training data by better capturing important discriminative structural features . 2.RE : Discussion on related work Following the suggestion , in Section 6 of the revised paper , we discuss the difference of our work to e.g. , [ 1 ] [ 2 ] . In short , the important difference is that [ 1 ] [ 2 ] both focus on the specific GNN architectures , while we provide a general framework for analyzing and characterizing the expressive power of a broad class of GNNs in the literature . 3.RE : Experimental setup and stopping criteria We selected an epoch with the highest cross-validation accuracy ( averaged over 10 folds ) following what previous deep learning papers do , e.g. , [ 3 ] [ 4 ] . This is for fair comparison as most previous papers on graph classification only report cross-validation accuracy . 4.RE : Network width Our proofs focus on existential analysis , i.e. , there exists a way we can represent multisets with unique representations . Thus , the network width necessary for the functions provided in our proofs may only serve as an upper bound . For practical purposes , in our experiments , we found 32 or 64 hidden units are usually sufficient to perfectly fit the training set . [ 3 ] Mathias Niepert , Mohamed Ahmed , and Konstantin Kutzkov . Learning convolutional neural networks for graphs . In International Conference on Machine Learning ( ICML ) , pp . 2014\u20132023 , 2016 . [ 4 ] Sergey Ivanov and Evgeny Burnaev . Anonymous walk embeddings . In International Conference on Machine Learning ( ICML ) , pp . 2191\u20132200 , 2018 ."}, "1": {"review_id": "ryGs6iA5Km-1", "review_text": "This papers presents an interesting take on Weisfeiler-Lehman-type GNNs, where it shows that a WL-GNNs classification power is related to its ability to represent multisets. The authors show a few exemplar networks where the mean and the max aggregators are unable to distinguish different multisets, thus losing classification power. The paper also proposes averaging the node representation with its neighbors (foregoing the \u201cconcatenate\u201d function) and using sum pooling rather than mean pooling as aggregator. All these observations are wrapped up in a GNN, called GIN. The experiments on Table 1 are inconclusive, unfortunately, as the average accuracies of the different methods are often close and there are no confidence intervals and statistical tests to help guide the reader to understand the significance of the results. My chief concern is equating the Weisfeiler-Lehman test (WL-test) with Weisfeiler-Lehman-type GNNs (WL-GNNs). The WL-test relies on countable set inputs and injective hash functions. Here, the paper is oversimplifying the WL-GNN problem. After the first layer, a WL-GNN is operating on uncountable sets. On uncountable sets, saying that a function is injective does not tells us much about it; we need a measure of how closely packed we find the points in the function\u2019s image (a measure in measure theory, a density in probability). On countable sets, saying a function is injective tells us much about the function. Moreover, the WL-test hash function does not even need to operate over sets with total or even partial orders. As a neural network, the WL-GNN \u201chash\u201d ($f$ in the paper) must operate over a totally ordered set (\\mathbb{R}^n, n > 0). Porting the WL-test argument of \u201cconvergence to unique isomorphic fingerprints\u201d to a WL-GNN requires a measure-theoretic analysis of the output of the WL-GNN layers, and careful analysis if the total order of the set does not create attractors when they are applied recursively. To illustrate the above *attractor* point, let\u2019s consider the construct of Theorem 1 of (Xu et al., 2018), where the WL-GNN \u201chash\u201d ($f$) is (roughly) described as the transition probability matrix of a random walk on the input graph. Under well-known conditions, the successive application of this operator (\"hash\" or transition probability matrix P in this case) can go towards an attractor (the steady state). Here, we need a measure-theoretic analysis of the \u201chash\u201d even if it is bijective: random walk mixing. The random walk transition operator can be invertible (bijective), but we still say the random walker will mix, i.e., the walker forgets where it started, even if the transition operation can be perfectly undone by inversion (P^{-1}). In a WL-GNN that only uses the last layer for classification, this would manifest itself as poor performance in a WL-GNN with a large number of layers, and vanishing gradients. Of course, since (Xu et al., 2018) argued to revert back to the framework of (Duvenaud et al., 2015) of using the embeddings of all layers, one can argue that this mixing problem is just a problem of \u201cwasted computation\u201d. The matrix analysis of the last paragraph also points to another potential problem with the sum aggregator. GIN needs to be shallow. With ReLU activations the reason is simple: for an adjacency matrix $A$, the value of $A^j$ grows very quickly with $j$ (diverges). With sigmoid activations, GIN would experience vanishing gradients in graphs with high variance in node degrees. The paper should be careful with oversimplifications. Simplifications are useful for insight but can be dangerous if not prefaced by clear warnings and a good understanding of their limitations. I am not asking for a measure-theoretic analysis revision of the paper (it could be left to a follow-up paper). I am asking for a *relatively long* discussion of the limitations of the analysis. Suggestions to strengthen the paper: \u2022 Please address the above concerns. \u2022 Table 1 should have confidence intervals (a statistical analysis of significance would be a welcome bonus). \u2022 Please mention the classes of graphs where the WL-test cannot distinguish two non-isomorphic graphs. See (Douglas, 2011), (Cai et al., 1992) and (Evdokimov and Ponomarenko, 1999) for the examples. It is important for the WL-GNN literature to keep track of the more fundamental limitations of the method. \u2022 (Hamilton et al, 2017) also uses the LSTM aggregator, besides max aggregator and mean aggregator, which outperforms both max and mean in some tasks. Does the LSTM aggregator also outperforms the sum aggregator in the tasks of Table 1? It is important for the community to know if unusual aggregators (such as the asymmetric LSTM) have some yet-to-be-discovered class-distinguishing power. --------- Update ------- The counter-example in https://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X is indeed a problem for Theorem 3 if \\{h_v^{(k-1)}, h_u^{(k-1)} : u \\in \\mathcal{N}_v\\} is not a typo for a set of tuples \\{(h_v^{(k-1)}, h_u^{(k-1)}) : u \\in \\mathcal{N}_v\\}. Unfortunately, in their proof, the submission states \"difficulty in proving this form of aggregation mainly lies in the fact that it does not immediately distinguish the root or central node from its neighbors\", which means \\{h_v^{(k-1)}, h_u^{(k-1)} : u \\in \\mathcal{N}_v\\} is actually \\{h_v^{(k-1)}\\} \\cup \\{ h_u^{(k-1)} : u \\in \\mathcal{N}_v\\}, which is not as powerful as WL. Concatenating is more powerful than the summing the node's own embedding, but it results in a simpler model and could be easier to learn in practice. And I am still concerned about the countable x uncountable domain/image issue I raised in my review. Still, the reviewers seem to be doing all the discussion among themselves, with no input from the authors. I am now following Reviewer 2. ---- Reverting my score to my original score. The authors have addressed most of my concerns, thank you. The restricted theorems and propositions better describe the contribution. I would like to note that while the proof of (Xu et al., 2018) is limited that does not mean it is not applicable to GIN or GraphSAGE or similar models. The paper uses 5 GNN layers, which in my experience is the maximum I could ever use with GNNs without seeing a degradation in performance. I don't think this should be a topic for this paper, though. Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K., & Jegelka, S. (2018). Representation Learning on Graphs with Jumping Knowledge Networks. In ICML. Cai, J. Y., F\u00fcrer, M., & Immerman, N. (1992). An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4), 389-410. Douglas, B. L. (2011). The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211. Evdokimov, S., & Ponomarenko, I. (1999). Isomorphism of coloured graphs with slowly increasing multiplicity of Jordan blocks. Combinatorica, 19(3), 321-333. ", "rating": "7: Good paper, accept", "reply_text": "We are now working hard for the thorough response and revision to fully address the concern of Reviewer2 and the anonymous reader . Thanks for your patience ."}, "2": {"review_id": "ryGs6iA5Km-2", "review_text": "This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different. The analysis is based on the study of injective representation functions on multisets. This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs. Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis. Experiments were done on node classification benchmarks to support the claims. Overall I quite liked this paper. The study of the expressive capabilities of GNNs is a very important problem. Given the popularity of this class of models recently, theoretical analysis for these models is largely missing. Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing). This paper presents a very different angle focusing on discriminative capabilities. Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights. I do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests. The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all. In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps. If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist. In equation 4.1 describes the GIN update, which is proposed as \u201cthe most powerful GNN\u201d. However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks. Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on. Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary. This isn\u2019t made very clear in the paper. The training set results presented in section 6.1 is not very clear. The plots show only one run for each model variant, which run was it? As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization. Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node. I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up. As mentioned earlier I quite liked the paper despite some restrictions anc things to clarify. I would vote for accepting this paper for publication at ICLR. -------- Considering the counter-example given above, I'm lowering my scores a bit. The proof of theorem 3 is less than clear. The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy. In the worst case, the second half of theorem 3 (a) will be invalid. The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds. The experiments will need to be rerun. -------- Update: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the positive review and constructive feedback ! We are glad that the reviewer likes our paper . First , we completely agree that the ability of GNNs to capture structural similarity of graphs is very important besides their discriminative power , and we believe this is one of the most important benefits of using GNNs over WL kernel . We have now made this point clearer in Section 4 . Furthermore , we emphasized that we do consider node features to lie in R^d so that they can capture the similarity . The subtlety is that ( as R1 nicely pointed out ) , we need a common assumption that node features at each layer are from countable set in R^d ( not from the entire R^d ) . This is satisfied if the input node features are from a countable set , because for a graph neural network , countability propagates across all layers in a GNN . We leave uncountable node input features for future work and add a more detailed discussion in Section 4 of the revised paper . In the following , we respond to R3 \u2019 s other helpful comments and suggestions : 1 . RE : Architecture is similar to , e.g. , Interaction Networks Thank you for the pointers . Some of our GIN \u2019 s building blocks , e.g.sum and MLP indeed appeared in other architectures . We emphasize that while previous work tend to be somewhat ad-hoc in designing GNN architectures , our main emphasis is on deriving our GIN architecture based on the theoretical motivation . In Section 6 of the revised version , we mention related GNN architectures and discuss the differences . 2.RE : Using MLP for mean or max in the initial step is more fair ? We think there might be a slight misunderstanding here : as we discussed with concrete examples in Section 5.2 , mean or max pooling are inherently incapable of capturing the multiset information regardless of the use of MLP . Especially , in our experiments , we use one-hot encodings as input node features , so the use of MLP on top of them does not increase the discriminative power of mean/max pooling . 3.RE : Training set results optimized for test performance ? The results were not actually optimized for test performance . Instead , we used exactly the same configurations for all the datasets : For all the GNNs , the same configurations were used across datasets : 5 GNN layers ( including the input layer ) , hidden units of size 64 , minibatch of size 128 , and 0.5 dropout ratio . For the WL subtree kernel , we set the number of iterations to 4 , which is comparable to the 5 GNN layers . We clarified this in Figure 6 of the revised paper ."}}