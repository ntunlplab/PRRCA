{"year": "2020", "forum": "r1ecqn4YwB", "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "decision": "Accept (Poster)", "meta_review": "The paper received positive recommendation from all reviewers. Accept.", "reviews": [{"review_id": "r1ecqn4YwB-0", "review_text": "The paper proposes a DL architecture that achieves better performance on time series prediction. The proposed architecture is relatively straightforward and composes residual blocks. While the paper does achieve superior results, a lot of the text is devoted to comparing to prior work and arguing that DL approaches can do better than hand-crafted approaches, instead of focussing on the importance of specific technical contributions made in the paper. My main concerns are: (a) The main technical idea in this paper is the use of back-casting and forecasting (i.e. doubly residual connections). However, no ablations are provided to show how important doubly residual connections are. What is the performance, if the DL architecture is kept exactly the same, except for: (i) No residual connections in backcasting (simply feed in the overall input to every block) (ii) In addition to (i), make no backcasting predictions (b) Given no architectural details of the previous ML methods are provided, its unclear if the current architecture is better because it has more parameters or it is indeed the doubly residual idea that is important. (c) Authors make a point about interpretability \u2014 but interpretability is only achieved using domain specific knowledge. I am not really sure, what is so novel about this. Overall, this seems to be a good applications paper which has been optimized for performance. As a research paper, the contributions are less clear. Answers to my concerns above will help clarify. Given my current concerns, I cannot recommend this paper to be accepted. -------------------------- Post Rebuttal: Authors have addressed several of my concerns by providing detailed ablations and the paper reads much better. I still have some concerns: (a) Interpretability in this work is obtained by using standard basis functions used by the TS community. Such basis functions can be used with any architecture, not just the doubly residual connection networks. Also, its common for people to make use of domain specific knowledge, for e.g. predicting spectrograms when analyzing audio data. In these cases, yes predictions are interpretable. Not sure, what is the contribution that authors are claiming. (b) The connection to meta-learning seems to be a bit mis-leading. The authors claim that all the weights of the network form the outer loop and the stage-wise predictions of \\theta corresponds to the inner loop. Can I simply not think of \\theta as features? In that case, this logic will apply to any deep neural network. Not sure what exactly authors want to say. Under this view all deep networks are doing meta-learning. I might have misunderstood something here -- please correct me, if so. I am changing my rating to weal accept, but I also hope the authors will address the concerns outlined above. ", "rating": "6: Weak Accept", "reply_text": "We would like to sincerely thank Reviewer 2 for the thorough analysis of the manuscript and for insightful feedback . We are committed to fully address the comments raised by the reviewer , including conducting the additional experiments . Our detailed response will follow as soon as the experimental results are obtained . In the meantime , we would like to ask the reviewer to kindly clarify the desired experimental setup . We have the following interpretations of the experimental setup and we would like to confirm which of our interpretations is correct . We apologize for any misunderstanding and we hope that Reviewer 2 may provide necessary clarifications . * * Interpretation 1 * * We interpret the setup in point ( a ) ( i ) as follows : Residual connections are disabled , and all blocks are fed the same overall input . We assume that the overall input refers to the model input . The blocks then forecast in parallel and their individual outputs are summed to make the final forecast . In this interpretation , the setup described in point ( a ) ( ii ) is unclear to us : ( a ) ( i ) already seems to imply that the backcast predictions are not made . * * Interpretation 2 * * We interpret the setup in point ( a ) ( i ) as follows : residual connections are disabled and the backcast connection of the previous block is fed as the input to the next block . The model input is only fed in the first block . In this interpretation , ( a ) ( i ) implies that the backcast connections are the only links between successive blocks . So , we interpret ( a ) ( ii ) as follows : the backcast connections are not made . Instead , the overall model input is fed into each block . The blocks then forecast in parallel and their individual outputs are summed to make the final forecast ."}, {"review_id": "r1ecqn4YwB-1", "review_text": "This goal of this paper is to present a strong empirical result showing that a \"pure\" machine learning based method can outperform all known methods on some of the most challenging time series forecasting benchmarks (TOURISM, M3 and especially M4). Since I am not from the field of forecasting, I can not be sure of this, but from my understanding these benchmark datasets are indeed challenging and the cited references back up the claims of the paper related to these datasets being important in the field. On the most challenging dataset (M4), the best known performing method combines RNNs with a traditional smoothing algorithm. The model proposed in this paper outperforms it without being combined with any classical approach, though it does utilize ensembling. The experimental setup is sound in my opinion, and the result appears to be of high potential significance. However, despite trying to go through Section 3 multiple times, the exact model architecture is not clear to me. Due to this reason, my current decision is a weak rejection since the model is a central contribution of the paper. I will be happy to increase my score if the authors can make the model description crystal clear. Even though my expertise is deep neural network architectures, I find it hard to follow the descriptions in Sec. 3. I faced the most difficulty understanding section 3.1, which obviously made the rest of subsections even harder to follow. Here are my main points of confusion: - One big issue is that the paper uses an illustration (Fig. 1) to explain the architecture instead of equations, but then uses symbols in the main text that do not appear on Fig. 1 at all such as g_theta. Is the \"FC Stack (4 layers)\" g_theta? - Where are (uppercase) phi functions? I could infer that these are the \"FC\" blocks but they should be labeled. - The Figure has the symbols g^b_theta and g^f_theta that do not appear in the text description. What exactly do they do? And is the theta that parameterizes each of them the same theta that parameterizes g_theta? How is this possible if g_theta is the \"FC Stack (4 layers)\"? - The description in second and third paragraph of Section 3.1 is very confusing and unclear. It should be replaced or augmented with equations using clearly defined symbols that match Figure 1. - More confusion stems from the use of the term \"parameters\" in (I believe) a different context than is used in neural networks, where \"parameters\" refers to connection weights. But here parameters are outputs of some functions, so either they are not connection weights or this is a fast-weight style architecture where outputs are weights [1], in which this should be made clear. - Design of the doubly residual architecture in Section 3.2 makes sense to me at a high level, but I feel it is still very hard to clearly understand and implement it. Again, use of equations to clearly define the computation would be very helpful. [1] Schmidhuber, J\u00fcrgen. \"Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\" Neural Computation 4.1 (1992): 131-139. --- Update after rebuttal --- I am happy to see the paper greatly improved by the authors in their updates. My concerns related to the presentation of the model have been addressed, and I find the architecture much easier to understand. I also appreciate the detailed supplementary material that is likely to help readers interested in the area. Related to the areas I work in, I noticed the following missing references: Densenets: Lang, K.J. and Witbrock, M.J., 1988, June. Learning to tell two spirals apart. In Proceedings of the 1988 connectionist models summer school (No. 1989, pp. 52-59). Metalearning: Schmidhuber, J., 1987. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook (Doctoral dissertation, Technische Universit\u00e4t M\u00fcnchen). While improving papers is generally the objective of the rebuttal phase, I suggest that the authors to not take this as an opportunity to submit unpolished papers in the first phase. That said, I have increased my rating to reflect my satisfaction with the current version of the paper. ", "rating": "8: Accept", "reply_text": "We would like to sincerely thank Reviewer 1 for the thorough analysis of the manuscript and for insightful feedback . We will add equations to describe the operation of the architecture in more detail , as suggested by the Reviewer . In the meanwhile , we would like to ask Reviewer 's advice on the placement of the equations . Would it be better to have a separate section with mathematical equations in the appendix , or embedding the equations directly in the text of Section 3 is a better choice ?"}, {"review_id": "r1ecqn4YwB-2", "review_text": "The paper investigates a pure deep learning architecture for Univariate time series analysis by simply ensembling feed-forward networks, along with the residual stacking mechanism for fluid learning. Each of the generic block consists of 4 FC layers followed by the use of forward and backward predictor to have forecast and backcast output of the original input. These blocks forms the stacks, where each stack provides the residuals and the forecast responses further to the next stacks, which ultimately provide the global forecast. To make the internal stack outputs interpretable, assumptions are imposed on the trend model, which follows a polynomial function of time vector, and seasonality model which follows periodic Fourier series. Further, the ensembling of models based on different metrics and input windows is used for better accuracy. Very well written paper and easy to follow. It advocates a pure DL framework (instead of hybrid statistical models and DL). I found the idea simple and effective, yielding results better than the previous approaches. Also, the experimental setup is well described. I also found the link between this work and meta-learning approaches interesting. However, I have several questions about the interpretability results. It looks like the inductive bias based on some general assumptions can fail in some cases. For example, in Fig. 2(a), the line for FORECAST-I and FORECAST-G deviates much in case of hourly/weekly/daily data frequency. What is the reason behind this? Also, in Fig. 2, while the StackT-I (fig.2(d)) and StackS-I (fig.2(e)) provide response lines different from the counterparts in Stack1-G (fig.2(b)) and Stack2-G (fig.2(c)), the summations in the combined line (fig.2(a)) yield similar curves of pretty much the same shapes, without much perceived difference. Is it expected or something is wrong? ", "rating": "6: Weak Accept", "reply_text": "We would like to sincerely thank Reviewer 3 for the thorough analysis of the manuscript and for insightful feedback . We will extend the discussion of Figure 2 to thoroughly address the concerns raised by the reviewer . In the meantime , we would like to clarify if we should interpret `` the line for FORECAST-I and FORECAST-G deviates much '' that the forecast provided by FORECAST-I is different from the forecast provided by FORECAST-G ?"}], "0": {"review_id": "r1ecqn4YwB-0", "review_text": "The paper proposes a DL architecture that achieves better performance on time series prediction. The proposed architecture is relatively straightforward and composes residual blocks. While the paper does achieve superior results, a lot of the text is devoted to comparing to prior work and arguing that DL approaches can do better than hand-crafted approaches, instead of focussing on the importance of specific technical contributions made in the paper. My main concerns are: (a) The main technical idea in this paper is the use of back-casting and forecasting (i.e. doubly residual connections). However, no ablations are provided to show how important doubly residual connections are. What is the performance, if the DL architecture is kept exactly the same, except for: (i) No residual connections in backcasting (simply feed in the overall input to every block) (ii) In addition to (i), make no backcasting predictions (b) Given no architectural details of the previous ML methods are provided, its unclear if the current architecture is better because it has more parameters or it is indeed the doubly residual idea that is important. (c) Authors make a point about interpretability \u2014 but interpretability is only achieved using domain specific knowledge. I am not really sure, what is so novel about this. Overall, this seems to be a good applications paper which has been optimized for performance. As a research paper, the contributions are less clear. Answers to my concerns above will help clarify. Given my current concerns, I cannot recommend this paper to be accepted. -------------------------- Post Rebuttal: Authors have addressed several of my concerns by providing detailed ablations and the paper reads much better. I still have some concerns: (a) Interpretability in this work is obtained by using standard basis functions used by the TS community. Such basis functions can be used with any architecture, not just the doubly residual connection networks. Also, its common for people to make use of domain specific knowledge, for e.g. predicting spectrograms when analyzing audio data. In these cases, yes predictions are interpretable. Not sure, what is the contribution that authors are claiming. (b) The connection to meta-learning seems to be a bit mis-leading. The authors claim that all the weights of the network form the outer loop and the stage-wise predictions of \\theta corresponds to the inner loop. Can I simply not think of \\theta as features? In that case, this logic will apply to any deep neural network. Not sure what exactly authors want to say. Under this view all deep networks are doing meta-learning. I might have misunderstood something here -- please correct me, if so. I am changing my rating to weal accept, but I also hope the authors will address the concerns outlined above. ", "rating": "6: Weak Accept", "reply_text": "We would like to sincerely thank Reviewer 2 for the thorough analysis of the manuscript and for insightful feedback . We are committed to fully address the comments raised by the reviewer , including conducting the additional experiments . Our detailed response will follow as soon as the experimental results are obtained . In the meantime , we would like to ask the reviewer to kindly clarify the desired experimental setup . We have the following interpretations of the experimental setup and we would like to confirm which of our interpretations is correct . We apologize for any misunderstanding and we hope that Reviewer 2 may provide necessary clarifications . * * Interpretation 1 * * We interpret the setup in point ( a ) ( i ) as follows : Residual connections are disabled , and all blocks are fed the same overall input . We assume that the overall input refers to the model input . The blocks then forecast in parallel and their individual outputs are summed to make the final forecast . In this interpretation , the setup described in point ( a ) ( ii ) is unclear to us : ( a ) ( i ) already seems to imply that the backcast predictions are not made . * * Interpretation 2 * * We interpret the setup in point ( a ) ( i ) as follows : residual connections are disabled and the backcast connection of the previous block is fed as the input to the next block . The model input is only fed in the first block . In this interpretation , ( a ) ( i ) implies that the backcast connections are the only links between successive blocks . So , we interpret ( a ) ( ii ) as follows : the backcast connections are not made . Instead , the overall model input is fed into each block . The blocks then forecast in parallel and their individual outputs are summed to make the final forecast ."}, "1": {"review_id": "r1ecqn4YwB-1", "review_text": "This goal of this paper is to present a strong empirical result showing that a \"pure\" machine learning based method can outperform all known methods on some of the most challenging time series forecasting benchmarks (TOURISM, M3 and especially M4). Since I am not from the field of forecasting, I can not be sure of this, but from my understanding these benchmark datasets are indeed challenging and the cited references back up the claims of the paper related to these datasets being important in the field. On the most challenging dataset (M4), the best known performing method combines RNNs with a traditional smoothing algorithm. The model proposed in this paper outperforms it without being combined with any classical approach, though it does utilize ensembling. The experimental setup is sound in my opinion, and the result appears to be of high potential significance. However, despite trying to go through Section 3 multiple times, the exact model architecture is not clear to me. Due to this reason, my current decision is a weak rejection since the model is a central contribution of the paper. I will be happy to increase my score if the authors can make the model description crystal clear. Even though my expertise is deep neural network architectures, I find it hard to follow the descriptions in Sec. 3. I faced the most difficulty understanding section 3.1, which obviously made the rest of subsections even harder to follow. Here are my main points of confusion: - One big issue is that the paper uses an illustration (Fig. 1) to explain the architecture instead of equations, but then uses symbols in the main text that do not appear on Fig. 1 at all such as g_theta. Is the \"FC Stack (4 layers)\" g_theta? - Where are (uppercase) phi functions? I could infer that these are the \"FC\" blocks but they should be labeled. - The Figure has the symbols g^b_theta and g^f_theta that do not appear in the text description. What exactly do they do? And is the theta that parameterizes each of them the same theta that parameterizes g_theta? How is this possible if g_theta is the \"FC Stack (4 layers)\"? - The description in second and third paragraph of Section 3.1 is very confusing and unclear. It should be replaced or augmented with equations using clearly defined symbols that match Figure 1. - More confusion stems from the use of the term \"parameters\" in (I believe) a different context than is used in neural networks, where \"parameters\" refers to connection weights. But here parameters are outputs of some functions, so either they are not connection weights or this is a fast-weight style architecture where outputs are weights [1], in which this should be made clear. - Design of the doubly residual architecture in Section 3.2 makes sense to me at a high level, but I feel it is still very hard to clearly understand and implement it. Again, use of equations to clearly define the computation would be very helpful. [1] Schmidhuber, J\u00fcrgen. \"Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\" Neural Computation 4.1 (1992): 131-139. --- Update after rebuttal --- I am happy to see the paper greatly improved by the authors in their updates. My concerns related to the presentation of the model have been addressed, and I find the architecture much easier to understand. I also appreciate the detailed supplementary material that is likely to help readers interested in the area. Related to the areas I work in, I noticed the following missing references: Densenets: Lang, K.J. and Witbrock, M.J., 1988, June. Learning to tell two spirals apart. In Proceedings of the 1988 connectionist models summer school (No. 1989, pp. 52-59). Metalearning: Schmidhuber, J., 1987. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook (Doctoral dissertation, Technische Universit\u00e4t M\u00fcnchen). While improving papers is generally the objective of the rebuttal phase, I suggest that the authors to not take this as an opportunity to submit unpolished papers in the first phase. That said, I have increased my rating to reflect my satisfaction with the current version of the paper. ", "rating": "8: Accept", "reply_text": "We would like to sincerely thank Reviewer 1 for the thorough analysis of the manuscript and for insightful feedback . We will add equations to describe the operation of the architecture in more detail , as suggested by the Reviewer . In the meanwhile , we would like to ask Reviewer 's advice on the placement of the equations . Would it be better to have a separate section with mathematical equations in the appendix , or embedding the equations directly in the text of Section 3 is a better choice ?"}, "2": {"review_id": "r1ecqn4YwB-2", "review_text": "The paper investigates a pure deep learning architecture for Univariate time series analysis by simply ensembling feed-forward networks, along with the residual stacking mechanism for fluid learning. Each of the generic block consists of 4 FC layers followed by the use of forward and backward predictor to have forecast and backcast output of the original input. These blocks forms the stacks, where each stack provides the residuals and the forecast responses further to the next stacks, which ultimately provide the global forecast. To make the internal stack outputs interpretable, assumptions are imposed on the trend model, which follows a polynomial function of time vector, and seasonality model which follows periodic Fourier series. Further, the ensembling of models based on different metrics and input windows is used for better accuracy. Very well written paper and easy to follow. It advocates a pure DL framework (instead of hybrid statistical models and DL). I found the idea simple and effective, yielding results better than the previous approaches. Also, the experimental setup is well described. I also found the link between this work and meta-learning approaches interesting. However, I have several questions about the interpretability results. It looks like the inductive bias based on some general assumptions can fail in some cases. For example, in Fig. 2(a), the line for FORECAST-I and FORECAST-G deviates much in case of hourly/weekly/daily data frequency. What is the reason behind this? Also, in Fig. 2, while the StackT-I (fig.2(d)) and StackS-I (fig.2(e)) provide response lines different from the counterparts in Stack1-G (fig.2(b)) and Stack2-G (fig.2(c)), the summations in the combined line (fig.2(a)) yield similar curves of pretty much the same shapes, without much perceived difference. Is it expected or something is wrong? ", "rating": "6: Weak Accept", "reply_text": "We would like to sincerely thank Reviewer 3 for the thorough analysis of the manuscript and for insightful feedback . We will extend the discussion of Figure 2 to thoroughly address the concerns raised by the reviewer . In the meantime , we would like to clarify if we should interpret `` the line for FORECAST-I and FORECAST-G deviates much '' that the forecast provided by FORECAST-I is different from the forecast provided by FORECAST-G ?"}}