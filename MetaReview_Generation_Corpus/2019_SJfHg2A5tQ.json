{"year": "2019", "forum": "SJfHg2A5tQ", "title": "BNN+: Improved Binary Network Training", "decision": "Reject", "meta_review": "The paper makes two fairly incremental contributions regarding training binarized neural networks: (1) the swish-based STE, and (2) a regularization that pushes weights to take on values in {-1, +1}. Reviewer1 and reviewer2 both pointed out concerns about the incremental contribution, the thoroughness of the evaluation, the poor clarity and consistency of the writing. Reviewer3 was muted during the discussion. Given the valid concerns from reviewer1/2, this paper is recommended for rejection. ", "reviews": [{"review_id": "SJfHg2A5tQ-0", "review_text": "Summary: This paper presents three small improvements for training binarized neural networks: (1) a modified straight-through estimator, (2) a novel regularizer to push weights to +/- 1, and (3) the use of scaling factors for the binarized weights. Using the methods presented, the validation accuracies on ImageNet and CIFAR-10 are improved by just under 2 percentage points. Pros: - Decent improvement in the performance of the binarized network in the end - The presented regularizers make sense and seem effective. The modified straight-through estimator seems reasonable as well, although the authors do not compare to recent work with a similar adjustment. Cons: - The paper is poorly written and confusing. It reads as if it was written in one pass with no editing or re-writing to clarify contributions or key points, or ensure consistency. - While the final numbers are acceptable, the experiments themselves could be stronger and could be presented more effectively. - The novelty of the scale factors is questionable. Questions and comments: 1. How exactly is the SS_\\beta activation used? It is entirely unclear from the paper, which contradicts itself in multiple ways. Is SS_\\beta used in the forward pass at all for either the weight or activation binarization? Or is only its derivative used in the backward pass? If the latter, then you are not replacing the activation anywhere but are simply using a different straight-through estimator in place of the saturated straight-through estimator (e.g., see [1]). (a) At the beginning of Section 3.3, you say that you modify the training procedure by replacing the sign binarization with the SS_\\beta activation. This sounds like it is referring to the activation function at each layer; however, the pseudocode says that you are using sign() as the per-layer activation. (b) Further, Figure 4 shows that you are using the SS_\\beta function to do weight binarization. However, again, the pseudocode shows that you are using sign() to do the weight binarization. 2. In [1], the authors used a similar type of straight-through estimator (essentially, the gradient of tanh instead of hard_tanh) and found that to be quite effective. You should compare to their method. Also, it's possible that SS_\\beta reduces to tanh for some choice of \\beta -- is this true? 3. The use of scale factors seems to greatly increase the number of parameters in the network and thus greatly decrease the compression benefits gained by using binarization, i.e., you require essentially #scale_factors = a constant factor times the number of actual parameters in the network (since you have a scale factor for each convolutional filter and for each column of each fully-connected layer). As a result of this, what is the actual compression multiplier that your network achieves relative to the original network? 4. For the scale factor, how does yours differ from that used in Rastegari et al. (2016)? It seems the same but you claim that it is a novel contribution of your work. Please clarify. 5. Why did learning \\beta not work? What was the behavior? What values of \\beta did learning settle on? 6. I assume that for each layer output y_i = f(W_i x_i), the regularizer is applied as R(y_i) while at the same time y_i is passed to the next layer -- is this correct? The figures do not clearly show this and should be changed to more clearly show how the regularizer is computed and used, particularly in relation to the activation. 7. In the pseudocode: (a) What does \"mostly bitwise operations\" mean? Are some floating point? (b) Is this the shift-based batchnorm of Hubara et al. (2016)? 8. For Table 1: (a) I assume these are accuracies? The caption should say. (b) Why are there no comparisons to the performance of other methods on this dataset? (c) Any thoughts as to why your method performs better than the full-precision method on this dataset for VGG? 8. For Table 2: (a) Does Table 2 show accuracies on ImageNet? You need to make this clear in the caption. (b) What type of behavior do the runs that do not converge show? This seems like a learning rate problem that is easily fixable. Are there no hyperparameter values that allow it to converge? (c) What behavior do you see when you use SS_1 or SS_2, i.e., \\beta = 1 or \\beta = 2? Since lower \\beta values seem better. (d) The regularization seems to be the most useful contribution -- do you agree? (e) Why did you not do any ablations for the scale factor? Please include these as well. 9. For Table 3, did you compute the numbers for the other approaches or did you use the numbers from their papers? Each approach has its own pros and cons. Please be clear. 10. Are there any plots of validation accuracy versus epoch/time for the different algorithms in order to ensure that the reported numbers were not simply cherry-picked from the run? I assume that you simply used the weights from the end of the 50th epoch -- correct? 11. Is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations' and 'maintaining a global structure to minimize a common cost function is important' ? If so, you should cite this evidence. If not, you should make it clear that these are hypotheses. 12. Why are there not more details about the particular architectures used? These should be included in the appendices to aid those who would like to rerun your experiments. In general, please include more experiment details in the body or appendices. Detailed comments: - R(l) is not defined in Figure 1 and thus is confusing. Also, its replacement of 'Error' from the original figure source makes the figure much more confusing and less clear. - Typos: - 'accustomed' (p.1) - 'the speed by quantizing the activation layers' doesn't make sense (p.1) - 'obtaining' (p.4) - 'asymmetric' doesn't make sense because these are actually symmetric functions across the y-axis (p.4) - 'primary difference is that this regularization ...' --> 'primary difference is that their regularization ...' (p.4) - 'the scales with 75th percentile of the absolute value ... ' is very confusing and unclear (p.7) - 'the loss metric used was the cross-entropy loss, the order of R_1.' I do not know what you're trying to say here (p.8) - Citations: Fix the capitalization issues, typos, and formatting inconsistencies. [1] Friesen and Domingos. Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. ICLR 2018. ------------------- After reading the author response, I do not think the paper does a sufficient job of evaluating the contributions or comparing to existing work. The authors should run ablation experiments, compare to existing work such as [1], and evaluate on additional datasets. These were easy tasks that could have been done during the review period but were not. If I wanted to build on top of this paper to train higher accuracy binary networks, I would have to perform all of these tasks myself to determine which contributions to employ and which are unnecessary. As such, the paper is currently not ready for publication. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "* comment : \u201c Are there any plots of validation accuracy versus epoch/time for the different algorithms in order to ensure that the reported numbers were not simply cherry-picked from the run ? I assume that you simply used the weights from the end of the 50th epoch -- correct ? \u201d Yes we simply evaluated the model at the end of the training procedure . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * comment : \u201c Is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations ' and 'maintaining a global structure to minimize a common cost function is important ' ? If so , you should cite this evidence . If not , you should make it clear that these are hypotheses. \u201d This intuition and reasoning was provided in [ 3 ] , thanks for pointing this out we made sure to cite the paper . Concerning \u201c maintaining a global structure to minimize a common cost function \u201d we re-wrote it as : \u201c How to quantize the weights locally , and maintaining a global structure to minimize common cost function is important [ 4 ] \u201d [ 3 ] Lin , Xiaofan , Cong Zhao , and Wei Pan . `` Towards accurate binary convolutional neural network . '' Advances in Neural Information Processing Systems . 2017 . [ 4 ] Li , Hao , et al . `` Training quantized nets : A deeper understanding . '' Advances in Neural Information Processing Systems . 2017. -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : \u201c Why are there not more details about the particular architectures used ? These should be included in the appendices to aid those who would like to rerun your experiments . In general , please include more experiment details in the body or appendices. \u201d We added more explanations on the architectures and experiments in the corresponding sections . We train both , AlexNet \\citep { krizhevsky2012imagenet } , and VGG \\citep { Simonyan2014VeryDC } using the ADAM \\citep { Kingma2014AdamAM } optimizer . The architecture used for VGG is conv ( 256 ) -conv ( 256-conv ( 512-conv ( 512-conv ( 1024 ) -conv ( 1024-fc ( 1024-fc ( 1024 ) where conv ( \\cdot ) is a convolutional layer , and fc ( \\cdot ) is a fully connected layer . The standard 3\\times3 filters are used in each layer . We also add a batch normalization layer ( Ioffe2015BatchNA ) prior to activation . For AlexNet , the architecture from ( Krizhevsky2014OneWT ) is used , and batch normalization layers are added prior to activations . We use a batch size of 256 for training . Many learning rates were experimented with such as 0.1 , 0.03 , 0.001 , etc , and the initial learning rate for AlexNet was set to 10^ { -3 } , and 3 \\times 10^ { -3 } for VGG . - Typos : Thank you for pointing these out . We made sure to correct all typos and unclear sentences in the revised version of the paper ."}, {"review_id": "SJfHg2A5tQ-1", "review_text": "1. The abstract of this paper should be further refined. I could not find the technical contributions of the proposed method in it. 2. The proposed method for training BNNs in Section 3 is designed by combining or modifying some existing techniques, such as regularized training and approximated gradient. Thus, the novelty of this paper is somewhat weak. 3. Fcn.3 is a complex function for deep neural networks, which integrates three terms of x. I am worried about the convergence of the proposed method. 4. Fortunately, the performance of the proposed method is very promising, especially the results on the Imagenet, which achieves the highest accuracy over the state-of-the-art methods. Considering that the difficulty for training BNNs, I vote it for acceptance. --------------------------------- After reading the responses from authors, I have clearer noticed some important contributions in the proposed methods: 1) A novel regularization function with a scaling factor was introduced for improving the capability of binary neural networks; 2) The proposed activation function can enhance the training procedure of BNNs effectively; 3) Binary networks trained using the proposed method achieved the highest performance over the state-of-the-art methods. Thus, I think this is a nice work for improving performance of binary neural networks, and some of techniques in this paper can be elegantly applied into any other approaches such as binary dictionary learning and binary projections. Therefore, I have increased my score.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for reviewing the paper and providing us comments . Below is a point-point response -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : `` The abstract of this paper should be further refined . I could not find the technical contributions of the proposed method in it . '' We have refined the abstract to include our contributions , please see the revision . \u201c Deep neural networks ( DNN ) are widely used in many applications . However , their deployment on edge devices has been difficult because they are resource hungry . Binary neural networks ( BNN ) help to alleviate the prohibitive resource requirements of DNN , where both activations and weights are limited to 1-bit . We propose an improved binary training method ( BNN+ ) , by introducing a regularization function that encourages training weights around binary values . In addition to this , to enhance model performance we add trainable scaling factors in our regularization functions . Furthermore , we use an improved approximation of the derivative of the $ \\sign $ activation function in the backward computation . These additions are based on linear operations that are easily implementable into the binary training framework and we show experimental results on CIFAR-10 obtaining an accuracy of 86.5 % , on AlexNet and 91.3 % with VGG network . On ImageNet , our method also outperforms the traditional BNN method and XNOR-net , using AlexNet by a margin of 4 % and 2 % top-1 accuracy respectively. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : `` The proposed method for training BNNs in Section 3 is designed by combining or modifying some existing techniques , such as regularized training and approximated gradient . Thus , the novelty of this paper is somewhat weak . '' Main contributions of this paper are as follows : Suggesting regularization functions that encourage training binary weights Embedding trainable scaling factors in the regularization function Adaptive backward approximation to the sign derivative Admittedly , the notion of regularization as well as that of approximation of the derivative are not new in the literature of binary neural networks ( BNN ) . But , the regularization introduced until then is different from ours because it does not penalize the weights greater than 1 or smaller than -1 , also the fact that it is a quadratic function ( 1 - w^2 in [ 1 ] ) and does not include a scaling factor . For the gradient approximation , to the best of our knowledge , there is no adaptive function capable of approximating the derivative of the sign function . Indeed , STE and the one proposed in [ 2 ] are both arbitrarily chosen and are fixed approximations . Thus , our novelty is the introduction of the scaling factor into a regularization function constructed for a BNN ( class of regularization functions R ( w ) = |scaling_factor - |weights| |^p where p=1 and 2 in the paper ) , as well as an adaptive approximation ( using a parameter ) of the derivative of the sign function . Hence , using back-propagation , the binary weights and scaling factors are learned using only one objective function . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : `` Fcn.3 is a complex function for deep neural networks , which integrates three terms of x. I am worried about the convergence of the proposed method . '' The function can be simplified to take on this form : tanh ( beta x / 2 ) + ( beta x / 2 ) sech^2 ( beta x / 2 ) . We formulated it as such so that the correspondence with the derivative of the swish function is more clear . Though this the similar complexity as the swish function and as demonstrated by the swish paper [ 3 ] , as well as our empirical results , we have not observed problems with convergence using the proposed method [ 1 ] Tang , Wei , Gang Hua , and Liang Wang . `` How to train a compact binary neural network with high accuracy ? . '' AAAI.2017 . [ 2 ] Zechun Liu , Baoyuan Wu , Wenhan Luo , Xin Yang , Wei Liu , Kwang-Ting Cheng . \u201d Bi-Real Net : Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm \u201d ECCV . 2018 [ 3 ] Prajit Ramachandran , Barret Zoph , Quoc V. Le . \u201c Searching for Activation Functions. \u201d https : //arxiv.org/abs/1710.05941 . 2018"}, {"review_id": "SJfHg2A5tQ-2", "review_text": "The authors of this paper aim to reduce the constraints required by neural networks so they can be evaluated on lower-power devices. Their approach is to quantize weights, i.e. rounding weights and hidden units so they can be evaluated using bit operations. There are many challenges in this approach, namely that one cannot back-propagate through discrete weights or discrete sign functions. The authors introduce an approximation of the sign function, which they call the SignSwish, and they back-propagate through this, quantizing the weights during the forward pass. Further, they introduce a regularization term to encourage weights to be around learned scales. They evaluate on CIFAR-10 and Imagenet, surpassing most other quantization methods. The paper is pretty clear throughout. The authors do a good job of motivating the problem and placing their approach in the context of previous work. I found Figures 1 and 2 helpful for understanding previous work and the SignSwish activation function, respectively. However, I did not get much out of Figures 3 or 4. I thought Figure 3 was unnecessary (it shows the difference between l1 and l2 regularization), and I thought the psuedo-code in Algorithm 1 was a lot clearer than Figure 4 for showing the scaling factors. Algorithm 1 helped with the clarity of the approach, although it left me with a question: In section 3.3, the authors say that they train by \"replacing the sign binarization with the SS_\\beta activation\" and that they can back-propagate through it. However, in the psuedo-code it seems like they indeed use the sign-function in the forward-pass, replacing it with the signswish in the backward pass. Which is it? The original aspects of their approach are in introducing a new continuous approximation to the sign function and introducing learnable scales for l1 and l2 regularization. The new activation function, the SignSwish, is based off the Swish-activation from Ramachandran et al. (2018). They modify it by centering it and taking the derivative. I'm not sure I understand the intuition behind using the derivative of the Swish as the new activation. It's also unclear how much of BNN+'s success is due to the modification of the Swish function over using the original Swish activation. For this reason I would've liked to see results with just fitting the Swish. In terms of their regularization, they point out that their L2 regularization term is a generalization of the one introduced in Tang et al. (2017). The authors parameterize the regularization term by a scale that is similar to one introduced by Rastegari et al. (2016). As far as I can tell, these are the main novel contributions of the authors' approach. This paper's main selling point isn't originality -- rather, it's that their combination of tweaks lead to state-of-the-art results. Their methods come very close to AlexNet and VGG in terms of top-1 and top-5 CIFAR10 accuracy (with the BNN+ VGG even eclipsing the full-precision VGG top-1 accuracy). When applied to ImageNet, BNN+ outperforms most of the other methods by a good margin, although there is still a lot of room between the BNN+ and full-precision accuracies. The fact that some of the architectures did not converge is a bit concerning. It's an important detail if a training method is unstable, so I would've liked to see more discussion of this instability. The authors don't compare their method to the Bi-Real Net from Liu et al. (2018) since it introduces a shortcut connection to the architecture, although the Bi-Real net is SOTA for Resnet-18 on Imagenet. Did you try implementing the shortcut connection in your architecture? Some more minor points: - The bolding on Table 2 is misleading. It makes it seem like BNN+ has the best top-5 accuracy for Resnet-18, although XNOR-net is in fact superior. - It's unclear to me why the zeros of the derivative of sign swish being at +/- 2.4beta means that when beta is larger, we get a closer approximation to the sign function. The derivative of the sign function is zero almost everywhere, so what's the connection? - Is the initialization of alpha a nice trick, or is it necessary for stable optimization? Experiments on the importance of alpha initialization would've been nice. PROS: - Results. The top-1 and top-5 accuracies for CIFAR10 and Imagenet are SOTA for binarized neural networks. - Importance of problem. Reducing the size of neural networks is an important direction of research in terms of machine learning applications. There is still a lot to be explored. - Clarity: The paper is generally clear throughout. CONS: -Originality. The contributions are an activation function that's a modification of the swish activation, along with parameterized l1 and l2 regularization. -Explanation. The authors don't provide much intuition for why the new activation function is superior to the swish (even including the swish in Figure 2 could improve this). Moreover, they mention that training is unstable without explaining more. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "* comment : `` The authors do n't compare their method to the Bi-Real Net from Liu et al . ( 2018 ) since it introduces a shortcut connection to the architecture , although the Bi-Real net is SOTA for Resnet-18 on Imagenet . Did you try implementing the shortcut connection in your architecture ? '' The connection was not added , as suggested in bi-real net . Here our main objective was to improve the training mechanism for Binary Networks . As future work we can investigate more efficient architectures for binarized neural networks , such as condensenet as opposed to residual networks due to their summation operator rids of too much information , whereas in condensenet the activations are appended hence information is maintained across layers ."}], "0": {"review_id": "SJfHg2A5tQ-0", "review_text": "Summary: This paper presents three small improvements for training binarized neural networks: (1) a modified straight-through estimator, (2) a novel regularizer to push weights to +/- 1, and (3) the use of scaling factors for the binarized weights. Using the methods presented, the validation accuracies on ImageNet and CIFAR-10 are improved by just under 2 percentage points. Pros: - Decent improvement in the performance of the binarized network in the end - The presented regularizers make sense and seem effective. The modified straight-through estimator seems reasonable as well, although the authors do not compare to recent work with a similar adjustment. Cons: - The paper is poorly written and confusing. It reads as if it was written in one pass with no editing or re-writing to clarify contributions or key points, or ensure consistency. - While the final numbers are acceptable, the experiments themselves could be stronger and could be presented more effectively. - The novelty of the scale factors is questionable. Questions and comments: 1. How exactly is the SS_\\beta activation used? It is entirely unclear from the paper, which contradicts itself in multiple ways. Is SS_\\beta used in the forward pass at all for either the weight or activation binarization? Or is only its derivative used in the backward pass? If the latter, then you are not replacing the activation anywhere but are simply using a different straight-through estimator in place of the saturated straight-through estimator (e.g., see [1]). (a) At the beginning of Section 3.3, you say that you modify the training procedure by replacing the sign binarization with the SS_\\beta activation. This sounds like it is referring to the activation function at each layer; however, the pseudocode says that you are using sign() as the per-layer activation. (b) Further, Figure 4 shows that you are using the SS_\\beta function to do weight binarization. However, again, the pseudocode shows that you are using sign() to do the weight binarization. 2. In [1], the authors used a similar type of straight-through estimator (essentially, the gradient of tanh instead of hard_tanh) and found that to be quite effective. You should compare to their method. Also, it's possible that SS_\\beta reduces to tanh for some choice of \\beta -- is this true? 3. The use of scale factors seems to greatly increase the number of parameters in the network and thus greatly decrease the compression benefits gained by using binarization, i.e., you require essentially #scale_factors = a constant factor times the number of actual parameters in the network (since you have a scale factor for each convolutional filter and for each column of each fully-connected layer). As a result of this, what is the actual compression multiplier that your network achieves relative to the original network? 4. For the scale factor, how does yours differ from that used in Rastegari et al. (2016)? It seems the same but you claim that it is a novel contribution of your work. Please clarify. 5. Why did learning \\beta not work? What was the behavior? What values of \\beta did learning settle on? 6. I assume that for each layer output y_i = f(W_i x_i), the regularizer is applied as R(y_i) while at the same time y_i is passed to the next layer -- is this correct? The figures do not clearly show this and should be changed to more clearly show how the regularizer is computed and used, particularly in relation to the activation. 7. In the pseudocode: (a) What does \"mostly bitwise operations\" mean? Are some floating point? (b) Is this the shift-based batchnorm of Hubara et al. (2016)? 8. For Table 1: (a) I assume these are accuracies? The caption should say. (b) Why are there no comparisons to the performance of other methods on this dataset? (c) Any thoughts as to why your method performs better than the full-precision method on this dataset for VGG? 8. For Table 2: (a) Does Table 2 show accuracies on ImageNet? You need to make this clear in the caption. (b) What type of behavior do the runs that do not converge show? This seems like a learning rate problem that is easily fixable. Are there no hyperparameter values that allow it to converge? (c) What behavior do you see when you use SS_1 or SS_2, i.e., \\beta = 1 or \\beta = 2? Since lower \\beta values seem better. (d) The regularization seems to be the most useful contribution -- do you agree? (e) Why did you not do any ablations for the scale factor? Please include these as well. 9. For Table 3, did you compute the numbers for the other approaches or did you use the numbers from their papers? Each approach has its own pros and cons. Please be clear. 10. Are there any plots of validation accuracy versus epoch/time for the different algorithms in order to ensure that the reported numbers were not simply cherry-picked from the run? I assume that you simply used the weights from the end of the 50th epoch -- correct? 11. Is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations' and 'maintaining a global structure to minimize a common cost function is important' ? If so, you should cite this evidence. If not, you should make it clear that these are hypotheses. 12. Why are there not more details about the particular architectures used? These should be included in the appendices to aid those who would like to rerun your experiments. In general, please include more experiment details in the body or appendices. Detailed comments: - R(l) is not defined in Figure 1 and thus is confusing. Also, its replacement of 'Error' from the original figure source makes the figure much more confusing and less clear. - Typos: - 'accustomed' (p.1) - 'the speed by quantizing the activation layers' doesn't make sense (p.1) - 'obtaining' (p.4) - 'asymmetric' doesn't make sense because these are actually symmetric functions across the y-axis (p.4) - 'primary difference is that this regularization ...' --> 'primary difference is that their regularization ...' (p.4) - 'the scales with 75th percentile of the absolute value ... ' is very confusing and unclear (p.7) - 'the loss metric used was the cross-entropy loss, the order of R_1.' I do not know what you're trying to say here (p.8) - Citations: Fix the capitalization issues, typos, and formatting inconsistencies. [1] Friesen and Domingos. Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. ICLR 2018. ------------------- After reading the author response, I do not think the paper does a sufficient job of evaluating the contributions or comparing to existing work. The authors should run ablation experiments, compare to existing work such as [1], and evaluate on additional datasets. These were easy tasks that could have been done during the review period but were not. If I wanted to build on top of this paper to train higher accuracy binary networks, I would have to perform all of these tasks myself to determine which contributions to employ and which are unnecessary. As such, the paper is currently not ready for publication. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "* comment : \u201c Are there any plots of validation accuracy versus epoch/time for the different algorithms in order to ensure that the reported numbers were not simply cherry-picked from the run ? I assume that you simply used the weights from the end of the 50th epoch -- correct ? \u201d Yes we simply evaluated the model at the end of the training procedure . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * comment : \u201c Is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations ' and 'maintaining a global structure to minimize a common cost function is important ' ? If so , you should cite this evidence . If not , you should make it clear that these are hypotheses. \u201d This intuition and reasoning was provided in [ 3 ] , thanks for pointing this out we made sure to cite the paper . Concerning \u201c maintaining a global structure to minimize a common cost function \u201d we re-wrote it as : \u201c How to quantize the weights locally , and maintaining a global structure to minimize common cost function is important [ 4 ] \u201d [ 3 ] Lin , Xiaofan , Cong Zhao , and Wei Pan . `` Towards accurate binary convolutional neural network . '' Advances in Neural Information Processing Systems . 2017 . [ 4 ] Li , Hao , et al . `` Training quantized nets : A deeper understanding . '' Advances in Neural Information Processing Systems . 2017. -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : \u201c Why are there not more details about the particular architectures used ? These should be included in the appendices to aid those who would like to rerun your experiments . In general , please include more experiment details in the body or appendices. \u201d We added more explanations on the architectures and experiments in the corresponding sections . We train both , AlexNet \\citep { krizhevsky2012imagenet } , and VGG \\citep { Simonyan2014VeryDC } using the ADAM \\citep { Kingma2014AdamAM } optimizer . The architecture used for VGG is conv ( 256 ) -conv ( 256-conv ( 512-conv ( 512-conv ( 1024 ) -conv ( 1024-fc ( 1024-fc ( 1024 ) where conv ( \\cdot ) is a convolutional layer , and fc ( \\cdot ) is a fully connected layer . The standard 3\\times3 filters are used in each layer . We also add a batch normalization layer ( Ioffe2015BatchNA ) prior to activation . For AlexNet , the architecture from ( Krizhevsky2014OneWT ) is used , and batch normalization layers are added prior to activations . We use a batch size of 256 for training . Many learning rates were experimented with such as 0.1 , 0.03 , 0.001 , etc , and the initial learning rate for AlexNet was set to 10^ { -3 } , and 3 \\times 10^ { -3 } for VGG . - Typos : Thank you for pointing these out . We made sure to correct all typos and unclear sentences in the revised version of the paper ."}, "1": {"review_id": "SJfHg2A5tQ-1", "review_text": "1. The abstract of this paper should be further refined. I could not find the technical contributions of the proposed method in it. 2. The proposed method for training BNNs in Section 3 is designed by combining or modifying some existing techniques, such as regularized training and approximated gradient. Thus, the novelty of this paper is somewhat weak. 3. Fcn.3 is a complex function for deep neural networks, which integrates three terms of x. I am worried about the convergence of the proposed method. 4. Fortunately, the performance of the proposed method is very promising, especially the results on the Imagenet, which achieves the highest accuracy over the state-of-the-art methods. Considering that the difficulty for training BNNs, I vote it for acceptance. --------------------------------- After reading the responses from authors, I have clearer noticed some important contributions in the proposed methods: 1) A novel regularization function with a scaling factor was introduced for improving the capability of binary neural networks; 2) The proposed activation function can enhance the training procedure of BNNs effectively; 3) Binary networks trained using the proposed method achieved the highest performance over the state-of-the-art methods. Thus, I think this is a nice work for improving performance of binary neural networks, and some of techniques in this paper can be elegantly applied into any other approaches such as binary dictionary learning and binary projections. Therefore, I have increased my score.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for reviewing the paper and providing us comments . Below is a point-point response -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : `` The abstract of this paper should be further refined . I could not find the technical contributions of the proposed method in it . '' We have refined the abstract to include our contributions , please see the revision . \u201c Deep neural networks ( DNN ) are widely used in many applications . However , their deployment on edge devices has been difficult because they are resource hungry . Binary neural networks ( BNN ) help to alleviate the prohibitive resource requirements of DNN , where both activations and weights are limited to 1-bit . We propose an improved binary training method ( BNN+ ) , by introducing a regularization function that encourages training weights around binary values . In addition to this , to enhance model performance we add trainable scaling factors in our regularization functions . Furthermore , we use an improved approximation of the derivative of the $ \\sign $ activation function in the backward computation . These additions are based on linear operations that are easily implementable into the binary training framework and we show experimental results on CIFAR-10 obtaining an accuracy of 86.5 % , on AlexNet and 91.3 % with VGG network . On ImageNet , our method also outperforms the traditional BNN method and XNOR-net , using AlexNet by a margin of 4 % and 2 % top-1 accuracy respectively. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : `` The proposed method for training BNNs in Section 3 is designed by combining or modifying some existing techniques , such as regularized training and approximated gradient . Thus , the novelty of this paper is somewhat weak . '' Main contributions of this paper are as follows : Suggesting regularization functions that encourage training binary weights Embedding trainable scaling factors in the regularization function Adaptive backward approximation to the sign derivative Admittedly , the notion of regularization as well as that of approximation of the derivative are not new in the literature of binary neural networks ( BNN ) . But , the regularization introduced until then is different from ours because it does not penalize the weights greater than 1 or smaller than -1 , also the fact that it is a quadratic function ( 1 - w^2 in [ 1 ] ) and does not include a scaling factor . For the gradient approximation , to the best of our knowledge , there is no adaptive function capable of approximating the derivative of the sign function . Indeed , STE and the one proposed in [ 2 ] are both arbitrarily chosen and are fixed approximations . Thus , our novelty is the introduction of the scaling factor into a regularization function constructed for a BNN ( class of regularization functions R ( w ) = |scaling_factor - |weights| |^p where p=1 and 2 in the paper ) , as well as an adaptive approximation ( using a parameter ) of the derivative of the sign function . Hence , using back-propagation , the binary weights and scaling factors are learned using only one objective function . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * comment : `` Fcn.3 is a complex function for deep neural networks , which integrates three terms of x. I am worried about the convergence of the proposed method . '' The function can be simplified to take on this form : tanh ( beta x / 2 ) + ( beta x / 2 ) sech^2 ( beta x / 2 ) . We formulated it as such so that the correspondence with the derivative of the swish function is more clear . Though this the similar complexity as the swish function and as demonstrated by the swish paper [ 3 ] , as well as our empirical results , we have not observed problems with convergence using the proposed method [ 1 ] Tang , Wei , Gang Hua , and Liang Wang . `` How to train a compact binary neural network with high accuracy ? . '' AAAI.2017 . [ 2 ] Zechun Liu , Baoyuan Wu , Wenhan Luo , Xin Yang , Wei Liu , Kwang-Ting Cheng . \u201d Bi-Real Net : Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm \u201d ECCV . 2018 [ 3 ] Prajit Ramachandran , Barret Zoph , Quoc V. Le . \u201c Searching for Activation Functions. \u201d https : //arxiv.org/abs/1710.05941 . 2018"}, "2": {"review_id": "SJfHg2A5tQ-2", "review_text": "The authors of this paper aim to reduce the constraints required by neural networks so they can be evaluated on lower-power devices. Their approach is to quantize weights, i.e. rounding weights and hidden units so they can be evaluated using bit operations. There are many challenges in this approach, namely that one cannot back-propagate through discrete weights or discrete sign functions. The authors introduce an approximation of the sign function, which they call the SignSwish, and they back-propagate through this, quantizing the weights during the forward pass. Further, they introduce a regularization term to encourage weights to be around learned scales. They evaluate on CIFAR-10 and Imagenet, surpassing most other quantization methods. The paper is pretty clear throughout. The authors do a good job of motivating the problem and placing their approach in the context of previous work. I found Figures 1 and 2 helpful for understanding previous work and the SignSwish activation function, respectively. However, I did not get much out of Figures 3 or 4. I thought Figure 3 was unnecessary (it shows the difference between l1 and l2 regularization), and I thought the psuedo-code in Algorithm 1 was a lot clearer than Figure 4 for showing the scaling factors. Algorithm 1 helped with the clarity of the approach, although it left me with a question: In section 3.3, the authors say that they train by \"replacing the sign binarization with the SS_\\beta activation\" and that they can back-propagate through it. However, in the psuedo-code it seems like they indeed use the sign-function in the forward-pass, replacing it with the signswish in the backward pass. Which is it? The original aspects of their approach are in introducing a new continuous approximation to the sign function and introducing learnable scales for l1 and l2 regularization. The new activation function, the SignSwish, is based off the Swish-activation from Ramachandran et al. (2018). They modify it by centering it and taking the derivative. I'm not sure I understand the intuition behind using the derivative of the Swish as the new activation. It's also unclear how much of BNN+'s success is due to the modification of the Swish function over using the original Swish activation. For this reason I would've liked to see results with just fitting the Swish. In terms of their regularization, they point out that their L2 regularization term is a generalization of the one introduced in Tang et al. (2017). The authors parameterize the regularization term by a scale that is similar to one introduced by Rastegari et al. (2016). As far as I can tell, these are the main novel contributions of the authors' approach. This paper's main selling point isn't originality -- rather, it's that their combination of tweaks lead to state-of-the-art results. Their methods come very close to AlexNet and VGG in terms of top-1 and top-5 CIFAR10 accuracy (with the BNN+ VGG even eclipsing the full-precision VGG top-1 accuracy). When applied to ImageNet, BNN+ outperforms most of the other methods by a good margin, although there is still a lot of room between the BNN+ and full-precision accuracies. The fact that some of the architectures did not converge is a bit concerning. It's an important detail if a training method is unstable, so I would've liked to see more discussion of this instability. The authors don't compare their method to the Bi-Real Net from Liu et al. (2018) since it introduces a shortcut connection to the architecture, although the Bi-Real net is SOTA for Resnet-18 on Imagenet. Did you try implementing the shortcut connection in your architecture? Some more minor points: - The bolding on Table 2 is misleading. It makes it seem like BNN+ has the best top-5 accuracy for Resnet-18, although XNOR-net is in fact superior. - It's unclear to me why the zeros of the derivative of sign swish being at +/- 2.4beta means that when beta is larger, we get a closer approximation to the sign function. The derivative of the sign function is zero almost everywhere, so what's the connection? - Is the initialization of alpha a nice trick, or is it necessary for stable optimization? Experiments on the importance of alpha initialization would've been nice. PROS: - Results. The top-1 and top-5 accuracies for CIFAR10 and Imagenet are SOTA for binarized neural networks. - Importance of problem. Reducing the size of neural networks is an important direction of research in terms of machine learning applications. There is still a lot to be explored. - Clarity: The paper is generally clear throughout. CONS: -Originality. The contributions are an activation function that's a modification of the swish activation, along with parameterized l1 and l2 regularization. -Explanation. The authors don't provide much intuition for why the new activation function is superior to the swish (even including the swish in Figure 2 could improve this). Moreover, they mention that training is unstable without explaining more. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "* comment : `` The authors do n't compare their method to the Bi-Real Net from Liu et al . ( 2018 ) since it introduces a shortcut connection to the architecture , although the Bi-Real net is SOTA for Resnet-18 on Imagenet . Did you try implementing the shortcut connection in your architecture ? '' The connection was not added , as suggested in bi-real net . Here our main objective was to improve the training mechanism for Binary Networks . As future work we can investigate more efficient architectures for binarized neural networks , such as condensenet as opposed to residual networks due to their summation operator rids of too much information , whereas in condensenet the activations are appended hence information is maintained across layers ."}}