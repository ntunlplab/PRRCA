{"year": "2020", "forum": "SJeOAJStwB", "title": "On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods", "decision": "Reject", "meta_review": "This paper studies the problem of federated learning for non-i.i.d. data, and looks at the hyperparameter optimization in this setting. As the reviewers have noted, this is a purely empirical paper. There are certain aspects of the experiments that need further discussion, especially the learning rate selection for different architectures. That said, the submission may not be ready for publication at its current stage.", "reviews": [{"review_id": "SJeOAJStwB-0", "review_text": "In this paper, the authors empirically investigate parameter divergence of local updates in federated learning with non-IID data. The authors study the effects of optimizers, network depth/width, and regularization techniques, and provide some observations. In overall, I think this paper study an important problem in federated learning. However, there are some weakness in this paper: 1. The paper is nearly pure empirical. There is no theoretical analysis supporting the observations proposed in Section 4.1, which weaken the contribution of this paper. 2. This paper only raises some issues in federated learning with non-IID data, and discusses the potential causes. No suggestions or potential solutions is proposed in this paper, which weaken the contribution of this paper. 3. Since this is nearly a pure empirical paper, I hope the authors can make the experiments thorough. However, there are some experiments I expect to see but not yet included in this paper: 3.1. The authors only studies Nesterov momentum in this paper. However, in practice, it is more common to use Polyak momentum. I hope the authors can also study FL SGD with Polyak momentum in this paper. 3.2. In this paper, the authors assume that different workers has the same number of local data samples (in Definition 1). However, due to the heterogeneous setting, it is very likely that different workers have different numbers of local data samples, which could be another source of divergence. Furthermore, different numbers of local data samples also results in different numbers of local steps, which may also cause divergence. 3.3. [1] proposes a regularization mechanism (FedProx) to deal with the heterogeneity. Instead of studying weight decay, it is more reasonable to study the regularization technique proposed by [1]. 4. There are some missing details (maybe they are already in the paper but I didn't find them): 4.1. What is the definition of Adam-A and Adam-WB? And, what are the differences between Adam-A, Adam-WB, and vanilla Adam? (and also, what is the \"A\" in NMom-A?) 4.2. When using Adam in federated learning, how are the variables synchronized? Note that for Adam, there are 3 sets of variables: model parameters, 1st moment, and 2nd moment. Due to the local updates, all the 3 sets of variables are not synchronized. When the authors use Adam in FL, did they only synchronize/average the model parameter and ignore the 1st and 2nd moments, or did they synchronize all the 3 sets of variables? ---------------- Reference [1] Li, Tian et al. \u201cFederated Optimization for Heterogeneous Networks.\u201d (2018).", "rating": "3: Weak Reject", "reply_text": "We first appreciate the valuable comments . We carefully looked through all the comments ; the following describes our answers . 1.The paper is nearly pure empirical . There is no theoretical analysis supporting the observations proposed in Section 4.1 , which weaken the contribution of this paper . 2.This paper only raises some issues in federated learning with non-IID data , and discusses the potential causes . No suggestions or potential solutions is proposed in this paper , which weaken the contribution of this paper . ===== ( Answer for Question 1 and 2 ) ===== We appreciate the valuable comments , and we admit your concerns . Nevertheless , we believe that focusing on federated learning with non-IID data , our work provides the meaningful exploratory analysis breaking the existing common wisdom about the considered hyperparameter optimization methods . In relation , here we intend to emphasize our contributions . Our distinct contributions can be highlighted as follows : * * Regarding Section 3 : In many previous literatures , e.g. , ( Zhao et al. , 2018 ) , parameter divergence is regarded as a direct response to learners \u2019 local data being non-IID sampled from the population distribution . In relation , it was reported that as the probabilistic distance ( e.g. , earth mover \u2019 s distance ) of learners \u2019 local data becomes farther away from the population distribution , bigger parameter divergence might appear ; this is correlated with the degradation of performance such as test accuracy ( please refer to Section 3.2 of ( Zhao et al. , 2018 ) ) . Also , we added our analysis of the relationship among the three factors ( i.e. , probabilistic distance , parameter divergence , and performance ) in the rebuttal period ; the relevant description can be found in Section 3 of the revised version of this paper . Regarding the parameter divergence , our distinct contribution can be summarized in two-fold : First , for the first time we identified the mechanism by which data non-IIDness affects the parameter divergence : \u201c if data distributions in each local dataset are highly skewed and heterogeneous over classes , subsets of neurons , which have especially big magnitudes of the gradients in back propagation , become significantly different across learners ; this leads to inordinate parameter divergence between them \u201d . It has been analyzed in both empirical and theoretical way . Second , many of the related literatures usually handle the parameter difference of each learner \u2019 s local model parameters from one computed with the population distribution ( this philosophy is connected to the definition of PD-VL ) ; meanwhile , in our study we also considered the parameter diversity between the local updates as well ( this is connected to the definition of PD-Ls ) . The reason of probing parameter divergence being important is that the federated learning are performed based on iterative parameter averaging . That is , investigating how local updates are diverged can give a clue whether the subsequent parameter averaging yields positive returns ; the proposed divergence metrics provide two ways for it . * * Regarding Section 4.1 : In this study , we focused on the well-known hyperparameter optimization strategies ( i.e. , hyperparametric strategies ) to improve learning performance : ( i ) using momentum SGD or Adam than pure SGD , ( ii ) network deepening/widening ( until a proper level ) , ( iii ) Batch Normalization , ( iv ) weight decay , ( v ) data augmentation , and ( vi ) Dropout . Their positive effects have been reported in a variety of literatures ; practically , they are being broadly used in deep net training . Also in our experiments , the hyperparametric methods yielded better outcome under vanilla training ( i.e. , non-distributed training ) and under the considered federated learning algorithm with the IID decentralized data setting . However , under the non-IID data setting , we newly identified that the hyperparametric methods could rather give negative/diminishing effects on performance of the federated learning algorithm ; we believe that these findings can be highly impactful to the upcoming works or industrial implementations ."}, {"review_id": "SJeOAJStwB-1", "review_text": "The paper experimentally studies the reasons for the slow convergence of the Federated Averaging algorithm when the data are non-iid distributed between workers in the multiclass-classification case. Paper performs extensive experimental study and observes that the main reasons for failure are connected to (i) the parameter divergence during the local steps, (ii) steep-fall phenomena when parameters on different nodes are getting close fast, and to the (iii) high training loss. My score is weak reject. The paper provides extensive but unclear experimental results. Improving presentation would significantly improve the paper. For example, why in experimental and theoretical study different parameter divergence metrics were used, etc (see below), why different networks use different optimizers. Moreover, provided experimental comparison might be unfair. The learning rate is constant throughout all of the experiments, depending only on the optimizer, but not on the neural network architecture. This can affect the final results. Concerns and questions that should be addressed: 1. The initial learning rates were not tuned properly. It is set to be the same for different neural network topologies, which might significantly affect the results. What did the choice of initial learning rates is based on? 2. Why the parameter divergence metric in Definition 1 is not the same as in the theoretical study (Appendix B)? What is the intuition behind Definition 1? 3. Why the divergence of parameters is considered only at the last layer? It seems to hide many important interactions in the other layers. 4. Some important experimental details --- should be added: - At which moment the parameter divergence is computed in the plots? Is it computed at the end of the local iterations right before synchronization? - How the training loss was computed in the plots? before or after synchronization? on the local only or the global data? - Which batch size was used? - Improve the figure caption to detail the experimental setup. (e.g. in fig 3. the network architecture was mentioned only for one of the figures, include which optimized was used, etc) 5. In experiments on Fig. 2. and Fig.3 (middle) what is the accuracy for IID baseline? Is the observed phenomena connected to the poor network architecture or to the non-iid data? 6. In table 5 of the appendix, why experiments use Adam optimizer, but not Momentum SGD as in the main paper to compare the performance of ResNet14 and ResNet20? 7. Better re-prase the definition of the steep fall phenomena, now it is not very clear: in the IID setting parameter divergence values are also sometimes reducing sharply; in the network width study parameters divergence doesn\u2019t experience sudden drop. Also, how does this phenomena (and parameter divergence too) connects to the training loss? 8. Why for different experiments different baseline models are used? (NetA, NetB, NetC) Other minor comments: - Appendix B, first equation on page 13. (d_q)^t -> (d_q)^t_k; The size of gradient \\nabla_w [E ...] is different from the size of (d_q)_k. They cannot be added together. - page 7, last sentence of the first paragraph: what is the accuracy achieved with Batch Renormalization? Why the reason for accuracy gap is \u201csignificant parameter divergence\u201d? on fig. 3 \u201cparameter divergence\u201d is smaller than for the baseline. - Why the name of the section on page 7 is \u201cexcessively high training loss of local updates\u201d if later it is stated that it is actually smaller than for the IID case? - Defenition 1, line 4: \u201cthe then\u201d -> \u201cthe\u201d - section 3: \u201cA pleasant level of parameter divergence can help to improve generalization\u201d -> where was it shown? - section 4.2, paragraph 2: what is meant by \u201chyperparametric methods\u201d? - section 4.2, paragraph 3: \u201cquantitative increase in a layer level\u201d -> not clear what does it mean. - page 4, effect of optimizers: what do you refer to as \u201call model parameters\u201d? - page 5, last paragraph: Hinton et al... -> (Hinton et al\u2026). Use \\citet(\\citep) instead of \\cite. - why Dropout yields bigger parameter divergence if on Fig 2, right it actually helps? - Last line of the page 5. Where was this observed? ", "rating": "3: Weak Reject", "reply_text": "We first appreciate the valuable comments . We carefully looked through all the comments ; the following describes our answers . 1.The initial learning rates were not tuned properly . It is set to be the same for different neural network topologies , which might significantly affect the results . What did the choice of initial learning rates is based on ? ===== ( Answer ) ===== As you remark , the initial learning rates were set to be the same for different model architectures . Therefore , the best results might not have been obtained with regard to the learning rates . Nevertheless , the choice of the initial learning rates was conducted based on the follows : ( i ) Based on the results of Appendix B as well as the intuitive thoughts , ( especially before the first learning rate drop ) learning rates may highly affect the values of the parameter divergence . Therefore , we set the initial learning rates the same for the compared cases ( e.g. , NetA-Baseline vs NetA-Deeper vs NetA-Deepest ) so that the corresponding parameter divergence values could be compared under the same conditions . ( ii ) In addition , one of the main objective in the paper is to show that the considered hyper parameter optimization strategies ( which have been reported that they yield better outcome under \u201c vanilla \u201d training or under the federated learning with IID data ) could rather result in the diminishing returns under non-IID data setting . As described in Tables 7-13 of Appendix C ( in the revised version of the paper ) , we can see that under \u201c vanilla training \u201d ( especially for batch size : 50 ) and under the federated learning with the IID data setting , most of the results are shown to be similar with what we already know ( e.g. , the advantages of deeper network architectures , global average pooling , Batch Normalization , and so on ) . However , under the federated learning with the Non-IID ( 2 ) data setting , we can see that some of the hyperparameter optmization methods rather yield the highly conflicted results ( i.e. , the diminishing returns ) . Therefore , in summary , our setting of the initial learning rates could be rather far from the best results ; nevertheless , from Tables 7-13 the results can be interpreted as still valid ( since the results under \u201c vanilla training \u201d and under the federated learning with the IID data setting follow the similar trends to those well known ) . In addition , we believe that our setting also provides the fair comparison of parameter divergence . 2.Why the parameter divergence metric in Definition 1 is not the same as in the theoretical study ( Appendix B ) ? What is the intuition behind Definition 1 ? ===== ( Answer ) ===== We first remark that PD-Ls in Definition 1 and || ( d_q ) ^ ( t+1 ) _i - ( d_q ) ^ ( t+1 ) _j|| are related . In the case of Figure 1 ( and Appendix B ) , we used the same network architecture and training methods . Manipulated variables here is only data distributions ( i.e. , IID , Non-IID ( 2 ) , and Non-IID ( 1 ) ) . Therefore , || ( d_q ) ^ ( t+1 ) _i - ( d_q ) ^ ( t+1 ) _j|| can be validly utilized . However , in most of our experiments , we compared the different network architectures ( e.g. , NetC-Baseline , NetC-Wider , and NetC-Widest ) or the effects of the different training settings ( e.g. , various weight decay factors ) together in a set . Therefore , for instance , in the case of NetC-Baseline , NetC-Wider , and NetC-Widest , the number of neurons in the output layer becomes different ( i.e. , 2560 , 10240 , and 40960 , respectively ) ; in the case of various weight decay factors , the degree to which the model parameters from the previous iteration are reflected in the current parameters highly depends on the factor values . Therefore , we thought that we need a normalized ( qualitative ) metric rather than simply considering the magnitude of parameter ( weight ) differences ; consequently , instead of the euclidean distance , we used cosine distance-based metrics in Definition 1 . This answer was reflected in the third paragraph of Section 3 in the revised version of the paper ."}, {"review_id": "SJeOAJStwB-2", "review_text": "Summary: The paper presents an empirical study of causes of parameter divergence in federated learning. Federated learning is the setting where parameter updates (e.g. gradients) are computed separately on possibly non-IID subsamples of the data and then aggregated by averaging. The paper examines the effects of choice of optimizer, network width and depth, batch normalization, weight decay, and data augmentation on the amount of parameter divergence. Divergence is defined as the average cosine distance between pairs of locally-updated weights, or between locally updated weights and weights trained with IID data. The paper generally concludes that regularization methods like BN and weight decay have an adverse effect in the federated setting, the deepening the network has an adverse effect while widening it might be beneficial, and that adaptive optimizers like Adam can perform poorly if their internal statistics are not aggregated. I recommend that the paper be rejected. The main shortcoming of the paper is the lack of rigororous statistical analysis to support its conclusions. The paper contains a lot of raw data, but the discussion mainly highlights trends that the authors seem to have observed in the results, without quantifying the relative sizes of effects, how consistent they are across experimental conditions, etc. The writing is also quite unclear, to the point that I often didn't understand exactly what argument was being made. Details / Questions: The main problem is the lack of quantitative analysis of the trends the paper identifies. For example, regarding \"Effects of Batch Normalization\", there seem to be two claims made: 1. Batch normalization makes things worse (somehow) in the federated setting 2. Batch re-normalization still makes things worse, but not as much How are these effects quantified? How large are they? Do they hold across all datasets, architectures, and optimizers considered? Ideally there would be a table summarizing each experimental manipulation, its effect on performance, whether that effect is significant, etc. Of course this requires some care because the paper is doing an exploratory analysis and there are many hypotheses to test; a good reference is [1]. The paper also relies heavily on parameter divergence as a measure of performance in federated learning, but I see no evidence presented that parameter divergence is predictive of test accuracy (which is presumably what we actually care about). Intuitively I can see how it might be related, but since divergence is basically being used as a proxy for accuracy, it is vital to show convincingly that the two are related. What do we gain by analyzing parameter divergence rather than simply comparing test accuracy? Regarding the \"steep fall phenomenon\": The paper seems to present this as an indicator that a manipulation performs poorly in the federated setting. But, isn't it a good thing if parameter divergence goes down? Why does specifically a sudden, sharp decrease in divergence indicate a problem? Finally, some improvements might be made to the experiment setup. For one, the case of completely-disjoint label sets in different local learners seems extreme to me. Wouldn't at least partial overlap be more common in practice? (This is not my area so I don't know). Experimenting with different degrees of overlap would be useful. As for network architectures, it would be valuable to look at a greater variety of standard architecture styles (e.g. ResNet, Inception, etc). I realize there are some experiments with ResNet, but the focus is mainly on the single-path VGG-like architecture. I do realize this is a lot of experiments to do. Minor points: * In the setting described as \"IID\" in Table 1 is not, the subsampled for each learner are not IID subsamples of the full dataset because they are class-balanced (if I'm understanding correctly) References: [1] Dem\u0161ar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7(Jan), 1-30.", "rating": "1: Reject", "reply_text": "We first appreciate the valuable comments . We carefully looked through all the comments ; the following describes our answers . 1.Regarding the lack of quantitative analysis of the trends the paper identifies ===== ( Answer ) ===== We appreciate the valuable comments . As you point out , we admit that the paper lacks a quantitive analysis of the findings . However , please remind that even under \u201c vanilla \u201d training , it is not easy to generally quantify the gains of the considered hyperparameter optimization methods since they highly depend on the training dataset or the remaining training strategies . Therefore , we were afraid to conclude the general quantification of the effects of the methods . Instead , by also providing the results under \u201c vanilla \u201d training and the federated learning with IID data , we tried to emphasize the negative effects of the hyperparametric methods ; we think our results show the severity of performance degradation of each method , even indirectly . 2.Regarding the relationship between test accuracy and parameter divergence ===== ( Answer ) ===== In many previous literatures , e.g. , ( Zhao et al. , 2018 ) , parameter divergence is regarded as a direct response to learners \u2019 local data being non-IID sampled from the population distribution . In relation , it was reported that as the probabilistic distance ( e.g. , earth mover \u2019 s distance ) of learners \u2019 local data becomes farther away from the population distribution , bigger parameter divergence might appear ; this is correlated with the degradation of performance such as test accuracy ( please refer to Section 3.2 of ( Zhao et al. , 2018 ) ) . Also , we added our analysis of the relationship among the three factors ( i.e. , probabilistic distance , parameter divergence , and performance ) in the rebuttal period ; the relevant description can be found in Section 3 of the revised version of this paper . The reason of probing parameter divergence being important is that the federated learning are performed based on iterative parameter averaging . That is , investigating how local updates are diverged can give a clue whether the subsequent parameter averaging yields positive returns ; the proposed divergence metrics provide two ways for it . ( Zhao et al. , 2018 ) Yue Zhao , Meng Li , Liangzhen Lai , Naveen Suda , Damon Civin , and Vikas Chandra . Federated learning with non-IID data . arXiv preprint arXiv : 1806.00582 , 2018 ."}], "0": {"review_id": "SJeOAJStwB-0", "review_text": "In this paper, the authors empirically investigate parameter divergence of local updates in federated learning with non-IID data. The authors study the effects of optimizers, network depth/width, and regularization techniques, and provide some observations. In overall, I think this paper study an important problem in federated learning. However, there are some weakness in this paper: 1. The paper is nearly pure empirical. There is no theoretical analysis supporting the observations proposed in Section 4.1, which weaken the contribution of this paper. 2. This paper only raises some issues in federated learning with non-IID data, and discusses the potential causes. No suggestions or potential solutions is proposed in this paper, which weaken the contribution of this paper. 3. Since this is nearly a pure empirical paper, I hope the authors can make the experiments thorough. However, there are some experiments I expect to see but not yet included in this paper: 3.1. The authors only studies Nesterov momentum in this paper. However, in practice, it is more common to use Polyak momentum. I hope the authors can also study FL SGD with Polyak momentum in this paper. 3.2. In this paper, the authors assume that different workers has the same number of local data samples (in Definition 1). However, due to the heterogeneous setting, it is very likely that different workers have different numbers of local data samples, which could be another source of divergence. Furthermore, different numbers of local data samples also results in different numbers of local steps, which may also cause divergence. 3.3. [1] proposes a regularization mechanism (FedProx) to deal with the heterogeneity. Instead of studying weight decay, it is more reasonable to study the regularization technique proposed by [1]. 4. There are some missing details (maybe they are already in the paper but I didn't find them): 4.1. What is the definition of Adam-A and Adam-WB? And, what are the differences between Adam-A, Adam-WB, and vanilla Adam? (and also, what is the \"A\" in NMom-A?) 4.2. When using Adam in federated learning, how are the variables synchronized? Note that for Adam, there are 3 sets of variables: model parameters, 1st moment, and 2nd moment. Due to the local updates, all the 3 sets of variables are not synchronized. When the authors use Adam in FL, did they only synchronize/average the model parameter and ignore the 1st and 2nd moments, or did they synchronize all the 3 sets of variables? ---------------- Reference [1] Li, Tian et al. \u201cFederated Optimization for Heterogeneous Networks.\u201d (2018).", "rating": "3: Weak Reject", "reply_text": "We first appreciate the valuable comments . We carefully looked through all the comments ; the following describes our answers . 1.The paper is nearly pure empirical . There is no theoretical analysis supporting the observations proposed in Section 4.1 , which weaken the contribution of this paper . 2.This paper only raises some issues in federated learning with non-IID data , and discusses the potential causes . No suggestions or potential solutions is proposed in this paper , which weaken the contribution of this paper . ===== ( Answer for Question 1 and 2 ) ===== We appreciate the valuable comments , and we admit your concerns . Nevertheless , we believe that focusing on federated learning with non-IID data , our work provides the meaningful exploratory analysis breaking the existing common wisdom about the considered hyperparameter optimization methods . In relation , here we intend to emphasize our contributions . Our distinct contributions can be highlighted as follows : * * Regarding Section 3 : In many previous literatures , e.g. , ( Zhao et al. , 2018 ) , parameter divergence is regarded as a direct response to learners \u2019 local data being non-IID sampled from the population distribution . In relation , it was reported that as the probabilistic distance ( e.g. , earth mover \u2019 s distance ) of learners \u2019 local data becomes farther away from the population distribution , bigger parameter divergence might appear ; this is correlated with the degradation of performance such as test accuracy ( please refer to Section 3.2 of ( Zhao et al. , 2018 ) ) . Also , we added our analysis of the relationship among the three factors ( i.e. , probabilistic distance , parameter divergence , and performance ) in the rebuttal period ; the relevant description can be found in Section 3 of the revised version of this paper . Regarding the parameter divergence , our distinct contribution can be summarized in two-fold : First , for the first time we identified the mechanism by which data non-IIDness affects the parameter divergence : \u201c if data distributions in each local dataset are highly skewed and heterogeneous over classes , subsets of neurons , which have especially big magnitudes of the gradients in back propagation , become significantly different across learners ; this leads to inordinate parameter divergence between them \u201d . It has been analyzed in both empirical and theoretical way . Second , many of the related literatures usually handle the parameter difference of each learner \u2019 s local model parameters from one computed with the population distribution ( this philosophy is connected to the definition of PD-VL ) ; meanwhile , in our study we also considered the parameter diversity between the local updates as well ( this is connected to the definition of PD-Ls ) . The reason of probing parameter divergence being important is that the federated learning are performed based on iterative parameter averaging . That is , investigating how local updates are diverged can give a clue whether the subsequent parameter averaging yields positive returns ; the proposed divergence metrics provide two ways for it . * * Regarding Section 4.1 : In this study , we focused on the well-known hyperparameter optimization strategies ( i.e. , hyperparametric strategies ) to improve learning performance : ( i ) using momentum SGD or Adam than pure SGD , ( ii ) network deepening/widening ( until a proper level ) , ( iii ) Batch Normalization , ( iv ) weight decay , ( v ) data augmentation , and ( vi ) Dropout . Their positive effects have been reported in a variety of literatures ; practically , they are being broadly used in deep net training . Also in our experiments , the hyperparametric methods yielded better outcome under vanilla training ( i.e. , non-distributed training ) and under the considered federated learning algorithm with the IID decentralized data setting . However , under the non-IID data setting , we newly identified that the hyperparametric methods could rather give negative/diminishing effects on performance of the federated learning algorithm ; we believe that these findings can be highly impactful to the upcoming works or industrial implementations ."}, "1": {"review_id": "SJeOAJStwB-1", "review_text": "The paper experimentally studies the reasons for the slow convergence of the Federated Averaging algorithm when the data are non-iid distributed between workers in the multiclass-classification case. Paper performs extensive experimental study and observes that the main reasons for failure are connected to (i) the parameter divergence during the local steps, (ii) steep-fall phenomena when parameters on different nodes are getting close fast, and to the (iii) high training loss. My score is weak reject. The paper provides extensive but unclear experimental results. Improving presentation would significantly improve the paper. For example, why in experimental and theoretical study different parameter divergence metrics were used, etc (see below), why different networks use different optimizers. Moreover, provided experimental comparison might be unfair. The learning rate is constant throughout all of the experiments, depending only on the optimizer, but not on the neural network architecture. This can affect the final results. Concerns and questions that should be addressed: 1. The initial learning rates were not tuned properly. It is set to be the same for different neural network topologies, which might significantly affect the results. What did the choice of initial learning rates is based on? 2. Why the parameter divergence metric in Definition 1 is not the same as in the theoretical study (Appendix B)? What is the intuition behind Definition 1? 3. Why the divergence of parameters is considered only at the last layer? It seems to hide many important interactions in the other layers. 4. Some important experimental details --- should be added: - At which moment the parameter divergence is computed in the plots? Is it computed at the end of the local iterations right before synchronization? - How the training loss was computed in the plots? before or after synchronization? on the local only or the global data? - Which batch size was used? - Improve the figure caption to detail the experimental setup. (e.g. in fig 3. the network architecture was mentioned only for one of the figures, include which optimized was used, etc) 5. In experiments on Fig. 2. and Fig.3 (middle) what is the accuracy for IID baseline? Is the observed phenomena connected to the poor network architecture or to the non-iid data? 6. In table 5 of the appendix, why experiments use Adam optimizer, but not Momentum SGD as in the main paper to compare the performance of ResNet14 and ResNet20? 7. Better re-prase the definition of the steep fall phenomena, now it is not very clear: in the IID setting parameter divergence values are also sometimes reducing sharply; in the network width study parameters divergence doesn\u2019t experience sudden drop. Also, how does this phenomena (and parameter divergence too) connects to the training loss? 8. Why for different experiments different baseline models are used? (NetA, NetB, NetC) Other minor comments: - Appendix B, first equation on page 13. (d_q)^t -> (d_q)^t_k; The size of gradient \\nabla_w [E ...] is different from the size of (d_q)_k. They cannot be added together. - page 7, last sentence of the first paragraph: what is the accuracy achieved with Batch Renormalization? Why the reason for accuracy gap is \u201csignificant parameter divergence\u201d? on fig. 3 \u201cparameter divergence\u201d is smaller than for the baseline. - Why the name of the section on page 7 is \u201cexcessively high training loss of local updates\u201d if later it is stated that it is actually smaller than for the IID case? - Defenition 1, line 4: \u201cthe then\u201d -> \u201cthe\u201d - section 3: \u201cA pleasant level of parameter divergence can help to improve generalization\u201d -> where was it shown? - section 4.2, paragraph 2: what is meant by \u201chyperparametric methods\u201d? - section 4.2, paragraph 3: \u201cquantitative increase in a layer level\u201d -> not clear what does it mean. - page 4, effect of optimizers: what do you refer to as \u201call model parameters\u201d? - page 5, last paragraph: Hinton et al... -> (Hinton et al\u2026). Use \\citet(\\citep) instead of \\cite. - why Dropout yields bigger parameter divergence if on Fig 2, right it actually helps? - Last line of the page 5. Where was this observed? ", "rating": "3: Weak Reject", "reply_text": "We first appreciate the valuable comments . We carefully looked through all the comments ; the following describes our answers . 1.The initial learning rates were not tuned properly . It is set to be the same for different neural network topologies , which might significantly affect the results . What did the choice of initial learning rates is based on ? ===== ( Answer ) ===== As you remark , the initial learning rates were set to be the same for different model architectures . Therefore , the best results might not have been obtained with regard to the learning rates . Nevertheless , the choice of the initial learning rates was conducted based on the follows : ( i ) Based on the results of Appendix B as well as the intuitive thoughts , ( especially before the first learning rate drop ) learning rates may highly affect the values of the parameter divergence . Therefore , we set the initial learning rates the same for the compared cases ( e.g. , NetA-Baseline vs NetA-Deeper vs NetA-Deepest ) so that the corresponding parameter divergence values could be compared under the same conditions . ( ii ) In addition , one of the main objective in the paper is to show that the considered hyper parameter optimization strategies ( which have been reported that they yield better outcome under \u201c vanilla \u201d training or under the federated learning with IID data ) could rather result in the diminishing returns under non-IID data setting . As described in Tables 7-13 of Appendix C ( in the revised version of the paper ) , we can see that under \u201c vanilla training \u201d ( especially for batch size : 50 ) and under the federated learning with the IID data setting , most of the results are shown to be similar with what we already know ( e.g. , the advantages of deeper network architectures , global average pooling , Batch Normalization , and so on ) . However , under the federated learning with the Non-IID ( 2 ) data setting , we can see that some of the hyperparameter optmization methods rather yield the highly conflicted results ( i.e. , the diminishing returns ) . Therefore , in summary , our setting of the initial learning rates could be rather far from the best results ; nevertheless , from Tables 7-13 the results can be interpreted as still valid ( since the results under \u201c vanilla training \u201d and under the federated learning with the IID data setting follow the similar trends to those well known ) . In addition , we believe that our setting also provides the fair comparison of parameter divergence . 2.Why the parameter divergence metric in Definition 1 is not the same as in the theoretical study ( Appendix B ) ? What is the intuition behind Definition 1 ? ===== ( Answer ) ===== We first remark that PD-Ls in Definition 1 and || ( d_q ) ^ ( t+1 ) _i - ( d_q ) ^ ( t+1 ) _j|| are related . In the case of Figure 1 ( and Appendix B ) , we used the same network architecture and training methods . Manipulated variables here is only data distributions ( i.e. , IID , Non-IID ( 2 ) , and Non-IID ( 1 ) ) . Therefore , || ( d_q ) ^ ( t+1 ) _i - ( d_q ) ^ ( t+1 ) _j|| can be validly utilized . However , in most of our experiments , we compared the different network architectures ( e.g. , NetC-Baseline , NetC-Wider , and NetC-Widest ) or the effects of the different training settings ( e.g. , various weight decay factors ) together in a set . Therefore , for instance , in the case of NetC-Baseline , NetC-Wider , and NetC-Widest , the number of neurons in the output layer becomes different ( i.e. , 2560 , 10240 , and 40960 , respectively ) ; in the case of various weight decay factors , the degree to which the model parameters from the previous iteration are reflected in the current parameters highly depends on the factor values . Therefore , we thought that we need a normalized ( qualitative ) metric rather than simply considering the magnitude of parameter ( weight ) differences ; consequently , instead of the euclidean distance , we used cosine distance-based metrics in Definition 1 . This answer was reflected in the third paragraph of Section 3 in the revised version of the paper ."}, "2": {"review_id": "SJeOAJStwB-2", "review_text": "Summary: The paper presents an empirical study of causes of parameter divergence in federated learning. Federated learning is the setting where parameter updates (e.g. gradients) are computed separately on possibly non-IID subsamples of the data and then aggregated by averaging. The paper examines the effects of choice of optimizer, network width and depth, batch normalization, weight decay, and data augmentation on the amount of parameter divergence. Divergence is defined as the average cosine distance between pairs of locally-updated weights, or between locally updated weights and weights trained with IID data. The paper generally concludes that regularization methods like BN and weight decay have an adverse effect in the federated setting, the deepening the network has an adverse effect while widening it might be beneficial, and that adaptive optimizers like Adam can perform poorly if their internal statistics are not aggregated. I recommend that the paper be rejected. The main shortcoming of the paper is the lack of rigororous statistical analysis to support its conclusions. The paper contains a lot of raw data, but the discussion mainly highlights trends that the authors seem to have observed in the results, without quantifying the relative sizes of effects, how consistent they are across experimental conditions, etc. The writing is also quite unclear, to the point that I often didn't understand exactly what argument was being made. Details / Questions: The main problem is the lack of quantitative analysis of the trends the paper identifies. For example, regarding \"Effects of Batch Normalization\", there seem to be two claims made: 1. Batch normalization makes things worse (somehow) in the federated setting 2. Batch re-normalization still makes things worse, but not as much How are these effects quantified? How large are they? Do they hold across all datasets, architectures, and optimizers considered? Ideally there would be a table summarizing each experimental manipulation, its effect on performance, whether that effect is significant, etc. Of course this requires some care because the paper is doing an exploratory analysis and there are many hypotheses to test; a good reference is [1]. The paper also relies heavily on parameter divergence as a measure of performance in federated learning, but I see no evidence presented that parameter divergence is predictive of test accuracy (which is presumably what we actually care about). Intuitively I can see how it might be related, but since divergence is basically being used as a proxy for accuracy, it is vital to show convincingly that the two are related. What do we gain by analyzing parameter divergence rather than simply comparing test accuracy? Regarding the \"steep fall phenomenon\": The paper seems to present this as an indicator that a manipulation performs poorly in the federated setting. But, isn't it a good thing if parameter divergence goes down? Why does specifically a sudden, sharp decrease in divergence indicate a problem? Finally, some improvements might be made to the experiment setup. For one, the case of completely-disjoint label sets in different local learners seems extreme to me. Wouldn't at least partial overlap be more common in practice? (This is not my area so I don't know). Experimenting with different degrees of overlap would be useful. As for network architectures, it would be valuable to look at a greater variety of standard architecture styles (e.g. ResNet, Inception, etc). I realize there are some experiments with ResNet, but the focus is mainly on the single-path VGG-like architecture. I do realize this is a lot of experiments to do. Minor points: * In the setting described as \"IID\" in Table 1 is not, the subsampled for each learner are not IID subsamples of the full dataset because they are class-balanced (if I'm understanding correctly) References: [1] Dem\u0161ar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7(Jan), 1-30.", "rating": "1: Reject", "reply_text": "We first appreciate the valuable comments . We carefully looked through all the comments ; the following describes our answers . 1.Regarding the lack of quantitative analysis of the trends the paper identifies ===== ( Answer ) ===== We appreciate the valuable comments . As you point out , we admit that the paper lacks a quantitive analysis of the findings . However , please remind that even under \u201c vanilla \u201d training , it is not easy to generally quantify the gains of the considered hyperparameter optimization methods since they highly depend on the training dataset or the remaining training strategies . Therefore , we were afraid to conclude the general quantification of the effects of the methods . Instead , by also providing the results under \u201c vanilla \u201d training and the federated learning with IID data , we tried to emphasize the negative effects of the hyperparametric methods ; we think our results show the severity of performance degradation of each method , even indirectly . 2.Regarding the relationship between test accuracy and parameter divergence ===== ( Answer ) ===== In many previous literatures , e.g. , ( Zhao et al. , 2018 ) , parameter divergence is regarded as a direct response to learners \u2019 local data being non-IID sampled from the population distribution . In relation , it was reported that as the probabilistic distance ( e.g. , earth mover \u2019 s distance ) of learners \u2019 local data becomes farther away from the population distribution , bigger parameter divergence might appear ; this is correlated with the degradation of performance such as test accuracy ( please refer to Section 3.2 of ( Zhao et al. , 2018 ) ) . Also , we added our analysis of the relationship among the three factors ( i.e. , probabilistic distance , parameter divergence , and performance ) in the rebuttal period ; the relevant description can be found in Section 3 of the revised version of this paper . The reason of probing parameter divergence being important is that the federated learning are performed based on iterative parameter averaging . That is , investigating how local updates are diverged can give a clue whether the subsequent parameter averaging yields positive returns ; the proposed divergence metrics provide two ways for it . ( Zhao et al. , 2018 ) Yue Zhao , Meng Li , Liangzhen Lai , Naveen Suda , Damon Civin , and Vikas Chandra . Federated learning with non-IID data . arXiv preprint arXiv : 1806.00582 , 2018 ."}}