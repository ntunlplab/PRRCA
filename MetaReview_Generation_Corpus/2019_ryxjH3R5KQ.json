{"year": "2019", "forum": "ryxjH3R5KQ", "title": "Single Shot Neural Architecture Search Via Direct Sparse Optimization", "decision": "Reject", "meta_review": "This paper proposes Direct Sparse Optimization (DSO)-NAS to obtain neural architectures on specific problems at a reasonable computational cost. Regularization by sparsity is a neat idea, but similar idea has been discussed by many pruning papers. \"model pruning formulation for neural architecture search based on sparse optimization\" is claimed to be the main contribution, but it's debatable if such contribution is strong: worse accuracy, more computation, more #parameters than Mnas (less search time, but also worse search quality). The effect of each proposed technique is appropriately evaluated. However, the reviewers are concerned that the proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy. There's also some concerns about the search space of the proposed method. It is debatable about claim that \"the first NAS algorithm to perform direct search on ImageNet\" and \"the first method to perform direct search without block structure sharing\". Given the acceptance rate of ICLR should be <30%, I would say this paper is good but not outstanding. ", "reviews": [{"review_id": "ryxjH3R5KQ-0", "review_text": "The authors present an architecture search method where connections are removed with sparse regularization. It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet. There are a few grammatical/spelling errors that need ironing out. e.g. \"In specific\" --> \"Specifically\" in the abstract, \"computational budge\" -> \"budget\" (page 6) etc. A few (roughly chronological comments). - Pioneering work is not necessarily equivalent to \"using all the GPUs\" - There are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work! - From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful? - Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS. - How do you specifically encode the number of surviving connections? Is it entirely dependent on budget? - You should add DARTS 1st order to table 1. - Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use? - The ablation study is good, and the results are impressive. I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time. However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified. ------------ UPDATE: Score changed based on author resposne ------------ ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your thoughtful review . We have given serious considerations of your concerns and revise our manuscript to accommodate your suggestions . Please see the details below . Q1 : \u201c There are a few grammatical/spelling errors that need ironing out. \u201d A1 : We have fixed the typos and grammatical errors in the revision . Q2 : \u201c Pioneering work is not necessarily equivalent to `` using all the GPUs '' \u201d A2 : This claim is indeed not accurate we have delete this claim in the revision . Q3 : \u201c There are better words than `` decent '' to describe the performance of DARTS , as it 's very similar to the results in this work ! \u201d A3 : We have changed the word to \u201c impressive \u201d in the revision . However , DSO-NAS indeed outperforms DARTS on ImageNet dataset as illustrated in Table2 . Q4 : \u201c From figure 2 it 's not clear why all non-zero connections in ( b ) are then equally weighted in ( c ) . Would keeping the non-zero weightings be at all helpful ? \u201d A4 : In the search stage , the scaling factors are only used to indicate which operators should be pruned . The value of scaling factors do not represent the importances of kept operators since they can be merged into the weights of convolution . We also add experiments in CIFAR-10 to compare the performance between keeping the non-zero weightings and equal weightings . The result shows that both of them yield similar performances . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Architecture params ( M ) test error -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-share+c/o 3.0 2.84 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-share+c/o+k/w 3.0 2.88 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Q5 : \u201c Why have you chosen the 4 operations at the bottom of page 4 ? \u201d A5 : These four operations were used by ENAS and commonly included in the search space of most NAS papers . Q6 : \u201c How do you specifically encode the number of surviving connections ? \u201d A6 : We don \u2019 t directly encode the number of surviving connections . Instead , the number of surviving connections is determined by the weight for L1 regularization , which can be incorporated with certain budget . Q7 : \u201c Measuring in GPU days is only meaningful if you use the same GPU make for every experiment . Which did you use ? \u201d A7 : All of our experiments were conducted by NVIDIA GTX 1080Ti GPU , which was also used by ENAS and DARTS . We have added it in the paper ."}, {"review_id": "ryxjH3R5KQ-1", "review_text": "Summary: This paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost. The main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph. All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a). The DAG\u2019s edges can be pruned via a sparsity regularization term. The optimization objective of DSO-NAS is thus: Accuracy + L2-regularization(W) + L1-sparsity(\\lambda), where W is the shared weights and \\lambda specifies which edges in the DAG are used. There are 3 phases of optimization: 1. All edges are activated and the shared weights W are trained using normal SGD. Note that this step does not involve \\lambda. 2. \\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018). 3. The best architecture is selected and retrained from scratch. This procedure works for all architectures and objectives. However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts. Their experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet). Strengths: 1. Regularization by sparsity is a neat idea. 2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet. Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive. 3. Incorporating architecture costs into the search objective is nice. However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper. Weaknesses: 1. Some experimental details are missing. I\u2019m going to list them here: - Was the auxiliary tower used during the training of the shared weights W? - Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer? - Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch? - In Section 3.3, it is written that \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d. This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments. 2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison. 3. The paper has some grammatical errors. I obviously missed many, but here are the one I found: - Section 3.3: \u201cDifferent from pruning, which the search space is usually quite limited\u201d. \u201cwhich\u201d should be \u201cwhose\u201d? - Section 4.4.1: \u201cDSO-NAS can also search architecture [...]\u201d -> \u201cDSO-NAS can also search for architectures [...]\u201d References. [1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf [2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable comments . It helps us to prepare the revision . We address all your concerns in the revision as below . Q1 : Was the auxiliary tower used during the training of the shared weights W ? A1 : Auxiliary tower is used only in the retraining stage . Q2 : \u201c Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule ? \u201d A2 : CIFAR : In the pretrain stage and search stage , the learning rate is fixed to 0.1 with batch size 128 ; In the retraining stage , we use cosine learning rate schedule . ImageNet : In the pretrain stage and search stage , the learning rate is fixed to 0.1 with batch 224 ; In the retraining stage , we use linear decay learning rate schedule . Q3 : \u201c Figure 4 does not illustrate M=4 and N=4 , e.g.which operation belongs to which layer ? \u201d A3 : In the revision , we replace the Figure 4 with a new version which has more details . As show in Figure 4 , all the operators in level 4 are pruned . Q4 : \u201c The sparse regularization of \\lambda induces great difficulties in optimization \u201d A4 : The non-smooth regularization introduced by l1 regularization makes traditional stochastic SGD failed to yield sparse results . If we need exact zero , we have to use heuristic thresholding on the \\lambda learned , which has already been demonstrated in SSS [ 1 ] that is inferior . Besides , traditional APG method is not friendly for deep learning as extra forward-backward computation is required , also as shown by SSS . Q5 : \u201c Missed citation : MnasNet also incorporates the cost of architectures in their search process . On ImageNet , your performance is similar to theirs . I think this will be a good comparison. \u201d A5 : We have added the result of MnasNet [ 2 ] in Table 2 . Indeed , MnasNet achieves similar results with us with less FLOPs . However , it is also need to note that MnasNet evaluates more than 8K models , which introduces much higher search cost than our method . Moreover , the design space of MnasNet is significant different from other existing NAS methods including ours . It is interesting to explore the combination of MnasNet with ours in the future work . Q6 : \u201c The paper has some grammatical errors. \u201d A6 : We have fixed the typos and grammatical errors in the revision . Q7 : About \u201c first NAS algorithm to perform direct search on ImageNet \u201d A7 : We check this claim again and find methods like MnasNet [ 2 ] and one-shot architecture search [ 3 ] also have the ability to perform direct search on ImageNet , we have delete this claim in the paper . However , to the best of our knowledge , our method is the first method to perform directly search without block structure sharing . We also report preliminary results that directly search on task beyond classification ( semantic segmentation ) . Please refer to Q1 of Reviewer3 for details . [ 1 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . ECCV 2018 . [ 2 ] MnasNet : Platform-Aware Neural Architecture Search for Mobile . https : //arxiv.org/pdf/1807.11626.pdf [ 3 ] Understanding and simplifying one-shot architecture search . ICML 2018 ."}, {"review_id": "ryxjH3R5KQ-2", "review_text": " - Summary This paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem. Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network. The proposed method is evaluated on CIFAR-10 and ImageNet dataset. - Pros - The proposed method shows competitive or better performance than existing neural architecture search methods. - The experiments are conducted thoroughly in the CIFAR-10 and ImageNet. The selection of the datasets is appropriate. Also, the selection of the methods to be compared is appropriate. - The effect of each proposed technique is appropriately evaluated. - Cons - The search space of the proposed method, such as the number of operations in the convolution block, is limited. - The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy. - The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works. Overall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for pointing out the pros and cons of our method . We address your concerns as follows : Q1 . \u201c The search space of the proposed method , such as the number of operations in the convolution block , is limited. \u201d A1 : First , the size of search space is not determined by the number of operations but the number of connections . The search space of our method is different from exiting NAS methods in that the number of input of certain operation is not limited . Second , the search space without block share is even much larger than existing NAS methods . Third , we can trivially extend our DSO-NAS to accommodate more operations such as dilated conv like our ongoing experiments on PASCAL VOC semantic segmentation task , we extend our search space to accommodate 3x3 and 5x5 separable convolution with dilated = 2 . The following table shows the performance of our model on the PASCAL VOC 2012 semantic segmentation task , where DSO-NAS-cls represents the architecture searched on ImageNet with block structure sharing and DSO-NAS-seg represents the architecture searched on PASCAL VOC segmentation task . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Architecture mIOU Params ( M ) FLOPS ( B ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-cls 72.1 6.5 13.0 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-seg ( more operations ) 72.7 6.7 13.2 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- All above models have been pre-trained on ImageNet classification task first . It \u2019 s notable that the architecture searched on semantic segmentation task with additional operations achieve better performance in our preliminary experiment , indicating that our DSO-NAS is capable to incorporate additional operations . We will present the full experiments of semantic segmentation in the future revision . Q2 : \u201c The technical contribution of the proposed method is not high , because the architecture space of neural network is similar to the prior works. \u201d A2 : Please refer to Q1 . Moreover , we never claim the main contribution of our work lies in augmenting the search space . And in fact , most existing NAS papers share the same architecture search space , the main differences between them is the search strategy . We believe that judging the novelty of a NAS paper solely by its architecture space is unfair ."}], "0": {"review_id": "ryxjH3R5KQ-0", "review_text": "The authors present an architecture search method where connections are removed with sparse regularization. It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet. There are a few grammatical/spelling errors that need ironing out. e.g. \"In specific\" --> \"Specifically\" in the abstract, \"computational budge\" -> \"budget\" (page 6) etc. A few (roughly chronological comments). - Pioneering work is not necessarily equivalent to \"using all the GPUs\" - There are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work! - From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful? - Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS. - How do you specifically encode the number of surviving connections? Is it entirely dependent on budget? - You should add DARTS 1st order to table 1. - Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use? - The ablation study is good, and the results are impressive. I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time. However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified. ------------ UPDATE: Score changed based on author resposne ------------ ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your thoughtful review . We have given serious considerations of your concerns and revise our manuscript to accommodate your suggestions . Please see the details below . Q1 : \u201c There are a few grammatical/spelling errors that need ironing out. \u201d A1 : We have fixed the typos and grammatical errors in the revision . Q2 : \u201c Pioneering work is not necessarily equivalent to `` using all the GPUs '' \u201d A2 : This claim is indeed not accurate we have delete this claim in the revision . Q3 : \u201c There are better words than `` decent '' to describe the performance of DARTS , as it 's very similar to the results in this work ! \u201d A3 : We have changed the word to \u201c impressive \u201d in the revision . However , DSO-NAS indeed outperforms DARTS on ImageNet dataset as illustrated in Table2 . Q4 : \u201c From figure 2 it 's not clear why all non-zero connections in ( b ) are then equally weighted in ( c ) . Would keeping the non-zero weightings be at all helpful ? \u201d A4 : In the search stage , the scaling factors are only used to indicate which operators should be pruned . The value of scaling factors do not represent the importances of kept operators since they can be merged into the weights of convolution . We also add experiments in CIFAR-10 to compare the performance between keeping the non-zero weightings and equal weightings . The result shows that both of them yield similar performances . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Architecture params ( M ) test error -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-share+c/o 3.0 2.84 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-share+c/o+k/w 3.0 2.88 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Q5 : \u201c Why have you chosen the 4 operations at the bottom of page 4 ? \u201d A5 : These four operations were used by ENAS and commonly included in the search space of most NAS papers . Q6 : \u201c How do you specifically encode the number of surviving connections ? \u201d A6 : We don \u2019 t directly encode the number of surviving connections . Instead , the number of surviving connections is determined by the weight for L1 regularization , which can be incorporated with certain budget . Q7 : \u201c Measuring in GPU days is only meaningful if you use the same GPU make for every experiment . Which did you use ? \u201d A7 : All of our experiments were conducted by NVIDIA GTX 1080Ti GPU , which was also used by ENAS and DARTS . We have added it in the paper ."}, "1": {"review_id": "ryxjH3R5KQ-1", "review_text": "Summary: This paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost. The main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph. All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a). The DAG\u2019s edges can be pruned via a sparsity regularization term. The optimization objective of DSO-NAS is thus: Accuracy + L2-regularization(W) + L1-sparsity(\\lambda), where W is the shared weights and \\lambda specifies which edges in the DAG are used. There are 3 phases of optimization: 1. All edges are activated and the shared weights W are trained using normal SGD. Note that this step does not involve \\lambda. 2. \\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018). 3. The best architecture is selected and retrained from scratch. This procedure works for all architectures and objectives. However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts. Their experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet). Strengths: 1. Regularization by sparsity is a neat idea. 2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet. Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive. 3. Incorporating architecture costs into the search objective is nice. However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper. Weaknesses: 1. Some experimental details are missing. I\u2019m going to list them here: - Was the auxiliary tower used during the training of the shared weights W? - Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer? - Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch? - In Section 3.3, it is written that \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d. This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments. 2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison. 3. The paper has some grammatical errors. I obviously missed many, but here are the one I found: - Section 3.3: \u201cDifferent from pruning, which the search space is usually quite limited\u201d. \u201cwhich\u201d should be \u201cwhose\u201d? - Section 4.4.1: \u201cDSO-NAS can also search architecture [...]\u201d -> \u201cDSO-NAS can also search for architectures [...]\u201d References. [1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf [2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your valuable comments . It helps us to prepare the revision . We address all your concerns in the revision as below . Q1 : Was the auxiliary tower used during the training of the shared weights W ? A1 : Auxiliary tower is used only in the retraining stage . Q2 : \u201c Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule ? \u201d A2 : CIFAR : In the pretrain stage and search stage , the learning rate is fixed to 0.1 with batch size 128 ; In the retraining stage , we use cosine learning rate schedule . ImageNet : In the pretrain stage and search stage , the learning rate is fixed to 0.1 with batch 224 ; In the retraining stage , we use linear decay learning rate schedule . Q3 : \u201c Figure 4 does not illustrate M=4 and N=4 , e.g.which operation belongs to which layer ? \u201d A3 : In the revision , we replace the Figure 4 with a new version which has more details . As show in Figure 4 , all the operators in level 4 are pruned . Q4 : \u201c The sparse regularization of \\lambda induces great difficulties in optimization \u201d A4 : The non-smooth regularization introduced by l1 regularization makes traditional stochastic SGD failed to yield sparse results . If we need exact zero , we have to use heuristic thresholding on the \\lambda learned , which has already been demonstrated in SSS [ 1 ] that is inferior . Besides , traditional APG method is not friendly for deep learning as extra forward-backward computation is required , also as shown by SSS . Q5 : \u201c Missed citation : MnasNet also incorporates the cost of architectures in their search process . On ImageNet , your performance is similar to theirs . I think this will be a good comparison. \u201d A5 : We have added the result of MnasNet [ 2 ] in Table 2 . Indeed , MnasNet achieves similar results with us with less FLOPs . However , it is also need to note that MnasNet evaluates more than 8K models , which introduces much higher search cost than our method . Moreover , the design space of MnasNet is significant different from other existing NAS methods including ours . It is interesting to explore the combination of MnasNet with ours in the future work . Q6 : \u201c The paper has some grammatical errors. \u201d A6 : We have fixed the typos and grammatical errors in the revision . Q7 : About \u201c first NAS algorithm to perform direct search on ImageNet \u201d A7 : We check this claim again and find methods like MnasNet [ 2 ] and one-shot architecture search [ 3 ] also have the ability to perform direct search on ImageNet , we have delete this claim in the paper . However , to the best of our knowledge , our method is the first method to perform directly search without block structure sharing . We also report preliminary results that directly search on task beyond classification ( semantic segmentation ) . Please refer to Q1 of Reviewer3 for details . [ 1 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . ECCV 2018 . [ 2 ] MnasNet : Platform-Aware Neural Architecture Search for Mobile . https : //arxiv.org/pdf/1807.11626.pdf [ 3 ] Understanding and simplifying one-shot architecture search . ICML 2018 ."}, "2": {"review_id": "ryxjH3R5KQ-2", "review_text": " - Summary This paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem. Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network. The proposed method is evaluated on CIFAR-10 and ImageNet dataset. - Pros - The proposed method shows competitive or better performance than existing neural architecture search methods. - The experiments are conducted thoroughly in the CIFAR-10 and ImageNet. The selection of the datasets is appropriate. Also, the selection of the methods to be compared is appropriate. - The effect of each proposed technique is appropriately evaluated. - Cons - The search space of the proposed method, such as the number of operations in the convolution block, is limited. - The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy. - The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works. Overall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for pointing out the pros and cons of our method . We address your concerns as follows : Q1 . \u201c The search space of the proposed method , such as the number of operations in the convolution block , is limited. \u201d A1 : First , the size of search space is not determined by the number of operations but the number of connections . The search space of our method is different from exiting NAS methods in that the number of input of certain operation is not limited . Second , the search space without block share is even much larger than existing NAS methods . Third , we can trivially extend our DSO-NAS to accommodate more operations such as dilated conv like our ongoing experiments on PASCAL VOC semantic segmentation task , we extend our search space to accommodate 3x3 and 5x5 separable convolution with dilated = 2 . The following table shows the performance of our model on the PASCAL VOC 2012 semantic segmentation task , where DSO-NAS-cls represents the architecture searched on ImageNet with block structure sharing and DSO-NAS-seg represents the architecture searched on PASCAL VOC segmentation task . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Architecture mIOU Params ( M ) FLOPS ( B ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-cls 72.1 6.5 13.0 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - DSO-NAS-seg ( more operations ) 72.7 6.7 13.2 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- All above models have been pre-trained on ImageNet classification task first . It \u2019 s notable that the architecture searched on semantic segmentation task with additional operations achieve better performance in our preliminary experiment , indicating that our DSO-NAS is capable to incorporate additional operations . We will present the full experiments of semantic segmentation in the future revision . Q2 : \u201c The technical contribution of the proposed method is not high , because the architecture space of neural network is similar to the prior works. \u201d A2 : Please refer to Q1 . Moreover , we never claim the main contribution of our work lies in augmenting the search space . And in fact , most existing NAS papers share the same architecture search space , the main differences between them is the search strategy . We believe that judging the novelty of a NAS paper solely by its architecture space is unfair ."}}