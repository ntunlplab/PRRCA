{"year": "2019", "forum": "BJl6AjC5F7", "title": "Learning to Represent Edits", "decision": "Accept (Poster)", "meta_review": "This paper investigates learning to represent edit operations for two domains: text and source code. The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). The technical novelty is relatively weak.\n\nPros:\nThe paper introduces a new dataset for source code edits.  \n\nCons:\nReviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. As a result, R3 updated their score from 4 to 6. \n\nVerdict:\nPossible weak accept. None of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real-world application of the automatic edits). However, the overall impact and novelty of the paper is relatively weak.", "reviews": [{"review_id": "BJl6AjC5F7-0", "review_text": "The authors state nicely and clearly the main contributions they see in their work (Intro, last paragraph). Specifically the state the paper: 1) present a new and important machine learning task, 2) present a family of models that capture the structure of edits and compute efficient representations, 3) create a new source code edit dataset, 4) perform a set of experiments on the learned edit representations and present promising empirical evidence that the models succeed in capturing the semantics of edits. We decided to organize this review by commenting on the above-stated contributions one at a time: \u201cA new and important machine learning task\u201d Regarding \u201cnew task\u201d: PRO: We are unfamiliar with past work which presents this precise task; the task is new. Section 5 makes a good case for the novelty of this work. CON: None. Regarding \u201cimportant task\u201d: PRO: The authors motivate the task with tantalizing prospective applications-- automatically editing text and code, e.g. for grammar, clarity, and style. Conceptualizing edits as NLP objects of interest that can be concretely represented, clustered, and used for prediction is an advance. CON: Many text editors, office suites, and coding IDEs already include features which automatically suggest or apply edits for grammar, clarity, and style. The authors do not describe shortcomings in existing tools that might be better addressed using distributed representations of edits. Consequently, the significance of the proposed contribution is unclear. \u201cA family of models that capture the structure of edits and compute efficient representations\u201d Regarding \u201ca family of models\u201d: PRO: The family of models presented by the authors clearly generalizes: such models may be utilized for computational experiments on datasets and edit types beyond those specifically utilized in this evaluation. The authors apply well-utilized neural network architectures that may be trained and applied to large datasets. The architecture of the neural editor permits evaluation of the degree to which the editor successfully predicts the correct edit given a pre-edit input and a known representation of a similar edit. CON: The authors do not propose any scheme under which edit representations might be utilized for automatically editing text or code when an edit very similar to the desired edit is not already known and its representation available as input. Hence, we find the authors do not sufficiently motivate the input scheme of their neural editor. The input scheme of the neural editor makes trivial the case in which no edit is needed, as the editor would learn during training that the output x+ should be the same as the input x- when the representation of the \u201czero edit\u201d is given as input. While the authors discuss the importance of \u201cbottlenecking\u201d the edit encoder so that it does not simply learn to encode the desired output x+, they do not concretely demonstrate that the edit encoder has done otherwise in the final experiments. Related to that: If the authors aimed to actually solve automated edits in text/code then it seems crucial their data contained \"negative examples\" i.e. segments which require no edits. In such an evaluation one would test also when the algorithm introduces unnecessary/erroneous edits. Regarding \u201ccapture structure of edits\u201d: PRO: The authors present evidence that edit encoders tightly cluster relatively simple edits which involve adding or removing common tokens. The authors present evidence that relatively simple edits completed automatically by a \u201cfixer\u201d often cluster together, i.e. a known signal is retained in clustering. The authors present evidence that the nearest neighbors of edits in an edit-representation space often are semantically or structurally similar, as judged by human annotators. Section 4.3 includes interesting observations comparing edit patterns better captured by the graph or seq edit encoders. CON: The details of the human annotation tasks which generated the numerical results in Tables 1 and 2 are unclear: were unbiased third parties utilized? Were the edits stripped of their source-encoder label when evaluated? Objectively, what separates an \u201cunrelated\u201d from a \u201csimilar\u201d edit, and what separates a \u201csimilar\u201d from a \u201csame\u201d edit? Did multiple human annotators undertake this task in parallel, and what was their overall concordance (e.g. \u201cintercoder reliability\u201d)? Without concrete answers to these questions, the validity and significance of the DCG/NDCG results reported in Tables 1 and 2 are unclear. It is not clear from the two examples given in Table 1 that the three nearest neighbors embedded by the Seq encoder are \u201cbetter\u201d, i.e. overall more semantically and/or syntactically similar to the example edit, than those embedded by the Bag of Words model. It is unclear which specific aspects of \u201cedit structure\u201d are better captured by the Seq encoder than the Bag of Words model. The overall structure of Tables 1 and 2 is awkward, with concrete numerical results dominated by a spatially large section containing a small number of examples. \u201ccreate a new source code edit dataset\u201d PRO: The authors create a new source code edit dataset, an important contribution to the study of this new task. CON: Minor: is the provided dataset large enough to do more than simple experiments? See note below on sample size. \u201cpresent promising empirical evidence that the models succeed in capturing the semantics of edits\u201d PRO: The experiment results show how frequently the end-to-end system successfully predicted the correct edit given a pre-edit input and a known representation of a similar edit. Gold standard accuracies of more than 70%, and averaged transfer learning accuracies of more than 30%, suggest that this system shows promise for capturing the semantics of edits. CON: Due to concerns expressed above about the model design and evaluation of the edit representations, it remains unclear to what degree the models succeed in capturing the semantics of edits. Table 11 shows dramatic variation in success levels across fixer ID in the transfer learning task, yet the authors do not propose ways their end-to-end system might be adjusted to address areas of weak performance. The authors do not discuss the impact of training set size on their evaluation metrics. The authors do not discuss the degree to which their model training task would scale to larger language datasets such as those needed for the motivating applications. ############## Based on the authors' response, revisions, and disucssions we have updated the review and the score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the careful reading of the paper ( including the lengthy appendices ! ) , and elucidating concerns about validity of the task and method . We believe that several of these were due to a lack of clarity in our exposition , that can be resolved . We have attempted to clarify these below and will revise the paper to make things more clear before the end of the review period . * Regarding `` important task '' Response : existing editing systems , like the the grammar checker in MS Word and code refactoring module in IDEs , often use heavily engineered , domain-specific , manually crafted rules to perform editing . Our proposed learning-based model is a data-driven approach that automatically * * learns * * to extract , represent and apply edits from large-scale edit data , and it is also a * * generic * * system that could be applied to heterogeneous domains like text and source code . Additionally , using distributed representations also facilitates visualization ( Figure 2 ) and clustering ( Appendix B ) of semantically similar edits . These novel applications open possibilities to develop human-assistance toolkits for discovering and extracting emerging edit patterns ( e.g. , new bug fixes from GitHub commits ) for rule-based systems from large-scale edit data . For example , this could be used to drive the development of new rules for existing edit tools , by identifying common patterns not covered by existing capabilities . We apologize and will make this clearer . * Regarding `` a family of models '' Response : We agree that our current system is not able to identify places where an edit should be performed , and that this is important future work . In this work , we have focused on ( 1 ) computing representations of edits that allow us to group similar changes , and ( 2 ) applying such representations in a new context . Both of these scenarios are already useful in human-in-the-loop scenarios . For example , a good solution to problem ( 1 ) can inform the development of new edit and refactoring tools ( by observing common changes ) , whereas ( 2 ) can be used to propose changes that can be accepted/rejected by a human . We will make this aspect of future work clearer in the next version of our paper . * Regarding \u201c capture structure of edits \u201d and Human Evaluation Response : please refer to our response regarding annotation . * Regarding \u201c present promising empirical evidence that the models succeed in capturing the semantics of edits \u201d and Results in Table 11 Response : we thank the reviewer for his effort in analyzing the many statistics we present in Table 11 ! We remark that this task is a transfer learning task is indeed non-trivial . For instance , some fixer categories cover many different types of edits ( e.g. , RCS1077 ( https : //github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1077.md handles 12 differents ways of optimizing LINQ expressions ) . In these cases , edits are semantically related ( \u201c improving a LINQ expression \u201d ) , but this relationship only exists at a high level and is not directly reflected to the syntactic transformations required by the fixer . Other categories contain complex refactoring rules that require reasoning about a chain of expressions ( e.g. , RCS1197 ( https : //github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1197.md turns sb.Append ( s1 + s2 + \u2026 + sN ) into sb.Append ( s1 ) .Append ( s2 ) . [ ... ] Append ( sN ) ) , which our current models are unable to reason about . We believe that further advances in ( general ) learning from source code are required to correctly handle theses cases . We will expand Appendix C with a more fine-grained analysis of the results in Table 11 , providing more background on categories whose results deviate substantially from the average . [ Impact of training set size and scalability ] : Thanks for the comments ! We will discuss this in our final version ."}, {"review_id": "BJl6AjC5F7-1", "review_text": "This paper looks at learning to represent edits for text revisions and code changes. The main contributions are as follows: * They define a new task of representing and predicting textual and code changes * They make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change * They try simple neural network models that show good performance in representing and predicting the changes The NLP community has recently defined the problem of predicting atomic edits for text data (Faraqui, et al. EMNLP 2018, cited in the paper), and that is the source of their Wikipedia revision dataset. Although it is an interesting problem, it is not immediately clear from the Introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and I hope the next version would elaborate on the motivation and significance for this new task. The \"Fixer\" dataset that they created is interesting. Those edits supposedly make the code better, so modeling those edits could lead to \"better\" code. Having that as labeled data enables a clean and convincing evaluation task of predicting similar edits. The paper focuses on the novelty of the task and the dataset, so the models are simple variations of the existing bidirectional LSTM and the gated graph neural network. Because much of the input text (or code) does not change, the decoder gets to directly copy parts of the input. For code data, the AST is used instead of flat text of the code. These small changes seem reasonable and work well for this problem. Evaluation is not easy for this task. For the task of representing the edits, they show visualizations of the clusters of similar edits and conduct a human evaluation to see how similar these edits actually are. This human evaluation is not described in detail, as they do not say how many people rated the similarity, who they were (how they were recruited), how they were instructed, and what the inter-rater agreement was. The edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better. That may be true, but then without another metric for better generalization, one cannot say that better performance means worse generalization. Despite these minor issues, the paper contributes significantly novel task, dataset, and results. I believe it will lead to interesting future research in representing text and code changes.", "rating": "7: Good paper, accept", "reply_text": "Question : \u201c what would be enabled by accurate prediction of atomic edits \u2026 elaborate on the motivation and significance for this new task \u201d Response : Our work focuses on developing a generic approach to represent and apply edits . On the WikiAtomicEdits data , one interesting application of our model would be facilitating the development of data exploration toolkits that cluster and visualizes semantically and syntactically similar edits ( e.g. , the example clusters shown in Table 9 ) . Since our proposed approach is relatively general , we believe we could explore more interesting applications given access to parallel data of other forms of natural language edits . For example , our model could be used to represent and apply syntactic transfer given parallel corpora of sentences with different syntactic structures . On the source code domain , our work enjoys more intriguing and immediate applications like learning to represent and apply code fixes from commit data , similar to the one-shot learning task we present in Section 4.4 . Our work could also enable human-in-loop machine learning applications like clustering commit streams on GitHub at large-scale and helping users identify emerging \u201c best practices \u201d or bug fixes . Indeed , the initial motivation for our research was to automatically identify common improvements to source code that are not covered by existing tools . Question : `` human evaluation is not described in detail ... '' Response : please refer to our general response regarding data annotation . Question : \u201c what it means when they say better prediction performance does not necessarily mean it generalizes better ... \u201d Response : This observation is grounded in the comparison of the results displayed in Tables 4 and 5 in our end-to-end experiment on GitHubEdits data ( Section 4.4 ) . Table 4 indicates that given the encoding of an edit ( x- , x+ ) , the Seq2Seq editor is most precise in generating x+ from x- , ( slightly ) outperforming the Graph2Tree editor . We evaluate the generality of edit representations in our \u201c one-shot \u201d experiment , where we use the encoding of a related edit ( x- , x+ ) to reconstruct x+ \u2019 from x- \u2019 . There , the Graph2Tree editor performs significantly better than the Seq2Seq editor . The latter experiment serves as a good proxy in evaluating the generalization ability of different system configurations , from whose result we derive the hypothesis that better performance with gold-standard edit encodings might not imply better performance with noisy edit encodings . We apologize for the confusion and will update the text of the paper to clarify what we mean by generalizable and how we draw that conclusion from our experiments ."}, {"review_id": "BJl6AjC5F7-2", "review_text": "The main contributions of the paper are an edit encoder model similar to (Guu et al. 2017 http://aclweb.org/anthology/Q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. The paper is clearly written, and provides clear support for each of their main claims. I think this would be of interest to NLP researchers and others working on sequence- and graph-transduction models, but I think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. This would also benefit greatly from a more direct comparison to Guu et al. 2017, which presents a very similar \"neural editor\" model. Some more specific points: - I really like the idea of transferring edits from one context to another. The one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are. - If I'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. I wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. Have you explored more constrained versions of the edit encoder (such as the bag-of-edits from Guu et al. 2017) or alternate learning objectives to control for this? - The WikiAtomicEdits corpus has 13.7 million English insertions - why did you subsample this to only 1M? There is also a human-annotated subset of that you might use as evaluation data, similar to the C#Fixers set. - On the human evaluation: Who were the annotators? The categories \"similar edit\", and \"semantically or syntactically same edit\" seem to leave a lot to interpretation; were more specific instructions given? It also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits. - On the automatic evaluation: accuracy seems brittle for evaluating sequence output. Did you consider reporting BLEU, ROUGE, or another \"soft\" sequence metric? - It would be worth citing existing literature on classification of Wikipedia edits, for example Yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). An interesting experiment would be to correlate your edit encodings with their taxonomy.", "rating": "6: Marginally above acceptance threshold", "reply_text": "* robustness of edit encodings * : Thanks for the comment ! Directly measuring the robustness of edit encodings is non-trivial , but our one-shot learning experiments ( Sec.4.4 ) serve as a good proxy by testing the editing accuracy using the edit encoding from a similar example . * applicability to other tasks * : Our proposed method is general and could be applied to other structured transduction tasks . We perform experiment on natural language edits ( sequential ) and source code commit data ( tree-structured ) , since these are two commonly occurring sources of edits . We leave applying our model to other data sources as interesting future work . * comparison with Guu et al. , 2017 * : Thanks for pointing out the related work by Guu et al ! As discussed in Section 5 , we remark that our motivation and research issues are very different , and these two models are not directly comparable -- - Guu et al.focus on learning a generative language model by marginalizing over latent edits , while our work focuses on discriminative learning of ( 1 ) representing edits given the original ( x- ) and edited ( x+ ) data , and ( 2 ) applying the learned edit to new input data . We therefore directly evaluate the quality of neighboring edit representations via human annotation , and the end-to-end performance of applying edits to both parallel data and in a novel one-shot learning scenario , which are not covered in Guu et al.Nevertheless , our model architecture shares a similar spirit with Guu et al.For example , the model in Guu et al.also has an edit encoder based on \u201c Bag-of-Edits \u201d ( i.e. , the posterior distribution $ q ( z|x- , x+ ) $ ) and a seq2seq generation ( reconstruction ) model of x+ given x- and the edit representation z . In some sense , our seq2seq editor with a \u201c Bag-of-Edits \u201d edit encoder would be similar as the \u201c discriminative \u201d version of Guu et al.We will make the difference between this research and Guu et al clearer in an updated version of the paper . Please also refer to below for our response to the \u201c Bag-of-Edits \u201d edit encoder . Response to your specific questions : * lower-bounding transfer learning results * : Thanks for the comments ! Having a lower-bound is helpful in understanding the relative advantage of our proposed method , however it is not clear what a reasonable lower-bounding baseline would be . One baseline would be an editor model ( e.g. , Graph2Tree with sequential edit encoder ) that doesn \u2019 t use edit encodings . * constrained versions of the edit encoder * : First , we remark that our Bag-of-Word edit encoder ( Table 1 and 2 ) is similar to a \u201c Bag-of-Edits \u201d model , where the representation of an edit is modeled by a vector of added/deleted tokens ( we use different vocabularies for added and deleted words ) . Our neural edit encoders have access to the full sequences x- and x+ . We also tried a distributional bag-of-edits model like the one used in Guu et al. , using an LSTM to summarize only changed tokens . This model had worse performance in our end-to-end experiment ( Table 4 ) and we therefore we did not include the results . Through error analysis we found that many edits are * * context and positional sensitive * * , and encoding context ( i.e. , full sequences ) is important . For instance , the WikiAtomicEdits examples we present in Table 9 clearly indicate that semantically similar insertions also share similar editing positions , which can not be captured by the bag-of-edits encoder as in Guu et al.This might be more obvious for structured data source like code edits ( c.f. , Table 10 ) . For instance , in the first example in Table 10 , ` Equal ( ) ` can be changed to ` Empty ( ) ` * * only * * in the ` Assert ` namespace ( i.e. , the context ) . We apologize for the confusion and will include more results and analysis in the final version , facilitating more direct comparison with the editor encoder in Guu et al.Nevertheless , we remark that as discussed above , our work is not directly comparable with Guu et al . * subsampling WikiAtomicEdits * : At the time of submission the WikiAtomicEdits dataset could not be downloaded in full , due to an error with the zip file provided . We managed to extract the first 1M edits from the dataset . We believe that the full corpus would not present significantly different statistical properties from the 1M samples we used . * human evaluation * : please refer to our response regarding annotation . The idea of separating syntactically and semantically similar edits is also very interesting , which we will explore in our final version . * soft metric * : Thanks for the comment ! We can definitely do BLEU evaluation on WikiAtomEdits . For source code data , a sensible \u201c soft \u201d metric on source code still remains an open research issue ( Yin and Neubig , 2017 ) . We will include more discussion in our final version . * classifying Wikipedia edits * : This is a very great idea , thanks for suggesting this . Given the time constraints , we will examine the feasibility of doing something like this for the final version of the paper ."}], "0": {"review_id": "BJl6AjC5F7-0", "review_text": "The authors state nicely and clearly the main contributions they see in their work (Intro, last paragraph). Specifically the state the paper: 1) present a new and important machine learning task, 2) present a family of models that capture the structure of edits and compute efficient representations, 3) create a new source code edit dataset, 4) perform a set of experiments on the learned edit representations and present promising empirical evidence that the models succeed in capturing the semantics of edits. We decided to organize this review by commenting on the above-stated contributions one at a time: \u201cA new and important machine learning task\u201d Regarding \u201cnew task\u201d: PRO: We are unfamiliar with past work which presents this precise task; the task is new. Section 5 makes a good case for the novelty of this work. CON: None. Regarding \u201cimportant task\u201d: PRO: The authors motivate the task with tantalizing prospective applications-- automatically editing text and code, e.g. for grammar, clarity, and style. Conceptualizing edits as NLP objects of interest that can be concretely represented, clustered, and used for prediction is an advance. CON: Many text editors, office suites, and coding IDEs already include features which automatically suggest or apply edits for grammar, clarity, and style. The authors do not describe shortcomings in existing tools that might be better addressed using distributed representations of edits. Consequently, the significance of the proposed contribution is unclear. \u201cA family of models that capture the structure of edits and compute efficient representations\u201d Regarding \u201ca family of models\u201d: PRO: The family of models presented by the authors clearly generalizes: such models may be utilized for computational experiments on datasets and edit types beyond those specifically utilized in this evaluation. The authors apply well-utilized neural network architectures that may be trained and applied to large datasets. The architecture of the neural editor permits evaluation of the degree to which the editor successfully predicts the correct edit given a pre-edit input and a known representation of a similar edit. CON: The authors do not propose any scheme under which edit representations might be utilized for automatically editing text or code when an edit very similar to the desired edit is not already known and its representation available as input. Hence, we find the authors do not sufficiently motivate the input scheme of their neural editor. The input scheme of the neural editor makes trivial the case in which no edit is needed, as the editor would learn during training that the output x+ should be the same as the input x- when the representation of the \u201czero edit\u201d is given as input. While the authors discuss the importance of \u201cbottlenecking\u201d the edit encoder so that it does not simply learn to encode the desired output x+, they do not concretely demonstrate that the edit encoder has done otherwise in the final experiments. Related to that: If the authors aimed to actually solve automated edits in text/code then it seems crucial their data contained \"negative examples\" i.e. segments which require no edits. In such an evaluation one would test also when the algorithm introduces unnecessary/erroneous edits. Regarding \u201ccapture structure of edits\u201d: PRO: The authors present evidence that edit encoders tightly cluster relatively simple edits which involve adding or removing common tokens. The authors present evidence that relatively simple edits completed automatically by a \u201cfixer\u201d often cluster together, i.e. a known signal is retained in clustering. The authors present evidence that the nearest neighbors of edits in an edit-representation space often are semantically or structurally similar, as judged by human annotators. Section 4.3 includes interesting observations comparing edit patterns better captured by the graph or seq edit encoders. CON: The details of the human annotation tasks which generated the numerical results in Tables 1 and 2 are unclear: were unbiased third parties utilized? Were the edits stripped of their source-encoder label when evaluated? Objectively, what separates an \u201cunrelated\u201d from a \u201csimilar\u201d edit, and what separates a \u201csimilar\u201d from a \u201csame\u201d edit? Did multiple human annotators undertake this task in parallel, and what was their overall concordance (e.g. \u201cintercoder reliability\u201d)? Without concrete answers to these questions, the validity and significance of the DCG/NDCG results reported in Tables 1 and 2 are unclear. It is not clear from the two examples given in Table 1 that the three nearest neighbors embedded by the Seq encoder are \u201cbetter\u201d, i.e. overall more semantically and/or syntactically similar to the example edit, than those embedded by the Bag of Words model. It is unclear which specific aspects of \u201cedit structure\u201d are better captured by the Seq encoder than the Bag of Words model. The overall structure of Tables 1 and 2 is awkward, with concrete numerical results dominated by a spatially large section containing a small number of examples. \u201ccreate a new source code edit dataset\u201d PRO: The authors create a new source code edit dataset, an important contribution to the study of this new task. CON: Minor: is the provided dataset large enough to do more than simple experiments? See note below on sample size. \u201cpresent promising empirical evidence that the models succeed in capturing the semantics of edits\u201d PRO: The experiment results show how frequently the end-to-end system successfully predicted the correct edit given a pre-edit input and a known representation of a similar edit. Gold standard accuracies of more than 70%, and averaged transfer learning accuracies of more than 30%, suggest that this system shows promise for capturing the semantics of edits. CON: Due to concerns expressed above about the model design and evaluation of the edit representations, it remains unclear to what degree the models succeed in capturing the semantics of edits. Table 11 shows dramatic variation in success levels across fixer ID in the transfer learning task, yet the authors do not propose ways their end-to-end system might be adjusted to address areas of weak performance. The authors do not discuss the impact of training set size on their evaluation metrics. The authors do not discuss the degree to which their model training task would scale to larger language datasets such as those needed for the motivating applications. ############## Based on the authors' response, revisions, and disucssions we have updated the review and the score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the careful reading of the paper ( including the lengthy appendices ! ) , and elucidating concerns about validity of the task and method . We believe that several of these were due to a lack of clarity in our exposition , that can be resolved . We have attempted to clarify these below and will revise the paper to make things more clear before the end of the review period . * Regarding `` important task '' Response : existing editing systems , like the the grammar checker in MS Word and code refactoring module in IDEs , often use heavily engineered , domain-specific , manually crafted rules to perform editing . Our proposed learning-based model is a data-driven approach that automatically * * learns * * to extract , represent and apply edits from large-scale edit data , and it is also a * * generic * * system that could be applied to heterogeneous domains like text and source code . Additionally , using distributed representations also facilitates visualization ( Figure 2 ) and clustering ( Appendix B ) of semantically similar edits . These novel applications open possibilities to develop human-assistance toolkits for discovering and extracting emerging edit patterns ( e.g. , new bug fixes from GitHub commits ) for rule-based systems from large-scale edit data . For example , this could be used to drive the development of new rules for existing edit tools , by identifying common patterns not covered by existing capabilities . We apologize and will make this clearer . * Regarding `` a family of models '' Response : We agree that our current system is not able to identify places where an edit should be performed , and that this is important future work . In this work , we have focused on ( 1 ) computing representations of edits that allow us to group similar changes , and ( 2 ) applying such representations in a new context . Both of these scenarios are already useful in human-in-the-loop scenarios . For example , a good solution to problem ( 1 ) can inform the development of new edit and refactoring tools ( by observing common changes ) , whereas ( 2 ) can be used to propose changes that can be accepted/rejected by a human . We will make this aspect of future work clearer in the next version of our paper . * Regarding \u201c capture structure of edits \u201d and Human Evaluation Response : please refer to our response regarding annotation . * Regarding \u201c present promising empirical evidence that the models succeed in capturing the semantics of edits \u201d and Results in Table 11 Response : we thank the reviewer for his effort in analyzing the many statistics we present in Table 11 ! We remark that this task is a transfer learning task is indeed non-trivial . For instance , some fixer categories cover many different types of edits ( e.g. , RCS1077 ( https : //github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1077.md handles 12 differents ways of optimizing LINQ expressions ) . In these cases , edits are semantically related ( \u201c improving a LINQ expression \u201d ) , but this relationship only exists at a high level and is not directly reflected to the syntactic transformations required by the fixer . Other categories contain complex refactoring rules that require reasoning about a chain of expressions ( e.g. , RCS1197 ( https : //github.com/JosefPihrt/Roslynator/blob/master/docs/analyzers/RCS1197.md turns sb.Append ( s1 + s2 + \u2026 + sN ) into sb.Append ( s1 ) .Append ( s2 ) . [ ... ] Append ( sN ) ) , which our current models are unable to reason about . We believe that further advances in ( general ) learning from source code are required to correctly handle theses cases . We will expand Appendix C with a more fine-grained analysis of the results in Table 11 , providing more background on categories whose results deviate substantially from the average . [ Impact of training set size and scalability ] : Thanks for the comments ! We will discuss this in our final version ."}, "1": {"review_id": "BJl6AjC5F7-1", "review_text": "This paper looks at learning to represent edits for text revisions and code changes. The main contributions are as follows: * They define a new task of representing and predicting textual and code changes * They make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change * They try simple neural network models that show good performance in representing and predicting the changes The NLP community has recently defined the problem of predicting atomic edits for text data (Faraqui, et al. EMNLP 2018, cited in the paper), and that is the source of their Wikipedia revision dataset. Although it is an interesting problem, it is not immediately clear from the Introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and I hope the next version would elaborate on the motivation and significance for this new task. The \"Fixer\" dataset that they created is interesting. Those edits supposedly make the code better, so modeling those edits could lead to \"better\" code. Having that as labeled data enables a clean and convincing evaluation task of predicting similar edits. The paper focuses on the novelty of the task and the dataset, so the models are simple variations of the existing bidirectional LSTM and the gated graph neural network. Because much of the input text (or code) does not change, the decoder gets to directly copy parts of the input. For code data, the AST is used instead of flat text of the code. These small changes seem reasonable and work well for this problem. Evaluation is not easy for this task. For the task of representing the edits, they show visualizations of the clusters of similar edits and conduct a human evaluation to see how similar these edits actually are. This human evaluation is not described in detail, as they do not say how many people rated the similarity, who they were (how they were recruited), how they were instructed, and what the inter-rater agreement was. The edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better. That may be true, but then without another metric for better generalization, one cannot say that better performance means worse generalization. Despite these minor issues, the paper contributes significantly novel task, dataset, and results. I believe it will lead to interesting future research in representing text and code changes.", "rating": "7: Good paper, accept", "reply_text": "Question : \u201c what would be enabled by accurate prediction of atomic edits \u2026 elaborate on the motivation and significance for this new task \u201d Response : Our work focuses on developing a generic approach to represent and apply edits . On the WikiAtomicEdits data , one interesting application of our model would be facilitating the development of data exploration toolkits that cluster and visualizes semantically and syntactically similar edits ( e.g. , the example clusters shown in Table 9 ) . Since our proposed approach is relatively general , we believe we could explore more interesting applications given access to parallel data of other forms of natural language edits . For example , our model could be used to represent and apply syntactic transfer given parallel corpora of sentences with different syntactic structures . On the source code domain , our work enjoys more intriguing and immediate applications like learning to represent and apply code fixes from commit data , similar to the one-shot learning task we present in Section 4.4 . Our work could also enable human-in-loop machine learning applications like clustering commit streams on GitHub at large-scale and helping users identify emerging \u201c best practices \u201d or bug fixes . Indeed , the initial motivation for our research was to automatically identify common improvements to source code that are not covered by existing tools . Question : `` human evaluation is not described in detail ... '' Response : please refer to our general response regarding data annotation . Question : \u201c what it means when they say better prediction performance does not necessarily mean it generalizes better ... \u201d Response : This observation is grounded in the comparison of the results displayed in Tables 4 and 5 in our end-to-end experiment on GitHubEdits data ( Section 4.4 ) . Table 4 indicates that given the encoding of an edit ( x- , x+ ) , the Seq2Seq editor is most precise in generating x+ from x- , ( slightly ) outperforming the Graph2Tree editor . We evaluate the generality of edit representations in our \u201c one-shot \u201d experiment , where we use the encoding of a related edit ( x- , x+ ) to reconstruct x+ \u2019 from x- \u2019 . There , the Graph2Tree editor performs significantly better than the Seq2Seq editor . The latter experiment serves as a good proxy in evaluating the generalization ability of different system configurations , from whose result we derive the hypothesis that better performance with gold-standard edit encodings might not imply better performance with noisy edit encodings . We apologize for the confusion and will update the text of the paper to clarify what we mean by generalizable and how we draw that conclusion from our experiments ."}, "2": {"review_id": "BJl6AjC5F7-2", "review_text": "The main contributions of the paper are an edit encoder model similar to (Guu et al. 2017 http://aclweb.org/anthology/Q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. The paper is clearly written, and provides clear support for each of their main claims. I think this would be of interest to NLP researchers and others working on sequence- and graph-transduction models, but I think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. This would also benefit greatly from a more direct comparison to Guu et al. 2017, which presents a very similar \"neural editor\" model. Some more specific points: - I really like the idea of transferring edits from one context to another. The one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are. - If I'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. I wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. Have you explored more constrained versions of the edit encoder (such as the bag-of-edits from Guu et al. 2017) or alternate learning objectives to control for this? - The WikiAtomicEdits corpus has 13.7 million English insertions - why did you subsample this to only 1M? There is also a human-annotated subset of that you might use as evaluation data, similar to the C#Fixers set. - On the human evaluation: Who were the annotators? The categories \"similar edit\", and \"semantically or syntactically same edit\" seem to leave a lot to interpretation; were more specific instructions given? It also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits. - On the automatic evaluation: accuracy seems brittle for evaluating sequence output. Did you consider reporting BLEU, ROUGE, or another \"soft\" sequence metric? - It would be worth citing existing literature on classification of Wikipedia edits, for example Yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). An interesting experiment would be to correlate your edit encodings with their taxonomy.", "rating": "6: Marginally above acceptance threshold", "reply_text": "* robustness of edit encodings * : Thanks for the comment ! Directly measuring the robustness of edit encodings is non-trivial , but our one-shot learning experiments ( Sec.4.4 ) serve as a good proxy by testing the editing accuracy using the edit encoding from a similar example . * applicability to other tasks * : Our proposed method is general and could be applied to other structured transduction tasks . We perform experiment on natural language edits ( sequential ) and source code commit data ( tree-structured ) , since these are two commonly occurring sources of edits . We leave applying our model to other data sources as interesting future work . * comparison with Guu et al. , 2017 * : Thanks for pointing out the related work by Guu et al ! As discussed in Section 5 , we remark that our motivation and research issues are very different , and these two models are not directly comparable -- - Guu et al.focus on learning a generative language model by marginalizing over latent edits , while our work focuses on discriminative learning of ( 1 ) representing edits given the original ( x- ) and edited ( x+ ) data , and ( 2 ) applying the learned edit to new input data . We therefore directly evaluate the quality of neighboring edit representations via human annotation , and the end-to-end performance of applying edits to both parallel data and in a novel one-shot learning scenario , which are not covered in Guu et al.Nevertheless , our model architecture shares a similar spirit with Guu et al.For example , the model in Guu et al.also has an edit encoder based on \u201c Bag-of-Edits \u201d ( i.e. , the posterior distribution $ q ( z|x- , x+ ) $ ) and a seq2seq generation ( reconstruction ) model of x+ given x- and the edit representation z . In some sense , our seq2seq editor with a \u201c Bag-of-Edits \u201d edit encoder would be similar as the \u201c discriminative \u201d version of Guu et al.We will make the difference between this research and Guu et al clearer in an updated version of the paper . Please also refer to below for our response to the \u201c Bag-of-Edits \u201d edit encoder . Response to your specific questions : * lower-bounding transfer learning results * : Thanks for the comments ! Having a lower-bound is helpful in understanding the relative advantage of our proposed method , however it is not clear what a reasonable lower-bounding baseline would be . One baseline would be an editor model ( e.g. , Graph2Tree with sequential edit encoder ) that doesn \u2019 t use edit encodings . * constrained versions of the edit encoder * : First , we remark that our Bag-of-Word edit encoder ( Table 1 and 2 ) is similar to a \u201c Bag-of-Edits \u201d model , where the representation of an edit is modeled by a vector of added/deleted tokens ( we use different vocabularies for added and deleted words ) . Our neural edit encoders have access to the full sequences x- and x+ . We also tried a distributional bag-of-edits model like the one used in Guu et al. , using an LSTM to summarize only changed tokens . This model had worse performance in our end-to-end experiment ( Table 4 ) and we therefore we did not include the results . Through error analysis we found that many edits are * * context and positional sensitive * * , and encoding context ( i.e. , full sequences ) is important . For instance , the WikiAtomicEdits examples we present in Table 9 clearly indicate that semantically similar insertions also share similar editing positions , which can not be captured by the bag-of-edits encoder as in Guu et al.This might be more obvious for structured data source like code edits ( c.f. , Table 10 ) . For instance , in the first example in Table 10 , ` Equal ( ) ` can be changed to ` Empty ( ) ` * * only * * in the ` Assert ` namespace ( i.e. , the context ) . We apologize for the confusion and will include more results and analysis in the final version , facilitating more direct comparison with the editor encoder in Guu et al.Nevertheless , we remark that as discussed above , our work is not directly comparable with Guu et al . * subsampling WikiAtomicEdits * : At the time of submission the WikiAtomicEdits dataset could not be downloaded in full , due to an error with the zip file provided . We managed to extract the first 1M edits from the dataset . We believe that the full corpus would not present significantly different statistical properties from the 1M samples we used . * human evaluation * : please refer to our response regarding annotation . The idea of separating syntactically and semantically similar edits is also very interesting , which we will explore in our final version . * soft metric * : Thanks for the comment ! We can definitely do BLEU evaluation on WikiAtomEdits . For source code data , a sensible \u201c soft \u201d metric on source code still remains an open research issue ( Yin and Neubig , 2017 ) . We will include more discussion in our final version . * classifying Wikipedia edits * : This is a very great idea , thanks for suggesting this . Given the time constraints , we will examine the feasibility of doing something like this for the final version of the paper ."}}