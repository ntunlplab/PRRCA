{"year": "2021", "forum": "gV3wdEOGy_V", "title": "MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering", "decision": "Accept (Poster)", "meta_review": "Thanks for your submission to ICLR!\n\nThis paper considers a novel unsupervised image clustering framework based on a mixture of contrastive experts framework.  Most of the reviewers were overall positive about the paper.  On the positive side, they noted that the paper had an interesting idea, was well motivated, written well, and had solid results.  Also, the authors provided detailed and useful responses to the reviews, which further strengthened the case for accepting the paper.  On the negative side, one reviewer felt that the paper seemed a bit preliminary and its presentation could improve.  Also, there was some concern about missing comparisons / discussion to previous work (including from a public comment) or data sets (e.g. ImageNet-10).  Again, the authors responded well to these concerns.\n\nGiven that the overall response was quite positive with the paper, I'm happy to recommend accepting it.\n\n", "reviews": [{"review_id": "gV3wdEOGy_V-0", "review_text": "The paper presents an image clustering methodology based on Mixture of Experts ( MoE ) for image clustering . Although MoE has been proposed for supervised learning problems , the authors exploit the instance discrimination framwork to apply the MoE idea for image clustering . This is a novel aspect of the proposed method . The MoCo framework ( unsupervised ) for contrastive learning of image representations is employed to define a mixture of MoCo experts model where each expert additionally includes a cluster prototype vector to facilitate clustering . This unified approach for simultaneous MoCo-based representation learning and clustering seems to provide better results that the two-stage approach of first applying MoCo and then using k-means clustering on the obtained representations . A probabilistic formulation of the method is presented , along with a training approach based on EM algorithm for likelihood maximization . There are several concerns related to presentation and clarity . Comments to be addressed : 1 ) It would be easier to understand the contribution of the paper , if the MoCo approach were initially described and then the proposed method was presented as a mixture of MoCo experts . The paper in its current form ( section 3 ) is difficult to follow , since several MoCo ideas are mentioned ( eg.student and teacher network , EMA , etc ) without been intuitively explained . 2 ) In section 3 that describes the method , there is no reference about image augmentation , although it is a critical aspect of the approach . Use of image augmentation is only mentioned at the end of the Appendix . 3 ) A pseudocode descibing the exact steps of the proposed method is imperative . 4 ) Due to some approximations made , is it possible to prove convergence of the proposed EM procedure ? 5 ) Gating prototypes \\omega remain fixed during training . It is important to provide more details on the MMD method used to specify them . 6 ) It seems strange that , while \\omega are specified using embeddings from the the initial network g ( x ) , they are not involved during training . 7 ) A bad specification of \\omega is expected to have strong negative influence on the results that can not be recovered . 8 ) What is the size of minibatch B ? ( eq . ( 10 ) ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive comments and acknowledgment of our novelty . We updated the submission accordingly . In particular , we formally proved the convergence and provided extra pseudocodes and experiments for clarity . Please kindly find the detailed responses below . # # # # Q1 : To introduce MoCo first Thank you for the concrete advice . Following the reviewer \u2019 s suggestion , we added a preliminary section to introduce contrastive learning , especially MoCo , before introducing MiCE in the revised version to make the paper easier to follow . In the preliminary section , we explained the use of the student and teacher networks , EMA , and data augmentation . Currently , our writing seems to tie up MiCE and MoCo [ 2 ] , while we think MiCE is a general framework that can construct the expert model based on different contrastive learning methods such as InstDisc [ 1 ] and SimCLR [ 3 ] ( with some minor adaptations needed ) . We have also clarified it in Sec.3 & 4 of the revised version . # # # # Q2 : Reference to data augmentation Thank you for pointing out . We updated the submission to mention data augmentation in the preliminary and Sec.6 of the revised version . For your convenience , we use the standard augmentation as in MoCo [ 2 ] . # # # # Q3 : Pseudocode Thank you for the advice . We added a Pytorch-like pseudocode in Appendix A.3 to show the implementation and exact steps of the proposed method , which follows MoCo [ 2 ] . # # # # Q4 : Convergence of EM Thank you for your concern . We formally proved the convergence of MiCE in Appendix A.4 . The proof spirit is highly similar to the original EM \u2019 s . In particular , we first rewrite the approximate ELBO used in MiCE as a sum of the original ELBO and a log-ratio between two normalization constants . We found that the log ratio has a constant upper bound . Therefore , by assuming that the log conditional likelihood of the incomplete data has an upper bound , similarly to the original proof in EM , we can upper bound the approximate ELBO . Since the approximated ELBO will not decrease in expectation , MiCE is convergent in expectation as well ."}, {"review_id": "gV3wdEOGy_V-1", "review_text": "Summary and Contributions : Inspired by the mixture of experts , authors propose an image clustering algorithm using a mixture of contrastive experts where , each of the conditional models is an expert in discriminating a subset of instances based on contrastive learning . To this end they use a gating function to partition an unlabeled dataset into subsets according to the latent semantics and discriminative distinct , where the gating function performs a soft partitioning of the dataset based on the cosine similarity between the image embeddings and the gating prototypes . The authors carry out experiments on four widely adopted natural image datasets to evaluate the performance of the method in these tasks and compare it to competing methods and baselines . Correctness and Clarity : The paper is well-written , with informative figures and tables . The paper presents the idea in a clear and straight-forward manner , and is solidly built on top of the current literature . Authors convincingly tested the method with multiple SOTA and baseline and the results look correct to me . Reproducibility : The details of the experiments , implementation , and the public datasets are included in the paper . Thanks also for sharing the code . Additional Feedback and Suggestions : Since the goal of the paper is image clustering , providing some visual results is appreciated . Also , I am curious to see the performance of the method when we have large number of clusters in our dataset e.g.ImageNet.Decision : The idea of using a scalable variant of the Expectation-Maximization ( EM ) algorithm to help with the nontrivial inference and learning problems caused by the latent variables seems interesting to me . And overall , the technical novelty together with the fine evaluation are good enough for ICLR , in my opinion .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the positive comments and acknowledgment of our contribution . We revised the submission by providing extra experiments , visualizations , and pseudocodes in both the main text and the appendix . Please kindly find the detailed responses below . # # # # Q1 : More visual results Thank you for the advice . Following your suggestion , we included several new figures and visualizations to support our ideas in the revised version . For instance , we show the histogram of the posterior estimates of the dataset at the different training epochs . Also , we visualize the image embeddings and the prototypes with t-SNE to investigate whether MiCE capture the cluster structure that matches the latent semantics . Please kindly refer to Sec.6.1 and Appendix E of the revision for the results . # # # # Q2 : The performance on datasets with a larger amount of clusters Thank you for your suggestion . We hypothesize that MiCE can perform well on datasets with a larger amount of clusters according to our current empirical results and analysis . This is a very interesting and important future work . We added the discussion in Sec.7 . * We would like to express our sincere gratitude and appreciation for the constructive and positive comments again , which help us to improve the quality of the paper . *"}, {"review_id": "gV3wdEOGy_V-2", "review_text": "Summary : Authors present \u201c mixture of experts \u201d type of method to solve a clustering with unsupervised learning problem . Method is called as Mixture of Contrastive Experts ( MiCE ) which uses contrastive learning as a base module and combines it with latent mixture models . Authors develop a scalable algorithm for MiCE and empirically evaluate the proposed method for image clustering . Recommendation : I am tending towards accepting the paper ( rating 6 ) . Reason for the acceptance is the novel method supported by empirical evidence . Reason for score not being too high are some weaknesses mentioned in the details later . Strengths : 1 ) Authors address an image clustering algorithm given number of clusters . Submission is clear , technically correct and present novel findings . 2 ) The proposed approach is well motivated . Weakness/Questions : 1 ) All recent papers have ImageNet-10 as one of the five common datasets [ 1-2 ] . Why was it omitted in the current paper ? 2 ) It looks like method is very tied up with MoCo and developed on top of it . Is there an easy/quick way to use other backbones like SimCLR instead of MoCo and still preserve all the steps in the method ? 3 ) Why were images in ImageNet-Dog resized to 96 x 96 x3 ? 4 ) Almost all prior methods and proposed method MiCE assume that a number of clusters are known ( which shouldn \u2019 t ideally be the case ) . But it looks like the proposed method MiCE uses the information in a better way by assuming number of experts = number of clusters . Can one use more or less number of experts ( than number of clusters K ) and still partition the sample data into K clusters ? Can one easily use over clustering as presented in IIC ? 5 ) How does the proposed method MiCE fare in terms of computation complexity when compared to MoCo ? 6 ) How do \\mu , w ( the expert and gating prototypes ) differ during the training ? Since the prediction value is the sum of the expert probability weighted by the gating function , one would expect the gating prototypes and expert prototypes to be similar ? Is this true ? How is consistency maintained ( is there a clear correspondence , prototype 1 in expert matches to prototype 2 in gating function ) ? 7 ) Can authors elaborate more about the usage of Max-Mahalanobis distribution ( MMD ) and how exactly does it solve the issue of \u201c unnecessary difficulties in partitioning the dataset if some of them are crowded together. \u201d 8 ) \u201c Since the images of CIFAR-10 and CIFAR-100 are smaller than ImageNet images , following ( Chen et al. , 2020 ) , we replace the first 7x7 Conv of stride 2 with a 3x3 Conv of stride 1 for all experiments on CIFAR-10 and CIFAR-100 . The first max-pooling operation is removed as well . For fair comparisons \u201d : Is removing first max-pooling operation standard practice ? Is there any performance loss when max pooling is not removed ? Minor : 1 ) What does NMI and ARI of > 1 mean ? Don \u2019 t they have to be in the range of [ 0,1 ] ? [ 1 ] Huang , Jiabo , Shaogang Gong , and Xiatian Zhu . `` Deep Semantic Clustering by Partition Confidence Maximisation . '' In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 8849-8858 . 2020 . [ 2 ] Wu , Jianlong , Keyu Long , Fei Wang , Chen Qian , Cheng Li , Zhouchen Lin , and Hongbin Zha . `` Deep comprehensive correlation mining for image clustering . '' In Proceedings of the IEEE International Conference on Computer Vision , pp . 8150-8159 . 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive comments and acknowledgment of our novelty . In the revision , we addressed all comments . In particular , we clarified the experiment settings and included the algorithm on the usage of MMD and extra experiments . Please kindly find the detailed responses below . # # # # Q1 : Results on ImageNet-10 We have started running experiments on the ImageNet-10 . Given the limited time of the response period , we plan to include the results in the camera-ready version if possible . Besides , ImageNet-Dog results in the submission can also verify the effectiveness of MiCE on complex datasets . Also , we added the results from the paper mentioned in the comment with the title \u201c Deep Semantic Clustering by Partition Confidence Maximisation \u201d [ 8 ] as one of the baseline methods to Tab.1 of the revised version . # # # # Q2 : The use of other contrastive learning backbones Thank you for your concern . The proposed framework is general and can be derived based on different contrastive methods . Indeed , for any contrastive learning methods that can define the model $ p ( Y|X ) $ , we can formulate the mixture model by introducing the latent variables as the cluster labels . Incorporating other possible contrastive learning backbones like SimCLR would be interesting future work . As the current writing seems to be tied up with MoCo , we added a preliminary section ( Sec.3 in the revised version ) to introduce the contrastive learning before introducing MiCE and clarify their relations as discussed above . # # # # Q3 : Why were images in ImageNet-Dog resized to 96 x 96 x3 ? For a fair comparison , the ImageNet-Dog is resized to 96x96x3 following the previous methods , including DCCM [ 1 ] and DAC [ 2 ] . We have included the references about the image size in Sec.6 of the revised version . # # # # Q4 : The number of experts and overclustering Thank you for your concern . In the cases where the number of the experts L differs from the number of ground-truth clusters K , the model will partition the datasets into L subsets instead of K. Even though the number of experts is currently tied with K , * it is not a drawback and does not prevent us from applying to the common clustering settings * . Also , MiCE does not use additional knowledge comparing to the baseline methods . If the ground-truth K is not known , we may treat K as a hyper-parameter and decide K following the methods described in [ 13 ] [ 14 ] , which is worth investigating in the future . The overclustering technique [ 10 ] is orthogonal to our methods and can be applied with minor adaptations . However , it may require additional hyper-parameter tuning to ensure overclustering improves the results . From the supplementary [ 11 ] provided by IIC [ 10 ] , we see that the numbers of clusters ( for overclustering ) are set differently for different datasets . Despite overclustering is an interesting technique , we would like to * * highlight the simplicity * * of the current version of MiCE . We provide a possible implementation in the following for your further reference : To perform overclustering ( of M clusters ) , we could add another ( M+1 ) output layers and another gating and expert prototypes with M embeddings each . Therefore , on top of the original objective function with K experts , we can add another objective with M experts , where the possible values of the latent variables are in { 1 , 2 , \u2026 , K } and { 1 , 2 , \u2026 , M } respectively . We included the above discussion in Appendix F of the revision ."}, {"review_id": "gV3wdEOGy_V-3", "review_text": "The paper proposes to use mixture of experts for image clustering . The individual expert for each cluster adopts an instance discrimination approach for training . The proposed method has shown superior clustering performance compared to an extensive number of clustering methods on a reasonable collection of data sets . Though the overall idea of the proposed method is clear , the paper does not seem to explain some technical details clear enough . In Equation , the probability P ( Y , Z|X ) does not seem to be correct . The product term over k appears to be valid only when it uses a hard assignment , i.e.P ( z_n|x_n ) = 1 for exactly one of the clusters . The student and teacher embeddings are used before Equation 4 but have not been explained until a later part of the paper . It is also unclear why an instance discrimination approach would lead to a better clustering performance . Although an extensive number of clustering methods have been included in the experiments , it has omitted some strong competitors including Variational Deep Embedding ( Jiang et al.2016 ) , Latent Tree Variational Autoencoder ( Li et al.2019 ) , Deep clustering via a Gaussian mixture VAE with Graph embedding ( Yang et al.2019 ) , etc . Those methods appear to perform better on some of the data sets . For example , those methods have been reported to yield over 85 % of accuracy on the STL-10 data set but the proposed method yields only 75 % on the same data set . It would be better if the paper could include those methods in the experiments or justify the selection of baseline methods . Overall , the proposed method appears to be an interesting combination of some existing methods . However , the technical details need better clarity and the experiments should include more relevant methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive comments and we have updated the submission accordingly . In particular , we clarified the main concerns on the technical details and selection of baselines . Please kindly find the detailed responses below . # # # # Q1 : Concerns about the probability p ( Y , Z|X ) Thank you for pointing out the issue . The number of the equation seems to be missing in the comment the reviewer provided . As the reviewer mentioned $ p ( Y , Z|X ) $ , we suppose that the reviewer is referring to Eq . ( 3 ) in the revision ( which was Eq . ( 1 ) in the first/original submission ) and will discuss it below . Please kindly let us know if the reviewer is referring to a different equation . There was a typo where the probability p ( y_n , z_n=k | x_n=k ) shall have an indicator function \\mathds { 1 } ( z_n = k ) as its exponent . The correct Eq . ( 3 ) is : p ( Y , Z |X ) = \\prod_ { n=1 } ^N \\prod_ { k=1 } ^K p ( y_n , z_n = k |x_n ) ^ { \\mathds { 1 } ( z_n = k ) } = \\prod_ { n=1 } ^N \\prod_ { k=1 } ^K p ( z_n = k | x_n ) ^ { \\mathds { 1 } ( z_n = k ) } p ( y_n | x_n , z_n = k ) ^ { \\mathds { 1 } ( z_n = k ) } . The gating function p ( z_n|x_n ) is modeled as a categorical distribution and allows soft weighting on the predictions from the $ z_n $ -th expert . The formulation follows the Mixture of Experts ( MoE ) [ 1 ] [ 2 ] closely . We have revised the submission accordingly . Please kindly refer to Eq . ( 3 ) in the revision for details . We also double-check the other equations and the algorithm , which remain correct . # # # # Q2 : The student and teacher embeddings are used before Equation 4 but have not been explained until a later part of the paper . Thank you for the advice . We have added * Preliminary * section to introduce contrastive learning methods , especially MoCo , before introducing MiCE to make the paper easier to follow . The technical details of MoCo , including the student and teacher embeddings , and the use of EMA are discussed in the Preliminary section . Please kindly refer to Sec.3 of the revision for more details . # # # # Q3 : It is also unclear why an instance discrimination approach would lead to a better clustering performance As mentioned in the second paragraph of Sec.1 and the experiment section of the original submission , the instance discrimination task leads to discriminative representations [ 11 ] , which are useful for clustering . In fact , improved representations can lead to better clustering results as shown in the clustering literature [ 3 ] [ 4 ] . Empirically , with instance discrimination , we can get a substantial improvement even with a simple clustering algorithm such as spherical k-means , as shown in Tab.1 in the revision . Further , with a unified framework , MiCE can improve the results by considering the semantic structure explicitly ( also see Tab.1 ) .We added the above discussion in Sec.1 of the revised version to make the motivation of using instance discrimination clearer ."}], "0": {"review_id": "gV3wdEOGy_V-0", "review_text": "The paper presents an image clustering methodology based on Mixture of Experts ( MoE ) for image clustering . Although MoE has been proposed for supervised learning problems , the authors exploit the instance discrimination framwork to apply the MoE idea for image clustering . This is a novel aspect of the proposed method . The MoCo framework ( unsupervised ) for contrastive learning of image representations is employed to define a mixture of MoCo experts model where each expert additionally includes a cluster prototype vector to facilitate clustering . This unified approach for simultaneous MoCo-based representation learning and clustering seems to provide better results that the two-stage approach of first applying MoCo and then using k-means clustering on the obtained representations . A probabilistic formulation of the method is presented , along with a training approach based on EM algorithm for likelihood maximization . There are several concerns related to presentation and clarity . Comments to be addressed : 1 ) It would be easier to understand the contribution of the paper , if the MoCo approach were initially described and then the proposed method was presented as a mixture of MoCo experts . The paper in its current form ( section 3 ) is difficult to follow , since several MoCo ideas are mentioned ( eg.student and teacher network , EMA , etc ) without been intuitively explained . 2 ) In section 3 that describes the method , there is no reference about image augmentation , although it is a critical aspect of the approach . Use of image augmentation is only mentioned at the end of the Appendix . 3 ) A pseudocode descibing the exact steps of the proposed method is imperative . 4 ) Due to some approximations made , is it possible to prove convergence of the proposed EM procedure ? 5 ) Gating prototypes \\omega remain fixed during training . It is important to provide more details on the MMD method used to specify them . 6 ) It seems strange that , while \\omega are specified using embeddings from the the initial network g ( x ) , they are not involved during training . 7 ) A bad specification of \\omega is expected to have strong negative influence on the results that can not be recovered . 8 ) What is the size of minibatch B ? ( eq . ( 10 ) ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive comments and acknowledgment of our novelty . We updated the submission accordingly . In particular , we formally proved the convergence and provided extra pseudocodes and experiments for clarity . Please kindly find the detailed responses below . # # # # Q1 : To introduce MoCo first Thank you for the concrete advice . Following the reviewer \u2019 s suggestion , we added a preliminary section to introduce contrastive learning , especially MoCo , before introducing MiCE in the revised version to make the paper easier to follow . In the preliminary section , we explained the use of the student and teacher networks , EMA , and data augmentation . Currently , our writing seems to tie up MiCE and MoCo [ 2 ] , while we think MiCE is a general framework that can construct the expert model based on different contrastive learning methods such as InstDisc [ 1 ] and SimCLR [ 3 ] ( with some minor adaptations needed ) . We have also clarified it in Sec.3 & 4 of the revised version . # # # # Q2 : Reference to data augmentation Thank you for pointing out . We updated the submission to mention data augmentation in the preliminary and Sec.6 of the revised version . For your convenience , we use the standard augmentation as in MoCo [ 2 ] . # # # # Q3 : Pseudocode Thank you for the advice . We added a Pytorch-like pseudocode in Appendix A.3 to show the implementation and exact steps of the proposed method , which follows MoCo [ 2 ] . # # # # Q4 : Convergence of EM Thank you for your concern . We formally proved the convergence of MiCE in Appendix A.4 . The proof spirit is highly similar to the original EM \u2019 s . In particular , we first rewrite the approximate ELBO used in MiCE as a sum of the original ELBO and a log-ratio between two normalization constants . We found that the log ratio has a constant upper bound . Therefore , by assuming that the log conditional likelihood of the incomplete data has an upper bound , similarly to the original proof in EM , we can upper bound the approximate ELBO . Since the approximated ELBO will not decrease in expectation , MiCE is convergent in expectation as well ."}, "1": {"review_id": "gV3wdEOGy_V-1", "review_text": "Summary and Contributions : Inspired by the mixture of experts , authors propose an image clustering algorithm using a mixture of contrastive experts where , each of the conditional models is an expert in discriminating a subset of instances based on contrastive learning . To this end they use a gating function to partition an unlabeled dataset into subsets according to the latent semantics and discriminative distinct , where the gating function performs a soft partitioning of the dataset based on the cosine similarity between the image embeddings and the gating prototypes . The authors carry out experiments on four widely adopted natural image datasets to evaluate the performance of the method in these tasks and compare it to competing methods and baselines . Correctness and Clarity : The paper is well-written , with informative figures and tables . The paper presents the idea in a clear and straight-forward manner , and is solidly built on top of the current literature . Authors convincingly tested the method with multiple SOTA and baseline and the results look correct to me . Reproducibility : The details of the experiments , implementation , and the public datasets are included in the paper . Thanks also for sharing the code . Additional Feedback and Suggestions : Since the goal of the paper is image clustering , providing some visual results is appreciated . Also , I am curious to see the performance of the method when we have large number of clusters in our dataset e.g.ImageNet.Decision : The idea of using a scalable variant of the Expectation-Maximization ( EM ) algorithm to help with the nontrivial inference and learning problems caused by the latent variables seems interesting to me . And overall , the technical novelty together with the fine evaluation are good enough for ICLR , in my opinion .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the positive comments and acknowledgment of our contribution . We revised the submission by providing extra experiments , visualizations , and pseudocodes in both the main text and the appendix . Please kindly find the detailed responses below . # # # # Q1 : More visual results Thank you for the advice . Following your suggestion , we included several new figures and visualizations to support our ideas in the revised version . For instance , we show the histogram of the posterior estimates of the dataset at the different training epochs . Also , we visualize the image embeddings and the prototypes with t-SNE to investigate whether MiCE capture the cluster structure that matches the latent semantics . Please kindly refer to Sec.6.1 and Appendix E of the revision for the results . # # # # Q2 : The performance on datasets with a larger amount of clusters Thank you for your suggestion . We hypothesize that MiCE can perform well on datasets with a larger amount of clusters according to our current empirical results and analysis . This is a very interesting and important future work . We added the discussion in Sec.7 . * We would like to express our sincere gratitude and appreciation for the constructive and positive comments again , which help us to improve the quality of the paper . *"}, "2": {"review_id": "gV3wdEOGy_V-2", "review_text": "Summary : Authors present \u201c mixture of experts \u201d type of method to solve a clustering with unsupervised learning problem . Method is called as Mixture of Contrastive Experts ( MiCE ) which uses contrastive learning as a base module and combines it with latent mixture models . Authors develop a scalable algorithm for MiCE and empirically evaluate the proposed method for image clustering . Recommendation : I am tending towards accepting the paper ( rating 6 ) . Reason for the acceptance is the novel method supported by empirical evidence . Reason for score not being too high are some weaknesses mentioned in the details later . Strengths : 1 ) Authors address an image clustering algorithm given number of clusters . Submission is clear , technically correct and present novel findings . 2 ) The proposed approach is well motivated . Weakness/Questions : 1 ) All recent papers have ImageNet-10 as one of the five common datasets [ 1-2 ] . Why was it omitted in the current paper ? 2 ) It looks like method is very tied up with MoCo and developed on top of it . Is there an easy/quick way to use other backbones like SimCLR instead of MoCo and still preserve all the steps in the method ? 3 ) Why were images in ImageNet-Dog resized to 96 x 96 x3 ? 4 ) Almost all prior methods and proposed method MiCE assume that a number of clusters are known ( which shouldn \u2019 t ideally be the case ) . But it looks like the proposed method MiCE uses the information in a better way by assuming number of experts = number of clusters . Can one use more or less number of experts ( than number of clusters K ) and still partition the sample data into K clusters ? Can one easily use over clustering as presented in IIC ? 5 ) How does the proposed method MiCE fare in terms of computation complexity when compared to MoCo ? 6 ) How do \\mu , w ( the expert and gating prototypes ) differ during the training ? Since the prediction value is the sum of the expert probability weighted by the gating function , one would expect the gating prototypes and expert prototypes to be similar ? Is this true ? How is consistency maintained ( is there a clear correspondence , prototype 1 in expert matches to prototype 2 in gating function ) ? 7 ) Can authors elaborate more about the usage of Max-Mahalanobis distribution ( MMD ) and how exactly does it solve the issue of \u201c unnecessary difficulties in partitioning the dataset if some of them are crowded together. \u201d 8 ) \u201c Since the images of CIFAR-10 and CIFAR-100 are smaller than ImageNet images , following ( Chen et al. , 2020 ) , we replace the first 7x7 Conv of stride 2 with a 3x3 Conv of stride 1 for all experiments on CIFAR-10 and CIFAR-100 . The first max-pooling operation is removed as well . For fair comparisons \u201d : Is removing first max-pooling operation standard practice ? Is there any performance loss when max pooling is not removed ? Minor : 1 ) What does NMI and ARI of > 1 mean ? Don \u2019 t they have to be in the range of [ 0,1 ] ? [ 1 ] Huang , Jiabo , Shaogang Gong , and Xiatian Zhu . `` Deep Semantic Clustering by Partition Confidence Maximisation . '' In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 8849-8858 . 2020 . [ 2 ] Wu , Jianlong , Keyu Long , Fei Wang , Chen Qian , Cheng Li , Zhouchen Lin , and Hongbin Zha . `` Deep comprehensive correlation mining for image clustering . '' In Proceedings of the IEEE International Conference on Computer Vision , pp . 8150-8159 . 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive comments and acknowledgment of our novelty . In the revision , we addressed all comments . In particular , we clarified the experiment settings and included the algorithm on the usage of MMD and extra experiments . Please kindly find the detailed responses below . # # # # Q1 : Results on ImageNet-10 We have started running experiments on the ImageNet-10 . Given the limited time of the response period , we plan to include the results in the camera-ready version if possible . Besides , ImageNet-Dog results in the submission can also verify the effectiveness of MiCE on complex datasets . Also , we added the results from the paper mentioned in the comment with the title \u201c Deep Semantic Clustering by Partition Confidence Maximisation \u201d [ 8 ] as one of the baseline methods to Tab.1 of the revised version . # # # # Q2 : The use of other contrastive learning backbones Thank you for your concern . The proposed framework is general and can be derived based on different contrastive methods . Indeed , for any contrastive learning methods that can define the model $ p ( Y|X ) $ , we can formulate the mixture model by introducing the latent variables as the cluster labels . Incorporating other possible contrastive learning backbones like SimCLR would be interesting future work . As the current writing seems to be tied up with MoCo , we added a preliminary section ( Sec.3 in the revised version ) to introduce the contrastive learning before introducing MiCE and clarify their relations as discussed above . # # # # Q3 : Why were images in ImageNet-Dog resized to 96 x 96 x3 ? For a fair comparison , the ImageNet-Dog is resized to 96x96x3 following the previous methods , including DCCM [ 1 ] and DAC [ 2 ] . We have included the references about the image size in Sec.6 of the revised version . # # # # Q4 : The number of experts and overclustering Thank you for your concern . In the cases where the number of the experts L differs from the number of ground-truth clusters K , the model will partition the datasets into L subsets instead of K. Even though the number of experts is currently tied with K , * it is not a drawback and does not prevent us from applying to the common clustering settings * . Also , MiCE does not use additional knowledge comparing to the baseline methods . If the ground-truth K is not known , we may treat K as a hyper-parameter and decide K following the methods described in [ 13 ] [ 14 ] , which is worth investigating in the future . The overclustering technique [ 10 ] is orthogonal to our methods and can be applied with minor adaptations . However , it may require additional hyper-parameter tuning to ensure overclustering improves the results . From the supplementary [ 11 ] provided by IIC [ 10 ] , we see that the numbers of clusters ( for overclustering ) are set differently for different datasets . Despite overclustering is an interesting technique , we would like to * * highlight the simplicity * * of the current version of MiCE . We provide a possible implementation in the following for your further reference : To perform overclustering ( of M clusters ) , we could add another ( M+1 ) output layers and another gating and expert prototypes with M embeddings each . Therefore , on top of the original objective function with K experts , we can add another objective with M experts , where the possible values of the latent variables are in { 1 , 2 , \u2026 , K } and { 1 , 2 , \u2026 , M } respectively . We included the above discussion in Appendix F of the revision ."}, "3": {"review_id": "gV3wdEOGy_V-3", "review_text": "The paper proposes to use mixture of experts for image clustering . The individual expert for each cluster adopts an instance discrimination approach for training . The proposed method has shown superior clustering performance compared to an extensive number of clustering methods on a reasonable collection of data sets . Though the overall idea of the proposed method is clear , the paper does not seem to explain some technical details clear enough . In Equation , the probability P ( Y , Z|X ) does not seem to be correct . The product term over k appears to be valid only when it uses a hard assignment , i.e.P ( z_n|x_n ) = 1 for exactly one of the clusters . The student and teacher embeddings are used before Equation 4 but have not been explained until a later part of the paper . It is also unclear why an instance discrimination approach would lead to a better clustering performance . Although an extensive number of clustering methods have been included in the experiments , it has omitted some strong competitors including Variational Deep Embedding ( Jiang et al.2016 ) , Latent Tree Variational Autoencoder ( Li et al.2019 ) , Deep clustering via a Gaussian mixture VAE with Graph embedding ( Yang et al.2019 ) , etc . Those methods appear to perform better on some of the data sets . For example , those methods have been reported to yield over 85 % of accuracy on the STL-10 data set but the proposed method yields only 75 % on the same data set . It would be better if the paper could include those methods in the experiments or justify the selection of baseline methods . Overall , the proposed method appears to be an interesting combination of some existing methods . However , the technical details need better clarity and the experiments should include more relevant methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive comments and we have updated the submission accordingly . In particular , we clarified the main concerns on the technical details and selection of baselines . Please kindly find the detailed responses below . # # # # Q1 : Concerns about the probability p ( Y , Z|X ) Thank you for pointing out the issue . The number of the equation seems to be missing in the comment the reviewer provided . As the reviewer mentioned $ p ( Y , Z|X ) $ , we suppose that the reviewer is referring to Eq . ( 3 ) in the revision ( which was Eq . ( 1 ) in the first/original submission ) and will discuss it below . Please kindly let us know if the reviewer is referring to a different equation . There was a typo where the probability p ( y_n , z_n=k | x_n=k ) shall have an indicator function \\mathds { 1 } ( z_n = k ) as its exponent . The correct Eq . ( 3 ) is : p ( Y , Z |X ) = \\prod_ { n=1 } ^N \\prod_ { k=1 } ^K p ( y_n , z_n = k |x_n ) ^ { \\mathds { 1 } ( z_n = k ) } = \\prod_ { n=1 } ^N \\prod_ { k=1 } ^K p ( z_n = k | x_n ) ^ { \\mathds { 1 } ( z_n = k ) } p ( y_n | x_n , z_n = k ) ^ { \\mathds { 1 } ( z_n = k ) } . The gating function p ( z_n|x_n ) is modeled as a categorical distribution and allows soft weighting on the predictions from the $ z_n $ -th expert . The formulation follows the Mixture of Experts ( MoE ) [ 1 ] [ 2 ] closely . We have revised the submission accordingly . Please kindly refer to Eq . ( 3 ) in the revision for details . We also double-check the other equations and the algorithm , which remain correct . # # # # Q2 : The student and teacher embeddings are used before Equation 4 but have not been explained until a later part of the paper . Thank you for the advice . We have added * Preliminary * section to introduce contrastive learning methods , especially MoCo , before introducing MiCE to make the paper easier to follow . The technical details of MoCo , including the student and teacher embeddings , and the use of EMA are discussed in the Preliminary section . Please kindly refer to Sec.3 of the revision for more details . # # # # Q3 : It is also unclear why an instance discrimination approach would lead to a better clustering performance As mentioned in the second paragraph of Sec.1 and the experiment section of the original submission , the instance discrimination task leads to discriminative representations [ 11 ] , which are useful for clustering . In fact , improved representations can lead to better clustering results as shown in the clustering literature [ 3 ] [ 4 ] . Empirically , with instance discrimination , we can get a substantial improvement even with a simple clustering algorithm such as spherical k-means , as shown in Tab.1 in the revision . Further , with a unified framework , MiCE can improve the results by considering the semantic structure explicitly ( also see Tab.1 ) .We added the above discussion in Sec.1 of the revised version to make the motivation of using instance discrimination clearer ."}}