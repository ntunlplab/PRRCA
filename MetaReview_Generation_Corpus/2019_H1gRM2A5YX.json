{"year": "2019", "forum": "H1gRM2A5YX", "title": "Analysis of Memory Organization for Dynamic Neural Networks", "decision": "Reject", "meta_review": "This paper presents a taxonomic study of neural network architectures, focussing on those which seek to map onto different part of the hierarchy of models of computation (DFAs, PDAs, etc). The paper splits between defining the taxonomy and comparing its elements on synthetic and \"NLP\" tasks (in fact, babi, which is also synthetic). I'm a fairly biased assessor of this sort of paper, as I generally like this topical area and think there is a need for more work of this nature in our field. I welcome, and believe the CFP calls for, papers like this (\"learning representations of outputs or [structured] states\", \"theoretical issues in deep learning\")). However, despite my personal enthusiasm, the reviews tell a different story.\n\nThe scores for this paper are all over the place, and that's after some attempt at harmonisation! I am satisfied that the authors have had a fair shot at defending their paper and that the reviewers have engaged with the discussion process. I'm afraid the emerging consensus still seems to be in favour of rejection. Despite my own views, I'm not comfortable bumping it up into acceptance territory on the basis of this assessment. Reviewer 1 is the only enthusiastic proponent of the paper, but their statement of support for the paper has done little to sway the others. The arguments by reviewer 3 specifically are quite salient: it is important to seek informative and useful taxonomies of the sort presented in this work, but they must have practical utility. From reading the paper, I share some of this reviewer's concerns: while it is clear to me what use there is the production of studies of the sort presented in this paper, it is not immediately clear what the utility of *this* study is. Would I, practically speaking, be able to make an informed choice as to what model class to attempt for a problem that wouldn't be indistinguishable from common approaches (e.g. \"start simple, add complexity\"). I am afraid I agree with this reviewer that I would not.\n\nMy conclusion is that there is not a strong consensus for accepting the paper. While I wouldn't mind seeing this work presented at the conference, but due to the competitive nature of the paper selection process, I'm afraid the line must be drawn somewhere. I do look forward to re-reading this paper after the authors have had a chance to improve and expand upon it.", "reviews": [{"review_id": "H1gRM2A5YX-0", "review_text": "I really liked this paper and believe it could be useful to many practitioners of NLP, conversational ML and sequential learning who may find themselves somewhat lost in the ever-expanding field of dynamic neural networks. Although the format of the paper is seemingly unusual (it may feel like reading a survey at first), the authors propose a concise and pedagogical presentation of Jordan Networks, LSTM, Neural Stacks and Neural RAMs while drawing connections between these different model families. The cornerstone of the analysis of the paper resides in the taxonomy presented in Figure 5 which, I believe, should be presented on the front page of the paper. The taxonomy is justified by a thorough theoretical analysis which may be found in appendix. The authors put the taxonomy to use on synthetic and real data sets. Although the data set taxonomy is less novel it is indeed insightful to go back to a classification of grammatical complexity and structure so as to enable a clearer thinking about sequential learning tasks. An analysis of sentiment analysis and question answering task is conducted which relates the properties of sequences in those datasets to the neural network taxonomy the authors devised. In each experiment, the choice of NN recommended by the taxonomy gives the best performance among the other elements presented in the taxonomy. Strength: o) The paper is thorough and the appendix presents all experiments in detail. o) The taxonomy is clearly a novel valuable contribution. o) The survey aspect of the paper is also a strength as it consolidates the reader's understanding of the families of dynamic NNs under consideration. Weaknesses: o) The taxonomy presented in the paper relies on an analysis of what the architectures can do, not what they can learn. I believe the authors should acknowledge that the presence of Long Range Dependence in sequences is still hard to capture by dynamic neural networks (in particular RNNs) and that alternate analysis have been proposed to understand the impact of the presence of such Long Range Dependence in the data on sequential learning. I believe that mentioning this issue along with older (http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf) and more recent (e.g. http://proceedings.mlr.press/v84/belletti18a/belletti18a.pdf and https://arxiv.org/pdf/1803.00144.pdf) papers on the topic is necessary for the paper to present a holistic view of the matter at hand. o) The arguments given in 5.2 are not most convincing and could benefit from a more thorough exposition, in particular for the sentiment analysis task. It is not clear enough in my view that it is true that \"since the goal is to classify the emotional tone as either 1 or 0, the specific contents of the text are not very important here\". One could argue that a single word in a sentence can change its meaning and sentiment. o) The written could be more polished. As a practitioner using RNNs daily I find this paper exciting as an attempt to conceptualize both data set properties and dynamic neural network families. I believe that the authors should address the shortcomings I think hinder the paper's arguments and exposition of pre-existing work on the analysis of dynamic neural networks.", "rating": "7: Good paper, accept", "reply_text": "Q1 ) The taxonomy presented in the paper relies on an analysis of what the architectures can do , not what they can learn . I believe the authors should acknowledge that the presence of Long Range Dependence in sequences is still hard to capture by dynamic neural networks ( in particular RNNs ) and that alternate analysis have been proposed to understand the impact of the presence of such Long Range Dependence in the data on sequential learning . I believe that mentioning this issue along with older ( http : //ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf ) and more recent ( e.g.http : //proceedings.mlr.press/v84/belletti18a/belletti18a.pdf and https : //arxiv.org/pdf/1803.00144.pdf ) papers on the topic is necessary for the paper to present a holistic view of the matter at hand . We agree that the LRD problem is hard for RNNs , and this is the major reason external memories are needed , In the revised version , we have motivated the need for the taxonomy by the LRD problem and included these three papers for completeness . However , we would like to say that the main goal of our paper is indeed to explain what each architecture can learn from data , against your first observation . Based on this we further analyzed what they can do . From your comment we may have emphasized too much the aspect of how they can be used to help the practitioner . We have characterized what information each architecture can extract from the data stream in the revised manuscript . Q2 ) The arguments given in 5.2 are not most convincing and could benefit from a more thorough exposition , in particular for the sentiment analysis task . It is not clear enough in my view that it is true that `` since the goal is to classify the emotional tone as either 1 or 0 , the specific contents of the text are not very important here '' . One could argue that a single word in a sentence can change its meaning and sentiment . From our analysis of the results , all the models solve the sentiment analysis problem mainly based on the occurrence and reoccurrence of some discriminating words . Specifically , when feeding the input sequence to the model , the output value would be increased when meeting positive words such as \u201c good , love \u201d and decreased when meeting negative words such as \u201c dislike , boring \u201d . If there were many positive words appearing in text , the sentiment would be judged as positive . Since the model only cares about whether the word is positive or negative and its number of occurrences in the text ( kind of a \u201c density \u201d ) , we deduct that a specific word is not crucial ( for example , as long as it is a positive word , whether it is \u201c love \u201d , \u2019 like \u2019 or \u2018 happy \u2019 is not that important ) , and translated this as \u201c the specific contents of the text are not very important \u201d . We have elaborated more on this point in the final version because as the reviewer pointed out the explanation is too brief and not specific . But we have to be aware that this \u201c discriminating words based \u201d method does not really solve the problem as human . As the reviewer mentioned \u201c a single word in a sentence can change its meaning and sentiment \u201d , therefore , in order to really solve this problem , the machine should learn how to decode the text meaning . But none of the current models can really achieve this . Although these models can capture some temporal dependencies ( for example , if there is a \u201c don \u2019 t \u201d before \u201c like \u201d , the sentence is more likely to be negative ) , the final decision still mostly depends on how many \u201c discriminating words \u201d appear in the text . That \u2019 s also the reason why they can not get 100 % accuracy . Q3 ) The written could be more polished . We have further polished the language ."}, {"review_id": "H1gRM2A5YX-1", "review_text": "Summary ========= The paper analyses the taxonomy over memory-based neural networks, in the decreasing order of capacity: Neural RAM to Neural Stack, Neural Stack to LSTM and LSTM to vanilla RNN. The experiments with synthetic and NLP datasets demonstrate the benefits of using models that fit with task types. Comment ======== Overall, the paper is well written and presents interesting analysis of different memory architectures. However, the contribution is rather limited. The proposed taxonomy is not new. It is a little bit obvious and mentioned before in [1] (Unfortunately, this was not cited in the manuscript). The theorems on inclusion relationship are also obvious and the main contribution of the paper is to formally show that in mathematical forms. The experiments on synthetic tasks give some insights into the models\u2019 operations, yet similar analyses can be found in [2, 3]. To verify the models really learn the task, the authors should include tests on unseen sequence lengths. There remains questions unexplained in NLP tasks such as why multi-slot memory did not show more advantages in Movie Review and why Neural Stack performed worse than LSTM in bAbI data. Minor potential errors: In Eq. (6), r_{t-1} should be r_t The LSTM presented in Section 3.2 is not the common one. Normally, there should be x_t term in Eq. (3) and h_t=g_{o,t}*\\tanh(r_t) in Eq. (6). The author should follow the common LSTM formulas (which may lead to different proofs) or include reference to their LSTM version. [1] Yogatama et al. Memory Architectures in Recurrent Neural Network Language Models. ICLR\u201918 [2] Joulin et al. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS\u201915 [3] Graves et al. Neural Turing Machines. arXiv preprint arXiv:1410.5401 (2014). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 ) The proposed taxonomy is not new . It is a little bit obvious and mentioned before in [ 1 ] ( Unfortunately , this was not cited in the manuscript ) . The theorems on inclusion relationship are also obvious and the main contribution of the paper is to formally show that in mathematical forms Thank you for mentioning [ 1 ] ( we have cited it in the revised version for completeness ) but we disagree that \u201c the proposed taxonomy is not new \u201d . The authors in [ 1 ] simply divided these models into sequential , random access and stack memory architectures , which bears some similarity with the taxonomy proposed in our paper , but it is more superficial and does not go to the mechanisms behind the memory types . Indeed , classifying models according to the type of memory seems obvious , but finding the essential relationship between classes and linking the descriptive power of learning machines to the properties of task data is not a trivial work . ( For example , what \u2019 s the difference between internal and external memory and what kind of tasks can they address ? Our taxonomy showed clearly that the gate mechanism in LSTM and the push/pop/no-op operators in the stack augmented memory had the same function in nature , which had never been mentioned before . Many discussions like these first appeared in our paper . ) Many papers proposed fancy models to improve the existing work , however there is still no paper providing a good approach to analyze what the memory architectures can learn and how to select the most parsimonious memory model for a specific task . As far as we know , our paper addresses for the first time how to codify and exploit the knowledge gained from the different characteristics of the memory within the taxonomy to help users select the type of memory network for an application . Moreover , the effectiveness of this analysis framework was also verified in some simple experiments . However , we agree that the analysis is not complete because on the one hand people has to analyze memory requirements of a task by themselves which is not trivial and on the other hand , the accuracy is also affected by the size and specific network structure even within a class . But we firmly believe that classifying architectures into these four classes and linking the architecture of the learning machine to its descriptive power , as we did in this paper , is a fundamental first step , and we think this paper is important to the machine learning community . We admit that the proofs of the theorems are not very hard and we included them to make our argument rigorous . Q2 ) The experiments on synthetic tasks give some insights into the models \u2019 operations , yet similar analyses can be found in [ 2 , 3 ] . Although [ 2 ] [ 3 ] and our paper used similar synthetic tasks in experiments , our goal was again very different . In [ 2 ] [ 3 ] , their goal was to show the effectiveness of their proposed architecture , so each of them only focused on analyzing the operation of one specific model . However , since our goal is to verify the proposed taxonomy , our experiment focused on showing the connections between different memory types and the growing capability of these four classes of models . For example , although [ 2 ] showed the details of the operation of neural stack and the neural stack performing better than LSTM on some tasks , we believe readers still won \u2019 t understand what was the connection between LSTM and neural stack and why LSTM could be seen as a special case of neural stack . However , in our \u201c counting with interference \u201d task , the results showed that ( Appendix D.2 ) content of the top element of the stack ( M0 in Fig.12 ) had the same changing trend as the external memory of LSTM ( M0 in Fig.11 ) and other content in the stack below the top one was redundant . Hence it helped verify our argument \u201c LSTM can be seen as neural stack with only the top element \u201d . Because of page limitation , we didn \u2019 t show how the three gates in LSTM related to the push/pop/no-op operators in neural stack , but our argument would be more convincing if these operator comparison results were added . We struggled to demonstrate the capabilities of each memory architecture , and our decision was to construct four representative tasks that would fit optimally the characteristics of each memory organization . Therefore , these four representative tasks were carefully selected to allow practitioners to compare their own problem with these four tasks and give them some hints to select the right model . This has been better explained in the revised paper , but the point is that we are not just simply repeating some existing experiments ."}, {"review_id": "H1gRM2A5YX-2", "review_text": "The authors propose a review-style overview of memory systems within neural networks, from simple RNNs to stack-based memory architectures and NTM / MemNet-style architectures. They propose some reductions to imply how one model can be used (or modify) to simulate another. They then make predictions about which type of models should be best on different types of tasks. Unfortunately I did not find the paper particularly well written and the taxonomy was not illuminating for me. I actually felt, in the endeavor of creating a simple taxonomy the authors have created confusing simplifications, e.g. \"LSTM: state memory and memory of a single external event\" to me is mis-leading as we know an LSTM can compress many external events into its hidden units. Furthermore the taxonomy did not provide me with any new insights or display a prediction that was actually clairvoyant. I.e. it was clear from the outset that a memory network (say) will be much better at bAbI than a stack-augmented neural network. It would be more interesting to me, for example, if the paper could thus formalize why NTMs & DNCs (say) do not outperform LSTMs at language modeling, for example. I found the reductions somewhat shady, e.g. the RAM simulation of a stack is possible, however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots --- or unless it had O(N) thinking steps per time step, where N is the number of memory slots, so it's not a very realistic reduction. You would never see a memory network, for example, simulating a stack due to the fixed write-one-slot-per-timestep interface. Nit: I'm not sure the authors should be saying they 'developed' four synthetic tasks, when many of these tasks have previously been proposed and published (counting, copy, reverse copy). ", "rating": "3: Clear rejection", "reply_text": "The intent of our paper was to analyze the type of memory utilized by different architectures to solve sequence learning problems . This is not an easy issue because \u2018 memory \u2019 is a very abstract concept and the specific memory requirements for a specific task are implicit , which means that quantitatively conceptualizing and analyzing memory is a very hard problem . Cognitive scientists have defined many different types of memory , which shows the richness of the topic , and there are only a few engineering quantifiers of memory such as memory depth and memory resolution , but they are not enough for the ever-growing applications of machine learning . Hence memory quantification is lacking in the current machine learning literature and it is our main contribution . The proposed taxonomy for the four most conventional memory architectures appears as a simple way to quantify the capabilities of extracting past information of each class . Our goal of providing methodologies for the practitioner relegated to a second objective of the paper . It is clear from your questions that our writing was not successful , and we have modified the writing in the final submission to make this point more explicit . As far as we know , our paper addresses for the first time how to exploit the knowledge gained from the different characteristics of the memory within the taxonomy to help users select the type of memory network for an application . However , we agree that the analysis is not complete yet because on the one hand users have to analyze task \u2019 s memory requirements by themselves which is not trivial and on the other hand , the algorithm accuracy is also affected by the size and specific network structure even within a given class of models . But we firmly believe that classifying memory architectures into these four classes and linking the architecture of the learning machine to its descriptive power , as we did in this paper , is a fundamental first step . At least , in this respect , we think this paper is important to the machine learning community . Q1 ) I actually felt , in the endeavor of creating a simple taxonomy the authors have created confusing simplifications , e.g . `` LSTM : state memory and memory of a single external event '' to me is mis-leading as we know an LSTM can compress many external events into its hidden units . We agree with your statement and we are sorry for the misleading \u201c single event \u201d . By \u201c single event \u201d we mean that if there is only one useful event , it can be stored as it is , but if there are multiple useful events , they have to be compressed into one compounded event and can only be accessed as a whole . This has been clarified in the text . Q2 ) It would be more interesting to me , for example , if the paper could thus formalize why NTMs & DNCs ( say ) do not outperform LSTMs at language modeling , for example . Please see our reply to Q4 ) of the second reviewer . Q3 ) I found the reductions somewhat shady , e.g.the RAM simulation of a stack is possible , however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots -- - or unless it had O ( N ) thinking steps per time step , where N is the number of memory slots , so it 's not a very realistic reduction . You would never see a memory network , for example , simulating a stack due to the fixed write-one-slot-per-timestep interface . The purpose of deriving the reductions was to get some insights by comparing neural stack and neural RAM , we didn \u2019 t suggest using neural RAM simulating neural stack to solve a problem , and that is the reason why we said \u201c for the tasks where the previous memory needs to be addressed sequentially , the stack neural network is our first choice. \u201d Q4 ) Nit : I 'm not sure the authors should be saying they 'developed ' four synthetic tasks , when many of these tasks have previously been proposed and published ( counting , copy , reverse copy ) . We said \u2018 developed \u2019 because some of our experiments were not same as before . Our experiments were slightly revised to highlight the advantage and limitation of different memory types . For example , compared to the previous counting task , we added some interferences to the input sequences ( see details in our \u2018 counting with interference \u2019 task ) . By comparing vRNN \u2019 s performance on our \u2018 counting \u2019 and \u2018 counting with interference \u2019 task , the limitation of internal memory in vRNN was shown more clearly . But since our revision is less novel , we have changed \u2018 develop \u2019 to \u2018 select \u2019 ."}], "0": {"review_id": "H1gRM2A5YX-0", "review_text": "I really liked this paper and believe it could be useful to many practitioners of NLP, conversational ML and sequential learning who may find themselves somewhat lost in the ever-expanding field of dynamic neural networks. Although the format of the paper is seemingly unusual (it may feel like reading a survey at first), the authors propose a concise and pedagogical presentation of Jordan Networks, LSTM, Neural Stacks and Neural RAMs while drawing connections between these different model families. The cornerstone of the analysis of the paper resides in the taxonomy presented in Figure 5 which, I believe, should be presented on the front page of the paper. The taxonomy is justified by a thorough theoretical analysis which may be found in appendix. The authors put the taxonomy to use on synthetic and real data sets. Although the data set taxonomy is less novel it is indeed insightful to go back to a classification of grammatical complexity and structure so as to enable a clearer thinking about sequential learning tasks. An analysis of sentiment analysis and question answering task is conducted which relates the properties of sequences in those datasets to the neural network taxonomy the authors devised. In each experiment, the choice of NN recommended by the taxonomy gives the best performance among the other elements presented in the taxonomy. Strength: o) The paper is thorough and the appendix presents all experiments in detail. o) The taxonomy is clearly a novel valuable contribution. o) The survey aspect of the paper is also a strength as it consolidates the reader's understanding of the families of dynamic NNs under consideration. Weaknesses: o) The taxonomy presented in the paper relies on an analysis of what the architectures can do, not what they can learn. I believe the authors should acknowledge that the presence of Long Range Dependence in sequences is still hard to capture by dynamic neural networks (in particular RNNs) and that alternate analysis have been proposed to understand the impact of the presence of such Long Range Dependence in the data on sequential learning. I believe that mentioning this issue along with older (http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf) and more recent (e.g. http://proceedings.mlr.press/v84/belletti18a/belletti18a.pdf and https://arxiv.org/pdf/1803.00144.pdf) papers on the topic is necessary for the paper to present a holistic view of the matter at hand. o) The arguments given in 5.2 are not most convincing and could benefit from a more thorough exposition, in particular for the sentiment analysis task. It is not clear enough in my view that it is true that \"since the goal is to classify the emotional tone as either 1 or 0, the specific contents of the text are not very important here\". One could argue that a single word in a sentence can change its meaning and sentiment. o) The written could be more polished. As a practitioner using RNNs daily I find this paper exciting as an attempt to conceptualize both data set properties and dynamic neural network families. I believe that the authors should address the shortcomings I think hinder the paper's arguments and exposition of pre-existing work on the analysis of dynamic neural networks.", "rating": "7: Good paper, accept", "reply_text": "Q1 ) The taxonomy presented in the paper relies on an analysis of what the architectures can do , not what they can learn . I believe the authors should acknowledge that the presence of Long Range Dependence in sequences is still hard to capture by dynamic neural networks ( in particular RNNs ) and that alternate analysis have been proposed to understand the impact of the presence of such Long Range Dependence in the data on sequential learning . I believe that mentioning this issue along with older ( http : //ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf ) and more recent ( e.g.http : //proceedings.mlr.press/v84/belletti18a/belletti18a.pdf and https : //arxiv.org/pdf/1803.00144.pdf ) papers on the topic is necessary for the paper to present a holistic view of the matter at hand . We agree that the LRD problem is hard for RNNs , and this is the major reason external memories are needed , In the revised version , we have motivated the need for the taxonomy by the LRD problem and included these three papers for completeness . However , we would like to say that the main goal of our paper is indeed to explain what each architecture can learn from data , against your first observation . Based on this we further analyzed what they can do . From your comment we may have emphasized too much the aspect of how they can be used to help the practitioner . We have characterized what information each architecture can extract from the data stream in the revised manuscript . Q2 ) The arguments given in 5.2 are not most convincing and could benefit from a more thorough exposition , in particular for the sentiment analysis task . It is not clear enough in my view that it is true that `` since the goal is to classify the emotional tone as either 1 or 0 , the specific contents of the text are not very important here '' . One could argue that a single word in a sentence can change its meaning and sentiment . From our analysis of the results , all the models solve the sentiment analysis problem mainly based on the occurrence and reoccurrence of some discriminating words . Specifically , when feeding the input sequence to the model , the output value would be increased when meeting positive words such as \u201c good , love \u201d and decreased when meeting negative words such as \u201c dislike , boring \u201d . If there were many positive words appearing in text , the sentiment would be judged as positive . Since the model only cares about whether the word is positive or negative and its number of occurrences in the text ( kind of a \u201c density \u201d ) , we deduct that a specific word is not crucial ( for example , as long as it is a positive word , whether it is \u201c love \u201d , \u2019 like \u2019 or \u2018 happy \u2019 is not that important ) , and translated this as \u201c the specific contents of the text are not very important \u201d . We have elaborated more on this point in the final version because as the reviewer pointed out the explanation is too brief and not specific . But we have to be aware that this \u201c discriminating words based \u201d method does not really solve the problem as human . As the reviewer mentioned \u201c a single word in a sentence can change its meaning and sentiment \u201d , therefore , in order to really solve this problem , the machine should learn how to decode the text meaning . But none of the current models can really achieve this . Although these models can capture some temporal dependencies ( for example , if there is a \u201c don \u2019 t \u201d before \u201c like \u201d , the sentence is more likely to be negative ) , the final decision still mostly depends on how many \u201c discriminating words \u201d appear in the text . That \u2019 s also the reason why they can not get 100 % accuracy . Q3 ) The written could be more polished . We have further polished the language ."}, "1": {"review_id": "H1gRM2A5YX-1", "review_text": "Summary ========= The paper analyses the taxonomy over memory-based neural networks, in the decreasing order of capacity: Neural RAM to Neural Stack, Neural Stack to LSTM and LSTM to vanilla RNN. The experiments with synthetic and NLP datasets demonstrate the benefits of using models that fit with task types. Comment ======== Overall, the paper is well written and presents interesting analysis of different memory architectures. However, the contribution is rather limited. The proposed taxonomy is not new. It is a little bit obvious and mentioned before in [1] (Unfortunately, this was not cited in the manuscript). The theorems on inclusion relationship are also obvious and the main contribution of the paper is to formally show that in mathematical forms. The experiments on synthetic tasks give some insights into the models\u2019 operations, yet similar analyses can be found in [2, 3]. To verify the models really learn the task, the authors should include tests on unseen sequence lengths. There remains questions unexplained in NLP tasks such as why multi-slot memory did not show more advantages in Movie Review and why Neural Stack performed worse than LSTM in bAbI data. Minor potential errors: In Eq. (6), r_{t-1} should be r_t The LSTM presented in Section 3.2 is not the common one. Normally, there should be x_t term in Eq. (3) and h_t=g_{o,t}*\\tanh(r_t) in Eq. (6). The author should follow the common LSTM formulas (which may lead to different proofs) or include reference to their LSTM version. [1] Yogatama et al. Memory Architectures in Recurrent Neural Network Language Models. ICLR\u201918 [2] Joulin et al. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS\u201915 [3] Graves et al. Neural Turing Machines. arXiv preprint arXiv:1410.5401 (2014). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 ) The proposed taxonomy is not new . It is a little bit obvious and mentioned before in [ 1 ] ( Unfortunately , this was not cited in the manuscript ) . The theorems on inclusion relationship are also obvious and the main contribution of the paper is to formally show that in mathematical forms Thank you for mentioning [ 1 ] ( we have cited it in the revised version for completeness ) but we disagree that \u201c the proposed taxonomy is not new \u201d . The authors in [ 1 ] simply divided these models into sequential , random access and stack memory architectures , which bears some similarity with the taxonomy proposed in our paper , but it is more superficial and does not go to the mechanisms behind the memory types . Indeed , classifying models according to the type of memory seems obvious , but finding the essential relationship between classes and linking the descriptive power of learning machines to the properties of task data is not a trivial work . ( For example , what \u2019 s the difference between internal and external memory and what kind of tasks can they address ? Our taxonomy showed clearly that the gate mechanism in LSTM and the push/pop/no-op operators in the stack augmented memory had the same function in nature , which had never been mentioned before . Many discussions like these first appeared in our paper . ) Many papers proposed fancy models to improve the existing work , however there is still no paper providing a good approach to analyze what the memory architectures can learn and how to select the most parsimonious memory model for a specific task . As far as we know , our paper addresses for the first time how to codify and exploit the knowledge gained from the different characteristics of the memory within the taxonomy to help users select the type of memory network for an application . Moreover , the effectiveness of this analysis framework was also verified in some simple experiments . However , we agree that the analysis is not complete because on the one hand people has to analyze memory requirements of a task by themselves which is not trivial and on the other hand , the accuracy is also affected by the size and specific network structure even within a class . But we firmly believe that classifying architectures into these four classes and linking the architecture of the learning machine to its descriptive power , as we did in this paper , is a fundamental first step , and we think this paper is important to the machine learning community . We admit that the proofs of the theorems are not very hard and we included them to make our argument rigorous . Q2 ) The experiments on synthetic tasks give some insights into the models \u2019 operations , yet similar analyses can be found in [ 2 , 3 ] . Although [ 2 ] [ 3 ] and our paper used similar synthetic tasks in experiments , our goal was again very different . In [ 2 ] [ 3 ] , their goal was to show the effectiveness of their proposed architecture , so each of them only focused on analyzing the operation of one specific model . However , since our goal is to verify the proposed taxonomy , our experiment focused on showing the connections between different memory types and the growing capability of these four classes of models . For example , although [ 2 ] showed the details of the operation of neural stack and the neural stack performing better than LSTM on some tasks , we believe readers still won \u2019 t understand what was the connection between LSTM and neural stack and why LSTM could be seen as a special case of neural stack . However , in our \u201c counting with interference \u201d task , the results showed that ( Appendix D.2 ) content of the top element of the stack ( M0 in Fig.12 ) had the same changing trend as the external memory of LSTM ( M0 in Fig.11 ) and other content in the stack below the top one was redundant . Hence it helped verify our argument \u201c LSTM can be seen as neural stack with only the top element \u201d . Because of page limitation , we didn \u2019 t show how the three gates in LSTM related to the push/pop/no-op operators in neural stack , but our argument would be more convincing if these operator comparison results were added . We struggled to demonstrate the capabilities of each memory architecture , and our decision was to construct four representative tasks that would fit optimally the characteristics of each memory organization . Therefore , these four representative tasks were carefully selected to allow practitioners to compare their own problem with these four tasks and give them some hints to select the right model . This has been better explained in the revised paper , but the point is that we are not just simply repeating some existing experiments ."}, "2": {"review_id": "H1gRM2A5YX-2", "review_text": "The authors propose a review-style overview of memory systems within neural networks, from simple RNNs to stack-based memory architectures and NTM / MemNet-style architectures. They propose some reductions to imply how one model can be used (or modify) to simulate another. They then make predictions about which type of models should be best on different types of tasks. Unfortunately I did not find the paper particularly well written and the taxonomy was not illuminating for me. I actually felt, in the endeavor of creating a simple taxonomy the authors have created confusing simplifications, e.g. \"LSTM: state memory and memory of a single external event\" to me is mis-leading as we know an LSTM can compress many external events into its hidden units. Furthermore the taxonomy did not provide me with any new insights or display a prediction that was actually clairvoyant. I.e. it was clear from the outset that a memory network (say) will be much better at bAbI than a stack-augmented neural network. It would be more interesting to me, for example, if the paper could thus formalize why NTMs & DNCs (say) do not outperform LSTMs at language modeling, for example. I found the reductions somewhat shady, e.g. the RAM simulation of a stack is possible, however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots --- or unless it had O(N) thinking steps per time step, where N is the number of memory slots, so it's not a very realistic reduction. You would never see a memory network, for example, simulating a stack due to the fixed write-one-slot-per-timestep interface. Nit: I'm not sure the authors should be saying they 'developed' four synthetic tasks, when many of these tasks have previously been proposed and published (counting, copy, reverse copy). ", "rating": "3: Clear rejection", "reply_text": "The intent of our paper was to analyze the type of memory utilized by different architectures to solve sequence learning problems . This is not an easy issue because \u2018 memory \u2019 is a very abstract concept and the specific memory requirements for a specific task are implicit , which means that quantitatively conceptualizing and analyzing memory is a very hard problem . Cognitive scientists have defined many different types of memory , which shows the richness of the topic , and there are only a few engineering quantifiers of memory such as memory depth and memory resolution , but they are not enough for the ever-growing applications of machine learning . Hence memory quantification is lacking in the current machine learning literature and it is our main contribution . The proposed taxonomy for the four most conventional memory architectures appears as a simple way to quantify the capabilities of extracting past information of each class . Our goal of providing methodologies for the practitioner relegated to a second objective of the paper . It is clear from your questions that our writing was not successful , and we have modified the writing in the final submission to make this point more explicit . As far as we know , our paper addresses for the first time how to exploit the knowledge gained from the different characteristics of the memory within the taxonomy to help users select the type of memory network for an application . However , we agree that the analysis is not complete yet because on the one hand users have to analyze task \u2019 s memory requirements by themselves which is not trivial and on the other hand , the algorithm accuracy is also affected by the size and specific network structure even within a given class of models . But we firmly believe that classifying memory architectures into these four classes and linking the architecture of the learning machine to its descriptive power , as we did in this paper , is a fundamental first step . At least , in this respect , we think this paper is important to the machine learning community . Q1 ) I actually felt , in the endeavor of creating a simple taxonomy the authors have created confusing simplifications , e.g . `` LSTM : state memory and memory of a single external event '' to me is mis-leading as we know an LSTM can compress many external events into its hidden units . We agree with your statement and we are sorry for the misleading \u201c single event \u201d . By \u201c single event \u201d we mean that if there is only one useful event , it can be stored as it is , but if there are multiple useful events , they have to be compressed into one compounded event and can only be accessed as a whole . This has been clarified in the text . Q2 ) It would be more interesting to me , for example , if the paper could thus formalize why NTMs & DNCs ( say ) do not outperform LSTMs at language modeling , for example . Please see our reply to Q4 ) of the second reviewer . Q3 ) I found the reductions somewhat shady , e.g.the RAM simulation of a stack is possible , however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots -- - or unless it had O ( N ) thinking steps per time step , where N is the number of memory slots , so it 's not a very realistic reduction . You would never see a memory network , for example , simulating a stack due to the fixed write-one-slot-per-timestep interface . The purpose of deriving the reductions was to get some insights by comparing neural stack and neural RAM , we didn \u2019 t suggest using neural RAM simulating neural stack to solve a problem , and that is the reason why we said \u201c for the tasks where the previous memory needs to be addressed sequentially , the stack neural network is our first choice. \u201d Q4 ) Nit : I 'm not sure the authors should be saying they 'developed ' four synthetic tasks , when many of these tasks have previously been proposed and published ( counting , copy , reverse copy ) . We said \u2018 developed \u2019 because some of our experiments were not same as before . Our experiments were slightly revised to highlight the advantage and limitation of different memory types . For example , compared to the previous counting task , we added some interferences to the input sequences ( see details in our \u2018 counting with interference \u2019 task ) . By comparing vRNN \u2019 s performance on our \u2018 counting \u2019 and \u2018 counting with interference \u2019 task , the limitation of internal memory in vRNN was shown more clearly . But since our revision is less novel , we have changed \u2018 develop \u2019 to \u2018 select \u2019 ."}}