{"year": "2019", "forum": "S1eK3i09YQ", "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper proves that gradient descent with random initialization converges to global minima for a squared loss penalty over a two layer ReLU network and arbitrarily labeled data. The paper has several weakness such as, 1) assuming top layer is fixed, 2) large number of hidden units 'm', 3) analysis is for squared loss. Despite these weaknesses the paper makes a novel contribution to a relatively challenging problem, and is able to show convergence results without strong assumptions on the input data or the model. Reviewers find the results mostly interesting and have some concerns about the \\lambda_0 requirement. I believe the authors have sufficiently addressed this issue in their response and  I suggest acceptance. ", "reviews": [{"review_id": "S1eK3i09YQ-0", "review_text": "This work considers optimizing a two-layer over-parameterized ReLU network with the squared loss and given a data set with arbitrary labels. It is shown that for a sufficiently large number of hidden neurons (polynomially in number of samples) gradient descent converges to a global minimum with a linear convergence rate. The proof idea is to show that a certain Gram matrix of the data, which depends also on the weights, has a lower bounded minimum eigenvalue throughout the optimization process. Then, it is shown that this property implies convergence of gradient descent. This work is very interesting. Proving convergence of gradient descent for over-parameterized networks with ReLU activations and data with arbitrary labels is a major challenge. It is surprising that the authors found a relatively concise proof in the case of two-layer networks. The insight on the connection between the spectral properties of the Gram matrix and convergence of gradient descent is nice and seems to be a very promising technique for future work. One weakness of the result is the extremely large number of hidden neurons that are required to guarantee convergence. The paper is clearly written in most parts. The statement of Lemma 3.2 and its application appear to be incorrect as mentioned in the comments. I am convinced by the authors' response and the current proof that it can be fixed by defining an event which is independent of t. Moreover, I think it would be nice to include experiments that corroborate the theoretical findings. Specifically, it would be interesting to see if in practice most of the patterns of ReLUs do not change or if there is some other phenomenon. As mentioned in the comments, it would be good to add a discussion on the assumption of non-degeneracy of the H^{infty} matrix and include a proof (or exact reference) which shows under which conditions the minimum eigenvalue is positive. -------------Revision-------------- I disagree with most of the points that AnonReviewer3 raised (e.g., second layer fixed is not hard, contribution is limited). I do agree that the main weakness is the number of neurons. However, I think that the result is significant nonetheless. I did not change my original score. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank for your encouraging review . We have modified our paper according to your suggestions : \u2022 We fixed lemma 3.2 . \u2022 We added a new theorem ( Theorem 3.1 ) showing the non-degeneracy of H^ { \\infty } matrix . \u2022 We also added some experiments to corroborate our theoretical findings . Indeed , most of the patterns of ReLUs do not change . Furthermore , over-parameterization leads to faster convergence rate . We thank the reviewer again . We welcome all further comments !"}, {"review_id": "S1eK3i09YQ-1", "review_text": "This paper studies one hidden layer neural networks with square loss, where they show that in over-parameterized setting, random initialization + gradient descent gets to zero loss. The results depend on the property of data matrix, but not the output values. The high level idea of the proof is quite different from recent papers, and it would be quite interesting to see how powerful this is for deep neural nets, and whether any insights could help practitioners in the future. Some discussions regarding the results: I would suggest the authors to be specific about \u2018with high probability\u2019, whether it is 1-c, or 1-n^{-c}. The proof step using Markov\u2019s inequality gives 1-c probability, which is stated as \u2018with high probability\u2019. What about other \u2018high probability\u2019 statements? In the statement of Theorem 3.1 and 4.1, please add \u2018i.i.d.\u2019 (independence) for generating w_r s. The current statement of Lemma 3.2 is confusing. The authors state that given t, w.h.p. (let\u2019s say 0.9 for now) over initialization, the minimum eigenvalue is lower bounded. This does not imply, for example, that there exists an initialization, such that for 20 different t s, the minimum eigenvalue is lower bounded. The proof uses Markov\u2019s inequality for a single t. Therefore, I am slightly worried about its correctness. I hope the authors could address my concern. Also, in the proof of Lemma 3.2, (just to improve the readability,) I would suggest the authors to make it clear that the expectation is taken over the initialization of the weights. Some typos: \u2018converges\u2019 -> \u2018converges to\u2019 in the abstract \u2018close\u2019 -> \u2018close to\u2019 on page 5 \u2018a crucial\u2019 -> \u2018a crucial role\u2019 on page 5 In the proof of Lemma 3.2, x_0 should be x_i whether using boldface for H_{ij} should be consistent 'The next lemma shows we show' in page 6 'Markov inequality' -> \u2018Markov\u2019s inequality\u2019 \u2018a fixed a neural network architecture\u2019 in page 8 It is good to see other comments and discussions on this paper. I believe the authors will make a revision and I would be happy to see the new version of the paper and re-evaluate if some of my comments are not correct. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank for your careful review . We have modified our draft according to your suggestions : \u2022 We changed the statement of Lemma 3.2 , and now it is independent of t. \u2022 We have added more discussions on how to generalize our technique to analyze multiple layers . In the conclusion section , we have described a concrete plan for analysis . \u2022 For all theorems and lemmas , we have added failure probability and how the amount of over-parameterization depends on this failure probability . \u2022 We have fixed the typos . \u2022 We have modified the statement of Theorem 3.1 , 4.1 and the proof of Lemma 3.2 according to your suggestions . Regarding your question on how our insights could help practitioners in the future since we have characterized the convergence rate of gradient descent from the Gram matrix perspective , we believe our insights can inspire practitioners to design faster optimization algorithms from this perspective . We kindly ask you to read our revised paper and our response to common questions and re-evaluate your comments . We thank the reviewer again and welcome all further comments ."}, {"review_id": "S1eK3i09YQ-2", "review_text": "Additional Review This paper did NOT handle the non-differentiability and non-linearity very well. We can see this from the following three perspectives: 1. Proof idea: the proof of this paper is noisy version of the convergence analysis of a simple convex problem --it treats the contribution of the non-linearity and non-differentiability as bounded noise. 2. The network size is of order n^6. 3. Network size requirement is dependent on \\lambda_0. 1.Proof idea: The proof is essentially a noisy version of the convergence analysis of a linear regression problem provided in Appendix (at the end of this updated review). The only difference between linear regression and the problem in this paper is the changing patterns due to the non-linearity of ReLU. However, this paper views the changing patterns as noises compared to those unchanging patterns (e.g., S_i v.s. S_i^\\perpendicular). The key trick is that if the actual trajectory radius (i.e.,the largest deviation from the initial point) R\u2019 is much smaller than the desired trajectory radius R (given by a formula), then along the trajectory, the contribution of non-linearity is just O(n^2 R), which is small compared to the contribution of linearity, i.e., -\\lambda_0 (shown in proof on page 9). Following the above analysis, if the experiment shows that R\u2019 is really small compared to R, then the approach of treating non-linearity as noise is fine. However, it is not the case for the problem studied in the experiments (Sec 5, Fig 1). In figure 1, we can easily see that the maximum distance R\u2019 is O(1), which is far larger than R = c*\\lambda_0/n^2 =10^-6 when n=1k. Therefore, the proof idea used in this paper is fundamentally not able to explain the phenomenon shown in the experiment. In fact, to address this issue, authors need to consider significant contribution of non-linearity, instead of just viewing them as noises. 2. The network size is too large. This paper requires O(n^6) neurons, that is 10^18 neurons for n=1000 samples used in the experiment. The theoretical trick to make R\u2019< R is to note that R\u2019 can be bounded by O(1/sqrt{m}) while R is independent of m, thus picking a sufficiently large m can make R\u2019 very small. In a word, the reason that this paper requires so many neurons because of the inability of properly addressing non-linearity. 3. I found the dependence of the network size on the least eigenvalue funny, although the authors claim this tool is elegant. After authors add Thm 3.1 in the revision, I realize that the dependence on \\lambda_0 might come from the fact that authors do NOT handle the issue of non-differentiability. Let us see a simple example. Assume I have a dataset with \\lambda_0 = 1. Now I am adding one more data point (x=0_d, y=1) to the dataset. After adding this sample, \\lambda_0 clearly becomes 0. It seems I am just adding a constant 1 to the loss function and the gradient descent can also converge to the global min with a linear convergence rate since the constant does NOT contribution to the gradient. However, it seems the proof does NOT work. This is due to the fact that the \u201cgradient\u201d of the non-differentiable points are NOT well defined. Here is a simple example: h(w)=(y-ReLU(w*x))^2, where x= 0, y =1. By the definition provided in this paper (Eq.4), we can easily see that dh/dw = 1 for any w, even if h(w) = 1 for any w. This means that the constant can provide \u201cfake\u201d gradient information and make the maximum distance become infinity, (R\u2019=\\inf). Therefore, the whole proof collapses. In fact, changing the gradient definition from I{z>=0} to I{z>0} does not address the issue and we can see this from this example w=g(w)=Relu(w)-Relu(-w) has a zero gradient at w=0. In summary, the problem considered in this paper where the size m=O(n^6), maximum distance R\u2019= O(1/n^2) is too easy compared to most problems in practice where m=\\Theta(n), R\u2019=O(1). To address the latter problem, we need a better definition of subgradient and need to analyze the significant contribution of non-linearity and non-differentiability, instead of just viewing them as noises. =================================Appendix=============================== The proof basically follows from the convergence analysis of the following linear regression problem (note that u_j is fixed): \\min_{w_1,...,w_m}\\sum_{i=1}^{n}(f(x_i;w_1,...,w_m)-y_i)^2 = L(w_1,...,w_m) where f(x;w_1,...,w_m)=1/\\sqrt{m}\\sum_{j=1}^{m} a_j*(w_j^T x)*1{u_j^T x>=0} Gradient Descent Algorithm: -Initialization: -For each j=1,...,m: a_j ~ U({-1,1}), u_j~N(0, I) -Fix a_1,...,a_m, u_1,...,u_m -Update: -For t = 1,...,T w_j(t+1) = w_j(t) - \\eta* \\nabla_{w_j}L(w_1,...,w_m) for j=1,..., m. In this problem, since a_j and u_j are fixed, then model f is just a linear model w.r.t. w_j\u2019s and the above problem is just a simple linear regression problem. Therefore, it is not difficult to prove the linear convergence rate for the gradient descent for the above problem under some mild assumptions. Note that in this paper, u_j(t)= w_j(t) and are not fixed in iterations, i.e., patterns can change. ========================= First, I apologize to the authors and ACs for the late review, since this paper desearves much more time to judge the quality. Summary: This paper proves that the gradient descent/flow converges to the global optimum with zero training error under the settings (1) the neural network is a heavily over-parameterized ReLU network (i.e., requiting Omega(n^6) neurons); (2) the algorithm update rule \u201cignores\u201d the non-differentiable point; (3) the parameters in the output layer (i.e., a_i\u2019s) are fixed; (4) the data set has some non-degenerate properties and comes from a unit ball. The proof relies on the fact that the Gram matrix is always positive definite on the converging trajectory. Pros: The proof is simple and seems to be correct. The paper is paper is written clearly and easy to follow. Cons: The problem setting considered in this paper does not seem to be difficult enough. The difficulty of analyzing the landscape property of a ReLU network and proving the global convergence of the gradient descent mainly lies in the following three perspective and this paper does not try to tackle any one of them. First, it is very hard to characterize the landscape or the convergence trajectory at/ near the non-differentiable point and this paper fails to touch it. The parameter space is separated into several regions by the hyperplanes and the loss function is differentiable in the interior of each region and non-differentiable on the boundary. I believe the very first question authors need to answer is wether there are critical points on the boundary and why the sub-gradient descent escapes from any of these points. However, in this paper, authors avoid this problem by defining an update rule used in practice and this rule does not use the sub-gradient at the non-differentiable point. Thus, it is totally unclear to me wether this global convergence result comes from the fact that this update rule can generally avoid the non-differentiable points on the boundary or the fact that the landscape is so nice such that there are no critical points on the boundary or the fact that all points on the convergence trajectory is differentiable only in this unique problem. Second, the problem is much easier if the loss is not jointly optimized over the parameters in the first and second layer. Having parameters in one layer fixed does not seem to be a big problem at first glance, but then I realize it indeed makes the problem much easier, which can be seen in the following example. If we randomly sample the weight vector w_i from N(0, I) and only optimize over the parameters in the second layer, then it is straightforward to show the following result. Result: If \\lambda_\\min(H^\\inf)>0 and m=\\Omega(n\\log n), then with high probability, the loss function L is strongly convex with respect to a=(a_1,\u2026, a_m) and the loss function is zero at the global minimum. The above result shows that if we fix the parameters in the first layer and only optimize the parameters in the second layer, it is easy to prove the global convergence with a linear convergence rate. In fact, this result does not require the samples coming from a unit ball and the network size is only slightly over-parameterized. Therefore, if we are allowed to fix the parameters in some layer, how are the result presented in this paper fundamentally different from the above result. Authors may say that the loss is not convex with respect to the weights in the first layer even if the second layer is fixed. However, when the second layer is fixed, the loss function is smooth and convex in each parameter region and some recent works have shown that in this case, the loss function is a weakly global function. This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin, the gradient descent is able to converge to a global min. However, the problem becomes far more difficult if the loss is jointly optimized over all parameters in the first and second layer. This can be easily seen since in each parameter region, the loss is no longer a convex function and this may lead to some high order saddle points such that the gradient descent cannot provably escape. Furthermore, the critical points on the boundary can be much more difficult to characterize for this joint optimization problem. Third, the dataset considered in this paper does not seem to be a fundamental pattern and it seems more like a technical condition required by the proof. It is easy to see that a linearly separable dataset does not necessarily satisfy the conditions that 1) the gram matrix is positive definite and that 2) samples come from the surface of a unit ball. Therefore, I do not understand the reason why we need to analyze this pattern. Clearly, in practice, the data samples is unlikely sampled from a ball surface and it is totally unclear to me why the gram matrix is necessarily positive definite. I understand that some technical assumptions are needed in a theoretical work, but I would like to see more discussions on the dataset, e.g., some necessary conditions on the dataset such that the global convergence is possible. Last, I understand that the over-parameterization assumption is needed. In fact, I expect the network size to be of the order Omega(n*ploylog(n)). I am wondering wether Omega(n^6) is a necessary condition or wether there exists a case such that Theta(n^6) is required. Above all, I believe this paper is a half-baked paper with some interesting explorations. In summary, it cannot deal with non-differential points, which is considered a major difficulty for analyzing ReLU. In addition, it makes an un-justified assumption on some matrix, it requires too many neurons, and fixed 2nd layer. With so many strong assumptions, and compared to related works like [1], Mei et al., Bach and ..., its contribution is rather limited. [1] https://arxiv.org/abs/1702.05777 ", "rating": "3: Clear rejection", "reply_text": "Thank for your long review . Unfortunately , we disagree with most of your comments . First , we would like to point out two wrong statements in your review . First , the \u201c result \u201d you claim is wrong . If the first layer is fixed and m = \\Omega ( n \\logn ) , and only a= ( a_1 , \u2026 , a_m ) is being optimized , this is a linear regression problem with respect to a= ( a_1 , \u2026 , a_m ) . Since m > n , this problem has more features than the number of samples , and the covariance matrix ( Hessian ) is degenerate . There is no way this problem is a strongly convex one . Second , you claimed there exists a linearly separable dataset whose corresponding H^ { \\infty } is degenerate . However , we are considering a regression problem whereas linearly separable condition is only a favorable condition for classification problems . We don \u2019 t understand what does linearly separable mean for regression . Now regarding your main complaint that the problem is not difficult enough : 1 . This is not true at all . Reviewer # 1 and Reviewer # 2 both explicitly agreed this is a challenging/difficult problem and we have devoted a whole paragraph ( second paragraph on page 2 ) and many sentences in Section 2 to describe the difficulty . 2.You complained that we are not analyzing the landscape of this non-differentiable function and we are using the \u201c practically used update rule instead of subgradient. \u201d We don \u2019 t understand the point here . Our primary goal is to understand why practically used rule ( gradient descent ) can achieve zero training loss . We have stated our goal at the beginning of the abstract and the introduction . For the non-differentiability issue , in the revised version we have cited papers and added discussions in the fourth paragraph of Section 2 on recent progress in dealing with non-differentiability . 3.You claimed fixing one layer and optimizing the other one is a trivial problem . We agree if one fixes the first layer and optimizes the output layer , then this is trivial because this is a convex problem . However , if one fixes the output layer and optimizes the first layer , the problem is significantly harder . You claimed in this case \u201c the loss function is a weakly global function . This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin , the gradient descent is able to converge to a global min . \u201d We kindly ask for a reference and why it can imply the global convergence of gradient descent analyzed in our paper . To our knowledge , none of the previous results implies the global convergence of gradient descent in the setting we are analyzing . We have discussed this point in Section 2 . Furthermore , we have never heard of the notion \u201c weakly global function \u201d . 4.You believed that the inputs are generated from a unit sphere is a strong assumption . In our original version , we said making this assumption is only for simplicity . In our revised version , we added more details on this assumption . Please check footnote 7 . 5.For your other concerns , we kindly ask you to read our response to common questions . We thank the reviewer again . We welcome all further comments !"}, {"review_id": "S1eK3i09YQ-3", "review_text": "This paper studies convergence of gradient descent on a two-layer fully connected ReLU network with binary output and square loss. The main result is that if the number of hidden units is polynomially large in terms of the number of training samples, then under suitable randomly initialization conditions and given that the output weights are fixed, gradient descent necessarily converge to zero training loss. Pros: The paper is presented clearly enough, but I still urge the authors to carefully check for typos and grammatical mistakes as they revise the paper. As far as I have checked, the proofs are correct. The analysis is quite simple and elegant. This is one thing that I really like about this paper compared to previous work. Cons: The current setting and conditions for the main result to hold are quite a bit limited. If one has polynomially large number of neurons (i.e. on the order of n^6 where n is number of training samples) as stated in the paper, then the weights of the hidden layer can be easily chosen so that the outputs of all training samples become linearly independent in the hidden layer (see e.g. [1] for the construction, which requires only n neurons even with weight sharing) , and thus fixing these weights and optimizing for the output weights would lead directly to a convex problem with the same theoretical guarantee. At this point, it would be good to explain why this paper is focusing on the opposite setting, namely fixing the output weights and learning just the hidden layer weights, because it seems that this just makes the problem become more non-trivial compared to the previous case while yielding almost the same results . Either way, this is not the way how practical neural networks are trained as only a subset of the weights are optimized. Thus it's hard to conclude from here why the commonly used GD w.r.t. all variables converges to zero loss as stated in the abstract. The condition on the Gram matrix H_infty in Theorem 3.1 seems to be critical. I would like to see the proof that this condition can be fulfilled under certain conditions on the training data. In Lemma 3.1, it seems that \"log^2(n/delta)\" should be \"log(n^2/delta)\"? Despite the above limitations, I think that the analysis in this paper is still interesting (mainly due to its simplicity) from a theoretical perspective. Given the difficulty of the problem, I'm happy to vote for its acceptance. [1] Optimization landscape and expressivity of deep CNNs", "rating": "7: Good paper, accept", "reply_text": "We thank for your careful and encouraging review . We believe our revised version has addressed most of your concerns . 1.We have added discussions on the problem of fixing the first layer and only training the output layer in footnote 3 . We believe the learned function is different from the function learned by fixing the output layer and only training the first layer . We would also like to point out that many previous papers considered the same setting but did not rigorously prove the global convergence of gradient descent . 2.We have added a new theorem ( Theorem 3.3 ) which shows applying gradient flow to optimize all variables still enjoys a linear convergence rate . To prove Theorem 3.3 , we use the same arguments as we used to prove Theorem 3.1 with slightly more calculations . Therefore , we have shown analyzing the case that both layers are trained is just as hard as analyzing the case where only the first layer is trained . 3.We have added a new theorem ( Theorem 3.1 ) which shows as long as no two inputs are parallel , H^ { \\infty } is non-degenerate . We thank the reviewer again . We welcome all further comments !"}], "0": {"review_id": "S1eK3i09YQ-0", "review_text": "This work considers optimizing a two-layer over-parameterized ReLU network with the squared loss and given a data set with arbitrary labels. It is shown that for a sufficiently large number of hidden neurons (polynomially in number of samples) gradient descent converges to a global minimum with a linear convergence rate. The proof idea is to show that a certain Gram matrix of the data, which depends also on the weights, has a lower bounded minimum eigenvalue throughout the optimization process. Then, it is shown that this property implies convergence of gradient descent. This work is very interesting. Proving convergence of gradient descent for over-parameterized networks with ReLU activations and data with arbitrary labels is a major challenge. It is surprising that the authors found a relatively concise proof in the case of two-layer networks. The insight on the connection between the spectral properties of the Gram matrix and convergence of gradient descent is nice and seems to be a very promising technique for future work. One weakness of the result is the extremely large number of hidden neurons that are required to guarantee convergence. The paper is clearly written in most parts. The statement of Lemma 3.2 and its application appear to be incorrect as mentioned in the comments. I am convinced by the authors' response and the current proof that it can be fixed by defining an event which is independent of t. Moreover, I think it would be nice to include experiments that corroborate the theoretical findings. Specifically, it would be interesting to see if in practice most of the patterns of ReLUs do not change or if there is some other phenomenon. As mentioned in the comments, it would be good to add a discussion on the assumption of non-degeneracy of the H^{infty} matrix and include a proof (or exact reference) which shows under which conditions the minimum eigenvalue is positive. -------------Revision-------------- I disagree with most of the points that AnonReviewer3 raised (e.g., second layer fixed is not hard, contribution is limited). I do agree that the main weakness is the number of neurons. However, I think that the result is significant nonetheless. I did not change my original score. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank for your encouraging review . We have modified our paper according to your suggestions : \u2022 We fixed lemma 3.2 . \u2022 We added a new theorem ( Theorem 3.1 ) showing the non-degeneracy of H^ { \\infty } matrix . \u2022 We also added some experiments to corroborate our theoretical findings . Indeed , most of the patterns of ReLUs do not change . Furthermore , over-parameterization leads to faster convergence rate . We thank the reviewer again . We welcome all further comments !"}, "1": {"review_id": "S1eK3i09YQ-1", "review_text": "This paper studies one hidden layer neural networks with square loss, where they show that in over-parameterized setting, random initialization + gradient descent gets to zero loss. The results depend on the property of data matrix, but not the output values. The high level idea of the proof is quite different from recent papers, and it would be quite interesting to see how powerful this is for deep neural nets, and whether any insights could help practitioners in the future. Some discussions regarding the results: I would suggest the authors to be specific about \u2018with high probability\u2019, whether it is 1-c, or 1-n^{-c}. The proof step using Markov\u2019s inequality gives 1-c probability, which is stated as \u2018with high probability\u2019. What about other \u2018high probability\u2019 statements? In the statement of Theorem 3.1 and 4.1, please add \u2018i.i.d.\u2019 (independence) for generating w_r s. The current statement of Lemma 3.2 is confusing. The authors state that given t, w.h.p. (let\u2019s say 0.9 for now) over initialization, the minimum eigenvalue is lower bounded. This does not imply, for example, that there exists an initialization, such that for 20 different t s, the minimum eigenvalue is lower bounded. The proof uses Markov\u2019s inequality for a single t. Therefore, I am slightly worried about its correctness. I hope the authors could address my concern. Also, in the proof of Lemma 3.2, (just to improve the readability,) I would suggest the authors to make it clear that the expectation is taken over the initialization of the weights. Some typos: \u2018converges\u2019 -> \u2018converges to\u2019 in the abstract \u2018close\u2019 -> \u2018close to\u2019 on page 5 \u2018a crucial\u2019 -> \u2018a crucial role\u2019 on page 5 In the proof of Lemma 3.2, x_0 should be x_i whether using boldface for H_{ij} should be consistent 'The next lemma shows we show' in page 6 'Markov inequality' -> \u2018Markov\u2019s inequality\u2019 \u2018a fixed a neural network architecture\u2019 in page 8 It is good to see other comments and discussions on this paper. I believe the authors will make a revision and I would be happy to see the new version of the paper and re-evaluate if some of my comments are not correct. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank for your careful review . We have modified our draft according to your suggestions : \u2022 We changed the statement of Lemma 3.2 , and now it is independent of t. \u2022 We have added more discussions on how to generalize our technique to analyze multiple layers . In the conclusion section , we have described a concrete plan for analysis . \u2022 For all theorems and lemmas , we have added failure probability and how the amount of over-parameterization depends on this failure probability . \u2022 We have fixed the typos . \u2022 We have modified the statement of Theorem 3.1 , 4.1 and the proof of Lemma 3.2 according to your suggestions . Regarding your question on how our insights could help practitioners in the future since we have characterized the convergence rate of gradient descent from the Gram matrix perspective , we believe our insights can inspire practitioners to design faster optimization algorithms from this perspective . We kindly ask you to read our revised paper and our response to common questions and re-evaluate your comments . We thank the reviewer again and welcome all further comments ."}, "2": {"review_id": "S1eK3i09YQ-2", "review_text": "Additional Review This paper did NOT handle the non-differentiability and non-linearity very well. We can see this from the following three perspectives: 1. Proof idea: the proof of this paper is noisy version of the convergence analysis of a simple convex problem --it treats the contribution of the non-linearity and non-differentiability as bounded noise. 2. The network size is of order n^6. 3. Network size requirement is dependent on \\lambda_0. 1.Proof idea: The proof is essentially a noisy version of the convergence analysis of a linear regression problem provided in Appendix (at the end of this updated review). The only difference between linear regression and the problem in this paper is the changing patterns due to the non-linearity of ReLU. However, this paper views the changing patterns as noises compared to those unchanging patterns (e.g., S_i v.s. S_i^\\perpendicular). The key trick is that if the actual trajectory radius (i.e.,the largest deviation from the initial point) R\u2019 is much smaller than the desired trajectory radius R (given by a formula), then along the trajectory, the contribution of non-linearity is just O(n^2 R), which is small compared to the contribution of linearity, i.e., -\\lambda_0 (shown in proof on page 9). Following the above analysis, if the experiment shows that R\u2019 is really small compared to R, then the approach of treating non-linearity as noise is fine. However, it is not the case for the problem studied in the experiments (Sec 5, Fig 1). In figure 1, we can easily see that the maximum distance R\u2019 is O(1), which is far larger than R = c*\\lambda_0/n^2 =10^-6 when n=1k. Therefore, the proof idea used in this paper is fundamentally not able to explain the phenomenon shown in the experiment. In fact, to address this issue, authors need to consider significant contribution of non-linearity, instead of just viewing them as noises. 2. The network size is too large. This paper requires O(n^6) neurons, that is 10^18 neurons for n=1000 samples used in the experiment. The theoretical trick to make R\u2019< R is to note that R\u2019 can be bounded by O(1/sqrt{m}) while R is independent of m, thus picking a sufficiently large m can make R\u2019 very small. In a word, the reason that this paper requires so many neurons because of the inability of properly addressing non-linearity. 3. I found the dependence of the network size on the least eigenvalue funny, although the authors claim this tool is elegant. After authors add Thm 3.1 in the revision, I realize that the dependence on \\lambda_0 might come from the fact that authors do NOT handle the issue of non-differentiability. Let us see a simple example. Assume I have a dataset with \\lambda_0 = 1. Now I am adding one more data point (x=0_d, y=1) to the dataset. After adding this sample, \\lambda_0 clearly becomes 0. It seems I am just adding a constant 1 to the loss function and the gradient descent can also converge to the global min with a linear convergence rate since the constant does NOT contribution to the gradient. However, it seems the proof does NOT work. This is due to the fact that the \u201cgradient\u201d of the non-differentiable points are NOT well defined. Here is a simple example: h(w)=(y-ReLU(w*x))^2, where x= 0, y =1. By the definition provided in this paper (Eq.4), we can easily see that dh/dw = 1 for any w, even if h(w) = 1 for any w. This means that the constant can provide \u201cfake\u201d gradient information and make the maximum distance become infinity, (R\u2019=\\inf). Therefore, the whole proof collapses. In fact, changing the gradient definition from I{z>=0} to I{z>0} does not address the issue and we can see this from this example w=g(w)=Relu(w)-Relu(-w) has a zero gradient at w=0. In summary, the problem considered in this paper where the size m=O(n^6), maximum distance R\u2019= O(1/n^2) is too easy compared to most problems in practice where m=\\Theta(n), R\u2019=O(1). To address the latter problem, we need a better definition of subgradient and need to analyze the significant contribution of non-linearity and non-differentiability, instead of just viewing them as noises. =================================Appendix=============================== The proof basically follows from the convergence analysis of the following linear regression problem (note that u_j is fixed): \\min_{w_1,...,w_m}\\sum_{i=1}^{n}(f(x_i;w_1,...,w_m)-y_i)^2 = L(w_1,...,w_m) where f(x;w_1,...,w_m)=1/\\sqrt{m}\\sum_{j=1}^{m} a_j*(w_j^T x)*1{u_j^T x>=0} Gradient Descent Algorithm: -Initialization: -For each j=1,...,m: a_j ~ U({-1,1}), u_j~N(0, I) -Fix a_1,...,a_m, u_1,...,u_m -Update: -For t = 1,...,T w_j(t+1) = w_j(t) - \\eta* \\nabla_{w_j}L(w_1,...,w_m) for j=1,..., m. In this problem, since a_j and u_j are fixed, then model f is just a linear model w.r.t. w_j\u2019s and the above problem is just a simple linear regression problem. Therefore, it is not difficult to prove the linear convergence rate for the gradient descent for the above problem under some mild assumptions. Note that in this paper, u_j(t)= w_j(t) and are not fixed in iterations, i.e., patterns can change. ========================= First, I apologize to the authors and ACs for the late review, since this paper desearves much more time to judge the quality. Summary: This paper proves that the gradient descent/flow converges to the global optimum with zero training error under the settings (1) the neural network is a heavily over-parameterized ReLU network (i.e., requiting Omega(n^6) neurons); (2) the algorithm update rule \u201cignores\u201d the non-differentiable point; (3) the parameters in the output layer (i.e., a_i\u2019s) are fixed; (4) the data set has some non-degenerate properties and comes from a unit ball. The proof relies on the fact that the Gram matrix is always positive definite on the converging trajectory. Pros: The proof is simple and seems to be correct. The paper is paper is written clearly and easy to follow. Cons: The problem setting considered in this paper does not seem to be difficult enough. The difficulty of analyzing the landscape property of a ReLU network and proving the global convergence of the gradient descent mainly lies in the following three perspective and this paper does not try to tackle any one of them. First, it is very hard to characterize the landscape or the convergence trajectory at/ near the non-differentiable point and this paper fails to touch it. The parameter space is separated into several regions by the hyperplanes and the loss function is differentiable in the interior of each region and non-differentiable on the boundary. I believe the very first question authors need to answer is wether there are critical points on the boundary and why the sub-gradient descent escapes from any of these points. However, in this paper, authors avoid this problem by defining an update rule used in practice and this rule does not use the sub-gradient at the non-differentiable point. Thus, it is totally unclear to me wether this global convergence result comes from the fact that this update rule can generally avoid the non-differentiable points on the boundary or the fact that the landscape is so nice such that there are no critical points on the boundary or the fact that all points on the convergence trajectory is differentiable only in this unique problem. Second, the problem is much easier if the loss is not jointly optimized over the parameters in the first and second layer. Having parameters in one layer fixed does not seem to be a big problem at first glance, but then I realize it indeed makes the problem much easier, which can be seen in the following example. If we randomly sample the weight vector w_i from N(0, I) and only optimize over the parameters in the second layer, then it is straightforward to show the following result. Result: If \\lambda_\\min(H^\\inf)>0 and m=\\Omega(n\\log n), then with high probability, the loss function L is strongly convex with respect to a=(a_1,\u2026, a_m) and the loss function is zero at the global minimum. The above result shows that if we fix the parameters in the first layer and only optimize the parameters in the second layer, it is easy to prove the global convergence with a linear convergence rate. In fact, this result does not require the samples coming from a unit ball and the network size is only slightly over-parameterized. Therefore, if we are allowed to fix the parameters in some layer, how are the result presented in this paper fundamentally different from the above result. Authors may say that the loss is not convex with respect to the weights in the first layer even if the second layer is fixed. However, when the second layer is fixed, the loss function is smooth and convex in each parameter region and some recent works have shown that in this case, the loss function is a weakly global function. This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin, the gradient descent is able to converge to a global min. However, the problem becomes far more difficult if the loss is jointly optimized over all parameters in the first and second layer. This can be easily seen since in each parameter region, the loss is no longer a convex function and this may lead to some high order saddle points such that the gradient descent cannot provably escape. Furthermore, the critical points on the boundary can be much more difficult to characterize for this joint optimization problem. Third, the dataset considered in this paper does not seem to be a fundamental pattern and it seems more like a technical condition required by the proof. It is easy to see that a linearly separable dataset does not necessarily satisfy the conditions that 1) the gram matrix is positive definite and that 2) samples come from the surface of a unit ball. Therefore, I do not understand the reason why we need to analyze this pattern. Clearly, in practice, the data samples is unlikely sampled from a ball surface and it is totally unclear to me why the gram matrix is necessarily positive definite. I understand that some technical assumptions are needed in a theoretical work, but I would like to see more discussions on the dataset, e.g., some necessary conditions on the dataset such that the global convergence is possible. Last, I understand that the over-parameterization assumption is needed. In fact, I expect the network size to be of the order Omega(n*ploylog(n)). I am wondering wether Omega(n^6) is a necessary condition or wether there exists a case such that Theta(n^6) is required. Above all, I believe this paper is a half-baked paper with some interesting explorations. In summary, it cannot deal with non-differential points, which is considered a major difficulty for analyzing ReLU. In addition, it makes an un-justified assumption on some matrix, it requires too many neurons, and fixed 2nd layer. With so many strong assumptions, and compared to related works like [1], Mei et al., Bach and ..., its contribution is rather limited. [1] https://arxiv.org/abs/1702.05777 ", "rating": "3: Clear rejection", "reply_text": "Thank for your long review . Unfortunately , we disagree with most of your comments . First , we would like to point out two wrong statements in your review . First , the \u201c result \u201d you claim is wrong . If the first layer is fixed and m = \\Omega ( n \\logn ) , and only a= ( a_1 , \u2026 , a_m ) is being optimized , this is a linear regression problem with respect to a= ( a_1 , \u2026 , a_m ) . Since m > n , this problem has more features than the number of samples , and the covariance matrix ( Hessian ) is degenerate . There is no way this problem is a strongly convex one . Second , you claimed there exists a linearly separable dataset whose corresponding H^ { \\infty } is degenerate . However , we are considering a regression problem whereas linearly separable condition is only a favorable condition for classification problems . We don \u2019 t understand what does linearly separable mean for regression . Now regarding your main complaint that the problem is not difficult enough : 1 . This is not true at all . Reviewer # 1 and Reviewer # 2 both explicitly agreed this is a challenging/difficult problem and we have devoted a whole paragraph ( second paragraph on page 2 ) and many sentences in Section 2 to describe the difficulty . 2.You complained that we are not analyzing the landscape of this non-differentiable function and we are using the \u201c practically used update rule instead of subgradient. \u201d We don \u2019 t understand the point here . Our primary goal is to understand why practically used rule ( gradient descent ) can achieve zero training loss . We have stated our goal at the beginning of the abstract and the introduction . For the non-differentiability issue , in the revised version we have cited papers and added discussions in the fourth paragraph of Section 2 on recent progress in dealing with non-differentiability . 3.You claimed fixing one layer and optimizing the other one is a trivial problem . We agree if one fixes the first layer and optimizes the output layer , then this is trivial because this is a convex problem . However , if one fixes the output layer and optimizes the first layer , the problem is significantly harder . You claimed in this case \u201c the loss function is a weakly global function . This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin , the gradient descent is able to converge to a global min . \u201d We kindly ask for a reference and why it can imply the global convergence of gradient descent analyzed in our paper . To our knowledge , none of the previous results implies the global convergence of gradient descent in the setting we are analyzing . We have discussed this point in Section 2 . Furthermore , we have never heard of the notion \u201c weakly global function \u201d . 4.You believed that the inputs are generated from a unit sphere is a strong assumption . In our original version , we said making this assumption is only for simplicity . In our revised version , we added more details on this assumption . Please check footnote 7 . 5.For your other concerns , we kindly ask you to read our response to common questions . We thank the reviewer again . We welcome all further comments !"}, "3": {"review_id": "S1eK3i09YQ-3", "review_text": "This paper studies convergence of gradient descent on a two-layer fully connected ReLU network with binary output and square loss. The main result is that if the number of hidden units is polynomially large in terms of the number of training samples, then under suitable randomly initialization conditions and given that the output weights are fixed, gradient descent necessarily converge to zero training loss. Pros: The paper is presented clearly enough, but I still urge the authors to carefully check for typos and grammatical mistakes as they revise the paper. As far as I have checked, the proofs are correct. The analysis is quite simple and elegant. This is one thing that I really like about this paper compared to previous work. Cons: The current setting and conditions for the main result to hold are quite a bit limited. If one has polynomially large number of neurons (i.e. on the order of n^6 where n is number of training samples) as stated in the paper, then the weights of the hidden layer can be easily chosen so that the outputs of all training samples become linearly independent in the hidden layer (see e.g. [1] for the construction, which requires only n neurons even with weight sharing) , and thus fixing these weights and optimizing for the output weights would lead directly to a convex problem with the same theoretical guarantee. At this point, it would be good to explain why this paper is focusing on the opposite setting, namely fixing the output weights and learning just the hidden layer weights, because it seems that this just makes the problem become more non-trivial compared to the previous case while yielding almost the same results . Either way, this is not the way how practical neural networks are trained as only a subset of the weights are optimized. Thus it's hard to conclude from here why the commonly used GD w.r.t. all variables converges to zero loss as stated in the abstract. The condition on the Gram matrix H_infty in Theorem 3.1 seems to be critical. I would like to see the proof that this condition can be fulfilled under certain conditions on the training data. In Lemma 3.1, it seems that \"log^2(n/delta)\" should be \"log(n^2/delta)\"? Despite the above limitations, I think that the analysis in this paper is still interesting (mainly due to its simplicity) from a theoretical perspective. Given the difficulty of the problem, I'm happy to vote for its acceptance. [1] Optimization landscape and expressivity of deep CNNs", "rating": "7: Good paper, accept", "reply_text": "We thank for your careful and encouraging review . We believe our revised version has addressed most of your concerns . 1.We have added discussions on the problem of fixing the first layer and only training the output layer in footnote 3 . We believe the learned function is different from the function learned by fixing the output layer and only training the first layer . We would also like to point out that many previous papers considered the same setting but did not rigorously prove the global convergence of gradient descent . 2.We have added a new theorem ( Theorem 3.3 ) which shows applying gradient flow to optimize all variables still enjoys a linear convergence rate . To prove Theorem 3.3 , we use the same arguments as we used to prove Theorem 3.1 with slightly more calculations . Therefore , we have shown analyzing the case that both layers are trained is just as hard as analyzing the case where only the first layer is trained . 3.We have added a new theorem ( Theorem 3.1 ) which shows as long as no two inputs are parallel , H^ { \\infty } is non-degenerate . We thank the reviewer again . We welcome all further comments !"}}