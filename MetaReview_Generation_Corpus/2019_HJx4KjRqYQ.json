{"year": "2019", "forum": "HJx4KjRqYQ", "title": "Ergodic Measure Preserving Flows", "decision": "Reject", "meta_review": "This paper proposes to a simple method for  tuning parameters of HMC by maximizing the log density under the final sample of the MCMC, and apply it for training VAE. The reviews and discussion raises some critical concerns and questions, which unfortunately, which unfortunately, is not adequately addressed. ", "reviews": [{"review_id": "HJx4KjRqYQ-0", "review_text": "This paper proposes a simple heuristic for tuning HMC's parameters: just optimize the expected log-density of the Lth sample. It seems to work reasonably well on the problems the authors evaluate on. This heuristic is arrived at by a somewhat roundabout derivation, which I found interesting (although many of the same ideas are implicit in Salimans et al. (2014; \"MCMC & VI: Bridging the Gap\")). But ultimately this derivation comes to a head at this very heuristic argument: \u201c\u2026since qL(zL; \u03c6) converges to p\u03b8(z|x) as L increases, we expect the effect of H[qL(zL; \u03c6)] on \u03c6 to be small and that most of the similarity of qL(zL; \u03c6) to p\u03b8(z|x) will be captured by the first term in the RHS of (15). Therefore, we propose to tune \u03c6 by optimizing the tractable objective given by the first term\u2026\u201d: I don\u2019t see why this argument applies to the entropy term and not to the log-joint term. In particular, If q_L has really converged to p(z|x), then there\u2019s no point optimizing \u03c6 either, right? Here\u2019s a concrete example of how I could imagine this procedure going wrong: make q(z0) a delta at the latent vector z* that maximizes the log-joint, and set the step size of the Hamiltonian simulation to 0. This will make the entropy term (which is ignored) -\u221e, maximize the log-joint term, and I think it even makes D^L_{KL}=0. It seems like this isn\u2019t what actually happens experimentally, though\u2014perhaps I\u2019m missing something? Regarding the experiments, a natural baseline would be something akin to the approach of Hoffman (2017; \"Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo\u201d), simply initializing the HMC sampler with a mean-field Gaussian. I would expect this to produce worse results for small numbers of steps, since the variational Gaussian would choose a single mode, but I\u2019m curious how the quantitative metrics would compare. Some more minor points: * \u201cVariational auto-encoders (VAEs) (Kingma & Welling, 2014) are DGMs trained by using mean-field VI with a Gaussian parametric distribuion and amortization.\u201d I disagree with this terminology\u2014DGMs trained with, say, IAF are routinely called VAEs. * Section 3.1: It might be good to clarify that you\u2019re describing exact Hamiltonian integration, whereas in practice one always uses a discretized numerical integrator. (The leapfrog integrator is reversible and preserves volume, but doesn\u2019t conserve energy, so this does make the results a bit more complicated.) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , we would like to thank the reviewer 's effort . `` Here \u2019 s a concrete example of how I could imagine this procedure going wrong : make q ( z0 ) a delta at the latent vector z * that maximizes the log-joint , and set the step size of the Hamiltonian simulation to 0 . This will make the entropy term ( which is ignored ) -\u221e , maximize the log-joint term , and I think it even makes D^L_ { KL } =0 . It seems like this isn \u2019 t what actually happens experimentally , though\u2014perhaps I \u2019 m missing something ? '' Perhaps we can clarify with the following points : 1 ) The irreducibility of ergodic Markov kernel ( not in the paper ) 2 ) HMC step size can never be 0 otherwise the kernel is not irreducible therefore not ergodic ( not in the paper ) 3 ) Our precondition ( under eq ( 16 ) ) for dropping the entropy term ( in the paper ) First , a Markov chain is ergodic must satisfy following condition of irreducibility ( precondition of recurrence ) : the probability of any state change to any other state must be positive . Second , if the simulation time/discretized step size in HMC is zero , then the state of Hamiltonian dynamics simply do not change . Therefore , the input and output of Hamiltonian simulation is the same with probability 1 . This breaks the condition of ergodicity : irreducibility . Finally , we made it clear that there is a precondition for ignoring the entropy term : `` If with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' ( * right under * the definition of the loss without entropy ( equation 16 ) ) We are happy to make this more connected with our motivation of ignoring the entropy term . It is not too hard to see why this precondition makes sense . In case of that we want to maximise this loss to converge to the target , then the initial loss value should be lower than the loss value of the target , otherwise maximising the loss will lead to diverge . On the comment : `` Regarding the experiments , a natural baseline would be something akin to the approach of Hoffman ( 2017 ; `` Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo \u201d ) , ... '' This is a fair point . We do have experiment results with this paper , but we did n't include it because Hoffman 's method is much worse than VAE with the same amount of computation time . We are happy to add this results in , if the paper is accepted ."}, {"review_id": "HJx4KjRqYQ-1", "review_text": "This paper presents an approximate bayesian inference method based on chaining measure preserving transformation with trainable parameters and optimizing for those using an ad-hoc objective based on a lower-bound on the likelihood. The paper is clearly written and easy to follow. The proofs seem correct. In terms of methods, I still have major questions: - The whole premise of the paper is based on chaining transformations that preserve the target density. However, in practice, you use a leapfrog operator without the Metropolis-Hastings step --what happens to the theoretical guarantees in that case? I'm guessing Eq (8), (9) don't hold anymore and neither does Theorem 1. - When swapping L for F, could you provide more justifications? You use the argument that p(z|x) ~= q_L so the effect of the entropy term will be negligible. It seems that if they are so similar for large L, why even train the \\phi? It also comes back to my first point that in your experiment, the transformations *do not* preserve the target density. - Regarding the use of measure preserving flow, I think it can be quite hurtful in certain settings -- a very simple example would be a mixture of two gaussians with vastly different variance. I think this paper also lacks recent references on training parameters for MCMC algorithms, most notably Song et al. (2017) and Levy et al. (2018). Both of these work seem quite related and should be mentioned and compared to. I would have also liked to see the authors contrast their work with Salimans (2015), especially the HVI part; is the main difference the reverse model? In terms of evaluation, the toy distributions show that the method seems to converge to the right target but does not compare to either vanilla HMC, A-NICE-MC or L2HMC --which all guarantee asymptotic convergence. There should probably also be a mention of one of ESS/Auto-correlation/ESS per sec to get a sense of how helpful the method could be. For the generative model experiments, I agree with the comments of AnonReviewer3 in that evaluating HMPF-VAE with AIS while evaluating HVI with IWAE is somewhat unfair as the latter can happen to be much looser. I also think a natural baseline to compare to would be Hoffman (2017) or Levy et al. (2018) where after obtaining an approximate posterior sample, these works run an MCMC algorithm before updating the decoder. The algorithms seem to be related (albeit the objectives are slightly different) and should be talked about I think. References: Hoffman, Matt. Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo, ICML 2017. Song, Jiaming et al. A-NICE-MC: Adversarial Training for MCMC, NIPS 2017. Levy, Daniel et al. Generalizing Hamiltonian Monte Carlo with Neural Networks, ICLR 2018.", "rating": "5: Marginally below acceptance threshold", "reply_text": "* '' - The whole premise of the paper is based on chaining transformations that preserve the target density . However , in practice , you use a leapfrog operator without the Metropolis-Hastings step -- what happens to the theoretical guarantees in that case ? I 'm guessing Eq ( 8 ) , ( 9 ) do n't hold anymore and neither does Theorem 1 . '' For Eq ( 8 ) and Theorem 1 , they still hold for a stationary distribution with difference from the target around \\epsilon^2 in log density , where \\epsilon is the leapfrog step size . Good reference on this is Radford Neal 's tutorial on HMC and `` Simulating Hamiltonian Dynamics , Chapter 4 , Leimkuhler , Benedict and Reich , Sebastian '' . Eq ( 9 ) holds even without the convergence to target distribution . The reason of that is also very clear in the paper . Radford Neal explains this more clear in his tutorial on HMC . * '' - When swapping L for F , could you provide more justifications ? You use the argument that p ( z|x ) ~= q_L so the effect of the entropy term will be negligible . It seems that if they are so similar for large L , why even train the \\phi ? It also comes back to my first point that in your experiment , the transformations * do not * preserve the target density. `` We made it clear that the precondition for ignoring the entropy term in the paper : `` If with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' ( * right under * the definition of the loss without entropy ( equation 16 ) ) We are happy to make this more obvious somehow . Intuitively , it is not hard to see why this precondition make sense . In particular , we want to maximize this loss to converge to the target , then the initial loss value should be lower than the loss value of the target . This precondition is what it means formally for `` p ( z|x ) ~= q_L so the effect of the entropy term will be negligible '' . If `` p ( z|x ) ~= q_L '' is taken out of the context , in particular , the formal condition we give in the paper , your question makes sense . But , the critical theoretical justification is in the paper . We can work on to make this formal precondition better explained and connected with `` p ( z|x ) ~= q_L '' . * '' - Regarding the use of measure preserving flow , I think it can be quite hurtful in certain settings -- a very simple example would be a mixture of two gaussians with vastly different variance. `` We guess by this example , the reviewer want to say MCMC can miss some mode in this example . I think what the review want to propose by this example is the classic problem of MCMC of being trapped in high density area , even the probability mass is small . Please confirm this is what you mean . If so , here is the answer . As long as the ergodicity holds , the convergence to arbitrary distribution holds for any measure preserving flow . Measure preserving is the precondition of ergodicity , so * measure preserving never hurts convergence to the correct target * . However , the target measure is preserved is not sufficient for ergodicity . In particular , many MCMC kernels are weak in irreducibility . On continuous target with the big density gap , many MCMC mehods , including HMC , have difficulties in exploring high probability area with low density . This is not an issue if the precondition `` with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' holds . Because this implies that with initial parameter , the flow distribution should explore the low density area even more than the target distribution ."}, {"review_id": "HJx4KjRqYQ-2", "review_text": "This paper proposes training latent variable models (as in VAE decoders) by running HMC to approximate the posterior of the latents, and then estimating model parameters by maximizing the complete data log-likelihood. This is not a new idea by itself and is used e.g. as a baseline in Kingma and Welling's original VAE paper. The novelty in this paper is that it proposes tuning the parameters of the HMC inference algo by maximizing the likelihood achieved by the final sample in the MCMC chain. This seems to work well in practice and might be a useful method, but it is not clear under what conditions it should work. The paper is written in an unnecessarily complicated and formal way. On first read it seems like the proposed method has much more formal justification than it really has. The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound (the ERLBO) that is optimized, but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain, that is Monte Carlo EM as e.g. used by Kingma & Welling, 2013 as a baseline. The propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used. They could have been put in an appendix, or even a reference to the exisiting literature on HMC would have sufficed. The experiments do not clearly demonstrate that the method is much better than previous methods from the literature, although it is much more expensive. (The reported settings require 150 likelihood evaluations per example per minibatch update, versus 1 likelihood evaluation for a VAE). Also see my previous comments about evaluation in this paper's thread. - Please explain why tuning the HMC algo by maximizing eq 16 should work. I don't think it is a method that generally would work, e.g. if the initial sample z0 ~ q(z|x) is drawn from a data dependent encoder as in HVI (Salimans et al) then I would expect the step size of the HMC to simply go to zero as the encoder gets good. However in your case this does not happen as the initial sample is unconditional from x. Are there general guidelines or guarantees we can conclude from this? - The authors write \"Because MPFs are equivalent to ergodic Markov chains, the density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution \u03c0 as L increases.\" This is true for the idealized flow in continuous time, but HMC with finite step size does generally NOT converge to the correct distribution. This is why practical use of HMC includes a Metropolis-Hastings correction step. You omit this step in your algorithm, with the justification that we don't care about asymptotic convergence in this case. Fair enough, but then you should also omit all statements in the paper that claim that your method converges to the correct posterior in the limit. E.g. the writing makes it seem like Proposition 2 and Theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size. Maybe the statements are still correct if we take the limit with L->inf and the stepsize delta->0 at a certain rate? This is not obvious to me. In practice, you learn the stepsize delta. Do we have any guarantees this will make delta go to zero at the right rate as we increase the number of steps L? I.e. is this statement from your abstract true? -> \"we propose a novel method which is scalable, like mean-field VI, and, due to its theoretical foundation in ergodic theory, is also asymptotically accurate\". (convergence of uncorrected HMC only holds in the idealized case with step size -> 0)", "rating": "4: Ok but not good enough - rejection", "reply_text": "* '' The paper is written in an unnecessarily complicated and formal way . On first read it seems like the proposed method has much more formal justification than it really has . The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound ( the ERLBO ) that is optimized , but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain , that is Monte Carlo EM as e.g.used by Kingma & Welling , 2013 as a baseline . The propositions and theorems seem to apply to an idealized setting , but not to the actual algorithm that is used . They could have been put in an appendix , or even a reference to the exisiting literature on HMC would have sufficed . '' For Eq ( 8 ) and Theorem 1 , they still hold for a stationary distribution with difference in log density from the target around \\epsilon^2 , where \\epsilon is the leapfrog step size . See reference on this in Radford Neal 's tutorial on HMC and `` Simulating Hamiltonian Dynamics , Chapter 4 , Leimkuhler , Benedict and Reich , Sebastian '' . Eq ( 9 ) holds even without the convergence to target distribution . The reason of that is also very clear in the paper . Radford Neal explains this more clear in his tutorial on HMC . `` Monte Carlo EM '' is for training latent variable model rather than optimising kernel parameters in MCMC . In Monte Carlo EM , the expectation is computed by Monte Carlo estimation using perfect or MCMC approximate samples . The Simulation of samples is not optimised or tuned w.r.t.any loss.This is * fundamentally * different from what we propose in this work . First , what we propose is an inference method not a training method for latent variable model . To avoid this confusion , we have the other experiment for Bayesian NNs . Second , the same loss function here is the same as likelihood , but we fit the approximation distribution rather than maximum likelihood for latent variable . I hope the reviewer would agree on this : even the same loss function , optimising completely different variable is not a trivial difference . Finally , we have the formal condition of ignoring the entropy term : `` If with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' ( * right under * the definition of the loss without entropy ( equation 16 ) ) For the explanation to this precondition , please check the reply to other reviewers ."}], "0": {"review_id": "HJx4KjRqYQ-0", "review_text": "This paper proposes a simple heuristic for tuning HMC's parameters: just optimize the expected log-density of the Lth sample. It seems to work reasonably well on the problems the authors evaluate on. This heuristic is arrived at by a somewhat roundabout derivation, which I found interesting (although many of the same ideas are implicit in Salimans et al. (2014; \"MCMC & VI: Bridging the Gap\")). But ultimately this derivation comes to a head at this very heuristic argument: \u201c\u2026since qL(zL; \u03c6) converges to p\u03b8(z|x) as L increases, we expect the effect of H[qL(zL; \u03c6)] on \u03c6 to be small and that most of the similarity of qL(zL; \u03c6) to p\u03b8(z|x) will be captured by the first term in the RHS of (15). Therefore, we propose to tune \u03c6 by optimizing the tractable objective given by the first term\u2026\u201d: I don\u2019t see why this argument applies to the entropy term and not to the log-joint term. In particular, If q_L has really converged to p(z|x), then there\u2019s no point optimizing \u03c6 either, right? Here\u2019s a concrete example of how I could imagine this procedure going wrong: make q(z0) a delta at the latent vector z* that maximizes the log-joint, and set the step size of the Hamiltonian simulation to 0. This will make the entropy term (which is ignored) -\u221e, maximize the log-joint term, and I think it even makes D^L_{KL}=0. It seems like this isn\u2019t what actually happens experimentally, though\u2014perhaps I\u2019m missing something? Regarding the experiments, a natural baseline would be something akin to the approach of Hoffman (2017; \"Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo\u201d), simply initializing the HMC sampler with a mean-field Gaussian. I would expect this to produce worse results for small numbers of steps, since the variational Gaussian would choose a single mode, but I\u2019m curious how the quantitative metrics would compare. Some more minor points: * \u201cVariational auto-encoders (VAEs) (Kingma & Welling, 2014) are DGMs trained by using mean-field VI with a Gaussian parametric distribuion and amortization.\u201d I disagree with this terminology\u2014DGMs trained with, say, IAF are routinely called VAEs. * Section 3.1: It might be good to clarify that you\u2019re describing exact Hamiltonian integration, whereas in practice one always uses a discretized numerical integrator. (The leapfrog integrator is reversible and preserves volume, but doesn\u2019t conserve energy, so this does make the results a bit more complicated.) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , we would like to thank the reviewer 's effort . `` Here \u2019 s a concrete example of how I could imagine this procedure going wrong : make q ( z0 ) a delta at the latent vector z * that maximizes the log-joint , and set the step size of the Hamiltonian simulation to 0 . This will make the entropy term ( which is ignored ) -\u221e , maximize the log-joint term , and I think it even makes D^L_ { KL } =0 . It seems like this isn \u2019 t what actually happens experimentally , though\u2014perhaps I \u2019 m missing something ? '' Perhaps we can clarify with the following points : 1 ) The irreducibility of ergodic Markov kernel ( not in the paper ) 2 ) HMC step size can never be 0 otherwise the kernel is not irreducible therefore not ergodic ( not in the paper ) 3 ) Our precondition ( under eq ( 16 ) ) for dropping the entropy term ( in the paper ) First , a Markov chain is ergodic must satisfy following condition of irreducibility ( precondition of recurrence ) : the probability of any state change to any other state must be positive . Second , if the simulation time/discretized step size in HMC is zero , then the state of Hamiltonian dynamics simply do not change . Therefore , the input and output of Hamiltonian simulation is the same with probability 1 . This breaks the condition of ergodicity : irreducibility . Finally , we made it clear that there is a precondition for ignoring the entropy term : `` If with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' ( * right under * the definition of the loss without entropy ( equation 16 ) ) We are happy to make this more connected with our motivation of ignoring the entropy term . It is not too hard to see why this precondition makes sense . In case of that we want to maximise this loss to converge to the target , then the initial loss value should be lower than the loss value of the target , otherwise maximising the loss will lead to diverge . On the comment : `` Regarding the experiments , a natural baseline would be something akin to the approach of Hoffman ( 2017 ; `` Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo \u201d ) , ... '' This is a fair point . We do have experiment results with this paper , but we did n't include it because Hoffman 's method is much worse than VAE with the same amount of computation time . We are happy to add this results in , if the paper is accepted ."}, "1": {"review_id": "HJx4KjRqYQ-1", "review_text": "This paper presents an approximate bayesian inference method based on chaining measure preserving transformation with trainable parameters and optimizing for those using an ad-hoc objective based on a lower-bound on the likelihood. The paper is clearly written and easy to follow. The proofs seem correct. In terms of methods, I still have major questions: - The whole premise of the paper is based on chaining transformations that preserve the target density. However, in practice, you use a leapfrog operator without the Metropolis-Hastings step --what happens to the theoretical guarantees in that case? I'm guessing Eq (8), (9) don't hold anymore and neither does Theorem 1. - When swapping L for F, could you provide more justifications? You use the argument that p(z|x) ~= q_L so the effect of the entropy term will be negligible. It seems that if they are so similar for large L, why even train the \\phi? It also comes back to my first point that in your experiment, the transformations *do not* preserve the target density. - Regarding the use of measure preserving flow, I think it can be quite hurtful in certain settings -- a very simple example would be a mixture of two gaussians with vastly different variance. I think this paper also lacks recent references on training parameters for MCMC algorithms, most notably Song et al. (2017) and Levy et al. (2018). Both of these work seem quite related and should be mentioned and compared to. I would have also liked to see the authors contrast their work with Salimans (2015), especially the HVI part; is the main difference the reverse model? In terms of evaluation, the toy distributions show that the method seems to converge to the right target but does not compare to either vanilla HMC, A-NICE-MC or L2HMC --which all guarantee asymptotic convergence. There should probably also be a mention of one of ESS/Auto-correlation/ESS per sec to get a sense of how helpful the method could be. For the generative model experiments, I agree with the comments of AnonReviewer3 in that evaluating HMPF-VAE with AIS while evaluating HVI with IWAE is somewhat unfair as the latter can happen to be much looser. I also think a natural baseline to compare to would be Hoffman (2017) or Levy et al. (2018) where after obtaining an approximate posterior sample, these works run an MCMC algorithm before updating the decoder. The algorithms seem to be related (albeit the objectives are slightly different) and should be talked about I think. References: Hoffman, Matt. Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo, ICML 2017. Song, Jiaming et al. A-NICE-MC: Adversarial Training for MCMC, NIPS 2017. Levy, Daniel et al. Generalizing Hamiltonian Monte Carlo with Neural Networks, ICLR 2018.", "rating": "5: Marginally below acceptance threshold", "reply_text": "* '' - The whole premise of the paper is based on chaining transformations that preserve the target density . However , in practice , you use a leapfrog operator without the Metropolis-Hastings step -- what happens to the theoretical guarantees in that case ? I 'm guessing Eq ( 8 ) , ( 9 ) do n't hold anymore and neither does Theorem 1 . '' For Eq ( 8 ) and Theorem 1 , they still hold for a stationary distribution with difference from the target around \\epsilon^2 in log density , where \\epsilon is the leapfrog step size . Good reference on this is Radford Neal 's tutorial on HMC and `` Simulating Hamiltonian Dynamics , Chapter 4 , Leimkuhler , Benedict and Reich , Sebastian '' . Eq ( 9 ) holds even without the convergence to target distribution . The reason of that is also very clear in the paper . Radford Neal explains this more clear in his tutorial on HMC . * '' - When swapping L for F , could you provide more justifications ? You use the argument that p ( z|x ) ~= q_L so the effect of the entropy term will be negligible . It seems that if they are so similar for large L , why even train the \\phi ? It also comes back to my first point that in your experiment , the transformations * do not * preserve the target density. `` We made it clear that the precondition for ignoring the entropy term in the paper : `` If with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' ( * right under * the definition of the loss without entropy ( equation 16 ) ) We are happy to make this more obvious somehow . Intuitively , it is not hard to see why this precondition make sense . In particular , we want to maximize this loss to converge to the target , then the initial loss value should be lower than the loss value of the target . This precondition is what it means formally for `` p ( z|x ) ~= q_L so the effect of the entropy term will be negligible '' . If `` p ( z|x ) ~= q_L '' is taken out of the context , in particular , the formal condition we give in the paper , your question makes sense . But , the critical theoretical justification is in the paper . We can work on to make this formal precondition better explained and connected with `` p ( z|x ) ~= q_L '' . * '' - Regarding the use of measure preserving flow , I think it can be quite hurtful in certain settings -- a very simple example would be a mixture of two gaussians with vastly different variance. `` We guess by this example , the reviewer want to say MCMC can miss some mode in this example . I think what the review want to propose by this example is the classic problem of MCMC of being trapped in high density area , even the probability mass is small . Please confirm this is what you mean . If so , here is the answer . As long as the ergodicity holds , the convergence to arbitrary distribution holds for any measure preserving flow . Measure preserving is the precondition of ergodicity , so * measure preserving never hurts convergence to the correct target * . However , the target measure is preserved is not sufficient for ergodicity . In particular , many MCMC kernels are weak in irreducibility . On continuous target with the big density gap , many MCMC mehods , including HMC , have difficulties in exploring high probability area with low density . This is not an issue if the precondition `` with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' holds . Because this implies that with initial parameter , the flow distribution should explore the low density area even more than the target distribution ."}, "2": {"review_id": "HJx4KjRqYQ-2", "review_text": "This paper proposes training latent variable models (as in VAE decoders) by running HMC to approximate the posterior of the latents, and then estimating model parameters by maximizing the complete data log-likelihood. This is not a new idea by itself and is used e.g. as a baseline in Kingma and Welling's original VAE paper. The novelty in this paper is that it proposes tuning the parameters of the HMC inference algo by maximizing the likelihood achieved by the final sample in the MCMC chain. This seems to work well in practice and might be a useful method, but it is not clear under what conditions it should work. The paper is written in an unnecessarily complicated and formal way. On first read it seems like the proposed method has much more formal justification than it really has. The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound (the ERLBO) that is optimized, but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain, that is Monte Carlo EM as e.g. used by Kingma & Welling, 2013 as a baseline. The propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used. They could have been put in an appendix, or even a reference to the exisiting literature on HMC would have sufficed. The experiments do not clearly demonstrate that the method is much better than previous methods from the literature, although it is much more expensive. (The reported settings require 150 likelihood evaluations per example per minibatch update, versus 1 likelihood evaluation for a VAE). Also see my previous comments about evaluation in this paper's thread. - Please explain why tuning the HMC algo by maximizing eq 16 should work. I don't think it is a method that generally would work, e.g. if the initial sample z0 ~ q(z|x) is drawn from a data dependent encoder as in HVI (Salimans et al) then I would expect the step size of the HMC to simply go to zero as the encoder gets good. However in your case this does not happen as the initial sample is unconditional from x. Are there general guidelines or guarantees we can conclude from this? - The authors write \"Because MPFs are equivalent to ergodic Markov chains, the density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution \u03c0 as L increases.\" This is true for the idealized flow in continuous time, but HMC with finite step size does generally NOT converge to the correct distribution. This is why practical use of HMC includes a Metropolis-Hastings correction step. You omit this step in your algorithm, with the justification that we don't care about asymptotic convergence in this case. Fair enough, but then you should also omit all statements in the paper that claim that your method converges to the correct posterior in the limit. E.g. the writing makes it seem like Proposition 2 and Theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size. Maybe the statements are still correct if we take the limit with L->inf and the stepsize delta->0 at a certain rate? This is not obvious to me. In practice, you learn the stepsize delta. Do we have any guarantees this will make delta go to zero at the right rate as we increase the number of steps L? I.e. is this statement from your abstract true? -> \"we propose a novel method which is scalable, like mean-field VI, and, due to its theoretical foundation in ergodic theory, is also asymptotically accurate\". (convergence of uncorrected HMC only holds in the idealized case with step size -> 0)", "rating": "4: Ok but not good enough - rejection", "reply_text": "* '' The paper is written in an unnecessarily complicated and formal way . On first read it seems like the proposed method has much more formal justification than it really has . The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound ( the ERLBO ) that is optimized , but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain , that is Monte Carlo EM as e.g.used by Kingma & Welling , 2013 as a baseline . The propositions and theorems seem to apply to an idealized setting , but not to the actual algorithm that is used . They could have been put in an appendix , or even a reference to the exisiting literature on HMC would have sufficed . '' For Eq ( 8 ) and Theorem 1 , they still hold for a stationary distribution with difference in log density from the target around \\epsilon^2 , where \\epsilon is the leapfrog step size . See reference on this in Radford Neal 's tutorial on HMC and `` Simulating Hamiltonian Dynamics , Chapter 4 , Leimkuhler , Benedict and Reich , Sebastian '' . Eq ( 9 ) holds even without the convergence to target distribution . The reason of that is also very clear in the paper . Radford Neal explains this more clear in his tutorial on HMC . `` Monte Carlo EM '' is for training latent variable model rather than optimising kernel parameters in MCMC . In Monte Carlo EM , the expectation is computed by Monte Carlo estimation using perfect or MCMC approximate samples . The Simulation of samples is not optimised or tuned w.r.t.any loss.This is * fundamentally * different from what we propose in this work . First , what we propose is an inference method not a training method for latent variable model . To avoid this confusion , we have the other experiment for Bayesian NNs . Second , the same loss function here is the same as likelihood , but we fit the approximation distribution rather than maximum likelihood for latent variable . I hope the reviewer would agree on this : even the same loss function , optimising completely different variable is not a trivial difference . Finally , we have the formal condition of ignoring the entropy term : `` If with the initial flow parameter \u03c60 , the objective F\u02dc < Ep\u03b8 ( x , z ) [ log p\u03b8 ( x , z ) ] ... '' ( * right under * the definition of the loss without entropy ( equation 16 ) ) For the explanation to this precondition , please check the reply to other reviewers ."}}