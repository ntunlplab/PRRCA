{"year": "2020", "forum": "BJxyzxrYPH", "title": "Deep geometric matrix completion:  Are we doing it right?", "decision": "Reject", "meta_review": "This paper proposes a multiresolution spectral geometric loss called the zoomout loss to help with matrix completion, and show state-of-the-art results on several recommendation benchmarks, although experiments also show that the result improvements are not always dependent upon the geometric loss itself.\nReviewers find the idea interesting and the results promising but also have important concerns about the experiments not establishing how the approach truly works. Authors have clarified their explanations in the revisions and provided requested experiments (e.g., on the importance of the initialization size), however important reservations re. why the approach works are still not sufficiently addressed, and would require more iterations to fulfill the potential of this paper.\nTherefore, we recommend rejection.", "reviews": [{"review_id": "BJxyzxrYPH-0", "review_text": "This paper proposes a new method for geometric matrix completion based on functional maps. The proposed algorithm is a simple shallow and fully linear network. Experimental results demonstrate the effectiveness of the proposed method. The proposed method is new and has been shown good empirical results. The paper also points out a new way to interpret matrix completion. On the other hand, the proposed method seems ad hoc and there is no clear evidence why it is better than other baselines except the empirical results. The paper also has some clearance issues, making it hard to understand. I vote for a weak reject of the paper at the current pace and would like to increase my score if the following questions can be clearly answered. 1. Why do we need to propose the algorithm? Is it because we have the functional maps technique motivated from shape correspondence, and we can see some connection of such technique with matric completion? If it is true, we surely can have a new algorithm based on such a new technique. But I can still not understand why the method work, at least, in an intuitive way. 2. What is the sample complexity of the proposed matrix completion algorithm? The introduction of the paper is poorly written. The first paragraph and the third one both contain some introduction to matric completion, which results in a lot of redundant information. The second paragraph and the fourth one are redundant in the same way since they both focus on geometric matrix completion. I think besides introducing what is matrix completion and what is geometric completion, the introduction part should focus more on the motivation to propose the algorithm. However, I can only see from the end of the second paragraph (some simple models need to be proposed) and the fifth paragraph (\u201cThe inspiration of our paper\u201d) some motivation information. The introduction part needs to be re-organized to provide more useful information about the paper rather than a literature review. There is some unclear/inaccurate/subjective statement in the introduction part. For example, \u201cSelf-supervised learning\u201d needs a reference. Why geometric matrix completion generalizes the standard deep learning approaches is not clear. What does it mean by \u201ctheir design is \u2026 cumbersome and non-intuitive\u201d? The shape correspondence is never explained until very later in the paper. Also, there are some unclear issues besides the Introduction part. For example, what does it mean by \u201cthe product graph\u201d? All these issues need to be clarified before the paper can be accepted. --------------------------------------------------- Thank you for the detailed rebuttal. For Q1, it clearly explains how does the method work. However, it is still not clear why does the method work. I also have another concern after reading the rebuttal, if the shape correspondence is not that important, why make it an important motivation in the paper? For Q2, it is interesting to see some theoretical results on the sample complexity, rather than an experimental one. The paper would also be much better if the clearance issues can be addressed. Even if I would not vote for an accept this time, I am looking forward to a revised version in the future. ", "rating": "3: Weak Reject", "reply_text": "Another interesting observation we made is that our method is essentially an overparameterized deep matrix factorization ( DMF ) method with some additional structure . DMF has been proven recently ( see Arora et al.2019 and the discussion with reviewer # 3 ) to promote a low rank via implicit regularization of gradient descent . This is a contributing factor to the success of our method . We made up a tutorial to allow experimenting with our method in the link below , and we hope you can find it useful to understand the method better : https : //colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6 ( 2 ) The number of available ratings for each dataset is provided in Table 4 . If by \u201c sample complexity \u201d you mean how the test error changes with the size of the training set , we believe that our cold start analysis ( Figure 1 ) provides an answer : we show that the SGMC-Z version of our method is particularly more effective in the data-poor regime than the SGMC and other competing algorithms . Even after retaining only 5 ratings for more than half the users , we still get competitive results compared , for example , to RGCNN ( compare Figure 1 and Table 1 ) . Regarding the presentation issues : we thank you for the suggestions and will reformulate some parts of the paper according to your recommendations . Yours sincerely , The authors ."}, {"review_id": "BJxyzxrYPH-1", "review_text": "This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. The proposed method consists of two ideas: (1) spectral regularization (i.e., Dirichlet energy) with a re-parameterizing basis and (2) multiresolution of spectral loss (i.e., zoomout loss). In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). Empirical results show the best performance compared to other recent methods under small-scale datasets. Moreover, the proposed method outperforms when the geometric model is accurate (verified on the synthetic setting) and this can reflect that the proposed method is a good choice when the graph structures are given. This work can be a significant contribution as it is a simple linear model but practically performs better than other deep nonlinear networks (e.g., RGCNN). Additionally, the proposed loss functions utilize only the spectral information of graph structure with novel approaches. However, there are some drawbacks to this work. First, it requires a good quality of geometric model which is hard to obtain in practical datasets. Second, the proposed method has a scalability issue since it requires eigendecompositions of graph Laplacians (as discussed in the paper). This can be a problem for real and large-scale datasets. Overall, this paper presents a novel approach utilizing graph spectral information with empirical improvements. But, I vote for weak acceptance due to its drawbacks as mentioned above. Main concerns: 1. It is not clear why minimizing Dirichlet energy can improve the performance of matrix completion. In the paper, the authors mention that it promotes smooth functions on the graph nodes, but not fully clear why smooth functions are good. And how much does the accuracy increase (or decrease) when using the Dirichlet regularization? 2. Authors argue that the re-parameterizing of the basis (emerging P and Q) can find a better geometric model (section 2). So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. However, the empirical results are reported poor improvements for inaccurate geometric models. Does this make sense? For experiments: 1. What is the number of trainable parameters for each method? Since the proposed method is overparameterized, it is not clear that the empirical improvements come from the overparameterizing or the proposed loss function. It would be great to report the number of parameters of all other methods by setting similar numbers. 2. It is not clear how to generate the synthetic dataset, i.e., projecting a random matrix on te the first few eigenvectors of L_r and L_c. It would be better to give more details. 3. What are the training times of the proposed method and other competitors? 4. Why results of FM are not reported under other datasets? Minor comments: 1. In page 4, please edit \u201cWe explore The\u201d -> \u201cWe explore the\u201d. 2. In equation (15), writing \u201c\\odot S\u201d twice seems to be unnecessary. ", "rating": "6: Weak Accept", "reply_text": "Experimentation : ( 1 ) The number of trainable parameters for our method is the number of elements in $ \\mathbf { P } , \\mathbf { C } , \\mathbf { Q } $ . This is chosen according to $ p_ { \\mathrm { max } } , q_ { \\mathrm { max } } $ - a hyperparameter in our setting reported in Table 5 . Overparameterization alone is not enough to produce the empirical improvements we reported . Without explicit regularization it would overfit the training data and perform poorly on the test data , specifically in the data poor regime . Also see comment ( 3 ) of reviewer # 3 and the answer we provided . Regarding the other methods : we omitted those details as it is not clear how to compare the number of parameters between methods of ostensibly different nature , and it wasn \u2019 t the focus of our paper . ( 2 ) To generate the Synthetic ML-100K dataset we did the following : - We computed the first $ k=50 $ eigenvectors $ \\mathbf { \\Phi } _k , \\mathbf { \\Psi } _k $ of $ \\mathbf { L } _\\mathrm { r } $ and $ \\mathbf { L } _\\mathrm { c } $ , the Laplacians of the row and column graphs for the ML-100K dataset - We projected $ \\mathbf { M } $ on this subspace , i.e. , \\ [ \\mathbf { M } _ { \\mathrm { proj } } =\\mathbf { \\Phi } _k\\mathbf { \\Phi } _k^\\top\\mathbf { M } \\mathbf { \\Psi } _k\\mathbf { \\Psi } _k^\\top ; \\ ] - We performed histogram matching between $ \\mathbf { M } _\\mathrm { proj } $ and $ \\mathbf { M } $ such that the histogram of entries in $ \\mathbf { M } _\\mathrm { proj } $ is the same as in $ \\mathbf { M } $ . This nonlinear operation increases the rank of the matrix , so it is no longer k , but the perturbation to the singular values is small ( verified empirically ) . ( 3 ) We do not have the training times for the other methods as we took the results from the corresponding papers . Our method ( SGMC ) runs in just a few minutes , depending on the datasets . For example , on ML-100K with the parameters reported in table 5 it takes about a minute , including the eigendecomposition and excluding the time taken to build the computational graph in Tensorflow . ( 4 ) The results of the FM method are poor for the other datasets so we did not include them . We will include them in the revised version . Minor comments : ( 2 ) Regarding equation ( 15 ) : $ \\odot S $ should appear twice , following from the computation of the gradient . If $ S $ is binary ( as in our case ) then $ S\\odot S = S $ and one of the $ S $ disappears . We again thank you for the suggestions and will incorporate your comments into the revised version of the paper . Yours sincerely , The authors ."}, {"review_id": "BJxyzxrYPH-2", "review_text": "This paper aims to solve the matrix completion problem by incorporating geometric information. The proposed approach involves using graphs encoding relations between rows (and columns), applying spectral decomposition to these graphs, and using a multi-resolution spectral geometric loss to reconstruct the functional map which could then be used to directly recover the underlying matrix. The paper evaluates the proposed network on both synthetic and real datasets and shows improvements over the existing geometric methods and convex relaxations. While the geometric approach looks interesting and the experimental results seem promising, it is unclear why the proposed approach works, and the comparison with [Arora et al. (2019)] is not fair. Below are the specific comments. (1) The proposed approach (formulation (10)) involves too many parameters (including the weights w in (9)) that need to be tuned. The authors should discuss how to select the parameters after (10). This also raises the question of how practical the proposed approach is. (2) The authors claim the first contribution is to provide the geometric interpretation of deep matrix factorization via the functional maps framework. However, I didn't see clearly the interpretation. If it refers to the parametrization of X by \\Phi P C Q^T \\Psi^T, then it is just a special case of deep matrix factorization since both \\Phi and \\Psi are fixed, and P and Q are optimized to be approximately orthonormal. (3) Due to over-parameterization, in general deep matrix factorization would suffer from overfitting. That being said [Gunasekar et al. (2017), Arora et al. (2019)] prove that gradient descent induces implicit regularization if the algorithm is initialized with factors that are very \"small\". However, in the experiments, both P and Q are initialized as the identity, which is not close to zero. Indeed, it was proved in the following paper that the generalization gap will be proportional to the energy of the initialization, even for matrix factorization. Yuanzhi Li, Tengyu Ma, and Hongyang Zhang, Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations. (4) As a followup question, without such implicit regularization, it is unclear why the proposed approach does not suffer from overfitting. A discussion along this line is required. Though the authors include the connection between [Arora et al. (2019)], this is not convincing enough since as explained above, the implicit regularization there depends on the smallness of the initialization.", "rating": "3: Weak Reject", "reply_text": "Dear reviewer # 3 , Thank you for your comments ! In what follows , we will try to address in detail the issues you raised : ( 1 ) We believe this is a misunderstanding of the hyperparams involved . While we stated in the paper the full scope of possible hyperparams in this general framework , we limited ourselves to only two settings : ( a ) SGMC - In this setting we set the weights $ w_ { ij } $ to 0 at all resolutions except the last one ( full resolution ) . ( b ) SGMC-Z - In this setting we chose a-priori some spectral skip parameters ( p_skip , q_skip ) and we set $ w_ { ij } =1 $ for ( i=1+k * p_skip , j=1+k * q_skip ) , i.e. , we sample the parameter space ( p , q ) on a grid with spacing ( p_skip , q_skip ) , and set $ w_ { ij } =1 $ only on the diagonal of this grid . We did not try to explore any other setting for $ w_ { ij } $ . Overall , the number of hyperparams involved is between 8 to 10 ( 2 for each energy involved and the p_max/q_max , p_skip/q_skip params ) . Moreover , we usually define the same hyperparams for the rows and columns energies , so it is about half that number . In a future work we will also make some of these parameters such as p_skip/q_skip learnable . Also note that , following our ablation study , the dependence on the hyperparams is quite small ( on some even negligible ) , and it is rather easy to tune them using a validation set . ( 2 ) As we noted , the data term of the SGMC is a special form of DMF from Arora et . al.But we also introduced two important additional terms : A Dirichlet energy term - promoting smoothness on the ( inferred ) graphs . A diagonalization term - promoting the new ( inferred ) bases to be Laplacian eigenbases . These three terms together provide the geometric interpretation : If we treat the two factors $ P , Q $ as corrections to some harmonic bases , and approximately enforce those new bases to also be harmonic bases ( i.e. , approximately diagonalizing the corresponding graph Laplacians ) , then we can model our matrix as some approximately bandlimited signal on a new product graph , whose functional space can be spanned by the new bases . Following your remark , we acknowledge there is some lack of clarity in the way we presented our approach : We do not provide a geometric interpretation to DMF but rather embed it within a bigger geometric framework . Once the aforementioned two terms are included , the geometric interpretation emerges . ( 3 ) We thank you for raising this point . Our intention in including a comparison to DMF was to show how a simple method such as DMF can produce results on par with state-of-the-art geometric methods . This is one of the main messages of our paper - to show how badly the underlying geometry is being used ( or how bad is the geometry being used ) in geometric matrix completion methods . Following your remark , we performed the experiments with DMF again , using a \u201c small \u201d initialization , and indeed we got a large improvement on the synthetic datasets ! On the real datasets , however , we did not observe any improvement . The experiments we report in the following link measure the reconstruction error achieved by each method when the initialization is scaled by $ 10^ { -\\alpha } $ , $ \\alpha > 0 $ , as suggested in Li et al.https : //drive.google.com/open ? id=1pduAXS_NHwC1DhornDD9A78YehwCAjzR ( 4 ) Our regularization is explicit . It follows from both the Dirichlet energy and the multiresolution loss which weighs more heavily the lower frequency part of the functional map , as explained in the text . In order to better illustrate why the approach works , we came up with a toy example that can be found in the following link : https : //colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6 This regularization does not necessitate depth , as in DMF , but still allows to enjoy the implicit regularization inherent to DMF with gradient descent methods . We have a compelling explanation for the better performance of our method compared to DMF : As Arora et al reports ( see Figure 2 in their paper ) , above a certain number of samples , DMF converges to the minimum norm solution . Below that number it induces a better regularization on the rank of the matrix , which still allows to recover low rank matrices . However , in the real datasets we tested on , the number of available samples is way below that threshold , as the rank of those matrices is not that low ( See Table 4 in our paper ) . In this extremely data poor regime , DMF performs poorly , and the extra information present in the graphs is crucial . This is consistent with our experimentation with the toy problem we shared in the link above , and you can test yourself by changing the number of training samples . For a rank-10 matrix , using more than 30 % of the entries allows for a very low reconstruction error with DMF , which outperforms our method ( by a small margin ) . However , when going below 20 % , our method demonstrates a clear advantage . We will add a discussion along these lines with relevant plots to the revised version . Yours sincerely , The authors ."}], "0": {"review_id": "BJxyzxrYPH-0", "review_text": "This paper proposes a new method for geometric matrix completion based on functional maps. The proposed algorithm is a simple shallow and fully linear network. Experimental results demonstrate the effectiveness of the proposed method. The proposed method is new and has been shown good empirical results. The paper also points out a new way to interpret matrix completion. On the other hand, the proposed method seems ad hoc and there is no clear evidence why it is better than other baselines except the empirical results. The paper also has some clearance issues, making it hard to understand. I vote for a weak reject of the paper at the current pace and would like to increase my score if the following questions can be clearly answered. 1. Why do we need to propose the algorithm? Is it because we have the functional maps technique motivated from shape correspondence, and we can see some connection of such technique with matric completion? If it is true, we surely can have a new algorithm based on such a new technique. But I can still not understand why the method work, at least, in an intuitive way. 2. What is the sample complexity of the proposed matrix completion algorithm? The introduction of the paper is poorly written. The first paragraph and the third one both contain some introduction to matric completion, which results in a lot of redundant information. The second paragraph and the fourth one are redundant in the same way since they both focus on geometric matrix completion. I think besides introducing what is matrix completion and what is geometric completion, the introduction part should focus more on the motivation to propose the algorithm. However, I can only see from the end of the second paragraph (some simple models need to be proposed) and the fifth paragraph (\u201cThe inspiration of our paper\u201d) some motivation information. The introduction part needs to be re-organized to provide more useful information about the paper rather than a literature review. There is some unclear/inaccurate/subjective statement in the introduction part. For example, \u201cSelf-supervised learning\u201d needs a reference. Why geometric matrix completion generalizes the standard deep learning approaches is not clear. What does it mean by \u201ctheir design is \u2026 cumbersome and non-intuitive\u201d? The shape correspondence is never explained until very later in the paper. Also, there are some unclear issues besides the Introduction part. For example, what does it mean by \u201cthe product graph\u201d? All these issues need to be clarified before the paper can be accepted. --------------------------------------------------- Thank you for the detailed rebuttal. For Q1, it clearly explains how does the method work. However, it is still not clear why does the method work. I also have another concern after reading the rebuttal, if the shape correspondence is not that important, why make it an important motivation in the paper? For Q2, it is interesting to see some theoretical results on the sample complexity, rather than an experimental one. The paper would also be much better if the clearance issues can be addressed. Even if I would not vote for an accept this time, I am looking forward to a revised version in the future. ", "rating": "3: Weak Reject", "reply_text": "Another interesting observation we made is that our method is essentially an overparameterized deep matrix factorization ( DMF ) method with some additional structure . DMF has been proven recently ( see Arora et al.2019 and the discussion with reviewer # 3 ) to promote a low rank via implicit regularization of gradient descent . This is a contributing factor to the success of our method . We made up a tutorial to allow experimenting with our method in the link below , and we hope you can find it useful to understand the method better : https : //colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6 ( 2 ) The number of available ratings for each dataset is provided in Table 4 . If by \u201c sample complexity \u201d you mean how the test error changes with the size of the training set , we believe that our cold start analysis ( Figure 1 ) provides an answer : we show that the SGMC-Z version of our method is particularly more effective in the data-poor regime than the SGMC and other competing algorithms . Even after retaining only 5 ratings for more than half the users , we still get competitive results compared , for example , to RGCNN ( compare Figure 1 and Table 1 ) . Regarding the presentation issues : we thank you for the suggestions and will reformulate some parts of the paper according to your recommendations . Yours sincerely , The authors ."}, "1": {"review_id": "BJxyzxrYPH-1", "review_text": "This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. The proposed method consists of two ideas: (1) spectral regularization (i.e., Dirichlet energy) with a re-parameterizing basis and (2) multiresolution of spectral loss (i.e., zoomout loss). In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). Empirical results show the best performance compared to other recent methods under small-scale datasets. Moreover, the proposed method outperforms when the geometric model is accurate (verified on the synthetic setting) and this can reflect that the proposed method is a good choice when the graph structures are given. This work can be a significant contribution as it is a simple linear model but practically performs better than other deep nonlinear networks (e.g., RGCNN). Additionally, the proposed loss functions utilize only the spectral information of graph structure with novel approaches. However, there are some drawbacks to this work. First, it requires a good quality of geometric model which is hard to obtain in practical datasets. Second, the proposed method has a scalability issue since it requires eigendecompositions of graph Laplacians (as discussed in the paper). This can be a problem for real and large-scale datasets. Overall, this paper presents a novel approach utilizing graph spectral information with empirical improvements. But, I vote for weak acceptance due to its drawbacks as mentioned above. Main concerns: 1. It is not clear why minimizing Dirichlet energy can improve the performance of matrix completion. In the paper, the authors mention that it promotes smooth functions on the graph nodes, but not fully clear why smooth functions are good. And how much does the accuracy increase (or decrease) when using the Dirichlet regularization? 2. Authors argue that the re-parameterizing of the basis (emerging P and Q) can find a better geometric model (section 2). So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. However, the empirical results are reported poor improvements for inaccurate geometric models. Does this make sense? For experiments: 1. What is the number of trainable parameters for each method? Since the proposed method is overparameterized, it is not clear that the empirical improvements come from the overparameterizing or the proposed loss function. It would be great to report the number of parameters of all other methods by setting similar numbers. 2. It is not clear how to generate the synthetic dataset, i.e., projecting a random matrix on te the first few eigenvectors of L_r and L_c. It would be better to give more details. 3. What are the training times of the proposed method and other competitors? 4. Why results of FM are not reported under other datasets? Minor comments: 1. In page 4, please edit \u201cWe explore The\u201d -> \u201cWe explore the\u201d. 2. In equation (15), writing \u201c\\odot S\u201d twice seems to be unnecessary. ", "rating": "6: Weak Accept", "reply_text": "Experimentation : ( 1 ) The number of trainable parameters for our method is the number of elements in $ \\mathbf { P } , \\mathbf { C } , \\mathbf { Q } $ . This is chosen according to $ p_ { \\mathrm { max } } , q_ { \\mathrm { max } } $ - a hyperparameter in our setting reported in Table 5 . Overparameterization alone is not enough to produce the empirical improvements we reported . Without explicit regularization it would overfit the training data and perform poorly on the test data , specifically in the data poor regime . Also see comment ( 3 ) of reviewer # 3 and the answer we provided . Regarding the other methods : we omitted those details as it is not clear how to compare the number of parameters between methods of ostensibly different nature , and it wasn \u2019 t the focus of our paper . ( 2 ) To generate the Synthetic ML-100K dataset we did the following : - We computed the first $ k=50 $ eigenvectors $ \\mathbf { \\Phi } _k , \\mathbf { \\Psi } _k $ of $ \\mathbf { L } _\\mathrm { r } $ and $ \\mathbf { L } _\\mathrm { c } $ , the Laplacians of the row and column graphs for the ML-100K dataset - We projected $ \\mathbf { M } $ on this subspace , i.e. , \\ [ \\mathbf { M } _ { \\mathrm { proj } } =\\mathbf { \\Phi } _k\\mathbf { \\Phi } _k^\\top\\mathbf { M } \\mathbf { \\Psi } _k\\mathbf { \\Psi } _k^\\top ; \\ ] - We performed histogram matching between $ \\mathbf { M } _\\mathrm { proj } $ and $ \\mathbf { M } $ such that the histogram of entries in $ \\mathbf { M } _\\mathrm { proj } $ is the same as in $ \\mathbf { M } $ . This nonlinear operation increases the rank of the matrix , so it is no longer k , but the perturbation to the singular values is small ( verified empirically ) . ( 3 ) We do not have the training times for the other methods as we took the results from the corresponding papers . Our method ( SGMC ) runs in just a few minutes , depending on the datasets . For example , on ML-100K with the parameters reported in table 5 it takes about a minute , including the eigendecomposition and excluding the time taken to build the computational graph in Tensorflow . ( 4 ) The results of the FM method are poor for the other datasets so we did not include them . We will include them in the revised version . Minor comments : ( 2 ) Regarding equation ( 15 ) : $ \\odot S $ should appear twice , following from the computation of the gradient . If $ S $ is binary ( as in our case ) then $ S\\odot S = S $ and one of the $ S $ disappears . We again thank you for the suggestions and will incorporate your comments into the revised version of the paper . Yours sincerely , The authors ."}, "2": {"review_id": "BJxyzxrYPH-2", "review_text": "This paper aims to solve the matrix completion problem by incorporating geometric information. The proposed approach involves using graphs encoding relations between rows (and columns), applying spectral decomposition to these graphs, and using a multi-resolution spectral geometric loss to reconstruct the functional map which could then be used to directly recover the underlying matrix. The paper evaluates the proposed network on both synthetic and real datasets and shows improvements over the existing geometric methods and convex relaxations. While the geometric approach looks interesting and the experimental results seem promising, it is unclear why the proposed approach works, and the comparison with [Arora et al. (2019)] is not fair. Below are the specific comments. (1) The proposed approach (formulation (10)) involves too many parameters (including the weights w in (9)) that need to be tuned. The authors should discuss how to select the parameters after (10). This also raises the question of how practical the proposed approach is. (2) The authors claim the first contribution is to provide the geometric interpretation of deep matrix factorization via the functional maps framework. However, I didn't see clearly the interpretation. If it refers to the parametrization of X by \\Phi P C Q^T \\Psi^T, then it is just a special case of deep matrix factorization since both \\Phi and \\Psi are fixed, and P and Q are optimized to be approximately orthonormal. (3) Due to over-parameterization, in general deep matrix factorization would suffer from overfitting. That being said [Gunasekar et al. (2017), Arora et al. (2019)] prove that gradient descent induces implicit regularization if the algorithm is initialized with factors that are very \"small\". However, in the experiments, both P and Q are initialized as the identity, which is not close to zero. Indeed, it was proved in the following paper that the generalization gap will be proportional to the energy of the initialization, even for matrix factorization. Yuanzhi Li, Tengyu Ma, and Hongyang Zhang, Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations. (4) As a followup question, without such implicit regularization, it is unclear why the proposed approach does not suffer from overfitting. A discussion along this line is required. Though the authors include the connection between [Arora et al. (2019)], this is not convincing enough since as explained above, the implicit regularization there depends on the smallness of the initialization.", "rating": "3: Weak Reject", "reply_text": "Dear reviewer # 3 , Thank you for your comments ! In what follows , we will try to address in detail the issues you raised : ( 1 ) We believe this is a misunderstanding of the hyperparams involved . While we stated in the paper the full scope of possible hyperparams in this general framework , we limited ourselves to only two settings : ( a ) SGMC - In this setting we set the weights $ w_ { ij } $ to 0 at all resolutions except the last one ( full resolution ) . ( b ) SGMC-Z - In this setting we chose a-priori some spectral skip parameters ( p_skip , q_skip ) and we set $ w_ { ij } =1 $ for ( i=1+k * p_skip , j=1+k * q_skip ) , i.e. , we sample the parameter space ( p , q ) on a grid with spacing ( p_skip , q_skip ) , and set $ w_ { ij } =1 $ only on the diagonal of this grid . We did not try to explore any other setting for $ w_ { ij } $ . Overall , the number of hyperparams involved is between 8 to 10 ( 2 for each energy involved and the p_max/q_max , p_skip/q_skip params ) . Moreover , we usually define the same hyperparams for the rows and columns energies , so it is about half that number . In a future work we will also make some of these parameters such as p_skip/q_skip learnable . Also note that , following our ablation study , the dependence on the hyperparams is quite small ( on some even negligible ) , and it is rather easy to tune them using a validation set . ( 2 ) As we noted , the data term of the SGMC is a special form of DMF from Arora et . al.But we also introduced two important additional terms : A Dirichlet energy term - promoting smoothness on the ( inferred ) graphs . A diagonalization term - promoting the new ( inferred ) bases to be Laplacian eigenbases . These three terms together provide the geometric interpretation : If we treat the two factors $ P , Q $ as corrections to some harmonic bases , and approximately enforce those new bases to also be harmonic bases ( i.e. , approximately diagonalizing the corresponding graph Laplacians ) , then we can model our matrix as some approximately bandlimited signal on a new product graph , whose functional space can be spanned by the new bases . Following your remark , we acknowledge there is some lack of clarity in the way we presented our approach : We do not provide a geometric interpretation to DMF but rather embed it within a bigger geometric framework . Once the aforementioned two terms are included , the geometric interpretation emerges . ( 3 ) We thank you for raising this point . Our intention in including a comparison to DMF was to show how a simple method such as DMF can produce results on par with state-of-the-art geometric methods . This is one of the main messages of our paper - to show how badly the underlying geometry is being used ( or how bad is the geometry being used ) in geometric matrix completion methods . Following your remark , we performed the experiments with DMF again , using a \u201c small \u201d initialization , and indeed we got a large improvement on the synthetic datasets ! On the real datasets , however , we did not observe any improvement . The experiments we report in the following link measure the reconstruction error achieved by each method when the initialization is scaled by $ 10^ { -\\alpha } $ , $ \\alpha > 0 $ , as suggested in Li et al.https : //drive.google.com/open ? id=1pduAXS_NHwC1DhornDD9A78YehwCAjzR ( 4 ) Our regularization is explicit . It follows from both the Dirichlet energy and the multiresolution loss which weighs more heavily the lower frequency part of the functional map , as explained in the text . In order to better illustrate why the approach works , we came up with a toy example that can be found in the following link : https : //colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6 This regularization does not necessitate depth , as in DMF , but still allows to enjoy the implicit regularization inherent to DMF with gradient descent methods . We have a compelling explanation for the better performance of our method compared to DMF : As Arora et al reports ( see Figure 2 in their paper ) , above a certain number of samples , DMF converges to the minimum norm solution . Below that number it induces a better regularization on the rank of the matrix , which still allows to recover low rank matrices . However , in the real datasets we tested on , the number of available samples is way below that threshold , as the rank of those matrices is not that low ( See Table 4 in our paper ) . In this extremely data poor regime , DMF performs poorly , and the extra information present in the graphs is crucial . This is consistent with our experimentation with the toy problem we shared in the link above , and you can test yourself by changing the number of training samples . For a rank-10 matrix , using more than 30 % of the entries allows for a very low reconstruction error with DMF , which outperforms our method ( by a small margin ) . However , when going below 20 % , our method demonstrates a clear advantage . We will add a discussion along these lines with relevant plots to the revised version . Yours sincerely , The authors ."}}