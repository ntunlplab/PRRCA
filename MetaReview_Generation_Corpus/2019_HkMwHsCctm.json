{"year": "2019", "forum": "HkMwHsCctm", "title": "Principled Deep Neural Network Training through Linear Programming", "decision": "Reject", "meta_review": "The strength of the paper is that it designs an LP-based algorithm for training neural networks with runtime exponential in the number of parameters and linear in the size of the datasets and the algorithm works for worst-case datasets. As reviewer 2 and reviewer 3 pointed out, the cons include a) it's not clear why the algorithm provides any theoretical insights on how to design in the future polynomial time algorithm --- it seems that the algorithms are inherently exponential time and b) it's not clear whether the algorithm is practically at all relevant. The AC also noted that brute-force search algorithm is also exponential in # parameters and linear in size of datasets, and the authors agreed with it. This leaves the main contribution of the paper be that it works for the worst datasets. \bHowever, theoretically speaking, it's not clear this should be counted as a feature for algorithm design because we cannot go beyond the intractability without making assumptions on the data and in the AC's opinion, the big open question is how to make additional assumptions on the data (instead of removing them.) In summary, the drawback b) makes this a purely theoretical paper and the theoretical significance of the paper is unclear due to a). Therefore based on a), the AC decided to recommend reject, although the AC suggested the authors to re-submit to other top theory or ML theory conferences which may better evaluate the theoretical significance of the paper. ", "reviews": [{"review_id": "HkMwHsCctm-0", "review_text": "This work reformulates the neural network training as an LP with size that is exponential in the size of the architecture and data dimension, and polynomial in the size of the data set. They further analyze generalization properties. It extends previous works on 1-hidden-layer neural-nets (say, In Arora et al. (2018)). Pros: Establish new time complexity (to my knowledge) of general neural-nets. Cons: It seems far from having a practical implication. Exponential complexity is huge (though common in TCS and IP communities). No simulation was presented. Not sure which part of the approach is useful for practitioners. My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience. More theoretical venues may be a better fit. Other questions: --The authors mentioned \u201cthat is exponential in the size of the architecture (and data dimension) and polynomial in the size of the data set;\u201d and \"this is the best one can hope for due to the NP-Hardness of the problem \". a) The time complexity is exponential in both the size of neural-net and the data dimension (the latter seems to be ignored in abstract). Is there a reference that presents results on NP-hardness in terms of both parameters, or just one parameter? b) The NP-hardness reduction may give an exp. time algorithm. Is there a simple exponential time algorithm? If so, I expect the dependence on the size of the data set is exponential, and the contribution of this paper is to improve to polynomial. The authors mentioned one discretization method, but are there others? More explanation of the importance of the proved time complexity will be helpful. -- Novelty in technical parts: The idea of tree-width graph was introduced in Bienstock and Mun\u0303oz (2018). The main theorem 3.1 is based on explicit construction for Theorem 2.5, and Theorem 2.5 is an immediate generalization of a theorem in Bienstock and Mun\u0303oz (2018) as mentioned in the paper. Thus, this paper looks like an easy extension of Bienstock and Mun\u0303oz (2018) --intuitively, minimizing polynomials by LP seems to be closely related to solving neural-nets problems by LP. Could the authors explain more on the technical novelty? Update after rebuttal: I'd like to thank the authors for the detailed response. It addressed most of my concerns. The analogy with MIP makes some sense, as huge LPs are indeed being solved every day. However, an important difference is that those problems cannot be solved in other better ways till now, while for deep learning people are already successfully solving the current formulations. I still think this work will probably not lead to a major empirical improvement. I just realize that my concern on the practical relevance is largely due to the title \"Principled Deep Neural Network Training through Linear Programming\". It sounds like it can provide a better \"training\" method, but it does not show a practical algorithm that works for CIFAR10 at this stage. The title should not sound like \"look, here is a new method that can change training\", but \"hey, check some new theoretical progress, it may lead to future progress\". I strongly suggest changing the title to something like \"Reformulating DNN as a uniform LP\" or \"polynomial time algorithm in the input dimension\", which reflects the theoretical nature. That being said, the MIP analogy makes me think that there might be some chance that this LP formulation is useful in the future, maybe for solving some special problems that current methods fail miserably. In addition, it does provide a solid theoretical contribution. For those reasons (and assuming the title will be changed), I increase my score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the comments and suggestions . The reviewer raised some important points that we hope the next version will amend . We are currently working on an improved version of the paper that will include these . In the meantime , we would like to comment on all points raised by this review . - `` It seems far from having a practical implication . Exponential complexity is huge ( though common in TCS and IP communities ) . No simulation was presented . Not sure which part of the approach is useful for practitioners . My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience . More theoretical venues may be a better fit . '' We have provided a justification and clarification in an above statement in this forum . We hope you find it satisfactory . - `` The authors mentioned \u201c that is exponential in the size of the architecture ( and data dimension ) and polynomial in the size of the data set ; \u201d and `` this is the best one can hope for due to the NP-Hardness of the problem `` . a ) The time complexity is exponential in both the size of neural-net and the data dimension ( the latter seems to be ignored in abstract ) . Is there a reference that presents results on NP-hardness in terms of both parameters , or just one parameter ? '' The referee raises an interesting point regarding whether these two parameters can be decoupled . The input dimension and the parameter space dimension are typically related to each other in the NP-hardness results . For example , in the paper by Blum and Rivest ( 1992 ) , NP-Hardness of the training problem is proved with respect to a parameter `` n '' which is the input dimension * and * the parameter space dimension ( roughly ) . If we plug in that architecture size into our result , we obtain an exponential dependency on n. In a sense , `` the best one can hope for '' . A recent paper submitted to this conference , entitled `` Complexity of Training ReLU Neural Network '' , works on this subject as well . They prove polynomial time solvability of the training problem for fixed input dimension , however , they consider a * fixed * architecture ( the polynomial dependency is with respect to the sample size ) . If these parameters are decoupled , it is not known if the exponential dependence in the parameter space dimension can be alleviated . Quoting the paper by Arora et al . ( 2018 ) : `` we are not aware of any complexity results which would rule out the possibility of an algorithm which trains to global optimality in time that is polynomial in the data size and/or the number of hidden nodes , assuming that the input dimension is a fixed constant '' As the referee noted , our phrasing is a bit confusing . In the phrase quoted by the reviewer , we were considering all the term `` n+m+N '' to be the `` size of the architecture '' , since the input dimension can be also considered as part of the architecture . We now realize this is not standard , and somewhat confusing , so we will clarify this in the revised version ."}, {"review_id": "HkMwHsCctm-1", "review_text": "This paper studies the problem of proper learning of deep neural network. In particular, the focus is on doing approximate empirical risk minimization over the class of neural networks of a fixed architecture. The main result of the paper is that approximate ERM can be formulated as an LP problem that is of size exponential in the network parameters and the input dimensionality. The paper uses a framework of Bienstock and Munoz that shows how to write a binary optimization problem as a linear problem with size dependent on the treewidth of an appropriate graph associated with the optimization problem. In order to apply the framework, the authors first discretize the parameter space appropriately and then apply analyze the treewidth of the discretized space. The authors also provide treewidth analysis of specific architectures including fully connected networks, and CNNs with various activations. Most of the technical work in the paper involves analyzing the treewidth of the resulting discretized problem. The nice feature of the result is that it holds for worst case data sets, and hence, the exponential dependence on various parameters is unavoidable. On the other hand, it is unclear to me as to how these ideas might eventually lead to practical algorithms or shed light on current training practices in the deep learning community. For instance, it would be very interesting to investigate if under certain assumptions on the data generation process, one can get small LPs that depend exponentially only in the depth, as opposed to the input dimensionality. I also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix. On a technical note, I don't see where the dependence on the input dimensionality appears in Theorem 5.1. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , thank you very much for the thorough review . We appreciate the feedback provided and we are happy that you consider the `` worst-case '' feature of our approach an important one . We are currently working on an improved version of the paper , which will include your suggestions and clarify your concerns . In the meantime , we would like to comment on the points raised in this review . - `` It is unclear to me as to how these ideas might eventually lead to practical algorithms or shed light on current training practices in the deep learning community . '' Given that the three reviewers touched on this aspect we provide a justification and clarification in a separate message in this forum . We hope you find it satisfactory . - `` For instance , it would be very interesting to investigate if under certain assumptions on the data generation process , one can get small LPs that depend exponentially only in the depth , as opposed to the input dimensionality . '' The reviewer is correct . In order to provide a cleaner analysis , we are assuming that all data in `` [ -1,1 ] ^ { n+m } '' is a possible input . The `` n+m '' term in the exponent of the LP sizes is a consequence of this assumption as per our approach . However , as the reviewer noted , one can make use of additional structure in the data generation process in order to alleviate the LP size from the input-dimensionality . If each data point belongs to a set `` U '' , for example , and our grid over `` U '' has `` M '' points then our LP size will depend on `` M '' instead of `` ( 2L/\\epsilon ) ^ { n+m } '' . Only the parameter space dimension will remain in the exponent , as the reviewers suggests . While interesting in its own right , this is beyond the scope of the current paper and left for future work . However , we will add a remark on this in the new manuscript for the curious reader . - `` I also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix . On a technical note , I do n't see where the dependence on the input dimensionality appears in Theorem 5.1 . '' We believe Section 5 is necessary , as Generalization of ERM estimators is an important feature to have . Nonetheless , we agree with the reviewer in that it is not at the core of the main results . The new version will have the entire Generalization discussion in the Appendix . Regarding the dependence on the input dimensionality , it is correct that Theorem 5.1 does not need any . We are working with Lipschitz constants that depend on the infinity norm , which only looks at entries individually . Moreover , this Theorem is only making a statement on the minimum number of data points needed to achieve a certain approximation guarantee , which depends on the distribution of the data ( through `` \\sigma^2 '' ) but not necessarily on their dimension ."}, {"review_id": "HkMwHsCctm-2", "review_text": "This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem using a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. In my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on a recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as Brandon Amos, Lei Xu, J. Zico Kolter: Input Convex Neural Networks. ICML 2017: 146-155 Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them. This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "First of all , thank you very much for the thorough review . We are glad you find our work a solid contribution . We are currently working on an improved version of the paper , which will include your suggestions and clarify your concerns . In the meantime , we would like to comment on all points raised in your review . - `` It would have been nice to also show some practical insights . The main take-aways message is that we need exponential time . Is this practical for networks with with millions of parameters ? Or does this imply that deep learning is hopeless ( in theory ) ? To be fair , the authors touch upon this in the conclusions , but only 1-2 sentences . This discussion should be extended . Nevertheless , I agree that the bridge built is important and may indeed trigger some very important future contributions . '' Thank you very much for viewing our work as providing a bridge between communities ; this was indeed our intention . We will extend the discussion on the exponential dependency of our approach . Regarding the practicality of our approach , given that the three reviewers touched on this aspect we provide a justification and clarification in a separate message in this forum for the three referees . We hope you find it satisfactory . - `` The authors should , however , also review other work on linear programming for deep networks coming from the machine learning community such as Brandon Amos , Lei Xu , J. Zico Kolter : Input Convex Neural Networks . ICML 2017 : 146-155 '' Thank you very much for the pointer to this work . We will review this work and add the corresponding citation in the revised manuscript . - `` Given the background of the average ICLR reader , the authors should also introduce ( at least the intuitions ) improper and proper learning setups in the introduction before using them . This also holds for other terminology from complexity theory . Indeed , the authors can not introduce/review all complexity theory . However , they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic . Without , while important for the ICLR community , the authors run the risk that the paper would better be suited by a learning theory venue . '' Thank you for this suggestion . We will provide a more careful introduction for technical terminology to expand the reach of our paper ."}], "0": {"review_id": "HkMwHsCctm-0", "review_text": "This work reformulates the neural network training as an LP with size that is exponential in the size of the architecture and data dimension, and polynomial in the size of the data set. They further analyze generalization properties. It extends previous works on 1-hidden-layer neural-nets (say, In Arora et al. (2018)). Pros: Establish new time complexity (to my knowledge) of general neural-nets. Cons: It seems far from having a practical implication. Exponential complexity is huge (though common in TCS and IP communities). No simulation was presented. Not sure which part of the approach is useful for practitioners. My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience. More theoretical venues may be a better fit. Other questions: --The authors mentioned \u201cthat is exponential in the size of the architecture (and data dimension) and polynomial in the size of the data set;\u201d and \"this is the best one can hope for due to the NP-Hardness of the problem \". a) The time complexity is exponential in both the size of neural-net and the data dimension (the latter seems to be ignored in abstract). Is there a reference that presents results on NP-hardness in terms of both parameters, or just one parameter? b) The NP-hardness reduction may give an exp. time algorithm. Is there a simple exponential time algorithm? If so, I expect the dependence on the size of the data set is exponential, and the contribution of this paper is to improve to polynomial. The authors mentioned one discretization method, but are there others? More explanation of the importance of the proved time complexity will be helpful. -- Novelty in technical parts: The idea of tree-width graph was introduced in Bienstock and Mun\u0303oz (2018). The main theorem 3.1 is based on explicit construction for Theorem 2.5, and Theorem 2.5 is an immediate generalization of a theorem in Bienstock and Mun\u0303oz (2018) as mentioned in the paper. Thus, this paper looks like an easy extension of Bienstock and Mun\u0303oz (2018) --intuitively, minimizing polynomials by LP seems to be closely related to solving neural-nets problems by LP. Could the authors explain more on the technical novelty? Update after rebuttal: I'd like to thank the authors for the detailed response. It addressed most of my concerns. The analogy with MIP makes some sense, as huge LPs are indeed being solved every day. However, an important difference is that those problems cannot be solved in other better ways till now, while for deep learning people are already successfully solving the current formulations. I still think this work will probably not lead to a major empirical improvement. I just realize that my concern on the practical relevance is largely due to the title \"Principled Deep Neural Network Training through Linear Programming\". It sounds like it can provide a better \"training\" method, but it does not show a practical algorithm that works for CIFAR10 at this stage. The title should not sound like \"look, here is a new method that can change training\", but \"hey, check some new theoretical progress, it may lead to future progress\". I strongly suggest changing the title to something like \"Reformulating DNN as a uniform LP\" or \"polynomial time algorithm in the input dimension\", which reflects the theoretical nature. That being said, the MIP analogy makes me think that there might be some chance that this LP formulation is useful in the future, maybe for solving some special problems that current methods fail miserably. In addition, it does provide a solid theoretical contribution. For those reasons (and assuming the title will be changed), I increase my score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the comments and suggestions . The reviewer raised some important points that we hope the next version will amend . We are currently working on an improved version of the paper that will include these . In the meantime , we would like to comment on all points raised by this review . - `` It seems far from having a practical implication . Exponential complexity is huge ( though common in TCS and IP communities ) . No simulation was presented . Not sure which part of the approach is useful for practitioners . My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience . More theoretical venues may be a better fit . '' We have provided a justification and clarification in an above statement in this forum . We hope you find it satisfactory . - `` The authors mentioned \u201c that is exponential in the size of the architecture ( and data dimension ) and polynomial in the size of the data set ; \u201d and `` this is the best one can hope for due to the NP-Hardness of the problem `` . a ) The time complexity is exponential in both the size of neural-net and the data dimension ( the latter seems to be ignored in abstract ) . Is there a reference that presents results on NP-hardness in terms of both parameters , or just one parameter ? '' The referee raises an interesting point regarding whether these two parameters can be decoupled . The input dimension and the parameter space dimension are typically related to each other in the NP-hardness results . For example , in the paper by Blum and Rivest ( 1992 ) , NP-Hardness of the training problem is proved with respect to a parameter `` n '' which is the input dimension * and * the parameter space dimension ( roughly ) . If we plug in that architecture size into our result , we obtain an exponential dependency on n. In a sense , `` the best one can hope for '' . A recent paper submitted to this conference , entitled `` Complexity of Training ReLU Neural Network '' , works on this subject as well . They prove polynomial time solvability of the training problem for fixed input dimension , however , they consider a * fixed * architecture ( the polynomial dependency is with respect to the sample size ) . If these parameters are decoupled , it is not known if the exponential dependence in the parameter space dimension can be alleviated . Quoting the paper by Arora et al . ( 2018 ) : `` we are not aware of any complexity results which would rule out the possibility of an algorithm which trains to global optimality in time that is polynomial in the data size and/or the number of hidden nodes , assuming that the input dimension is a fixed constant '' As the referee noted , our phrasing is a bit confusing . In the phrase quoted by the reviewer , we were considering all the term `` n+m+N '' to be the `` size of the architecture '' , since the input dimension can be also considered as part of the architecture . We now realize this is not standard , and somewhat confusing , so we will clarify this in the revised version ."}, "1": {"review_id": "HkMwHsCctm-1", "review_text": "This paper studies the problem of proper learning of deep neural network. In particular, the focus is on doing approximate empirical risk minimization over the class of neural networks of a fixed architecture. The main result of the paper is that approximate ERM can be formulated as an LP problem that is of size exponential in the network parameters and the input dimensionality. The paper uses a framework of Bienstock and Munoz that shows how to write a binary optimization problem as a linear problem with size dependent on the treewidth of an appropriate graph associated with the optimization problem. In order to apply the framework, the authors first discretize the parameter space appropriately and then apply analyze the treewidth of the discretized space. The authors also provide treewidth analysis of specific architectures including fully connected networks, and CNNs with various activations. Most of the technical work in the paper involves analyzing the treewidth of the resulting discretized problem. The nice feature of the result is that it holds for worst case data sets, and hence, the exponential dependence on various parameters is unavoidable. On the other hand, it is unclear to me as to how these ideas might eventually lead to practical algorithms or shed light on current training practices in the deep learning community. For instance, it would be very interesting to investigate if under certain assumptions on the data generation process, one can get small LPs that depend exponentially only in the depth, as opposed to the input dimensionality. I also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix. On a technical note, I don't see where the dependence on the input dimensionality appears in Theorem 5.1. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , thank you very much for the thorough review . We appreciate the feedback provided and we are happy that you consider the `` worst-case '' feature of our approach an important one . We are currently working on an improved version of the paper , which will include your suggestions and clarify your concerns . In the meantime , we would like to comment on the points raised in this review . - `` It is unclear to me as to how these ideas might eventually lead to practical algorithms or shed light on current training practices in the deep learning community . '' Given that the three reviewers touched on this aspect we provide a justification and clarification in a separate message in this forum . We hope you find it satisfactory . - `` For instance , it would be very interesting to investigate if under certain assumptions on the data generation process , one can get small LPs that depend exponentially only in the depth , as opposed to the input dimensionality . '' The reviewer is correct . In order to provide a cleaner analysis , we are assuming that all data in `` [ -1,1 ] ^ { n+m } '' is a possible input . The `` n+m '' term in the exponent of the LP sizes is a consequence of this assumption as per our approach . However , as the reviewer noted , one can make use of additional structure in the data generation process in order to alleviate the LP size from the input-dimensionality . If each data point belongs to a set `` U '' , for example , and our grid over `` U '' has `` M '' points then our LP size will depend on `` M '' instead of `` ( 2L/\\epsilon ) ^ { n+m } '' . Only the parameter space dimension will remain in the exponent , as the reviewers suggests . While interesting in its own right , this is beyond the scope of the current paper and left for future work . However , we will add a remark on this in the new manuscript for the curious reader . - `` I also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix . On a technical note , I do n't see where the dependence on the input dimensionality appears in Theorem 5.1 . '' We believe Section 5 is necessary , as Generalization of ERM estimators is an important feature to have . Nonetheless , we agree with the reviewer in that it is not at the core of the main results . The new version will have the entire Generalization discussion in the Appendix . Regarding the dependence on the input dimensionality , it is correct that Theorem 5.1 does not need any . We are working with Lipschitz constants that depend on the infinity norm , which only looks at entries individually . Moreover , this Theorem is only making a statement on the minimum number of data points needed to achieve a certain approximation guarantee , which depends on the distribution of the data ( through `` \\sigma^2 '' ) but not necessarily on their dimension ."}, "2": {"review_id": "HkMwHsCctm-2", "review_text": "This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem using a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. In my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on a recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as Brandon Amos, Lei Xu, J. Zico Kolter: Input Convex Neural Networks. ICML 2017: 146-155 Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them. This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "First of all , thank you very much for the thorough review . We are glad you find our work a solid contribution . We are currently working on an improved version of the paper , which will include your suggestions and clarify your concerns . In the meantime , we would like to comment on all points raised in your review . - `` It would have been nice to also show some practical insights . The main take-aways message is that we need exponential time . Is this practical for networks with with millions of parameters ? Or does this imply that deep learning is hopeless ( in theory ) ? To be fair , the authors touch upon this in the conclusions , but only 1-2 sentences . This discussion should be extended . Nevertheless , I agree that the bridge built is important and may indeed trigger some very important future contributions . '' Thank you very much for viewing our work as providing a bridge between communities ; this was indeed our intention . We will extend the discussion on the exponential dependency of our approach . Regarding the practicality of our approach , given that the three reviewers touched on this aspect we provide a justification and clarification in a separate message in this forum for the three referees . We hope you find it satisfactory . - `` The authors should , however , also review other work on linear programming for deep networks coming from the machine learning community such as Brandon Amos , Lei Xu , J. Zico Kolter : Input Convex Neural Networks . ICML 2017 : 146-155 '' Thank you very much for the pointer to this work . We will review this work and add the corresponding citation in the revised manuscript . - `` Given the background of the average ICLR reader , the authors should also introduce ( at least the intuitions ) improper and proper learning setups in the introduction before using them . This also holds for other terminology from complexity theory . Indeed , the authors can not introduce/review all complexity theory . However , they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic . Without , while important for the ICLR community , the authors run the risk that the paper would better be suited by a learning theory venue . '' Thank you for this suggestion . We will provide a more careful introduction for technical terminology to expand the reach of our paper ."}}