{"year": "2021", "forum": "ZKyd0bkFmom", "title": "Parametric Copula-GP model for analyzing multidimensional neuronal and behavioral relationships", "decision": "Reject", "meta_review": "Summary: The authors built on existing work of GP vine copula\nmodels. Some modifications are made, to conditional marginals and\nmixing. Applications to mutual information estimation are discussed\nand evaluated, and the approach is applied to joint\nneural/behavioral data.\n\n\nDiscussion:\nStrengths mentioned in the reviews are that the\napplication is (from a neuroscience perspective) interesting, that\nestimating mutual information is an important problem, and that the\npaper is very well-written. Weaknesses are the limited novelty (from a\nmachine learning perspective), and weak empirical validation. \n\nThe authors have responded in detail, and were able to clarify a\nnumber of unclear points. Clearly, however, the main criticisms noted\nabove are hard to address in discussion.\n\nDespite the paper being overall clearly written, I agree with\nreviewers that it is hard to tell from\nabstract and introduction where the paper is going (even after\nmodifications made by the authors in the course of the discussion); of\nthe fairly long abstract, just about half a sentence relates to where\nthe proposed model differs from previous work. \n\n\nRecommendation:\nI recommend rejection. Despite some clearly positive aspects, the two main criticisms voiced\nby reviewers are serious: Weak validation and minimal \nnovelty from a machine learning perspective. I agree that the\nneuroscience application may be interesting, but requires more validation.\n\nIf the authors want to pursue this work further, I would suggest to\nperhaps consider first where to position the paper's focus.\nEstimation of mutual information is\na problem that is both hard and important. Any progress here would be\nwelcome, and simple usefulness could offset any lack of model novelty,\nbut it would have to be carefully and comprehensively\nevaluated. On the other hand, a focus on neuroscience applications would\nrequire more emphasis on, and presumably more space in the paper for,\nrelevant experiments. \n", "reviews": [{"review_id": "ZKyd0bkFmom-0", "review_text": "This manuscript models the conditional joint distribution over variables by using Copula models and copula vines . The experimental data shows that when the observed variables are highly correlated that the proposed approach improves estimation of entropy over competing benchmark approaches ( MINE and KSG ) when the variables are highly correlated . Synthetic results demonstrate good improvements , and application to real scientific data seems promising . Strengths : Application to real , novel scientific data shows potential utility of the model . Synthetic results show a good improvement over competing methods , albeit in a limited setup ( highly correlated variables ) Mixtures of copulas seems an effective way to produce model complexity , and by linking it to a GP can make sure that it 's smooth over the conditioned variable x . Weaknesses : Not all modeling steps are clear . In particular , the interaction between the GP prior and the model selection step is undescribed . Since there are multiple ways to do this , needs a full description . No comparison to more neuroscience focuses techniques . Experimental setup and utility is not fully described . Lacks ablation studies to elicit key model components . Questions : The choice to make the link function on the different copula families all dependent on the same $ f $ seems like a strange and limiting choice . Why was this choice made ? The choice to model calcium trace level and not neural spiking seems mathematically convenient for this method , but it 's not clear to me that this is the correct scientific choice . Typically , calcium imaging traces are preprocesses to extract spikes . Is there a scientific rationale for using the raw data ? Or was this primarily motivated by avoiding discrete measurements that would be harder to model in the copula ? Please describe more of the scientific setup . For example , why is there any relationship to licks outside of the reward ? What is the scientific question on this neuroscience application ? Why is entropy relevant , rather than using one of the many predictive problems ? Update after author response : Most of my methodological concerns have been address ( except for the ablation studies ) . The scientific application here is not super well-motivated . It would improve the article greatly to show a greater utility ( or at least , clearly describing future utility for answering scientific questions ) . It is mentioned that `` A full application of the method to study the dependence of contextual signals in mouse visual cortex will be the focus of a follow-up publication . '' That 's vague ; it would be nice to at least clearly discuss how this could be used to facilitate or enhance these scientific experiments .", "rating": "7: Good paper, accept", "reply_text": "The authors would like to thank AnonReviewer2 for helpful feedback . Below we clarify the details of the method and the experimental setup ( in a second comment ) . _ > Not all modeling steps are clear . In particular , the interaction between the GP prior and the model selection step is undescribed . Since there are multiple ways to do this , needs a full description._ We provide the information on the GP priors and model selection in Appx . A.We also intend to release the code together with the paper upon its acceptance . We now share it with the reviewers and AC ( see a separate post ) . _ > No comparison to more neuroscience focuses techniques._ We would be grateful if the reviewer could provide more details on this comment , e.g.which particular techniques would be interesting for the comparison from their perspective . We can comment on the key differences between our Copula-GP model and some of the methods , typically used in neuroscience literature for the analysis of population recordings . First , unlike linear methods ( such as PCA , SVD , CCA ... ) , copula mixtures describe non-linear statistical relationships between variables . If our model was restricted to the use of only Gaussian copulas , it would have been linear and would be equivalent to the typical noise correlation analysis . Second , unlike other statistical non-copula approaches ( such as GLM or GPFA ) , our Copula-GP framework allows us to model the dependencies between elements with utterly different statistics ( e.g.licks vs. velocity ) . In addition , Copula-GP explicitly represents the dependencies as a function of position , revealing insightful information about the task structure . To the best of our knowledge , there are currently no other methods with this combination of features . We have added the above explanation to the discussion . _ > Lacks ablation studies to elicit key model components._ Copula-GP is a Bayesian model . A big advantage of Bayesian models is that the role of each of the components ( copula elements ) can be studied without the need for ablation . For example , plots in Fig.A7 ( right ) show priors over model parameters , where the mixing parameters ( rightmost ) illustrate the key components that describe the data in different contexts ( i.e.in different places in VR in our case ) . Additionally , we explain how to attribute the data-points to certain components of the models in Appx . B.3._ > The choice to make the link function on the different copula families all dependent on the same f seems like a strange and limiting choice . Why was this choice made ? _ Our copula mixture models are parameterized with 2K-1 Gaussian processes ( $ f $ s ) , so each copula element depends on its own $ f $ ( K independent GPs in total ) + the mixture of copulas is parameterized by ( K-1 ) additional mixing parameters . This is the most general parametrization , which can then be constrained . For example , for performing factor analysis , one may consider [ linearly dependent GPs ] ( https : //docs.gpytorch.ai/en/latest/variational.html # lmcvariationalstrategy ) . _ > The choice to model calcium trace level and not neural spiking seems mathematically convenient for this method , but it 's not clear to me that this is the correct scientific choice . Typically , calcium imaging traces are preprocesses to extract spikes . Is there a scientific rationale for using the raw data ? Or was this primarily motivated by avoiding discrete measurements that would be harder to model in the copula ? _ The rationale for using the raw data ( instead of average firing rates , for example , which are also continuous ) was motivated by avoiding additional steps in data processing that introduce biases through arbitrary parameters . Deconvolution algorithms require fine-tuning of parameters that depend on the type of calcium indicator used ( e.g.decay time of the fluorescence signal ) and the cell type under investigation ( e.g.spiking rate ) . Indeed , applying copula analysis to continuous variables is easier than to discrete ones . Yet , the latter is also possible and we plan to extend our package to discrete data as well ( using methods from [ Onken , 2016 ] ( https : //papers.nips.cc/paper/6069-mixed-vine-copulas-as-joint-models-of-spike-counts-and-local-field-potentials ) ) ."}, {"review_id": "ZKyd0bkFmom-1", "review_text": "The authors develop a Gaussian process vine copula model , very much in the flavor of the modeling approach of Lopez-Paz et al . ( 2013 ) .The improvement to the earlier work seems to be a framework for flexible copula modeling including a copula mixture model , approximate inference , model selection , and calculation of mutual information . The paper on its own is a modest improvement on existing work and is both an engineering accomplishment and has the potential for a useful model . The paper is exceptionally well-written and clear . I found it a breeze to read and I credit the authors for that . However , both the validation and the motivation for the model ( namely characterizing the probabilistic relationships for neural and behavioral variables ) seems particularly thin and could be substantially improved . The paper falls short on these points to such a degree that I am hesitant to recommend acceptance since it notably impacts my evaluation of the significance of the works # # Major points : First , the authors claim ( page 7 , paragraph 2 ) that their model out-performs all non-parametric methods . This is by no means obvious form Fig2b , c . Moreover , If the model performs `` similarly '' to MINE , then it is maybe not worth using when a convenient technique already exists . What is the advantage of the present model ? Second , the authors show that the estimation of mutual information is only unbiased for a narrow range of distributions ( Gaussian or transformed Gaussian for small dimensions according to Fig 2 ) and fails for the heavy-tailed Student's-T . However , many neural and behavioral variables are themselves heavy-tailed and the authors did not demonstrate that the real data are sub-gaussian . Third , it seems like there was a wasted opportunity with this paper . The authors spent $ \\approx $ 2/3 of a page discussing the estimation of mutual information without motivating why that was a good example metric that could be derived from their model . This modeling framework is an opportunity to determine virtually any expectation over the entire distribution and it is entirely possible that MI is neither all that interesting , nor does it play to the strengths of the model . They then describe changes to the pairwise distributions of variables from the copula model but we did n't need the copula model to estimate . Finally , what are we to make of the real data results * vis-a-vis * the validation experiments with simulated data in Fig 2 ? Besides what I mentioned above regarding the tails of the real data , its not clear that the variance explained would n't behave differently . Could the authors report the variance explained for their simulation experiments ? # # Minor points : The authors state ( page 7 , last paragraph ) That the `` stimulus-related changes in the joint variability of the two neuronal signals are commonly described as _noise correlations_ . '' But , is n't that the definition of _signal correlations_ ? the authors state ( page 8 , paragraph 5 ) > The Copula-GP \u201c estimated \u201d ( dashed line ) almost perfectly matches the \u201c integrated \u201d result , which suggests that the model was able to tightly approximate both $ p ( u^x|x ) $ and $ p ( u^x ) $ , and , as a result , $ I ( x , \\ { u|^x_ { i < N } \\ } ) $ . However , it is not clear at all that the later statement regarding $ I ( x , \\ { u|^x_ { i < N } \\ } ) $ follows from the former . In fact Figure 2 demonstrates that the integrated result may not estimate $ I ( x , \\ { u|^x_ { i < N } \\ } ) $ well at all .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the thoroughness of the review and the recommendations for improving the motivation of the study . _ > First , the authors claim ( page 7 , paragraph 2 ) that their model out-performs all non-parametric methods . This is by no means obvious form Fig2b , c._ We are grateful to AnonReviewer4 for explaining the misleading impression created by Fig.2.This figure is focused on the specifically difficult cases for the information estimators . Panel ( A ) shows a trivial example , in which MC integration of our model is guaranteed to produce an unbiased entropy estimate , assuming that the implementation of log-likelihood and sampling is correct , which is ensured by the tests ( see the code , provided to reviewers in a private comment ) and correctly estimated parameters . The bias in parameter estimation is addressed separately in validation of parameter inference ( see Appx.B and integration tests in the code ) . Yet , even such a trivial example becomes challenging for the non-parametric models in higher dimensions . Note , that the estimate is unbiased not only for Gaussian , but also for Clayton , Frank or Gumbel copula , or any linear mixture of those . Next , the example in panel ( B ) is specifically designed as a challenging case for the MINE estimator , while the one in panel ( C ) is extremely difficult for our estimator . Thus , the aim of Fig.2 was to show the limitations of these methods and differences between them , rather than their typical performance ( which would depend on the application ) . We have added a few introductory sentences before Fig.2 , explaining that the parametric model is guaranteed to produce unbiased estimates , if the true underlying dependence matches the parametric model . To eliminate any concerns regarding unbiased estimates of the heavy-tailed distributions , we have added an analog of Fig.2A with a Clayton copula into Appx . B.3.We should also emphasize , that MINE estimates depend on the choice of hyper-parameters . In fig.2B , MINE with 200 and 500 hidden units overestimated the mutual information while being theoretically a lower bound . We now change the line style in Fig.2 to draw the reader 's attention to this fact , which was previously only explained in the text . _ > Moreover , If the model performs `` similarly '' to MINE , then it is maybe not worth using when a convenient technique already exists . What is the advantage of the present model ? _ Our model is technically a semi-parametric model , while MINE is non-parametric . We would like to show that Copula-GP can take the best of both worlds with the use of the mixture models . It has a small number of elements and explicit parameterization , so it remains quite interpretable . The interpretable parameters include the copula parameters of the heavy-tailed copula families and the family identities ( see Appx.C.2 on model interpretation ) . Yet , what is impressive , is that Copula-GP performs * at least * similarly to MINE on the examples where it is not guaranteed to perform well . Such a good performance is made possible by mixing of copulas . We acknowledge that we have not properly conveyed this message and we now added an explanation to the Discussion ( see the sentence starting with \u2018 Unlike black-box ... \u2019 ) . _ > Second , the authors show that the estimation of mutual information is only unbiased for a narrow range of distributions ... and fails for the heavy-tailed Student's-T._ The MC estimate is unbiased for any such continuous distribution , for which the dependence can be represented as a linear mixture of the copula families used in our model . Student-T distribution can not be represented as a linear mixture , only approximated . Therefore , Copula-GP approximates Student-T with a mixture of Gaussian and Gumbel copulas , and estimates the mutual information . The result appears to be closer to the ground truth than the estimates from non-parametric methods ( fig.2B , non-dashed lines ) . We also provide variance explained ( R^2 ) for the approximation of the Student T distribution ( 99 % in bivariate case ) , following one of the comments below . Note that the example in Fig.2B is challenging mainly due to the chosen parametrization of Student-T copula : constant correlation , variable degrees of freedom ."}, {"review_id": "ZKyd0bkFmom-2", "review_text": "- This is an interesting neuroscience application where Copula estimation has shown to be effective . - Having said that , I believe that the technical novelty is minimal . To the authors ' credit , they did not claim a huge theoretical edge . They honestly reported the findings of the paper . - The paper is well written . Ideas flow very neatly . - p2 : `` It was previously shown that such a combination of parametric copula models with GP priors ... '' : Precisely .. Therefore , given the work by Hernandez-Lobato et al . ( 2013 ) , as well as the works which have eventually built on top of it , novelty of the proposed method is minimal . I am not saying the application is not useful though . - Along the same lines from above , equations ( 2 ) , ( 3 ) and ( 4 ) which depict the core of the method , are all taken from seminal previous works of copula mixture estimation . - p3 : `` Since none of the aforementioned families alone could describe such conditional dependency , we combined multiple copulas into a linear mixture model '' : Is it possible to elaborate a little bit more on whether this is actually the best technical choice here ? For instance are there any side effects ( e.g.computational ) resulting from using this linear mixture ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank AnonReviewer1 for the constructive comments , which pointed out that the difference of our model from the Hernandez-Lobato et al.2013 was not explained very well and novelty was not emphasized . Therefore , we would like to start the discussion by explaining this difference ( and by modifying the paper accordingly ) . First , and a major difference , is that the marginals are * conditional * . This detail dramatically changes the dependence that the copula is supposed to model ( see $ \\mathbf { u } $ vs. $ \\mathbf { u } ^x $ throughout the paper ) . The difference can be best illustrated with a toy example , which we now provide in Appx.C.1 . In brief , we consider a pair of simulated neurons , which both fire strongly and independently in a certain context ( for certain values of x ) . We apply unconditional ( like in Hernandez-Lobato et al.paper ) and conditional ( like Copula-GP ) marginal models to the virtual recordings from these simulated neurons . We show that empirical copula marginals in an unconditional model are far from being uniform . Therefore , one of the modeling assumptions does not hold , and the copula model fit is poor . This illustrates that copula models with unconditional marginals are not applicable to the complex neuronal data with highly variable marginal statistics . We also show that the model with conditional marginals is more interpretable . In the toy example , our Copula-GP model correctly shows no noise correlation between simulated neurons ( see Fig.A6 ) .The second difference is the use of mixture models . We observed that the same pair of neurons may exhibit , for example , an upper tail dependence in one zone of the VR environment , and lower tail dependence in the other ( see Figure 3D and Figure A7B ( former A5 ) ) , or have a heavy tail in only one particular zone ( Figure 3C and Figure A7A ( former A5 ) ) . Therefore , a linear mixture of copula models with different tail dependencies was a straightforward solution . Finally , there are technical improvements over the Hernandez-Lobato 2013 paper . We use scalable SVI instead of EP and GPU-accelerated PyTorch and GPyTorch libraries . To the best of our knowledge , there are currently no other libraries that implement copula distributions as pytorch distributions . We believe that this makes our code significantly more reusable . The modular structure of our package allows us to change the copula elements or the model selection algorithms with ease . Due to the sequential training of the vines , it is also easy to cast the training onto a multi-GPU system ( or even simulate heterogeneously on CPUs and GPUs ) . We are happy to share the code privately with the reviewers and AC ( see a separate private comment ) , and we intend to make it publicly available upon the acceptance of the paper . Also , see Appx . B.4 ( former B.3 ) for the direct comparison with the Hernandez-Lobato model on the UCL Shuttle dataset . It shows that our Copula-GP model produces higher log-likelihood on the test data , compared to the original Hernandez-Lobato model . _ > Is it possible to elaborate a little bit more on whether this is actually the best technical choice here ? For instance are there any side effects ( e.g.computational ) resulting from using this linear mixture ? _ We now provide a detailed discussion of the advantages of using linear mixtures ( Appx.C.2 & 4 ) . First , linear mixtures are quite interpretable . We can look at the mixing coefficients for the copula elements and immediately tell the qualitative properties of the dependence ( see Appendix C.2 with two examples ) . Second , we can highlight and backtrack the data-points that belong to certain heavy tails . We briefly mention this on page 8 , but the details lay beyond the scope of this paper . Following this reviewer \u2019 s comment , we now added one method for heavy tail detection ( Appx.C.4 ) , which is specific for our mixture model . _ > equations ( 2 ) , ( 3 ) and ( 4 ) which depict the core of the method , are all taken from seminal previous works of copula mixture estimation._ While equations ( 2 ) and ( 4 ) are widely known and are provided as a necessary background on information measures and copulas , we are not aware of any other studies mentioning anything like equation ( 3 ) and the associated methodological proposal . The equation ( 3 ) is proposed by us for our Copula-GP model with * conditional * marginals , and we are not aware of any other works with such marginal models ."}, {"review_id": "ZKyd0bkFmom-3", "review_text": "The authors exploit the expressive power of Copula mixtures to model time-varying multi-modal data , and employ Gaussian Processes to model the time-varying copula parameters . They demonstrate the efficacy of their method using information theoretic metrics on a synthetic dataset and a real-world joint neural-behavioral dataset from a neuroscience experiment . Results demonstrate that the proposed techniques are comparable to the state of the art nonparametric methods , while being more scalable due to the use of stochastic optimization based methods that are commonly used with parametric methods . The paper was quite thorough in the motivation , development and empirical analysis of the proposed technique . The use of Copulas , that are commonly employed in the Finance community , but are rare in statistical neuroscience , should interest the more theoretically inclined reader . Given the accelerating trend of collecting long-term joint neural and behavior in experimental neuroscience , the authors make an interesting and timely contribution to the statistical neuroscience literature . I found that the motivations of the paper were difficult to extract from the Introduction , as there was substantial use of jargon and the intuition behind the technical results were not accessible . To encourage the adoption of their methods in the neuroscience community , the authors should consider improving the readability of their manuscript by making it more friendly to the non-statistician reader .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to AnonReviewer3 for the feedback on our paper . _ > I found that the motivations of the paper were difficult to extract from the Introduction , as there was substantial use of jargon and the intuition behind the technical results were not accessible._ We have now clarified the motivations of our article and extended the introduction to include a more accessible description of the key theoretical concepts ( e.g.mutual information and marginal statistics ) . We hope that these changes would improve the readability of our paper ."}], "0": {"review_id": "ZKyd0bkFmom-0", "review_text": "This manuscript models the conditional joint distribution over variables by using Copula models and copula vines . The experimental data shows that when the observed variables are highly correlated that the proposed approach improves estimation of entropy over competing benchmark approaches ( MINE and KSG ) when the variables are highly correlated . Synthetic results demonstrate good improvements , and application to real scientific data seems promising . Strengths : Application to real , novel scientific data shows potential utility of the model . Synthetic results show a good improvement over competing methods , albeit in a limited setup ( highly correlated variables ) Mixtures of copulas seems an effective way to produce model complexity , and by linking it to a GP can make sure that it 's smooth over the conditioned variable x . Weaknesses : Not all modeling steps are clear . In particular , the interaction between the GP prior and the model selection step is undescribed . Since there are multiple ways to do this , needs a full description . No comparison to more neuroscience focuses techniques . Experimental setup and utility is not fully described . Lacks ablation studies to elicit key model components . Questions : The choice to make the link function on the different copula families all dependent on the same $ f $ seems like a strange and limiting choice . Why was this choice made ? The choice to model calcium trace level and not neural spiking seems mathematically convenient for this method , but it 's not clear to me that this is the correct scientific choice . Typically , calcium imaging traces are preprocesses to extract spikes . Is there a scientific rationale for using the raw data ? Or was this primarily motivated by avoiding discrete measurements that would be harder to model in the copula ? Please describe more of the scientific setup . For example , why is there any relationship to licks outside of the reward ? What is the scientific question on this neuroscience application ? Why is entropy relevant , rather than using one of the many predictive problems ? Update after author response : Most of my methodological concerns have been address ( except for the ablation studies ) . The scientific application here is not super well-motivated . It would improve the article greatly to show a greater utility ( or at least , clearly describing future utility for answering scientific questions ) . It is mentioned that `` A full application of the method to study the dependence of contextual signals in mouse visual cortex will be the focus of a follow-up publication . '' That 's vague ; it would be nice to at least clearly discuss how this could be used to facilitate or enhance these scientific experiments .", "rating": "7: Good paper, accept", "reply_text": "The authors would like to thank AnonReviewer2 for helpful feedback . Below we clarify the details of the method and the experimental setup ( in a second comment ) . _ > Not all modeling steps are clear . In particular , the interaction between the GP prior and the model selection step is undescribed . Since there are multiple ways to do this , needs a full description._ We provide the information on the GP priors and model selection in Appx . A.We also intend to release the code together with the paper upon its acceptance . We now share it with the reviewers and AC ( see a separate post ) . _ > No comparison to more neuroscience focuses techniques._ We would be grateful if the reviewer could provide more details on this comment , e.g.which particular techniques would be interesting for the comparison from their perspective . We can comment on the key differences between our Copula-GP model and some of the methods , typically used in neuroscience literature for the analysis of population recordings . First , unlike linear methods ( such as PCA , SVD , CCA ... ) , copula mixtures describe non-linear statistical relationships between variables . If our model was restricted to the use of only Gaussian copulas , it would have been linear and would be equivalent to the typical noise correlation analysis . Second , unlike other statistical non-copula approaches ( such as GLM or GPFA ) , our Copula-GP framework allows us to model the dependencies between elements with utterly different statistics ( e.g.licks vs. velocity ) . In addition , Copula-GP explicitly represents the dependencies as a function of position , revealing insightful information about the task structure . To the best of our knowledge , there are currently no other methods with this combination of features . We have added the above explanation to the discussion . _ > Lacks ablation studies to elicit key model components._ Copula-GP is a Bayesian model . A big advantage of Bayesian models is that the role of each of the components ( copula elements ) can be studied without the need for ablation . For example , plots in Fig.A7 ( right ) show priors over model parameters , where the mixing parameters ( rightmost ) illustrate the key components that describe the data in different contexts ( i.e.in different places in VR in our case ) . Additionally , we explain how to attribute the data-points to certain components of the models in Appx . B.3._ > The choice to make the link function on the different copula families all dependent on the same f seems like a strange and limiting choice . Why was this choice made ? _ Our copula mixture models are parameterized with 2K-1 Gaussian processes ( $ f $ s ) , so each copula element depends on its own $ f $ ( K independent GPs in total ) + the mixture of copulas is parameterized by ( K-1 ) additional mixing parameters . This is the most general parametrization , which can then be constrained . For example , for performing factor analysis , one may consider [ linearly dependent GPs ] ( https : //docs.gpytorch.ai/en/latest/variational.html # lmcvariationalstrategy ) . _ > The choice to model calcium trace level and not neural spiking seems mathematically convenient for this method , but it 's not clear to me that this is the correct scientific choice . Typically , calcium imaging traces are preprocesses to extract spikes . Is there a scientific rationale for using the raw data ? Or was this primarily motivated by avoiding discrete measurements that would be harder to model in the copula ? _ The rationale for using the raw data ( instead of average firing rates , for example , which are also continuous ) was motivated by avoiding additional steps in data processing that introduce biases through arbitrary parameters . Deconvolution algorithms require fine-tuning of parameters that depend on the type of calcium indicator used ( e.g.decay time of the fluorescence signal ) and the cell type under investigation ( e.g.spiking rate ) . Indeed , applying copula analysis to continuous variables is easier than to discrete ones . Yet , the latter is also possible and we plan to extend our package to discrete data as well ( using methods from [ Onken , 2016 ] ( https : //papers.nips.cc/paper/6069-mixed-vine-copulas-as-joint-models-of-spike-counts-and-local-field-potentials ) ) ."}, "1": {"review_id": "ZKyd0bkFmom-1", "review_text": "The authors develop a Gaussian process vine copula model , very much in the flavor of the modeling approach of Lopez-Paz et al . ( 2013 ) .The improvement to the earlier work seems to be a framework for flexible copula modeling including a copula mixture model , approximate inference , model selection , and calculation of mutual information . The paper on its own is a modest improvement on existing work and is both an engineering accomplishment and has the potential for a useful model . The paper is exceptionally well-written and clear . I found it a breeze to read and I credit the authors for that . However , both the validation and the motivation for the model ( namely characterizing the probabilistic relationships for neural and behavioral variables ) seems particularly thin and could be substantially improved . The paper falls short on these points to such a degree that I am hesitant to recommend acceptance since it notably impacts my evaluation of the significance of the works # # Major points : First , the authors claim ( page 7 , paragraph 2 ) that their model out-performs all non-parametric methods . This is by no means obvious form Fig2b , c . Moreover , If the model performs `` similarly '' to MINE , then it is maybe not worth using when a convenient technique already exists . What is the advantage of the present model ? Second , the authors show that the estimation of mutual information is only unbiased for a narrow range of distributions ( Gaussian or transformed Gaussian for small dimensions according to Fig 2 ) and fails for the heavy-tailed Student's-T . However , many neural and behavioral variables are themselves heavy-tailed and the authors did not demonstrate that the real data are sub-gaussian . Third , it seems like there was a wasted opportunity with this paper . The authors spent $ \\approx $ 2/3 of a page discussing the estimation of mutual information without motivating why that was a good example metric that could be derived from their model . This modeling framework is an opportunity to determine virtually any expectation over the entire distribution and it is entirely possible that MI is neither all that interesting , nor does it play to the strengths of the model . They then describe changes to the pairwise distributions of variables from the copula model but we did n't need the copula model to estimate . Finally , what are we to make of the real data results * vis-a-vis * the validation experiments with simulated data in Fig 2 ? Besides what I mentioned above regarding the tails of the real data , its not clear that the variance explained would n't behave differently . Could the authors report the variance explained for their simulation experiments ? # # Minor points : The authors state ( page 7 , last paragraph ) That the `` stimulus-related changes in the joint variability of the two neuronal signals are commonly described as _noise correlations_ . '' But , is n't that the definition of _signal correlations_ ? the authors state ( page 8 , paragraph 5 ) > The Copula-GP \u201c estimated \u201d ( dashed line ) almost perfectly matches the \u201c integrated \u201d result , which suggests that the model was able to tightly approximate both $ p ( u^x|x ) $ and $ p ( u^x ) $ , and , as a result , $ I ( x , \\ { u|^x_ { i < N } \\ } ) $ . However , it is not clear at all that the later statement regarding $ I ( x , \\ { u|^x_ { i < N } \\ } ) $ follows from the former . In fact Figure 2 demonstrates that the integrated result may not estimate $ I ( x , \\ { u|^x_ { i < N } \\ } ) $ well at all .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the thoroughness of the review and the recommendations for improving the motivation of the study . _ > First , the authors claim ( page 7 , paragraph 2 ) that their model out-performs all non-parametric methods . This is by no means obvious form Fig2b , c._ We are grateful to AnonReviewer4 for explaining the misleading impression created by Fig.2.This figure is focused on the specifically difficult cases for the information estimators . Panel ( A ) shows a trivial example , in which MC integration of our model is guaranteed to produce an unbiased entropy estimate , assuming that the implementation of log-likelihood and sampling is correct , which is ensured by the tests ( see the code , provided to reviewers in a private comment ) and correctly estimated parameters . The bias in parameter estimation is addressed separately in validation of parameter inference ( see Appx.B and integration tests in the code ) . Yet , even such a trivial example becomes challenging for the non-parametric models in higher dimensions . Note , that the estimate is unbiased not only for Gaussian , but also for Clayton , Frank or Gumbel copula , or any linear mixture of those . Next , the example in panel ( B ) is specifically designed as a challenging case for the MINE estimator , while the one in panel ( C ) is extremely difficult for our estimator . Thus , the aim of Fig.2 was to show the limitations of these methods and differences between them , rather than their typical performance ( which would depend on the application ) . We have added a few introductory sentences before Fig.2 , explaining that the parametric model is guaranteed to produce unbiased estimates , if the true underlying dependence matches the parametric model . To eliminate any concerns regarding unbiased estimates of the heavy-tailed distributions , we have added an analog of Fig.2A with a Clayton copula into Appx . B.3.We should also emphasize , that MINE estimates depend on the choice of hyper-parameters . In fig.2B , MINE with 200 and 500 hidden units overestimated the mutual information while being theoretically a lower bound . We now change the line style in Fig.2 to draw the reader 's attention to this fact , which was previously only explained in the text . _ > Moreover , If the model performs `` similarly '' to MINE , then it is maybe not worth using when a convenient technique already exists . What is the advantage of the present model ? _ Our model is technically a semi-parametric model , while MINE is non-parametric . We would like to show that Copula-GP can take the best of both worlds with the use of the mixture models . It has a small number of elements and explicit parameterization , so it remains quite interpretable . The interpretable parameters include the copula parameters of the heavy-tailed copula families and the family identities ( see Appx.C.2 on model interpretation ) . Yet , what is impressive , is that Copula-GP performs * at least * similarly to MINE on the examples where it is not guaranteed to perform well . Such a good performance is made possible by mixing of copulas . We acknowledge that we have not properly conveyed this message and we now added an explanation to the Discussion ( see the sentence starting with \u2018 Unlike black-box ... \u2019 ) . _ > Second , the authors show that the estimation of mutual information is only unbiased for a narrow range of distributions ... and fails for the heavy-tailed Student's-T._ The MC estimate is unbiased for any such continuous distribution , for which the dependence can be represented as a linear mixture of the copula families used in our model . Student-T distribution can not be represented as a linear mixture , only approximated . Therefore , Copula-GP approximates Student-T with a mixture of Gaussian and Gumbel copulas , and estimates the mutual information . The result appears to be closer to the ground truth than the estimates from non-parametric methods ( fig.2B , non-dashed lines ) . We also provide variance explained ( R^2 ) for the approximation of the Student T distribution ( 99 % in bivariate case ) , following one of the comments below . Note that the example in Fig.2B is challenging mainly due to the chosen parametrization of Student-T copula : constant correlation , variable degrees of freedom ."}, "2": {"review_id": "ZKyd0bkFmom-2", "review_text": "- This is an interesting neuroscience application where Copula estimation has shown to be effective . - Having said that , I believe that the technical novelty is minimal . To the authors ' credit , they did not claim a huge theoretical edge . They honestly reported the findings of the paper . - The paper is well written . Ideas flow very neatly . - p2 : `` It was previously shown that such a combination of parametric copula models with GP priors ... '' : Precisely .. Therefore , given the work by Hernandez-Lobato et al . ( 2013 ) , as well as the works which have eventually built on top of it , novelty of the proposed method is minimal . I am not saying the application is not useful though . - Along the same lines from above , equations ( 2 ) , ( 3 ) and ( 4 ) which depict the core of the method , are all taken from seminal previous works of copula mixture estimation . - p3 : `` Since none of the aforementioned families alone could describe such conditional dependency , we combined multiple copulas into a linear mixture model '' : Is it possible to elaborate a little bit more on whether this is actually the best technical choice here ? For instance are there any side effects ( e.g.computational ) resulting from using this linear mixture ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank AnonReviewer1 for the constructive comments , which pointed out that the difference of our model from the Hernandez-Lobato et al.2013 was not explained very well and novelty was not emphasized . Therefore , we would like to start the discussion by explaining this difference ( and by modifying the paper accordingly ) . First , and a major difference , is that the marginals are * conditional * . This detail dramatically changes the dependence that the copula is supposed to model ( see $ \\mathbf { u } $ vs. $ \\mathbf { u } ^x $ throughout the paper ) . The difference can be best illustrated with a toy example , which we now provide in Appx.C.1 . In brief , we consider a pair of simulated neurons , which both fire strongly and independently in a certain context ( for certain values of x ) . We apply unconditional ( like in Hernandez-Lobato et al.paper ) and conditional ( like Copula-GP ) marginal models to the virtual recordings from these simulated neurons . We show that empirical copula marginals in an unconditional model are far from being uniform . Therefore , one of the modeling assumptions does not hold , and the copula model fit is poor . This illustrates that copula models with unconditional marginals are not applicable to the complex neuronal data with highly variable marginal statistics . We also show that the model with conditional marginals is more interpretable . In the toy example , our Copula-GP model correctly shows no noise correlation between simulated neurons ( see Fig.A6 ) .The second difference is the use of mixture models . We observed that the same pair of neurons may exhibit , for example , an upper tail dependence in one zone of the VR environment , and lower tail dependence in the other ( see Figure 3D and Figure A7B ( former A5 ) ) , or have a heavy tail in only one particular zone ( Figure 3C and Figure A7A ( former A5 ) ) . Therefore , a linear mixture of copula models with different tail dependencies was a straightforward solution . Finally , there are technical improvements over the Hernandez-Lobato 2013 paper . We use scalable SVI instead of EP and GPU-accelerated PyTorch and GPyTorch libraries . To the best of our knowledge , there are currently no other libraries that implement copula distributions as pytorch distributions . We believe that this makes our code significantly more reusable . The modular structure of our package allows us to change the copula elements or the model selection algorithms with ease . Due to the sequential training of the vines , it is also easy to cast the training onto a multi-GPU system ( or even simulate heterogeneously on CPUs and GPUs ) . We are happy to share the code privately with the reviewers and AC ( see a separate private comment ) , and we intend to make it publicly available upon the acceptance of the paper . Also , see Appx . B.4 ( former B.3 ) for the direct comparison with the Hernandez-Lobato model on the UCL Shuttle dataset . It shows that our Copula-GP model produces higher log-likelihood on the test data , compared to the original Hernandez-Lobato model . _ > Is it possible to elaborate a little bit more on whether this is actually the best technical choice here ? For instance are there any side effects ( e.g.computational ) resulting from using this linear mixture ? _ We now provide a detailed discussion of the advantages of using linear mixtures ( Appx.C.2 & 4 ) . First , linear mixtures are quite interpretable . We can look at the mixing coefficients for the copula elements and immediately tell the qualitative properties of the dependence ( see Appendix C.2 with two examples ) . Second , we can highlight and backtrack the data-points that belong to certain heavy tails . We briefly mention this on page 8 , but the details lay beyond the scope of this paper . Following this reviewer \u2019 s comment , we now added one method for heavy tail detection ( Appx.C.4 ) , which is specific for our mixture model . _ > equations ( 2 ) , ( 3 ) and ( 4 ) which depict the core of the method , are all taken from seminal previous works of copula mixture estimation._ While equations ( 2 ) and ( 4 ) are widely known and are provided as a necessary background on information measures and copulas , we are not aware of any other studies mentioning anything like equation ( 3 ) and the associated methodological proposal . The equation ( 3 ) is proposed by us for our Copula-GP model with * conditional * marginals , and we are not aware of any other works with such marginal models ."}, "3": {"review_id": "ZKyd0bkFmom-3", "review_text": "The authors exploit the expressive power of Copula mixtures to model time-varying multi-modal data , and employ Gaussian Processes to model the time-varying copula parameters . They demonstrate the efficacy of their method using information theoretic metrics on a synthetic dataset and a real-world joint neural-behavioral dataset from a neuroscience experiment . Results demonstrate that the proposed techniques are comparable to the state of the art nonparametric methods , while being more scalable due to the use of stochastic optimization based methods that are commonly used with parametric methods . The paper was quite thorough in the motivation , development and empirical analysis of the proposed technique . The use of Copulas , that are commonly employed in the Finance community , but are rare in statistical neuroscience , should interest the more theoretically inclined reader . Given the accelerating trend of collecting long-term joint neural and behavior in experimental neuroscience , the authors make an interesting and timely contribution to the statistical neuroscience literature . I found that the motivations of the paper were difficult to extract from the Introduction , as there was substantial use of jargon and the intuition behind the technical results were not accessible . To encourage the adoption of their methods in the neuroscience community , the authors should consider improving the readability of their manuscript by making it more friendly to the non-statistician reader .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are grateful to AnonReviewer3 for the feedback on our paper . _ > I found that the motivations of the paper were difficult to extract from the Introduction , as there was substantial use of jargon and the intuition behind the technical results were not accessible._ We have now clarified the motivations of our article and extended the introduction to include a more accessible description of the key theoretical concepts ( e.g.mutual information and marginal statistics ) . We hope that these changes would improve the readability of our paper ."}}