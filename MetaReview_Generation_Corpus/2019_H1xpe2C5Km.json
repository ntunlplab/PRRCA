{"year": "2019", "forum": "H1xpe2C5Km", "title": "Trace-back along capsules and its application on semantic segmentation  \t\t", "decision": "Reject", "meta_review": "This paper proposes a method for tracing activations in a capsule-based network in order to obtain semantic segmentation from classification predictions.\n\nReviewers 1 and 2 rate the paper as marginally above threshold, while Reviewer 3 rates it as marginally below. Reviewer 3 particularly points to experimental validation as a major weakness, stating: \"not sure if the method will generalize well beyond MNIST\", \"I\u2019m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only.\"\n\nThe AC shares these concerns and does not believe the current experimental validation is sufficient. MNIST is a toy dataset, and may have been appropriate for introducing capsules as a new concept, but it is simply not difficult enough to serve as a quantitative benchmark to distinguish capsule performance from U-Net. U-Net and Tr-CapsNet appear to have similar performance on both MNIST and the hippocampus dataset; the relatively small advantage to Tr-CapsNet is not convincing.\n\nFurthermore, as Reviewer 1 suggests, it would seem appropriate to include experimental comparison to other capsule-based segmentation approaches (e.g. LaLonde and Bagci, Capsules for Object Segmentation, 2018). This related work is mentioned, but not used as an experimental baseline.\n", "reviews": [{"review_id": "H1xpe2C5Km-0", "review_text": "Authors present a trace-back mechanism to associate lowest level of Capsules with their respective classes. Their method effectively gets better segmentation results on the two (relatively small) datasets. Authors explore an original idea with good quality of experiments (relatively strong baseline, proper experimental setup). They also back up their claim on advantage of classification with the horizontal redaction experiment. The manuscript can benefit from a more clear description of the architecture used for each set of experiments. Specially how the upsampling is connected to the traceback layer. This is an interesting idea that can probably generalize to CNNs with attention and tracing back the attention in a typical CNN as well. Pros: The idea behind tracing the part-whole assignments back to primary capsule layer is interesting and original. It increases the resolution significantly in compare to disregarding the connections in the encoder (up to class capsules). The comparisons on MNIST & the Hippocampus dataset w.r.t the U-Net baseline are compelling and indicate a significant performance boost. Cons: Although the classification signal is counted as the advantage of this system, it is not clear how it will adopt to multi-class scenarios which is one of the major applications of segmentation (such as SUN dataset). The assumption that convolutional capsules can have multiple parents is incorrect. In Hinton 2018, where they use convolutional Capsule layers, the normalization for each position of a capsule in layer below is done separately and each position of each capsule type has the one-parent assumption. However, since in this work only primary capsules and class capsules are used this does not concern the current experiment results in this paper. The related work section should expand more on the SOTA segmentation techniques and the significance of this work including [2]. Question: How is the traceback layer converted to image mask? After one gets p(c_k | i) for all primary capsules, are primary capsule pose parameters multiplied by their p(c_k |i ) and passed all to a deconv layer? Authors should specify in the manuscript the details of the upsampling layer (s) used in their architecture. It is only mentioned that deconv, dilated, bilinear interpolation are options. Which one is used in the end and how many is not clear. Comments: For the Hippocampus dataset, the ensemble U-Net approach used in [1] is close to your baseline and should be mentioned cited as the related work, SOTA on the dataset. Also since they use all 9 views have you considered accessing all the 9 views as well? [1]: Hippocampus segmentation through multi-view ensemble ConvNets Yani Chen ; Bibo Shi ; Zhewei Wang ; Pin Zhang ; Charles D. Smith ; Jundong Liu [2]: RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > The manuscript can benefit from a more clear description of the architecture used for each set of experiments . Specially how the upsampling is connected to the traceback layer . Response : \u200b we agree with the reviewer \u2019 s assessment . To clear up our presentation , we rewrote the description of our proposed Tr-CapsNet model , added figure 2 to illustrate the overall network structure and the traceback pipeline . > > Although the classification signal is counted as the advantage of this system , it is not clear how it will adopt to multi-class scenarios which is one of the major applications of segmentation ( such as SUN dataset ) . Response : \u200b we added a paragraph titled \u201c From capsule-based recognition to capsule-based segmentation and beyond \u201d to explain the assumptions and limitations in the discussion section , on page 10 . > > The assumption that convolutional capsules can have multiple parents is incorrect . In Hinton 2018 , where they use convolutional Capsule layers , the normalization for each position of a capsule in layer below is done separately and each position of each capsule type has the one-parent assumption . However , since in this work only primary capsules and class capsules are used this does not concern the current experiment results in this paper . Response : Here is our understanding\u200b . Just like a convolutional layer in CNN , each capsule in Layer L+1 takes capsules in an area of Layer L ( sized kernel_size * kernel_size , i.e.receptive field ) as its inputs . If the stride is set smaller than the kernel_size , \u201c each convolutional instance of a capsule in layer L receives at most ( kernel_size * kernel_size ) feedback from each capsule type in layer L + 1. \u201d ( Hinton 2018 ) . With respect to capsule networks , a significant assumption ( Sabour 2017 and Hinton 2018 ) is that there is only one instance of the entity at a location . As for multi-instances issue , please refer to the \u201c from capsule-based recognition to capsule-based segmentation and beyond \u201d paragraph for our thoughts . > > The related work section should expand more on the SOTA segmentation techniques and the significance of this work including [ 2 ] . Response : We did an extensive literature review on the recent FCN based semantic segmentation and included them the \u201c Discussion and Related Work \u201d section on page 10 . On page 6 , we also include a paragraph and a figure to provide more details regarding Hippocampus segmentation , as well as the SOTA solutions . We also commented and compared our work with the ( sole ) capsule-based segmentation solution on page 10 . > > How is the traceback layer converted to image mask ? After one gets p ( c_k | i ) for all primary capsules , are primary capsule pose parameters multiplied by their p ( c_k |i ) and passed all to a deconv layer ? Response : After P ( c_k | i ) is available for each primary capsule , it will be multiplied by P ( i ) , the presence probability of the corresponding capsule , to produce P ( c_k ) . Eqn . ( 1 ) on page 4 ( revised paper ) shows how P ( c_k ) is generated through the traceback pipeline . > > Authors should specify in the manuscript the details of the upsampling layer ( s ) used in their architecture . It is only mentioned that deconv , dilated , bilinear interpolation are options . Which one is used in the end and how many is not clear . Response : we used the deconvolution scheme in Long et al . ( 2015 ) in this paper . In the revised paper , we rewrote the description to provide detailed and better presented description of the network modules . > > For the Hippocampus dataset , the ensemble U-Net approach used in [ 1 ] is close to your baseline and should be mentioned cited as the related work , SOTA on the dataset . Also since they use all 9 views have you considered accessing all the 9 views as well ? Response : We cite [ 1 ] and several more SOTA solutions in the revised manuscript . The main focus of this work is to explore the power of capsules in semantic segmentation , rather than an extremely accurate hippocampus segmentation solution . The 9 views in [ 1 ] needs to combined through an ensemble net , which is not the interest of this work ."}, {"review_id": "H1xpe2C5Km-1", "review_text": "This paper proposes a traceback layer for capsule networks to do semantic segmentation. Comparing to previous works that use capsule networks for semantic segmentation, this paper makes explicit use of part-whole relationship in the capsule layers. Experiments are done on modified MNIST and Hippocampus dataset. Results demonstrate encouraging improvements over U-Net. The writing could be tremendously improved if some background of the capsule networks is included. I have a question about the traceback layer. It seems to me that the traceback layer re-uses the learned weights c_{ij} between the primary capsules and the class capsules as guidance when \u201cdistributing\u201d class probabilities to a spatial class probabilistic heatmap. One piece of information I feel missing is the affine transformation that happens between the primary capsule and the class capsule. The traceback layer doesn\u2019t seem to invert such a transformation. Should it do so? Since there have been works that use capsule networks for semantic segmentation, does it make sense to compare to them (e.g. LaLonde & Bagci, 2018) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > The writing could be tremendously improved if some background of the capsule networks is included . Response : we agree with this assessment . Based this comment , we added paragraphs in the Background section ( page 2 ) to provide more details on capsule and capsule nets . Comparisons are also made with CNNs . > > I have a question about the traceback layer . It seems to me that the traceback layer re-uses the learned weights c_ { ij } between the primary capsules and the class capsules as guidance when \u201c distributing \u201d class probabilities to a spatial class probabilistic heatmap . Response : the assessment is accurate . We take a good use of c_ij to compute P ( c_k ) , the class probability maps for capsule layers . We added figure 2 and rewrote the description to explain the architecture of our proposed Tr-CapsNet model , as well as the traceback procedure . > > One piece of information I feel missing is the affine transformation that happens between the primary capsule and the class capsule . The traceback layer doesn \u2019 t seem to invert such a transformation . Should it do so ? Response : In capsule net , the weight matrix W between capsule layers indeed represents a linear transformation that would map parts of an object into a cluster for the same whole . This is one of the major differences between capsule nets and CNNs . The purpose of our traceback pipeline is to estimate the ( inverse ) part-whole relations , based on which we can derive the class probability maps for capsule layers and later pixels . We observed such maps can be inferred through repeated applications of the product rule and the sum rule in probability theory , and only c_ij would be needed . In other words , W is not needed nor modified over the traceback pipeline . Additional introduction on capsule nets , as well as the comparisons with CNNs are added into the background section ( page 2 ) . > > Since there have been works that use capsule networks for semantic segmentation , does it make sense to compare to them ( e.g.LaLonde & Bagci , 2018 ) ? Response : we commented on LaLonde & Bagci \u2019 s work in Discussion and Related Work section . Additional thoughts on their work and our models have been added into this revised manuscript ( on page 10 ) . We also add two paragraphs to comment on capsule based solutions in general ."}, {"review_id": "H1xpe2C5Km-2", "review_text": "Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset. The paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model? Additionally, think about a pseudo-code for improved understandability. Some minor concerns/ notes to the authors: 1. At page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters? 2. Page 6; baseline model: Why do you removed the pooling layers? 3. I\u2019m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I\u2019m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar. 4. Why is the magnitude of lambda1 so different between the two dataset that you used? 5. Could you add the inference times to your tables and discuss that in addition? 6. What kind of noise is added to MNIST? 7. What is the state-of-the-art performance on the Hippocampus dataset? 8. What would be the performance in your experiments with a MaskRCNN segmentation network? 9. I\u2019m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. 10. For both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix\u2026 My major concern is that both datasets are not dealing with real background noise. I\u2019m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete. To make your results transparent you could think about to publish the code somewhere. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> > The paper is well-written and well-explained . Nevertheless , I think it would be useful to have some illustrations about the network architecture . Some stuff which is explained in text could be easily visualized in a flow chart . For example , the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart . With the text only , it is hard to follow . Response : thanks for the assessment , and we fully agree that more illustrations should be added to make our approach more understable . In this revised manuscript , we added figure 2 ( flow chart ) and rewrote the first paragraph of Architecture section to describe the modules of our Tr-CapsNet . > > Please think about some plots in the final version or in the appendix . One question which is aligned to that : How many convolutional filters are used in the baseline model ? Response : we added an Appendix in this version to provide the detailed network configurations . The number of layers and number of filters in each layer are all listed there . > > 1.At page 5 : You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune . But in the results you are not explaining how the parameters were tuned . So my question is : How do you tuned the parameters ? In which range do you varied the parameters ? Response : we agree with the reviewer , and we added a paragraph on page 7 to provide more \u201c Implementation details \u201d . As for the setting of lambda1 and lambda 2 , \u201c the ratio was selected from { 1 ~ 5 } for the MNIST experiment . In the Hippocampus experiment , we observed that the recognition task was far more difficult , largely due to the greater complexity of the dataset . Accordingly , we selected the ratio from a larger range , { 1 \u223c 20 } with an interval of 1. \u201d > > 2 . Page 6 ; baseline model : Why do you removed the pooling layers ? Response : We didn \u2019 t use max-pooling operation in the modified U-Net , based on two observations/considerations : 1 ) based on [ 1 ] max-pooling can simply be replaced by a convolution layer with increased stride without loss in accuracy ; 2 ) our Tr-CapsNet doesn \u2019 t have pooling operations , and instead relies on stride to achieve multi-scale processing . We figure using a modified U-Net with wide stride instead of pooling would facilitate the comparison , analysis and future development the models . [ 1 ] Springenberg , Jost Tobias , Alexey Dosovitskiy , Thomas Brox , and Martin Riedmiller . `` Striving for simplicity : The all convolutional net . '' arXiv preprint arXiv:1412.6806 ( 2014 ) . > > 3.I \u2019 m curious about the number of parameters in each model . To have a valid discussion about your model is better than the U-Net-6 architecture , I would take into account the number of parameters . In case that your model is noticeably greater , it could be that your increased performance is just due to more parameters . As long as your discussion is without the number of parameters I \u2019 m not convinced that your model is better . A comparison between models should be always fair if two models are architectural similar . Response : again we fully agree with the reviewer on this regard . In the revised version , we added the number of parameters for each model , which can be seen in Tables 1 and 2 . Two observations : 1 ) both U-Net and our Tr-CapsNet have variable numbers of parameters , depending on the setting , and it \u2019 s not the case that Tr-CapsNets have a lot of more parameters than U-Nets ; 2 ) more parameters do not automatically translate into better performance . This can be seen in table 1 , where Tr-CapsNet-9 ( 1:1 ) has the best performance , while it doesn \u2019 t have the most parameters . In table 2 , U-Net of 1.14M parameters actually outperforms the version of 2.19M parameters . > > 4.Why is the magnitude of lambda1 so different between the two dataset that you used ? Response : As the Hippocampus dataset is obviously more complicated than MNIST , in terms of intensity separability , a large weight on recognition term is needed , and it should help to \u201c force the network to build more accurate part-whole relationships , which lays a solid foundation to produce more accurate overall segmentations \u201d . We added the above explanation to the revised paper . > > 5.Could you add the inference times to your tables and discuss that in addition ? Response : In both capsule nets and our Tr-CapsNets , an iterative routing-by-agreement mechanism is used . In this mechanism each capsule chooses its parent capsule in the higher layer through an iterative routing procedure . This is certainly a downside comparing with CNNs . The average inference time for one Hippocampal slice , the U-Net ( feature map 4 x 20 ) and Tr-CapsNet ( 4 x 20 , 1 ) take roughly 0.3 ms and 0.6 ms , respectively ."}], "0": {"review_id": "H1xpe2C5Km-0", "review_text": "Authors present a trace-back mechanism to associate lowest level of Capsules with their respective classes. Their method effectively gets better segmentation results on the two (relatively small) datasets. Authors explore an original idea with good quality of experiments (relatively strong baseline, proper experimental setup). They also back up their claim on advantage of classification with the horizontal redaction experiment. The manuscript can benefit from a more clear description of the architecture used for each set of experiments. Specially how the upsampling is connected to the traceback layer. This is an interesting idea that can probably generalize to CNNs with attention and tracing back the attention in a typical CNN as well. Pros: The idea behind tracing the part-whole assignments back to primary capsule layer is interesting and original. It increases the resolution significantly in compare to disregarding the connections in the encoder (up to class capsules). The comparisons on MNIST & the Hippocampus dataset w.r.t the U-Net baseline are compelling and indicate a significant performance boost. Cons: Although the classification signal is counted as the advantage of this system, it is not clear how it will adopt to multi-class scenarios which is one of the major applications of segmentation (such as SUN dataset). The assumption that convolutional capsules can have multiple parents is incorrect. In Hinton 2018, where they use convolutional Capsule layers, the normalization for each position of a capsule in layer below is done separately and each position of each capsule type has the one-parent assumption. However, since in this work only primary capsules and class capsules are used this does not concern the current experiment results in this paper. The related work section should expand more on the SOTA segmentation techniques and the significance of this work including [2]. Question: How is the traceback layer converted to image mask? After one gets p(c_k | i) for all primary capsules, are primary capsule pose parameters multiplied by their p(c_k |i ) and passed all to a deconv layer? Authors should specify in the manuscript the details of the upsampling layer (s) used in their architecture. It is only mentioned that deconv, dilated, bilinear interpolation are options. Which one is used in the end and how many is not clear. Comments: For the Hippocampus dataset, the ensemble U-Net approach used in [1] is close to your baseline and should be mentioned cited as the related work, SOTA on the dataset. Also since they use all 9 views have you considered accessing all the 9 views as well? [1]: Hippocampus segmentation through multi-view ensemble ConvNets Yani Chen ; Bibo Shi ; Zhewei Wang ; Pin Zhang ; Charles D. Smith ; Jundong Liu [2]: RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > The manuscript can benefit from a more clear description of the architecture used for each set of experiments . Specially how the upsampling is connected to the traceback layer . Response : \u200b we agree with the reviewer \u2019 s assessment . To clear up our presentation , we rewrote the description of our proposed Tr-CapsNet model , added figure 2 to illustrate the overall network structure and the traceback pipeline . > > Although the classification signal is counted as the advantage of this system , it is not clear how it will adopt to multi-class scenarios which is one of the major applications of segmentation ( such as SUN dataset ) . Response : \u200b we added a paragraph titled \u201c From capsule-based recognition to capsule-based segmentation and beyond \u201d to explain the assumptions and limitations in the discussion section , on page 10 . > > The assumption that convolutional capsules can have multiple parents is incorrect . In Hinton 2018 , where they use convolutional Capsule layers , the normalization for each position of a capsule in layer below is done separately and each position of each capsule type has the one-parent assumption . However , since in this work only primary capsules and class capsules are used this does not concern the current experiment results in this paper . Response : Here is our understanding\u200b . Just like a convolutional layer in CNN , each capsule in Layer L+1 takes capsules in an area of Layer L ( sized kernel_size * kernel_size , i.e.receptive field ) as its inputs . If the stride is set smaller than the kernel_size , \u201c each convolutional instance of a capsule in layer L receives at most ( kernel_size * kernel_size ) feedback from each capsule type in layer L + 1. \u201d ( Hinton 2018 ) . With respect to capsule networks , a significant assumption ( Sabour 2017 and Hinton 2018 ) is that there is only one instance of the entity at a location . As for multi-instances issue , please refer to the \u201c from capsule-based recognition to capsule-based segmentation and beyond \u201d paragraph for our thoughts . > > The related work section should expand more on the SOTA segmentation techniques and the significance of this work including [ 2 ] . Response : We did an extensive literature review on the recent FCN based semantic segmentation and included them the \u201c Discussion and Related Work \u201d section on page 10 . On page 6 , we also include a paragraph and a figure to provide more details regarding Hippocampus segmentation , as well as the SOTA solutions . We also commented and compared our work with the ( sole ) capsule-based segmentation solution on page 10 . > > How is the traceback layer converted to image mask ? After one gets p ( c_k | i ) for all primary capsules , are primary capsule pose parameters multiplied by their p ( c_k |i ) and passed all to a deconv layer ? Response : After P ( c_k | i ) is available for each primary capsule , it will be multiplied by P ( i ) , the presence probability of the corresponding capsule , to produce P ( c_k ) . Eqn . ( 1 ) on page 4 ( revised paper ) shows how P ( c_k ) is generated through the traceback pipeline . > > Authors should specify in the manuscript the details of the upsampling layer ( s ) used in their architecture . It is only mentioned that deconv , dilated , bilinear interpolation are options . Which one is used in the end and how many is not clear . Response : we used the deconvolution scheme in Long et al . ( 2015 ) in this paper . In the revised paper , we rewrote the description to provide detailed and better presented description of the network modules . > > For the Hippocampus dataset , the ensemble U-Net approach used in [ 1 ] is close to your baseline and should be mentioned cited as the related work , SOTA on the dataset . Also since they use all 9 views have you considered accessing all the 9 views as well ? Response : We cite [ 1 ] and several more SOTA solutions in the revised manuscript . The main focus of this work is to explore the power of capsules in semantic segmentation , rather than an extremely accurate hippocampus segmentation solution . The 9 views in [ 1 ] needs to combined through an ensemble net , which is not the interest of this work ."}, "1": {"review_id": "H1xpe2C5Km-1", "review_text": "This paper proposes a traceback layer for capsule networks to do semantic segmentation. Comparing to previous works that use capsule networks for semantic segmentation, this paper makes explicit use of part-whole relationship in the capsule layers. Experiments are done on modified MNIST and Hippocampus dataset. Results demonstrate encouraging improvements over U-Net. The writing could be tremendously improved if some background of the capsule networks is included. I have a question about the traceback layer. It seems to me that the traceback layer re-uses the learned weights c_{ij} between the primary capsules and the class capsules as guidance when \u201cdistributing\u201d class probabilities to a spatial class probabilistic heatmap. One piece of information I feel missing is the affine transformation that happens between the primary capsule and the class capsule. The traceback layer doesn\u2019t seem to invert such a transformation. Should it do so? Since there have been works that use capsule networks for semantic segmentation, does it make sense to compare to them (e.g. LaLonde & Bagci, 2018) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > The writing could be tremendously improved if some background of the capsule networks is included . Response : we agree with this assessment . Based this comment , we added paragraphs in the Background section ( page 2 ) to provide more details on capsule and capsule nets . Comparisons are also made with CNNs . > > I have a question about the traceback layer . It seems to me that the traceback layer re-uses the learned weights c_ { ij } between the primary capsules and the class capsules as guidance when \u201c distributing \u201d class probabilities to a spatial class probabilistic heatmap . Response : the assessment is accurate . We take a good use of c_ij to compute P ( c_k ) , the class probability maps for capsule layers . We added figure 2 and rewrote the description to explain the architecture of our proposed Tr-CapsNet model , as well as the traceback procedure . > > One piece of information I feel missing is the affine transformation that happens between the primary capsule and the class capsule . The traceback layer doesn \u2019 t seem to invert such a transformation . Should it do so ? Response : In capsule net , the weight matrix W between capsule layers indeed represents a linear transformation that would map parts of an object into a cluster for the same whole . This is one of the major differences between capsule nets and CNNs . The purpose of our traceback pipeline is to estimate the ( inverse ) part-whole relations , based on which we can derive the class probability maps for capsule layers and later pixels . We observed such maps can be inferred through repeated applications of the product rule and the sum rule in probability theory , and only c_ij would be needed . In other words , W is not needed nor modified over the traceback pipeline . Additional introduction on capsule nets , as well as the comparisons with CNNs are added into the background section ( page 2 ) . > > Since there have been works that use capsule networks for semantic segmentation , does it make sense to compare to them ( e.g.LaLonde & Bagci , 2018 ) ? Response : we commented on LaLonde & Bagci \u2019 s work in Discussion and Related Work section . Additional thoughts on their work and our models have been added into this revised manuscript ( on page 10 ) . We also add two paragraphs to comment on capsule based solutions in general ."}, "2": {"review_id": "H1xpe2C5Km-2", "review_text": "Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset. The paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model? Additionally, think about a pseudo-code for improved understandability. Some minor concerns/ notes to the authors: 1. At page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters? 2. Page 6; baseline model: Why do you removed the pooling layers? 3. I\u2019m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I\u2019m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar. 4. Why is the magnitude of lambda1 so different between the two dataset that you used? 5. Could you add the inference times to your tables and discuss that in addition? 6. What kind of noise is added to MNIST? 7. What is the state-of-the-art performance on the Hippocampus dataset? 8. What would be the performance in your experiments with a MaskRCNN segmentation network? 9. I\u2019m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. 10. For both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix\u2026 My major concern is that both datasets are not dealing with real background noise. I\u2019m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete. To make your results transparent you could think about to publish the code somewhere. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> > The paper is well-written and well-explained . Nevertheless , I think it would be useful to have some illustrations about the network architecture . Some stuff which is explained in text could be easily visualized in a flow chart . For example , the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart . With the text only , it is hard to follow . Response : thanks for the assessment , and we fully agree that more illustrations should be added to make our approach more understable . In this revised manuscript , we added figure 2 ( flow chart ) and rewrote the first paragraph of Architecture section to describe the modules of our Tr-CapsNet . > > Please think about some plots in the final version or in the appendix . One question which is aligned to that : How many convolutional filters are used in the baseline model ? Response : we added an Appendix in this version to provide the detailed network configurations . The number of layers and number of filters in each layer are all listed there . > > 1.At page 5 : You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune . But in the results you are not explaining how the parameters were tuned . So my question is : How do you tuned the parameters ? In which range do you varied the parameters ? Response : we agree with the reviewer , and we added a paragraph on page 7 to provide more \u201c Implementation details \u201d . As for the setting of lambda1 and lambda 2 , \u201c the ratio was selected from { 1 ~ 5 } for the MNIST experiment . In the Hippocampus experiment , we observed that the recognition task was far more difficult , largely due to the greater complexity of the dataset . Accordingly , we selected the ratio from a larger range , { 1 \u223c 20 } with an interval of 1. \u201d > > 2 . Page 6 ; baseline model : Why do you removed the pooling layers ? Response : We didn \u2019 t use max-pooling operation in the modified U-Net , based on two observations/considerations : 1 ) based on [ 1 ] max-pooling can simply be replaced by a convolution layer with increased stride without loss in accuracy ; 2 ) our Tr-CapsNet doesn \u2019 t have pooling operations , and instead relies on stride to achieve multi-scale processing . We figure using a modified U-Net with wide stride instead of pooling would facilitate the comparison , analysis and future development the models . [ 1 ] Springenberg , Jost Tobias , Alexey Dosovitskiy , Thomas Brox , and Martin Riedmiller . `` Striving for simplicity : The all convolutional net . '' arXiv preprint arXiv:1412.6806 ( 2014 ) . > > 3.I \u2019 m curious about the number of parameters in each model . To have a valid discussion about your model is better than the U-Net-6 architecture , I would take into account the number of parameters . In case that your model is noticeably greater , it could be that your increased performance is just due to more parameters . As long as your discussion is without the number of parameters I \u2019 m not convinced that your model is better . A comparison between models should be always fair if two models are architectural similar . Response : again we fully agree with the reviewer on this regard . In the revised version , we added the number of parameters for each model , which can be seen in Tables 1 and 2 . Two observations : 1 ) both U-Net and our Tr-CapsNet have variable numbers of parameters , depending on the setting , and it \u2019 s not the case that Tr-CapsNets have a lot of more parameters than U-Nets ; 2 ) more parameters do not automatically translate into better performance . This can be seen in table 1 , where Tr-CapsNet-9 ( 1:1 ) has the best performance , while it doesn \u2019 t have the most parameters . In table 2 , U-Net of 1.14M parameters actually outperforms the version of 2.19M parameters . > > 4.Why is the magnitude of lambda1 so different between the two dataset that you used ? Response : As the Hippocampus dataset is obviously more complicated than MNIST , in terms of intensity separability , a large weight on recognition term is needed , and it should help to \u201c force the network to build more accurate part-whole relationships , which lays a solid foundation to produce more accurate overall segmentations \u201d . We added the above explanation to the revised paper . > > 5.Could you add the inference times to your tables and discuss that in addition ? Response : In both capsule nets and our Tr-CapsNets , an iterative routing-by-agreement mechanism is used . In this mechanism each capsule chooses its parent capsule in the higher layer through an iterative routing procedure . This is certainly a downside comparing with CNNs . The average inference time for one Hippocampal slice , the U-Net ( feature map 4 x 20 ) and Tr-CapsNet ( 4 x 20 , 1 ) take roughly 0.3 ms and 0.6 ms , respectively ."}}