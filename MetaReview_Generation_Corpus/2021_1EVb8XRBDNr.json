{"year": "2021", "forum": "1EVb8XRBDNr", "title": "RMIX: Risk-Sensitive Multi-Agent Reinforcement Learning", "decision": "Reject", "meta_review": "This paper proposes a method of risk-sensitive multi-agent reinforcement learning in cooperative settings.  The proposed method introduces several new ideas, but they are not theoretically well founded, which has caused many confusions among the reviewers.  Although some of the confusions are resolved through discussion, there remain major concerns about the validity of the method.  ", "reviews": [{"review_id": "1EVb8XRBDNr-0", "review_text": "The authors propose RMIX to deal with the randomness of rewards and the uncertainty in environments . RMIX learns the individual value distributions of each agent and uses a predictor to calculate the dynamic risk level . Given the individual value distribution and the risk level , a CVaR operator outputs the C value for execution . For training , the $ C $ values are mixed as $ C^ { tot } $ and updated by TD error end-to-end . RMIX outperforms a series of value decomposition baselines on many challenging StarCraft II tasks . The paper is very clear and well-structured . Expanding value decomposition methods to the risk-sensitive field is a novel idea , and it shows competitive performance in empirical studies . However , my main concern is the definition of the risk level $ \\alpha $ . More in-depth analysis is expected to interpret why the discrepancy between the embedding of current individual return distributions and the embedding of historical return distributions could reflect the risk level . Since the embedding parameters $ femb $ and $ \\phi $ are trained end-to-end by TD-error , the real meaning of $ \\alpha $ is unknowable . Eq.3 is a little confusing , and I get the left side of Eq.3 should be $ p_ { \\alpha_i^k } $ , not $ \\alpha_i^k $ . It is hard to understand how to get the final $ \\alpha_i $ from the K-range probability . Figure 12 helps a lot in this understanding , and I expect it to be more detailed and put on the main pages . RMIX is built on QMIX . However , the individual value function in QMIX does not estimate a real expected return , and the value has no meaning . Is the theoretical analysis of the risk still valid in QMIX\uff1f RMIX is proposed for the randomness of rewards and the uncertainty in environments . However , I think they are not usually observed in the environment StarCraft II , since the policies of the enemy are fixed . Increasing the randomness and uncertainty could verify the advantages of RMIX . -Update after author response- I thank the authors for the detailed response . Most of my concerns have been addressed , and I decide to keep my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , we thank you for providing very valuable suggestions . We will explain your concerns and answer your questions point by point . * * Q1 : the definition of the risk level $ \\alpha $ . More in-depth analysis is expected to interpret why the discrepancy between the embedding of current individual return distributions and the embedding of historical return distributions could reflect the risk level . * * * * A1 * * : ( 1 ) $ \\alpha $ ( a hyper-parameter ) is a risk level ( $ \\alpha \\in ( 0 , 1 ] $ ) in CVaR and it can be static or dynamic . ( 2 ) The differences between the current individual return distributions and the local historical distributions are that the local historical distributions do not have the current individual return distributions as embedding and features are learned by GRU with trajectories as inputs . ( 3 ) Intuitively , the current local return distribution maybe different from the local distribution of the last step . If the local return distribution changes a little , the risk level would not change a lot . Therefore , we can model this change and measure the discrepancy . ( 4 ) As shown in Figure 3 , with two GRUs modelling the current local return distribution and the historical local return distribution ( here , we use the local return distribution of the last step as the historical local return distribution as the GRU has memorised the features of historical return distribution ) . Therefore , by utilizing the features , we can use the embedding of current individual return distributions and the embedding of historical return distributions to predict the risk level . * * Q2 : Since the embedding parameters $ f_ { emb } $ and $ \\phi $ are trained end-to-end by TD-error , the real meaning of $ \\alpha $ is unknowable . * * * * A2 * * : The $ \\alpha $ is a parameter in CVaR . It presents the risk level in CVaR . The learned $ \\alpha $ is used as risk level for CVaR calculation in decentralized training . As we use CVaR on return distributions , it corresponds to risk-neutrality ( expectation ) and indicates the improving degree of risk-aversion when the risk level $ \\alpha_i=1 $ and $ \\alpha_i \\rightarrow 0 $ , respectively . * * Q3 : Eq.3 is a bit confusing , and I get the left side of Eq.3 should be $ p_ { \\alpha_i^k } $ , not $ \\alpha_i^k $ . It is hard to understand how to get the final $ \\alpha_i $ from the K-range probability . Figure 12 helps a lot in this understanding , and I expect it to be more detailed and put on the main pages . * * * * A3 * * : We agree it is confusing . The left side should be probability of $ p_ { \\alpha_i^k } $ . As we can use Eq.3 to calculate the probability of each risk level . can get a probability vector containing K probability values , and we can get the $ k \\in [ 1 , \u2026 , K ] $ with the maximal probability via argmax and normalize $ k $ into $ ( 0 , 1 ] $ , thus $ \\alpha_i $ is calculated .. We can use this value to mask out parts of local return distribution out of the risk level to get the final CVaR value.We updated this part in the main text . * * Q4 : RMIX is built on QMIX . However , the individual value function in QMIX does not estimate a real expected return , and the value has no meaning . Is the theoretical analysis of the risk still valid in QMIX ? * * * * A4 * * : The theoretical analysis of the risk is still valid . ( 1 ) In Dec-POMDP-based problems , the reward at each time step is a global reward which is shared by each agent and each agent has no access to its real contribution to the global reward . VDN , QMIX and many other MARL algorithms [ 1,2,3,4 ] were proposed to address this credit assignment issue in Dec-POMDP-based problems . ( 2 ) Concretely , the global values are factorized ( sum of individual values in VDN and monotonicity in QMIX ) to each individual value and agents can use these individual values to get execution policies for decentralized execution . During execution , at each time step , each agent takes its local observation and previous time step actions as inputs to the agent network to get the local estimated values which are further used as policies . ( 3 ) Therefore , the global value factorization estimates the local value . In RMIX , we also use the monotonic network for centralized training and the risk operator works over the local estimated return distribution . IGM and monotonic value network can be applied to many Dec-POMDP-based MARL algorithm and the theoretical analysis of the risk still valid ."}, {"review_id": "1EVb8XRBDNr-1", "review_text": "Overall The paper considers the cooperative multiagent MARL setting where agents have partial observability and share a team reward . Instead of optimizing the expected cumulative reward , the paper considers optimizing the CVaR measure . The proposed method relies on CTDE and features a dynamic risk-level predictor . Although CVaR optimization is an important problem for MARL and the experimental results seem to be convincing , the formulation of the problem and the description of the proposed method are not written with enough clarity . Specifically , I have the following comments/questions . Comments/Questions 1 . For CVaR optimization , is the risk level ( i.e.alpha ) given as part of the problem itself ? If it is given , what \u2019 s the intuition behind the \u201c dynamic risk level predictor \u201d that handles \u201c the temporal nature of stochastic outcomes \u201d . What is \u201c the temporal nature \u201d ? 2.It \u2019 s known that , for a discrete random variable , its pdf is composed by Dirac functions weighted by the pmf . Is Definition 1 repeating this ? 3.I have difficulty understanding Equations ( 1 ) and ( 2 ) . In ( 1 ) , what \u2019 s the meaning of delta ( tau_i , u_i ) ? Isn \u2019 t that the Dirac function takes a number as its argument ? Why can the Dirac function take a trajectory as its argument ? For ( 2 ) , why does the estimate not depend on P_j that appears in ( 1 ) ? 4.In Theorem 1 , what is the definition of IGM , and the definition of C^tot ? If the reward is shared , why would the local CVaR be different from the global CVaR ? Although the paper considers an important optimization objective for MARL , I don \u2019 t think the presentation is ready for publication yet .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , we thank you for your very helpful comments . We appreciate you recognise the significance of the risk-sensitive problem in MARL . These are some parts we did not clarify in our paper . To enhance the clarity , we have improved the paper . We will explain your concerns and answer your questions point by point . * * Q1 : is the risk level ( i.e.alpha ) given as part of the problem itself ? If it is given , what \u2019 s the intuition behind the \u201c dynamic risk level predictor \u201d that handles \u201c the temporal nature of stochastic outcomes \u201d . What is \u201c the temporal nature \u201d ? * * * * A1 * * : ( 1 ) The risk level is not a problem . It is a parameter in CVaR and it can be static ( predefined ) or dynamic . ( 2 ) The motivation behind the \u201c dynamic risk level predictor \u201d is that there are some consecutive states where the risk level should be fixed in high range while for other consecutive states , the risk level should stay in other low range . Assume a team of marines pass a long corridor where the number of enemies is larger than that of marines . Under such circumstances , the risk level should be low ( As we use CVaR on return distributions , it corresponds to risk-neutrality ( expectation , $ \\alpha_i=1 $ ) and indicates the improving degree of risk-aversion ( $ \\alpha_i \\rightarrow 0 $ ) ) as marines should focus on the worst cases ( return is low ) , that is the whole army is ( completely ) wiped out . If the marines successfully passed the corridor and the environment is much safer then marines can explore and the risk level is low . This is the time-consistency issue [ 1,2 ] and is an issue in risk-sensitive RL . We also provide an analysis in Figure 17 , Appendix G.2 in the revised paper . ( 3 ) Temporal nature means the characteristic of stochastic rewards related to time . * * Q2 : It \u2019 s known that , for a discrete random variable , its pdf is composed by Dirac functions weighted by the pmf . Is Definition 1 repeating this ? * * * * A2 * * : Yes . Definition 1 defines the generalized return PDF of each agent given the trajectories of each agent and Dirac functions are parameterized by neural networks . * * Q3 : For ( 2 ) , why does the estimate not depend on P_j that appears in ( 1 ) ? * * * * A3 * * : We thank the reviewer for finding it . Yes , the calculation of CVaR depends on the $ P_j $ . We have update the paper on this part to improve clarity . * * Q4 : In ( 1 ) , what \u2019 s the meaning of delta ( tau_i , u_i ) ? Isn \u2019 t that the Dirac function takes a number as its argument ? Why can the Dirac function take a trajectory as its argument ? For ( 2 ) , why does the estimate not depend on P_j that appears in ( 1 ) ? * * * * A4 * * : Yes , Dirac function takes a number as its argument . The reason is that we want to generalize it with agent \u2019 s experiences as the high dimension input as widely used in MARL methods [ 3,4 ] . While some other papers may use observations [ 5 ] or the hidden states [ 6 ] . Parameterized by GRU , we use the hidden states ( numbers ) as inputs to output the Dirac outputs . * * Q5 : what is the definition of IGM , and the definition of $ C^ { tot } $ ? * * * * A5 * * : QTRAN [ 3 ] introduced IGM . IGM says the optimal joint actions across agents are equivalent to the collection of individual optimal actions of each agent . This principle is used for global value factorization . $ C^ { tot } $ is the output from the monotonic network by taking the individual CVaR values as inputs . * * Q6 : If the reward is shared , why would the local CVaR be different from the global CVaR ? * * * * A6 * * : The shared reward is a global reward and each agent has no access to its real contribution ( local reward ) . Therefore , credit assignment is needed to estimate the contribution of each agent and training the policy of each agent . VDN [ 6 ] and QMIX [ 4 ] were proposed to address this problem . In RMIX , each agent also has no access to its real contribution to the global reward and instead gets the global reward . Possessing various kinds of abilities in each agent , the local CVaR values , which compose the global CVaR values via a monotonic network , are different from the global CVaR values . * * References * * : [ 1 ] Ruszczy\u0144ski , Andrzej . `` Risk-averse dynamic programming for Markov decision processes . '' Mathematical programming 125.2 ( 2010 ) : 235-261 . [ 2 ] Tamar , Aviv , et al . `` Policy gradient for coherent risk measures . '' Advances in Neural Information Processing Systems . 2015 . [ 3 ] Son , Kyunghwan , et al . `` QTRAN : Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning . '' International Conference on Machine Learning . 2019 . [ 4 ] Rashid , Tabish , et al . `` QMIX : Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning . '' International Conference on Machine Learning . 2018 . [ 5 ] Yang , Yaodong , et al . `` Q-value Path Decomposition for Deep Multiagent Reinforcement Learning . '' International Conference on Machine Learning . 2020 . [ 6 ] Sunehag , Peter , et al . `` Value-decomposition networks for cooperative multi-agent learning . '' arXiv preprint arXiv:1706.05296 ( 2017 ) ."}, {"review_id": "1EVb8XRBDNr-2", "review_text": "Strengths : 1 ) The paper is well-written , and versed in the pulse of related works on the topic . This makes it very easy to assess the points of differentiation of the formulation/focus from prior art . Especially the problem class of MARL with risk measures is salient . 2 ) The authors clearly demonstrate the practical merits of the proposed techniques in ambitious experiments . This is a major upshot of the work : one may clearly see across challenging domains the performance gains achieved by RMIX . 3 ) Solving Dec-POMDPs is in general intractable , and one must often resort to particle filtering/belief representations of the state . Combining these technical challenges with risk sensitive utilities is nontrivial , and the authors have made a bold effort to obtain a tractable algorithm despite these issues . Importantly , their work has some conceptual justification . Weaknesses : 1 ) While the conceptual setup and theoretical contributions are clear , the actual training mechanism is given very little explanation . There should at least be an iterative update scheme or pseudo-code presenting the key algorithm of this work . This would serve to make it easier to distinguish what are the unique attributes to the algorithm put forth in this work outside of a contextual discussion and at a more granular algorithmic level . By reading the paper it is very difficult to understand what information agents must exchange during training . 2 ) What is the purpose of the generalized return PDF ? How does the information required for its estimation get processed algorithmically , and can each agent estimate its component of it with local information only ? This is not easy to discern . 3 ) Theorem 1 must somehow assume that each agent 's marginal utility is concave and that the joint utility is jointly concave in agents ' local policies , but a discussion of this seems missing . Without concavity , then one can only ensure that agents ' policies in the decentralized setting converge to stationarity , and that these stationary points belong to the set of extrema of the global utility . It may be the case that the fact that the risk-sensitive Bellman operator being a contraction is enough to mitigate these subtleties and ensure convergence to a global extrema , but this is not discussed . 4 ) Just defining the TD loss in eqn . ( 4 ) is not enough because the presence of a risk measure means that a vanilla TD step in terms of approximating an expected value does not hold . How does the `` `` ` blocking to avoid changing the weights of the agents \u2019 network from the dynamic risk level predictor '' change the TD population objective ? Under normal circumstances , this would be the Bellman optimality operator -- how does C^ { tot } mitigate this issue , and what relationship does this hold to the notions of risk-sensitive Bellman equations in Andrzej Ruszczy\u00b4nski . Risk-averse dynamic programming for markov decision processes . Mathematical Programming , 125 ( 2 ) :235\u2013261 , 2010 . See also : Kose , U. , & Ruszczynski , A . ( 2020 ) .Risk-Averse Learning by Temporal Difference Methods . arXiv preprint arXiv:2003.00780 . 5 ) In general , a discussion of the technical innovations required to establish the theorems is absent . Are these theorems inherited from the algorithms appearing in earlier work ? What is new ? Minor Comments : 1 ) References missing regarding risk-sensitive RL : Zhang , Junyu , Amrit Singh Bedi , Mengdi Wang , and Alec Koppel . `` Cautious Reinforcement Learning via Distributional Risk in the Dual Domain . '' arXiv preprint arXiv:2002.12475 ( 2020 ) . 2 ) The link between equation ( 2 ) and estimating a conditional expectation should be made more explicit . 3 ) The meaning and interpretation of IGM is ambiguous . More effort needs to be expended to properly explain it . The idea seems to be that individual agents ' optimal policy is equivalent to an oracle/meta-agent that aggregates information over the network . This notion is then key to interpreting the importance of Theorem 1 .", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , we appreciate your high-quality reviews and constructive feedbacks . We have cited the new risk-sensitive papers in our paper . Due to the 8-page main text limit , there are some parts are not discussed in detail and we have clarified them in the paper . Our goal is to design a risk-sensitive MARL method to fill the gap as few works have been done and the risk-sensitive problem is also a significant problem in real world applications . We will address your concerns and answer your questions point by point . * * Q1 : the actual training mechanism is given very little explanation * * * * A1 * * : The training is centralized training . The target of RMIX is to update the weights in agent network , risk operator and monotonic network . ( i ) First , a mini-batch of experience ( state , observations , actions , rewards , next states ) collected by each agent at each step is sampled from the replay buffer . ( ii ) Then we calculate the individual $ C^ { t } _ { i } $ of each agent by the equation in section 3.1 . ( iii ) Third , we input the $ C^ { t } _ { i } $ into the monotonic network and get the $ C^ { tot } $ . By following this step , we can get the target value and the predicted value in equation 7 and thus we get the TD loss . ( iv ) Finally , we can use gradient descent methods to minimize the TD loss . We also have added pseudo code Algorithm 1 in Appendix in the revised paper . * * Q2 : What is the purpose of the generalized return PDF ? * * * * A2 * * : We define the generalized return PDF for the local return distribution modelled by many Dirac functions which are further used to estimate the CVaR values . * * Q3 : Theorem 1 must somehow assume that each agent 's marginal utility is concave and that the joint utility is jointly concave in agents ' local policies , but a discussion of this seems missing . * * * * A3 * * : We will put the discussion in the final version . ( 1 ) Theorem 1 is defined for decentralized execution in Centralized Training and Decentralized Execution ( CTDE ) . In decentralized execution , each agent takes actions independently . As a consequence , there should be some rules designed for decentralized execution . ( 2 ) CVaR is a widely used risk measure and it enjoys many properties such as convexity ( it can be converted into the concave form ) . Under the monotonic global CVaR value factorization , Theorem 1 holds and it can be used for decentralized execution . ( 3 ) Your comment on the assumption of the concavity of each agent \u2019 s marginal utility is insightful . It seems that there are few works discussing whether such concavity assumption exists in QMIX , QTRAN and other methods . If such assumption truly exists or there are some methods that can help to maintain this assumption , the representational capacity will be increased . * * Q4 : Just defining the TD loss in eqn . ( 4 ) is not enough because the presence of a risk measure means that a vanilla TD step in terms of approximating an expected value does not hold . * * * * A4 * * : The TD loss defined in Eqn . ( 4 ) is the centralized training loss function for updating the weights of the network of agents , risk operator and monotonic network . With risk measure , the CVaR values can be seen as the expectation value of local return values of the chosen region of the local return distribution given the current risk level . With such expectation values , the optimization of TD loss is minimized with expectation value from the chosen pessimistic values in the local return distribution . * * Q5 : How does the `` blocking to avoid changing the weights of the agents \u2019 network from the dynamic risk level predictor '' change the TD population objective ? * * * * A5 * * : In TD learning , both weights of risk level predictor and agent network should be updated . The motivation of this blocking gradient design ( as shown in Figure 3 ) is to avoid gradients from risk level predictor to change the weights in the agent network because the agent network does not predict risk level and it estimates the local return distribution . This blocking gradient method is widely used in deep learning and deep RL , for example blocking gradients from the target network in DQN and its variants ."}, {"review_id": "1EVb8XRBDNr-3", "review_text": "This paper proposes a new value-based method using risk measures in cooperative multi-agent reinforcement learning . The authors propose a new network structure that calculates global CVaR through individual distribution and learns risk-sensitized multi-agent policies . The authors also propose a new dynamic risk level prediction method that can dynamically adjust the risk level according to the agent \u2019 s observation and action . Applying risk-sensitive reinforcement learning in multi-agent reinforcement learning is interesting , but several points can be improved . This paper has a fundamental difference compared to the distributional reinforcement learning recently studied in single-agent reinforcement learning like IQN . In single-agent reinforcement learning , the distribution of Q-function is first learned through the distributional Bellman operator , and the learned distribution can be utilized in additional applications such as risk-sensitive reinforcement learning . Even if the mean value of the distribution is used without considering the risk , it shows higher performance than the existing reinforcement learning algorithms without distribution . The distributional Bellman operator has a richer training signal than the Bellman operator for the existing scalar Q-function , which enables fast representation learning . Moreover , by changing the sampling distribution through the learned distribution , risk-sensitive reinforcement learning can be easily applied to multiple risk measures . However , in this paper , the authors do not use distribution as a direct learning objective but only aim to maximize CVaR . Unlike the distributional bellman operator , this learning method can not be expected to improve training speed , and it is difficult to understand the meaning of the learned return distribution learned with the scalar loss function . Also , this paper proposes a dynamic risk level prediction , but this part is confused . The authors argue that this method solves time-consistency issues and unstable learning . However , there is an insufficient explanation as to why this problem occurred and how to solve it . There is also a lack of explanation on why risk level is divided into K steps and why alpha is defined as in equation ( 3 ) . Finally , detailed analyses are needed on how dynamic risk level prediction affects performance . There is a paper , which the authors must cite and compare to . This is not the same as RMIX , but close enough that it has to be compared . Hu et al . ( 2020 ) .QR-MIX : Distributional Value Function Factorisation for Cooperative Multi-Agent Reinforcement Learning . URL : https : //arxiv.org/pdf/2009.04197.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear reviewer , we thank you for your insightful comments on our paper . In the revised version , we cited QR-MIX paper and compared RMIX with QR-MIX on 10 scenarios . The results can be found in Section 4.2 and Appendix G.3 . We also present results analysis of RMIX on corridor in Appendix G.2 . We now explain your concerns and answer your questions point by point . * * Q1 : the authors do not use distribution as a direct learning objective but only aim to maximize CVaR . * * * * A1 * * : We use CVaR values as policies for decentralized execution in RMIX . ( 1 ) CVaR is a widely used risk measure and enjoys many properties such as convexity . The risk-sensitive RL [ 1,2,3,4 ] is an important research problem . In RMIX , each agent aims to estimate a local return distribution at each time step and uses the local return distribution to estimate the CVaR analytically for decentralized execution . RMIX estimates the global CVaR value via the monotonic network . ( 2 ) Risk-sensitive RL and distributional RL are orthogonal research topics . IQN ( a distributional RL method ) [ 5 ] made a connection between distributional RL and risk-sensitive RL by utilizing a risk measure over the return distribution . Distributional RL updates the return distribution while risk-sensitive RL optimizes the risk values ( policies ) . * * Q2 : Unlike the distributional bellman operator , this learning method can not be expected to improve training speed * * * * A2 * * : We answer your question in the following . ( 1 ) RMIX aims to learn risk-sensitive policies for enhanced cooperation and improving the training speed is not our main focus . ( 2 ) RMIX shows slower training speed compared with baselines due to the network architecture . We present training time cost of each algorithm on some scenarios as shown in the table below . Despite this fact , RMIX gains superior performance on corridor ( very hard ) , 6h_vs_8z ( very hard ) , 27m_vs_30m ( very hard ) and many other scenarios . We train our model on NVIDIA Tesla V100 GPU 16G and Intel ( R ) Xeon ( R ) CPU E5-2683 v3 @ 2.00GHz . | Scenarios | Training steps | RMIX | QMIX | VDN | IQL | QTRAN | MAVEN | | -- | -- | | -- | -- | -- | -- | | | corridor | 3 million | 1 day 13 hours | 24 hours | 21 hours | 22 hours | 21 hours | 23 hours | | 3s5z\\_vs\\_3s6z | 3 million | 23 hours | 20 hours | 18 hours | 19 hours | 19 hours | 17 hours | | MMM2 | 3 million | 22 hours | 20 hours | 18 hours | 19 hours | 19 hours | 16 hours | | 6h\\_vs\\_8z | 3 million | 1 day 18 hours | 20 hours | 19 hours | 19 hours | 20 hours | 15 hours | | 5m\\_vs\\_6m | 2 million | 19 hours | 18 hours | 9 hours | 12 hours | 13 hours | 11 hours | | 8m\\_vs\\_9m | 1 million | 8 hours | 8 hours | 8 hours | 8 hours | 8 hours | 6 hours | | 10m\\_vs\\_11m | 1 million | 9.5 hours | 8 hours | 6 hours | 7 hours | 8 hours | 7.5 hours | | 27m\\_vs\\_30m | 1.5 million | 23.5 hours | 18 hours | 17hours | 16 hours | 12 hours | 20 hours | However , the training speed of Deep RL and MARL algorithms varies on different computational platforms , hyper-parameters and training schemes . For training StarCraft II scenarios , when using high performance CPUs such as Intel ( R ) Core ( TM ) i9-9820X CPU @ 3.30GHz , it shows faster training speed especially on scenarios where the number of agent is large , for example 27m_vs_30m . ( 3 ) In fact , RMIX shows good sample efficiency compared with baseline methods ( QMIX , MAVEN , Qatten , etc . ) when training super hard scenarios such as corridor , 27m_vs_30 , 3s5z_vs_5s6z ( Figure 6 and 7 in the paper ) . Improving sample efficiency is vital for RL and MARL . ( 4 ) To our best knowledge , the paper of C51 [ 6 ] shows that \u201c for N = 51 , our TensorFlow implementation trains at roughly 75 % of DQN \u2019 s speed \u201d ( in footnote 2 , page 6 in the C51 paper ) . So , with Distributional Bellman Operator , distributional RL methods can be slower than DQN . The main reason is updating the return distribution takes more computational resources as the loss function causes the distribution update to requires complex distribution projection as shown in C51 \u2019 s paper . Besides that modelling distribution adds more parameters in the neural network , which can slow down the training speed ."}], "0": {"review_id": "1EVb8XRBDNr-0", "review_text": "The authors propose RMIX to deal with the randomness of rewards and the uncertainty in environments . RMIX learns the individual value distributions of each agent and uses a predictor to calculate the dynamic risk level . Given the individual value distribution and the risk level , a CVaR operator outputs the C value for execution . For training , the $ C $ values are mixed as $ C^ { tot } $ and updated by TD error end-to-end . RMIX outperforms a series of value decomposition baselines on many challenging StarCraft II tasks . The paper is very clear and well-structured . Expanding value decomposition methods to the risk-sensitive field is a novel idea , and it shows competitive performance in empirical studies . However , my main concern is the definition of the risk level $ \\alpha $ . More in-depth analysis is expected to interpret why the discrepancy between the embedding of current individual return distributions and the embedding of historical return distributions could reflect the risk level . Since the embedding parameters $ femb $ and $ \\phi $ are trained end-to-end by TD-error , the real meaning of $ \\alpha $ is unknowable . Eq.3 is a little confusing , and I get the left side of Eq.3 should be $ p_ { \\alpha_i^k } $ , not $ \\alpha_i^k $ . It is hard to understand how to get the final $ \\alpha_i $ from the K-range probability . Figure 12 helps a lot in this understanding , and I expect it to be more detailed and put on the main pages . RMIX is built on QMIX . However , the individual value function in QMIX does not estimate a real expected return , and the value has no meaning . Is the theoretical analysis of the risk still valid in QMIX\uff1f RMIX is proposed for the randomness of rewards and the uncertainty in environments . However , I think they are not usually observed in the environment StarCraft II , since the policies of the enemy are fixed . Increasing the randomness and uncertainty could verify the advantages of RMIX . -Update after author response- I thank the authors for the detailed response . Most of my concerns have been addressed , and I decide to keep my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , we thank you for providing very valuable suggestions . We will explain your concerns and answer your questions point by point . * * Q1 : the definition of the risk level $ \\alpha $ . More in-depth analysis is expected to interpret why the discrepancy between the embedding of current individual return distributions and the embedding of historical return distributions could reflect the risk level . * * * * A1 * * : ( 1 ) $ \\alpha $ ( a hyper-parameter ) is a risk level ( $ \\alpha \\in ( 0 , 1 ] $ ) in CVaR and it can be static or dynamic . ( 2 ) The differences between the current individual return distributions and the local historical distributions are that the local historical distributions do not have the current individual return distributions as embedding and features are learned by GRU with trajectories as inputs . ( 3 ) Intuitively , the current local return distribution maybe different from the local distribution of the last step . If the local return distribution changes a little , the risk level would not change a lot . Therefore , we can model this change and measure the discrepancy . ( 4 ) As shown in Figure 3 , with two GRUs modelling the current local return distribution and the historical local return distribution ( here , we use the local return distribution of the last step as the historical local return distribution as the GRU has memorised the features of historical return distribution ) . Therefore , by utilizing the features , we can use the embedding of current individual return distributions and the embedding of historical return distributions to predict the risk level . * * Q2 : Since the embedding parameters $ f_ { emb } $ and $ \\phi $ are trained end-to-end by TD-error , the real meaning of $ \\alpha $ is unknowable . * * * * A2 * * : The $ \\alpha $ is a parameter in CVaR . It presents the risk level in CVaR . The learned $ \\alpha $ is used as risk level for CVaR calculation in decentralized training . As we use CVaR on return distributions , it corresponds to risk-neutrality ( expectation ) and indicates the improving degree of risk-aversion when the risk level $ \\alpha_i=1 $ and $ \\alpha_i \\rightarrow 0 $ , respectively . * * Q3 : Eq.3 is a bit confusing , and I get the left side of Eq.3 should be $ p_ { \\alpha_i^k } $ , not $ \\alpha_i^k $ . It is hard to understand how to get the final $ \\alpha_i $ from the K-range probability . Figure 12 helps a lot in this understanding , and I expect it to be more detailed and put on the main pages . * * * * A3 * * : We agree it is confusing . The left side should be probability of $ p_ { \\alpha_i^k } $ . As we can use Eq.3 to calculate the probability of each risk level . can get a probability vector containing K probability values , and we can get the $ k \\in [ 1 , \u2026 , K ] $ with the maximal probability via argmax and normalize $ k $ into $ ( 0 , 1 ] $ , thus $ \\alpha_i $ is calculated .. We can use this value to mask out parts of local return distribution out of the risk level to get the final CVaR value.We updated this part in the main text . * * Q4 : RMIX is built on QMIX . However , the individual value function in QMIX does not estimate a real expected return , and the value has no meaning . Is the theoretical analysis of the risk still valid in QMIX ? * * * * A4 * * : The theoretical analysis of the risk is still valid . ( 1 ) In Dec-POMDP-based problems , the reward at each time step is a global reward which is shared by each agent and each agent has no access to its real contribution to the global reward . VDN , QMIX and many other MARL algorithms [ 1,2,3,4 ] were proposed to address this credit assignment issue in Dec-POMDP-based problems . ( 2 ) Concretely , the global values are factorized ( sum of individual values in VDN and monotonicity in QMIX ) to each individual value and agents can use these individual values to get execution policies for decentralized execution . During execution , at each time step , each agent takes its local observation and previous time step actions as inputs to the agent network to get the local estimated values which are further used as policies . ( 3 ) Therefore , the global value factorization estimates the local value . In RMIX , we also use the monotonic network for centralized training and the risk operator works over the local estimated return distribution . IGM and monotonic value network can be applied to many Dec-POMDP-based MARL algorithm and the theoretical analysis of the risk still valid ."}, "1": {"review_id": "1EVb8XRBDNr-1", "review_text": "Overall The paper considers the cooperative multiagent MARL setting where agents have partial observability and share a team reward . Instead of optimizing the expected cumulative reward , the paper considers optimizing the CVaR measure . The proposed method relies on CTDE and features a dynamic risk-level predictor . Although CVaR optimization is an important problem for MARL and the experimental results seem to be convincing , the formulation of the problem and the description of the proposed method are not written with enough clarity . Specifically , I have the following comments/questions . Comments/Questions 1 . For CVaR optimization , is the risk level ( i.e.alpha ) given as part of the problem itself ? If it is given , what \u2019 s the intuition behind the \u201c dynamic risk level predictor \u201d that handles \u201c the temporal nature of stochastic outcomes \u201d . What is \u201c the temporal nature \u201d ? 2.It \u2019 s known that , for a discrete random variable , its pdf is composed by Dirac functions weighted by the pmf . Is Definition 1 repeating this ? 3.I have difficulty understanding Equations ( 1 ) and ( 2 ) . In ( 1 ) , what \u2019 s the meaning of delta ( tau_i , u_i ) ? Isn \u2019 t that the Dirac function takes a number as its argument ? Why can the Dirac function take a trajectory as its argument ? For ( 2 ) , why does the estimate not depend on P_j that appears in ( 1 ) ? 4.In Theorem 1 , what is the definition of IGM , and the definition of C^tot ? If the reward is shared , why would the local CVaR be different from the global CVaR ? Although the paper considers an important optimization objective for MARL , I don \u2019 t think the presentation is ready for publication yet .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , we thank you for your very helpful comments . We appreciate you recognise the significance of the risk-sensitive problem in MARL . These are some parts we did not clarify in our paper . To enhance the clarity , we have improved the paper . We will explain your concerns and answer your questions point by point . * * Q1 : is the risk level ( i.e.alpha ) given as part of the problem itself ? If it is given , what \u2019 s the intuition behind the \u201c dynamic risk level predictor \u201d that handles \u201c the temporal nature of stochastic outcomes \u201d . What is \u201c the temporal nature \u201d ? * * * * A1 * * : ( 1 ) The risk level is not a problem . It is a parameter in CVaR and it can be static ( predefined ) or dynamic . ( 2 ) The motivation behind the \u201c dynamic risk level predictor \u201d is that there are some consecutive states where the risk level should be fixed in high range while for other consecutive states , the risk level should stay in other low range . Assume a team of marines pass a long corridor where the number of enemies is larger than that of marines . Under such circumstances , the risk level should be low ( As we use CVaR on return distributions , it corresponds to risk-neutrality ( expectation , $ \\alpha_i=1 $ ) and indicates the improving degree of risk-aversion ( $ \\alpha_i \\rightarrow 0 $ ) ) as marines should focus on the worst cases ( return is low ) , that is the whole army is ( completely ) wiped out . If the marines successfully passed the corridor and the environment is much safer then marines can explore and the risk level is low . This is the time-consistency issue [ 1,2 ] and is an issue in risk-sensitive RL . We also provide an analysis in Figure 17 , Appendix G.2 in the revised paper . ( 3 ) Temporal nature means the characteristic of stochastic rewards related to time . * * Q2 : It \u2019 s known that , for a discrete random variable , its pdf is composed by Dirac functions weighted by the pmf . Is Definition 1 repeating this ? * * * * A2 * * : Yes . Definition 1 defines the generalized return PDF of each agent given the trajectories of each agent and Dirac functions are parameterized by neural networks . * * Q3 : For ( 2 ) , why does the estimate not depend on P_j that appears in ( 1 ) ? * * * * A3 * * : We thank the reviewer for finding it . Yes , the calculation of CVaR depends on the $ P_j $ . We have update the paper on this part to improve clarity . * * Q4 : In ( 1 ) , what \u2019 s the meaning of delta ( tau_i , u_i ) ? Isn \u2019 t that the Dirac function takes a number as its argument ? Why can the Dirac function take a trajectory as its argument ? For ( 2 ) , why does the estimate not depend on P_j that appears in ( 1 ) ? * * * * A4 * * : Yes , Dirac function takes a number as its argument . The reason is that we want to generalize it with agent \u2019 s experiences as the high dimension input as widely used in MARL methods [ 3,4 ] . While some other papers may use observations [ 5 ] or the hidden states [ 6 ] . Parameterized by GRU , we use the hidden states ( numbers ) as inputs to output the Dirac outputs . * * Q5 : what is the definition of IGM , and the definition of $ C^ { tot } $ ? * * * * A5 * * : QTRAN [ 3 ] introduced IGM . IGM says the optimal joint actions across agents are equivalent to the collection of individual optimal actions of each agent . This principle is used for global value factorization . $ C^ { tot } $ is the output from the monotonic network by taking the individual CVaR values as inputs . * * Q6 : If the reward is shared , why would the local CVaR be different from the global CVaR ? * * * * A6 * * : The shared reward is a global reward and each agent has no access to its real contribution ( local reward ) . Therefore , credit assignment is needed to estimate the contribution of each agent and training the policy of each agent . VDN [ 6 ] and QMIX [ 4 ] were proposed to address this problem . In RMIX , each agent also has no access to its real contribution to the global reward and instead gets the global reward . Possessing various kinds of abilities in each agent , the local CVaR values , which compose the global CVaR values via a monotonic network , are different from the global CVaR values . * * References * * : [ 1 ] Ruszczy\u0144ski , Andrzej . `` Risk-averse dynamic programming for Markov decision processes . '' Mathematical programming 125.2 ( 2010 ) : 235-261 . [ 2 ] Tamar , Aviv , et al . `` Policy gradient for coherent risk measures . '' Advances in Neural Information Processing Systems . 2015 . [ 3 ] Son , Kyunghwan , et al . `` QTRAN : Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning . '' International Conference on Machine Learning . 2019 . [ 4 ] Rashid , Tabish , et al . `` QMIX : Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning . '' International Conference on Machine Learning . 2018 . [ 5 ] Yang , Yaodong , et al . `` Q-value Path Decomposition for Deep Multiagent Reinforcement Learning . '' International Conference on Machine Learning . 2020 . [ 6 ] Sunehag , Peter , et al . `` Value-decomposition networks for cooperative multi-agent learning . '' arXiv preprint arXiv:1706.05296 ( 2017 ) ."}, "2": {"review_id": "1EVb8XRBDNr-2", "review_text": "Strengths : 1 ) The paper is well-written , and versed in the pulse of related works on the topic . This makes it very easy to assess the points of differentiation of the formulation/focus from prior art . Especially the problem class of MARL with risk measures is salient . 2 ) The authors clearly demonstrate the practical merits of the proposed techniques in ambitious experiments . This is a major upshot of the work : one may clearly see across challenging domains the performance gains achieved by RMIX . 3 ) Solving Dec-POMDPs is in general intractable , and one must often resort to particle filtering/belief representations of the state . Combining these technical challenges with risk sensitive utilities is nontrivial , and the authors have made a bold effort to obtain a tractable algorithm despite these issues . Importantly , their work has some conceptual justification . Weaknesses : 1 ) While the conceptual setup and theoretical contributions are clear , the actual training mechanism is given very little explanation . There should at least be an iterative update scheme or pseudo-code presenting the key algorithm of this work . This would serve to make it easier to distinguish what are the unique attributes to the algorithm put forth in this work outside of a contextual discussion and at a more granular algorithmic level . By reading the paper it is very difficult to understand what information agents must exchange during training . 2 ) What is the purpose of the generalized return PDF ? How does the information required for its estimation get processed algorithmically , and can each agent estimate its component of it with local information only ? This is not easy to discern . 3 ) Theorem 1 must somehow assume that each agent 's marginal utility is concave and that the joint utility is jointly concave in agents ' local policies , but a discussion of this seems missing . Without concavity , then one can only ensure that agents ' policies in the decentralized setting converge to stationarity , and that these stationary points belong to the set of extrema of the global utility . It may be the case that the fact that the risk-sensitive Bellman operator being a contraction is enough to mitigate these subtleties and ensure convergence to a global extrema , but this is not discussed . 4 ) Just defining the TD loss in eqn . ( 4 ) is not enough because the presence of a risk measure means that a vanilla TD step in terms of approximating an expected value does not hold . How does the `` `` ` blocking to avoid changing the weights of the agents \u2019 network from the dynamic risk level predictor '' change the TD population objective ? Under normal circumstances , this would be the Bellman optimality operator -- how does C^ { tot } mitigate this issue , and what relationship does this hold to the notions of risk-sensitive Bellman equations in Andrzej Ruszczy\u00b4nski . Risk-averse dynamic programming for markov decision processes . Mathematical Programming , 125 ( 2 ) :235\u2013261 , 2010 . See also : Kose , U. , & Ruszczynski , A . ( 2020 ) .Risk-Averse Learning by Temporal Difference Methods . arXiv preprint arXiv:2003.00780 . 5 ) In general , a discussion of the technical innovations required to establish the theorems is absent . Are these theorems inherited from the algorithms appearing in earlier work ? What is new ? Minor Comments : 1 ) References missing regarding risk-sensitive RL : Zhang , Junyu , Amrit Singh Bedi , Mengdi Wang , and Alec Koppel . `` Cautious Reinforcement Learning via Distributional Risk in the Dual Domain . '' arXiv preprint arXiv:2002.12475 ( 2020 ) . 2 ) The link between equation ( 2 ) and estimating a conditional expectation should be made more explicit . 3 ) The meaning and interpretation of IGM is ambiguous . More effort needs to be expended to properly explain it . The idea seems to be that individual agents ' optimal policy is equivalent to an oracle/meta-agent that aggregates information over the network . This notion is then key to interpreting the importance of Theorem 1 .", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , we appreciate your high-quality reviews and constructive feedbacks . We have cited the new risk-sensitive papers in our paper . Due to the 8-page main text limit , there are some parts are not discussed in detail and we have clarified them in the paper . Our goal is to design a risk-sensitive MARL method to fill the gap as few works have been done and the risk-sensitive problem is also a significant problem in real world applications . We will address your concerns and answer your questions point by point . * * Q1 : the actual training mechanism is given very little explanation * * * * A1 * * : The training is centralized training . The target of RMIX is to update the weights in agent network , risk operator and monotonic network . ( i ) First , a mini-batch of experience ( state , observations , actions , rewards , next states ) collected by each agent at each step is sampled from the replay buffer . ( ii ) Then we calculate the individual $ C^ { t } _ { i } $ of each agent by the equation in section 3.1 . ( iii ) Third , we input the $ C^ { t } _ { i } $ into the monotonic network and get the $ C^ { tot } $ . By following this step , we can get the target value and the predicted value in equation 7 and thus we get the TD loss . ( iv ) Finally , we can use gradient descent methods to minimize the TD loss . We also have added pseudo code Algorithm 1 in Appendix in the revised paper . * * Q2 : What is the purpose of the generalized return PDF ? * * * * A2 * * : We define the generalized return PDF for the local return distribution modelled by many Dirac functions which are further used to estimate the CVaR values . * * Q3 : Theorem 1 must somehow assume that each agent 's marginal utility is concave and that the joint utility is jointly concave in agents ' local policies , but a discussion of this seems missing . * * * * A3 * * : We will put the discussion in the final version . ( 1 ) Theorem 1 is defined for decentralized execution in Centralized Training and Decentralized Execution ( CTDE ) . In decentralized execution , each agent takes actions independently . As a consequence , there should be some rules designed for decentralized execution . ( 2 ) CVaR is a widely used risk measure and it enjoys many properties such as convexity ( it can be converted into the concave form ) . Under the monotonic global CVaR value factorization , Theorem 1 holds and it can be used for decentralized execution . ( 3 ) Your comment on the assumption of the concavity of each agent \u2019 s marginal utility is insightful . It seems that there are few works discussing whether such concavity assumption exists in QMIX , QTRAN and other methods . If such assumption truly exists or there are some methods that can help to maintain this assumption , the representational capacity will be increased . * * Q4 : Just defining the TD loss in eqn . ( 4 ) is not enough because the presence of a risk measure means that a vanilla TD step in terms of approximating an expected value does not hold . * * * * A4 * * : The TD loss defined in Eqn . ( 4 ) is the centralized training loss function for updating the weights of the network of agents , risk operator and monotonic network . With risk measure , the CVaR values can be seen as the expectation value of local return values of the chosen region of the local return distribution given the current risk level . With such expectation values , the optimization of TD loss is minimized with expectation value from the chosen pessimistic values in the local return distribution . * * Q5 : How does the `` blocking to avoid changing the weights of the agents \u2019 network from the dynamic risk level predictor '' change the TD population objective ? * * * * A5 * * : In TD learning , both weights of risk level predictor and agent network should be updated . The motivation of this blocking gradient design ( as shown in Figure 3 ) is to avoid gradients from risk level predictor to change the weights in the agent network because the agent network does not predict risk level and it estimates the local return distribution . This blocking gradient method is widely used in deep learning and deep RL , for example blocking gradients from the target network in DQN and its variants ."}, "3": {"review_id": "1EVb8XRBDNr-3", "review_text": "This paper proposes a new value-based method using risk measures in cooperative multi-agent reinforcement learning . The authors propose a new network structure that calculates global CVaR through individual distribution and learns risk-sensitized multi-agent policies . The authors also propose a new dynamic risk level prediction method that can dynamically adjust the risk level according to the agent \u2019 s observation and action . Applying risk-sensitive reinforcement learning in multi-agent reinforcement learning is interesting , but several points can be improved . This paper has a fundamental difference compared to the distributional reinforcement learning recently studied in single-agent reinforcement learning like IQN . In single-agent reinforcement learning , the distribution of Q-function is first learned through the distributional Bellman operator , and the learned distribution can be utilized in additional applications such as risk-sensitive reinforcement learning . Even if the mean value of the distribution is used without considering the risk , it shows higher performance than the existing reinforcement learning algorithms without distribution . The distributional Bellman operator has a richer training signal than the Bellman operator for the existing scalar Q-function , which enables fast representation learning . Moreover , by changing the sampling distribution through the learned distribution , risk-sensitive reinforcement learning can be easily applied to multiple risk measures . However , in this paper , the authors do not use distribution as a direct learning objective but only aim to maximize CVaR . Unlike the distributional bellman operator , this learning method can not be expected to improve training speed , and it is difficult to understand the meaning of the learned return distribution learned with the scalar loss function . Also , this paper proposes a dynamic risk level prediction , but this part is confused . The authors argue that this method solves time-consistency issues and unstable learning . However , there is an insufficient explanation as to why this problem occurred and how to solve it . There is also a lack of explanation on why risk level is divided into K steps and why alpha is defined as in equation ( 3 ) . Finally , detailed analyses are needed on how dynamic risk level prediction affects performance . There is a paper , which the authors must cite and compare to . This is not the same as RMIX , but close enough that it has to be compared . Hu et al . ( 2020 ) .QR-MIX : Distributional Value Function Factorisation for Cooperative Multi-Agent Reinforcement Learning . URL : https : //arxiv.org/pdf/2009.04197.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear reviewer , we thank you for your insightful comments on our paper . In the revised version , we cited QR-MIX paper and compared RMIX with QR-MIX on 10 scenarios . The results can be found in Section 4.2 and Appendix G.3 . We also present results analysis of RMIX on corridor in Appendix G.2 . We now explain your concerns and answer your questions point by point . * * Q1 : the authors do not use distribution as a direct learning objective but only aim to maximize CVaR . * * * * A1 * * : We use CVaR values as policies for decentralized execution in RMIX . ( 1 ) CVaR is a widely used risk measure and enjoys many properties such as convexity . The risk-sensitive RL [ 1,2,3,4 ] is an important research problem . In RMIX , each agent aims to estimate a local return distribution at each time step and uses the local return distribution to estimate the CVaR analytically for decentralized execution . RMIX estimates the global CVaR value via the monotonic network . ( 2 ) Risk-sensitive RL and distributional RL are orthogonal research topics . IQN ( a distributional RL method ) [ 5 ] made a connection between distributional RL and risk-sensitive RL by utilizing a risk measure over the return distribution . Distributional RL updates the return distribution while risk-sensitive RL optimizes the risk values ( policies ) . * * Q2 : Unlike the distributional bellman operator , this learning method can not be expected to improve training speed * * * * A2 * * : We answer your question in the following . ( 1 ) RMIX aims to learn risk-sensitive policies for enhanced cooperation and improving the training speed is not our main focus . ( 2 ) RMIX shows slower training speed compared with baselines due to the network architecture . We present training time cost of each algorithm on some scenarios as shown in the table below . Despite this fact , RMIX gains superior performance on corridor ( very hard ) , 6h_vs_8z ( very hard ) , 27m_vs_30m ( very hard ) and many other scenarios . We train our model on NVIDIA Tesla V100 GPU 16G and Intel ( R ) Xeon ( R ) CPU E5-2683 v3 @ 2.00GHz . | Scenarios | Training steps | RMIX | QMIX | VDN | IQL | QTRAN | MAVEN | | -- | -- | | -- | -- | -- | -- | | | corridor | 3 million | 1 day 13 hours | 24 hours | 21 hours | 22 hours | 21 hours | 23 hours | | 3s5z\\_vs\\_3s6z | 3 million | 23 hours | 20 hours | 18 hours | 19 hours | 19 hours | 17 hours | | MMM2 | 3 million | 22 hours | 20 hours | 18 hours | 19 hours | 19 hours | 16 hours | | 6h\\_vs\\_8z | 3 million | 1 day 18 hours | 20 hours | 19 hours | 19 hours | 20 hours | 15 hours | | 5m\\_vs\\_6m | 2 million | 19 hours | 18 hours | 9 hours | 12 hours | 13 hours | 11 hours | | 8m\\_vs\\_9m | 1 million | 8 hours | 8 hours | 8 hours | 8 hours | 8 hours | 6 hours | | 10m\\_vs\\_11m | 1 million | 9.5 hours | 8 hours | 6 hours | 7 hours | 8 hours | 7.5 hours | | 27m\\_vs\\_30m | 1.5 million | 23.5 hours | 18 hours | 17hours | 16 hours | 12 hours | 20 hours | However , the training speed of Deep RL and MARL algorithms varies on different computational platforms , hyper-parameters and training schemes . For training StarCraft II scenarios , when using high performance CPUs such as Intel ( R ) Core ( TM ) i9-9820X CPU @ 3.30GHz , it shows faster training speed especially on scenarios where the number of agent is large , for example 27m_vs_30m . ( 3 ) In fact , RMIX shows good sample efficiency compared with baseline methods ( QMIX , MAVEN , Qatten , etc . ) when training super hard scenarios such as corridor , 27m_vs_30 , 3s5z_vs_5s6z ( Figure 6 and 7 in the paper ) . Improving sample efficiency is vital for RL and MARL . ( 4 ) To our best knowledge , the paper of C51 [ 6 ] shows that \u201c for N = 51 , our TensorFlow implementation trains at roughly 75 % of DQN \u2019 s speed \u201d ( in footnote 2 , page 6 in the C51 paper ) . So , with Distributional Bellman Operator , distributional RL methods can be slower than DQN . The main reason is updating the return distribution takes more computational resources as the loss function causes the distribution update to requires complex distribution projection as shown in C51 \u2019 s paper . Besides that modelling distribution adds more parameters in the neural network , which can slow down the training speed ."}}