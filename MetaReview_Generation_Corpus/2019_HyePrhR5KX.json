{"year": "2019", "forum": "HyePrhR5KX", "title": "DyRep: Learning Representations over Dynamic Graphs", "decision": "Accept (Poster)", "meta_review": "After discussion, all reviewers agree to accept this paper. Congratulations!!", "reviews": [{"review_id": "HyePrhR5KX-0", "review_text": "Overall the paper suffers from a lack of clarity in the presentation, especially in algorithm 1, and does not communicate well why the assumption of different dynamical processes should be important in practice. Experiments show some improvement compared to (Trivedi et al. 2017) but are limited to two datasets and it is unclear to what extend end the proposed method would help for a larger variety of datasets. Not allowing for deletion of node, and especially edges, is a potential draw-back of the proposed method, but more importantly, in many graph datasets the type of nodes and edges is very important (e.g. a knowledge base graph without edges loses most relevant information) so not considering different types is a big limitation. Comments on the method (sections 2-4). About equation (1): \\bar{t} is not defined and its meaning is not obvious. The rate of event occurrence does not seem to depend on l (links status) whereas is seems to be dependent of l in algorithm 1. I don\u2019t see how the timings of association and communication processes are related, both \\lambda_k seem defined independently. Should we expect some temporal dependence between different types of events here? The authors mention that both point processes are \u201crelated through the mediation process and in the embedding space\u201d, a more rigorous definition would be helpful here. The authors claim to learn functions to compute node representations, however the representations z^u seem to be direct embeddings of the nodes. If the representations are computed as functions it should be clear what is the input and which functional form is assumed. I find algorithm 1 unclear and do not understand how it is formally derived, its justification seems rather fuzzy. It is also unclear how algorithm 1 relates to the loss optimisation presented in section 4. What is the mechanism for addition of new nodes to the graph? I don\u2019t see in algorithm 1 a step where nodes can be added but this might be handled in a different part of the training. Comments on the experiments section. Since the proposed method is a variation on (Trivedi et al. 2017), a strong baseline would include experiments performed on the same datasets (or at least one dataset) from that paper. It is not clear which events are actually observed. I can see how a structural change in the network can be observed but what exactly constitutes a communication event for the datasets presented? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for providing detailed comments . Below we provide clarifications on your specific points : - Importance of Two-time scale Process : We emphasize that the two-time scale expression of dynamic processes over graphs is not an assumption of our work ; it is a naturally observed phenomenon in any dynamic network . For instance , consider the dynamics over a social network . The growth of network ( topology change ) by addition of new users ( nodes ) or new friendships ( edges ) occurs at significantly different rate/dynamics compared to various activities on a * fixed * network topology ( self evolution of user \u2019 s features , effect on user from activities external to network , information propagation on network or interactions ( sending a message , liking a post , comments , etc . ) . Further , both these dynamics affect each other significantly - befriending someone on social network increases the likelihood of activities between those nodes and on the other way around , activities such as regularly liking or sharing a post or mere prolonged interest in posts from friends of friends may lead to a friendship or follow edge between non-friends . This dichotomy of expressing network processes at two different time-scales ( dynamic * of * the network or network evolution ) and ( dynamic * on * the network or network activities ) is a widely known phenomenon that is subject of several studies in dynamic networks literature [ 1,2,3,4,5 ] . However , to the best of our knowledge , our work is the first to adopt this paradigm for large scale representation learning over dynamic graphs and propose an end-to-end framework for the same . - Support for Node and Edge Types is inherent in our approach and not a limitation of our model . As both node and edge types are essentially features , our model does not require any modification in the approach incorporate them . We have added a brief discussion in Appendix B to explain how our model works in presence of them . Consequently , DyRep can learn representations over various categories of dynamic graphs including but not limited to social networks , biological networks , dynamic knowledge graphs etc . as long as data provides time stamped events for both network evolution and activities on the network . - Support for Deletion : Being a continuous-time model , our work captures fine-grained temporal dependencies among network processes . To achieve this , the model needs time stamped edges for graphs . However , as we mention in conclusion of our paper , it is difficult to procure data with fine grained deletion time stamps . Further , the temporal point process model requires more sophistication to support deletion . For example , one can augment the model with a survival process formulation to account for lack of node/edge at future time which is an involved task and requires a dedicated investigation outside the scope of this paper . - Temporal Dependence between events : $ lambda $ is the conditional intensity function the * conditional * part represents the occurrence of current event conditional on all past events . Hence , $ \\lambda ( t ) $ can also be written as $ \\lambda ( t|\\amthcal { H } _t ) $ to mention the conditional part where $ \\mathcal { H } _t $ represents history of all previous event occurrences . In the point process literature , $ \\mathcal { H } _t $ is often omitted as it is well understood . Next , the conditional intensity function is derived based on the most recent embeddings of the two nodes in the event . However the node embeddings get updated after every event ( whether k = 0 or k=1 ) . For instance , consider that a node $ u $ was involved in a communication event ( k=1 ) at time $ t1 $ , association event ( k=0 ) at time $ t2 $ and another communication event ( k=1 ) at time $ t3 $ ( $ t1 $ < $ t2 $ < $ t3 $ ) . In this case , the conditional intensity function computed for time $ t3 $ ( when k = 1 ) will use most recent embeddings of node $ u $ updated after its event at time $ t2 $ ( when k =0 ) and similarly the conditional intensity function computed for time $ t2 $ ( when k=0 ) will use most recent embeddings of node $ u $ updated after its event at time $ t1 $ ( when k=1 ) . This is how the two processes are interleaved with each other through evolving representations whose learning is the latent mediation process . [ 1 ] Bernard Chazelle . Natural Algorithms and Influence Systems , 2012 . [ 2 ] Damien Farine . The dynamics of transmission and the dynamics of networks , 2017 . [ 3 ] Oriol Artime et . al. , Dynamics on networks : competition of temporal and topological correlations , 2017 . [ 4 ] Haijun Zhou et . al. , Dynamic pattern evolution on scale-free networks , 2005 . [ 5 ] Farajtabar et . al. , Coevolve : A Joint Point Process Model for Information Diffusion and Network Evolution , 2015 ."}, {"review_id": "HyePrhR5KX-1", "review_text": "Overall, the contribution of the paper is somewhat limited [but a little more than my initial assessment, thanks to the rebuttal]. It is essentially an extension of (Trivedi et al. 2017), adding attention to provide self-exciting rates, applied to two types of edges (communication edges and \u201cfriendship\u201d edges). Conditioned on past edges, future edges are assumed independent, which makes the math trivial. The work would be better described as modeling a Marked Point Process with marks k \\in {0,1}. Other comments: 1. [addressed] DyRep-No-SP is as good as the proposed approach, maybe because the graph is assumed undirected and the embedding of u can be described by its neighbors (author rebuttal describes as Localized Propagation), as the neighbors themselves use the embedding of u for their own embedding (which means that self-propagation is never \"really off\"). Highly active nodes have a disproportional effect in the embedding, resulting in the better separated embeddings of Figure 4. [after rebuttal: what is the effect of node activity on the embeddings?] 2. [unresolved, comment still misundertood] The Exogenous Drive W_t(t_p \u2013 t_{p\u22121}) should be more personalized. Some nodes are intrinsically more active than others. [after rebuttal: answer \"$W_t(t_p - t_{p-1})$ is personalized as $t_p$ is node specific\", I meant personalized as in Exogenous Drive of people like Alice or Bob] 3. [unresolved] Fig 4 embeddings should be compared against (Trivedi et al. 2017) [after rebuttal: author revision does not make qualitative comparison against Trivedi et al. (2017)] Besides the limited innovation, the writing needs work. 4. [resolved] Equation 1 defines $g_k(\\bar{t})$ but does not define \\bar{t}. Knowing (Trivedi et al. 2017), I immediately knew what it was, but this is not standard notation and should be defined. 5. [resolved] $g_k$ must be a function of u and v 6. [resolved] \u201c$k$ represent the dynamic process\u201d = > \u201c$k$ represent the type of edge\u201d . The way it is written $k$ would need to be a stochastic process (it is just a mark, k \\in {0,1}) 7. [resolved] Algorithm 1 is impossibly confusing. I read it 8 times and I still cannot tell what it is supposed to do. It contains recursive definitions like $z_i = b + \\lambda_k^{ji}(t)$, where $\\lambda_k^{ji}(t)$ itself is a function of $z_i(t)$. Maybe the z_i(t) and z_i are different variables with the same name? 8. [resolved] The only hint that the graph under consideration is undirected comes from Algorithm 1, A_{uv}(t) = A_{vu}(t) = 1. It is *very* important information for the reader. Related work (to be added to literature): Dynamic graph embedding: (Yuan et al., 2017) (Ghassen et al., 2017) Dynamic sub-graph embedding: (Meng et al., 2018) Minor: state-of-arts => state-of-the-art methods list enumeration \u201c1.)\u201d , \u201c2.)\u201d is strange. Decide either 1) , 2) or 1. , 2. . I have never seen both. MAE => mean absolute error (MAE) Yuan, Y., Liang, X., Wang, X., Yeung, D. Y., & Gupta, A., Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection. ICCV, 2017. Jerfel, , Mehmet E. Basbug, and Barbara E. Engelhardt. \"Dynamic Collaborative Filtering with Compound Poisson Factorization.\" AISTATS 2017. Meng, C., Mouli, S.C., Ribeiro, B. and Neville, J., Subgraph Pattern Neural Networks for High-Order Graph Evolution Prediction. AAAI 2018. --- --- After rebuttal Authors addressed most of my concerns. The paper has merit and would be of interest to the community. I am increasing my score.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review ! We appreciate your comments and suggestions . As a preface to our response , we wish to mention that , unlike existing approaches , our work expresses dynamic graphs at multiple time-scales as follows : a ) Dynamic \u201d of \u201d the Network : This corresponds to the topological changes of the network \u2013 insertion or deletion of nodes and edges . We use `` Association '' to label the observed process corresponding to this dynamic . b ) Dynamic \u201d on \u201d the Network : This corresponds to activities on a * fixed * network topology \u2013 self evolution of node \u2019 s features , change in node \u2019 s features due to exogenous drive ( activities external to network ) , information propagation within network and interactions between nodes which may or may not have direct edge between them . We use `` Communication '' to label the observed process of interaction between nodes ( only the observed part of dynamic \u201d on \u201d the network ) . General Comment : ============== Overall , the contribution of the paper is limited . It is essentially a minor extension of ( Trivedi et al.2017 ) , adding attention , applied to two types of edges ( communication edges and \u201c friendship \u201d edges ) . Edges are assumed independent , which makes the math trivial . The work would be better described as modeling a Marked Poisson Process with marks k \\in { 0,1 } . Response : ========= We politely disagree with these comments as this is an incorrect characterization of our work . It seems that the misunderstanding arises from your assumption ( including point 6 ) that \u2018 k \u2019 is type of an edge , \u2018 k \u2019 is a mark and \u2018 k \u2019 has independence , none of which is true . \u2018 k \u2019 truly distinguishes scale of event dynamics ( not type of edge ) in our two-time scale model . In fact , when k=1 , it is an interaction event which is not considered as an edge between nodes in our model . The edge ( which forms graph structure ) only appears through an association event ( k=0 ) . Indeed , \u2018 k \u2019 corresponds to stochastic processes at different time scales and hence $ \\psi_k $ is the rate ( scale ) parameter corresponding to each dynamic . Further , every time when k=0 , an edge is created between different node pairs . As we clearly mention in the paper , we do not consider edge type in this work and hence \u2018 k \u2019 is not a mark . However , edge type can be added to Eq 4 in case it is available . Finally , dynamic processes realized by k=0 and k=1 are not independent and are highly interleaved in a nonlinear fashion . For instance , formation of a structural edge ( k=0 ) affects interactions ( k=1 ) and vice versa . Algorithm 1 captures this intricate dependencies as we will describe below . Based on the above points , it follows that our model is not a marked Poisson process . In fact , it does not take any specific form of point process - rather learns the conditional intensity function through a function approximation . In terms of contributions , we argue that our approach of modeling dynamic graphs at multiple scales and learning dynamic representations as latent mediation process bridging the two dynamic processes , is a significant innovation compared to any existing approaches . This is a non-trivial effort for a setting where the dynamic processes evolve in a complex and nonlinear fashion . Further , our temporal point process based structural-temporal self-attention mechanism to model attention based on event history of a node is very novel and has not been attempted before . Our attention model can : 1 ) take into account temporal dynamics of activities on edge and 2 ) capture effects from faraway nodes due to dependence on event history . This is a formal advancement to state-of-the-art models of non-uniform attention ( such as Graph Attention networks ) . Further , the paper provides an in-depth comparison with ( Trivedi et.al.2017 ) ( including Table 1 ) . Here we reiterate the differences : ( Trivedi et.al.2017 ) model events at single time scale and do not distinguish between two dynamic processes . They only consider edge level information for learning the embeddings . Our model considers a higher order neighborhood structure to compute embeddings . More importantly , in their work , the embedding update for a node \u2018 u \u2019 considers the edge information for the same node \u2018 u \u2019 at a previous time step . This is entirely different from our structural model based on \u201d Localized Embedding Propagation \u201d principle which states : Two nodes involved in an event form a temporary ( communication ) or a permanent ( association ) pathway for the information to propagate from the neighborhood of one node to the other node . This means , during the update of embedding for node \u2018 u \u2019 , information is propagated from the neighborhood of node \u2018 v \u2019 ( and not node \u2018 u \u2019 , please check Eq.4 ) to node \u2018 u \u2019 . Subsequently , ( Trivedi et.al.2017 ) does not have any attention mechanism as they do n't consider structure ."}, {"review_id": "HyePrhR5KX-2", "review_text": "The paper is very well written. The proposed approach is appropriate on modeling the node representations when the two types of events happen in the dynamic networks. Authors also clearly discussed the relevance and difference to related work. Experimental results show that the presented method outperforms the other baselines. Overall, it is a high-quality paper. There are only some minor comments for improving the paper: \u03bd Page 6, there is a typo. \u201cfor node v by employing \u2026\u201d should be \u201cfor node u\u201d \u03bd Page 6, \u201cBoth GAT and GaAN has\u201d should be \u201cBoth GAT and GaAN have\u201d \u03bd In section 5.1, it will be great if authors can explain more what are the \u201cassociation events\u201d and \u201ccommunication events\u201d with more details in these two evaluation datasets. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review ! We appreciate your time and supportive feedback and we are glad that you find our work interesting . Details about the corresponding association and communication events in the two datasets are provided in Appendix E.1 . We uploaded a revised version that contains your suggested changes ."}], "0": {"review_id": "HyePrhR5KX-0", "review_text": "Overall the paper suffers from a lack of clarity in the presentation, especially in algorithm 1, and does not communicate well why the assumption of different dynamical processes should be important in practice. Experiments show some improvement compared to (Trivedi et al. 2017) but are limited to two datasets and it is unclear to what extend end the proposed method would help for a larger variety of datasets. Not allowing for deletion of node, and especially edges, is a potential draw-back of the proposed method, but more importantly, in many graph datasets the type of nodes and edges is very important (e.g. a knowledge base graph without edges loses most relevant information) so not considering different types is a big limitation. Comments on the method (sections 2-4). About equation (1): \\bar{t} is not defined and its meaning is not obvious. The rate of event occurrence does not seem to depend on l (links status) whereas is seems to be dependent of l in algorithm 1. I don\u2019t see how the timings of association and communication processes are related, both \\lambda_k seem defined independently. Should we expect some temporal dependence between different types of events here? The authors mention that both point processes are \u201crelated through the mediation process and in the embedding space\u201d, a more rigorous definition would be helpful here. The authors claim to learn functions to compute node representations, however the representations z^u seem to be direct embeddings of the nodes. If the representations are computed as functions it should be clear what is the input and which functional form is assumed. I find algorithm 1 unclear and do not understand how it is formally derived, its justification seems rather fuzzy. It is also unclear how algorithm 1 relates to the loss optimisation presented in section 4. What is the mechanism for addition of new nodes to the graph? I don\u2019t see in algorithm 1 a step where nodes can be added but this might be handled in a different part of the training. Comments on the experiments section. Since the proposed method is a variation on (Trivedi et al. 2017), a strong baseline would include experiments performed on the same datasets (or at least one dataset) from that paper. It is not clear which events are actually observed. I can see how a structural change in the network can be observed but what exactly constitutes a communication event for the datasets presented? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for providing detailed comments . Below we provide clarifications on your specific points : - Importance of Two-time scale Process : We emphasize that the two-time scale expression of dynamic processes over graphs is not an assumption of our work ; it is a naturally observed phenomenon in any dynamic network . For instance , consider the dynamics over a social network . The growth of network ( topology change ) by addition of new users ( nodes ) or new friendships ( edges ) occurs at significantly different rate/dynamics compared to various activities on a * fixed * network topology ( self evolution of user \u2019 s features , effect on user from activities external to network , information propagation on network or interactions ( sending a message , liking a post , comments , etc . ) . Further , both these dynamics affect each other significantly - befriending someone on social network increases the likelihood of activities between those nodes and on the other way around , activities such as regularly liking or sharing a post or mere prolonged interest in posts from friends of friends may lead to a friendship or follow edge between non-friends . This dichotomy of expressing network processes at two different time-scales ( dynamic * of * the network or network evolution ) and ( dynamic * on * the network or network activities ) is a widely known phenomenon that is subject of several studies in dynamic networks literature [ 1,2,3,4,5 ] . However , to the best of our knowledge , our work is the first to adopt this paradigm for large scale representation learning over dynamic graphs and propose an end-to-end framework for the same . - Support for Node and Edge Types is inherent in our approach and not a limitation of our model . As both node and edge types are essentially features , our model does not require any modification in the approach incorporate them . We have added a brief discussion in Appendix B to explain how our model works in presence of them . Consequently , DyRep can learn representations over various categories of dynamic graphs including but not limited to social networks , biological networks , dynamic knowledge graphs etc . as long as data provides time stamped events for both network evolution and activities on the network . - Support for Deletion : Being a continuous-time model , our work captures fine-grained temporal dependencies among network processes . To achieve this , the model needs time stamped edges for graphs . However , as we mention in conclusion of our paper , it is difficult to procure data with fine grained deletion time stamps . Further , the temporal point process model requires more sophistication to support deletion . For example , one can augment the model with a survival process formulation to account for lack of node/edge at future time which is an involved task and requires a dedicated investigation outside the scope of this paper . - Temporal Dependence between events : $ lambda $ is the conditional intensity function the * conditional * part represents the occurrence of current event conditional on all past events . Hence , $ \\lambda ( t ) $ can also be written as $ \\lambda ( t|\\amthcal { H } _t ) $ to mention the conditional part where $ \\mathcal { H } _t $ represents history of all previous event occurrences . In the point process literature , $ \\mathcal { H } _t $ is often omitted as it is well understood . Next , the conditional intensity function is derived based on the most recent embeddings of the two nodes in the event . However the node embeddings get updated after every event ( whether k = 0 or k=1 ) . For instance , consider that a node $ u $ was involved in a communication event ( k=1 ) at time $ t1 $ , association event ( k=0 ) at time $ t2 $ and another communication event ( k=1 ) at time $ t3 $ ( $ t1 $ < $ t2 $ < $ t3 $ ) . In this case , the conditional intensity function computed for time $ t3 $ ( when k = 1 ) will use most recent embeddings of node $ u $ updated after its event at time $ t2 $ ( when k =0 ) and similarly the conditional intensity function computed for time $ t2 $ ( when k=0 ) will use most recent embeddings of node $ u $ updated after its event at time $ t1 $ ( when k=1 ) . This is how the two processes are interleaved with each other through evolving representations whose learning is the latent mediation process . [ 1 ] Bernard Chazelle . Natural Algorithms and Influence Systems , 2012 . [ 2 ] Damien Farine . The dynamics of transmission and the dynamics of networks , 2017 . [ 3 ] Oriol Artime et . al. , Dynamics on networks : competition of temporal and topological correlations , 2017 . [ 4 ] Haijun Zhou et . al. , Dynamic pattern evolution on scale-free networks , 2005 . [ 5 ] Farajtabar et . al. , Coevolve : A Joint Point Process Model for Information Diffusion and Network Evolution , 2015 ."}, "1": {"review_id": "HyePrhR5KX-1", "review_text": "Overall, the contribution of the paper is somewhat limited [but a little more than my initial assessment, thanks to the rebuttal]. It is essentially an extension of (Trivedi et al. 2017), adding attention to provide self-exciting rates, applied to two types of edges (communication edges and \u201cfriendship\u201d edges). Conditioned on past edges, future edges are assumed independent, which makes the math trivial. The work would be better described as modeling a Marked Point Process with marks k \\in {0,1}. Other comments: 1. [addressed] DyRep-No-SP is as good as the proposed approach, maybe because the graph is assumed undirected and the embedding of u can be described by its neighbors (author rebuttal describes as Localized Propagation), as the neighbors themselves use the embedding of u for their own embedding (which means that self-propagation is never \"really off\"). Highly active nodes have a disproportional effect in the embedding, resulting in the better separated embeddings of Figure 4. [after rebuttal: what is the effect of node activity on the embeddings?] 2. [unresolved, comment still misundertood] The Exogenous Drive W_t(t_p \u2013 t_{p\u22121}) should be more personalized. Some nodes are intrinsically more active than others. [after rebuttal: answer \"$W_t(t_p - t_{p-1})$ is personalized as $t_p$ is node specific\", I meant personalized as in Exogenous Drive of people like Alice or Bob] 3. [unresolved] Fig 4 embeddings should be compared against (Trivedi et al. 2017) [after rebuttal: author revision does not make qualitative comparison against Trivedi et al. (2017)] Besides the limited innovation, the writing needs work. 4. [resolved] Equation 1 defines $g_k(\\bar{t})$ but does not define \\bar{t}. Knowing (Trivedi et al. 2017), I immediately knew what it was, but this is not standard notation and should be defined. 5. [resolved] $g_k$ must be a function of u and v 6. [resolved] \u201c$k$ represent the dynamic process\u201d = > \u201c$k$ represent the type of edge\u201d . The way it is written $k$ would need to be a stochastic process (it is just a mark, k \\in {0,1}) 7. [resolved] Algorithm 1 is impossibly confusing. I read it 8 times and I still cannot tell what it is supposed to do. It contains recursive definitions like $z_i = b + \\lambda_k^{ji}(t)$, where $\\lambda_k^{ji}(t)$ itself is a function of $z_i(t)$. Maybe the z_i(t) and z_i are different variables with the same name? 8. [resolved] The only hint that the graph under consideration is undirected comes from Algorithm 1, A_{uv}(t) = A_{vu}(t) = 1. It is *very* important information for the reader. Related work (to be added to literature): Dynamic graph embedding: (Yuan et al., 2017) (Ghassen et al., 2017) Dynamic sub-graph embedding: (Meng et al., 2018) Minor: state-of-arts => state-of-the-art methods list enumeration \u201c1.)\u201d , \u201c2.)\u201d is strange. Decide either 1) , 2) or 1. , 2. . I have never seen both. MAE => mean absolute error (MAE) Yuan, Y., Liang, X., Wang, X., Yeung, D. Y., & Gupta, A., Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection. ICCV, 2017. Jerfel, , Mehmet E. Basbug, and Barbara E. Engelhardt. \"Dynamic Collaborative Filtering with Compound Poisson Factorization.\" AISTATS 2017. Meng, C., Mouli, S.C., Ribeiro, B. and Neville, J., Subgraph Pattern Neural Networks for High-Order Graph Evolution Prediction. AAAI 2018. --- --- After rebuttal Authors addressed most of my concerns. The paper has merit and would be of interest to the community. I am increasing my score.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review ! We appreciate your comments and suggestions . As a preface to our response , we wish to mention that , unlike existing approaches , our work expresses dynamic graphs at multiple time-scales as follows : a ) Dynamic \u201d of \u201d the Network : This corresponds to the topological changes of the network \u2013 insertion or deletion of nodes and edges . We use `` Association '' to label the observed process corresponding to this dynamic . b ) Dynamic \u201d on \u201d the Network : This corresponds to activities on a * fixed * network topology \u2013 self evolution of node \u2019 s features , change in node \u2019 s features due to exogenous drive ( activities external to network ) , information propagation within network and interactions between nodes which may or may not have direct edge between them . We use `` Communication '' to label the observed process of interaction between nodes ( only the observed part of dynamic \u201d on \u201d the network ) . General Comment : ============== Overall , the contribution of the paper is limited . It is essentially a minor extension of ( Trivedi et al.2017 ) , adding attention , applied to two types of edges ( communication edges and \u201c friendship \u201d edges ) . Edges are assumed independent , which makes the math trivial . The work would be better described as modeling a Marked Poisson Process with marks k \\in { 0,1 } . Response : ========= We politely disagree with these comments as this is an incorrect characterization of our work . It seems that the misunderstanding arises from your assumption ( including point 6 ) that \u2018 k \u2019 is type of an edge , \u2018 k \u2019 is a mark and \u2018 k \u2019 has independence , none of which is true . \u2018 k \u2019 truly distinguishes scale of event dynamics ( not type of edge ) in our two-time scale model . In fact , when k=1 , it is an interaction event which is not considered as an edge between nodes in our model . The edge ( which forms graph structure ) only appears through an association event ( k=0 ) . Indeed , \u2018 k \u2019 corresponds to stochastic processes at different time scales and hence $ \\psi_k $ is the rate ( scale ) parameter corresponding to each dynamic . Further , every time when k=0 , an edge is created between different node pairs . As we clearly mention in the paper , we do not consider edge type in this work and hence \u2018 k \u2019 is not a mark . However , edge type can be added to Eq 4 in case it is available . Finally , dynamic processes realized by k=0 and k=1 are not independent and are highly interleaved in a nonlinear fashion . For instance , formation of a structural edge ( k=0 ) affects interactions ( k=1 ) and vice versa . Algorithm 1 captures this intricate dependencies as we will describe below . Based on the above points , it follows that our model is not a marked Poisson process . In fact , it does not take any specific form of point process - rather learns the conditional intensity function through a function approximation . In terms of contributions , we argue that our approach of modeling dynamic graphs at multiple scales and learning dynamic representations as latent mediation process bridging the two dynamic processes , is a significant innovation compared to any existing approaches . This is a non-trivial effort for a setting where the dynamic processes evolve in a complex and nonlinear fashion . Further , our temporal point process based structural-temporal self-attention mechanism to model attention based on event history of a node is very novel and has not been attempted before . Our attention model can : 1 ) take into account temporal dynamics of activities on edge and 2 ) capture effects from faraway nodes due to dependence on event history . This is a formal advancement to state-of-the-art models of non-uniform attention ( such as Graph Attention networks ) . Further , the paper provides an in-depth comparison with ( Trivedi et.al.2017 ) ( including Table 1 ) . Here we reiterate the differences : ( Trivedi et.al.2017 ) model events at single time scale and do not distinguish between two dynamic processes . They only consider edge level information for learning the embeddings . Our model considers a higher order neighborhood structure to compute embeddings . More importantly , in their work , the embedding update for a node \u2018 u \u2019 considers the edge information for the same node \u2018 u \u2019 at a previous time step . This is entirely different from our structural model based on \u201d Localized Embedding Propagation \u201d principle which states : Two nodes involved in an event form a temporary ( communication ) or a permanent ( association ) pathway for the information to propagate from the neighborhood of one node to the other node . This means , during the update of embedding for node \u2018 u \u2019 , information is propagated from the neighborhood of node \u2018 v \u2019 ( and not node \u2018 u \u2019 , please check Eq.4 ) to node \u2018 u \u2019 . Subsequently , ( Trivedi et.al.2017 ) does not have any attention mechanism as they do n't consider structure ."}, "2": {"review_id": "HyePrhR5KX-2", "review_text": "The paper is very well written. The proposed approach is appropriate on modeling the node representations when the two types of events happen in the dynamic networks. Authors also clearly discussed the relevance and difference to related work. Experimental results show that the presented method outperforms the other baselines. Overall, it is a high-quality paper. There are only some minor comments for improving the paper: \u03bd Page 6, there is a typo. \u201cfor node v by employing \u2026\u201d should be \u201cfor node u\u201d \u03bd Page 6, \u201cBoth GAT and GaAN has\u201d should be \u201cBoth GAT and GaAN have\u201d \u03bd In section 5.1, it will be great if authors can explain more what are the \u201cassociation events\u201d and \u201ccommunication events\u201d with more details in these two evaluation datasets. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review ! We appreciate your time and supportive feedback and we are glad that you find our work interesting . Details about the corresponding association and communication events in the two datasets are provided in Appendix E.1 . We uploaded a revised version that contains your suggested changes ."}}