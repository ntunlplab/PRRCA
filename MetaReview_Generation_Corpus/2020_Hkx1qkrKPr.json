{"year": "2020", "forum": "Hkx1qkrKPr", "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification", "decision": "Accept (Poster)", "meta_review": "The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs. Though the reviews are mixed, and in the post-rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews. Accept.", "reviews": [{"review_id": "Hkx1qkrKPr-0", "review_text": "The authors propose a simple and interesting strategy, DropEdge, to alleviate the over-fitting and over-smoothing in GCN. The logic is simple and clear and the paper is well-written. Major concerns: 1. After randomly enforce a certain rate of edges to be zero, how to preserve properties in the original complex network, such as degree power-law distribution, communities? If it was not necessary to preserve the properties, then what information should be preserved from the original graph. 2. Randomly drop edges may result in disconnected components, how to handle disconnected components? 3. Why do the authors use dimension difference as the measure to quantitatively evaluate information loss in Thm 1. More dimension reduction does not mean more information loss. 4. As a follow-up concern for C1, graph sparsification makes more sense than DropEdge because it has clear information reserve targets while there is no target for the randomness in DropEdge. 5. In Table 1 and Fig 2, why the improvements for more layers are bigger than those of the fewer layers? 6. In Fig 2, why the trend of Reddit dataset is so different from others (the more layers the more improvements by applying DropEdge)? 7. In Table 2, why there are the DropEdge versions for some methods while not for some other methods (e.g., FastGCN, ASGCN)? Why there is no result of GAT? Minor: 1. Sec 3, \"notation\", \"\\mathbf{x}_n\" -> \"\\mathbf{x}_N\" 2. Eq (1), \"\\mathbf{h}_n^{(l+1)}\" -> \"\\mathbf{x}_N^{(l+1)}\" 3. What's C_l in the explaination under Eq(1)?", "rating": "3: Weak Reject", "reply_text": "We appreciate the reviewer for ordering the questions with numbers , which helps us to respond more conveniently . Q1 : DropEdge does change the graph properties for each epoch . But statistically , as discussed in our reply to Q4 , Reviewer # 4 , DropEdge does not change the expectation of neighbor aggregation that plays a crucial role in characterizing input graphs . Hence , the statistics of graph properties are still preserved . Q2 : Drawing for our reply in Q1 , DropEdge will not change the connectivity in expectation , even it may result in disconnected components occasionally in one epoch . Q3 : The information measurement in Thm.1 refers to how much freedom we have to describe a point in a certain space . The dimensionality of the space is a natural and direct choice , thus we use dimension reduction to reflect information loss . Q4 : As we discussed in Section 4.3 , The purpose of the graph sparsification and DropEdge are different . Graph sparsification aims to remove unnecessary edges of graphs , while keeping almost all information of the input graph , while DropEdge is an efficient approach to reduce the over-smoothing based on our theoretical analysis . Moreover , as mentioned in Q1 , DropEdge preserves the statistic of graph properties , and involves no bias . Q5 : According to our theoretical analysis , deeper GNN models suffer from more serious over-smoothing issues than that of shallower ones . It is thus not surprising that DropEdge can gain more improvements from more layers . The experimental results in Tab . 1 and Fig.2 validate our theoretical findings . Q6 : The trend of Reddit dataset is still generally consistent with other datasets if we compare the results of 4/8/32 layers ( the more layers the more improvements by applying DropEdge ) . The corner case happens when the depth is 16 . If we check Table 7 in the appendix , there is a huge performance drop in GCN without DropEdge at 16 layers , making the improvement by DropEdge bigger than that of 32 layers . Q7 : The motivation of FastGCN and ASGCN is to speed up GCN , and they can be considered as different efficient implementations of GCN . We believe performing a comparison on GCN is sufficient without further consideration on FastGCN and ASGCN . GAT is different from GCN , and we are willing to provide the results below : | | Cora | Citeseer | | GAT | 0.863 | 0.781 | | GAT w/ DropEdge | 0.881 | 0.792 | As expected , DropEdge can still enhance its performance . Minor Q3 : $ C_l $ refers to the size of $ l $ -th hidden layer ."}, {"review_id": "Hkx1qkrKPr-1", "review_text": "This paper studied the problem of \"deep\" GCNs where the goal is to develop training methods that can make GCN becomes deeper while maintaining good test accuracy. The authors proposed a new method called \"DropEdge\", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. This paper is clearly well-written and the authors conducted a comprehensive study on deep GCNs. I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper. The extensive experiment results also show that for deeper GCNs, DropEdge always win over other baselines (see Tab 1) despite most of them are marginal except the backbone being GraphSAGE on Citeseer. Can you explain why this is the case? Why other backbones seem to have similar performance even with DropEdge (i.e. most of the accuracy increase are less than 3 %). Question: 1. When looking at Tab 1, it looks like most of the time, 2-layers networks are already the best (or close to the best) and are clearly better than 32 layers. Therefore, this makes me wonder: why do we need deeper networks at all if the shallow networks can already achieve a good (almost best) performance and it is also much similar and efficient in training? Can you please clarify why do we care to train a deeper network at all under this scenario? Are there any reasons that we would like to use deeper network as opposed to shallower networks? 2. It is less clear to me regarding this sentence: \"DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it\" Overall, I think this paper presents an interesting study on making deeper GCNs comparable to shallow network performance, but since the the boosted performance doesn't really outperform most of the 2-layer networks, I would like to hear the justification of why we need the deeper networks for this node classification task. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for accepting the interestingness and the completeness of our paper . We present our responses below . Q1.About the significance of performance improvement . As mentioned in Section 5.1 , the benchmark datasets are well-studied and well-tuned in the graph learning field . Achieving a 1-2 % increase can be regarded as a remarkable improvement . For the baselines we consider in Tab.1 , DropEdge generally improves them around ( or bigger than ) 1 % under different depths , which is significant considering the challenge on these datasets . Q2.About the claim of our paper on deeper GNNs . The reviewer possibly misunderstood our claim . Our paper is not showing deeper is better . Instead , we are more interested in investigating why GCN failed with deep layers and how over-smoothing happened . We hence propose DropEdge , a simple but effective method that is capable of enhancing various kinds of GNNs regardless of the network depth . Our motivation of discussing and reporting the results of varying depth in Tab.1 is to study how much DropEdge can enhance deep GNNs . The reviewer raised that `` it looks like most of the time , 2-layers networks are already the best ( or close to the best ) '' , which is true but only when all models ( including the 2-layer ones ) have applied DropEdge . Q3.About the clarification of `` DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it '' . This sentence reflects two parts of Theorem 1 . As the first part , it retards the convergence speed of over-smoothing and as the second part , it relieves the information loss caused by over-smoothing ."}, {"review_id": "Hkx1qkrKPr-2", "review_text": "The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs). Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors. Ultimately, the authors provide solid empirical evidence that, while a bit heuristic, their method is effective at alleviating at least partially the issues of overfitting and oversmoothing. I vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty. Pros: Simple, intuitive method Draws from existing literature relating to dropout-like methods Little computational overhead Solid experimental justification Some theoretical support for the method Cons: Method is somewhat heuristic Mitigates, rather than solves, the issue of oversmoothing Limited novelty (straightforward extension of dropout to graphs edges) Unclear why dropping edges is \"valid\" augmentation Followup-questions/areas for improving score: It would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well). There is at least a nice interpretation of choosing 0.5 for the dropout proportion in regular dropout (maximum regularization). As brought up in the comments, edges to drop out to the graph's properties is an interesting direction to explore. While the authors state that they would like to keep the method simple and general, the method is ultimately devised as an adaptation of dropout to graphs, so exploiting graph-specific properties seems reasonable and a potential avenue to further improving performance. p2: \"First, DropEdge can be considered as a data augmentation technique\" Why are these augmentations valid; why should the output of the network be invariant to these augmentations? I would like to see some justification for why the proposed random modification of the graph structure is valid; intuitively, it seems like it might make the learning problem impossible in some cases. Deeper analysis of the (more interesting, I think) layer-independent regime would be nice. (As a side-note, the name \"layer-independent\" for this regime is a bit confusing, as the edges dropped out *do* depend on the layer here, whereas in the \"layer dependent\" regime, edges dropped out do *not* depend on the layer). Comments: Figure 1 could probably be re-organized to better highlight the comparison between GCNs with and without DropEdge; consolidating the content into 2 figures instead of 4 might be more easily parsable. Adding figure-specific captions and defining the x axis would also be nice. Use \"reduce\" in place of \"retard\" p2 \" With contending the scalability\" improve phrasing p2 \"By recent,\" -> \"Recently,\" p2 \"difficulty on\" -> \"difficulty in\" p2 \" deep networks lying\" -> \"deep networks lies\" p3 \"which is a generation of the conclusion\" improve phrasing p3 \" disconnected between\" -> \"disconnected from\" p4 \"adjacent matrix\" -> \"adjacency matrix\" p4 \"severer \" -> \"more severe\" p5 \"but has no help\" -> \"but is no help\" p5 \"no touch to the adjacency matrix\" -> improve phrasing ", "rating": "6: Weak Accept", "reply_text": "We really thank the reviewer for the recognition of our contributions to the experimental evaluations and the theoretical justification . Here , we would like to provide more explanations to address the reviewer 's concerns . Q1.The novelty of DropEdge . We agree that our DropEdge is simple and is inspired by Dropout . Yet , when we put it in the context of graph learning , DropEdge is indeed a novel method that is able to alleviate over-smoothing , while Dropout can not . DropEdge can be regarded as an extension of Dropout to graph edges , but this extension , in certain sense , is not straightforward\u2014people usually adapt the idea of Dropout in GNNs to drop the network activations with less thinking in dropping the network input . Interestingly , simply dropping edges by random is sufficient to deliver promising results , as verified by our paper experimentally and theoretically . Q2.Choosing the dropout proportion . The reviewer probably misunderstood the experimental setting . We did not fix the drop proportion $ p $ to be 0.8 for the main of our experiments . Instead , as already presented in the second paragraph in Section 5.1 , we use the validation set to determine the dropping rate for each benchmark in Tab.1 ; different datasets could have different dropping rates . In Section 5.2.1 ( and Fig.3 ) , we fix it to be 0.8 for a case study on evaluating how DropEdge can prevent from over-smoothing . To further illustrate how the dropping proportion actually acts , we have conducted an example experiment for GCN-4 ( the best GCN model for Cora in Tab.2 ) by varying $ p $ from 0 to 1 with an increasing step of 0.2 . The results are $ p $ | 1 | 0.8 | 0.6 | 0.4 |0.2 | 0 GCN|0.624| 0.778|0.87 |0.869 |0.862 |0.855 Clearly , it generally improves the performance when $ 0 < p < 0.8 $ . The exceptional cases are $ p=0.8\uff0c1 $ when GCN degenerates ( or closely degenerates ) to MLP , which is reasonable due to the less expressive power . Furthermore , the best performance is achieved when $ 0.4 \\leq p \\leq 0.6 $ ; the selection of dropout proportion $ p $ near 0.5 may also be a good choice . Q3.Exploiting graph-specific properties when considering DropEdge . Yes , this potentially promotes the performance . Given that our particular interest here is to keep the method simple and general , we are happy to explore more variants of sophisticated DropEdge in future work . Q4.The justification of why \u201c DropEdge can be considered as a data augmentation technique \u201d is valid . Thanks for the comment and sorry for the unclear clarification in the current submission . We provide an intuitive understanding here . The key in GNNs is to aggregate neighbors ' information for each node , which can be understood as a weighted sum of the neighbor features ( the weights are associated with the edges ) . From the perspective of neighbor aggregation , DropEdge enables a random subset aggregation instead of the full aggregation during GNN training . Statistically , DropEdge only changes the expectation of the neighbor aggregation up to a multiplier $ p $ , if we drop edges with probability $ p $ . This multiplier will be actually removed after weights normalization , which is often the case in practice . Therefore , DropEdge does not change the expectation of neighbor aggregation and is an unbiased data augmentation technique for GNN training . We have added the above specifications in the paper . Q5.On the layer-independent DropEdge . Thanks for the comment . Our original purpose of naming it like this is to reflect that DropEdge is conducted independently across layers . If this is a problem , we are willing to rename it as \u201c layer-wise DropEdge \u201d to remove the confusion . We agree that the analysis of \u201c layer-wise DropEdge \u201d is interesting and it is an important future work of theoretical aspects . Q6.Other comments . Thanks for the valuable suggestions and we will re-organize Fig.1 accordingly . We will also fix the typos throughout our paper ."}], "0": {"review_id": "Hkx1qkrKPr-0", "review_text": "The authors propose a simple and interesting strategy, DropEdge, to alleviate the over-fitting and over-smoothing in GCN. The logic is simple and clear and the paper is well-written. Major concerns: 1. After randomly enforce a certain rate of edges to be zero, how to preserve properties in the original complex network, such as degree power-law distribution, communities? If it was not necessary to preserve the properties, then what information should be preserved from the original graph. 2. Randomly drop edges may result in disconnected components, how to handle disconnected components? 3. Why do the authors use dimension difference as the measure to quantitatively evaluate information loss in Thm 1. More dimension reduction does not mean more information loss. 4. As a follow-up concern for C1, graph sparsification makes more sense than DropEdge because it has clear information reserve targets while there is no target for the randomness in DropEdge. 5. In Table 1 and Fig 2, why the improvements for more layers are bigger than those of the fewer layers? 6. In Fig 2, why the trend of Reddit dataset is so different from others (the more layers the more improvements by applying DropEdge)? 7. In Table 2, why there are the DropEdge versions for some methods while not for some other methods (e.g., FastGCN, ASGCN)? Why there is no result of GAT? Minor: 1. Sec 3, \"notation\", \"\\mathbf{x}_n\" -> \"\\mathbf{x}_N\" 2. Eq (1), \"\\mathbf{h}_n^{(l+1)}\" -> \"\\mathbf{x}_N^{(l+1)}\" 3. What's C_l in the explaination under Eq(1)?", "rating": "3: Weak Reject", "reply_text": "We appreciate the reviewer for ordering the questions with numbers , which helps us to respond more conveniently . Q1 : DropEdge does change the graph properties for each epoch . But statistically , as discussed in our reply to Q4 , Reviewer # 4 , DropEdge does not change the expectation of neighbor aggregation that plays a crucial role in characterizing input graphs . Hence , the statistics of graph properties are still preserved . Q2 : Drawing for our reply in Q1 , DropEdge will not change the connectivity in expectation , even it may result in disconnected components occasionally in one epoch . Q3 : The information measurement in Thm.1 refers to how much freedom we have to describe a point in a certain space . The dimensionality of the space is a natural and direct choice , thus we use dimension reduction to reflect information loss . Q4 : As we discussed in Section 4.3 , The purpose of the graph sparsification and DropEdge are different . Graph sparsification aims to remove unnecessary edges of graphs , while keeping almost all information of the input graph , while DropEdge is an efficient approach to reduce the over-smoothing based on our theoretical analysis . Moreover , as mentioned in Q1 , DropEdge preserves the statistic of graph properties , and involves no bias . Q5 : According to our theoretical analysis , deeper GNN models suffer from more serious over-smoothing issues than that of shallower ones . It is thus not surprising that DropEdge can gain more improvements from more layers . The experimental results in Tab . 1 and Fig.2 validate our theoretical findings . Q6 : The trend of Reddit dataset is still generally consistent with other datasets if we compare the results of 4/8/32 layers ( the more layers the more improvements by applying DropEdge ) . The corner case happens when the depth is 16 . If we check Table 7 in the appendix , there is a huge performance drop in GCN without DropEdge at 16 layers , making the improvement by DropEdge bigger than that of 32 layers . Q7 : The motivation of FastGCN and ASGCN is to speed up GCN , and they can be considered as different efficient implementations of GCN . We believe performing a comparison on GCN is sufficient without further consideration on FastGCN and ASGCN . GAT is different from GCN , and we are willing to provide the results below : | | Cora | Citeseer | | GAT | 0.863 | 0.781 | | GAT w/ DropEdge | 0.881 | 0.792 | As expected , DropEdge can still enhance its performance . Minor Q3 : $ C_l $ refers to the size of $ l $ -th hidden layer ."}, "1": {"review_id": "Hkx1qkrKPr-1", "review_text": "This paper studied the problem of \"deep\" GCNs where the goal is to develop training methods that can make GCN becomes deeper while maintaining good test accuracy. The authors proposed a new method called \"DropEdge\", where they randomly drop out the edges of the input graphs and demonstrate in experiments that this technique can indeed boost up the testing accuracy of deep GCN compared to other baselines. This paper is clearly well-written and the authors conducted a comprehensive study on deep GCNs. I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper. The extensive experiment results also show that for deeper GCNs, DropEdge always win over other baselines (see Tab 1) despite most of them are marginal except the backbone being GraphSAGE on Citeseer. Can you explain why this is the case? Why other backbones seem to have similar performance even with DropEdge (i.e. most of the accuracy increase are less than 3 %). Question: 1. When looking at Tab 1, it looks like most of the time, 2-layers networks are already the best (or close to the best) and are clearly better than 32 layers. Therefore, this makes me wonder: why do we need deeper networks at all if the shallow networks can already achieve a good (almost best) performance and it is also much similar and efficient in training? Can you please clarify why do we care to train a deeper network at all under this scenario? Are there any reasons that we would like to use deeper network as opposed to shallower networks? 2. It is less clear to me regarding this sentence: \"DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it\" Overall, I think this paper presents an interesting study on making deeper GCNs comparable to shallow network performance, but since the the boosted performance doesn't really outperform most of the 2-layer networks, I would like to hear the justification of why we need the deeper networks for this node classification task. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for accepting the interestingness and the completeness of our paper . We present our responses below . Q1.About the significance of performance improvement . As mentioned in Section 5.1 , the benchmark datasets are well-studied and well-tuned in the graph learning field . Achieving a 1-2 % increase can be regarded as a remarkable improvement . For the baselines we consider in Tab.1 , DropEdge generally improves them around ( or bigger than ) 1 % under different depths , which is significant considering the challenge on these datasets . Q2.About the claim of our paper on deeper GNNs . The reviewer possibly misunderstood our claim . Our paper is not showing deeper is better . Instead , we are more interested in investigating why GCN failed with deep layers and how over-smoothing happened . We hence propose DropEdge , a simple but effective method that is capable of enhancing various kinds of GNNs regardless of the network depth . Our motivation of discussing and reporting the results of varying depth in Tab.1 is to study how much DropEdge can enhance deep GNNs . The reviewer raised that `` it looks like most of the time , 2-layers networks are already the best ( or close to the best ) '' , which is true but only when all models ( including the 2-layer ones ) have applied DropEdge . Q3.About the clarification of `` DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it '' . This sentence reflects two parts of Theorem 1 . As the first part , it retards the convergence speed of over-smoothing and as the second part , it relieves the information loss caused by over-smoothing ."}, "2": {"review_id": "Hkx1qkrKPr-2", "review_text": "The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs). Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors. Ultimately, the authors provide solid empirical evidence that, while a bit heuristic, their method is effective at alleviating at least partially the issues of overfitting and oversmoothing. I vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty. Pros: Simple, intuitive method Draws from existing literature relating to dropout-like methods Little computational overhead Solid experimental justification Some theoretical support for the method Cons: Method is somewhat heuristic Mitigates, rather than solves, the issue of oversmoothing Limited novelty (straightforward extension of dropout to graphs edges) Unclear why dropping edges is \"valid\" augmentation Followup-questions/areas for improving score: It would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well). There is at least a nice interpretation of choosing 0.5 for the dropout proportion in regular dropout (maximum regularization). As brought up in the comments, edges to drop out to the graph's properties is an interesting direction to explore. While the authors state that they would like to keep the method simple and general, the method is ultimately devised as an adaptation of dropout to graphs, so exploiting graph-specific properties seems reasonable and a potential avenue to further improving performance. p2: \"First, DropEdge can be considered as a data augmentation technique\" Why are these augmentations valid; why should the output of the network be invariant to these augmentations? I would like to see some justification for why the proposed random modification of the graph structure is valid; intuitively, it seems like it might make the learning problem impossible in some cases. Deeper analysis of the (more interesting, I think) layer-independent regime would be nice. (As a side-note, the name \"layer-independent\" for this regime is a bit confusing, as the edges dropped out *do* depend on the layer here, whereas in the \"layer dependent\" regime, edges dropped out do *not* depend on the layer). Comments: Figure 1 could probably be re-organized to better highlight the comparison between GCNs with and without DropEdge; consolidating the content into 2 figures instead of 4 might be more easily parsable. Adding figure-specific captions and defining the x axis would also be nice. Use \"reduce\" in place of \"retard\" p2 \" With contending the scalability\" improve phrasing p2 \"By recent,\" -> \"Recently,\" p2 \"difficulty on\" -> \"difficulty in\" p2 \" deep networks lying\" -> \"deep networks lies\" p3 \"which is a generation of the conclusion\" improve phrasing p3 \" disconnected between\" -> \"disconnected from\" p4 \"adjacent matrix\" -> \"adjacency matrix\" p4 \"severer \" -> \"more severe\" p5 \"but has no help\" -> \"but is no help\" p5 \"no touch to the adjacency matrix\" -> improve phrasing ", "rating": "6: Weak Accept", "reply_text": "We really thank the reviewer for the recognition of our contributions to the experimental evaluations and the theoretical justification . Here , we would like to provide more explanations to address the reviewer 's concerns . Q1.The novelty of DropEdge . We agree that our DropEdge is simple and is inspired by Dropout . Yet , when we put it in the context of graph learning , DropEdge is indeed a novel method that is able to alleviate over-smoothing , while Dropout can not . DropEdge can be regarded as an extension of Dropout to graph edges , but this extension , in certain sense , is not straightforward\u2014people usually adapt the idea of Dropout in GNNs to drop the network activations with less thinking in dropping the network input . Interestingly , simply dropping edges by random is sufficient to deliver promising results , as verified by our paper experimentally and theoretically . Q2.Choosing the dropout proportion . The reviewer probably misunderstood the experimental setting . We did not fix the drop proportion $ p $ to be 0.8 for the main of our experiments . Instead , as already presented in the second paragraph in Section 5.1 , we use the validation set to determine the dropping rate for each benchmark in Tab.1 ; different datasets could have different dropping rates . In Section 5.2.1 ( and Fig.3 ) , we fix it to be 0.8 for a case study on evaluating how DropEdge can prevent from over-smoothing . To further illustrate how the dropping proportion actually acts , we have conducted an example experiment for GCN-4 ( the best GCN model for Cora in Tab.2 ) by varying $ p $ from 0 to 1 with an increasing step of 0.2 . The results are $ p $ | 1 | 0.8 | 0.6 | 0.4 |0.2 | 0 GCN|0.624| 0.778|0.87 |0.869 |0.862 |0.855 Clearly , it generally improves the performance when $ 0 < p < 0.8 $ . The exceptional cases are $ p=0.8\uff0c1 $ when GCN degenerates ( or closely degenerates ) to MLP , which is reasonable due to the less expressive power . Furthermore , the best performance is achieved when $ 0.4 \\leq p \\leq 0.6 $ ; the selection of dropout proportion $ p $ near 0.5 may also be a good choice . Q3.Exploiting graph-specific properties when considering DropEdge . Yes , this potentially promotes the performance . Given that our particular interest here is to keep the method simple and general , we are happy to explore more variants of sophisticated DropEdge in future work . Q4.The justification of why \u201c DropEdge can be considered as a data augmentation technique \u201d is valid . Thanks for the comment and sorry for the unclear clarification in the current submission . We provide an intuitive understanding here . The key in GNNs is to aggregate neighbors ' information for each node , which can be understood as a weighted sum of the neighbor features ( the weights are associated with the edges ) . From the perspective of neighbor aggregation , DropEdge enables a random subset aggregation instead of the full aggregation during GNN training . Statistically , DropEdge only changes the expectation of the neighbor aggregation up to a multiplier $ p $ , if we drop edges with probability $ p $ . This multiplier will be actually removed after weights normalization , which is often the case in practice . Therefore , DropEdge does not change the expectation of neighbor aggregation and is an unbiased data augmentation technique for GNN training . We have added the above specifications in the paper . Q5.On the layer-independent DropEdge . Thanks for the comment . Our original purpose of naming it like this is to reflect that DropEdge is conducted independently across layers . If this is a problem , we are willing to rename it as \u201c layer-wise DropEdge \u201d to remove the confusion . We agree that the analysis of \u201c layer-wise DropEdge \u201d is interesting and it is an important future work of theoretical aspects . Q6.Other comments . Thanks for the valuable suggestions and we will re-organize Fig.1 accordingly . We will also fix the typos throughout our paper ."}}