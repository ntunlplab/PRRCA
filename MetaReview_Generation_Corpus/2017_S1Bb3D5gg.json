{"year": "2017", "forum": "S1Bb3D5gg", "title": "Learning End-to-End Goal-Oriented Dialog", "decision": "Accept (Oral)", "meta_review": "Most dialog systems are based on chit-chat models. This paper explores goal-directed conversations, such as those that arise in booking a restaurant. While the methodology is rather thin, this is not the main focus of the paper. The authors provide creative evaluation protocols, and datasets. The reviewers liked the paper, and so does the AC. The paper will make a nice oral, on a topic that is largely explored but opening a direction that is quite novel.", "reviews": [{"review_id": "S1Bb3D5gg-0", "review_text": "SYNOPSIS: This paper introduces a new dataset for evaluating end-to-end goal-oriented dialog systems. All data is generated in the restaurant setting, where the goal is to find availability and eventually book a table based on parameters provided by the user to the bot as part of a dialog. Data is generated by running a simulation using an underlying knowledge base to generate samples for the different parameters (cuisine, price range, etc), and then applying rule-based transformations to render natural language descriptions. The objective is to rank a set of candidate responses for each next turn of the dialog, and evaluation is reported in terms of per-response accuracy and per-dialog accuracy. The authors show that Memory Networks are able to improve over basic bag-of-words baselines. THOUGHTS: I want to thank the authors for an interesting contribution. Having said that, I am skeptical about the utility of end-to-end trained systems in the narrow-domain setting. In the open-domain setting, there is a strong argument to be made that hand-coding all states and responses would not scale, and hence end-to-end trained methods make a lot of sense. However, in the narrow-domain setting, we usually know and understand the domain quite well, and the goal is to obtain high user satisfaction. Doesn't it then make sense in these cases to use the domain knowledge to engineer the best system possible? Given that the domain is already restricted, I'm also a bit disappointed that the goal is to RANK instead of GENERATE responses, although I understand that this makes evaluation much easier. I'm also unsure how these candidate responses would actually be obtained in practice? It seems that the models rank the set of all responses in train/val/test (last sentence before Sec 3.2). Since a key argument for the end-to-end training approach is ease of scaling to new domains without having to manually re-engineer the system, where is this information obtained for a new domain in practice? Generating responses would allow much better generalization to new domains, as opposed to simply ranking some list of hand-collected generic responses, and in my mind this is the weakest part of this work. Finally, as data is generated using a simulation by expanding (cuisine, price, ...) tuples using NL-generation rules, it necessarily constrains the variability in the training responses. Of course, this is traded off with the ability to generate unlimited data using the simulator. But I was unable to see the list of rules that was used. It would be good to publish this as well. Overall, despite my skepticism, I think it is an interesting contribution worthy of publication at the conference. ------ I've updated my score following the clarifications and new results.", "rating": "7: Good paper, accept", "reply_text": "> > > \u201c * Does n't it then make sense in these cases to use the domain knowledge to engineer the best system possible ? \u201d * We totally agree with this statement . Our paper should not be interpreted as a manual on how to build a restaurant booking bot in practice , but mostly as a way to address generalizability across all domains . We use the restaurant booking domain as a pretext for outlining key capabilities that a dialog system should have ( querying KBs , sorting options , etc . ) and we use this to study end-to-end models , a subset of all possible dialog systems . The choice of restaurant booking is meant to make comparisons to existing methods easier , but the value of domain-knowledge-free methods is that they generalize to all domains . We believe that this paper and his accompanying dataset can make valuable material for research in dialogue . > > > \u201c * Given that the domain is already restricted , I 'm also a bit disappointed that the goal is to RANK instead of GENERATE responses , although I understand that this makes evaluation much easier. \u201d * We indeed made the choice of ranking instead of generating in order to have the clearest evaluation possible . This evaluation is already non-trivial for end-to-end models as confirmed by the recent paper \u201c On the Evaluation of Dialogue Systems with Next Utterance Classification \u201d by Lowe et al. , so we think that performing well in this setting is a meaningful step before generating answers . The dataset could also be used in a generation setting by switching the metric to BLEU or something similar . > > > \u201c * I 'm also unsure how these candidate responses would actually be obtained in practice ? \u201d * Our goal in this paper is to provide an analysis research tool so this work can not be applied for building a bot in practice directly . However , many strategies could be used to create candidates such as harvesting large corpora of conversation logs or running simulations ."}, {"review_id": "S1Bb3D5gg-1", "review_text": "This paper presents a new, public dataset and tasks for goal-oriented dialogue applications. The dataset and tasks are constructed artificially using rule-based programs, in such a way that different aspects of dialogue system performance can be evaluated ranging from issuing API calls to displaying options, as well as full-fledged dialogue. This is a welcome contribution to the dialogue literature, which will help facilitate future research into developing and understanding dialogue systems. Still, there are pitfalls in taking this approach. First, it is not clear how suitable Deep Learning models are for these tasks compared to traditional methods (rule-based systems or shallow models), since Deep Learning models are known to require many training examples and therefore performance difference between different neural networks may simply boil down to regularization techniques. The tasks 1-5 are also completely deterministic, which means evaluating performance on these tasks won't measure the ability of the models to handle noisy and ambiguous interactions (e.g. inferring a distribution over user goals, or executing dialogue repair strategies), which is a very important aspect in dialogue applications. Overall, I still believe this is an interesting direction to explore. As discussed in the comments below, the paper does not have any baseline model with word order information. I think this is a strong weakness of the paper, because it makes the neural networks appear unreasonably strong, yet simpler baselines could very likely be be competitive (or better) than the proposed neural networks. To maintain a fair evaluation and correctly assess the power of representation learning for this task, I think it's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information. This would more convincly demonstrate the utility of Deep Learning models for this task. For example, the one could experiment with a logistic regression model which takes as input 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features. If such a baseline is included, I will increase my rating to 8. Final minor comment: in the conclusion, the paper states \"the existing work has no well defined measures of performances\". This is not really true. End-to-end trainable models for task-oriented dialogue have well-defined performance measures. See, for example \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al. On the other hand, non-goal-oriented dialogue are generally harder to evaluate, but given human subjects these can also be evaluated. In fact, this is what Liu et al (2016) do for Twitter. See also, \"Strategy and Policy Learning for Non-Task-Oriented Conversational Systems\" by Yu et al. ---- I've updated my score following the new results added in the paper.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "> > > \u201c * As discussed in the comments below , the paper does not have any baseline model with word order information . I think this is a strong weakness of the paper , because it makes the neural networks appear unreasonably strong , yet simpler baselines could very likely be be competitive ( or better ) than the proposed neural networks . To maintain a fair evaluation and correctly assess the power of representation learning for this task , I think it 's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information . This would more convincly demonstrate the utility of Deep Learning models for this task . For example , the one could experiment with a logistic regression model which takes as input 1 ) word embeddings ( similar to the Supervised Embeddings model ) , 2 ) bi-gram features , and 3 ) match-type features. \u201d * Thanks for these suggestions . We have added extra experiments augmenting supervised embeddings and TF-IDF methods with match-type features , and a vocabulary including all bigrams to leverage order information . The bigrams hardly make any difference in practice . Match-type features do help , however the memory networks still retain their clear edge compared to memory-less supervised embeddings and TF-IDF . The results of the new experiments have been added to the result table ( for TF-IDF ) and a new table in appendix D ( for supervised embeddings ) . > > > * Final minor comment : in the conclusion , the paper states `` the existing work has no well defined measures of performances '' . This is not really true . End-to-end trainable models for task-oriented dialogue have well-defined performance measures . See , for example `` A Network-based End-to-End Trainable Task-oriented Dialogue System '' by Wen et al.On the other hand , non-goal-oriented dialogue are generally harder to evaluate , but given human subjects these can also be evaluated . In fact , this is what Liu et al ( 2016 ) do for Twitter . See also , `` Strategy and Policy Learning for Non-Task-Oriented Conversational Systems '' by Yu et al . * ( Wen et al. , 2016 ) use a collection of measures including BLEU scores , entity matching rate and objective task success rate ( computed by auxiliary networks ) as well as human judgements collected through Mechanical Turk . It is true that all these measures are well defined but with this paper , we argue that they might not be enough . For clarity , we rephased this part of the conclusion as \u201c ( i ) existing measures of performance either prevent reproducibility ( different Mechanical Turk jobs ) or do not correlate well with human judgements \\citep { liu2016not } ; \u201d ."}, {"review_id": "S1Bb3D5gg-2", "review_text": "Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of high significance. Two important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper: 1) The performance of end2end ML approaches is still insufficient for goal oriented dialogs. 2) When comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions. While its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops. While this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than for revision. First they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact. Would the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one. For the concierge data, what would happen if \u2018correct\u2019 meant being the best, not among the 5-best? While I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator Another issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code \u2018Indian\u2019 as \u2018type of cuisine\u2019, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "> > > This review contains many very interesting points about dialogue learning and evaluation that go beyond the scope of this paper . If we were to sell Virtual Assistants as a service , we agree that we would not necessarily use per dialog accuracy as a metric . We might actually not use a single metric and but a combination assessing task completion and user satisfaction for instance . > > > * \u201c what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base \u201d * The fact that the vocabulary matches perfectly between dialogues and the KB is clearly a simplification of our setting . In a real world scenario , one would have to run entity and coreference resolution algorithms which might cause additional errors . The easier setting of this paper grants a clearer interpretation of errors and success ."}], "0": {"review_id": "S1Bb3D5gg-0", "review_text": "SYNOPSIS: This paper introduces a new dataset for evaluating end-to-end goal-oriented dialog systems. All data is generated in the restaurant setting, where the goal is to find availability and eventually book a table based on parameters provided by the user to the bot as part of a dialog. Data is generated by running a simulation using an underlying knowledge base to generate samples for the different parameters (cuisine, price range, etc), and then applying rule-based transformations to render natural language descriptions. The objective is to rank a set of candidate responses for each next turn of the dialog, and evaluation is reported in terms of per-response accuracy and per-dialog accuracy. The authors show that Memory Networks are able to improve over basic bag-of-words baselines. THOUGHTS: I want to thank the authors for an interesting contribution. Having said that, I am skeptical about the utility of end-to-end trained systems in the narrow-domain setting. In the open-domain setting, there is a strong argument to be made that hand-coding all states and responses would not scale, and hence end-to-end trained methods make a lot of sense. However, in the narrow-domain setting, we usually know and understand the domain quite well, and the goal is to obtain high user satisfaction. Doesn't it then make sense in these cases to use the domain knowledge to engineer the best system possible? Given that the domain is already restricted, I'm also a bit disappointed that the goal is to RANK instead of GENERATE responses, although I understand that this makes evaluation much easier. I'm also unsure how these candidate responses would actually be obtained in practice? It seems that the models rank the set of all responses in train/val/test (last sentence before Sec 3.2). Since a key argument for the end-to-end training approach is ease of scaling to new domains without having to manually re-engineer the system, where is this information obtained for a new domain in practice? Generating responses would allow much better generalization to new domains, as opposed to simply ranking some list of hand-collected generic responses, and in my mind this is the weakest part of this work. Finally, as data is generated using a simulation by expanding (cuisine, price, ...) tuples using NL-generation rules, it necessarily constrains the variability in the training responses. Of course, this is traded off with the ability to generate unlimited data using the simulator. But I was unable to see the list of rules that was used. It would be good to publish this as well. Overall, despite my skepticism, I think it is an interesting contribution worthy of publication at the conference. ------ I've updated my score following the clarifications and new results.", "rating": "7: Good paper, accept", "reply_text": "> > > \u201c * Does n't it then make sense in these cases to use the domain knowledge to engineer the best system possible ? \u201d * We totally agree with this statement . Our paper should not be interpreted as a manual on how to build a restaurant booking bot in practice , but mostly as a way to address generalizability across all domains . We use the restaurant booking domain as a pretext for outlining key capabilities that a dialog system should have ( querying KBs , sorting options , etc . ) and we use this to study end-to-end models , a subset of all possible dialog systems . The choice of restaurant booking is meant to make comparisons to existing methods easier , but the value of domain-knowledge-free methods is that they generalize to all domains . We believe that this paper and his accompanying dataset can make valuable material for research in dialogue . > > > \u201c * Given that the domain is already restricted , I 'm also a bit disappointed that the goal is to RANK instead of GENERATE responses , although I understand that this makes evaluation much easier. \u201d * We indeed made the choice of ranking instead of generating in order to have the clearest evaluation possible . This evaluation is already non-trivial for end-to-end models as confirmed by the recent paper \u201c On the Evaluation of Dialogue Systems with Next Utterance Classification \u201d by Lowe et al. , so we think that performing well in this setting is a meaningful step before generating answers . The dataset could also be used in a generation setting by switching the metric to BLEU or something similar . > > > \u201c * I 'm also unsure how these candidate responses would actually be obtained in practice ? \u201d * Our goal in this paper is to provide an analysis research tool so this work can not be applied for building a bot in practice directly . However , many strategies could be used to create candidates such as harvesting large corpora of conversation logs or running simulations ."}, "1": {"review_id": "S1Bb3D5gg-1", "review_text": "This paper presents a new, public dataset and tasks for goal-oriented dialogue applications. The dataset and tasks are constructed artificially using rule-based programs, in such a way that different aspects of dialogue system performance can be evaluated ranging from issuing API calls to displaying options, as well as full-fledged dialogue. This is a welcome contribution to the dialogue literature, which will help facilitate future research into developing and understanding dialogue systems. Still, there are pitfalls in taking this approach. First, it is not clear how suitable Deep Learning models are for these tasks compared to traditional methods (rule-based systems or shallow models), since Deep Learning models are known to require many training examples and therefore performance difference between different neural networks may simply boil down to regularization techniques. The tasks 1-5 are also completely deterministic, which means evaluating performance on these tasks won't measure the ability of the models to handle noisy and ambiguous interactions (e.g. inferring a distribution over user goals, or executing dialogue repair strategies), which is a very important aspect in dialogue applications. Overall, I still believe this is an interesting direction to explore. As discussed in the comments below, the paper does not have any baseline model with word order information. I think this is a strong weakness of the paper, because it makes the neural networks appear unreasonably strong, yet simpler baselines could very likely be be competitive (or better) than the proposed neural networks. To maintain a fair evaluation and correctly assess the power of representation learning for this task, I think it's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information. This would more convincly demonstrate the utility of Deep Learning models for this task. For example, the one could experiment with a logistic regression model which takes as input 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features. If such a baseline is included, I will increase my rating to 8. Final minor comment: in the conclusion, the paper states \"the existing work has no well defined measures of performances\". This is not really true. End-to-end trainable models for task-oriented dialogue have well-defined performance measures. See, for example \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al. On the other hand, non-goal-oriented dialogue are generally harder to evaluate, but given human subjects these can also be evaluated. In fact, this is what Liu et al (2016) do for Twitter. See also, \"Strategy and Policy Learning for Non-Task-Oriented Conversational Systems\" by Yu et al. ---- I've updated my score following the new results added in the paper.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "> > > \u201c * As discussed in the comments below , the paper does not have any baseline model with word order information . I think this is a strong weakness of the paper , because it makes the neural networks appear unreasonably strong , yet simpler baselines could very likely be be competitive ( or better ) than the proposed neural networks . To maintain a fair evaluation and correctly assess the power of representation learning for this task , I think it 's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information . This would more convincly demonstrate the utility of Deep Learning models for this task . For example , the one could experiment with a logistic regression model which takes as input 1 ) word embeddings ( similar to the Supervised Embeddings model ) , 2 ) bi-gram features , and 3 ) match-type features. \u201d * Thanks for these suggestions . We have added extra experiments augmenting supervised embeddings and TF-IDF methods with match-type features , and a vocabulary including all bigrams to leverage order information . The bigrams hardly make any difference in practice . Match-type features do help , however the memory networks still retain their clear edge compared to memory-less supervised embeddings and TF-IDF . The results of the new experiments have been added to the result table ( for TF-IDF ) and a new table in appendix D ( for supervised embeddings ) . > > > * Final minor comment : in the conclusion , the paper states `` the existing work has no well defined measures of performances '' . This is not really true . End-to-end trainable models for task-oriented dialogue have well-defined performance measures . See , for example `` A Network-based End-to-End Trainable Task-oriented Dialogue System '' by Wen et al.On the other hand , non-goal-oriented dialogue are generally harder to evaluate , but given human subjects these can also be evaluated . In fact , this is what Liu et al ( 2016 ) do for Twitter . See also , `` Strategy and Policy Learning for Non-Task-Oriented Conversational Systems '' by Yu et al . * ( Wen et al. , 2016 ) use a collection of measures including BLEU scores , entity matching rate and objective task success rate ( computed by auxiliary networks ) as well as human judgements collected through Mechanical Turk . It is true that all these measures are well defined but with this paper , we argue that they might not be enough . For clarity , we rephased this part of the conclusion as \u201c ( i ) existing measures of performance either prevent reproducibility ( different Mechanical Turk jobs ) or do not correlate well with human judgements \\citep { liu2016not } ; \u201d ."}, "2": {"review_id": "S1Bb3D5gg-2", "review_text": "Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of high significance. Two important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper: 1) The performance of end2end ML approaches is still insufficient for goal oriented dialogs. 2) When comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions. While its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops. While this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than for revision. First they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact. Would the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one. For the concierge data, what would happen if \u2018correct\u2019 meant being the best, not among the 5-best? While I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator Another issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code \u2018Indian\u2019 as \u2018type of cuisine\u2019, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "> > > This review contains many very interesting points about dialogue learning and evaluation that go beyond the scope of this paper . If we were to sell Virtual Assistants as a service , we agree that we would not necessarily use per dialog accuracy as a metric . We might actually not use a single metric and but a combination assessing task completion and user satisfaction for instance . > > > * \u201c what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base \u201d * The fact that the vocabulary matches perfectly between dialogues and the KB is clearly a simplification of our setting . In a real world scenario , one would have to run entity and coreference resolution algorithms which might cause additional errors . The easier setting of this paper grants a clearer interpretation of errors and success ."}}