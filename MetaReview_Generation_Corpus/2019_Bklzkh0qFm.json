{"year": "2019", "forum": "Bklzkh0qFm", "title": "Relational Graph Attention Networks", "decision": "Reject", "meta_review": "The authors propose an architecture for learning and predicting graphs with relations between nodes. The approach is a combination of recent research efforts into Graph Attention Networks and Relational Graph Convolutional Networks. The authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. However, the paper simply doesn't have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. While the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. Rather, this could be a technical report.", "reviews": [{"review_id": "Bklzkh0qFm-0", "review_text": "This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations. The paper is well written and the equations easy to follow. The results are not strong. And, unfortunately, the model contribution currently is too modest. Inductive task results: Wu et al. (2018) reports that for Tox21 (Duvenauld et al. 2015) is the best-performing approach. We should see the performance on other datasets (e.g., some of the other datasets in Wu et al. (2018)). My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs). Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution. --- After rebuttal --- Still not convinced of the value of the work to the community. Will keep my score the same.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer 2 , thank you for your constructive feedback and for taking the time to review our work . Your specific points are addressed below . > \u201c The results are not strong . And , unfortunately , the model contribution currently is too modest. \u201d Indeed , the model is a minor contribution and , especially in light of a more thorough evaluation of RGCN , the sum attention RGAT results do not improve on those in Schlichtkrull et al . ( 2017 ) .However , we would like to highlight that despite being a simple modification , the implementation of such a model that is trainable in a reasonable time frame is non-trivial . We plan to make our implementation public to aid future research in the area . To generalise the model , following the recommendations in one of the comments , we have investigated applying the Transformer-style dot product attention that was presented in GaAN : Gated Attention Networks for Learning on Large and Spatiotemporal Graphs ( Zhang et al.2018 ) .We believe that this additional investigation , accompanied by marginally improved results on Tox21 for dot product attention ( Mean test AUCs : WIRGAT 0.838 +/- 0.007 , ARGAT 0.837 +/- 0.007 ) enhance model contributions of the paper . We also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs . > \u201c We should see the performance on other datasets ( e.g. , some of the other datasets in Wu et al . ( 2018 ) \u201d We agree that including experiments on the other data sets presented in Wu et al . ( 2018 ) would be a valuable addition to the paper . Unfortunately , we are unable to perform the required thorough hyperparameter exploration required to draw meaningful conclusions within the remaining time of the rebuttal period . This is something we will investigate in the future . > \u201c My introduction suggestion : do not talk about Convolutional neural networks ( CNNs ) . \u201d Thank you for this stylistic critique . We see how the approach taken did not provide an intuition about the problem as well as it could have . We agree with your point regarding the wealth of graph neural network studies . We feel that the field of geometric deep learning , from which part of the direction of this work originated , is important to keep as part of the development of graph-based machine learning models . Some recent publications in the area of graph based ML have put less emphasis on geometric deep learning , the generalisations of convolutions from grids to graph , and the modifications of convolutions to achieve non-basis dependent methods . We felt that it is important to keep these concepts associated with the field of graph based ML . This introduction could , however , talk less in detail about CNNs themselves , and deal more with graphs - the main focus of the paper . We will produce a reworked introduction where graphs play a larger role . This should provide a more intuitive introduction to our work , whilst maintaining cornerstone concepts ."}, {"review_id": "Bklzkh0qFm-1", "review_text": "This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I don\u2019t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are \u201calmost\u201d identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer 3 , thank you for taking time to read and review our paper and for your useful comments . Hopefully the new results in our response will better aid discussion . Your specific points are addressed below . > i ) \u201c The proposed architecture is mainly adopted from the graph attention networks ( Veli\u010dkovi\u0107 et al.2017 ) and the relational graph networks ( Schlichtkrull et al.2018 ) .Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. \u201d We concede that the modifications to the existing models is a minor contribution . We would like to highlight that despite being a simple modification , the implementation of such a model that is trainable in a reasonable time frame is non-trivial . We plan to make our implementation public to aid research in the area . To generalise the model , following the recommendations in one of the comments , we have investigated applying the Transformer-style dot product attention that was presented in GaAN : Gated Attention Networks for Learning on Large and Spatiotemporal Graphs ( Zhang et al.2018 ) for non-relational graphs . We believe that this additional investigation , accompanied by marginally improved results on Tox21 for dot product attention ( Mean test AUCs : WIRGAT 0.838 +/- 0.007 , ARGAT 0.837 +/- 0.007 ) enhance the paper \u2019 s contribution . > ii ) \u201c In table 2 , I don \u2019 t really see any promising results compared to baselines . There are little improvements over the baselines or even significantly worse. \u201d We agree that the results do not improve on those in Schlichtkrull et al . ( 2017 ) or Wu et al . ( 2017 ) in a significant way . This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT . These results will be included in the new manuscript . We also feel that some of the results being \u201c significantly worse \u201d is one of the main contributions of our paper . Specifically , the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart . This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [ a statement along these lines was made in Schlichtkrull et al . ( 2017 ) ] .The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem . On the other hand , the newly evaluated dot-product attention does no worse ( or better ) than RGCN , indicating a more promising research direction to pursue . > iii ) \u201c Could you explain why your MUTAG is now a single graph and is cast as node classification problem ? \u201d The MUTAG dataset in its standard form from e.g.TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description . There is an alternative dataset version of mutag distributed with dl-learner [ 1 ] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [ 2 ] . In this scenario , MUTAG is in presented in the RDF , where each of the 340 presented complex molecules is described by the links in an RDF with edges ( triples ) of the form d1 - > hasAtom - > d1_1 d1 - > hasBond - > bond1 d1- > hasStructure - > ring_size_6-1 where in this case , d1 is the complex molecule , d1_1 is the first atom in complex molecule d1 , and bond1 is the first bond in complex molecule d1 , and so on . There are many more types than this , and are viewable in the .owl located at [ 2 ] . Nodes correspond to entities from the point of view of RDF . Each complex molecule is given a label according to whether it is mutagenic or not ( determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation ) . This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task ( nodes corresponding to anything other than the complex molecule itself are unlabelled ) . Each molecule can be viewed as a separate graph , or the RDF can be viewed as a single graph comprising of a description of all molecules . If we train on batches of separate graphs and can fit all of the molecule graphs in memory , then these two are the same from the point of view of the training objective . [ 1 ] https : //github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [ 2 ] https : //www.semanticscholar.org/paper/RDF2Vec % 3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752"}, {"review_id": "Bklzkh0qFm-2", "review_text": "The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer 1 , thank you for taking time to read and review our paper and for your useful comments . Hopefully the new results in our response will better aid discussion . Your specific points are addressed below . > \u201c ... the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution . We would like to highlight that despite being a simple modification , producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details . We plan to make our code public to aid research in the area . To generalise the model , following the recommendations in one of the comments , we have investigated applying the Transformer-style dot product attention that was presented in GaAN ( Zhang at al.2018 https : //arxiv.org/abs/1803.07294 ) . This generalises the notion of RGAT , and we believe that this increase the contributions in terms of model modification that our paper offers . > \u201c ... and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to ( e.g.this method builds heavily on Velickovic et al 2017 ) . \u201d Unfortunately we can not directly compare our approach to Velickovic et al . ( 2017 ) on relational graphs since their proposed model doesn \u2019 t support relationship types . Hence , we only compare to Schlichtkrull et al . ( 2017 ) and the conventional baselines , whereas Velickovic et al ( 2017 ) reports results on the non-relational graphs Cora , Citeseer , Pubmed and PPI . In the case of the RDF tasks , our model hyperparameter search space does include a model very similar to vanilla GAT . In the case where we only have both one basis kernel for each of the convolution and the attention , i.e.where B_V and B_v in equation ( 8 ) are set to 1 , then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d . During our hyperparameter search , however , no favourable points for evaluation set performance were discovered with basis sizes lower than 10 ( a basis size of 5 is permitted in the search ) . This leads us to conclude that vanilla GAT would not perform well on the RDF tasks . As mentioned above , we have now also evaluated the dot-product style attention of the transformer and have included it in our results . As one of the public comments mentioned , a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks ( Li et al.2015 ) would be extremely worthwhile . We we feel that evaluation lies outside of the scope of this work , however , which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour . > \u201c ... the results achieved in the experiments are very small improvements compared to the baseline of RGCN\u2026 \u201d We agree that any improvements compared to RGCN are marginal . In light of your third comment ( below ) regarding hyperparameters , the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT . The new dot product attention results on Tox21 ( Mean test AUCs : WIRGAT 0.838 +/- 0.007 , ARGAT 0.837 +/- 0.007 ) are slightly improved compared those of the sum attention , however , due to the retrained RGCN baseline , the relative gap between the best performing RGAT and RGCN is now smaller than it was before . We also see value in reporting these negative results . It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [ a statement along these lines was made in Schlichtkrull et al . ( 2017 ) ] .The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem . On the other hand , the newly evaluated dot-product attention does no worse ( or better ) than RGCN , indicating a more promising research direction to pursue . > \u201c ... often these small variations in results can be compensated with better baselines training\u2026 \u201d We also suspected this was a possibility , and in the same of sum-style attention it turns out to be true . To determine whether this was the case , we performed the same hyperparameter optimisations to our implementation of RGCN . In the case of the RDF datasets AIFB and MUTAG , we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al . ( 2017 ) .On the other hand , our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al , 2017 ti match the performance of the sum-style attention mechanism RGAT performance . This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism , however , although this is not significant as discussed above ."}], "0": {"review_id": "Bklzkh0qFm-0", "review_text": "This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations. The paper is well written and the equations easy to follow. The results are not strong. And, unfortunately, the model contribution currently is too modest. Inductive task results: Wu et al. (2018) reports that for Tox21 (Duvenauld et al. 2015) is the best-performing approach. We should see the performance on other datasets (e.g., some of the other datasets in Wu et al. (2018)). My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs). Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution. --- After rebuttal --- Still not convinced of the value of the work to the community. Will keep my score the same.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer 2 , thank you for your constructive feedback and for taking the time to review our work . Your specific points are addressed below . > \u201c The results are not strong . And , unfortunately , the model contribution currently is too modest. \u201d Indeed , the model is a minor contribution and , especially in light of a more thorough evaluation of RGCN , the sum attention RGAT results do not improve on those in Schlichtkrull et al . ( 2017 ) .However , we would like to highlight that despite being a simple modification , the implementation of such a model that is trainable in a reasonable time frame is non-trivial . We plan to make our implementation public to aid future research in the area . To generalise the model , following the recommendations in one of the comments , we have investigated applying the Transformer-style dot product attention that was presented in GaAN : Gated Attention Networks for Learning on Large and Spatiotemporal Graphs ( Zhang et al.2018 ) .We believe that this additional investigation , accompanied by marginally improved results on Tox21 for dot product attention ( Mean test AUCs : WIRGAT 0.838 +/- 0.007 , ARGAT 0.837 +/- 0.007 ) enhance model contributions of the paper . We also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs . > \u201c We should see the performance on other datasets ( e.g. , some of the other datasets in Wu et al . ( 2018 ) \u201d We agree that including experiments on the other data sets presented in Wu et al . ( 2018 ) would be a valuable addition to the paper . Unfortunately , we are unable to perform the required thorough hyperparameter exploration required to draw meaningful conclusions within the remaining time of the rebuttal period . This is something we will investigate in the future . > \u201c My introduction suggestion : do not talk about Convolutional neural networks ( CNNs ) . \u201d Thank you for this stylistic critique . We see how the approach taken did not provide an intuition about the problem as well as it could have . We agree with your point regarding the wealth of graph neural network studies . We feel that the field of geometric deep learning , from which part of the direction of this work originated , is important to keep as part of the development of graph-based machine learning models . Some recent publications in the area of graph based ML have put less emphasis on geometric deep learning , the generalisations of convolutions from grids to graph , and the modifications of convolutions to achieve non-basis dependent methods . We felt that it is important to keep these concepts associated with the field of graph based ML . This introduction could , however , talk less in detail about CNNs themselves , and deal more with graphs - the main focus of the paper . We will produce a reworked introduction where graphs play a larger role . This should provide a more intuitive introduction to our work , whilst maintaining cornerstone concepts ."}, "1": {"review_id": "Bklzkh0qFm-1", "review_text": "This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I don\u2019t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are \u201calmost\u201d identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer 3 , thank you for taking time to read and review our paper and for your useful comments . Hopefully the new results in our response will better aid discussion . Your specific points are addressed below . > i ) \u201c The proposed architecture is mainly adopted from the graph attention networks ( Veli\u010dkovi\u0107 et al.2017 ) and the relational graph networks ( Schlichtkrull et al.2018 ) .Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. \u201d We concede that the modifications to the existing models is a minor contribution . We would like to highlight that despite being a simple modification , the implementation of such a model that is trainable in a reasonable time frame is non-trivial . We plan to make our implementation public to aid research in the area . To generalise the model , following the recommendations in one of the comments , we have investigated applying the Transformer-style dot product attention that was presented in GaAN : Gated Attention Networks for Learning on Large and Spatiotemporal Graphs ( Zhang et al.2018 ) for non-relational graphs . We believe that this additional investigation , accompanied by marginally improved results on Tox21 for dot product attention ( Mean test AUCs : WIRGAT 0.838 +/- 0.007 , ARGAT 0.837 +/- 0.007 ) enhance the paper \u2019 s contribution . > ii ) \u201c In table 2 , I don \u2019 t really see any promising results compared to baselines . There are little improvements over the baselines or even significantly worse. \u201d We agree that the results do not improve on those in Schlichtkrull et al . ( 2017 ) or Wu et al . ( 2017 ) in a significant way . This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT . These results will be included in the new manuscript . We also feel that some of the results being \u201c significantly worse \u201d is one of the main contributions of our paper . Specifically , the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart . This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [ a statement along these lines was made in Schlichtkrull et al . ( 2017 ) ] .The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem . On the other hand , the newly evaluated dot-product attention does no worse ( or better ) than RGCN , indicating a more promising research direction to pursue . > iii ) \u201c Could you explain why your MUTAG is now a single graph and is cast as node classification problem ? \u201d The MUTAG dataset in its standard form from e.g.TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description . There is an alternative dataset version of mutag distributed with dl-learner [ 1 ] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [ 2 ] . In this scenario , MUTAG is in presented in the RDF , where each of the 340 presented complex molecules is described by the links in an RDF with edges ( triples ) of the form d1 - > hasAtom - > d1_1 d1 - > hasBond - > bond1 d1- > hasStructure - > ring_size_6-1 where in this case , d1 is the complex molecule , d1_1 is the first atom in complex molecule d1 , and bond1 is the first bond in complex molecule d1 , and so on . There are many more types than this , and are viewable in the .owl located at [ 2 ] . Nodes correspond to entities from the point of view of RDF . Each complex molecule is given a label according to whether it is mutagenic or not ( determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation ) . This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task ( nodes corresponding to anything other than the complex molecule itself are unlabelled ) . Each molecule can be viewed as a separate graph , or the RDF can be viewed as a single graph comprising of a description of all molecules . If we train on batches of separate graphs and can fit all of the molecule graphs in memory , then these two are the same from the point of view of the training objective . [ 1 ] https : //github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [ 2 ] https : //www.semanticscholar.org/paper/RDF2Vec % 3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752"}, "2": {"review_id": "Bklzkh0qFm-2", "review_text": "The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer 1 , thank you for taking time to read and review our paper and for your useful comments . Hopefully the new results in our response will better aid discussion . Your specific points are addressed below . > \u201c ... the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution . We would like to highlight that despite being a simple modification , producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details . We plan to make our code public to aid research in the area . To generalise the model , following the recommendations in one of the comments , we have investigated applying the Transformer-style dot product attention that was presented in GaAN ( Zhang at al.2018 https : //arxiv.org/abs/1803.07294 ) . This generalises the notion of RGAT , and we believe that this increase the contributions in terms of model modification that our paper offers . > \u201c ... and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to ( e.g.this method builds heavily on Velickovic et al 2017 ) . \u201d Unfortunately we can not directly compare our approach to Velickovic et al . ( 2017 ) on relational graphs since their proposed model doesn \u2019 t support relationship types . Hence , we only compare to Schlichtkrull et al . ( 2017 ) and the conventional baselines , whereas Velickovic et al ( 2017 ) reports results on the non-relational graphs Cora , Citeseer , Pubmed and PPI . In the case of the RDF tasks , our model hyperparameter search space does include a model very similar to vanilla GAT . In the case where we only have both one basis kernel for each of the convolution and the attention , i.e.where B_V and B_v in equation ( 8 ) are set to 1 , then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d . During our hyperparameter search , however , no favourable points for evaluation set performance were discovered with basis sizes lower than 10 ( a basis size of 5 is permitted in the search ) . This leads us to conclude that vanilla GAT would not perform well on the RDF tasks . As mentioned above , we have now also evaluated the dot-product style attention of the transformer and have included it in our results . As one of the public comments mentioned , a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks ( Li et al.2015 ) would be extremely worthwhile . We we feel that evaluation lies outside of the scope of this work , however , which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour . > \u201c ... the results achieved in the experiments are very small improvements compared to the baseline of RGCN\u2026 \u201d We agree that any improvements compared to RGCN are marginal . In light of your third comment ( below ) regarding hyperparameters , the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT . The new dot product attention results on Tox21 ( Mean test AUCs : WIRGAT 0.838 +/- 0.007 , ARGAT 0.837 +/- 0.007 ) are slightly improved compared those of the sum attention , however , due to the retrained RGCN baseline , the relative gap between the best performing RGAT and RGCN is now smaller than it was before . We also see value in reporting these negative results . It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [ a statement along these lines was made in Schlichtkrull et al . ( 2017 ) ] .The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem . On the other hand , the newly evaluated dot-product attention does no worse ( or better ) than RGCN , indicating a more promising research direction to pursue . > \u201c ... often these small variations in results can be compensated with better baselines training\u2026 \u201d We also suspected this was a possibility , and in the same of sum-style attention it turns out to be true . To determine whether this was the case , we performed the same hyperparameter optimisations to our implementation of RGCN . In the case of the RDF datasets AIFB and MUTAG , we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al . ( 2017 ) .On the other hand , our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al , 2017 ti match the performance of the sum-style attention mechanism RGAT performance . This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism , however , although this is not significant as discussed above ."}}