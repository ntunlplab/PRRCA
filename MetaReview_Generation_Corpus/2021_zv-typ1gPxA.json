{"year": "2021", "forum": "zv-typ1gPxA", "title": "Retrieval-Augmented Generation for Code Summarization via Hybrid GNN", "decision": "Accept (Spotlight)", "meta_review": "This paper proposes an interesting method for combining retrieval-based models and graph neural networks for source code summarization. Finding new ways of bringing in additional context for graph-based models is an important research direction in this space, and the paper presents a novel and effective approach. The initial submission was missing experiments on existing benchmarks, but new experiments presented in the discussion phase are enough to resolve that concern. Reviewers are unanimously in support of acceptance. ", "reviews": [{"review_id": "zv-typ1gPxA-0", "review_text": "Overview : The authors tackle the code summarization problem . They retrieve the most similar code and use it as additional input features . They also using GNN to get features from static and dynamic graphs . They evaluate their results on their collected C projects ( C-Code-Summarization Benchmark ) with 1-2 % improvement on automatic evaluation . Reasons to accept : * They collect and release a new challenging C benchmark for code summarization . * They propose a hybrid-GNN solution to capture global graph information . Reasons to reject : * No evaluation on any publicly available datasets . Even though there are some issues about duplication , I will still expect to see such a comparison with other baselines . * Attention-based dynamic message passing of graph model is not proposed in this paper , and I do n't think it is necessary to have this `` hybrid '' design , maybe only dynamic one is enough . As shown in Table 1 , the dynamic is more important than static ( Although we do see an overall performance , except the out-of-domain meteor score , using both are still better , but it is marginal ) . Questions & Suggestions : * Can you provide more dataset information ? For example , the average lines of the code , the average length of the natural language summarization . It seems to me from the examples that each example only has few lines of code and the summarization is very short , it is more like a topic modeling task . * To my understanding , this work is not the first work combining retrieval solution with generation model for code summarization ( e.g. , Retrieval-based Neural Source Code Summarization ) so please modify some of the claims in the paper . * Can you provide more details about the human evaluation ? What is the agreement value ? * `` z is the similarity score , which is introduced to weaken the negative impact of c\u2032 on the original training data c '' , how do you find the best c ? * When you run baselines on your dataset , did you do a hyper-parameter search or just use their default setting ( especially the Rencos model ) ? * There are some retrieved-augment language models in the NLP field that the authors may want to take a look and compare with , for example , `` Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks '' . * Did you run any baselines that are based on pre-trained language model , such as BERT , BART , T5 , or even more code related like CodeBERT : A Pre-Trained Model for Programming and Natural Languages ?", "rating": "7: Good paper, accept", "reply_text": "Please refer to A1 in the common responses for the evaluation on a public dataset . Besides , we thank the reviewer on other detailed comments and we addressed these comments as follows : Q1 : Attention-based dynamic message passing of graph model is not proposed in this paper , and I do n't think it is necessary to have this `` hybrid '' design , maybe only dynamic one is enough . As shown in Table 1 , the dynamic is more important than static ( Although we do see an overall performance , except the out-of-domain meteor score , using both are still better , but it is marginal ) . A1 : We agree that the idea of using the dynamic graph with GNNs is not our contribution since prior works have explored this idea as well . Our contributions mainly include : 1 ) the retrieval-based augmentation for generation models and 2 ) the Hybrid GNN leveraging both static and dynamic graphs . We will make the statement on contributions more clear in the revision . We also added more ablation study results for the retrieval-based augmentation . Please refer to A2 in the common response . As for the Hybrid GNN , although overall the dynamic graph performs better than the static graph in our experiments , we think the static graph is still needed and useful : 1 ) Considering the complexity of this task and size of the testing set ( 6388 samples in CCSD and 21028 in PCSD ) , we think the performance improvement of HGNN ( i.e. , using both static and dynamic graphs ) compared with HGNN w/o static and HGNN w/o dynamic is still promising . 2 ) There are still some results showing that the static graph achieves better performance than the dynamic graph . Please see the results a ) BLUE-4 values ( i.e. , 12.00 vs 11.87 ) between ( HGNN w/o augment & dynamic ) and ( HGNN w/o augment & static ) and b ) The METEOR results in the new added python results in Q1 of common response , i.e. , the METEOR values of HGNN w/o dynamic and HGNN w/o static are 18.42 and 18.36 , respectively . 3 ) Moreover , we think GNN with hybrid static and dynamic graphs is a promising idea in general , and we believe researchers working on other domain applications might find it interesting and helpful . Q2 : Can you provide more dataset information ? For example , the average lines of the code , the average length of the natural language summarization . It seems to me from the examples that each example only has few lines of code and the summarization is very short , it is more like a topic modeling task . A2 : For our C dataset , the average lines of the code are 12.59 and the average token length of summary is 8.22 . For the new added dataset PCSD , the average lines of the code are 14.24 and the average token length is 10.91 . Although the code summarization task and the topic modeling task share some kind of similarity , we think they are still quite different . First of all , the summarization task usually aims to provide a more fine-grained description of the input data while topic modeling aims to provide some high-level description ( e.g. , keywords ) of the themes of input data . Secondly , different from topic modeling which usually takes as input free-form text and outputs a few keywords to describe its themes , the key challenge of code summarization compared to regular text summarization is that the input ( i.e. , source code ) and the expected output ( i.e. , summary ) are from two very different domains , i.e. , they are heterogeneous data . To solve this challenge , the state-of-the-art techniques adopt the deep neural networks to learn the code semantics and generate the summary . Our work follows this line of research and proposes a novel method , i.e. , the retrieval-augmented hybrid GNN . Q3 : To my understanding , this work is not the first work combining retrieval solution with generation model for code summarization ( e.g. , Retrieval-based Neural Source Code Summarization ) so please modify some of the claims in the paper . A3 : Thanks for the suggestions ! We agree that this is not the first work that proposes the retrieval-generation method for code summarization . And we did cite the Retrieval-based Neural Source Code Summarization work ( which is the Rencos baseline in our experiments ) in our paper . In fact , different from Rencos , which feeds the combination of the retrieved code and the test code to a seq2seq model , we propose a novel retrieval augment mechanism to employ the similar code and its summary for model training and encode more program semantics with GNN for the summary generation . We will make our claim more clear and add more discussions on the differences in the revision ."}, {"review_id": "zv-typ1gPxA-1", "review_text": "Summary This paper proposes a retrieval-augmented method for generating code summarization . The model encodes the input code based on its graph structure ( Code Property Graph ) with a hybrid GNN architecture . The model augments the initial graph representation of the input code based on the representation of the top-1 retrieval result . It also augments the final graph encoding with the BiLSTM encoding of the retrieved summary . The proposed model is evaluated on a newly curated C code summarization data set and shows state of the art performance compared against previous systems . Strengths - Releases a new C code summarization data , which will be beneficial for the community . - This paper reports human evaluation results . - Ablation study shows the retrieval augmentation and the new hybrid GNN architecture is helpful . Weaknesses - The model is not evaluated on any existing code summarization benchmarks . Showing that the proposed architecture is generally applicable by getting good results on more benchmarks will make the story a lot more convincing . - Could include more analysis ( see below for details ) Other questions/comments - Would be nice to provide the mathematical formulation of \u201c Step 3 : Retrieved Summary-based Augmentation \u201d - Would it be possible to ablate code-based summarization vs. summary-based augmentation separately ? It would be interesting to see their relative impact . - Suggestion : it would be helpful to provide an input-output example earlier in the paper . - Suggestion : would be nice to include real examples of retrieval results in the analysis section . - Have you considered using top-k retrieval results instead of top-1 ?", "rating": "7: Good paper, accept", "reply_text": "Please see the evaluation results on other dataset and the ablation study on Retrieval-based Augmentation in the common response . Other questions and comments : We thank the reviewer for the detailed comments again and we address them in the revision as follows : Q1 : Would be nice to provide the mathematical formulation of \u201c Step 3 : Retrieved Summary-based Augmentation \u201d A1 : We provide the formula as follows in the revision : We further encode the retrieved summary $ s ' $ with another BiLSTM model . We represent each token $ t'_i $ of $ s ' $ using the learned embedding matrix $ \\boldsymbol E^ { seqtoken } $ . Then $ s ' $ can be encoded as : \\begin { equation } \\boldsymbol h_ { t_1 ' } , ... , \\boldsymbol h_ { t_T ' } = \\mathrm { BiLSTM } ( E^ { seqtoken } _ { t_1 ' } , ... , E^ { seqtoken } _ { t_T ' } ) \\end { equation } where $ h_ { t'_i } $ is the state of the BiLSTM model for the token $ t_i ' $ in $ s ' $ and $ T $ is the length of $ s ' $ . We also multiply the similarity score $ z $ to $ [ \\boldsymbol h_ { t_1 ' } , ... , \\boldsymbol h_ { t_T ' } ] $ and concatenate with the graph encoding results ( i.e. , the outputs of the GNN encoder ) as the input $ [ \\mathrm { GNN } , z \\boldsymbol h_ { t_1 ' } , ... , z \\boldsymbol h_ { t_T ' } ] $ to the decoder . Q2 : Suggestion : it would be helpful to provide an input-output example earlier in the paper . A2 : We will add an illustrative example ( including the input and the expected summary results ) in our paper such that the readers could better understand our method . Q3 : Suggestion : would be nice to include real examples of retrieval results in the analysis section . A3 : We will follow the suggestion and add concrete cases for illustrating the retrieval results in our revision . For example , one concrete example ( Example 2 in Table 3 ) is shown as follows : Input code : void ReleaseCedar ( CEDAR * c ) { if ( c == NULL ) { return ; } if ( Release ( c- > ref ) == 0 ) { CleanupCedar ( c ) ; } } Ground-Truth : release reference of the cedar . Retrieved code : void DelConnection ( CEDAR * cedar , CONNECTION * c ) { if ( cedar == NULL || c == NULL ) { return ; } LockList ( cedar- > ConnectionList ) ; { Debug ( `` Connection % s Deleted from Cedar.\\n '' , c- > Name ) ; if ( Delete ( cedar- > ConnectionList , c ) ) { ReleaseConnection ( c ) ; } } UnlockList ( cedar- > ConnectionList ) ; } Retrieved summary : delete connection from cedar . Our result ( HGNN ) : release reference of cedar . We conjecture that the reason our method can generate a high-quality summary for this example could probably be that : With the retrieved summary and code , our method might be able to learn the mapping pattern between the method name and summary . For example , DelConnection ( CEDAR * cedar , CONNECTION * c ) - > \u201c delete connection from cedar \u201c . Then for this test example ReleaseCedar ( CEDAR * c ) , our method might learn to leverage this pattern and get the correct result . Q4 : Have you considered using top-k retrieval results instead of top-1 ? A4 : Thanks for this great suggestion ! In fact , we also explored the top-2 and top-3 retrieval results with our method . However , there was no significant performance boost when more retrieval results were used . Moreover , more GPU resources were needed for expensive training . Thus , we only consider the top-1 retrieval result in this work . We will add more discussion on the effect of different top-k retrieval results in the revision . And we will leave how to effectively utilize top-k retrieval results in our hybrid framework as future work ."}, {"review_id": "zv-typ1gPxA-2", "review_text": "Summary : This paper leverages similar code-summary pairs from existing data to assist code summary generation . The model first retrieves a similar code snippet from the existing database . Then , the author applied GNN over the code property graphs ( CPGs ) . A challenge is that CPGs are typically deep therefore it is difficult to capture long dependencies . The author proposed an attention mechanism to capture global information between nodes , and then a hybrid GNN layer encodes the retrieve-augmented graph . Finally , a generator takes both GNN 's output and the retrieved text summary and predict outputs . Experimental results over a new C code indicates that the proposed method outperforms both IR and neural generation methods . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reason for score : Overall , I vote for accepting . Both the idea of leveraging existing code and also the adaptive layer to capture long dependencies are interesting and the experiments look solid . Although I would still like to see the results from previous existing datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Some comments about the experiments : a . As an application study , it is still necessary to compare the model over previous benchmarks , even though there are some issues with those datasets . b.A pair of missing ablation studies are : a generator still takes the text summary of retrieved code , but not use the augmented graph ; and vice versa , the generator only takes the graph information but not the retrieved text summary . This can further indicate which part of the retrieved information is more useful .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer again for the useful comments . Please refer to A1 and A2 in the common responses for the evaluation results on a public dataset and the ablation study with only code-based augmentation and only summary-based augmentation ."}], "0": {"review_id": "zv-typ1gPxA-0", "review_text": "Overview : The authors tackle the code summarization problem . They retrieve the most similar code and use it as additional input features . They also using GNN to get features from static and dynamic graphs . They evaluate their results on their collected C projects ( C-Code-Summarization Benchmark ) with 1-2 % improvement on automatic evaluation . Reasons to accept : * They collect and release a new challenging C benchmark for code summarization . * They propose a hybrid-GNN solution to capture global graph information . Reasons to reject : * No evaluation on any publicly available datasets . Even though there are some issues about duplication , I will still expect to see such a comparison with other baselines . * Attention-based dynamic message passing of graph model is not proposed in this paper , and I do n't think it is necessary to have this `` hybrid '' design , maybe only dynamic one is enough . As shown in Table 1 , the dynamic is more important than static ( Although we do see an overall performance , except the out-of-domain meteor score , using both are still better , but it is marginal ) . Questions & Suggestions : * Can you provide more dataset information ? For example , the average lines of the code , the average length of the natural language summarization . It seems to me from the examples that each example only has few lines of code and the summarization is very short , it is more like a topic modeling task . * To my understanding , this work is not the first work combining retrieval solution with generation model for code summarization ( e.g. , Retrieval-based Neural Source Code Summarization ) so please modify some of the claims in the paper . * Can you provide more details about the human evaluation ? What is the agreement value ? * `` z is the similarity score , which is introduced to weaken the negative impact of c\u2032 on the original training data c '' , how do you find the best c ? * When you run baselines on your dataset , did you do a hyper-parameter search or just use their default setting ( especially the Rencos model ) ? * There are some retrieved-augment language models in the NLP field that the authors may want to take a look and compare with , for example , `` Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks '' . * Did you run any baselines that are based on pre-trained language model , such as BERT , BART , T5 , or even more code related like CodeBERT : A Pre-Trained Model for Programming and Natural Languages ?", "rating": "7: Good paper, accept", "reply_text": "Please refer to A1 in the common responses for the evaluation on a public dataset . Besides , we thank the reviewer on other detailed comments and we addressed these comments as follows : Q1 : Attention-based dynamic message passing of graph model is not proposed in this paper , and I do n't think it is necessary to have this `` hybrid '' design , maybe only dynamic one is enough . As shown in Table 1 , the dynamic is more important than static ( Although we do see an overall performance , except the out-of-domain meteor score , using both are still better , but it is marginal ) . A1 : We agree that the idea of using the dynamic graph with GNNs is not our contribution since prior works have explored this idea as well . Our contributions mainly include : 1 ) the retrieval-based augmentation for generation models and 2 ) the Hybrid GNN leveraging both static and dynamic graphs . We will make the statement on contributions more clear in the revision . We also added more ablation study results for the retrieval-based augmentation . Please refer to A2 in the common response . As for the Hybrid GNN , although overall the dynamic graph performs better than the static graph in our experiments , we think the static graph is still needed and useful : 1 ) Considering the complexity of this task and size of the testing set ( 6388 samples in CCSD and 21028 in PCSD ) , we think the performance improvement of HGNN ( i.e. , using both static and dynamic graphs ) compared with HGNN w/o static and HGNN w/o dynamic is still promising . 2 ) There are still some results showing that the static graph achieves better performance than the dynamic graph . Please see the results a ) BLUE-4 values ( i.e. , 12.00 vs 11.87 ) between ( HGNN w/o augment & dynamic ) and ( HGNN w/o augment & static ) and b ) The METEOR results in the new added python results in Q1 of common response , i.e. , the METEOR values of HGNN w/o dynamic and HGNN w/o static are 18.42 and 18.36 , respectively . 3 ) Moreover , we think GNN with hybrid static and dynamic graphs is a promising idea in general , and we believe researchers working on other domain applications might find it interesting and helpful . Q2 : Can you provide more dataset information ? For example , the average lines of the code , the average length of the natural language summarization . It seems to me from the examples that each example only has few lines of code and the summarization is very short , it is more like a topic modeling task . A2 : For our C dataset , the average lines of the code are 12.59 and the average token length of summary is 8.22 . For the new added dataset PCSD , the average lines of the code are 14.24 and the average token length is 10.91 . Although the code summarization task and the topic modeling task share some kind of similarity , we think they are still quite different . First of all , the summarization task usually aims to provide a more fine-grained description of the input data while topic modeling aims to provide some high-level description ( e.g. , keywords ) of the themes of input data . Secondly , different from topic modeling which usually takes as input free-form text and outputs a few keywords to describe its themes , the key challenge of code summarization compared to regular text summarization is that the input ( i.e. , source code ) and the expected output ( i.e. , summary ) are from two very different domains , i.e. , they are heterogeneous data . To solve this challenge , the state-of-the-art techniques adopt the deep neural networks to learn the code semantics and generate the summary . Our work follows this line of research and proposes a novel method , i.e. , the retrieval-augmented hybrid GNN . Q3 : To my understanding , this work is not the first work combining retrieval solution with generation model for code summarization ( e.g. , Retrieval-based Neural Source Code Summarization ) so please modify some of the claims in the paper . A3 : Thanks for the suggestions ! We agree that this is not the first work that proposes the retrieval-generation method for code summarization . And we did cite the Retrieval-based Neural Source Code Summarization work ( which is the Rencos baseline in our experiments ) in our paper . In fact , different from Rencos , which feeds the combination of the retrieved code and the test code to a seq2seq model , we propose a novel retrieval augment mechanism to employ the similar code and its summary for model training and encode more program semantics with GNN for the summary generation . We will make our claim more clear and add more discussions on the differences in the revision ."}, "1": {"review_id": "zv-typ1gPxA-1", "review_text": "Summary This paper proposes a retrieval-augmented method for generating code summarization . The model encodes the input code based on its graph structure ( Code Property Graph ) with a hybrid GNN architecture . The model augments the initial graph representation of the input code based on the representation of the top-1 retrieval result . It also augments the final graph encoding with the BiLSTM encoding of the retrieved summary . The proposed model is evaluated on a newly curated C code summarization data set and shows state of the art performance compared against previous systems . Strengths - Releases a new C code summarization data , which will be beneficial for the community . - This paper reports human evaluation results . - Ablation study shows the retrieval augmentation and the new hybrid GNN architecture is helpful . Weaknesses - The model is not evaluated on any existing code summarization benchmarks . Showing that the proposed architecture is generally applicable by getting good results on more benchmarks will make the story a lot more convincing . - Could include more analysis ( see below for details ) Other questions/comments - Would be nice to provide the mathematical formulation of \u201c Step 3 : Retrieved Summary-based Augmentation \u201d - Would it be possible to ablate code-based summarization vs. summary-based augmentation separately ? It would be interesting to see their relative impact . - Suggestion : it would be helpful to provide an input-output example earlier in the paper . - Suggestion : would be nice to include real examples of retrieval results in the analysis section . - Have you considered using top-k retrieval results instead of top-1 ?", "rating": "7: Good paper, accept", "reply_text": "Please see the evaluation results on other dataset and the ablation study on Retrieval-based Augmentation in the common response . Other questions and comments : We thank the reviewer for the detailed comments again and we address them in the revision as follows : Q1 : Would be nice to provide the mathematical formulation of \u201c Step 3 : Retrieved Summary-based Augmentation \u201d A1 : We provide the formula as follows in the revision : We further encode the retrieved summary $ s ' $ with another BiLSTM model . We represent each token $ t'_i $ of $ s ' $ using the learned embedding matrix $ \\boldsymbol E^ { seqtoken } $ . Then $ s ' $ can be encoded as : \\begin { equation } \\boldsymbol h_ { t_1 ' } , ... , \\boldsymbol h_ { t_T ' } = \\mathrm { BiLSTM } ( E^ { seqtoken } _ { t_1 ' } , ... , E^ { seqtoken } _ { t_T ' } ) \\end { equation } where $ h_ { t'_i } $ is the state of the BiLSTM model for the token $ t_i ' $ in $ s ' $ and $ T $ is the length of $ s ' $ . We also multiply the similarity score $ z $ to $ [ \\boldsymbol h_ { t_1 ' } , ... , \\boldsymbol h_ { t_T ' } ] $ and concatenate with the graph encoding results ( i.e. , the outputs of the GNN encoder ) as the input $ [ \\mathrm { GNN } , z \\boldsymbol h_ { t_1 ' } , ... , z \\boldsymbol h_ { t_T ' } ] $ to the decoder . Q2 : Suggestion : it would be helpful to provide an input-output example earlier in the paper . A2 : We will add an illustrative example ( including the input and the expected summary results ) in our paper such that the readers could better understand our method . Q3 : Suggestion : would be nice to include real examples of retrieval results in the analysis section . A3 : We will follow the suggestion and add concrete cases for illustrating the retrieval results in our revision . For example , one concrete example ( Example 2 in Table 3 ) is shown as follows : Input code : void ReleaseCedar ( CEDAR * c ) { if ( c == NULL ) { return ; } if ( Release ( c- > ref ) == 0 ) { CleanupCedar ( c ) ; } } Ground-Truth : release reference of the cedar . Retrieved code : void DelConnection ( CEDAR * cedar , CONNECTION * c ) { if ( cedar == NULL || c == NULL ) { return ; } LockList ( cedar- > ConnectionList ) ; { Debug ( `` Connection % s Deleted from Cedar.\\n '' , c- > Name ) ; if ( Delete ( cedar- > ConnectionList , c ) ) { ReleaseConnection ( c ) ; } } UnlockList ( cedar- > ConnectionList ) ; } Retrieved summary : delete connection from cedar . Our result ( HGNN ) : release reference of cedar . We conjecture that the reason our method can generate a high-quality summary for this example could probably be that : With the retrieved summary and code , our method might be able to learn the mapping pattern between the method name and summary . For example , DelConnection ( CEDAR * cedar , CONNECTION * c ) - > \u201c delete connection from cedar \u201c . Then for this test example ReleaseCedar ( CEDAR * c ) , our method might learn to leverage this pattern and get the correct result . Q4 : Have you considered using top-k retrieval results instead of top-1 ? A4 : Thanks for this great suggestion ! In fact , we also explored the top-2 and top-3 retrieval results with our method . However , there was no significant performance boost when more retrieval results were used . Moreover , more GPU resources were needed for expensive training . Thus , we only consider the top-1 retrieval result in this work . We will add more discussion on the effect of different top-k retrieval results in the revision . And we will leave how to effectively utilize top-k retrieval results in our hybrid framework as future work ."}, "2": {"review_id": "zv-typ1gPxA-2", "review_text": "Summary : This paper leverages similar code-summary pairs from existing data to assist code summary generation . The model first retrieves a similar code snippet from the existing database . Then , the author applied GNN over the code property graphs ( CPGs ) . A challenge is that CPGs are typically deep therefore it is difficult to capture long dependencies . The author proposed an attention mechanism to capture global information between nodes , and then a hybrid GNN layer encodes the retrieve-augmented graph . Finally , a generator takes both GNN 's output and the retrieved text summary and predict outputs . Experimental results over a new C code indicates that the proposed method outperforms both IR and neural generation methods . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reason for score : Overall , I vote for accepting . Both the idea of leveraging existing code and also the adaptive layer to capture long dependencies are interesting and the experiments look solid . Although I would still like to see the results from previous existing datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Some comments about the experiments : a . As an application study , it is still necessary to compare the model over previous benchmarks , even though there are some issues with those datasets . b.A pair of missing ablation studies are : a generator still takes the text summary of retrieved code , but not use the augmented graph ; and vice versa , the generator only takes the graph information but not the retrieved text summary . This can further indicate which part of the retrieved information is more useful .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer again for the useful comments . Please refer to A1 and A2 in the common responses for the evaluation results on a public dataset and the ablation study with only code-based augmentation and only summary-based augmentation ."}}