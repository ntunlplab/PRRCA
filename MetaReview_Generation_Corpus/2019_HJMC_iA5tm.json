{"year": "2019", "forum": "HJMC_iA5tm", "title": "Learning a SAT Solver from Single-Bit Supervision", "decision": "Accept (Poster)", "meta_review": "The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable.  The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training.\n\nAlthough the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re-introduction of SAT in a machine learning context using deep networks.  It may be nice to mention e.g. (W. Ruml. Adaptive Tree Search. PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems.  The empirical validation on variable sized problems, etc. is a nice contribution showing interesting generalization properties of the proposed approach.\n\nThe reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting.", "reviews": [{"review_id": "HJMC_iA5tm-0", "review_text": "The paper describes a general neural network architecture for predicting satisfiability. Specifically, the contributions include an encoding for SAT problems, and predicting SAT using a message passing method, where the embeddings for literals and clauses are iteratively changed until convergence. The paper seems significant considering that it brings together SAT solving and neural network architectures. The paper is very clearly written and quite precise about its contributions. The analysis especially figures 3,4, and 7 seems to give a nice intuitive ideas as to what the neural network is trying to do. However, one weakness is that the problems are run on a specific type of SAT problem the authors have created. Of course, the authors make it clear that the objective is not really to create a. State-of-the-art solver but rather to understand what a neural network trying to do SAT solving is capable of doing. On this front, I think the paper succeeds in doing this. One thing that was a little confusing is that why should all the literals turn to SAT (turn red) to prove SAT (as it is shown in figure 3). Is it that the neural network does not realize that it has found a SAT solution with a smaller subset of SAT literals. In other words, is it not capable of taking advantage of the problem structure. In general though, this seemed to be an interesting paper though its practical implications are quite hard to know either in the SAT community or in the neural network community.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . > One thing that was a little confusing is that why should all the literals turn to SAT ( turn red ) to prove SAT ( as it is shown in figure 3 ) . Is it that the neural network does not realize that it has found a SAT solution with a smaller subset of SAT literals . In other words , is it not capable of taking advantage of the problem structure . Remember that the network is only trained to make the * mean * vote of the literals large on satisfiable problems and small ( i.e.large and negative ) on unsatisfiable problems . Thus on satisfiable problems it has a strong incentive to make all the literals vote _sat_ instead of only half of them ."}, {"review_id": "HJMC_iA5tm-1", "review_text": "This paper presents the NeuroSAT architecture, which uses a deep, message passing neural net for predicting the satisfiability of CNF instances. The architecture is also able to predict a satisfiable assignment in the SAT case, and the literals involved in some minimal conflicting set of clauses (i.e. core) in the UNSAT case. The NeuroSAT architecture is based on a vector space embedding of literals and clauses, which exploits (with message passing) some important symmetries of SAT instances (permutation invariance and negation invariance). This architecture is tested on various classes of random SAT instances, involving both unstructured (RS) problems, and structured ones (e.g. graph colorings, vertex covers, dominating sets, etc.). Overall the paper is well-motivated, and the experimental results are quite convincing. Arguably, the salient characteristic of NeuroSAT is to iteratively refine the confidence of literals voting for the SAT - or UNSAT - output, using a voting scheme on the last iteration of the literal matrix. This is very interesting, and NeuroSAT might be used to help existing solvers in choosing variable orderings for tackling hard instances, or hard queries (e.g. find a core). On the other hand, the technical description of the architecture (sec. 3) is perhaps a little vague for having a clear intuition of how the classification task - for SAT instances - is handled in the NeuroSAT architecture. Namely, a brief description of the initial matrices (which encode the literal en clause embeddings) would be nice. Some comments on the role played by the multilayer perceptron units and the normalization units would also be welcome. The two update rules in Page 3 could be explained in more detail. For the sake of clarity, I would suggest to provide a figure for depicting a transition (from iteration t-1 to iteration t) in the architecture. As a minor comment, it would be nice (in Section 2) to define the main parameters $n$, $m$, and $d$ used in the rest of the paper. Concerning the experimental part of the paper, Sections 4 & 5 are well-explained but, in Section 6, the solution decoding method, inspired from PCA is a bit confusing. Specifically, we don\u2019t know how a satisfying assignment is extracted from the last layer, and this should be explained in detail. According to Figure 4 and the comments above, it seems that a clustering method (with two centroids) is advocated, but this is not clear. In Table 1, the correlation between the accuracy on SAT instances, and the percent of SAT instances solved is not clear. Is the ratio of 70% measured on the CNF instances which have been predicted to be satisfiable? Or, is this ratio measured on the whole set of test instances? Finally, for the results established in Table 1, how many training instances and test instances have been used? In Section 7, some important aspects related to experiments, are missing. In Sec 7.1, for SR(200) tasks, was NeuroSAT tested on the same conditions as those for SR(40) tasks? Notably, what is the input dimension $d$ of the embedding space here? (I guess that $d = 128$ is too small for such large instances). Also, how many training and test instances have been used to plot the curves in Figure 5? For the 4,888 satisfiable instances generated in Sec. 7.2, which solver have been used to determine the satisfiability of those instances (I guess it is Minisat, but this should be mentioned somewhere). In Section 8, I found interesting the the ability of NeuroSAT in predicting the literals that participate in an UNSAT core. Indeed the problem of finding an UNSAT core in CNF instances is computationally harder than determining the satisfiability of this instance. So, NeuroSAT might be used here to help a solver in finding a core. But the notion of \u201cconfidence\u201d should be explained in more detail in this section, and more generally, in the whole paper. Namely, it seems that in the last layer of each iteration, literals are voting for SAT (red colors) with some confidence (say $\\delta$) and voting for UNSAT (blue colors) with some confidence (say $\\delta\u2019$). Are $\\delta$ and $\\delta\u2019$ correlated in the neural architecture? And, how confidences for UNSAT votes are updated? Finally, I found that the different benchmarks where relevant, but I would also suggest (for future work, or in the appendix) to additionally perform experiments on the well-known random 3-SAT instances ($k$ is fixed to 3). Here, it is well-known that a phase transition (on the instances, not the solver/learner) occurs at 4.26 for the clause/variable ratio. A plot displaying the performance of NeuroSAT (accuracy in predicting the label of the instance) versus the clause/variable ratio would be very helpful in assessing the robustness of NeuroSAT on the so-called \u201chard\u201d instances (which are close to 4.26). By extension, there have been a lot of recent work in generating \u201cpseudo-industrial\u201d random SAT instances, which incorporate some structure (e.g. communities) in order to mimic real-world structured SAT instances. To this point, it would be interesting to analyze the performance of NeuroSAT on such pseudo-industrial instances. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and questions . > a brief description of the initial matrices ( which encode the literal en clause embeddings ) would be nice . The initial vectors L_init and C_init are simply parameters of the model , that we learn simultaneously with the other parameters . > For the sake of clarity , I would suggest to provide a figure for depicting a transition ( from iteration t-1 to iteration t ) in the architecture . We graphically depict a single iteration in Figure 2 , though it is very high-level . Do you have a particular middle-ground in mind , between the figure we have and the equations themselves ? > As a minor comment , it would be nice ( in Section 2 ) to define the main parameters $ n $ , $ m $ , and $ d $ used in the rest of the paper . We updated S2 to introduce n and m. We can not introduce d there since d only makes sense in the context of the model , which is not discussed until S3 . > Concerning the experimental part of the paper , Sections 4 & 5 are well-explained but , in Section 6 , the solution decoding method , inspired from PCA is a bit confusing . Specifically , we don \u2019 t know how a satisfying assignment is extracted from the last layer , and this should be explained in detail . According to Figure 4 and the comments above , it seems that a clustering method ( with two centroids ) is advocated , but this is not clear Here is the description we give in the paper on the 2-clustering approach : `` 2-cluster $ L^ { ( T ) } $ to get cluster centers $ \\Delta_1 $ and $ \\Delta_2 $ , partition the variables according to the predicate \\ ( \\| x_i - \\Delta_1 \\|^2 + \\| \\flip { x_i } - \\Delta_2 \\|^2 < \\| x_i - \\Delta_2 \\|^2 + \\| \\flip { x_i } - \\Delta_1 \\|^2 \\ ) , and then try both candidate assignments that result from mapping the partitions to truth values . '' If you clarify what you find confusing or missing from this explanation , we will try to improve the explanation in the paper . > In Table 1 , the correlation between the accuracy on SAT instances , and the percent of SAT instances solved is not clear . Is the ratio of 70 % measured on the CNF instances which have been predicted to be satisfiable ? Or , is this ratio measured on the whole set of test instances ? As the caption says , in that experiment we were able to decode a satisfying assignment for 70 % of the satisfiable problems . The satisfiable problems includes the subset of satisfiable problems for which the network incorrectly predicted _unsat_ . To a first approximation , the 70 % number we report means that we could decode solutions for approximately 96 % of the problems correctly predicted to be _sat_ ; however , the 70 % does include a few problems for which the network found a solution but nonetheless incorrectly guessed _unsat_ . We expect this case to happen when the network finds the solution towards the very end of message passing , and does not have enough time to flip all the literal votes . Also note that the percentage of satisfiable problems solved is the metric we actually care about , whereas we only care about classification accuracy for instrumental reasons . > Finally , for the results established in Table 1 , how many training instances and test instances have been used ? It is easy to generate unlimited data from these distributions . We trained on millions of problems , and tested on hundreds of thousands of them . > In Sec 7.1 , for SR ( 200 ) tasks , was NeuroSAT tested on the same conditions as those for SR ( 40 ) tasks ? Notably , what is the input dimension $ d $ of the embedding space here ? ( I guess that $ d = 128 $ is too small for such large instances ) . Once trained , NeuroSAT has learned parameters whose dimensions depend on the hyperparameter $ d $ . It is not possible to run NeuroSAT with a larger $ d $ at test time . For SR ( 200 ) , we use the exact same trained NeuroSAT model as in Table 1 , which was trained only on SR ( U ( 10 , 40 ) ) and has $ d $ = 128 . > For the 4,888 satisfiable instances generated in Sec.7.2 , which solver have been used to determine the satisfiability of those instances ( I guess it is Minisat , but this should be mentioned somewhere ) . Yes , we used Minisat . We updated the paper to mention this , and also updated S4 to make it clear we use Minisat to generate SR ( n ) as well . > But the notion of \u201c confidence \u201d should be explained in more detail in this section , and more generally , in the whole paper . We only use the phrase `` confidence '' informally . The semantics of the literal votes is defined by the network architecture . > Namely , it seems that in the last layer of each iteration , literals are voting for SAT ( red colors ) with some confidence ( say $ \\delta $ ) and voting for UNSAT ( blue colors ) with some confidence ( say $ \\delta \u2019 $ ) . Are $ \\delta $ and $ \\delta \u2019 $ correlated in the neural architecture ? And , how confidences for UNSAT votes are updated ? I am afraid I do not understand what you are asking . Can you please clarify your use of 'correlated ' and 'updated ' in the last two sentences ?"}, {"review_id": "HJMC_iA5tm-2", "review_text": "This paper trains a neural network to solve the satisfiability problems. Based on the message passing neural network, it presents NeuroSAT and trains it as a classifier to predict satisfiability under a single bit of supervision. After training, NeuroSAT can solve problems that are larger and more difficult than it ever saw during training. Furthermore, the authors present a way to decode the solutions from the network's activations. Besides, for unsatisfiable problems, the paper also presents NeuroUNSAT, which learns to detect the contradictions in the form of UNSAT cores. Relevance: this paper is likely to be of interest to a large proportion of the community for several reasons. Firstly, satisfiability problems arise from a variety of domains. This paper starts with a new angle to solve the SAT problem. Secondly, it uses neural networks in the SAT problem and establishes that neural networks can learn to perform a discrete search. Thirdly, the system used in this paper may also help improve existing SAT solvers. Significance: I think the results are significant. For the decoding satisfying assignments section, the two-dimensional PCA embeddings are very clear. And the NeuroSAT's success rate for more significant problems and different problems has shown it's generalization ability. Finally, the sequences of literal votes in NeuroUNSAT have proved its ability to detect unsatisfied cores. Novelty: NeuroSAT\u2019s approach is novel. Based on message passing neural networks, it trains a neural network to learn to solve the SAT problem. Soundness: This paper is technically sound. Evaluation: The experimental section is comprehensive. There are a variety of graphs showing the performance and ability of your architecture. However, the theoretical analysis isn't very sufficient. For instance, why does the change of the dataset from the original SR(n) to SRC(n,u) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores? Clarity: As a whole, the paper is clear. The definition of the problem, the model structure, the data generation, the training procedure, and the evaluation are all well organized. However, there is still a few points requiring more explanation. For instance, in figure 3, I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero, blue negative and red positive. Also, in figure 7, I am not sure whether those black grids represent higher positive values or lower negative values. A few questions: What's the initialization of the two vectors the authors use for tiling operation? Does the initialization differ for different types of SAT problems? How do the authors decide the number of iterations necessary for solving a particular SAT problem? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and questions . > However , the theoretical analysis is n't very sufficient . For instance , why does the change of the dataset from the original SR ( n ) to SRC ( n , u ) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores ? For SRC ( n , u ) , the objective function assigns much lower cost to the parameters that detect the presence of the planted unsat cores than to the parameters that search for satisfying assignments , because unlike the latter , the former allow perfect classification of the dataset in a fixed , small number of steps . Such a simple approach is not an option on SR ( n ) , because the cores are bigger and more diverse . > For instance , in figure 3 , I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero , blue negative and red positive . Also , in figure 7 , I am not sure whether those black grids represent higher positive values or lower negative values . We also write in two places that `` for several iterations , almost every literal is voting \\emph { unsat } with low confidence ( \\ie light blue ) '' . We updated the paper to include two more similar parenthetical notes , one for `` _sat_ with high confidence '' and `` dark red '' , and one for `` _unsat_ with high confidence '' and `` dark blue '' . What you saw as black is just dark blue . > What 's the initialization of the two vectors the authors use for tiling operation ? Does the initialization differ for different types of SAT problems ? It is just the parameters L_init and C_init that are learned by gradient descent at the same time as the other parameters are learned . When a trained NeuroSAT is run on a SAT problem , no matter the size or origin , the same L_init and C_init are used . > How do the authors decide the number of iterations necessary for solving a particular SAT problem ? Since the network usually converges once it finds a solution , one does not need to try to decode solutions after each round of message passing , and instead can run for a predetermined number of rounds and only check at the end . This is a desirable feature since it makes it easy to solve a very large number of SAT problems simultaneously as a single batch ( e.g.on a GPU ) without any problem-specific control flow . As for rules of thumb , Figure 5 provides data on how many iterations it took to solve what percentage of problems in SR ( n ) for a range of n. For the graph problems in S7.2 , we simply ran NeuroSAT for ( the somewhat arbitrary ) 512 iterations on every problem ."}], "0": {"review_id": "HJMC_iA5tm-0", "review_text": "The paper describes a general neural network architecture for predicting satisfiability. Specifically, the contributions include an encoding for SAT problems, and predicting SAT using a message passing method, where the embeddings for literals and clauses are iteratively changed until convergence. The paper seems significant considering that it brings together SAT solving and neural network architectures. The paper is very clearly written and quite precise about its contributions. The analysis especially figures 3,4, and 7 seems to give a nice intuitive ideas as to what the neural network is trying to do. However, one weakness is that the problems are run on a specific type of SAT problem the authors have created. Of course, the authors make it clear that the objective is not really to create a. State-of-the-art solver but rather to understand what a neural network trying to do SAT solving is capable of doing. On this front, I think the paper succeeds in doing this. One thing that was a little confusing is that why should all the literals turn to SAT (turn red) to prove SAT (as it is shown in figure 3). Is it that the neural network does not realize that it has found a SAT solution with a smaller subset of SAT literals. In other words, is it not capable of taking advantage of the problem structure. In general though, this seemed to be an interesting paper though its practical implications are quite hard to know either in the SAT community or in the neural network community.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . > One thing that was a little confusing is that why should all the literals turn to SAT ( turn red ) to prove SAT ( as it is shown in figure 3 ) . Is it that the neural network does not realize that it has found a SAT solution with a smaller subset of SAT literals . In other words , is it not capable of taking advantage of the problem structure . Remember that the network is only trained to make the * mean * vote of the literals large on satisfiable problems and small ( i.e.large and negative ) on unsatisfiable problems . Thus on satisfiable problems it has a strong incentive to make all the literals vote _sat_ instead of only half of them ."}, "1": {"review_id": "HJMC_iA5tm-1", "review_text": "This paper presents the NeuroSAT architecture, which uses a deep, message passing neural net for predicting the satisfiability of CNF instances. The architecture is also able to predict a satisfiable assignment in the SAT case, and the literals involved in some minimal conflicting set of clauses (i.e. core) in the UNSAT case. The NeuroSAT architecture is based on a vector space embedding of literals and clauses, which exploits (with message passing) some important symmetries of SAT instances (permutation invariance and negation invariance). This architecture is tested on various classes of random SAT instances, involving both unstructured (RS) problems, and structured ones (e.g. graph colorings, vertex covers, dominating sets, etc.). Overall the paper is well-motivated, and the experimental results are quite convincing. Arguably, the salient characteristic of NeuroSAT is to iteratively refine the confidence of literals voting for the SAT - or UNSAT - output, using a voting scheme on the last iteration of the literal matrix. This is very interesting, and NeuroSAT might be used to help existing solvers in choosing variable orderings for tackling hard instances, or hard queries (e.g. find a core). On the other hand, the technical description of the architecture (sec. 3) is perhaps a little vague for having a clear intuition of how the classification task - for SAT instances - is handled in the NeuroSAT architecture. Namely, a brief description of the initial matrices (which encode the literal en clause embeddings) would be nice. Some comments on the role played by the multilayer perceptron units and the normalization units would also be welcome. The two update rules in Page 3 could be explained in more detail. For the sake of clarity, I would suggest to provide a figure for depicting a transition (from iteration t-1 to iteration t) in the architecture. As a minor comment, it would be nice (in Section 2) to define the main parameters $n$, $m$, and $d$ used in the rest of the paper. Concerning the experimental part of the paper, Sections 4 & 5 are well-explained but, in Section 6, the solution decoding method, inspired from PCA is a bit confusing. Specifically, we don\u2019t know how a satisfying assignment is extracted from the last layer, and this should be explained in detail. According to Figure 4 and the comments above, it seems that a clustering method (with two centroids) is advocated, but this is not clear. In Table 1, the correlation between the accuracy on SAT instances, and the percent of SAT instances solved is not clear. Is the ratio of 70% measured on the CNF instances which have been predicted to be satisfiable? Or, is this ratio measured on the whole set of test instances? Finally, for the results established in Table 1, how many training instances and test instances have been used? In Section 7, some important aspects related to experiments, are missing. In Sec 7.1, for SR(200) tasks, was NeuroSAT tested on the same conditions as those for SR(40) tasks? Notably, what is the input dimension $d$ of the embedding space here? (I guess that $d = 128$ is too small for such large instances). Also, how many training and test instances have been used to plot the curves in Figure 5? For the 4,888 satisfiable instances generated in Sec. 7.2, which solver have been used to determine the satisfiability of those instances (I guess it is Minisat, but this should be mentioned somewhere). In Section 8, I found interesting the the ability of NeuroSAT in predicting the literals that participate in an UNSAT core. Indeed the problem of finding an UNSAT core in CNF instances is computationally harder than determining the satisfiability of this instance. So, NeuroSAT might be used here to help a solver in finding a core. But the notion of \u201cconfidence\u201d should be explained in more detail in this section, and more generally, in the whole paper. Namely, it seems that in the last layer of each iteration, literals are voting for SAT (red colors) with some confidence (say $\\delta$) and voting for UNSAT (blue colors) with some confidence (say $\\delta\u2019$). Are $\\delta$ and $\\delta\u2019$ correlated in the neural architecture? And, how confidences for UNSAT votes are updated? Finally, I found that the different benchmarks where relevant, but I would also suggest (for future work, or in the appendix) to additionally perform experiments on the well-known random 3-SAT instances ($k$ is fixed to 3). Here, it is well-known that a phase transition (on the instances, not the solver/learner) occurs at 4.26 for the clause/variable ratio. A plot displaying the performance of NeuroSAT (accuracy in predicting the label of the instance) versus the clause/variable ratio would be very helpful in assessing the robustness of NeuroSAT on the so-called \u201chard\u201d instances (which are close to 4.26). By extension, there have been a lot of recent work in generating \u201cpseudo-industrial\u201d random SAT instances, which incorporate some structure (e.g. communities) in order to mimic real-world structured SAT instances. To this point, it would be interesting to analyze the performance of NeuroSAT on such pseudo-industrial instances. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and questions . > a brief description of the initial matrices ( which encode the literal en clause embeddings ) would be nice . The initial vectors L_init and C_init are simply parameters of the model , that we learn simultaneously with the other parameters . > For the sake of clarity , I would suggest to provide a figure for depicting a transition ( from iteration t-1 to iteration t ) in the architecture . We graphically depict a single iteration in Figure 2 , though it is very high-level . Do you have a particular middle-ground in mind , between the figure we have and the equations themselves ? > As a minor comment , it would be nice ( in Section 2 ) to define the main parameters $ n $ , $ m $ , and $ d $ used in the rest of the paper . We updated S2 to introduce n and m. We can not introduce d there since d only makes sense in the context of the model , which is not discussed until S3 . > Concerning the experimental part of the paper , Sections 4 & 5 are well-explained but , in Section 6 , the solution decoding method , inspired from PCA is a bit confusing . Specifically , we don \u2019 t know how a satisfying assignment is extracted from the last layer , and this should be explained in detail . According to Figure 4 and the comments above , it seems that a clustering method ( with two centroids ) is advocated , but this is not clear Here is the description we give in the paper on the 2-clustering approach : `` 2-cluster $ L^ { ( T ) } $ to get cluster centers $ \\Delta_1 $ and $ \\Delta_2 $ , partition the variables according to the predicate \\ ( \\| x_i - \\Delta_1 \\|^2 + \\| \\flip { x_i } - \\Delta_2 \\|^2 < \\| x_i - \\Delta_2 \\|^2 + \\| \\flip { x_i } - \\Delta_1 \\|^2 \\ ) , and then try both candidate assignments that result from mapping the partitions to truth values . '' If you clarify what you find confusing or missing from this explanation , we will try to improve the explanation in the paper . > In Table 1 , the correlation between the accuracy on SAT instances , and the percent of SAT instances solved is not clear . Is the ratio of 70 % measured on the CNF instances which have been predicted to be satisfiable ? Or , is this ratio measured on the whole set of test instances ? As the caption says , in that experiment we were able to decode a satisfying assignment for 70 % of the satisfiable problems . The satisfiable problems includes the subset of satisfiable problems for which the network incorrectly predicted _unsat_ . To a first approximation , the 70 % number we report means that we could decode solutions for approximately 96 % of the problems correctly predicted to be _sat_ ; however , the 70 % does include a few problems for which the network found a solution but nonetheless incorrectly guessed _unsat_ . We expect this case to happen when the network finds the solution towards the very end of message passing , and does not have enough time to flip all the literal votes . Also note that the percentage of satisfiable problems solved is the metric we actually care about , whereas we only care about classification accuracy for instrumental reasons . > Finally , for the results established in Table 1 , how many training instances and test instances have been used ? It is easy to generate unlimited data from these distributions . We trained on millions of problems , and tested on hundreds of thousands of them . > In Sec 7.1 , for SR ( 200 ) tasks , was NeuroSAT tested on the same conditions as those for SR ( 40 ) tasks ? Notably , what is the input dimension $ d $ of the embedding space here ? ( I guess that $ d = 128 $ is too small for such large instances ) . Once trained , NeuroSAT has learned parameters whose dimensions depend on the hyperparameter $ d $ . It is not possible to run NeuroSAT with a larger $ d $ at test time . For SR ( 200 ) , we use the exact same trained NeuroSAT model as in Table 1 , which was trained only on SR ( U ( 10 , 40 ) ) and has $ d $ = 128 . > For the 4,888 satisfiable instances generated in Sec.7.2 , which solver have been used to determine the satisfiability of those instances ( I guess it is Minisat , but this should be mentioned somewhere ) . Yes , we used Minisat . We updated the paper to mention this , and also updated S4 to make it clear we use Minisat to generate SR ( n ) as well . > But the notion of \u201c confidence \u201d should be explained in more detail in this section , and more generally , in the whole paper . We only use the phrase `` confidence '' informally . The semantics of the literal votes is defined by the network architecture . > Namely , it seems that in the last layer of each iteration , literals are voting for SAT ( red colors ) with some confidence ( say $ \\delta $ ) and voting for UNSAT ( blue colors ) with some confidence ( say $ \\delta \u2019 $ ) . Are $ \\delta $ and $ \\delta \u2019 $ correlated in the neural architecture ? And , how confidences for UNSAT votes are updated ? I am afraid I do not understand what you are asking . Can you please clarify your use of 'correlated ' and 'updated ' in the last two sentences ?"}, "2": {"review_id": "HJMC_iA5tm-2", "review_text": "This paper trains a neural network to solve the satisfiability problems. Based on the message passing neural network, it presents NeuroSAT and trains it as a classifier to predict satisfiability under a single bit of supervision. After training, NeuroSAT can solve problems that are larger and more difficult than it ever saw during training. Furthermore, the authors present a way to decode the solutions from the network's activations. Besides, for unsatisfiable problems, the paper also presents NeuroUNSAT, which learns to detect the contradictions in the form of UNSAT cores. Relevance: this paper is likely to be of interest to a large proportion of the community for several reasons. Firstly, satisfiability problems arise from a variety of domains. This paper starts with a new angle to solve the SAT problem. Secondly, it uses neural networks in the SAT problem and establishes that neural networks can learn to perform a discrete search. Thirdly, the system used in this paper may also help improve existing SAT solvers. Significance: I think the results are significant. For the decoding satisfying assignments section, the two-dimensional PCA embeddings are very clear. And the NeuroSAT's success rate for more significant problems and different problems has shown it's generalization ability. Finally, the sequences of literal votes in NeuroUNSAT have proved its ability to detect unsatisfied cores. Novelty: NeuroSAT\u2019s approach is novel. Based on message passing neural networks, it trains a neural network to learn to solve the SAT problem. Soundness: This paper is technically sound. Evaluation: The experimental section is comprehensive. There are a variety of graphs showing the performance and ability of your architecture. However, the theoretical analysis isn't very sufficient. For instance, why does the change of the dataset from the original SR(n) to SRC(n,u) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores? Clarity: As a whole, the paper is clear. The definition of the problem, the model structure, the data generation, the training procedure, and the evaluation are all well organized. However, there is still a few points requiring more explanation. For instance, in figure 3, I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero, blue negative and red positive. Also, in figure 7, I am not sure whether those black grids represent higher positive values or lower negative values. A few questions: What's the initialization of the two vectors the authors use for tiling operation? Does the initialization differ for different types of SAT problems? How do the authors decide the number of iterations necessary for solving a particular SAT problem? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and questions . > However , the theoretical analysis is n't very sufficient . For instance , why does the change of the dataset from the original SR ( n ) to SRC ( n , u ) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores ? For SRC ( n , u ) , the objective function assigns much lower cost to the parameters that detect the presence of the planted unsat cores than to the parameters that search for satisfying assignments , because unlike the latter , the former allow perfect classification of the dataset in a fixed , small number of steps . Such a simple approach is not an option on SR ( n ) , because the cores are bigger and more diverse . > For instance , in figure 3 , I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero , blue negative and red positive . Also , in figure 7 , I am not sure whether those black grids represent higher positive values or lower negative values . We also write in two places that `` for several iterations , almost every literal is voting \\emph { unsat } with low confidence ( \\ie light blue ) '' . We updated the paper to include two more similar parenthetical notes , one for `` _sat_ with high confidence '' and `` dark red '' , and one for `` _unsat_ with high confidence '' and `` dark blue '' . What you saw as black is just dark blue . > What 's the initialization of the two vectors the authors use for tiling operation ? Does the initialization differ for different types of SAT problems ? It is just the parameters L_init and C_init that are learned by gradient descent at the same time as the other parameters are learned . When a trained NeuroSAT is run on a SAT problem , no matter the size or origin , the same L_init and C_init are used . > How do the authors decide the number of iterations necessary for solving a particular SAT problem ? Since the network usually converges once it finds a solution , one does not need to try to decode solutions after each round of message passing , and instead can run for a predetermined number of rounds and only check at the end . This is a desirable feature since it makes it easy to solve a very large number of SAT problems simultaneously as a single batch ( e.g.on a GPU ) without any problem-specific control flow . As for rules of thumb , Figure 5 provides data on how many iterations it took to solve what percentage of problems in SR ( n ) for a range of n. For the graph problems in S7.2 , we simply ran NeuroSAT for ( the somewhat arbitrary ) 512 iterations on every problem ."}}