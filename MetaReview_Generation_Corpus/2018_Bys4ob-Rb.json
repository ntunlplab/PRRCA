{"year": "2018", "forum": "Bys4ob-Rb", "title": "Certified Defenses against Adversarial Examples ", "decision": "Accept (Poster)", "meta_review": "The paper presents a differentiable upper bound on the performance of classifier on an adversarially perturbed example (with small perturbation in the L-infinity sense). The paper presents novel ideas, is well-written, and appears technically sound. It will likely be of interest to the ICLR community.\n\nThe only downside of the paper is its limited empirical evaluation: there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to much higher-dimensional datasets, for instance, ImageNet. The paper could, therefore, would benefit from empirical evaluations of the defenses on a dataset like ImageNet.", "reviews": [{"review_id": "Bys4ob-Rb-0", "review_text": "This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied. While the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive. However the new bound is an \"upper\" bound of the worst-case performance which is very different from the conventional sampling based \"lower\" bounds. Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier. This paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound). In conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier, and the paper is clearly written and easy to follow. There are possible future directions to be developed. 1. Apply the sum-of-squares (SOS) method. The paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy. One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound. The paper already mentions about this direction and it would be interesting to see the experimental results. 2. Develop a similar relaxation for deep neural networks. The author already mentioned that they are pursuing this direction. While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the comments and thoughtful suggestions . Adding to the recommendations about the future work : 1 . Sum-of-squares ( SOS ) method -- It is indeed interesting to check whether the higher degree of SOS gives us sufficient tightness to significantly improve the results . An obvious bottleneck in trying this out is the expensive computation . Given that our objective is similar to MAXCUT , for which it is currently unknown whether higher degree SOS relaxations give better approximation ratios , it is apriori unclear how much we could gain . 2.Develop a similar relaxation for deeper networks -- We agree with the reviewer that this is an interesting direction to pursue . In fact , we have already begun implementing an algorithm that works for arbitrary depth networks . As described in the response to reviewer 1 , the basic idea is that the adversarial loss for arbitrary ReLU networks can be written as a non-convex quadratic program , which can then be relaxed to an SDP and trained with similar ideas to the present paper . As the reviewer mentions , it \u2019 s possible that resnets have additional structure that can be exploited efficiently , but our current proposal handles resnets as well ."}, {"review_id": "Bys4ob-Rb-1", "review_text": "The authors propose a new defense against security attacks on neural networks. The attack model involves a standard l_inf norm constraint. Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice. Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks. The approach is evaluated for several attacks on MNIST data. First of all, the paper is very well written and structured. As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic). The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. An innovative training criterion based on that certificate is proposed. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks. In summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation. For me, it is a clear accept. The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your interest in our work and the pointer to the relevant recent work by Hein and Andriushschenko . We have fixed this omission and include a discussion of the paper in the newest uploaded version of our work . To summarize the comparison : Firstly , their work focuses on perturbations in l-2 norm while ours considers the l-infty norm . Hence , there is no direct way to compare the experimental results . Theoretically , the general bound proposed for any l-p norm perturbation is similar to what we have in our work . However , the main challenge is to efficiently evaluate this bound . Hein and Andriushschenko show how to do this for p=2 . In our work , we consider the attack model where p = \\infty . This makes a significant difference in the computations involved ."}, {"review_id": "Bys4ob-Rb-2", "review_text": "This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer. The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time. The main idea of using upper bound (as opposed to lower bound) is reasonable. However, I find there are some limitations/weakness of the proposed method: 1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer) 2. SDP while tractable, would still require very expensive computation to solve exactly. 3. The relaxation seems a bit loose - in particular, in above step 2 and 3, the authors replace the gradient value by a global upper bound on that, which to me seems can be pretty loose.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments ! From our understanding of your review , it seems that there are three concerns , which we highlight and address below . 1. \u201c The method is likely not extendable to more complicated and more practical networks , beyond the ones discussed in the paper ( ie with one hidden layer ) \u201d Our general approach for obtaining networks with certified robustness can in fact extend to deeper networks . We have already begun implementing an algorithm that works for arbitrary depth networks . The basic idea is that the adversarial loss for arbitrary ReLU networks can be written as a non-convex quadratic program , which can then be relaxed to an SDP and trained with similar ideas to the present paper . We would be happy to give details if it would be helpful . 2. \u201c SDP while tractable , would still require very expensive computation to solve exactly. \u201d We would like to stress that we do not need to solve the SDP exactly . As discussed in Section 4 of our paper , our network can be trained via gradient descent on the dual . Even with inexact minimization of these dual variables , we get valid certificates . We acknowledge that training our model is slower than training regular networks , but it is not nearly as bad as if one had to exactly solve the SDP . We also note that at test time , the trained dual variables directly provide a certificate ( with no extra computation ) and hence checking robustness at test time is no slower than generating predictions for a regular network . 3 . `` The relaxation seems a bit loose - in particular , in above step 2 and 3 , the authors replace the gradient value by a global upper bound on that , which to me seems can be pretty loose . '' An important insight from our experiments is the following : while our SDP bound can be quite loose on arbitrary networks , optimizing against this SDP certificate leads to networks where this certificate is substantially tighter ( as seen in Figure 3 ) . Minimizing the SDP upper bound forces the optimizer to avoid where the bound is loose , as such points have higher objective values . Hence , the general looseness of the relaxation does not impede the utility of the relaxation as a way of obtaining provably robust networks ."}], "0": {"review_id": "Bys4ob-Rb-0", "review_text": "This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied. While the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive. However the new bound is an \"upper\" bound of the worst-case performance which is very different from the conventional sampling based \"lower\" bounds. Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier. This paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound). In conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier, and the paper is clearly written and easy to follow. There are possible future directions to be developed. 1. Apply the sum-of-squares (SOS) method. The paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy. One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound. The paper already mentions about this direction and it would be interesting to see the experimental results. 2. Develop a similar relaxation for deep neural networks. The author already mentioned that they are pursuing this direction. While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the comments and thoughtful suggestions . Adding to the recommendations about the future work : 1 . Sum-of-squares ( SOS ) method -- It is indeed interesting to check whether the higher degree of SOS gives us sufficient tightness to significantly improve the results . An obvious bottleneck in trying this out is the expensive computation . Given that our objective is similar to MAXCUT , for which it is currently unknown whether higher degree SOS relaxations give better approximation ratios , it is apriori unclear how much we could gain . 2.Develop a similar relaxation for deeper networks -- We agree with the reviewer that this is an interesting direction to pursue . In fact , we have already begun implementing an algorithm that works for arbitrary depth networks . As described in the response to reviewer 1 , the basic idea is that the adversarial loss for arbitrary ReLU networks can be written as a non-convex quadratic program , which can then be relaxed to an SDP and trained with similar ideas to the present paper . As the reviewer mentions , it \u2019 s possible that resnets have additional structure that can be exploited efficiently , but our current proposal handles resnets as well ."}, "1": {"review_id": "Bys4ob-Rb-1", "review_text": "The authors propose a new defense against security attacks on neural networks. The attack model involves a standard l_inf norm constraint. Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice. Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks. The approach is evaluated for several attacks on MNIST data. First of all, the paper is very well written and structured. As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic). The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. An innovative training criterion based on that certificate is proposed. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks. In summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation. For me, it is a clear accept. The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your interest in our work and the pointer to the relevant recent work by Hein and Andriushschenko . We have fixed this omission and include a discussion of the paper in the newest uploaded version of our work . To summarize the comparison : Firstly , their work focuses on perturbations in l-2 norm while ours considers the l-infty norm . Hence , there is no direct way to compare the experimental results . Theoretically , the general bound proposed for any l-p norm perturbation is similar to what we have in our work . However , the main challenge is to efficiently evaluate this bound . Hein and Andriushschenko show how to do this for p=2 . In our work , we consider the attack model where p = \\infty . This makes a significant difference in the computations involved ."}, "2": {"review_id": "Bys4ob-Rb-2", "review_text": "This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer. The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time. The main idea of using upper bound (as opposed to lower bound) is reasonable. However, I find there are some limitations/weakness of the proposed method: 1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer) 2. SDP while tractable, would still require very expensive computation to solve exactly. 3. The relaxation seems a bit loose - in particular, in above step 2 and 3, the authors replace the gradient value by a global upper bound on that, which to me seems can be pretty loose.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments ! From our understanding of your review , it seems that there are three concerns , which we highlight and address below . 1. \u201c The method is likely not extendable to more complicated and more practical networks , beyond the ones discussed in the paper ( ie with one hidden layer ) \u201d Our general approach for obtaining networks with certified robustness can in fact extend to deeper networks . We have already begun implementing an algorithm that works for arbitrary depth networks . The basic idea is that the adversarial loss for arbitrary ReLU networks can be written as a non-convex quadratic program , which can then be relaxed to an SDP and trained with similar ideas to the present paper . We would be happy to give details if it would be helpful . 2. \u201c SDP while tractable , would still require very expensive computation to solve exactly. \u201d We would like to stress that we do not need to solve the SDP exactly . As discussed in Section 4 of our paper , our network can be trained via gradient descent on the dual . Even with inexact minimization of these dual variables , we get valid certificates . We acknowledge that training our model is slower than training regular networks , but it is not nearly as bad as if one had to exactly solve the SDP . We also note that at test time , the trained dual variables directly provide a certificate ( with no extra computation ) and hence checking robustness at test time is no slower than generating predictions for a regular network . 3 . `` The relaxation seems a bit loose - in particular , in above step 2 and 3 , the authors replace the gradient value by a global upper bound on that , which to me seems can be pretty loose . '' An important insight from our experiments is the following : while our SDP bound can be quite loose on arbitrary networks , optimizing against this SDP certificate leads to networks where this certificate is substantially tighter ( as seen in Figure 3 ) . Minimizing the SDP upper bound forces the optimizer to avoid where the bound is loose , as such points have higher objective values . Hence , the general looseness of the relaxation does not impede the utility of the relaxation as a way of obtaining provably robust networks ."}}