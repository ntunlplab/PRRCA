{"year": "2021", "forum": "lvXLfNeCQdK", "title": "Loss Landscape Matters: Training Certifiably Robust Models with Favorable Loss Landscape", "decision": "Reject", "meta_review": "The authors argue that tighter relaxations for certified robustness suffer from a worse loss landscape and thus are outperformed by the much simpler and less tight IBP relaxation and come up with a new relaxation to overcome this problem. \n\nAfter the rebuttal there still remain doubts about the reasoning regarding the loss landscape (even though I acknowledge that the authors have invested significant amount of work to support their hypothesis). Moreover, the differences to existing certified training methods is small or the proposed method performs worse while being significantly more expensive (in particular if one takes into account the results which are reported on the IBP-Crown github page where the reported numbers are significantly lower than reported in the present paper) so that the benefit is unclear.\n\nThus the majority of the reviewers still suggests rejection and I agree with that even though I think that the paper has its merits and I encourage the authors to continue this line of work. For a next version, the authors should evaluate all the methods ideally with an exact verification method resp. use the best relaxation for all methods. Otherwise the differences can come just from the weaker relaxation but not from a difference in real robustness.\n\n", "reviews": [{"review_id": "lvXLfNeCQdK-0", "review_text": "This paper claims to make three contributions : ( i ) a theoretical and experimental demonstration that the driving force behind performance in certifiable training methods is loss landscape smoothness ( ii ) a new training method and domain designed to have a smooth loss landscape ( iii ) an evaluation showing their new training method performs comparably to the state of the art . Pros : * The paper provides some experimental analysis support a hypothesis that has been conjectured since Mirman et al . ( 2018 ) , that smooth loss landscapes produced by smoother transformers ( zSmooth/hSmooth ) would improve training . * The paper does show experimentally that certain methods tend to have less smooth loss landscapes . * The theoretical result connects the smoothness of the relaxed gradient approximations to the smoothness of the loss landscape , which may be useful for designing future certifiable training methods . Cons : * In terms of accuracy and verified error , the proposed system performs comparably to some pre-existing systems . The main claim that the authors have identified an important factor for improving certified training remains unsupported by the inability to improve noticeably upon results from prior work . * The authors do not compare it to the state of the art , COLT [ 1 ] , which achieves for example 21.6/39.5 ( standard , verified error ) compared to this paper \u2019 s 31.49/49.42 results on CIFAR10 2/255 . This is a much more significant difference than for any of the comparisons made to other work in this paper . * It is unclear how fast the proposed method is . No comparison appears to be made on training speed or memory usage compared with other systems . Given comparable accuracy and verified error results , I would not see a reason to use this system if this one is slower . * I am not sure about the utility of the experiment on tightness and Figure 3 . Different methods could have different scales of loss while producing similarly verifiable networks ( as this paper in fact demonstrates ) . Further , even the worst case margin for a class could be fooled by a network with entirely zero weights . * Loss value is also compared between methods in Figure 1 . * The paper claims that state of the art methods such as CROWN-IBP and CAP are held back by their non-smooth loss landscapes while failing to meaningfully outperform them . 5 out of 10 of the standard accuracies are worse than CROWN-IBP \u2019 s for example . * Section 4.1 claims that the experiments show that \u201c the non-smoothness of the relaxed gradient approximation of linear relaxations negatively affects their performance \u201d yet the evidence presented is circumstantial , and not capable of indicating causality . * Section 4.2 claims that the theoretical analysis states that some landscapes are more favourable than others , whereas in fact it only demonstrates the smoothness of some landscapes not their \u201c favourability. \u201d * The paper leaves out additional related work . In particular , I would like to see a discussion of the difference between the analysis used by Fastened Crown [ 2 ] . While this paper does provide a new technique with some theoretical justification , unfortunately neither the theoretical justification or experimental results are significant enough to recommend acceptance . Furthermore , because the paper \u2019 s central assumption , that a smooth loss landscape leads to better results , is unsupported by their own experimental results , I must rate the paper a rejection . [ 1 ] Mislav Balunovic and Martin Vechev . Adversarial training and provable defenses : Bridging the gap . In International Conference on Learning Representations , 2020 . [ 2 ] Lyu , Z. , Ko , C.-Y. , Kong , Z. , Wong , N. , Lin , D. , and Daniel , L. Fastened crown : Tightened neural network robustness certificates . In AAAI , 2020 . == Update : While some of my points have been addressed and the quality of the paper has been improved , my main concern , that the experiments do not support the central claim . The authors argue that because their method has a smoother loss landscape then similar methods , it performs better . In the updated paper , the evidence that the method performs better than similar methods has been made clearer , but the improvement is is still marginal . The more pressing matter however is that the conclusion that the cause of the improvement is from a change in the loss landscape smoothness is based only on a qualitative comparison of only five methods ( as the benefit is not consistent between methods in any category ) . This is enough to at best demonstrate a weak correlation , but not enough to demonstrate causation .", "rating": "3: Clear rejection", "reply_text": "Thank you for the valuable feedback . We will provide further detailed comments within a few days . - C2-1 : In terms of accuracy and verified error , the proposed system performs comparably to some pre-existing systems . The main claim that the authors have identified an important factor for improving certified training remains unsupported by the inability to improve noticeably upon results from prior work . - A2-1 : The main purpose of this paper is to understand previous linear relaxation-based methods ( IBP , CAP , and CROWN-IBP $ \\_ { 1\\rightarrow 1 } $ ) in terms of tightness of bound and smoothness of landscape and to propose a training method that can utilize both terms . In the analysis ( Section 4 ) , we compared the three methods ( IBP , CROWN-IBP $ \\_ { 1\\rightarrow 1 } $ , CAP ) with OURS . Note that CROWN-IBP $ \\_ { 1\\rightarrow 0 } $ is not compared because of the complex behavior of $ \\beta $ -scheduling . In Table 1 , OURS shows the best performance among those 4 ( except CROWN-IBP $ \\_ { 1\\rightarrow 0 } $ ) except for the case when $ \\epsilon $ is small . When $ \\epsilon $ is small , OURS is the second best ( see A2-2 for details ) . - However , compared to CROWN-IBP $ \\_ { 1\u21920 } $ , OURS shows only marginal improvement or even worse in the case of $ \\epsilon=0.3 , 0.4 $ on MNIST . There is some explanation of why this $ \\beta $ -scheduling ( 1\u21920 ) in ( 3 ) ( from CROWN-IBP $ \\_ { 1\u21921 } $ to IBP ) can improve the performance [ CROWN-IBP ] . We can add on it with our understanding that CROWN-IBP $ \\_ { 1\u21920 } $ * * starts with a tighter bound * * ( $ \\beta=1 $ , CROWN-IBP only ) * * but not overfits to small perturbation by gradually introducing the IBP objective which has a smoother landscape . * * We note that IBP has the most smooth landscape compared to others ( Theorem1 ) because IBP has a relaxed gradient approximation $ g=0 $ . We will update the paper with the above discussion . - C2-2 : The authors do not compare it to the state of the art , COLT [ 1 ] , which achieves for example 21.6/39.5 ( standard , verified error ) compared to this paper \u2019 s 31.49/49.42 results on CIFAR10 2/255 . This is a much more significant difference than for any of the comparisons made to other work in this paper . - cf ) A2-1 - A2-2 : As mentioned above , OURS shows the best performance among the four methods ( except CROWN-IBP $ \\_ { 1\u21920 } $ ) except for the case when $ \\epsilon $ is small . Especially , under $ \\epsilon=2/255 $ on CIFAR-10 or $ \\epsilon=0.01 $ on SVHN , CAP is better than OURS . This can be explained by the fact that CAP has a tighter bound than OURS ( see Fig 1 , Left , at epoch=10 ) . Similarly , COLT shows better results than OURS for $ \\epsilon=2/255 $ on CIFAR-10 . But , this does not contradict our understanding that smoothness and `` tightness '' are important . For small $ \\epsilon $ values , tightness seems more important than smoothness . Our understanding is that with a non-smooth landscape ( e.g.CAP , CROWN-IBP $ _ { 1\u21921 } $ ) , it tends to overfit to a small perturbation case during $ \\epsilon $ -scheduling . Thus , it does not generalize well to a large perturbation . Therefore , when trained with a small ( target ) perturbation , it shows decent robustness against the perturbation . Note that both CAP and COLT have worse performance than OURS for large $ \\epsilon $ since they overfit to small $ \\epsilon $ during $ \\epsilon $ -scheduling . We summarize the VE ( verification error ) as follows : - * * OURS ( 2.36 ) * * / CAP ( 3.19 ) / COLT ( 2.9 ) for $ \\epsilon=0.1 $ on MNIST - * * OURS ( 9.79 ) * * / CAP ( 47.85 ) / COLT ( 14.3 ) for $ \\epsilon=0.3 $ on MNIST - OURS ( 49.42 ) / CAP ( 48.50 ) / * * COLT ( 39.5 ) * * for $ \\epsilon=2/255 $ on CIFAR10 - * * OURS ( 69.70 ) * * / CAP ( 73.02 ) / COLT ( 72.5 ) for $ \\epsilon=8/255 $ on CIFAR10 - We also note that COLT additionally uses MILP and [ Ehlers ] to compute the VE ( which takes roughly 2 days for CIFAR-10 test data ) , and thus it might be unfair to compare with this value directly . We will update the paper with the above discussion with COLT . - C2-3 : It is unclear how fast the proposed method is . No comparison appears to be made on training speed or memory usage compared with other systems . Given comparable accuracy and verified error results , I would not see a reason to use this system if this one is slower . - A2-3 : We will update the complete comparison results soon . Our method is not as fast as IBP and CROWN-IBP ( x1/2 ) , but faster than CAP ( x6-7 ) and COLT . We note the approximate speedup in parentheses . In the case of COLT , it is hard to compare the speed since they only use a 4-layer small network ( we use 7-layer ) and they require 4 stages ( one stage for each layer ) of training with 200 epochs each . Anyhow , COLT is slower than CAP . In addition , our method requires more memory than IBP and CROWN-IBP but much less than CAP and COLT . Again , we emphasize that our focus is on understanding certifiable training , and thus speed and memory efficiency are not our focus unless it does not run ."}, {"review_id": "lvXLfNeCQdK-1", "review_text": "This paper studies why training with looser bounds ( IBP ) can outperform tighter linear relaxation based methods in certified defense . The authors argue that this is because IBP has a smoother loss landscape compared to linear relaxation based methods . Then the paper proposes to optimize the lower bound in the CROWN relaxation for unstable ReLU neurons during training , for tighter bounds and a smoother loss landscape . Pros : * This paper studies an important problem in certified defense and tries to improve linear relaxation based certified defense methods , especially CROWN . * This paper novelly proposes to optimize the lower bound in the relaxation for unstable ReLU neurons to train tighter bounds in certified defense . * The paper showed some improvement on test verifier error , compared to CROWN-IBP ( 1- > 1 ) , and it also showed improvement on tightness and loss smoothness . Cons : * The link between the motivation and the proposed method is not well justified . Paragraph \u201c Favorable Landscape \u201d somewhat explains the connection between optimizing the lower bound in relaxation and improving loss landscape . But it states that the benefit comes from preferring dead ReLU neurons to unstable ones . If the proposed method is actually trying to make more unstable neurons dead , the paper does not discuss or compare with paper Xiao et al. , 2018 ( https : //arxiv.org/pdf/1809.03008.pdf ) which directly adds a regularization to induce ReLU stability . * It does not seem to make sense that the proposed method can prefer to have dead neurons via optimizing the relaxation for unstable neurons . With such an optimization , bounds of unstable neurons become tighter , so how can such tighter bounds make the model favor unstable neurons less than CROWN-IBP with relatively looser bounds ? * As the paper mentions that CROWN-IBP does not penalize unstable ReLU neurons with ( |u| < =|l| ) , but how about just make the lower bound equal to the upper bound as Fast-lin ( Weng et al. , 2018 , https : //arxiv.org/abs/1804.09699 ) does , so the lower bound is non-zero for unstable neurons ? * An improvement over CROWN-IBP ( 1- > 1 ) is shown in the paper . However , the proposed method does not make enough improvement over CROWN-IBP ( 1- > 0 ) . The improvement on CIFAR10 with eps=8/255 or 16/255 is very small ( error 69.70 v.s.69.97 , 77.87 v.s . 77.88 ) , while there is a large performance drop on MNIST with eps=0.3/0.4 ( increased error 7.07- > 9.79 and 12.35- > 15.42 ) . What will the performance of the proposed method look like if a changing $ \\beta $ is used in training ? * This paper missed some previous works , e.g. , Xiao et al. , 2018 and Weng et al. , 2018 as mentioned above . More discussions or comparisons with them may be needed . Update after rebuttal : Thanks for the detailed author response . I think the paper is interesting in providing an analysis on how optimizing the linear relaxation in CROWN ( although the verification method itself seems to be similar as the one in Fastened CROWN in AAAI 2020 ) can lead to better loss smoothness and tightness , which seems to improve the performance of certified training . The author replies have addressed some of my concerns in my initial review . However , there are still some outstanding concerns : 1 . After more consideration , I think the \u201c More favorable landscape \u201d paragraph is still insufficient to address the second point in my above cons . The author response argues that some \u201c $ ( p/q ) $ \u201d have looser or tighter bounds , but these are considered for relaxation * locally * , not the tightness on the final output , while the relaxation optimized in the paper is to tighten the final output . Thus it remains unclear why tighter final bounds with improvement from the unstable ReLU neurons makes the model favor unstable neurons less . 2.AnonReviewer2 has reminded me that the \u201c Fastened CROWN \u201d work had a similar method about optimizing the lower bound of the linear relaxation in verification , which seems to be very similar to the method proposed in this paper , in terms of the verification part in certified training . Although this paper focuses on certified training and has some different analysis , the major modification on the method is still on the verification part , and thus I agree that a discussion on the comparison with Fastened CROWN should not be missed . The authors did not add it in the discussion period . 3.It is promising that the proposed method outperformed the modified CROWN-IBP ( $ \\beta=1 $ ) and IBP , and there seems to be a significant margin . But the proposed method fails to make a significant improvement compared to the original CROWN-IBP ( the 1- > 0 one ) , e.g. , the improvement on CIFAR-10 eps=8/255 or 16/255 is negligible . Overall , I am keeping my recommendation as rejection for the current version of the manuscript .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We will be updating comments for the other feedback soon ( C3-1 , C3-2 , C3-5 ) . It requires some additional experiments and we are working on it . Thank you for pointing this out . - C3-3 : As the paper mentions that CROWN-IBP does not penalize unstable ReLU neurons with ( |u| < =|l| ) , but how about just make the lower bound equal to the upper bound as Fast-lin ( Weng et al. , 2018 , [ https : //arxiv.org/abs/1804.09699 ] ( https : //arxiv.org/abs/1804.09699 ) ) does , so the lower bound is non-zero for unstable neurons ? - A3-3 : With the lower bound equal to the upper bound ( $ \\underline { a } =\\frac { u^+ } { u^+-l^- } =\\overline { a } ) $ as Fast-lin , it yields the same as `` * * CAP * * -IBP '' bound in Fig3 which has a looser upper bound than OURS and CROWN-IBP . It may have better smoothness , but it has worse tightness . We will check on that . - C3-4 : An improvement over CROWN-IBP ( 1- > 1 ) is shown in the paper . However , the proposed method does not make enough improvement over CROWN-IBP ( 1- > 0 ) . The improvement on CIFAR10 with eps=8/255 or 16/255 is very small ( error 69.70 v.s.69.97 , 77.87 v.s . 77.88 ) , while there is a large performance drop on MNIST with eps=0.3/0.4 ( increased error 7.07- > 9.79 and 12.35- > 15.42 ) . What will the performance of the proposed method look like if a changing is \u03b2 used in training ? - A3-4 : Table 3 in Section J shows the results with different $ \\beta $ -schedulings . It did n't improve performance . We will add the results on MNIST . - cf.A2-1 : Compared to CROWN-IBP $ \\_ { 1\u21920 } $ , OURS shows only marginal improvement or even worse in the case of $ \\epsilon=0.3 , 0.4 $ on MNIST . There is some explanation of why this $ \\beta $ -scheduling ( 1\u21920 ) in ( 3 ) ( from CROWN-IBP $ \\_ { 1\u21921 } $ to IBP ) can improve the performance [ CROWN-IBP ] . We can add on it with our understanding that CROWN-IBP $ \\_ { 1\u21920 } $ starts with a tighter bound ( $ \\beta=1 $ , CROWN-IBP only ) but not overfits to small perturbation by gradually introducing the IBP objective which has a smoother landscape . We note that IBP has the most smooth landscape compared to others ( Theorem1 ) because IBP has a relaxed gradient approximation $ g=0 $ . We will update the paper with the above discussion . [ CROWN-IBP ] Zhang et al.2019 ( https : //arxiv.org/abs/1906.06316 )"}, {"review_id": "lvXLfNeCQdK-2", "review_text": "In this paper , the authors studied the role of loss landscape in training certifiable robust models . The authors reviewed linear relaxation based methods , and showed that Interval Bound Propagation ( IBP ) is a special case of linear relaxation based methods . Although linear relaxation based methods have a tighter bound on worst case loss with adversarial perturbations than IBP based method , the authors found in numerical studies that towards the end of training , IBP outperforms linear relaxation based methods . The authors hypothesized that this was because IBP loss landscape was more smooth , which helped optimization . The authors demonstrated in a theorem that IBP loss was indeed more smooth under certain assumptions . Based on this insight , the authors proposed a favorable landscape method . The authors showed in numerical studies that the sum over the worst-case margin for each class is lowest for their method . The loss of their method is also the most smooth among competing methods . Their method achieved a consistent performance in a range of perturbations , which is not achieved in competing methods . The paper is clearly written . The insight is interesting , and could help future researchers . On page 4 , in Figure 1 , what is the mathematical definition of `` variation '' ? Why does the variation of CROWN-IBP first increase , and then decrease ? A related question is why does the proportion of unstable ReLu first increase and then decrease in CROWN-IBP ? Does it have something to do with the curvature shown in Figure 1 ( right ) ? One page 7 , it would be great to also show the comparison of training time between these methods .", "rating": "7: Good paper, accept", "reply_text": "Q1-1 : On page 4 , in Figure 1 , what is the mathematical definition of `` variation '' ? - A1-1 : The definition of variation is defined in ( 8 ) in Appendix B as follows : $ \\mathcal { L } ( \\overline { s } ( \\theta ( t ) ) ) \\text { where } \\mathcal { L } ( \\overline { s } ( \\theta ) ) \\equiv\\mathcal { L } ( \\overline { s } ( x , y , \\epsilon ; \\theta ) , y ) \\text { and } \\theta ( t ) \\equiv \\theta_0-t\\eta\\nabla_\\theta \\mathcal { L } ( \\overline { s } ( \\theta_0 ) ) \\text { for } t\\in [ 0,5 ] $ It is the range of the loss values along the gradient descent direction with the stepsize $ \\in [ 0,5 ] \\times\\eta $ where $ \\eta $ is the learning rate . Q1-2 : Why does the variation of CROWN-IBP first increase , and then decrease ? - A1-2 : It is hard to tell why they show such behavior . Certifiable training prefers stable ( dead/alive ) ReLUs ( 97-99\\ % ) to unstable ReLUs ( 1-3\\ % ) since unstable ReLUs make the upper bound looser ( see Fig 11 ) because of the relaxation in unstable ReLU . However , without unstable ReLU , the model is ( * provably/certifiably * ) linear under the perturbation and has very low expressive power . Therefore , the two factors conflict with each other . We hypothesize that as CROWN-IBP $ _ { 1\\rightarrow1 } $ approaches to a flatter local minimum , the model is stabilized and focuses on reducing unstable ReLUs . This behavior is likely to be related to the curvature in Figure 1 ( right ) . Near a flatter minimum , the curvature becomes flattened and loss variations are reduced . Q1-3 : A related question is why does the proportion of unstable ReLu first increase and then decrease in CROWN-IBP ? - cf ) A1-2 Q1-4 : Does it have something to do with the curvature shown in Figure 1 ( right ) ? - cf ) A1-2 Q1-5 : One page 7 , it would be great to also show the comparison of training time between these methods . - A1-5 : Roughly speaking , our method is not as fast as IBP and CROWN-IBP ( x1/2 ) , but faster than CAP ( x6-7 ) . We note the approximate speedup in parentheses . We will update complete comparison results soon ."}], "0": {"review_id": "lvXLfNeCQdK-0", "review_text": "This paper claims to make three contributions : ( i ) a theoretical and experimental demonstration that the driving force behind performance in certifiable training methods is loss landscape smoothness ( ii ) a new training method and domain designed to have a smooth loss landscape ( iii ) an evaluation showing their new training method performs comparably to the state of the art . Pros : * The paper provides some experimental analysis support a hypothesis that has been conjectured since Mirman et al . ( 2018 ) , that smooth loss landscapes produced by smoother transformers ( zSmooth/hSmooth ) would improve training . * The paper does show experimentally that certain methods tend to have less smooth loss landscapes . * The theoretical result connects the smoothness of the relaxed gradient approximations to the smoothness of the loss landscape , which may be useful for designing future certifiable training methods . Cons : * In terms of accuracy and verified error , the proposed system performs comparably to some pre-existing systems . The main claim that the authors have identified an important factor for improving certified training remains unsupported by the inability to improve noticeably upon results from prior work . * The authors do not compare it to the state of the art , COLT [ 1 ] , which achieves for example 21.6/39.5 ( standard , verified error ) compared to this paper \u2019 s 31.49/49.42 results on CIFAR10 2/255 . This is a much more significant difference than for any of the comparisons made to other work in this paper . * It is unclear how fast the proposed method is . No comparison appears to be made on training speed or memory usage compared with other systems . Given comparable accuracy and verified error results , I would not see a reason to use this system if this one is slower . * I am not sure about the utility of the experiment on tightness and Figure 3 . Different methods could have different scales of loss while producing similarly verifiable networks ( as this paper in fact demonstrates ) . Further , even the worst case margin for a class could be fooled by a network with entirely zero weights . * Loss value is also compared between methods in Figure 1 . * The paper claims that state of the art methods such as CROWN-IBP and CAP are held back by their non-smooth loss landscapes while failing to meaningfully outperform them . 5 out of 10 of the standard accuracies are worse than CROWN-IBP \u2019 s for example . * Section 4.1 claims that the experiments show that \u201c the non-smoothness of the relaxed gradient approximation of linear relaxations negatively affects their performance \u201d yet the evidence presented is circumstantial , and not capable of indicating causality . * Section 4.2 claims that the theoretical analysis states that some landscapes are more favourable than others , whereas in fact it only demonstrates the smoothness of some landscapes not their \u201c favourability. \u201d * The paper leaves out additional related work . In particular , I would like to see a discussion of the difference between the analysis used by Fastened Crown [ 2 ] . While this paper does provide a new technique with some theoretical justification , unfortunately neither the theoretical justification or experimental results are significant enough to recommend acceptance . Furthermore , because the paper \u2019 s central assumption , that a smooth loss landscape leads to better results , is unsupported by their own experimental results , I must rate the paper a rejection . [ 1 ] Mislav Balunovic and Martin Vechev . Adversarial training and provable defenses : Bridging the gap . In International Conference on Learning Representations , 2020 . [ 2 ] Lyu , Z. , Ko , C.-Y. , Kong , Z. , Wong , N. , Lin , D. , and Daniel , L. Fastened crown : Tightened neural network robustness certificates . In AAAI , 2020 . == Update : While some of my points have been addressed and the quality of the paper has been improved , my main concern , that the experiments do not support the central claim . The authors argue that because their method has a smoother loss landscape then similar methods , it performs better . In the updated paper , the evidence that the method performs better than similar methods has been made clearer , but the improvement is is still marginal . The more pressing matter however is that the conclusion that the cause of the improvement is from a change in the loss landscape smoothness is based only on a qualitative comparison of only five methods ( as the benefit is not consistent between methods in any category ) . This is enough to at best demonstrate a weak correlation , but not enough to demonstrate causation .", "rating": "3: Clear rejection", "reply_text": "Thank you for the valuable feedback . We will provide further detailed comments within a few days . - C2-1 : In terms of accuracy and verified error , the proposed system performs comparably to some pre-existing systems . The main claim that the authors have identified an important factor for improving certified training remains unsupported by the inability to improve noticeably upon results from prior work . - A2-1 : The main purpose of this paper is to understand previous linear relaxation-based methods ( IBP , CAP , and CROWN-IBP $ \\_ { 1\\rightarrow 1 } $ ) in terms of tightness of bound and smoothness of landscape and to propose a training method that can utilize both terms . In the analysis ( Section 4 ) , we compared the three methods ( IBP , CROWN-IBP $ \\_ { 1\\rightarrow 1 } $ , CAP ) with OURS . Note that CROWN-IBP $ \\_ { 1\\rightarrow 0 } $ is not compared because of the complex behavior of $ \\beta $ -scheduling . In Table 1 , OURS shows the best performance among those 4 ( except CROWN-IBP $ \\_ { 1\\rightarrow 0 } $ ) except for the case when $ \\epsilon $ is small . When $ \\epsilon $ is small , OURS is the second best ( see A2-2 for details ) . - However , compared to CROWN-IBP $ \\_ { 1\u21920 } $ , OURS shows only marginal improvement or even worse in the case of $ \\epsilon=0.3 , 0.4 $ on MNIST . There is some explanation of why this $ \\beta $ -scheduling ( 1\u21920 ) in ( 3 ) ( from CROWN-IBP $ \\_ { 1\u21921 } $ to IBP ) can improve the performance [ CROWN-IBP ] . We can add on it with our understanding that CROWN-IBP $ \\_ { 1\u21920 } $ * * starts with a tighter bound * * ( $ \\beta=1 $ , CROWN-IBP only ) * * but not overfits to small perturbation by gradually introducing the IBP objective which has a smoother landscape . * * We note that IBP has the most smooth landscape compared to others ( Theorem1 ) because IBP has a relaxed gradient approximation $ g=0 $ . We will update the paper with the above discussion . - C2-2 : The authors do not compare it to the state of the art , COLT [ 1 ] , which achieves for example 21.6/39.5 ( standard , verified error ) compared to this paper \u2019 s 31.49/49.42 results on CIFAR10 2/255 . This is a much more significant difference than for any of the comparisons made to other work in this paper . - cf ) A2-1 - A2-2 : As mentioned above , OURS shows the best performance among the four methods ( except CROWN-IBP $ \\_ { 1\u21920 } $ ) except for the case when $ \\epsilon $ is small . Especially , under $ \\epsilon=2/255 $ on CIFAR-10 or $ \\epsilon=0.01 $ on SVHN , CAP is better than OURS . This can be explained by the fact that CAP has a tighter bound than OURS ( see Fig 1 , Left , at epoch=10 ) . Similarly , COLT shows better results than OURS for $ \\epsilon=2/255 $ on CIFAR-10 . But , this does not contradict our understanding that smoothness and `` tightness '' are important . For small $ \\epsilon $ values , tightness seems more important than smoothness . Our understanding is that with a non-smooth landscape ( e.g.CAP , CROWN-IBP $ _ { 1\u21921 } $ ) , it tends to overfit to a small perturbation case during $ \\epsilon $ -scheduling . Thus , it does not generalize well to a large perturbation . Therefore , when trained with a small ( target ) perturbation , it shows decent robustness against the perturbation . Note that both CAP and COLT have worse performance than OURS for large $ \\epsilon $ since they overfit to small $ \\epsilon $ during $ \\epsilon $ -scheduling . We summarize the VE ( verification error ) as follows : - * * OURS ( 2.36 ) * * / CAP ( 3.19 ) / COLT ( 2.9 ) for $ \\epsilon=0.1 $ on MNIST - * * OURS ( 9.79 ) * * / CAP ( 47.85 ) / COLT ( 14.3 ) for $ \\epsilon=0.3 $ on MNIST - OURS ( 49.42 ) / CAP ( 48.50 ) / * * COLT ( 39.5 ) * * for $ \\epsilon=2/255 $ on CIFAR10 - * * OURS ( 69.70 ) * * / CAP ( 73.02 ) / COLT ( 72.5 ) for $ \\epsilon=8/255 $ on CIFAR10 - We also note that COLT additionally uses MILP and [ Ehlers ] to compute the VE ( which takes roughly 2 days for CIFAR-10 test data ) , and thus it might be unfair to compare with this value directly . We will update the paper with the above discussion with COLT . - C2-3 : It is unclear how fast the proposed method is . No comparison appears to be made on training speed or memory usage compared with other systems . Given comparable accuracy and verified error results , I would not see a reason to use this system if this one is slower . - A2-3 : We will update the complete comparison results soon . Our method is not as fast as IBP and CROWN-IBP ( x1/2 ) , but faster than CAP ( x6-7 ) and COLT . We note the approximate speedup in parentheses . In the case of COLT , it is hard to compare the speed since they only use a 4-layer small network ( we use 7-layer ) and they require 4 stages ( one stage for each layer ) of training with 200 epochs each . Anyhow , COLT is slower than CAP . In addition , our method requires more memory than IBP and CROWN-IBP but much less than CAP and COLT . Again , we emphasize that our focus is on understanding certifiable training , and thus speed and memory efficiency are not our focus unless it does not run ."}, "1": {"review_id": "lvXLfNeCQdK-1", "review_text": "This paper studies why training with looser bounds ( IBP ) can outperform tighter linear relaxation based methods in certified defense . The authors argue that this is because IBP has a smoother loss landscape compared to linear relaxation based methods . Then the paper proposes to optimize the lower bound in the CROWN relaxation for unstable ReLU neurons during training , for tighter bounds and a smoother loss landscape . Pros : * This paper studies an important problem in certified defense and tries to improve linear relaxation based certified defense methods , especially CROWN . * This paper novelly proposes to optimize the lower bound in the relaxation for unstable ReLU neurons to train tighter bounds in certified defense . * The paper showed some improvement on test verifier error , compared to CROWN-IBP ( 1- > 1 ) , and it also showed improvement on tightness and loss smoothness . Cons : * The link between the motivation and the proposed method is not well justified . Paragraph \u201c Favorable Landscape \u201d somewhat explains the connection between optimizing the lower bound in relaxation and improving loss landscape . But it states that the benefit comes from preferring dead ReLU neurons to unstable ones . If the proposed method is actually trying to make more unstable neurons dead , the paper does not discuss or compare with paper Xiao et al. , 2018 ( https : //arxiv.org/pdf/1809.03008.pdf ) which directly adds a regularization to induce ReLU stability . * It does not seem to make sense that the proposed method can prefer to have dead neurons via optimizing the relaxation for unstable neurons . With such an optimization , bounds of unstable neurons become tighter , so how can such tighter bounds make the model favor unstable neurons less than CROWN-IBP with relatively looser bounds ? * As the paper mentions that CROWN-IBP does not penalize unstable ReLU neurons with ( |u| < =|l| ) , but how about just make the lower bound equal to the upper bound as Fast-lin ( Weng et al. , 2018 , https : //arxiv.org/abs/1804.09699 ) does , so the lower bound is non-zero for unstable neurons ? * An improvement over CROWN-IBP ( 1- > 1 ) is shown in the paper . However , the proposed method does not make enough improvement over CROWN-IBP ( 1- > 0 ) . The improvement on CIFAR10 with eps=8/255 or 16/255 is very small ( error 69.70 v.s.69.97 , 77.87 v.s . 77.88 ) , while there is a large performance drop on MNIST with eps=0.3/0.4 ( increased error 7.07- > 9.79 and 12.35- > 15.42 ) . What will the performance of the proposed method look like if a changing $ \\beta $ is used in training ? * This paper missed some previous works , e.g. , Xiao et al. , 2018 and Weng et al. , 2018 as mentioned above . More discussions or comparisons with them may be needed . Update after rebuttal : Thanks for the detailed author response . I think the paper is interesting in providing an analysis on how optimizing the linear relaxation in CROWN ( although the verification method itself seems to be similar as the one in Fastened CROWN in AAAI 2020 ) can lead to better loss smoothness and tightness , which seems to improve the performance of certified training . The author replies have addressed some of my concerns in my initial review . However , there are still some outstanding concerns : 1 . After more consideration , I think the \u201c More favorable landscape \u201d paragraph is still insufficient to address the second point in my above cons . The author response argues that some \u201c $ ( p/q ) $ \u201d have looser or tighter bounds , but these are considered for relaxation * locally * , not the tightness on the final output , while the relaxation optimized in the paper is to tighten the final output . Thus it remains unclear why tighter final bounds with improvement from the unstable ReLU neurons makes the model favor unstable neurons less . 2.AnonReviewer2 has reminded me that the \u201c Fastened CROWN \u201d work had a similar method about optimizing the lower bound of the linear relaxation in verification , which seems to be very similar to the method proposed in this paper , in terms of the verification part in certified training . Although this paper focuses on certified training and has some different analysis , the major modification on the method is still on the verification part , and thus I agree that a discussion on the comparison with Fastened CROWN should not be missed . The authors did not add it in the discussion period . 3.It is promising that the proposed method outperformed the modified CROWN-IBP ( $ \\beta=1 $ ) and IBP , and there seems to be a significant margin . But the proposed method fails to make a significant improvement compared to the original CROWN-IBP ( the 1- > 0 one ) , e.g. , the improvement on CIFAR-10 eps=8/255 or 16/255 is negligible . Overall , I am keeping my recommendation as rejection for the current version of the manuscript .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We will be updating comments for the other feedback soon ( C3-1 , C3-2 , C3-5 ) . It requires some additional experiments and we are working on it . Thank you for pointing this out . - C3-3 : As the paper mentions that CROWN-IBP does not penalize unstable ReLU neurons with ( |u| < =|l| ) , but how about just make the lower bound equal to the upper bound as Fast-lin ( Weng et al. , 2018 , [ https : //arxiv.org/abs/1804.09699 ] ( https : //arxiv.org/abs/1804.09699 ) ) does , so the lower bound is non-zero for unstable neurons ? - A3-3 : With the lower bound equal to the upper bound ( $ \\underline { a } =\\frac { u^+ } { u^+-l^- } =\\overline { a } ) $ as Fast-lin , it yields the same as `` * * CAP * * -IBP '' bound in Fig3 which has a looser upper bound than OURS and CROWN-IBP . It may have better smoothness , but it has worse tightness . We will check on that . - C3-4 : An improvement over CROWN-IBP ( 1- > 1 ) is shown in the paper . However , the proposed method does not make enough improvement over CROWN-IBP ( 1- > 0 ) . The improvement on CIFAR10 with eps=8/255 or 16/255 is very small ( error 69.70 v.s.69.97 , 77.87 v.s . 77.88 ) , while there is a large performance drop on MNIST with eps=0.3/0.4 ( increased error 7.07- > 9.79 and 12.35- > 15.42 ) . What will the performance of the proposed method look like if a changing is \u03b2 used in training ? - A3-4 : Table 3 in Section J shows the results with different $ \\beta $ -schedulings . It did n't improve performance . We will add the results on MNIST . - cf.A2-1 : Compared to CROWN-IBP $ \\_ { 1\u21920 } $ , OURS shows only marginal improvement or even worse in the case of $ \\epsilon=0.3 , 0.4 $ on MNIST . There is some explanation of why this $ \\beta $ -scheduling ( 1\u21920 ) in ( 3 ) ( from CROWN-IBP $ \\_ { 1\u21921 } $ to IBP ) can improve the performance [ CROWN-IBP ] . We can add on it with our understanding that CROWN-IBP $ \\_ { 1\u21920 } $ starts with a tighter bound ( $ \\beta=1 $ , CROWN-IBP only ) but not overfits to small perturbation by gradually introducing the IBP objective which has a smoother landscape . We note that IBP has the most smooth landscape compared to others ( Theorem1 ) because IBP has a relaxed gradient approximation $ g=0 $ . We will update the paper with the above discussion . [ CROWN-IBP ] Zhang et al.2019 ( https : //arxiv.org/abs/1906.06316 )"}, "2": {"review_id": "lvXLfNeCQdK-2", "review_text": "In this paper , the authors studied the role of loss landscape in training certifiable robust models . The authors reviewed linear relaxation based methods , and showed that Interval Bound Propagation ( IBP ) is a special case of linear relaxation based methods . Although linear relaxation based methods have a tighter bound on worst case loss with adversarial perturbations than IBP based method , the authors found in numerical studies that towards the end of training , IBP outperforms linear relaxation based methods . The authors hypothesized that this was because IBP loss landscape was more smooth , which helped optimization . The authors demonstrated in a theorem that IBP loss was indeed more smooth under certain assumptions . Based on this insight , the authors proposed a favorable landscape method . The authors showed in numerical studies that the sum over the worst-case margin for each class is lowest for their method . The loss of their method is also the most smooth among competing methods . Their method achieved a consistent performance in a range of perturbations , which is not achieved in competing methods . The paper is clearly written . The insight is interesting , and could help future researchers . On page 4 , in Figure 1 , what is the mathematical definition of `` variation '' ? Why does the variation of CROWN-IBP first increase , and then decrease ? A related question is why does the proportion of unstable ReLu first increase and then decrease in CROWN-IBP ? Does it have something to do with the curvature shown in Figure 1 ( right ) ? One page 7 , it would be great to also show the comparison of training time between these methods .", "rating": "7: Good paper, accept", "reply_text": "Q1-1 : On page 4 , in Figure 1 , what is the mathematical definition of `` variation '' ? - A1-1 : The definition of variation is defined in ( 8 ) in Appendix B as follows : $ \\mathcal { L } ( \\overline { s } ( \\theta ( t ) ) ) \\text { where } \\mathcal { L } ( \\overline { s } ( \\theta ) ) \\equiv\\mathcal { L } ( \\overline { s } ( x , y , \\epsilon ; \\theta ) , y ) \\text { and } \\theta ( t ) \\equiv \\theta_0-t\\eta\\nabla_\\theta \\mathcal { L } ( \\overline { s } ( \\theta_0 ) ) \\text { for } t\\in [ 0,5 ] $ It is the range of the loss values along the gradient descent direction with the stepsize $ \\in [ 0,5 ] \\times\\eta $ where $ \\eta $ is the learning rate . Q1-2 : Why does the variation of CROWN-IBP first increase , and then decrease ? - A1-2 : It is hard to tell why they show such behavior . Certifiable training prefers stable ( dead/alive ) ReLUs ( 97-99\\ % ) to unstable ReLUs ( 1-3\\ % ) since unstable ReLUs make the upper bound looser ( see Fig 11 ) because of the relaxation in unstable ReLU . However , without unstable ReLU , the model is ( * provably/certifiably * ) linear under the perturbation and has very low expressive power . Therefore , the two factors conflict with each other . We hypothesize that as CROWN-IBP $ _ { 1\\rightarrow1 } $ approaches to a flatter local minimum , the model is stabilized and focuses on reducing unstable ReLUs . This behavior is likely to be related to the curvature in Figure 1 ( right ) . Near a flatter minimum , the curvature becomes flattened and loss variations are reduced . Q1-3 : A related question is why does the proportion of unstable ReLu first increase and then decrease in CROWN-IBP ? - cf ) A1-2 Q1-4 : Does it have something to do with the curvature shown in Figure 1 ( right ) ? - cf ) A1-2 Q1-5 : One page 7 , it would be great to also show the comparison of training time between these methods . - A1-5 : Roughly speaking , our method is not as fast as IBP and CROWN-IBP ( x1/2 ) , but faster than CAP ( x6-7 ) . We note the approximate speedup in parentheses . We will update complete comparison results soon ."}}