{"year": "2019", "forum": "rk4Qso0cKm", "title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "decision": "Accept (Poster)", "meta_review": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n", "reviews": [{"review_id": "rk4Qso0cKm-0", "review_text": "After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. I believe this is a strong paper and it represents an interesting contribution for the community. Still things to fix: a) a dataset used in 4.2 is not stated b) missing articles, for example, p.5 \".In practice, however, we need a weaker regularization for A small dataset or A large model\" c) upper case at the beginning of a sentence after question: p.8 \"Is our Adv-BNN model susceptible to transfer attack? we answer\" - \"we\" -> \"We\" ==================================================================================== The paper proposes a Bayesian neural network with adversarial training as an approach for defence against adversarial attacks. Main pro: It is an interesting and reasonable idea for defence against adversarial attacks to combine adversarial training and randomness in a NN (bringing randomness into a new level in the form of a BNN), which is shown to outperform both adversarial training and random NN alone. Main con: Clarity. The paper does not crucially lack clarity but some claims, general organisation of the paper and style of quite a few sentences can be largely improved. In general, the paper is sound, the main idea appears to be novel and the paper addresses the very important and relevant problem in deep learning such as defence against adversarial attacks. Writing and general presentation can be improved especially regarding Bayesian neural networks, where some clarity issues almost become quality issues. Style of some sentences can be tuned to more formal. In details: 1. The organisation of Section 1.1 can be improved: a general concept \"Attack\" and specific example \"PGD Attack\" are on the same level of representation, while it seems more logical that \"PGD Attack\" should be a subsection of \"Attack\". And while there is a paragraph \"Attack\" there is no paragraph \"Defence\" but rather only specific examples 2. The claim \u201cwe can either sample w \u223c p(w|x, y) efficiently without knowing the closed-form formula through the method known as Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011)\u201d sounds like SGLD is the only sampling method for BNN, which is not true, see, e.g., Hamiltonian Monte Carlo (Neal\u2019s PhD thesis 1994). It is better to be formulated as \"through, for example, the method ...\" 3. Issues regarding eq. (7): a) Why there is an expectation over (x, y)? There should be the joint probability of all (x, y) in the evidence. b) Could the authors add more details about why it is the ELBO given that it is unconventional with adversarial examples added? c) It seems that it should be log p(y | x^{adv}, \\omega) rather than p(x^{adv}, y | \\omega). d) If the authors assume noise component, i.e., y = f(x; \\omega) + \\epsilon, then they do not need to have a compulsory Softmax layer in their network, which is important, for example, for regression models. Then the claim \u201cour Adv-BNN method trains an arbitrary Bayesian neural network\u201d would be more justified 4. It would make the paper more self-contained if the Bayes by Backprop algorithm would be described in more details (space can be taken from the BNN introduction). And it seems to be a typo that it is Bayes by Backprop rather than Bayes by Prop 5. There are missing citations in the text: a) no models from NIPS 2017 Adversarial Attack and Defence competition (Kurakin et al. 2018) are mentioned b) citation to justify the claim \u201cC&W attack and PGD attack (mentioned below) have been recognized as two state-of-the-art white-box attacks for image classification task\u201d c) \u201cwe can approximate the true posterior p(w|x, y) by a parametric distribution q_\u03b8(w), where the unknown parameter \u03b8 is estimated by minimizing KL(q_\u03b8(w) || p(w|x, y)) over \u03b8\u201d - there are a variety of works in approximate inference in BNN, it would be better to cite some of them here d) citation to justify the claim \"although in these cases the KL-divergence of prior and posterior is hard to compute and practically we replace it with the Monte-Carlo estimator, which has higher variance, resulting in slower convergence rate.\u201d 6. The goal and result interpretation of the correlation experiment is not very clear 7. From the presentation of Figure 4 it is unclear that this is a distribution of standard deviations of approximated posterior. 8. \u201cTo sum up, our Adv-BNN method trains an arbitrary Bayesian neural network with the adversarial examples of the same model\u201d \u2013 unclear which same model is meant 9. \"Among them, there are two lines of work showing effective results on medium-sized convolutional networks (e.g., CIFAR-10)\" - from this sentence it looks like CIFAR-10 is a network rather than a dataset 10. In \"Notations\" y introduction is missing 11. It is better to use other symbol for perturbation rather than \\boldsymbol\\delta since \\delta is already used for the Dirac delta function 12. \u201cvia tuning the coefficient c in the composite loss function\u201d \u2013 the coefficient c is never introduced Minor: 1. There are a few missing articles, for example, in Notations, \u201cIn this paper, we focus on the attack under THE norm constraint\u2026\u201d 2. Kurakin et al. (2017) is described in the past tense whereas Carlini & Wagner (2017a) is described in the present tense 3. Inner brackets in eq. (2) are bigger than outer brackets 4. In eq. (11) $\\delta$ is not bold 5. In eq. (12) it seems that the second and third terms should have \u201c-\u201d rather than \u201c+\u201d 6. Footnote in page 6 seems to be incorrectly labelled as 1 instead of 2 ", "rating": "7: Good paper, accept", "reply_text": "Please see the revised paper as well as the change list for details , below we address your comments . We find your comments very informative and we absorbed most of them in the new version . 1.We revised Section 1.1 following your suggestions . Specifically , we merged the PGD attack into the Attack part , and we also modified the defense part in the same way . 2.Thanks for pointing out this mistake , we agree that we left HMC behind when writing the initial draft , we modified this sentence as suggested . 3 . ( a ) We are indeed meant to it , we changed a lot to eq . ( 7 ) in response to the suggestions of all reviewers , I hope the revised version is clearer . ( b ) We added more details why the new objective function is still an ELBO in the updated version , briefly speaking , we made a lower bound of the original ELBO , and the lower bound of ELBO is still an evidence lower bound . ( c ) Good point , we modified the expression in the revision , thanks for pointing out . ( d ) It is a very good suggestion , adding an error term makes our model more general to both regression and classification problems , thanks ! 4.We added a brief introduction to Bayes by Backprop . The space is really limited so forgive us if you find this part hands-waving . 5.We added more citations to support our claims 6 . We gave the motivation of this experiment in section 4.2 . The goal of this experiment is to test the robustness under black-box attack , specifically we answer the question : \u201c How does the Adv-BNN perform under transfer attack from other models ? \u201d and the key finding is our AdvBNN model is also very robust to blackbox attack , no matter which the source model is . Blackbox defense is also a very important task because in reality , attackers may not have access to the target model . 7.We agree with Reviewer 2 that Section 3.3 is not necessary and not quite relevant to the main point of this paper , so we removed this subsection . Instead , we added two other experiments aiming at showing the sample efficiency as well as the robustness of our model . 8 ~ 12.Thanks for pointing out our mistakes , we fixed all the typos and unclear parts as suggested . About your minor points -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 1 , 2 , 3 , 4 , 6 : Thanks for pointing out our typos ! We fixed all of them in the revised paper . 5 : I think Eq . ( 12 ) should be the plus sign , because we are doing the Taylor expansion : f ( x+\\delta , w ) ~ f ( x ) +\\delta^T \\nabla f ( x ) + ..."}, {"review_id": "rk4Qso0cKm-1", "review_text": "I have read the feedback and discussed with the authors on my concerns for a few rounds. The revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments. I have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. I have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score \"6\" reflects my opinion that the approach is not principled. ========================================================= Thank you for an interesting read. The paper proposes training a Bayesian neural network (BNN) with adversarial training. To the best of my knowledge the idea is new (although from my perspective is quite straight-forward, but see some discussions below). The paper is well written and easy to understand. Experimental results are promising, but I don't understand how the last experiment relates to the main idea, see comments below. There are a few issues to be addressed in revision: 1. The paper seems to have ignored many papers in BNN literature on defending adversarial attacks. See e.g. [1][2][3][4] and papers citing them. In fact robustness to adversarial attacks is becoming a standard test case for developing approximate inference on Bayesian neural networks. This means Figure 2 is misleading as in the paper \"BNN\" actually refers to BNN with mean-field variational Gaussian approximations. 2. Carlini and Wagner (2017a) has discussed a CW-based attack that can increase the success rate of attack on (dropout) BNNs, which can be easily transferred to a corresponding PGD version. Essentially the PGD attack tested in the paper does not assume the knowledge of BNN, let alone the adversarial training. This seems to contradict to the pledge in Athalye et al. that the defence method should be tested against an attack that is aware of the defence. 3. I am not exactly sure if equation 7 is the most appropriate way to do adversarial training for BNNs. From a modelling perspective, if we can do Bayesian inference exactly, then after marginalisation of w, the model does NOT assume independence between datapoints. This means if we want to attack the model, then we need to do \\min_{||\\delta_x|| < \\gamma} log p(D_adv), D_adv = {(x + \\delta_x, y) | (x, y) \\sim \\sim D_tr}, log p(D_adv) = \\log \\int \\prod_{(x, y) \\sim D_tr} p(y|x + \\delta_x, w) p(w) dw. Now the model evidence log p(D_adv) is intractable and you resort to variational lower-bound. But from the above equation we can see the lower bound writes as \\min_{||\\delta_x|| < \\gamma} \\max_{q} E_{q} [\\sum_{(x, y) \\sim D_tr} \\log p(y|x + \\delta_x, w) ] - KL[q||p], which is different from your equation 7. In fact equation 7 is a lower-bound of the above, which means the adversaries are somehow \"weakened\". 4. I am not exactly sure the purpose of section 3.3. True, that variational inference has been used for compressing neural networks, and the experiment in section 3.3 also support this. However, how does network pruning relate to adversarial robustness? I didn't see any discussion on this point. Therefore section 3.3 seems to be irrelevant to the paper. Some papers on BNN's adversarial robustness: [1] Li and Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. ICML 2017 [2] Feinman et al. Detecting Adversarial Samples from Artifacts. arXiv:1703.00410 [3] Louizos and Welling. Multiplicative Normalizing Flows for Variational Bayesian Neural Networks. ICML 2017 [4] Smith and Gal. Understanding Measures of Uncertainty for Adversarial Example Detection. UAI 2018", "rating": "6: Marginally above acceptance threshold", "reply_text": "Please see the revised paper as well as the change list for details , we believe that the revised paper has already addressed the issues . Below we give more details on that , 1 . The references you mentioned are indeed very relevant to our topic , we discussed some of them in the introduction section . However , we still think it does not diminish the main contributions of our paper due to the following reasons : i ) [ 1 ] includes one small scale experiment on MNIST dataset , the goal is to show that although the Bayesian NN is still easily \u201c cheated \u201d by adversarial images , the uncertainty of predictions also increases . Meaning the Bayesian NN is aware of the epistemic uncertainty . And the authors explored this nice property in adversarial detection . Similar to [ 1 ] , the experiments in [ 3 ] are still small scale ( MNIST/CIFAR10 ) , although the paper shows that the Bayesian NN has stronger adversarial robustness than a plain NN , the authors also admit that \u201c adversarial examples are harder to escape and be uncertain about in CIFAR10 , due to higher dimension \u201d . In contrast , our proposed AdvBNN has made a huge progress in adversarial robustness : the accuracy under strong adversarial attack algorithm on even more complex , high dimensional datasets is much higher than baselines ( including the Bayesian NN ) . ii ) [ 2 ] and [ 4 ] are both on adversarial detection , while our focus is the adversarial defense , these are similar topics but different scenarios . Yes , perhaps it is not very suitable to call `` BNN with factorized gaussian as approximated posterior '' simply as BNN , because it does not include the previous works on BNN + adversarial attacks . But it is very straightforward to extend our work to include other inference methods . I think the major contribution of our work is that we show Bayesian neural networks empower the robustness of adversarially trained neural networks . Moreover , we demonstrate that even the most simple approximate inference method can benefit a lot to model robustness , and our method scales easily to large datasets ( not just MNIST ) . 2.In fact we already assumed the attacker knows the structure of BNN in our setting ( using the same approach in Carlini and Wagner ( 2017a ) and Athalye et al ) . We briefly mentioned this in Section 3.1 in the initial version , and we have added more details in the revised version ( see Appendix ) . Therefore , as you can see in our Figure 2 that BNN has a very low accuracy under attack in all datasets , which does not contradict to Athalye et al.We also use the same attack ( assume the adversary knows every details of model ) to test the robustness for the proposed AdvBNN model . Our conclusion is that BNN itself does not help much , but using the proposed framework , one can combine the idea of BNN with adversarial training to achieve much better robustness . Athalye et al.does not negate the effectiveness of adversarial training , for detailed information , please refer to their Github page : https : //github.com/anishathalye/obfuscated-gradients , there is a table comparing the performance of different methods , among them , the adversarial training ( Madry et al ) has a pretty good accuracy . 3.We are not quite sure if we understand your point , do you mean the actual objective function should be \\min_ { ||\\delta_x|| } \u2026 . \\max_ { q } .... while our objective function is \\max_ { q } \u2026\u2026 \\min_ { ||\\delta_x|| } \u2026\u2026 and so you think Eq 7 is an lower bound of your equation ? Our objective function Eq 7 is indeed a lower bound of your proposed equation , this is because we are maximizing the \u201c worst case \u201d evidence lower bound . So the \\max_ { q } should be moved to the leftmost position . In summary , in training the model , we need to do \\max_q \\min_ { ||\\delta_x|| < \\gamma } log p ( D_adv ) , where log p ( D_adv ) = \\log \\int \\prod_ { ( x , y ) \\sim D_tr } p ( y|x + \\delta_x , w ) p ( w ) dw This can be further simplified to our objective function . We have added more details in the revised paper to make it clearer . 4.Sorry about the confusion , we also think section 3.3 is diverged from the main topic , in the revised paper , we replaced this experiment with other controlled experiments . We hope these experiments can strengthen our findings ."}, {"review_id": "rk4Qso0cKm-2", "review_text": "The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). The proposed method defines a generative process that ties the prediction output and the adversarial input pattern via a set of shared neural net weights. These weights are then assinged a prior and the resultant posterior is approximated by variational inference. Strength: * The proposed approach is incremental, but anyway novel. * The results are groundbreaking. * There are some technical flaws in the way the method has been presented, but the rest of the paper is very well-written. Major Weaknesses: * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\\sigma)). * Second, the constraint that x_adv should lie within the \\gamma-ball of x has some implications on the validity of the Jensen's inequality, which relates Equation 7 to proper posterior inference. * Blundell et al.'s algorithm should be renamed to \"Bayes-by-BACKprop\". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience. And Some Minor Issues: * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles. * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python. Overall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for valuing our paper and giving informative suggestions , below we address your comments in detail . Major Weaknesses : 1 . Thanks for pointing out this mistake , this issue is also noticed by AnonReviewer 2 and we have already fixed it in the revised paper . And perhaps it will be clearer to think our objective function as an expectation on the original data , rather than on x_adv . Because the new objective function is a lower bound of the original ELBO . 2.The evidence is not calculated on x_adv , but on the original data $ x $ . So it does not interfere with Jensen 's ineq.when deriving the ELBO . I think it will be clearer to see the revised paper , where we give more details regarding the objective function . 3.We have renamed to `` Bayes by Backprop '' in the revised version . We agree with you that the local reparamterization trick has much smaller variance during the training time , replacing Bayes-by-Backprop by local reparameterization trick will definitely have a faster convergence . The reason is that we didn \u2019 t think very carefully at the implementation stage and somehow `` forgot '' it . Nevertheless , both algorithms should yield similar results and we will definitely try this idea and replace the code base . We want to address that our main goal is to combine Bayesian NN with adversarial training , and there are many ways we could do the approximate inference efficiently . Here we only choose a naive approach considering its simplicity and effectiveness . In the revised paper , we give readers a reminder that local reparametrization trick should perform better . Minor issues : 1 . The original introduction includes intro , background and related work . We have split it into 2 sections and shortened each of them . We have also added more details in the proposed method section . 2.Thanks for your suggestion , we rewrite the algorithm box to pseudo code in order to make it looks more formal ."}], "0": {"review_id": "rk4Qso0cKm-0", "review_text": "After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. I believe this is a strong paper and it represents an interesting contribution for the community. Still things to fix: a) a dataset used in 4.2 is not stated b) missing articles, for example, p.5 \".In practice, however, we need a weaker regularization for A small dataset or A large model\" c) upper case at the beginning of a sentence after question: p.8 \"Is our Adv-BNN model susceptible to transfer attack? we answer\" - \"we\" -> \"We\" ==================================================================================== The paper proposes a Bayesian neural network with adversarial training as an approach for defence against adversarial attacks. Main pro: It is an interesting and reasonable idea for defence against adversarial attacks to combine adversarial training and randomness in a NN (bringing randomness into a new level in the form of a BNN), which is shown to outperform both adversarial training and random NN alone. Main con: Clarity. The paper does not crucially lack clarity but some claims, general organisation of the paper and style of quite a few sentences can be largely improved. In general, the paper is sound, the main idea appears to be novel and the paper addresses the very important and relevant problem in deep learning such as defence against adversarial attacks. Writing and general presentation can be improved especially regarding Bayesian neural networks, where some clarity issues almost become quality issues. Style of some sentences can be tuned to more formal. In details: 1. The organisation of Section 1.1 can be improved: a general concept \"Attack\" and specific example \"PGD Attack\" are on the same level of representation, while it seems more logical that \"PGD Attack\" should be a subsection of \"Attack\". And while there is a paragraph \"Attack\" there is no paragraph \"Defence\" but rather only specific examples 2. The claim \u201cwe can either sample w \u223c p(w|x, y) efficiently without knowing the closed-form formula through the method known as Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011)\u201d sounds like SGLD is the only sampling method for BNN, which is not true, see, e.g., Hamiltonian Monte Carlo (Neal\u2019s PhD thesis 1994). It is better to be formulated as \"through, for example, the method ...\" 3. Issues regarding eq. (7): a) Why there is an expectation over (x, y)? There should be the joint probability of all (x, y) in the evidence. b) Could the authors add more details about why it is the ELBO given that it is unconventional with adversarial examples added? c) It seems that it should be log p(y | x^{adv}, \\omega) rather than p(x^{adv}, y | \\omega). d) If the authors assume noise component, i.e., y = f(x; \\omega) + \\epsilon, then they do not need to have a compulsory Softmax layer in their network, which is important, for example, for regression models. Then the claim \u201cour Adv-BNN method trains an arbitrary Bayesian neural network\u201d would be more justified 4. It would make the paper more self-contained if the Bayes by Backprop algorithm would be described in more details (space can be taken from the BNN introduction). And it seems to be a typo that it is Bayes by Backprop rather than Bayes by Prop 5. There are missing citations in the text: a) no models from NIPS 2017 Adversarial Attack and Defence competition (Kurakin et al. 2018) are mentioned b) citation to justify the claim \u201cC&W attack and PGD attack (mentioned below) have been recognized as two state-of-the-art white-box attacks for image classification task\u201d c) \u201cwe can approximate the true posterior p(w|x, y) by a parametric distribution q_\u03b8(w), where the unknown parameter \u03b8 is estimated by minimizing KL(q_\u03b8(w) || p(w|x, y)) over \u03b8\u201d - there are a variety of works in approximate inference in BNN, it would be better to cite some of them here d) citation to justify the claim \"although in these cases the KL-divergence of prior and posterior is hard to compute and practically we replace it with the Monte-Carlo estimator, which has higher variance, resulting in slower convergence rate.\u201d 6. The goal and result interpretation of the correlation experiment is not very clear 7. From the presentation of Figure 4 it is unclear that this is a distribution of standard deviations of approximated posterior. 8. \u201cTo sum up, our Adv-BNN method trains an arbitrary Bayesian neural network with the adversarial examples of the same model\u201d \u2013 unclear which same model is meant 9. \"Among them, there are two lines of work showing effective results on medium-sized convolutional networks (e.g., CIFAR-10)\" - from this sentence it looks like CIFAR-10 is a network rather than a dataset 10. In \"Notations\" y introduction is missing 11. It is better to use other symbol for perturbation rather than \\boldsymbol\\delta since \\delta is already used for the Dirac delta function 12. \u201cvia tuning the coefficient c in the composite loss function\u201d \u2013 the coefficient c is never introduced Minor: 1. There are a few missing articles, for example, in Notations, \u201cIn this paper, we focus on the attack under THE norm constraint\u2026\u201d 2. Kurakin et al. (2017) is described in the past tense whereas Carlini & Wagner (2017a) is described in the present tense 3. Inner brackets in eq. (2) are bigger than outer brackets 4. In eq. (11) $\\delta$ is not bold 5. In eq. (12) it seems that the second and third terms should have \u201c-\u201d rather than \u201c+\u201d 6. Footnote in page 6 seems to be incorrectly labelled as 1 instead of 2 ", "rating": "7: Good paper, accept", "reply_text": "Please see the revised paper as well as the change list for details , below we address your comments . We find your comments very informative and we absorbed most of them in the new version . 1.We revised Section 1.1 following your suggestions . Specifically , we merged the PGD attack into the Attack part , and we also modified the defense part in the same way . 2.Thanks for pointing out this mistake , we agree that we left HMC behind when writing the initial draft , we modified this sentence as suggested . 3 . ( a ) We are indeed meant to it , we changed a lot to eq . ( 7 ) in response to the suggestions of all reviewers , I hope the revised version is clearer . ( b ) We added more details why the new objective function is still an ELBO in the updated version , briefly speaking , we made a lower bound of the original ELBO , and the lower bound of ELBO is still an evidence lower bound . ( c ) Good point , we modified the expression in the revision , thanks for pointing out . ( d ) It is a very good suggestion , adding an error term makes our model more general to both regression and classification problems , thanks ! 4.We added a brief introduction to Bayes by Backprop . The space is really limited so forgive us if you find this part hands-waving . 5.We added more citations to support our claims 6 . We gave the motivation of this experiment in section 4.2 . The goal of this experiment is to test the robustness under black-box attack , specifically we answer the question : \u201c How does the Adv-BNN perform under transfer attack from other models ? \u201d and the key finding is our AdvBNN model is also very robust to blackbox attack , no matter which the source model is . Blackbox defense is also a very important task because in reality , attackers may not have access to the target model . 7.We agree with Reviewer 2 that Section 3.3 is not necessary and not quite relevant to the main point of this paper , so we removed this subsection . Instead , we added two other experiments aiming at showing the sample efficiency as well as the robustness of our model . 8 ~ 12.Thanks for pointing out our mistakes , we fixed all the typos and unclear parts as suggested . About your minor points -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 1 , 2 , 3 , 4 , 6 : Thanks for pointing out our typos ! We fixed all of them in the revised paper . 5 : I think Eq . ( 12 ) should be the plus sign , because we are doing the Taylor expansion : f ( x+\\delta , w ) ~ f ( x ) +\\delta^T \\nabla f ( x ) + ..."}, "1": {"review_id": "rk4Qso0cKm-1", "review_text": "I have read the feedback and discussed with the authors on my concerns for a few rounds. The revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments. I have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. I have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score \"6\" reflects my opinion that the approach is not principled. ========================================================= Thank you for an interesting read. The paper proposes training a Bayesian neural network (BNN) with adversarial training. To the best of my knowledge the idea is new (although from my perspective is quite straight-forward, but see some discussions below). The paper is well written and easy to understand. Experimental results are promising, but I don't understand how the last experiment relates to the main idea, see comments below. There are a few issues to be addressed in revision: 1. The paper seems to have ignored many papers in BNN literature on defending adversarial attacks. See e.g. [1][2][3][4] and papers citing them. In fact robustness to adversarial attacks is becoming a standard test case for developing approximate inference on Bayesian neural networks. This means Figure 2 is misleading as in the paper \"BNN\" actually refers to BNN with mean-field variational Gaussian approximations. 2. Carlini and Wagner (2017a) has discussed a CW-based attack that can increase the success rate of attack on (dropout) BNNs, which can be easily transferred to a corresponding PGD version. Essentially the PGD attack tested in the paper does not assume the knowledge of BNN, let alone the adversarial training. This seems to contradict to the pledge in Athalye et al. that the defence method should be tested against an attack that is aware of the defence. 3. I am not exactly sure if equation 7 is the most appropriate way to do adversarial training for BNNs. From a modelling perspective, if we can do Bayesian inference exactly, then after marginalisation of w, the model does NOT assume independence between datapoints. This means if we want to attack the model, then we need to do \\min_{||\\delta_x|| < \\gamma} log p(D_adv), D_adv = {(x + \\delta_x, y) | (x, y) \\sim \\sim D_tr}, log p(D_adv) = \\log \\int \\prod_{(x, y) \\sim D_tr} p(y|x + \\delta_x, w) p(w) dw. Now the model evidence log p(D_adv) is intractable and you resort to variational lower-bound. But from the above equation we can see the lower bound writes as \\min_{||\\delta_x|| < \\gamma} \\max_{q} E_{q} [\\sum_{(x, y) \\sim D_tr} \\log p(y|x + \\delta_x, w) ] - KL[q||p], which is different from your equation 7. In fact equation 7 is a lower-bound of the above, which means the adversaries are somehow \"weakened\". 4. I am not exactly sure the purpose of section 3.3. True, that variational inference has been used for compressing neural networks, and the experiment in section 3.3 also support this. However, how does network pruning relate to adversarial robustness? I didn't see any discussion on this point. Therefore section 3.3 seems to be irrelevant to the paper. Some papers on BNN's adversarial robustness: [1] Li and Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. ICML 2017 [2] Feinman et al. Detecting Adversarial Samples from Artifacts. arXiv:1703.00410 [3] Louizos and Welling. Multiplicative Normalizing Flows for Variational Bayesian Neural Networks. ICML 2017 [4] Smith and Gal. Understanding Measures of Uncertainty for Adversarial Example Detection. UAI 2018", "rating": "6: Marginally above acceptance threshold", "reply_text": "Please see the revised paper as well as the change list for details , we believe that the revised paper has already addressed the issues . Below we give more details on that , 1 . The references you mentioned are indeed very relevant to our topic , we discussed some of them in the introduction section . However , we still think it does not diminish the main contributions of our paper due to the following reasons : i ) [ 1 ] includes one small scale experiment on MNIST dataset , the goal is to show that although the Bayesian NN is still easily \u201c cheated \u201d by adversarial images , the uncertainty of predictions also increases . Meaning the Bayesian NN is aware of the epistemic uncertainty . And the authors explored this nice property in adversarial detection . Similar to [ 1 ] , the experiments in [ 3 ] are still small scale ( MNIST/CIFAR10 ) , although the paper shows that the Bayesian NN has stronger adversarial robustness than a plain NN , the authors also admit that \u201c adversarial examples are harder to escape and be uncertain about in CIFAR10 , due to higher dimension \u201d . In contrast , our proposed AdvBNN has made a huge progress in adversarial robustness : the accuracy under strong adversarial attack algorithm on even more complex , high dimensional datasets is much higher than baselines ( including the Bayesian NN ) . ii ) [ 2 ] and [ 4 ] are both on adversarial detection , while our focus is the adversarial defense , these are similar topics but different scenarios . Yes , perhaps it is not very suitable to call `` BNN with factorized gaussian as approximated posterior '' simply as BNN , because it does not include the previous works on BNN + adversarial attacks . But it is very straightforward to extend our work to include other inference methods . I think the major contribution of our work is that we show Bayesian neural networks empower the robustness of adversarially trained neural networks . Moreover , we demonstrate that even the most simple approximate inference method can benefit a lot to model robustness , and our method scales easily to large datasets ( not just MNIST ) . 2.In fact we already assumed the attacker knows the structure of BNN in our setting ( using the same approach in Carlini and Wagner ( 2017a ) and Athalye et al ) . We briefly mentioned this in Section 3.1 in the initial version , and we have added more details in the revised version ( see Appendix ) . Therefore , as you can see in our Figure 2 that BNN has a very low accuracy under attack in all datasets , which does not contradict to Athalye et al.We also use the same attack ( assume the adversary knows every details of model ) to test the robustness for the proposed AdvBNN model . Our conclusion is that BNN itself does not help much , but using the proposed framework , one can combine the idea of BNN with adversarial training to achieve much better robustness . Athalye et al.does not negate the effectiveness of adversarial training , for detailed information , please refer to their Github page : https : //github.com/anishathalye/obfuscated-gradients , there is a table comparing the performance of different methods , among them , the adversarial training ( Madry et al ) has a pretty good accuracy . 3.We are not quite sure if we understand your point , do you mean the actual objective function should be \\min_ { ||\\delta_x|| } \u2026 . \\max_ { q } .... while our objective function is \\max_ { q } \u2026\u2026 \\min_ { ||\\delta_x|| } \u2026\u2026 and so you think Eq 7 is an lower bound of your equation ? Our objective function Eq 7 is indeed a lower bound of your proposed equation , this is because we are maximizing the \u201c worst case \u201d evidence lower bound . So the \\max_ { q } should be moved to the leftmost position . In summary , in training the model , we need to do \\max_q \\min_ { ||\\delta_x|| < \\gamma } log p ( D_adv ) , where log p ( D_adv ) = \\log \\int \\prod_ { ( x , y ) \\sim D_tr } p ( y|x + \\delta_x , w ) p ( w ) dw This can be further simplified to our objective function . We have added more details in the revised paper to make it clearer . 4.Sorry about the confusion , we also think section 3.3 is diverged from the main topic , in the revised paper , we replaced this experiment with other controlled experiments . We hope these experiments can strengthen our findings ."}, "2": {"review_id": "rk4Qso0cKm-2", "review_text": "The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). The proposed method defines a generative process that ties the prediction output and the adversarial input pattern via a set of shared neural net weights. These weights are then assinged a prior and the resultant posterior is approximated by variational inference. Strength: * The proposed approach is incremental, but anyway novel. * The results are groundbreaking. * There are some technical flaws in the way the method has been presented, but the rest of the paper is very well-written. Major Weaknesses: * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\\sigma)). * Second, the constraint that x_adv should lie within the \\gamma-ball of x has some implications on the validity of the Jensen's inequality, which relates Equation 7 to proper posterior inference. * Blundell et al.'s algorithm should be renamed to \"Bayes-by-BACKprop\". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience. And Some Minor Issues: * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles. * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python. Overall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for valuing our paper and giving informative suggestions , below we address your comments in detail . Major Weaknesses : 1 . Thanks for pointing out this mistake , this issue is also noticed by AnonReviewer 2 and we have already fixed it in the revised paper . And perhaps it will be clearer to think our objective function as an expectation on the original data , rather than on x_adv . Because the new objective function is a lower bound of the original ELBO . 2.The evidence is not calculated on x_adv , but on the original data $ x $ . So it does not interfere with Jensen 's ineq.when deriving the ELBO . I think it will be clearer to see the revised paper , where we give more details regarding the objective function . 3.We have renamed to `` Bayes by Backprop '' in the revised version . We agree with you that the local reparamterization trick has much smaller variance during the training time , replacing Bayes-by-Backprop by local reparameterization trick will definitely have a faster convergence . The reason is that we didn \u2019 t think very carefully at the implementation stage and somehow `` forgot '' it . Nevertheless , both algorithms should yield similar results and we will definitely try this idea and replace the code base . We want to address that our main goal is to combine Bayesian NN with adversarial training , and there are many ways we could do the approximate inference efficiently . Here we only choose a naive approach considering its simplicity and effectiveness . In the revised paper , we give readers a reminder that local reparametrization trick should perform better . Minor issues : 1 . The original introduction includes intro , background and related work . We have split it into 2 sections and shortened each of them . We have also added more details in the proposed method section . 2.Thanks for your suggestion , we rewrite the algorithm box to pseudo code in order to make it looks more formal ."}}