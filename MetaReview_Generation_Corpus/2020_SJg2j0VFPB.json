{"year": "2020", "forum": "SJg2j0VFPB", "title": "Knowledge Graph Embedding: A Probabilistic Perspective and Generalization Bounds", "decision": "Reject", "meta_review": "The paper provides a generalization error bound, which extends the results from PU learning, for the problem of knowledge graph completion. The authors assume a missing at random setting, and provide bounds on the triples (two nodes and an edge) that could be mistakes. Then the paper provides a maximum likelihood interpretation, as well as relations to existing knowledge graph completion methods. The problem setting is interesting, and the writing clear.\n\nThis discussion was extensive, with reviewers and authors following the spirit of ICLR and having a constructive discussion which resulted in improvements to the paper. However, there seems to be still some remaining improvements to be made in terms of clarity of presentation, as well as precision of the theoretical arguments.\n\nUnfortunately, there are many strong submissions, and the paper as it currently stands does not satisfy the quality threshold of ICLR.", "reviews": [{"review_id": "SJg2j0VFPB-0", "review_text": "The paper presents a study on bound of the number of the expected triples wrongly predicted by general graph embedding methods. The paper considers methods that follow the \"completely at random\" assumption and aim to maximize the likelihood (or minimize the log-loss). The study considers the learning from positive and unlabelled data problem and theoretically demonstrates the correctness of the bounds on the number of triples that embedding methods can add during the completion of the knowledge graph. The paper is well written, theorems and proofs seem to me to be mathematically sound and correct. To the best of my knowledge, I agree with the authors when they claim to be the first to inspect the problem to give bounds under the setting they have considered. However, I have to admit that I am not an expert on the problem, I am more aware of the practical part of this field and less aware of the theoretical part. However, I have carefully checked the validity of the analysis and I have not found any flawless or critical errors in the proofs. I would suggest removing aliases for |O|, |R|, etc. from the proofs as they do not simplify the formulas significantly. Other corrections: - on page 5 the word \"the\" is written twice in the sentence just before Theorem 2. - on page 7 C is defined as constant but C should be C_1. Also C_2 should be defined. - on page 7, in \"Lipschitz Continuity\" after the sentence \"Our result requires Lipschitz continuity\" there must be a full stop or the word Most must begin with a lowercase m. - on page 8 there is inside the Figure 1 (on the red line, near 1*10^4 number of objects in the domain |O|) a link to page 6 that should be removed.", "rating": "6: Weak Accept", "reply_text": "Thank you for the review ! > I would suggest removing aliases for |O| , |R| , etc . > from the proofs as they do not simplify the formulas significantly . > Other corrections : > - on page 5 the word `` the '' is written twice in the sentence just before Theorem 2 . > - on page 7 C is defined as constant but C should be C_1 . Also C_2 should be defined . - > on page 7 , in `` Lipschitz Continuity '' after the sentence `` Our result requires Lipschitz > continuity '' there must be a full stop or the word Most must begin with a lowercase m. > - on page 8 there is inside the Figure 1 ( on the red line , near 1 * 10^4 number of objects in > the domain |O| ) a link to page 6 that should be removed . We have fixed all of these in the revision . Thank you !"}, {"review_id": "SJg2j0VFPB-1", "review_text": "Summary This paper studied the knowledge completion problem from the viewpoint of statistical learning theory. More specifically, it formalized a knowledge graph completion problem as an estimation of the optimal embedding by assuming the canonical distribution derived from the embedding and derived the upper bound of misclassification rate of entities under the missing-completely-at-random assumption. Decision Although the formalization of knowledge graph completion problems is novel (to the best of my knowledge), there is much room for improvement in the organization of the paper. Therefore, I would recommend to reject the paper this time and to ask authors to revise the paper so that the paper is more accessible to researchers and engineers. There are numerous existing works about the knowledge graph embedding problem and the knowledge graph completion problem using embeddings. However, few works have been done to justify these methods theoretically. This paper attempted to answer this question via statistical learning theorem perspectives. The theorems gave sufficient conditions in terms of the size of a knowledge graph and properties of embedding functions under which the misclassification rate goes to $0$. In this aspect, this paper gave insights into how we can give justifications to existing embedding methods. However, I think it is hard for those who are not familiar with this field to follow the logic of the paper, as I write in detail in the following sections. I understand some people argue that the paper organization is not essential for how a paper contributes to science and technology. However, the accessibility to the paper is vital to promote technical communications in different fields. Also, I believe it is beneficial for the paper to maximize its value. Thus, I would ask the authors to polish the paper. Suggestions - Introduction - Please explain what knowledge graphs are in the introduction before explaining how knowledge graphs represent knowledge, or what problems of practically available knowledge graphs have. - I think it is a bit too casual and conversational to use wordings like \"Please bare in mind\" in papers. - I think it is helpful if the authors add a summary of theorems and its implication in the introduction to grasp the theoretical contributions the paper has made. - Section 6, Theorem 1 - Please describe what is the probability of $P$ and with respect to which probability distribution $\\mathbb{E}$ takes the expectation. - Section 7 - The authors claim that Pinsker's inequality plays a central role in deriving the upper bound for the ratio of wrongly predicted triples. However, we cannot know how the authors used the inequality unless we see the proof of Theorem 3, which is available in the appendix. - In Theorem 3, the definition of \"wrongly predicted triple\" is missing. I only find a description related to it in the proof of Theorem 3 in the appendix. Could you add the definition in the main part? - Section 8, Theorem 4 - The statement of Theorem 4 says that $\\mathbb{X}$ is a learned representation. But it is not available in the main article how we obtain it. We know that the authors used the maximum-likelihood estimator if we read the proof of the theorem. The author should clarify it in the statement of the theorem. - Section 10 - What does \"this\" mean in the title of the section? - In the initial reading, I could not understand the main point the authors want to address in the paragraph starting with Loss. I think authors can clarify the point by restructuring the paragraph. Minor comments - Section 2 and Section 6 - The authors treated $\\mathbb{X}$, which is the set of vector representations of objects and relations, as a lookup table in Section 2.2 and reinterpreted it as the subset of $\\mathcal{X}^{|\\mathcal{O}|+|\\mathcal{R}|}$ in Section 6. I think we can simplify the description if we treat $\\mathbb{X}$ as the subset of $\\mathcal{X}^{\\mathcal{O} \\sqcup \\mathcal{R}} := \\{f: \\mathcal{O} \\sqcup \\mathcal{R} \\to \\mathcal{X} \\}$ and denote $\\mathbb{X}(o)$ as $x_o$ in short hand for a representation $\\mathbb{X}$ and an object $o\\in \\mathcal{O}$ (and similarly for $x_r$). - Section 4 - It would be helpful to make the correspondence explicit between each sentence and theorem later in the last paragraph. For example, \"We prove a generalization bound for log-likelihood from which a bound on Kullback-Leibler divergence follows (Theorem 3)\" or something like that. - Section 6, Theorem 1 - The definitions of $\\mathbb{X}_o$ and $\\mathbb{X}_r$ are missing, if I do not miss something. - It is better to add the assumption that $\\sup_{h, r, t} |\\psi(x_h, x_r, x_t)|$ is finite. - Section 12 - \"simplistic\" means too simple by itself. So, \"too simplistic\" should be \"too simple\" or \"simplistic\". Questions - Section 4 - I could not fully understand the intuition of the proof in the second paragraph. The authors think that the situation is desirable if multiple knowledge graphs are available. However, in the first approach, they concatenated the graphs into one (I interpreted the union of graphs as $\\widehat{\\mathcal{G}}_1 \\cup \\widehat{\\mathcal{G}}_2 \\cup \\cdots \\widehat{\\mathcal{G}}_n$. Correct me if I am wrong). Since this operation would reduce the situation to the single-graph case, we cannot take advantage of multiple graphs. - Section 7, Theorem 4 - The upper bound is in terms of the estimator which maximizes the expected log-likelihood. I am wondering whether this estimator can achieve the best possible misclassification rate (i.e., $|\\mathcal{F}/|\\mathcal{O}^2|/|\\mathcal{R}||$). I understand that it minimizes the discrepancy between the inferred distribution and the data generating distribution. But I am not sure it implies the least misclassification rate. - The small terms in the upper bound (i.e., first and second terms) depend on the number of objects $|\\mathcal{O}|$ and not on relations $|\\mathcal{R}|$. Since the role of $\\mathcal{O}^2$ and $\\mathcal{R}$ is symmetric mathematically (if I understand correctly), I feel it is weird that the additional terms do not necessarily go to zero when $\\mathcal{R}\\to \\infty$. I particular, I expected that the additonal small terms depend solely on $|\\mathcal{G}|=|\\mathcal{O}|^2|\\mathcal{R}|$.", "rating": "1: Reject", "reply_text": "Thank you for the very constructive feedback ! We tried to incorporate your suggestions into the revision of our paper . > Suggestions > > - Introduction > - Please explain what knowledge graphs are in the introduction before explaining > how knowledge graphs represent knowledge , or what problems of practically available > knowledge graphs have . We have rewritten the beginning of the introduction . > - I think it is a bit too casual and conversational to use wordings like `` Please bare in > mind '' in papers . We have reworded this sentence . > - I think it is helpful if the authors add a summary of theorems and its implication in the > introduction to grasp the theoretical contributions the paper has made . We have added a paragraph \u201c Main Technical Contributions \u201d at the end of the introduction section . > - Section 6 , Theorem 1 > - Please describe what is the probability of and with respect to which probability > distribution takes the expectation . Done. > - Section 7 > - The authors claim that Pinsker 's inequality plays a central role in deriving the > upper bound for the ratio of wrongly predicted triples . However , we can not know how the > authors used the inequality unless we see the proof of Theorem 3 , which is available in > the appendix . We have expanded this section . Now we also give a sketch of the proof idea which explains how Pinsker \u2019 s inequality is used to obtain the result in this section . > - In Theorem 3 , the definition of `` wrongly predicted triple '' is missing . I only find a > description related to it in the proof of Theorem 3 in the appendix . Could you add the > definition in the main part ? We have now added it in the text at the beginning of this section and we have also recalled it again in the statement of the theorem . > - Section 8 , Theorem 4 > - The statement of Theorem 4 says that is a learned representation . But it is not > available in the main article how we obtain it . We know that the authors used the > maximum-likelihood estimator if we read the proof of the theorem . The author should > clarify it in the statement of the theorem . It had actually been said in words in the statement of the theorem that X is obtained by maximizing log-likelihood , but it was probably confusing . We have added text before the theorem that describes how the vector representations are learned and also added an argmax L ( X|G ) to the statement of the theorem . > - Section 10 > - What does `` this '' mean in the title of the section ? We have changed the name of the section . > - In the initial reading , I could not understand the main point the authors want to > address in the paragraph starting with Loss . I think authors can clarify the point by > restructuring the paragraph . We have expanded the section and show the correspondence between our setting and existing settings explicitly in detail . > Minor comments > > - Section 2 and Section 6 > - The authors treated , which is the set of vector representations of objects and > relations , as a lookup table in Section 2.2 and reinterpreted it as the subset of in Section > 6 . I think we can simplify the description if we treat as the subset of and denote as in > short hand for a representation and an object ( and similarly for ) . We agree that this would be more elegant mathematically but it might be a bit less intuitive ( but that may be subjective ) . However , we are open to still implement it in the paper . > - Section 4 > - It would be helpful to make the correspondence explicit between each sentence > and theorem later in the last paragraph . For example , `` We prove a generalization bound > for log-likelihood from which a bound on Kullback-Leibler divergence follows ( Theorem > 3 ) \u201d or something like that . Done. > - Section 6 , Theorem 1 > - The definitions of and are missing , if I do not miss something . Sorry , that was a relict from a previous version of the draft of our paper . It is fixed now . > - It is better to add the assumption that is finite . Done. > - Section 12 > - `` simplistic '' means too simple by itself . So , `` too simplistic '' should be `` too simple '' > or \u201c simplistic '' . Done The response to your question is posted separately ( we ran into character limit ) ."}, {"review_id": "SJg2j0VFPB-2", "review_text": "This work conducts a theoretical study of the generalization bounds for the number of wrongly predicted triples of KG embedding methods. Under a \"\"missing completely at random\" assumption, the authors model the distributions of sampled KG and ground-truth KG, and derivate the bounds as a function of the KL-divergence by leveraging Pinsker's inequality. Preliminary discussions on how existing KG embedding methods fit into this theoretical study have been conducted. Pros: - This work raises a novel problem, i.e., analyzing the theoretical generalization ability of existing KG embedding methods. - The paper is well motivated with clear writing and technically sound with rigorous formula derivation. Cons: 1) For KG construction, people usually collect/mine in a batch/incremental fashion in which each batch focuses on certain aspects. For example, one can collect some bio facts about Persons from Wikipedia, then collect some entertainment related facts from news. So in practice, \"\"missing completely at random\" assumption may not hold for KG. 2) The derivation is suitable for a class of KG models that maximize log-likelihood losses, while many KG embedding methods use margin-based loss functions. Although the authors mentioned \"\"log-loss can in principle be also used and it was observed by Trouillon & Nickel (2017) that the margin-based loss functions, used by many knowledge graph embedding methods, are more prone to overfitting compared to log-likelihood\", more rigorous theoretical analysis is suggested to verify (or refine) the applicability of the proposed analysis 3) It would be better to see more examples that applying the proposed analysis to different KG embedding methods, corresponding comparisons help to get a deeper understanding for existing KG embedding methods, and may shed further light on how to design a KG embedding method that achieves a good enough generalization with fewer model parameters. ", "rating": "3: Weak Reject", "reply_text": "Thank you for the review ! > Cons : > 1 ) For KG construction , people usually collect/mine in a batch/incremental fashion in > which each batch focuses on certain aspects . For example , one can collect some bio > facts about Persons from Wikipedia , then collect some entertainment related facts from > news . So in practice , `` '' missing completely at random '' assumption may not hold for KG . First , we believe that we are transparent about this in the paper . We say that the setting corresponds to how these methods are evaluated in papers and we write \u201c practice \u201d ( in quotes ) . We also mention in the conclusions that people have only recently started to look into more realistic train-test splits . Second , we also think that our analysis could help to explain the cases ( if they happen ) when a KGE method works well on standard train/test splits generated using a method similar to the one analyzed in our paper but fails for more realistic settings . Finally , it is impossible to obtain theoretical guarantees without any assumptions . We believe that it is vital to first analyze simpler settings such as the missing-completely-at-random assumption ( which in our case corresponds to actual way people do experiments in the literature ! ) and only after that analyze more complex settings , such as the one suggested by the reviewer . > 2 ) The derivation is suitable for a class of KG models that maximize log-likelihood > losses , while many KG embedding methods use margin-based loss functions . Although > the authors mentioned `` '' log-loss can in principle be also used and it was observed by > Trouillon & Nickel ( 2017 ) that the margin-based loss functions , used by many knowledge > graph embedding methods , are more prone to overfitting compared to log-likelihood '' , > more rigorous theoretical analysis is suggested to verify ( or refine ) the applicability of the > proposed analysis We agree that it would be interesting to also analyze the margin-based loss functions , but that may require different techniques and our paper has already 21 pages ( including the appendix ) . To make sure that we do not overclaim our contribution in this direction , we added the following text to the revision : \u201c Our results may shed further light on this latter observation , however , to verify it theoretically would require to perform a similar type of analysis as we did in this paper for the margin-based loss functions ( which might require completely different techniques ) . Therefore we leave this comparison for future work. \u201d > 3 ) It would be better to see more examples that applying the proposed analysis to > different KG embedding methods , corresponding comparisons help to get a deeper > understanding for existing KG embedding methods , and may shed further light on how to > design a KG embedding method that achieves a good enough generalization with fewer > model parameters . We have expanded the section on numerical examples and added SimplE in the revision . It is not difficult to apply the derived bounds on other KGE methods ( as long as we can compute a bound on the Lipschitz constant which is not difficult for most methods , excluding those based on neural networks ) but we are limited by the space in the main part of the document . However , if the reviewer found it crucial , we could add another to the appendix ( e.g.ComplEx - but the analysis would be very similar to the one for SimplE ) ."}], "0": {"review_id": "SJg2j0VFPB-0", "review_text": "The paper presents a study on bound of the number of the expected triples wrongly predicted by general graph embedding methods. The paper considers methods that follow the \"completely at random\" assumption and aim to maximize the likelihood (or minimize the log-loss). The study considers the learning from positive and unlabelled data problem and theoretically demonstrates the correctness of the bounds on the number of triples that embedding methods can add during the completion of the knowledge graph. The paper is well written, theorems and proofs seem to me to be mathematically sound and correct. To the best of my knowledge, I agree with the authors when they claim to be the first to inspect the problem to give bounds under the setting they have considered. However, I have to admit that I am not an expert on the problem, I am more aware of the practical part of this field and less aware of the theoretical part. However, I have carefully checked the validity of the analysis and I have not found any flawless or critical errors in the proofs. I would suggest removing aliases for |O|, |R|, etc. from the proofs as they do not simplify the formulas significantly. Other corrections: - on page 5 the word \"the\" is written twice in the sentence just before Theorem 2. - on page 7 C is defined as constant but C should be C_1. Also C_2 should be defined. - on page 7, in \"Lipschitz Continuity\" after the sentence \"Our result requires Lipschitz continuity\" there must be a full stop or the word Most must begin with a lowercase m. - on page 8 there is inside the Figure 1 (on the red line, near 1*10^4 number of objects in the domain |O|) a link to page 6 that should be removed.", "rating": "6: Weak Accept", "reply_text": "Thank you for the review ! > I would suggest removing aliases for |O| , |R| , etc . > from the proofs as they do not simplify the formulas significantly . > Other corrections : > - on page 5 the word `` the '' is written twice in the sentence just before Theorem 2 . > - on page 7 C is defined as constant but C should be C_1 . Also C_2 should be defined . - > on page 7 , in `` Lipschitz Continuity '' after the sentence `` Our result requires Lipschitz > continuity '' there must be a full stop or the word Most must begin with a lowercase m. > - on page 8 there is inside the Figure 1 ( on the red line , near 1 * 10^4 number of objects in > the domain |O| ) a link to page 6 that should be removed . We have fixed all of these in the revision . Thank you !"}, "1": {"review_id": "SJg2j0VFPB-1", "review_text": "Summary This paper studied the knowledge completion problem from the viewpoint of statistical learning theory. More specifically, it formalized a knowledge graph completion problem as an estimation of the optimal embedding by assuming the canonical distribution derived from the embedding and derived the upper bound of misclassification rate of entities under the missing-completely-at-random assumption. Decision Although the formalization of knowledge graph completion problems is novel (to the best of my knowledge), there is much room for improvement in the organization of the paper. Therefore, I would recommend to reject the paper this time and to ask authors to revise the paper so that the paper is more accessible to researchers and engineers. There are numerous existing works about the knowledge graph embedding problem and the knowledge graph completion problem using embeddings. However, few works have been done to justify these methods theoretically. This paper attempted to answer this question via statistical learning theorem perspectives. The theorems gave sufficient conditions in terms of the size of a knowledge graph and properties of embedding functions under which the misclassification rate goes to $0$. In this aspect, this paper gave insights into how we can give justifications to existing embedding methods. However, I think it is hard for those who are not familiar with this field to follow the logic of the paper, as I write in detail in the following sections. I understand some people argue that the paper organization is not essential for how a paper contributes to science and technology. However, the accessibility to the paper is vital to promote technical communications in different fields. Also, I believe it is beneficial for the paper to maximize its value. Thus, I would ask the authors to polish the paper. Suggestions - Introduction - Please explain what knowledge graphs are in the introduction before explaining how knowledge graphs represent knowledge, or what problems of practically available knowledge graphs have. - I think it is a bit too casual and conversational to use wordings like \"Please bare in mind\" in papers. - I think it is helpful if the authors add a summary of theorems and its implication in the introduction to grasp the theoretical contributions the paper has made. - Section 6, Theorem 1 - Please describe what is the probability of $P$ and with respect to which probability distribution $\\mathbb{E}$ takes the expectation. - Section 7 - The authors claim that Pinsker's inequality plays a central role in deriving the upper bound for the ratio of wrongly predicted triples. However, we cannot know how the authors used the inequality unless we see the proof of Theorem 3, which is available in the appendix. - In Theorem 3, the definition of \"wrongly predicted triple\" is missing. I only find a description related to it in the proof of Theorem 3 in the appendix. Could you add the definition in the main part? - Section 8, Theorem 4 - The statement of Theorem 4 says that $\\mathbb{X}$ is a learned representation. But it is not available in the main article how we obtain it. We know that the authors used the maximum-likelihood estimator if we read the proof of the theorem. The author should clarify it in the statement of the theorem. - Section 10 - What does \"this\" mean in the title of the section? - In the initial reading, I could not understand the main point the authors want to address in the paragraph starting with Loss. I think authors can clarify the point by restructuring the paragraph. Minor comments - Section 2 and Section 6 - The authors treated $\\mathbb{X}$, which is the set of vector representations of objects and relations, as a lookup table in Section 2.2 and reinterpreted it as the subset of $\\mathcal{X}^{|\\mathcal{O}|+|\\mathcal{R}|}$ in Section 6. I think we can simplify the description if we treat $\\mathbb{X}$ as the subset of $\\mathcal{X}^{\\mathcal{O} \\sqcup \\mathcal{R}} := \\{f: \\mathcal{O} \\sqcup \\mathcal{R} \\to \\mathcal{X} \\}$ and denote $\\mathbb{X}(o)$ as $x_o$ in short hand for a representation $\\mathbb{X}$ and an object $o\\in \\mathcal{O}$ (and similarly for $x_r$). - Section 4 - It would be helpful to make the correspondence explicit between each sentence and theorem later in the last paragraph. For example, \"We prove a generalization bound for log-likelihood from which a bound on Kullback-Leibler divergence follows (Theorem 3)\" or something like that. - Section 6, Theorem 1 - The definitions of $\\mathbb{X}_o$ and $\\mathbb{X}_r$ are missing, if I do not miss something. - It is better to add the assumption that $\\sup_{h, r, t} |\\psi(x_h, x_r, x_t)|$ is finite. - Section 12 - \"simplistic\" means too simple by itself. So, \"too simplistic\" should be \"too simple\" or \"simplistic\". Questions - Section 4 - I could not fully understand the intuition of the proof in the second paragraph. The authors think that the situation is desirable if multiple knowledge graphs are available. However, in the first approach, they concatenated the graphs into one (I interpreted the union of graphs as $\\widehat{\\mathcal{G}}_1 \\cup \\widehat{\\mathcal{G}}_2 \\cup \\cdots \\widehat{\\mathcal{G}}_n$. Correct me if I am wrong). Since this operation would reduce the situation to the single-graph case, we cannot take advantage of multiple graphs. - Section 7, Theorem 4 - The upper bound is in terms of the estimator which maximizes the expected log-likelihood. I am wondering whether this estimator can achieve the best possible misclassification rate (i.e., $|\\mathcal{F}/|\\mathcal{O}^2|/|\\mathcal{R}||$). I understand that it minimizes the discrepancy between the inferred distribution and the data generating distribution. But I am not sure it implies the least misclassification rate. - The small terms in the upper bound (i.e., first and second terms) depend on the number of objects $|\\mathcal{O}|$ and not on relations $|\\mathcal{R}|$. Since the role of $\\mathcal{O}^2$ and $\\mathcal{R}$ is symmetric mathematically (if I understand correctly), I feel it is weird that the additional terms do not necessarily go to zero when $\\mathcal{R}\\to \\infty$. I particular, I expected that the additonal small terms depend solely on $|\\mathcal{G}|=|\\mathcal{O}|^2|\\mathcal{R}|$.", "rating": "1: Reject", "reply_text": "Thank you for the very constructive feedback ! We tried to incorporate your suggestions into the revision of our paper . > Suggestions > > - Introduction > - Please explain what knowledge graphs are in the introduction before explaining > how knowledge graphs represent knowledge , or what problems of practically available > knowledge graphs have . We have rewritten the beginning of the introduction . > - I think it is a bit too casual and conversational to use wordings like `` Please bare in > mind '' in papers . We have reworded this sentence . > - I think it is helpful if the authors add a summary of theorems and its implication in the > introduction to grasp the theoretical contributions the paper has made . We have added a paragraph \u201c Main Technical Contributions \u201d at the end of the introduction section . > - Section 6 , Theorem 1 > - Please describe what is the probability of and with respect to which probability > distribution takes the expectation . Done. > - Section 7 > - The authors claim that Pinsker 's inequality plays a central role in deriving the > upper bound for the ratio of wrongly predicted triples . However , we can not know how the > authors used the inequality unless we see the proof of Theorem 3 , which is available in > the appendix . We have expanded this section . Now we also give a sketch of the proof idea which explains how Pinsker \u2019 s inequality is used to obtain the result in this section . > - In Theorem 3 , the definition of `` wrongly predicted triple '' is missing . I only find a > description related to it in the proof of Theorem 3 in the appendix . Could you add the > definition in the main part ? We have now added it in the text at the beginning of this section and we have also recalled it again in the statement of the theorem . > - Section 8 , Theorem 4 > - The statement of Theorem 4 says that is a learned representation . But it is not > available in the main article how we obtain it . We know that the authors used the > maximum-likelihood estimator if we read the proof of the theorem . The author should > clarify it in the statement of the theorem . It had actually been said in words in the statement of the theorem that X is obtained by maximizing log-likelihood , but it was probably confusing . We have added text before the theorem that describes how the vector representations are learned and also added an argmax L ( X|G ) to the statement of the theorem . > - Section 10 > - What does `` this '' mean in the title of the section ? We have changed the name of the section . > - In the initial reading , I could not understand the main point the authors want to > address in the paragraph starting with Loss . I think authors can clarify the point by > restructuring the paragraph . We have expanded the section and show the correspondence between our setting and existing settings explicitly in detail . > Minor comments > > - Section 2 and Section 6 > - The authors treated , which is the set of vector representations of objects and > relations , as a lookup table in Section 2.2 and reinterpreted it as the subset of in Section > 6 . I think we can simplify the description if we treat as the subset of and denote as in > short hand for a representation and an object ( and similarly for ) . We agree that this would be more elegant mathematically but it might be a bit less intuitive ( but that may be subjective ) . However , we are open to still implement it in the paper . > - Section 4 > - It would be helpful to make the correspondence explicit between each sentence > and theorem later in the last paragraph . For example , `` We prove a generalization bound > for log-likelihood from which a bound on Kullback-Leibler divergence follows ( Theorem > 3 ) \u201d or something like that . Done. > - Section 6 , Theorem 1 > - The definitions of and are missing , if I do not miss something . Sorry , that was a relict from a previous version of the draft of our paper . It is fixed now . > - It is better to add the assumption that is finite . Done. > - Section 12 > - `` simplistic '' means too simple by itself . So , `` too simplistic '' should be `` too simple '' > or \u201c simplistic '' . Done The response to your question is posted separately ( we ran into character limit ) ."}, "2": {"review_id": "SJg2j0VFPB-2", "review_text": "This work conducts a theoretical study of the generalization bounds for the number of wrongly predicted triples of KG embedding methods. Under a \"\"missing completely at random\" assumption, the authors model the distributions of sampled KG and ground-truth KG, and derivate the bounds as a function of the KL-divergence by leveraging Pinsker's inequality. Preliminary discussions on how existing KG embedding methods fit into this theoretical study have been conducted. Pros: - This work raises a novel problem, i.e., analyzing the theoretical generalization ability of existing KG embedding methods. - The paper is well motivated with clear writing and technically sound with rigorous formula derivation. Cons: 1) For KG construction, people usually collect/mine in a batch/incremental fashion in which each batch focuses on certain aspects. For example, one can collect some bio facts about Persons from Wikipedia, then collect some entertainment related facts from news. So in practice, \"\"missing completely at random\" assumption may not hold for KG. 2) The derivation is suitable for a class of KG models that maximize log-likelihood losses, while many KG embedding methods use margin-based loss functions. Although the authors mentioned \"\"log-loss can in principle be also used and it was observed by Trouillon & Nickel (2017) that the margin-based loss functions, used by many knowledge graph embedding methods, are more prone to overfitting compared to log-likelihood\", more rigorous theoretical analysis is suggested to verify (or refine) the applicability of the proposed analysis 3) It would be better to see more examples that applying the proposed analysis to different KG embedding methods, corresponding comparisons help to get a deeper understanding for existing KG embedding methods, and may shed further light on how to design a KG embedding method that achieves a good enough generalization with fewer model parameters. ", "rating": "3: Weak Reject", "reply_text": "Thank you for the review ! > Cons : > 1 ) For KG construction , people usually collect/mine in a batch/incremental fashion in > which each batch focuses on certain aspects . For example , one can collect some bio > facts about Persons from Wikipedia , then collect some entertainment related facts from > news . So in practice , `` '' missing completely at random '' assumption may not hold for KG . First , we believe that we are transparent about this in the paper . We say that the setting corresponds to how these methods are evaluated in papers and we write \u201c practice \u201d ( in quotes ) . We also mention in the conclusions that people have only recently started to look into more realistic train-test splits . Second , we also think that our analysis could help to explain the cases ( if they happen ) when a KGE method works well on standard train/test splits generated using a method similar to the one analyzed in our paper but fails for more realistic settings . Finally , it is impossible to obtain theoretical guarantees without any assumptions . We believe that it is vital to first analyze simpler settings such as the missing-completely-at-random assumption ( which in our case corresponds to actual way people do experiments in the literature ! ) and only after that analyze more complex settings , such as the one suggested by the reviewer . > 2 ) The derivation is suitable for a class of KG models that maximize log-likelihood > losses , while many KG embedding methods use margin-based loss functions . Although > the authors mentioned `` '' log-loss can in principle be also used and it was observed by > Trouillon & Nickel ( 2017 ) that the margin-based loss functions , used by many knowledge > graph embedding methods , are more prone to overfitting compared to log-likelihood '' , > more rigorous theoretical analysis is suggested to verify ( or refine ) the applicability of the > proposed analysis We agree that it would be interesting to also analyze the margin-based loss functions , but that may require different techniques and our paper has already 21 pages ( including the appendix ) . To make sure that we do not overclaim our contribution in this direction , we added the following text to the revision : \u201c Our results may shed further light on this latter observation , however , to verify it theoretically would require to perform a similar type of analysis as we did in this paper for the margin-based loss functions ( which might require completely different techniques ) . Therefore we leave this comparison for future work. \u201d > 3 ) It would be better to see more examples that applying the proposed analysis to > different KG embedding methods , corresponding comparisons help to get a deeper > understanding for existing KG embedding methods , and may shed further light on how to > design a KG embedding method that achieves a good enough generalization with fewer > model parameters . We have expanded the section on numerical examples and added SimplE in the revision . It is not difficult to apply the derived bounds on other KGE methods ( as long as we can compute a bound on the Lipschitz constant which is not difficult for most methods , excluding those based on neural networks ) but we are limited by the space in the main part of the document . However , if the reviewer found it crucial , we could add another to the appendix ( e.g.ComplEx - but the analysis would be very similar to the one for SimplE ) ."}}