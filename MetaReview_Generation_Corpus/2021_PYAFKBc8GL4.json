{"year": "2021", "forum": "PYAFKBc8GL4", "title": "Client Selection in Federated Learning: Convergence Analysis and Power-of-Choice Selection Strategies", "decision": "Reject", "meta_review": "In federated learning, distributed and resource-limited client nodes cooperatively train a model without sharing their local data. The results thus far on analyzing the  convergence of federated learning are restricted to \u201cunbiased\u201d client participation, where the probability of a client c being selected is proportional to c\u2019s data size. This work presents the first convergence analysis of federated learning for biased client selection, and quantifies the impact of selection skew on time to convergence. Specifically, biasing toward clients with higher local loss is shown to be beneficial, and a protocol is developed based on this, to trade between convergence time and solution bias.\n\nThe paper is in general well-written, and develops a natural idea. \n\nThe strong-convexity assumption is a concern: how much can it be weakened? The authors are also asked to run experiments systematically on (much) larger datasets. The test-accuracy and possible-overfitting concerns also need to be addressed in more depth. The authors are also encouraged to see how much Assumption 3.4---uniformly-bounded stochastic gradients---can be dispensed with. \n", "reviews": [{"review_id": "PYAFKBc8GL4-0", "review_text": "This work investigates federated optimization considering data heterogeneity , communication and computation limitations , and partial client participation . In contrast to past works , this paper focuses on deeper understanding of the effect of partial client participation on the convergence rate by considering biased client participation . The paper provides convergence analysis for any biased selection strategy , showing that the rate is composed of vanishing error term and non-vanishing bias term . The obtained rates explicitly show the effect of client selection strategy and the trade-off between convergence speed and the solution bias . Then it proposes a parametric family of biased selection strategy , called power-of-choice , which aims to speed up the convergence of the error term at the cost of possibly bigger bias term . Experiments are provided to highlight the benefits of the proposed pow-d strategy over the standard unbiased selection strategies . In terms of the assumptions on the loss functions , there is no improvement as the same assumptions are used in recent works [ 4,5 ] . The Assumption 3.4 seems problematic as together with Assumption 3.3 it implies that gradients of local loss functions $ F_k $ are uniformly bounded . This is in conflict with Assumption 3.2 as local losses $ F_k $ are assumed to be strongly convex . For instance , in papers [ 1,2,3 ] the Assumption 3.4 is not needed . Can the current theory be extended by relaxing Assumption 3.4 ? For Definition 3.1 on local-global objectivity gap , it should be mentioned that this notion was defined and used earlier in [ 4 ] . It should be credited properly . The metrics $ \\bar { \\rho } $ and $ \\tilde { \\rho } $ describing the skewness of client selection strategy , and their explicit effect on convergence rate ( 7 ) are interesting . It is also nice that the theory , e.g.Theorem 3.1 , recovers the result of unbiased client selection case without any solution bias . Since Theorem 3.1 is generic and works for any selection strategy , it does not explicitly show a clear benefit of biased selection strategy over unbiased one . Biased selection strategy reduces the vanishing error term by $ \\bar { \\rho } $ , and adds a non-vanishing bias $ Q ( \\bar { \\rho } , \\tilde { \\rho } ) =O ( \\frac { \\tilde { \\rho } } { \\bar { \\rho } } -1 ) $ . So , based on the rate given in Theorem 3.1 , biased selection is beneficial only if $ \\bar { \\rho } > 1 $ and the difference $ \\tilde { \\rho } -\\bar { \\rho } $ is small ( formally it should be $ O ( ( \\bar { \\rho } -1 ) /T ) ) $ . The question is , can such biased selection strategy be designed so that it is theoretically better ( or at least not worse ) than unbiased selection strategy ? The proof of the main Theorem 3.1 largely follows the proof of Theorem 1 of [ 4 ] . It seems that the proof of Theorem 3.1 deviates from the proof of Theorem 1 [ 4 ] only in derivations ( 53 ) - ( 63 ) , where biased-ness of the client selection kicks in and skewness metrics get involved . Such overlaps in proof techniques should be mentioned and some discussion is needed to highlight the novelty of the proposed analysis . Using insights from Theorem 3.1 , the paper proposes a client selection strategy ( with two practical variations ) , called power-of-choice , which aims to speed up the convergence of vanishing error by maximizing rho_bar . However , it is not clear how the other metric $ \\tilde { \\rho } $ would behave in this selection strategy . Although inspired from Theorem 3.1 , I view the power-of-choice selection strategy as a heuristic idea as ( i ) it focuses only on the vanishing error term and does nothing to minimize the bias term , ( ii ) no theoretical estimates are developed for the metrics $ \\bar { \\rho } , \\tilde { \\rho } $ in this specific selection strategy . With points made above , the theoretical contribution of the paper is weak . I think , the paper would largely improved if it were developed a way on how to avoid the bias term while using biased selection . One possible way is to design a suitable selection strategy which allows to bound the bias . For instance , from the experiment shown on Figure 2.b , it is tempting to show that for the proposed pow-d selection strategy the bias term $ ( \\frac { \\tilde { \\rho } } { \\bar { \\rho } } - 1 ) $ is $ O ( d/K ) $ , which can be controlled via parameter $ d $ . Another possible way might be to incorporate some mechanism on top of FedAvg algorithm similar to what error compensation ( or error feedback ) does for biased ( contractive ) compression operators . Experiments are okay , but they use up to 100 clients which is far from the scale of typical FL applications . In addition , in the experiment shown in Figure 4.b , the training loss seems to be decreasing steadily and non vanishing bias term is not dominated there . As the vanishing error term reduces by choosing larger $ d $ , why in this experiment smaller $ d=6 $ performs better than larger $ d=15 $ ? [ 1 ] A Khaled , K Mishchenko , and P Richt\u00e1rik . Tighter theory for local SGD on identical and heterogeneous data . In The 23rd International Conference on Artificial Intelligence and Statistics ( AISTATS 2020 ) , 2020 . [ 2 ] Blake Woodworth , Kumar Kshitij Patel , Sebastian U Stich , Zhen Dai , Brian Bullins , H Brendan McMahan , Ohad Shamir , and Nathan Srebro . Is local SGD better than minibatch SGD ? arXiv preprint arXiv:2002.07839 , 2020 . [ 3 ] Anastasia Koloskova , Nicolas Loizou , Sadra Boreiri , Martin Jaggi , and Sebastian U Stich . A unified theory of decentralized SGD with changing topology and local updates . arXiv preprint arXiv:2003.10422 , 2020 . [ 4 ] Xiang Li , Kaixuan Huang , Wenhao Yang , Shusen Wang , and Zhihua Zhang . On the convergence of fedavg on non-iid data . In International Conference on Learning Representations ( ICLR ) , July 2020 . [ 5 ] Yichen Ruan , Xiaoxi Zhang , Shu-Che Liang , and Carlee Joe-Wong . Towards flexible device participation in federated learning for non-iid data . ArXiv , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the positive feedback and detailed constructive suggestions on our paper . We believe that we have addressed all the reviewer \u2019 s comments fully , and hope the reviewer can increase the score in reflect of our responses and improvements in the paper . We address the reviewer 's feedback accordingly as follows : Q1 : * Assumption 3.4 seems problematic as together with Assumption 3.3 it implies that gradients of local loss functions Fk are uniformly bounded . This is in conflict with Assumption 3.2 as local losses Fk are assumed to be strongly convex . For instance , in papers [ 1,2,3 ] the Assumption 3.4 is not needed . Can the current theory be extended by relaxing Assumption 3.4 ? * A1 : - We acknowledge that our analysis is based on assumption 3.4 , uniformly bounded stochastic gradients . We would like to stress that the problem of the convergence of biased client selection strategies is non-trivial and the papers the reviewer mentioned only consider unbiased client selection strategies . Although we started off with the corresponding assumptions , we intend to relax these assumptions in our future work ( i.e. , non-convex scenarios without assumption 3.4 ) . We also would like to stress that our paper 's significance is in the novelty of the first analysis for biased client selection strategies that have not been looked into before and the insight it gives to the FL community along with the effectiveness of our proposed power-of-d client strategy . To the best of our knowledge , there has been no work giving convergence insights for biased client selection strategies that are cognizant of the training process for either strongly-convex or non-convex functions . Our work presents the first insight in this area , and we intend to extend the analysis to non-convex scenarios as the next step . We do not claim novelty in the analysis technique itself . Q2 : * For Definition 3.1 on local-global objectivity gap , it should be mentioned that this notion was defined and used earlier in [ 4 ] . It should be credited properly . * A2 : - We thank the reviewer for pointing this out ! We completely agree that the definition of the local-global objectivity gap should be duly credited to [ 4 ] . We have included a sentence about this in the updated version , which will be uploaded soon . Q3 : * The question is , can such biased selection strategy be designed so that it is theoretically better ( or at least not worse ) than unbiased selection strategy ? * A3 : - Similar point with Q5 below , addressed in reply for Q5 . Q4 : * Such overlaps in proof techniques should be mentioned and some discussion is needed to highlight the novelty of the proposed analysis . * A4 : - Thank you very much for pointing this out ! We completely agree with the reviewer that the overlap with proof techniques used in [ 4 ] should be duly acknowledged . We will certainly discuss this in the updated version and will highlight that the main novelty of our paper as compared to [ 4 ] is that we consider biased selection policies and show how the selection skew affects convergence . Q5 : * However , it is not clear how the other metric \u03c1~ would behave in this selection strategy . Although inspired by Theorem 3.1 , I view the power-of-choice selection strategy as a heuristic idea as ( i ) it focuses only on the vanishing error term and does nothing to minimize the bias term * A5 : - We appreciate the reviewer 's opinion regarding the bias term and the suggestions for improving the pow-d strategy . We would first like to highlight that the pow-d strategy spans a natural trade-off between convergence speed and selection bias . The choice of $ d $ controls this trade-off . For example , $ d=m $ can entirely eliminate the selection bias with seeing no benefit in convergence speed . Increasing $ d $ will improve the convergence speed but may increase the non-vanishing error term . In our view , the understanding of this trade-off is one of the main contributions of our paper . Nevertheless , we agree with the reviewer 's point that the paper can be improved with a selection strategy that tackles the selection bias while gaining the benefit of convergence speed . This can be simply achieved by adaptively reducing $ d $ of our pow-d strategy during the course of training . Henceforth , we included the proposition of 'adapow-d ' in the updated paper , which decreases $ d $ throughout training eventually dropping down to $ d=m $ . This enables the training loss to eventually converge to the minimal point without the selection bias , while also enjoying the convergence speed in the initial training phase with larger $ d > m $ . We verify the performance of 'adapow-d ' in both the quadratic and synthetic simulations wherein both simulations the training loss converges without the selection bias while having faster convergence speed than the baseline strategy . We plan to add these result plots of \u2018 adapow-d \u2019 to the updated version of the paper ."}, {"review_id": "PYAFKBc8GL4-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper analyzes a biased client selection policy in the context of the federated learning paradigm . In particular , instead of random selection of the clients with a probability that is proportional to the size of the local dataset , clients with higher local loss values are selected . Despite being already used as heuristics , the authors claim that their work is the first one to propose a convergence analysis of a biased client selection mechanism in the context of federated learning . Under some ( quite stringent ) assumptions , the authors derive a global bound for the expected error after $ T $ iterations . The bound is composed of two terms : a vanishing term and a non-vanishing one which comes as a consequence of the biased client selection mechanism . From Theorem 3.1 it emerges clearly that the biased selection mechanism poses a trade-off : on one hand the non-vanishing term increases the more biased is the selection , on the other hand the vanishing one benefits from the biased selection mechanism since the more biased is the selection the faster it will decay to zero . The biased selection mechanism comes with some additional computational and communication costs with respect to a fully random selection : the authors propose a couple of heuristics to alleviate these extra-costs . In the experimental section the proposed method is evaluated on different benchmarks and with different datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : The paper is well-written and the theoretical results look correct . At the same time , there are some weaknesses ( see the section on Cons ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper takes is well-written and really easy to follow . 2.The authors attempt to provide a more rigorous analysis to an already-in-use heuristic and it is the first time that such analysis is carried out in this context . 3.The paper has a good balance across the sections and it touches on theory and experimental part in a balanced way . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The assumptions on which the theoretical analysis is based are quite stringent and could be further relaxed ( especially strong convexity ) . 2.Incoherent benchmarks : the authors are not reasoning regarding the introduced assumptions and whether they hold . It actually seems that at least in the DNN case the assumptions are not holding since the landscape is notoriously non-convex . 3.Experiments only show the behavior of the heuristics pow-d. What happens when d=K ? In order for the experiment to be in line with the theoretical results this case should be analyzed at least once . Introducing the extra random selection of $ m\\leq d\\leq K $ clients could indeed change the setting and therefore the convergence . 4.Way too strong and unjustified statements , i.e.faster convergence to a global minimum ( according to Th.3.1.it does not converge ) , convergence up to 3 times faster ( in one benchmark , might easily depend on hyperparameter settings or on the heuristic added on top on power-of-chance ) . 5.All the statements regarding the test accuracy are pure speculations as there are no theoretical and/or consistent and extensive empirical evidence which suggest that the biased selection mechanism leads to better generalization . Intuitively adding a bias in the selection should lead more easily to overfitting . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Please address and clarify the cons above # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Interesting General Aspect Not Considered 1. local SGD iterations : how does $ \\tau $ impacts on the convergence ? Is it beneficial to perform more local SGD steps or does it become harmful for the convergence ? How does this relate with the biased selection mechanism ? 2. first random selection of $ d $ clients based on proportions of dataset : unbalanced work load might create inefficiency since the general method described is synchronized .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the detailed review of the paper ! We appreciate the reviewer 's comment on our paper being balanced on both theory and experiments , well-written , and novelty of the analysis of biased client selection . We address the reviewer 's concerns below . We hope the reviewer re-evaluates the score in favor of the paper ! Q1 : * The assumptions on which the theoretical analysis is based are quite stringent and could be further relaxed ( especially strong convexity ) . * A1 : - We acknowledge that our analysis is based on strongly-convex functions , and do not include non-convex ones . Our paper 's significance is in the novelty of the first analysis for biased client selection strategies that have not been looked into before and the insight it gives to the FL community along with the effectiveness of our proposed power-of-d client strategy . To the best of our knowledge , there has been no work giving convergence insights for biased client selection strategies that are cognizant of the training process for either strongly-convex or non-convex functions . Our work presents the first insight in this area , and we intend to extend the analysis to non-convex scenarios as the next step . We would like to highlight that between two different sets of analysis techniques ( strongly-convex and non-convex ) , considering the latter is outside the scope of the current paper . Q2 : * Incoherent benchmarks : the authors are not reasoning regarding the introduced assumptions and whether they hold . It actually seems that at least in the DNN case the assumptions are not holding since the landscape is notoriously non-convex . * A2 : - The reviewer is correct in that the DNN loss landscape is non-convex . While we do not claim that our convergence analysis holds in this case , the experiments on DNN show that the proposed client selection strategies do work well even for non-convex loss landscapes . The strongly-convex assumption does hold for the quadratic simulation results presented in Fig.2.Note that we do not introduce any new assumptions that haven \u2019 t been used in previous literature . [ 1-4 ] also use the assumptions used in our paper to provide theoretical insight into distributed algorithms . - [ 1 ] Sebastian Urban Stich . Local sgd converges fast and communicates little . In ICLR 2019 [ 2 ] Debraj Basu , Deepesh Data , Can Karakus , and Suhas Diggavi . Qsparse-local-SGD : Distributed SGD with Quantization , Sparsification , and Local Computations . In NeurIPS 2019 [ 3 ] Xiang Li , Kaixuan Huang , Wenhao Yang , Shusen Wang , and Zhihua Zhang . On the convergence of fedavg on non-iid data . In International Conference on Learning Representations ( ICLR ) , July 2020 . [ 4 ] Yichen Ruan , Xiaoxi Zhang , Shu-Che Liang , and Carlee Joe-Wong . Towards flexible device participation in federated learning for non-iid data . ArXiv , 2020 . Q3 : * Experiments only show the behavior of the heuristics pow-d. What happens when d=K ? In order for the experiment to be in line with the theoretical results this case should be analyzed at least once . Introducing the extra random selection of m\u2264d\u2264K clients could indeed change the setting and therefore the convergence . * A3 : - We would like to clarify that our theoretical analysis is a general analysis that is applicable for any selection strategy $ \\pi $ that is cognizant of the training progress . It is not limited to the pow-d strategy or any specific value of $ d $ such as $ d=K $ . The case when $ m\u2264d\u2264K $ clients is subsumed in the convergence analysis . As $ d $ increases , the $ \\overline { \\rho } $ also increases along with the selection bias , as shown in Fig.2 ( b ) , being consistent with our theoretical results . - We show the effect of $ d=K $ for the quadratic simulations in Fig.2 ( a ) and Fig.2 ( b ) and the synthetic simulations for $ K=30 , m=3 $ in Fig.3 . We show that for $ d=K $ which is the case for maximum selection bias , we achieve higher convergence speed but also the highest selection bias which is in line with our theoretical results ."}, {"review_id": "PYAFKBc8GL4-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper studies federated learning ( FL ) and proposes nonuniform sampling of participating clients . Clients are selected according to their losses ; clients with big losses are likely selected . The proposed algorithm has a rigorous convergence analysis . The proposed algorithm is demonstrated powerful on small scale datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I find this submission a borderline paper . I am fine with either acceptance or rejection . This paper has theoretical guarantees and strong empirical advantages . But the actually used algorithm may not be very useful . The experiments are conducted on small datasets . I am not confident whether this work will have an advantage on large-scale data . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : + The idea of choosing participating clients according to loss is an interesting and reasonable idea . + The non-uniform sampling of clients is proved to converge . + The nonuniform sampling algorithm has a strong empirical advantage . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The actually used algorithm is the power-of-choice strategy . It firstly uses a uniformly sampled set of $ d $ clients to compute losses and then sample a subset of $ m < d $ clients out of the $ d $ clients . Here is my question : The server needs to communicate with the $ d $ clients from the candidate set . The server must wait for their responses . So the $ d $ clients are actually active . Why not use all the $ d $ clients to compute gradients ? Is the proposed strategy comparable to using all the $ d $ clients ? 2 FMNIST seems to be the only used dataset . The dataset is not big enough . Even if it is big enough , I would like to see empirical results on other datasets , e.g. , MNIST , CIFAR10 , mini-ImageNet , etc . Admittedly , this work has an advantage on FMNIST . But I am not convinced that this work is better in general . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "6: Marginally above acceptance threshold", "reply_text": "We greatly thank the reviewer for the positive feedback on our paper . The reviewer raised two issues : 1 ) why the pow-d strategy does not use all d clients for training , and 2 ) that the experiments on the FMNIST dataset are not enough . We address both these concerns below . Q1 : * The actually used algorithm is the power-of-choice strategy . It firstly uses a uniformly sampled set of d clients to compute losses and then sample a subset of m < d clients out of the d clients . Here is my question : The server needs to communicate with the d clients from the candidate set . The server must wait for their responses . So the d clients are actually active . Why not use all the d clients to compute gradients ? Is the proposed strategy comparable to using all the d clients ? * A1 : - First , having the $ d $ clients actually perform the local updates rather than just send their local loss values will incur a significantly higher computation cost than performing updates at only $ m < d $ clients . We highlight that pow-d does not require $ d $ clients to actually perform gradient updates , but just requires inference tasks on the training data for the loss values . - Moreover , to address the possible communication and computation cost for the loss inference task of the $ d $ clients , in the paper we propose rpow-d , which simply uses the last received loss values from the previously selected clients to select $ m $ clients from the selected $ d $ clients in the subset . rpow-d requires no additional computation or communication cost compared to the baseline random strategy , but performs better than the baseline as shown in our DNN experiments . - Lastly , we emphasize that pow-d does not uniformly sample the final $ m $ clients , but sample them according to their importance ( i.e. , loss values ) . Therefore , our strategy is distinguished from simply uniformly sampling in proportion to dataset size of $ d $ clients and performing local updates . Q2 : * FMNIST seems to be the only used dataset . The dataset is not big enough . Even if it is big enough , I would like to see empirical results on other datasets , e.g. , MNIST , CIFAR10 , mini-ImageNet , etc . Admittedly , this work has an advantage on FMNIST . But I am not convinced that this work is better in general . * A2 : - For more experimental validation , since the MNIST and CIFAR10 datasets have the same size as the FMNIST dataset considered in our experiments , we have added experiments on a larger dataset EMNIST ( `` by Digits '' ) . The EMNIST dataset is 4 times larger ( 240,000 training data with 40,000 testing data ) than FMNIST , MNIST , and CIFAR10 ( which have 60000 training samples each ) . - We show consistency in the effective performance in training loss and test accuracy results for our proposed pow-d strategy . Note that the performance improvement offered by the pow-d strategy and its variants is due to data heterogeneity across clients and not specific to one particular dataset . So we expect to see similar performance on other datasets . Moreover , we stress that the experiments are a sanity check for the performance of our proposed pow-d strategy and its variants . Our paper \u2019 s main novelty and contribution is that it gives the first convergence analysis of biased client selection strategies , which has not been investigated before ."}, {"review_id": "PYAFKBc8GL4-3", "review_text": "This paper analyzes the convergence of FedAvg with biased client selection strategies . And the new strategy power-of-choice is designed based on the convergence results . This new strategy is numerically compared with two benchmark strategies and shows faster convergence and higher testing accuracy . Pros : 1.The convergence analysis is new and novel . This paper provides the first convergence analysis for a general class of biased client selection strategies . New concept , selection skew , is introduced as the measure of strategies . Based on the new concept , the analysis quantifies how the bias of the strategy affects the convergence speed of FedAvg . 2.New strategy pow-d is proposed based on the insights of the convergence analysis . The performance of pow-d is impressive in the FMNIST experiment , where it significantly improves the test accuracy under random selection strategy . Cons : 1.Although the analysis is interesting and insightful , the effect of \\rho seems to be limited . There are three constant terms in the vanishing error term in Eq . ( 7 ) . And only the first term will be decreased when increasing \\bar { \\rho } . However , it is not obvious if the first term is the dominating term . 2.Is it possible to give an exact or estimated values of \\bar\\rho and \\tilde\\rho for different strategies in the numerical experiments ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for highlighting the paper \u2019 s key novel contributions : 1 ) that we provide the first convergence analysis of biased client selection strategies and 2 ) the proposed pow-d strategies gives an impressive improvement in the FMNIST experiments . Our responses to both the concerns are given below . Q1 : * Although the analysis is interesting and insightful , the effect of \\rho seems to be limited . There are three constant terms in the vanishing error term in Eq . ( 7 ) . And only the first term will be decreased when increasing \\bar { \\rho } . However , it is not obvious if the first term is the dominating term . * A1 : - We show that the first term $ \\frac { 4L ( 32\\tau^2 G^2+\\sigma^2/m ) } { 3\\mu^2\\overline { \\rho } } $ is indeed significant in the vanishing error term , below , $ \\frac { 1 } { ( T+\\gamma ) } \\left [ \\frac { 4L ( 32\\tau^2 G^2+\\sigma^2/m ) } { 3\\mu^2\\overline { \\rho } } +\\frac { 8L^2\\Gamma } { \\mu^2 } +\\frac { L\\gamma\\|\\overline { \\mathbf { w } } ^ { ( 0 ) } -\\mathbf { w } ^ * \\|^2 } { 2 } \\right ] $ , through experiments held out in three different settings : quadratic ( strongly-convex case ) , synthetic dataset ( logistic regression problem ) , and DNN with FMNIST dataset . Note that the quadratic simulation directly reflects the theory due to the quadratic function 's strongly-convex property . Especially in the quadratic and synthetic simulations we show that biasing more towards the clients with larger loss , i.e. , larger $ \\overline { \\rho } > 1 $ , leads to faster convergence . If the first term not was not dominant , larger $ \\overline { \\rho } > 1 $ would not have led to faster convergence due to the other two terms in the vanishing error term . Figure 2 shows how $ \\overline { \\rho } $ actually matters with estimated $ \\overline { \\rho } $ values by showing the convergence speed up . - In addition to the point above , note that the first term is the only term that can be controlled through client selection ( the number of local updates $ \\tau $ , stochastic gradient bound $ G $ , and variance bound $ \\sigma $ ) . The other two terms are properties of the objective function , dataset distributions , and the initial point which can not be controlled . Therefore we believe that the decrease by a factor of $ \\overline { \\rho } $ in the first term is sufficient to show the positive effect of biasing client selection . Q2 : * Is it possible to give an exact or estimated values of \\bar\\rho and \\tilde\\rho for different strategies in the numerical experiments ? * A2 : - This question is already answered in Fig 2 ( b ) , where we plot the estimated values of $ \\overline { \\rho } $ and $ \\widetilde { \\rho } /\\overline { \\rho } $ for varying $ d $ and $ K $ and for different client selection strategies . Perhaps the reviewer missed seeing it -- we will make it more prominent in an updated version . Our theory matches well with the quadratic simulations in that as $ d $ increases the estimated $ \\overline { \\rho } $ increases but the selection bias $ \\widetilde { \\rho } /\\overline { \\rho } $ also increases . The methodology of getting these estimated theoretical values is elaborated in Appendix F of the original paper ."}], "0": {"review_id": "PYAFKBc8GL4-0", "review_text": "This work investigates federated optimization considering data heterogeneity , communication and computation limitations , and partial client participation . In contrast to past works , this paper focuses on deeper understanding of the effect of partial client participation on the convergence rate by considering biased client participation . The paper provides convergence analysis for any biased selection strategy , showing that the rate is composed of vanishing error term and non-vanishing bias term . The obtained rates explicitly show the effect of client selection strategy and the trade-off between convergence speed and the solution bias . Then it proposes a parametric family of biased selection strategy , called power-of-choice , which aims to speed up the convergence of the error term at the cost of possibly bigger bias term . Experiments are provided to highlight the benefits of the proposed pow-d strategy over the standard unbiased selection strategies . In terms of the assumptions on the loss functions , there is no improvement as the same assumptions are used in recent works [ 4,5 ] . The Assumption 3.4 seems problematic as together with Assumption 3.3 it implies that gradients of local loss functions $ F_k $ are uniformly bounded . This is in conflict with Assumption 3.2 as local losses $ F_k $ are assumed to be strongly convex . For instance , in papers [ 1,2,3 ] the Assumption 3.4 is not needed . Can the current theory be extended by relaxing Assumption 3.4 ? For Definition 3.1 on local-global objectivity gap , it should be mentioned that this notion was defined and used earlier in [ 4 ] . It should be credited properly . The metrics $ \\bar { \\rho } $ and $ \\tilde { \\rho } $ describing the skewness of client selection strategy , and their explicit effect on convergence rate ( 7 ) are interesting . It is also nice that the theory , e.g.Theorem 3.1 , recovers the result of unbiased client selection case without any solution bias . Since Theorem 3.1 is generic and works for any selection strategy , it does not explicitly show a clear benefit of biased selection strategy over unbiased one . Biased selection strategy reduces the vanishing error term by $ \\bar { \\rho } $ , and adds a non-vanishing bias $ Q ( \\bar { \\rho } , \\tilde { \\rho } ) =O ( \\frac { \\tilde { \\rho } } { \\bar { \\rho } } -1 ) $ . So , based on the rate given in Theorem 3.1 , biased selection is beneficial only if $ \\bar { \\rho } > 1 $ and the difference $ \\tilde { \\rho } -\\bar { \\rho } $ is small ( formally it should be $ O ( ( \\bar { \\rho } -1 ) /T ) ) $ . The question is , can such biased selection strategy be designed so that it is theoretically better ( or at least not worse ) than unbiased selection strategy ? The proof of the main Theorem 3.1 largely follows the proof of Theorem 1 of [ 4 ] . It seems that the proof of Theorem 3.1 deviates from the proof of Theorem 1 [ 4 ] only in derivations ( 53 ) - ( 63 ) , where biased-ness of the client selection kicks in and skewness metrics get involved . Such overlaps in proof techniques should be mentioned and some discussion is needed to highlight the novelty of the proposed analysis . Using insights from Theorem 3.1 , the paper proposes a client selection strategy ( with two practical variations ) , called power-of-choice , which aims to speed up the convergence of vanishing error by maximizing rho_bar . However , it is not clear how the other metric $ \\tilde { \\rho } $ would behave in this selection strategy . Although inspired from Theorem 3.1 , I view the power-of-choice selection strategy as a heuristic idea as ( i ) it focuses only on the vanishing error term and does nothing to minimize the bias term , ( ii ) no theoretical estimates are developed for the metrics $ \\bar { \\rho } , \\tilde { \\rho } $ in this specific selection strategy . With points made above , the theoretical contribution of the paper is weak . I think , the paper would largely improved if it were developed a way on how to avoid the bias term while using biased selection . One possible way is to design a suitable selection strategy which allows to bound the bias . For instance , from the experiment shown on Figure 2.b , it is tempting to show that for the proposed pow-d selection strategy the bias term $ ( \\frac { \\tilde { \\rho } } { \\bar { \\rho } } - 1 ) $ is $ O ( d/K ) $ , which can be controlled via parameter $ d $ . Another possible way might be to incorporate some mechanism on top of FedAvg algorithm similar to what error compensation ( or error feedback ) does for biased ( contractive ) compression operators . Experiments are okay , but they use up to 100 clients which is far from the scale of typical FL applications . In addition , in the experiment shown in Figure 4.b , the training loss seems to be decreasing steadily and non vanishing bias term is not dominated there . As the vanishing error term reduces by choosing larger $ d $ , why in this experiment smaller $ d=6 $ performs better than larger $ d=15 $ ? [ 1 ] A Khaled , K Mishchenko , and P Richt\u00e1rik . Tighter theory for local SGD on identical and heterogeneous data . In The 23rd International Conference on Artificial Intelligence and Statistics ( AISTATS 2020 ) , 2020 . [ 2 ] Blake Woodworth , Kumar Kshitij Patel , Sebastian U Stich , Zhen Dai , Brian Bullins , H Brendan McMahan , Ohad Shamir , and Nathan Srebro . Is local SGD better than minibatch SGD ? arXiv preprint arXiv:2002.07839 , 2020 . [ 3 ] Anastasia Koloskova , Nicolas Loizou , Sadra Boreiri , Martin Jaggi , and Sebastian U Stich . A unified theory of decentralized SGD with changing topology and local updates . arXiv preprint arXiv:2003.10422 , 2020 . [ 4 ] Xiang Li , Kaixuan Huang , Wenhao Yang , Shusen Wang , and Zhihua Zhang . On the convergence of fedavg on non-iid data . In International Conference on Learning Representations ( ICLR ) , July 2020 . [ 5 ] Yichen Ruan , Xiaoxi Zhang , Shu-Che Liang , and Carlee Joe-Wong . Towards flexible device participation in federated learning for non-iid data . ArXiv , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the positive feedback and detailed constructive suggestions on our paper . We believe that we have addressed all the reviewer \u2019 s comments fully , and hope the reviewer can increase the score in reflect of our responses and improvements in the paper . We address the reviewer 's feedback accordingly as follows : Q1 : * Assumption 3.4 seems problematic as together with Assumption 3.3 it implies that gradients of local loss functions Fk are uniformly bounded . This is in conflict with Assumption 3.2 as local losses Fk are assumed to be strongly convex . For instance , in papers [ 1,2,3 ] the Assumption 3.4 is not needed . Can the current theory be extended by relaxing Assumption 3.4 ? * A1 : - We acknowledge that our analysis is based on assumption 3.4 , uniformly bounded stochastic gradients . We would like to stress that the problem of the convergence of biased client selection strategies is non-trivial and the papers the reviewer mentioned only consider unbiased client selection strategies . Although we started off with the corresponding assumptions , we intend to relax these assumptions in our future work ( i.e. , non-convex scenarios without assumption 3.4 ) . We also would like to stress that our paper 's significance is in the novelty of the first analysis for biased client selection strategies that have not been looked into before and the insight it gives to the FL community along with the effectiveness of our proposed power-of-d client strategy . To the best of our knowledge , there has been no work giving convergence insights for biased client selection strategies that are cognizant of the training process for either strongly-convex or non-convex functions . Our work presents the first insight in this area , and we intend to extend the analysis to non-convex scenarios as the next step . We do not claim novelty in the analysis technique itself . Q2 : * For Definition 3.1 on local-global objectivity gap , it should be mentioned that this notion was defined and used earlier in [ 4 ] . It should be credited properly . * A2 : - We thank the reviewer for pointing this out ! We completely agree that the definition of the local-global objectivity gap should be duly credited to [ 4 ] . We have included a sentence about this in the updated version , which will be uploaded soon . Q3 : * The question is , can such biased selection strategy be designed so that it is theoretically better ( or at least not worse ) than unbiased selection strategy ? * A3 : - Similar point with Q5 below , addressed in reply for Q5 . Q4 : * Such overlaps in proof techniques should be mentioned and some discussion is needed to highlight the novelty of the proposed analysis . * A4 : - Thank you very much for pointing this out ! We completely agree with the reviewer that the overlap with proof techniques used in [ 4 ] should be duly acknowledged . We will certainly discuss this in the updated version and will highlight that the main novelty of our paper as compared to [ 4 ] is that we consider biased selection policies and show how the selection skew affects convergence . Q5 : * However , it is not clear how the other metric \u03c1~ would behave in this selection strategy . Although inspired by Theorem 3.1 , I view the power-of-choice selection strategy as a heuristic idea as ( i ) it focuses only on the vanishing error term and does nothing to minimize the bias term * A5 : - We appreciate the reviewer 's opinion regarding the bias term and the suggestions for improving the pow-d strategy . We would first like to highlight that the pow-d strategy spans a natural trade-off between convergence speed and selection bias . The choice of $ d $ controls this trade-off . For example , $ d=m $ can entirely eliminate the selection bias with seeing no benefit in convergence speed . Increasing $ d $ will improve the convergence speed but may increase the non-vanishing error term . In our view , the understanding of this trade-off is one of the main contributions of our paper . Nevertheless , we agree with the reviewer 's point that the paper can be improved with a selection strategy that tackles the selection bias while gaining the benefit of convergence speed . This can be simply achieved by adaptively reducing $ d $ of our pow-d strategy during the course of training . Henceforth , we included the proposition of 'adapow-d ' in the updated paper , which decreases $ d $ throughout training eventually dropping down to $ d=m $ . This enables the training loss to eventually converge to the minimal point without the selection bias , while also enjoying the convergence speed in the initial training phase with larger $ d > m $ . We verify the performance of 'adapow-d ' in both the quadratic and synthetic simulations wherein both simulations the training loss converges without the selection bias while having faster convergence speed than the baseline strategy . We plan to add these result plots of \u2018 adapow-d \u2019 to the updated version of the paper ."}, "1": {"review_id": "PYAFKBc8GL4-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper analyzes a biased client selection policy in the context of the federated learning paradigm . In particular , instead of random selection of the clients with a probability that is proportional to the size of the local dataset , clients with higher local loss values are selected . Despite being already used as heuristics , the authors claim that their work is the first one to propose a convergence analysis of a biased client selection mechanism in the context of federated learning . Under some ( quite stringent ) assumptions , the authors derive a global bound for the expected error after $ T $ iterations . The bound is composed of two terms : a vanishing term and a non-vanishing one which comes as a consequence of the biased client selection mechanism . From Theorem 3.1 it emerges clearly that the biased selection mechanism poses a trade-off : on one hand the non-vanishing term increases the more biased is the selection , on the other hand the vanishing one benefits from the biased selection mechanism since the more biased is the selection the faster it will decay to zero . The biased selection mechanism comes with some additional computational and communication costs with respect to a fully random selection : the authors propose a couple of heuristics to alleviate these extra-costs . In the experimental section the proposed method is evaluated on different benchmarks and with different datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : The paper is well-written and the theoretical results look correct . At the same time , there are some weaknesses ( see the section on Cons ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper takes is well-written and really easy to follow . 2.The authors attempt to provide a more rigorous analysis to an already-in-use heuristic and it is the first time that such analysis is carried out in this context . 3.The paper has a good balance across the sections and it touches on theory and experimental part in a balanced way . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The assumptions on which the theoretical analysis is based are quite stringent and could be further relaxed ( especially strong convexity ) . 2.Incoherent benchmarks : the authors are not reasoning regarding the introduced assumptions and whether they hold . It actually seems that at least in the DNN case the assumptions are not holding since the landscape is notoriously non-convex . 3.Experiments only show the behavior of the heuristics pow-d. What happens when d=K ? In order for the experiment to be in line with the theoretical results this case should be analyzed at least once . Introducing the extra random selection of $ m\\leq d\\leq K $ clients could indeed change the setting and therefore the convergence . 4.Way too strong and unjustified statements , i.e.faster convergence to a global minimum ( according to Th.3.1.it does not converge ) , convergence up to 3 times faster ( in one benchmark , might easily depend on hyperparameter settings or on the heuristic added on top on power-of-chance ) . 5.All the statements regarding the test accuracy are pure speculations as there are no theoretical and/or consistent and extensive empirical evidence which suggest that the biased selection mechanism leads to better generalization . Intuitively adding a bias in the selection should lead more easily to overfitting . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Please address and clarify the cons above # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Interesting General Aspect Not Considered 1. local SGD iterations : how does $ \\tau $ impacts on the convergence ? Is it beneficial to perform more local SGD steps or does it become harmful for the convergence ? How does this relate with the biased selection mechanism ? 2. first random selection of $ d $ clients based on proportions of dataset : unbalanced work load might create inefficiency since the general method described is synchronized .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the detailed review of the paper ! We appreciate the reviewer 's comment on our paper being balanced on both theory and experiments , well-written , and novelty of the analysis of biased client selection . We address the reviewer 's concerns below . We hope the reviewer re-evaluates the score in favor of the paper ! Q1 : * The assumptions on which the theoretical analysis is based are quite stringent and could be further relaxed ( especially strong convexity ) . * A1 : - We acknowledge that our analysis is based on strongly-convex functions , and do not include non-convex ones . Our paper 's significance is in the novelty of the first analysis for biased client selection strategies that have not been looked into before and the insight it gives to the FL community along with the effectiveness of our proposed power-of-d client strategy . To the best of our knowledge , there has been no work giving convergence insights for biased client selection strategies that are cognizant of the training process for either strongly-convex or non-convex functions . Our work presents the first insight in this area , and we intend to extend the analysis to non-convex scenarios as the next step . We would like to highlight that between two different sets of analysis techniques ( strongly-convex and non-convex ) , considering the latter is outside the scope of the current paper . Q2 : * Incoherent benchmarks : the authors are not reasoning regarding the introduced assumptions and whether they hold . It actually seems that at least in the DNN case the assumptions are not holding since the landscape is notoriously non-convex . * A2 : - The reviewer is correct in that the DNN loss landscape is non-convex . While we do not claim that our convergence analysis holds in this case , the experiments on DNN show that the proposed client selection strategies do work well even for non-convex loss landscapes . The strongly-convex assumption does hold for the quadratic simulation results presented in Fig.2.Note that we do not introduce any new assumptions that haven \u2019 t been used in previous literature . [ 1-4 ] also use the assumptions used in our paper to provide theoretical insight into distributed algorithms . - [ 1 ] Sebastian Urban Stich . Local sgd converges fast and communicates little . In ICLR 2019 [ 2 ] Debraj Basu , Deepesh Data , Can Karakus , and Suhas Diggavi . Qsparse-local-SGD : Distributed SGD with Quantization , Sparsification , and Local Computations . In NeurIPS 2019 [ 3 ] Xiang Li , Kaixuan Huang , Wenhao Yang , Shusen Wang , and Zhihua Zhang . On the convergence of fedavg on non-iid data . In International Conference on Learning Representations ( ICLR ) , July 2020 . [ 4 ] Yichen Ruan , Xiaoxi Zhang , Shu-Che Liang , and Carlee Joe-Wong . Towards flexible device participation in federated learning for non-iid data . ArXiv , 2020 . Q3 : * Experiments only show the behavior of the heuristics pow-d. What happens when d=K ? In order for the experiment to be in line with the theoretical results this case should be analyzed at least once . Introducing the extra random selection of m\u2264d\u2264K clients could indeed change the setting and therefore the convergence . * A3 : - We would like to clarify that our theoretical analysis is a general analysis that is applicable for any selection strategy $ \\pi $ that is cognizant of the training progress . It is not limited to the pow-d strategy or any specific value of $ d $ such as $ d=K $ . The case when $ m\u2264d\u2264K $ clients is subsumed in the convergence analysis . As $ d $ increases , the $ \\overline { \\rho } $ also increases along with the selection bias , as shown in Fig.2 ( b ) , being consistent with our theoretical results . - We show the effect of $ d=K $ for the quadratic simulations in Fig.2 ( a ) and Fig.2 ( b ) and the synthetic simulations for $ K=30 , m=3 $ in Fig.3 . We show that for $ d=K $ which is the case for maximum selection bias , we achieve higher convergence speed but also the highest selection bias which is in line with our theoretical results ."}, "2": {"review_id": "PYAFKBc8GL4-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper studies federated learning ( FL ) and proposes nonuniform sampling of participating clients . Clients are selected according to their losses ; clients with big losses are likely selected . The proposed algorithm has a rigorous convergence analysis . The proposed algorithm is demonstrated powerful on small scale datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I find this submission a borderline paper . I am fine with either acceptance or rejection . This paper has theoretical guarantees and strong empirical advantages . But the actually used algorithm may not be very useful . The experiments are conducted on small datasets . I am not confident whether this work will have an advantage on large-scale data . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : + The idea of choosing participating clients according to loss is an interesting and reasonable idea . + The non-uniform sampling of clients is proved to converge . + The nonuniform sampling algorithm has a strong empirical advantage . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The actually used algorithm is the power-of-choice strategy . It firstly uses a uniformly sampled set of $ d $ clients to compute losses and then sample a subset of $ m < d $ clients out of the $ d $ clients . Here is my question : The server needs to communicate with the $ d $ clients from the candidate set . The server must wait for their responses . So the $ d $ clients are actually active . Why not use all the $ d $ clients to compute gradients ? Is the proposed strategy comparable to using all the $ d $ clients ? 2 FMNIST seems to be the only used dataset . The dataset is not big enough . Even if it is big enough , I would like to see empirical results on other datasets , e.g. , MNIST , CIFAR10 , mini-ImageNet , etc . Admittedly , this work has an advantage on FMNIST . But I am not convinced that this work is better in general . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "6: Marginally above acceptance threshold", "reply_text": "We greatly thank the reviewer for the positive feedback on our paper . The reviewer raised two issues : 1 ) why the pow-d strategy does not use all d clients for training , and 2 ) that the experiments on the FMNIST dataset are not enough . We address both these concerns below . Q1 : * The actually used algorithm is the power-of-choice strategy . It firstly uses a uniformly sampled set of d clients to compute losses and then sample a subset of m < d clients out of the d clients . Here is my question : The server needs to communicate with the d clients from the candidate set . The server must wait for their responses . So the d clients are actually active . Why not use all the d clients to compute gradients ? Is the proposed strategy comparable to using all the d clients ? * A1 : - First , having the $ d $ clients actually perform the local updates rather than just send their local loss values will incur a significantly higher computation cost than performing updates at only $ m < d $ clients . We highlight that pow-d does not require $ d $ clients to actually perform gradient updates , but just requires inference tasks on the training data for the loss values . - Moreover , to address the possible communication and computation cost for the loss inference task of the $ d $ clients , in the paper we propose rpow-d , which simply uses the last received loss values from the previously selected clients to select $ m $ clients from the selected $ d $ clients in the subset . rpow-d requires no additional computation or communication cost compared to the baseline random strategy , but performs better than the baseline as shown in our DNN experiments . - Lastly , we emphasize that pow-d does not uniformly sample the final $ m $ clients , but sample them according to their importance ( i.e. , loss values ) . Therefore , our strategy is distinguished from simply uniformly sampling in proportion to dataset size of $ d $ clients and performing local updates . Q2 : * FMNIST seems to be the only used dataset . The dataset is not big enough . Even if it is big enough , I would like to see empirical results on other datasets , e.g. , MNIST , CIFAR10 , mini-ImageNet , etc . Admittedly , this work has an advantage on FMNIST . But I am not convinced that this work is better in general . * A2 : - For more experimental validation , since the MNIST and CIFAR10 datasets have the same size as the FMNIST dataset considered in our experiments , we have added experiments on a larger dataset EMNIST ( `` by Digits '' ) . The EMNIST dataset is 4 times larger ( 240,000 training data with 40,000 testing data ) than FMNIST , MNIST , and CIFAR10 ( which have 60000 training samples each ) . - We show consistency in the effective performance in training loss and test accuracy results for our proposed pow-d strategy . Note that the performance improvement offered by the pow-d strategy and its variants is due to data heterogeneity across clients and not specific to one particular dataset . So we expect to see similar performance on other datasets . Moreover , we stress that the experiments are a sanity check for the performance of our proposed pow-d strategy and its variants . Our paper \u2019 s main novelty and contribution is that it gives the first convergence analysis of biased client selection strategies , which has not been investigated before ."}, "3": {"review_id": "PYAFKBc8GL4-3", "review_text": "This paper analyzes the convergence of FedAvg with biased client selection strategies . And the new strategy power-of-choice is designed based on the convergence results . This new strategy is numerically compared with two benchmark strategies and shows faster convergence and higher testing accuracy . Pros : 1.The convergence analysis is new and novel . This paper provides the first convergence analysis for a general class of biased client selection strategies . New concept , selection skew , is introduced as the measure of strategies . Based on the new concept , the analysis quantifies how the bias of the strategy affects the convergence speed of FedAvg . 2.New strategy pow-d is proposed based on the insights of the convergence analysis . The performance of pow-d is impressive in the FMNIST experiment , where it significantly improves the test accuracy under random selection strategy . Cons : 1.Although the analysis is interesting and insightful , the effect of \\rho seems to be limited . There are three constant terms in the vanishing error term in Eq . ( 7 ) . And only the first term will be decreased when increasing \\bar { \\rho } . However , it is not obvious if the first term is the dominating term . 2.Is it possible to give an exact or estimated values of \\bar\\rho and \\tilde\\rho for different strategies in the numerical experiments ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for highlighting the paper \u2019 s key novel contributions : 1 ) that we provide the first convergence analysis of biased client selection strategies and 2 ) the proposed pow-d strategies gives an impressive improvement in the FMNIST experiments . Our responses to both the concerns are given below . Q1 : * Although the analysis is interesting and insightful , the effect of \\rho seems to be limited . There are three constant terms in the vanishing error term in Eq . ( 7 ) . And only the first term will be decreased when increasing \\bar { \\rho } . However , it is not obvious if the first term is the dominating term . * A1 : - We show that the first term $ \\frac { 4L ( 32\\tau^2 G^2+\\sigma^2/m ) } { 3\\mu^2\\overline { \\rho } } $ is indeed significant in the vanishing error term , below , $ \\frac { 1 } { ( T+\\gamma ) } \\left [ \\frac { 4L ( 32\\tau^2 G^2+\\sigma^2/m ) } { 3\\mu^2\\overline { \\rho } } +\\frac { 8L^2\\Gamma } { \\mu^2 } +\\frac { L\\gamma\\|\\overline { \\mathbf { w } } ^ { ( 0 ) } -\\mathbf { w } ^ * \\|^2 } { 2 } \\right ] $ , through experiments held out in three different settings : quadratic ( strongly-convex case ) , synthetic dataset ( logistic regression problem ) , and DNN with FMNIST dataset . Note that the quadratic simulation directly reflects the theory due to the quadratic function 's strongly-convex property . Especially in the quadratic and synthetic simulations we show that biasing more towards the clients with larger loss , i.e. , larger $ \\overline { \\rho } > 1 $ , leads to faster convergence . If the first term not was not dominant , larger $ \\overline { \\rho } > 1 $ would not have led to faster convergence due to the other two terms in the vanishing error term . Figure 2 shows how $ \\overline { \\rho } $ actually matters with estimated $ \\overline { \\rho } $ values by showing the convergence speed up . - In addition to the point above , note that the first term is the only term that can be controlled through client selection ( the number of local updates $ \\tau $ , stochastic gradient bound $ G $ , and variance bound $ \\sigma $ ) . The other two terms are properties of the objective function , dataset distributions , and the initial point which can not be controlled . Therefore we believe that the decrease by a factor of $ \\overline { \\rho } $ in the first term is sufficient to show the positive effect of biasing client selection . Q2 : * Is it possible to give an exact or estimated values of \\bar\\rho and \\tilde\\rho for different strategies in the numerical experiments ? * A2 : - This question is already answered in Fig 2 ( b ) , where we plot the estimated values of $ \\overline { \\rho } $ and $ \\widetilde { \\rho } /\\overline { \\rho } $ for varying $ d $ and $ K $ and for different client selection strategies . Perhaps the reviewer missed seeing it -- we will make it more prominent in an updated version . Our theory matches well with the quadratic simulations in that as $ d $ increases the estimated $ \\overline { \\rho } $ increases but the selection bias $ \\widetilde { \\rho } /\\overline { \\rho } $ also increases . The methodology of getting these estimated theoretical values is elaborated in Appendix F of the original paper ."}}