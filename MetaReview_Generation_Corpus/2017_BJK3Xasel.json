{"year": "2017", "forum": "BJK3Xasel", "title": "Nonparametric Neural Networks", "decision": "Accept (Poster)", "meta_review": "The paper presents a clean framework for optimizing for the network size during the training cycle. While the complexity of each iteration is increased, they argue that overall, the cost is significantly reduced since we do not need to train networks of varying sizes and cross-validate across them. The reviewers recommend acceptance of the paper and I am in agreement with them.", "reviews": [{"review_id": "BJK3Xasel-0", "review_text": "This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below: What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments. It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , Thank you for the review . The additional complexity of AdaRad and unit addition / deletion over parametric SGD is cp per minibatch , where c is a constant and p is the number of parameters , assuming a dense weight matrix . The relevance of this overhead depends thus on the size of the minibatch . I will add this comment and some runtimes to the final version of the paper . `` It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks . Any insight on this ? '' We 're working on this ATM ... My current guess would be that we need to figure out how to train new units even faster , i.e.50 iterations is still not enough to initialize new units . ( Of course , I 'm reluctant to reduce the radial step size to keep the overhead of useless units during training bearable . ) Other things we are looking at are : bad co-adaptations of units early in training ( also referred to as `` stuck in bad local minima '' ) which we try to combat with noise / regularization or `` shocking '' the parameters from time to time as in `` DSD : Dense-Sparse-Dense Training for Deep Neural Networks '' , submitted to this very conference . Thanks , George"}, {"review_id": "BJK3Xasel-1", "review_text": "I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge. The authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data. That reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights. I would have like more intuition to be given for this theorem. It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper. For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results. Please also see below (point 2) for some suggestions. I have a few other comments to make: 1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory? I understand that experiments along this line are given in Figure 2, with mixed results. The Figure i must say, is not very clear, but it is possible to interpret under careful inspection. In some the non-parametric nets are doing better and others are doing worse than the parametric ones. Even in such case i could see the usefulness of the method as it helps discovering the structure. What i don't fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance? Could you give more insight? 2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight? As i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up. For example, can regularizer parameter lambda be predicted given the data - is there a property in the data that can help guessing the right lambda? My feeling is that lambda is the key factor for determining the final net structure. Is this true? How much does the structure of the final net depend on the initialization? Do you get different nets if you start from different random weights? How different are they? What happens when fan in and fan out regularizers are combined? Do you still have the same theoretical result? I have a few additional questions: 1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values? 2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think. I changed my rating to 7, while hoping that the authors will address my comments above. ", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , I am dismayed that you believe that our paper only warrants 2 short paragraphs of review . While our paper is n't perfect , I think it deserves more effort . The review also did not justify the rating you gave the paper . Could you clarify why you chose the rating you did ? Nevertheless , I will look at each paragraph . `` An interesting experiment would be to show that a model such as yours , where the nodes ( neurons ) are added or removed automatically can outperform a net with the same numb er of nodes ( at the end , after complete learning ) , in which the size and number of nodes per layer are fixed from the start . This would prove the efficiency of the idea . This is where your method is interesting : do you save nodes that are not needed and replace them with nodes that are needed ? Do you optimize performance vs . memory ? `` I am not quite sure what you mean here . If you are asking me to compare the performance of our method against training networks with a fixed number of neurons throughout the entire training process , we do precisely this in section 4.1 / Figure 2 . If you are asking me to compare our method against starting with a large network and then eliminating neurons but not adding them , I would agree that such a comparison is interesting . I guess one could incorporate the l2 regularization hyperparameter into the random search process and allow it to eliminate neurons . I will make sure to include this in the next version as an additional experiment . `` Can you better discuss the meaning and implications of Theorem 1 . I feel this theorem is just put there with no proper discussion . Beyond the proofs , from the Appendix , what is the key insight of the Theorem ? What does it say , in plain English ? To me , the conclusion seems almost natural and obvious . Is there some powerful insight ? '' Please refer to the fourth paragraph of section 2 , which is : * * * In the nonparametric setting , because the existence of a global minimum is not guaranteed , we may be able to reduce the error further and further by using larger and larger networks . This would be problematic , because as networks become better and better with regards to the objective , they would become more and more undesirable in practice . * * * The theorem simply states that this does not occur . While this result is important , it is not spectacular . It is more of a bookkeeping result , a confirmation of a basic property of our algorithm . Hence , the proof is also in the appendix . If you were looking for some deep insight , I do n't think there is any to find . The theorem may seem `` natural and obvious '' to you , but the complexity of the proof shows that it clearly is n't . Are you aware of a shorter and simpler proof ? George"}, {"review_id": "BJK3Xasel-2", "review_text": "This paper addresses the problem of allowing networks to change the number of units that are used during training. This is done in a simple but elegant and well-motivated way. Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero. The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units. In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case. One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters. One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem. The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero. It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches. In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case. The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically. This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular. However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks. Another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time. Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy. This means that the cost of grid search is not always paid, but the slowness of the authors\u2019 approach may be endemic. The authors do not discuss how this issue will scale as much larger networks are trained. It is a concern that this approach may not be practical for large-scale networks, because training will be very slow. In general, the experiments are helpful and encouraging, but not comprehensive or totally convincing. I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful. Overall, I found this to be an interesting and clearly written paper that makes a potentially useful point. The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision. But the current results remain pretty speculative. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , Thank you for your time and effort in writing this review . Before I respond to each paragraph of the review , I am would love to ask you if there are any specific experiments that you would like to see done . This would greatly help me improve the paper . Thank you very much . My main response is that most of your criticisms apply to many or all hyperparameter optimization papers published at top ML conferences in recent years and are indeed intrinsic to the task of hyperparameter optimization . I don \u2019 t think it makes sense to punish this paper for the difficulties that make hyperparameter optimization a hard problem and , in my opinion , and interesting problem to study in the first place . On to the paragraphs : `` In practice , this result does not guarantee that the resulting network will not over- or under-fit the training data , but some initial experiments show that this does not seem to be the case . '' Guaranteeing that a model does not overfit or underfit data is a very difficult problem in general . I am not aware of any hyperparameter optimization method that achieves this without an expensive validation procedure . Note though that our model can guarantee overfitting by setting lambda to zero and guarantee underfitting by setting lambda to infinity . Hence it is reasonable to expect that there exists some value in between these two extremes that balances capacity and generalization . `` One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters . One disadvantage of this approach ( and maybe any such approach ) is that it does not really solve this problem . The network still has several hyperparameters that implicitly control the number of units that will emerge , including parameters that control how often new units are added and how rapidly weights may decay to zero . It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches . In fairness , the authors do not claim that they have made training easier , but it is a little disappointing that this does not seem to be the case . '' Every single hyperparameter optimization algorithm for neural networks has itself hyperparameters . It lies in the nature of hyperparameters that all algorithms that are not extremely simple have them . Since setting hyperparameters of deep networks is a complex problem , algorithms created to address this problem are themselves complex and thus have hyperparameters . Consider the papers : Jost Tobias Springenberg , Aaron Klein , Stefan Falkner , and Frank Hutter . Bayesian optimization with robust bayesian neural networks . In NIPS , 2016 . Jasper Snoek , Oren Rippel , Kevin Swersky , Ryan Kiros , Nadathur Satish , Narayanan Sundaram , Md . Mostofa Ali Patwary , Prabhat , and Ryan P. Adams . Scalable bayesian optimization using deep neural networks . In ICML , 2015 . Both of those papers describe extremely complex hyperpareter optimization schemes that introduce many more and more complex hyperameters than our method . If anything , the fact that we add so few hyperparameters compared to other papers is an advantage for us . The goal of automated hyperparameter optimization is generally to replace `` hard hyperparameters '' with `` easy hyperparameters '' . Hard hyperparameters are those to which the algorithm is sensitive and which need to be set to different values for different datasets . Easy hyperparameters are those to which the algorithm is not sensitive or hyperparameters for which a single value can be chosen for all datasets or which can be adaptively chosen via a rule of thumb . The standard methodology for hyperparameter optimization papers is to ( i ) identify hard hyperparameters , ( ii ) introduce an algorithm with new hyperparameters that automatically chooses the original hyperparameters and ( iii ) argue that the new hyperparameters are easy hyperparameters . For example , the two papers cited above use this methodology and argue that their plethora of newly introduced hyperparameters are easy . We follow this same 3-step methodology in our paper . First , we identify hard hyperparameters ( size of each layer ) . Then , we introduce an algorithm with three additional hyperparameters : ( 1 ) unit addition rate ( 2 ) radial step size ( 3 ) regularization hyperparameter In our experiments , we choose hyperparameters ( 1 ) and ( 2 ) uniformly across datasets and in advance . We set the unit addition rate to one and the radial step size to one over fifty . While this does not prove that those values are good for all datasets , it provides significant evidence that those hyperparameters are indeed easy , at least for small-to-medium datasets . Hyperparameter ( 3 ) is hard . However , note that we have replaced one hard parameter per layer ( size ) with one hard parameter for the entire network ( lambda ) . Secondly , note that even in parametric training the regularization hyperparameter exists , hence it is debatable whether we even added it in the first place . Thirdly , eliminating the regularization hyperparameter altogether would be tantamount to eliminating the possibility of over- and underfitting which , as I pointed out above , is an unrealistic expectation . So , we introduce two hyperparameters that we were able to choose `` blind '' in our experiments ( no pre-tuning ) and one hyperparameter that is not really new to begin with . The two papers I cited above don \u2019 t get anywhere close to this . They simply state that `` after experiments , we found a way of setting the new hyperparameters that worked for us '' . If anything , the criterion of additional hyperparameters is a strength of our paper , not a weakness . In summary , we very much DO claim to make training easier . Our solution is : instead of tuning layer size and lambda , tune only lambda while setting unit addition rate to one and radial step size to one over fifty . `` The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically . This is potentially important , because smaller networks can reduce run-time at testing , and power consumption and memory footprint , which is important on mobile devices in particular . However , the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks ( eg. , by pruning trained networks ) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks . '' Based on your review , I get a sense that you see this paper as \u201e pruning but with unit addition \u201c . This paper is NOT comparable to pruning papers , because pruning uses a good network size as prior information to start training , whereas we do not . I can not overemphasize the importance of this difference . Pruning papers invariably use the following strategy : - Train a network we know works for a given task - Remove some edges / neurons that seem unimportant based on rules of thumb that are often as simple as \u201e remove edges with low absolute weights \u201c Compare this to the recent paper by Springenberg et . al . ( see full citation above ) This paper performs hyperparameter optimization for neural networks . To do this , it performs global model-based optimization via a bayesian neural network that is significantly more complicated than even the original network that is being tuned and introduce a ton of new hyperparameters . So the difference between knowing a good starting network size and not knowing it is the difference between \u201e remove some edges with small weights \u201c and \u201e run the network hundreds of times guided by a Bayesian neural network \u201c . If you don \u2019 t appreciate this difference , it is impossible for you to review our paper fairly . The reason we don \u2019 t compare our method to pruning methods is because they address different problems . If we were to compare to pruning , we would have to give pruning a starting network size . If we did , our method , like all hyperparameter optimization methods , could not hope to outperform pruning , or even to achieve comparable performance . `` In practice , many parametric approaches require methods like grid search to choose hyperparameters , which can be very slow , but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy . This means that the cost of grid search is not always paid , '' We specifically state that our method is most useful when there is no prior knowledge regarding what a good network looks like . This is the case for the vast majority of datasets in the world . There are many more datasets in the world than experiments published in ML conferences . It is hardly a significant weakness that we only cater to this vast majority . `` but the slowness of the authors \u2019 approach may be endemic . The authors do not discuss how this issue will scale as much larger networks are trained . It is a concern that this approach may not be practical for large-scale networks , because training will be very slow . '' I agree that results on large datasets would make the paper stronger . However , again , not all datasets in the world are large . A method that is simple , innovative and powerful for small to medium-sized datasets may still be interesting . I do n't think that a machine learning method must necessary be applicable to ImageNet to be scientifically interesting or useful in practice , or to warrant publication . Also , note that our methodology for evaluating our algorithm required around 1000 training runs per dataset . The requirements for expensive validation procedures is also intrinsic to hyperparameter optimization . Consider for example , in addition to Springenberg et al , the following papers : Jelena Luketina , Mathias Berglund , Klaus Greff , and Raiko Tapani . Scalable gradient-based tuning of continuous regularization hyperparameters . In ICML , 2016 . Dougal Maclaurin , David Duvenaud , and Ryan P. Adams . Gradient-based hyperparameter optimization through reversible learning . In ICML , 2015 . All of these hyperparameter optimization papers were published at top conferences , but did not apply their algorithm to large-scale datasets . All of them used many training runs to validate their methods . Thanks again , George"}], "0": {"review_id": "BJK3Xasel-0", "review_text": "This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below: What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments. It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , Thank you for the review . The additional complexity of AdaRad and unit addition / deletion over parametric SGD is cp per minibatch , where c is a constant and p is the number of parameters , assuming a dense weight matrix . The relevance of this overhead depends thus on the size of the minibatch . I will add this comment and some runtimes to the final version of the paper . `` It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks . Any insight on this ? '' We 're working on this ATM ... My current guess would be that we need to figure out how to train new units even faster , i.e.50 iterations is still not enough to initialize new units . ( Of course , I 'm reluctant to reduce the radial step size to keep the overhead of useless units during training bearable . ) Other things we are looking at are : bad co-adaptations of units early in training ( also referred to as `` stuck in bad local minima '' ) which we try to combat with noise / regularization or `` shocking '' the parameters from time to time as in `` DSD : Dense-Sparse-Dense Training for Deep Neural Networks '' , submitted to this very conference . Thanks , George"}, "1": {"review_id": "BJK3Xasel-1", "review_text": "I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge. The authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data. That reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights. I would have like more intuition to be given for this theorem. It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper. For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results. Please also see below (point 2) for some suggestions. I have a few other comments to make: 1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory? I understand that experiments along this line are given in Figure 2, with mixed results. The Figure i must say, is not very clear, but it is possible to interpret under careful inspection. In some the non-parametric nets are doing better and others are doing worse than the parametric ones. Even in such case i could see the usefulness of the method as it helps discovering the structure. What i don't fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance? Could you give more insight? 2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight? As i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up. For example, can regularizer parameter lambda be predicted given the data - is there a property in the data that can help guessing the right lambda? My feeling is that lambda is the key factor for determining the final net structure. Is this true? How much does the structure of the final net depend on the initialization? Do you get different nets if you start from different random weights? How different are they? What happens when fan in and fan out regularizers are combined? Do you still have the same theoretical result? I have a few additional questions: 1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values? 2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think. I changed my rating to 7, while hoping that the authors will address my comments above. ", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , I am dismayed that you believe that our paper only warrants 2 short paragraphs of review . While our paper is n't perfect , I think it deserves more effort . The review also did not justify the rating you gave the paper . Could you clarify why you chose the rating you did ? Nevertheless , I will look at each paragraph . `` An interesting experiment would be to show that a model such as yours , where the nodes ( neurons ) are added or removed automatically can outperform a net with the same numb er of nodes ( at the end , after complete learning ) , in which the size and number of nodes per layer are fixed from the start . This would prove the efficiency of the idea . This is where your method is interesting : do you save nodes that are not needed and replace them with nodes that are needed ? Do you optimize performance vs . memory ? `` I am not quite sure what you mean here . If you are asking me to compare the performance of our method against training networks with a fixed number of neurons throughout the entire training process , we do precisely this in section 4.1 / Figure 2 . If you are asking me to compare our method against starting with a large network and then eliminating neurons but not adding them , I would agree that such a comparison is interesting . I guess one could incorporate the l2 regularization hyperparameter into the random search process and allow it to eliminate neurons . I will make sure to include this in the next version as an additional experiment . `` Can you better discuss the meaning and implications of Theorem 1 . I feel this theorem is just put there with no proper discussion . Beyond the proofs , from the Appendix , what is the key insight of the Theorem ? What does it say , in plain English ? To me , the conclusion seems almost natural and obvious . Is there some powerful insight ? '' Please refer to the fourth paragraph of section 2 , which is : * * * In the nonparametric setting , because the existence of a global minimum is not guaranteed , we may be able to reduce the error further and further by using larger and larger networks . This would be problematic , because as networks become better and better with regards to the objective , they would become more and more undesirable in practice . * * * The theorem simply states that this does not occur . While this result is important , it is not spectacular . It is more of a bookkeeping result , a confirmation of a basic property of our algorithm . Hence , the proof is also in the appendix . If you were looking for some deep insight , I do n't think there is any to find . The theorem may seem `` natural and obvious '' to you , but the complexity of the proof shows that it clearly is n't . Are you aware of a shorter and simpler proof ? George"}, "2": {"review_id": "BJK3Xasel-2", "review_text": "This paper addresses the problem of allowing networks to change the number of units that are used during training. This is done in a simple but elegant and well-motivated way. Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero. The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units. In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case. One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters. One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem. The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero. It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches. In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case. The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically. This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular. However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks. Another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time. Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy. This means that the cost of grid search is not always paid, but the slowness of the authors\u2019 approach may be endemic. The authors do not discuss how this issue will scale as much larger networks are trained. It is a concern that this approach may not be practical for large-scale networks, because training will be very slow. In general, the experiments are helpful and encouraging, but not comprehensive or totally convincing. I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful. Overall, I found this to be an interesting and clearly written paper that makes a potentially useful point. The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision. But the current results remain pretty speculative. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , Thank you for your time and effort in writing this review . Before I respond to each paragraph of the review , I am would love to ask you if there are any specific experiments that you would like to see done . This would greatly help me improve the paper . Thank you very much . My main response is that most of your criticisms apply to many or all hyperparameter optimization papers published at top ML conferences in recent years and are indeed intrinsic to the task of hyperparameter optimization . I don \u2019 t think it makes sense to punish this paper for the difficulties that make hyperparameter optimization a hard problem and , in my opinion , and interesting problem to study in the first place . On to the paragraphs : `` In practice , this result does not guarantee that the resulting network will not over- or under-fit the training data , but some initial experiments show that this does not seem to be the case . '' Guaranteeing that a model does not overfit or underfit data is a very difficult problem in general . I am not aware of any hyperparameter optimization method that achieves this without an expensive validation procedure . Note though that our model can guarantee overfitting by setting lambda to zero and guarantee underfitting by setting lambda to infinity . Hence it is reasonable to expect that there exists some value in between these two extremes that balances capacity and generalization . `` One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters . One disadvantage of this approach ( and maybe any such approach ) is that it does not really solve this problem . The network still has several hyperparameters that implicitly control the number of units that will emerge , including parameters that control how often new units are added and how rapidly weights may decay to zero . It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches . In fairness , the authors do not claim that they have made training easier , but it is a little disappointing that this does not seem to be the case . '' Every single hyperparameter optimization algorithm for neural networks has itself hyperparameters . It lies in the nature of hyperparameters that all algorithms that are not extremely simple have them . Since setting hyperparameters of deep networks is a complex problem , algorithms created to address this problem are themselves complex and thus have hyperparameters . Consider the papers : Jost Tobias Springenberg , Aaron Klein , Stefan Falkner , and Frank Hutter . Bayesian optimization with robust bayesian neural networks . In NIPS , 2016 . Jasper Snoek , Oren Rippel , Kevin Swersky , Ryan Kiros , Nadathur Satish , Narayanan Sundaram , Md . Mostofa Ali Patwary , Prabhat , and Ryan P. Adams . Scalable bayesian optimization using deep neural networks . In ICML , 2015 . Both of those papers describe extremely complex hyperpareter optimization schemes that introduce many more and more complex hyperameters than our method . If anything , the fact that we add so few hyperparameters compared to other papers is an advantage for us . The goal of automated hyperparameter optimization is generally to replace `` hard hyperparameters '' with `` easy hyperparameters '' . Hard hyperparameters are those to which the algorithm is sensitive and which need to be set to different values for different datasets . Easy hyperparameters are those to which the algorithm is not sensitive or hyperparameters for which a single value can be chosen for all datasets or which can be adaptively chosen via a rule of thumb . The standard methodology for hyperparameter optimization papers is to ( i ) identify hard hyperparameters , ( ii ) introduce an algorithm with new hyperparameters that automatically chooses the original hyperparameters and ( iii ) argue that the new hyperparameters are easy hyperparameters . For example , the two papers cited above use this methodology and argue that their plethora of newly introduced hyperparameters are easy . We follow this same 3-step methodology in our paper . First , we identify hard hyperparameters ( size of each layer ) . Then , we introduce an algorithm with three additional hyperparameters : ( 1 ) unit addition rate ( 2 ) radial step size ( 3 ) regularization hyperparameter In our experiments , we choose hyperparameters ( 1 ) and ( 2 ) uniformly across datasets and in advance . We set the unit addition rate to one and the radial step size to one over fifty . While this does not prove that those values are good for all datasets , it provides significant evidence that those hyperparameters are indeed easy , at least for small-to-medium datasets . Hyperparameter ( 3 ) is hard . However , note that we have replaced one hard parameter per layer ( size ) with one hard parameter for the entire network ( lambda ) . Secondly , note that even in parametric training the regularization hyperparameter exists , hence it is debatable whether we even added it in the first place . Thirdly , eliminating the regularization hyperparameter altogether would be tantamount to eliminating the possibility of over- and underfitting which , as I pointed out above , is an unrealistic expectation . So , we introduce two hyperparameters that we were able to choose `` blind '' in our experiments ( no pre-tuning ) and one hyperparameter that is not really new to begin with . The two papers I cited above don \u2019 t get anywhere close to this . They simply state that `` after experiments , we found a way of setting the new hyperparameters that worked for us '' . If anything , the criterion of additional hyperparameters is a strength of our paper , not a weakness . In summary , we very much DO claim to make training easier . Our solution is : instead of tuning layer size and lambda , tune only lambda while setting unit addition rate to one and radial step size to one over fifty . `` The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically . This is potentially important , because smaller networks can reduce run-time at testing , and power consumption and memory footprint , which is important on mobile devices in particular . However , the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks ( eg. , by pruning trained networks ) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks . '' Based on your review , I get a sense that you see this paper as \u201e pruning but with unit addition \u201c . This paper is NOT comparable to pruning papers , because pruning uses a good network size as prior information to start training , whereas we do not . I can not overemphasize the importance of this difference . Pruning papers invariably use the following strategy : - Train a network we know works for a given task - Remove some edges / neurons that seem unimportant based on rules of thumb that are often as simple as \u201e remove edges with low absolute weights \u201c Compare this to the recent paper by Springenberg et . al . ( see full citation above ) This paper performs hyperparameter optimization for neural networks . To do this , it performs global model-based optimization via a bayesian neural network that is significantly more complicated than even the original network that is being tuned and introduce a ton of new hyperparameters . So the difference between knowing a good starting network size and not knowing it is the difference between \u201e remove some edges with small weights \u201c and \u201e run the network hundreds of times guided by a Bayesian neural network \u201c . If you don \u2019 t appreciate this difference , it is impossible for you to review our paper fairly . The reason we don \u2019 t compare our method to pruning methods is because they address different problems . If we were to compare to pruning , we would have to give pruning a starting network size . If we did , our method , like all hyperparameter optimization methods , could not hope to outperform pruning , or even to achieve comparable performance . `` In practice , many parametric approaches require methods like grid search to choose hyperparameters , which can be very slow , but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy . This means that the cost of grid search is not always paid , '' We specifically state that our method is most useful when there is no prior knowledge regarding what a good network looks like . This is the case for the vast majority of datasets in the world . There are many more datasets in the world than experiments published in ML conferences . It is hardly a significant weakness that we only cater to this vast majority . `` but the slowness of the authors \u2019 approach may be endemic . The authors do not discuss how this issue will scale as much larger networks are trained . It is a concern that this approach may not be practical for large-scale networks , because training will be very slow . '' I agree that results on large datasets would make the paper stronger . However , again , not all datasets in the world are large . A method that is simple , innovative and powerful for small to medium-sized datasets may still be interesting . I do n't think that a machine learning method must necessary be applicable to ImageNet to be scientifically interesting or useful in practice , or to warrant publication . Also , note that our methodology for evaluating our algorithm required around 1000 training runs per dataset . The requirements for expensive validation procedures is also intrinsic to hyperparameter optimization . Consider for example , in addition to Springenberg et al , the following papers : Jelena Luketina , Mathias Berglund , Klaus Greff , and Raiko Tapani . Scalable gradient-based tuning of continuous regularization hyperparameters . In ICML , 2016 . Dougal Maclaurin , David Duvenaud , and Ryan P. Adams . Gradient-based hyperparameter optimization through reversible learning . In ICML , 2015 . All of these hyperparameter optimization papers were published at top conferences , but did not apply their algorithm to large-scale datasets . All of them used many training runs to validate their methods . Thanks again , George"}}