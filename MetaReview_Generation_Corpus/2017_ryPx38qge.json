{"year": "2017", "forum": "ryPx38qge", "title": "A hybrid network: Scattering and Convnet", "decision": "Reject", "meta_review": "The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected.", "reviews": [{"review_id": "ryPx38qge-0", "review_text": "In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications. I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well. Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard. If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7. Summary: + An interesting approach is presented that might be useful for real-world limited data scenarios. + Limited data results look promising. - Adversarial examples are not investigated in the experimental section. - No realistic small-data problem is addressed. Minor: - The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days. - Some typos: tacke, developping, learni. [1] https://arxiv.org/abs/1610.00768v3 [2] https://arxiv.org/abs/1511.04599 ", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , I thank you for your positive and constructive review . I have added the results of the Wide Resnet in the Table 3 , as well as the VGG , in the limited sample situation setting . They interestingly show that a resnet is extremly robust to these situations . I should add , but I will not put it in the final paper since this is not the purpose of this work , that cascading a resnet on top of a scattering network does not reduce ( when the appropriate J parameter is selected ) the resulting accuracy on the whole dataset . I have added a SOTA result on STL10 , which leads to 77.4 % accuracy . This dataset is a challenging dataset with few labeled samples available . I agree this was a missing result , and I thank you for this suggestion . However , I believe CIFAR datasets are also interesting since one can observe the evolution of the accuracy with respect to the amount of available labeled data . Interestingly , a scattering network that does not lead to SOTA beats the resnet when 2000 samples are available . I thank you for your observation about the stability w.r.t.additive perturbations . Unfortunately , I did not use the softwares you mention since the implementations are unfortunately not done in Lua , however I believe this will not be a limitation . Indeed , in the Appendix B of this paper , I did theoritically quantify the instabilities . One observes that the scattering is unlikely to reduce the additive instabilities that are due to the learned and cascaded deep network ; recent works seem to indicate those instabilities are always present ( https : //arxiv.org/pdf/1610.08401.pdf ) . However a scattering transform can help to build invariance to other source of instabilities such as deformations . I thank you again very much for your helpful comments , questions , review and your time . Best regards , EO"}, {"review_id": "ryPx38qge-1", "review_text": "The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network. By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations. Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime. I have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained. In theory, using fixed features could save parameters and training time. As far as I am aware, this paper is the first to investigate this question. In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features. This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization. For the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size. For the full dataset, the hybrid network is clearly outperformed by fully learned models. If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion. The authors claim the hybrid network has the theoretical advantage of stability. However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability. Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks. In conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets. Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper. Minor comments: -section 3.1.2: \u201clearni\u201d ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , I thank you for your constructive review that was very helpful for the ( hopefully ) final submission of this work . Thank you again for your comments and several issues you highlight . I would like to answer to each of them . The paper does not claim that the hybrid network is strictly superior in term of generalization on the full dataset , and I do not believe this would be possible , with such a wide litterature of incremental improvements obtained via a huge process of hyper parameters fine tuning . ( Or at least until the mathematical fundations are fully understood ) . Instead , we fix a simple architecture that was not fine tuned , since this is simply a preliminary work . The author has indeed positively investigates the question of cascading a resnet , but this is not the purpose of this paper . However , the comparison with a resnet in the limited sample situation is very interesting . It shows that the resnet architecture is really robust to limited sample situation on CIFAR datasets , but it is beaten by the hybrid architecture when really few samples are available , e.g.2000 samples . Adapting the architecture of those deep networks to the limited sample situation ( e.g.reducing the number of parameters w.r.t.the number of samples ) will add another process of selecting hyper parameters that is desirable to avoid . In order to convince you that scattering networks are an enjoyable initialization , I have added a state-of-the-art result on STL10 that leads to 77.4 % accuracy . STL10 is a challenging dataset with large images , 10 classes and 500 samples per class . I believe the notion of stability is not specific to the scattering litterature , yet it has mathematical foundations . Concretely talking , let us take the example of a navigation system of a car which is controlled by a deep network : for the safety of its passengers , such system should never be fooled by small additive perturbations . I believe this is a central topic to build robust systems and that it is extremely connected to understanding deep networks . I agree that the additive stability of the whole network is unlikely to happen , and this is proven in the Appendix B of this paper . It shows the instabilities of the hybrid network are bounded by the instabilities of the learned network . The resulting instabilities come from the cascaded and learned deep network . Such instabilities could be avoided if they were correctly handled during the optimization process , yet more work in this direction has to be adressed , and I believe a scattering transform is the first step to this kind of work , because this property is explicitly implemented at no cost . However , as developed in the section 2.1.2 , the stability to other sources ( such as deformation ) of variability is explicitly obtained . Unfortunately , I do not know a software that permits to quantify these and this is not the purpose of this paper Finally , I do agree that faster computation time are one of the natural outcome of this paper . While being not explicitely adressed here , in our future work , this will be a central question not only for mobile device issues but as well for processing bigger dataset such as imagenet . I thank you again for your comments , review and time that were very helpful to improve this paper . Best regards , EO"}, {"review_id": "ryPx38qge-2", "review_text": "Thanks a lot for your detailed response and clarifications. The paper proposes to use a scattering transform as the lower layers of a deep network. This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior. The top layers of the network are trained to perform a given supervised task. This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform. Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines. I find the paper very interesting. The idea of cascading these representations seems very natural thing to try. To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches. While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point. The paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation. The of the hybrid approach become crucial in the low data regime. The author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive. This naturally suggests that the model is more robust to adversarial examples. It would be extremely interesting to present an empirical evaluation of this task. What's the practical impact? Can this hybrid network be fooled with adversarial? If this is the case, it would render the use of scattering initialization very attractive. ", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , I thank you for your positive review . I complete my global answer specifically to your review , for which I thank you again . Each deepnet seems to be fooled by small valued vectors ( https : //arxiv.org/pdf/1610.08401.pdf ) , and if no additional constraints are added during the optimization process , there is no reason that the hybrid network becomes more robust . In the Appendix B , I have added a note that shows that the amplitude of the instabilities of the hybrid network is likely to be of the order of magnitude of the instabilities of the cascaded network . Bounds are derived . However , a scattering network is stable to other source of transformation such as deformation , as developed in the paper , which means a scattering network is potentially a good initialization . Thank you very much for your remarks , comments that were helpful to resubmit this paper and thank you again for your time . Best regards , EO"}], "0": {"review_id": "ryPx38qge-0", "review_text": "In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications. I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well. Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard. If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7. Summary: + An interesting approach is presented that might be useful for real-world limited data scenarios. + Limited data results look promising. - Adversarial examples are not investigated in the experimental section. - No realistic small-data problem is addressed. Minor: - The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days. - Some typos: tacke, developping, learni. [1] https://arxiv.org/abs/1610.00768v3 [2] https://arxiv.org/abs/1511.04599 ", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , I thank you for your positive and constructive review . I have added the results of the Wide Resnet in the Table 3 , as well as the VGG , in the limited sample situation setting . They interestingly show that a resnet is extremly robust to these situations . I should add , but I will not put it in the final paper since this is not the purpose of this work , that cascading a resnet on top of a scattering network does not reduce ( when the appropriate J parameter is selected ) the resulting accuracy on the whole dataset . I have added a SOTA result on STL10 , which leads to 77.4 % accuracy . This dataset is a challenging dataset with few labeled samples available . I agree this was a missing result , and I thank you for this suggestion . However , I believe CIFAR datasets are also interesting since one can observe the evolution of the accuracy with respect to the amount of available labeled data . Interestingly , a scattering network that does not lead to SOTA beats the resnet when 2000 samples are available . I thank you for your observation about the stability w.r.t.additive perturbations . Unfortunately , I did not use the softwares you mention since the implementations are unfortunately not done in Lua , however I believe this will not be a limitation . Indeed , in the Appendix B of this paper , I did theoritically quantify the instabilities . One observes that the scattering is unlikely to reduce the additive instabilities that are due to the learned and cascaded deep network ; recent works seem to indicate those instabilities are always present ( https : //arxiv.org/pdf/1610.08401.pdf ) . However a scattering transform can help to build invariance to other source of instabilities such as deformations . I thank you again very much for your helpful comments , questions , review and your time . Best regards , EO"}, "1": {"review_id": "ryPx38qge-1", "review_text": "The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network. By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations. Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime. I have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained. In theory, using fixed features could save parameters and training time. As far as I am aware, this paper is the first to investigate this question. In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features. This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization. For the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size. For the full dataset, the hybrid network is clearly outperformed by fully learned models. If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion. The authors claim the hybrid network has the theoretical advantage of stability. However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability. Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks. In conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets. Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper. Minor comments: -section 3.1.2: \u201clearni\u201d ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , I thank you for your constructive review that was very helpful for the ( hopefully ) final submission of this work . Thank you again for your comments and several issues you highlight . I would like to answer to each of them . The paper does not claim that the hybrid network is strictly superior in term of generalization on the full dataset , and I do not believe this would be possible , with such a wide litterature of incremental improvements obtained via a huge process of hyper parameters fine tuning . ( Or at least until the mathematical fundations are fully understood ) . Instead , we fix a simple architecture that was not fine tuned , since this is simply a preliminary work . The author has indeed positively investigates the question of cascading a resnet , but this is not the purpose of this paper . However , the comparison with a resnet in the limited sample situation is very interesting . It shows that the resnet architecture is really robust to limited sample situation on CIFAR datasets , but it is beaten by the hybrid architecture when really few samples are available , e.g.2000 samples . Adapting the architecture of those deep networks to the limited sample situation ( e.g.reducing the number of parameters w.r.t.the number of samples ) will add another process of selecting hyper parameters that is desirable to avoid . In order to convince you that scattering networks are an enjoyable initialization , I have added a state-of-the-art result on STL10 that leads to 77.4 % accuracy . STL10 is a challenging dataset with large images , 10 classes and 500 samples per class . I believe the notion of stability is not specific to the scattering litterature , yet it has mathematical foundations . Concretely talking , let us take the example of a navigation system of a car which is controlled by a deep network : for the safety of its passengers , such system should never be fooled by small additive perturbations . I believe this is a central topic to build robust systems and that it is extremely connected to understanding deep networks . I agree that the additive stability of the whole network is unlikely to happen , and this is proven in the Appendix B of this paper . It shows the instabilities of the hybrid network are bounded by the instabilities of the learned network . The resulting instabilities come from the cascaded and learned deep network . Such instabilities could be avoided if they were correctly handled during the optimization process , yet more work in this direction has to be adressed , and I believe a scattering transform is the first step to this kind of work , because this property is explicitly implemented at no cost . However , as developed in the section 2.1.2 , the stability to other sources ( such as deformation ) of variability is explicitly obtained . Unfortunately , I do not know a software that permits to quantify these and this is not the purpose of this paper Finally , I do agree that faster computation time are one of the natural outcome of this paper . While being not explicitely adressed here , in our future work , this will be a central question not only for mobile device issues but as well for processing bigger dataset such as imagenet . I thank you again for your comments , review and time that were very helpful to improve this paper . Best regards , EO"}, "2": {"review_id": "ryPx38qge-2", "review_text": "Thanks a lot for your detailed response and clarifications. The paper proposes to use a scattering transform as the lower layers of a deep network. This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior. The top layers of the network are trained to perform a given supervised task. This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform. Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines. I find the paper very interesting. The idea of cascading these representations seems very natural thing to try. To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches. While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point. The paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation. The of the hybrid approach become crucial in the low data regime. The author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive. This naturally suggests that the model is more robust to adversarial examples. It would be extremely interesting to present an empirical evaluation of this task. What's the practical impact? Can this hybrid network be fooled with adversarial? If this is the case, it would render the use of scattering initialization very attractive. ", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , I thank you for your positive review . I complete my global answer specifically to your review , for which I thank you again . Each deepnet seems to be fooled by small valued vectors ( https : //arxiv.org/pdf/1610.08401.pdf ) , and if no additional constraints are added during the optimization process , there is no reason that the hybrid network becomes more robust . In the Appendix B , I have added a note that shows that the amplitude of the instabilities of the hybrid network is likely to be of the order of magnitude of the instabilities of the cascaded network . Bounds are derived . However , a scattering network is stable to other source of transformation such as deformation , as developed in the paper , which means a scattering network is potentially a good initialization . Thank you very much for your remarks , comments that were helpful to resubmit this paper and thank you again for your time . Best regards , EO"}}