{"year": "2020", "forum": "SylOlp4FvH", "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control", "decision": "Accept (Poster)", "meta_review": "This paper proposes an extension of MPO for on-policy reinforcement learning. The proposed method achieved promising results in a relatively hyper-parameter insensitive manner.\n\nOne concern of the reviewers is the lack of comparison with previous works, such as original MPO, which has been partially addressed by the authors in rebuttal. In addition, Blind Review #3 has some concerns with the fairness of the experimental comparison, though other reviews accept the comparison using standardized benchmark.\n\nOverall, the paper proposes a promising extension of MPO; thus, I recommend it for acceptance.\n", "reviews": [{"review_id": "SylOlp4FvH-0", "review_text": "Summary: This paper presents the V-MPO algorithm for on-policy reinforcement learning that can handle both continuous/discrete control, single/multi-task learning and use both low dimensional states and pixels. V-MPO adapts MPO, a recent off-policy deep reinforcement learning algorithm, to the on-policy setting with the following changes: (1) In policy evaluation step, instead of using state-action value Q estimated from off-policy replay data, the authors use on-policy data to estimate the state value V; (2) In E-step of policy learning, construct the target distribution q(a|s) with estimated advantages; and (3) in M-step, rewrite the KL divergence constraints with Lagrangian relaxation and alternate between optimising policy and the multiplier alpha. The experiments are two-fold: (1) For discrete control, a multi-task problem setting is considered (DMLab-30 and Atari-57). V-MPO shows improved performance compared to IMPALA and R2D2, in terms of asymptotic scores. The result also suggest good stability for different hyper-parameters. (2) In continuous problems, the experiments follow the single-task problem setting, including Humanoids tasks and OpenAI Gym tasks. V-MPO achieves higher asymptotic returns compared with standard Deep RL algorithms (MPO, SAC, PPO). However, it has a lower sample-efficiency, especially compared to off-policy algorithms (MPO and SAC). Pros: + This paper demonstrates successful adaption of MPO to the on-policy problem setting and achieves better asymptotic score comparing to baseline methods. ++ Results are achieved in a relatively hyper-parameter insensitive manner. And some commonly used tricks like entropy regularisation are not necessary. + Along with the previous results of MPO, this demonstrates an alternative framework of policy gradient-based RL: first construct a nonparametric target behavioural distribution and then move the parametric policy towards this distribution. Cons and Questions: 1. The comparison between V-MPO with other single-task deep RL methods is non-standard because: (1) V-MPO is trained with far more samples, and (2) only asymptotic score is reported for baselines. It would be more informative to add the learning curve of baselines methods to show sample-efficiency and convergence properties. One may wonder if the baselines would provide better performance than reported if provided with a comparable number of steps in an appropriate way (E.g., seed search, hyper parameter sweeps). 2. The proposed adaption of MPO from off-policy to on-policy applies to both discrete and continuous problem settings. However, the designed experiments include only the discrete multi-task setting and continuous single-task setting. How does V-MPO compare to other methods in single-task discrete problems? This may enable comparison to a wider set of prior work. 3. Only top 50% advantages samples are used for generating target distribution in E-step, and all samples are used for policy updates in M-step. Does this mismatch between E-step and M-step sample pool make a difference in optimisation? 4. Some small tricks we used like PopArt, Top-K advantage. It\u2019s not clear if these are things that (were/could-also-be used) in competitor methods, and how important these are to achieve the good results shown here. At least an ablation study on their impact for V-MPO would be nice. Minor: - The mixture use of subscripts \u201cold\u201d and \u201ctarget\u201d is rather confusing, for example, in equation (5) and (15).", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their thoughtful questions and comments , and we appreciate their thorough understanding of the paper . \u201c The comparison between V-MPO with other single-task deep RL methods is non-standard because : ( 1 ) V-MPO is trained with far more samples , and ( 2 ) only asymptotic score is reported for baselines . It would be more informative to add the learning curve of baselines methods to show sample-efficiency and convergence properties . One may wonder if the baselines would provide better performance than reported if provided with a comparable number of steps in an appropriate way ( E.g. , seed search , hyper parameter sweeps ) . \u201d We have wondered this ourselves ; however , it is difficult to do justice to other algorithms ( see , for example , the discussion in https : //openreview.net/forum ? id=H1gdF34FvS ) , including all of the tricks used by other works , so we have made the baseline numbers explicit and encourage others to show that these numbers could be improved if optimized for the high-data regime . On the other hand , we acknowledge that this version of the algorithm is likely to be outperformed by other methods in the extremely low data regime . Even in DMLab-30 and Atari-57 , it is evident that with the current set of hyperparameters V-MPO underperforms IMPALA early on but eventually reaches substantially higher scores later . Our goal was to show that these long-studied tasks have solutions with far higher returns than has previously been reported , and to demonstrate the existence of an algorithm , namely V-MPO , that can reliably reach them . We think this is interesting to know . It is unclear to us whether other algorithms , if run for longer , could find these solutions - we hope others will try ! \u201c However , the designed experiments include only the discrete multi-task setting and continuous single-task setting . How does V-MPO compare to other methods in single-task discrete problems ? This may enable comparison to a wider set of prior work. \u201d Precisely for this reason , Fig.3 ( in the updated numbering ) shows V-MPO trained on example * single * tasks in DMLab , and Fig.4 shows V-MPO applied to example * single * tasks in Atari . In hindsight this is not so clear in the caption and we have changed this in the updated paper . \u201c Only top 50 % advantages samples are used for generating target distribution in E-step , and all samples are used for policy updates in M-step . Does this mismatch between E-step and M-step sample pool make a difference in optimisation ? \u201d To clarify , only the samples corresponding to the top 50 % of advantages are also used for the weighted maximum likelihood fitting in the M-step , as these are the weights generated by the E-step and the bottom half of samples get zero weight . The KL prior in the M-step does use all samples , but this is not a problem as this parametric prior is separate from the nonparametric target distribution proposed in the E-step ."}, {"review_id": "SylOlp4FvH-1", "review_text": "The paper proposes an online variant of MPO, V-MPO. Compared to MPO, the main difference seems to be in the E-step. Instead of optimizing the non-parametric distribution towards a parameterized Q-function, V-MPO learns the V-function and updates the non-parametric distribution towards the advantages, which can be estimated on the samples of the last roll-outs based on the empirical returns and the learned V-function. Of course, updating towards (exponentiated and normalized) Q- or A-function does not make a difference. There are also some minor changes (which might still be crucial) such as an entropy constraint in the M-step and using only the top-k advantages during the E-Step (an option that was discussed in Abdolmaleki, et al. 2018a). V-MPO is evaluated on DLM, ALE and two humanoid tasks from the DeepMind Control Suite. In most of these tasks V-MPO achieves returns that are to the best of my knowledge higher than any previously reported ones. However, the experiments also use a very large number of system interactions (in the order of billions). Contribution / Significance: I think that there will be relatively high interest in the paper due to the reported performances. The technical contribution seems a bit incremental compared to MPO. Also, by learning a value function V-MPO gets closer to REPS. The submission lists the use of top-k samples and the M-step KL bound as the main differences to REPS. However, the former is not evaluated in the submission and the latter, albeit crucial, seems to be a relatively small modification. I do think that there are more differences to REPS, most important probably in the way of learning the value function and the corresponding differences in the derivations. However, I think that the differences abd similarities to MPO and REPS need to be discussed more thoroughly. Soundness: The derivation of V-MPO is relatively sound. The optimization of the KL constraints seems very approximate, although it seems to work well in practice. Clarity: I do not like the way the algorithm is presented. The submission specifies the complete loss function already at the beginning of the \"Method\"-Section and derives/motivates the individual terms in hindsight. The spaghetti-code like structure unnecessarily forces the reader to jump between pages or keeps the reader in the dark. I also do not like the \"stop-gradient\" notation which in my opinion puts the focus on low-level implementation details at the cost of not properly explaining what the optimization actually does. I think that paper is well-written in general, but the structure needs to be improved. Experiments: The evaluation clearly focuses on achieving the best performance, and does a good job in that regard. However, a good evaluation should also help in understanding the mechanics of V-MPO. How does k (in top-k) affect the performance? How well are the constraints met during optimization? How does V-MPO compare to related on-policy methods (e.g. TRPO/PPO) on slightly more computational constrainted settings (eg rllab/mujoco with < 1e7 steps)? Questions: - Did you experiment with controlled entropy reduction akin to MORE, instead of using fixed epsilon eta? - Can you give a rough estimate of the computational time required to perform these experiments on a standard desktop pc? It really is difficult for me to even roughly estimate it. Assessment: Currently, I am leaning to accept because I think that V-MPO is overall a nice work. However I do think that submission needs to be revised. I mainly think that the structure needs to be improved and that V-MPO needs to better related to closely related work. I listed some additional experiments that would significantly improve the submission in my opinion.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their thoughtful questions and comments , and we appreciate their thorough understanding of the paper . We kept the discussion of similarities to MPO and REPS to a minimum due to space constraints in the initial submission , but in the revised paper the Related Works section has been expanded . It is additionally worth noting , though the reviewer is probably already aware , that in REPS the value function is a linear function of a learned feature representation whose features are learned by matching the feature distributions under the policy \u2019 s stationary state distribution . In contrast , in V-MPO the nonlinear neural network value function is learned directly from n-step returns . This is now noted in the Related Works section . \u201c I do not like the way the algorithm is presented . The submission specifies the complete loss function already at the beginning of the `` Method '' -Section and derives/motivates the individual terms in hindsight . The spaghetti-code like structure unnecessarily forces the reader to jump between pages or keeps the reader in the dark . I also do not like the `` stop-gradient '' notation which in my opinion puts the focus on low-level implementation details at the cost of not properly explaining what the optimization actually does . I think that paper is well-written in general , but the structure needs to be improved. \u201d Presenting the complete loss at the beginning was suggested to us by an earlier reader of the paper as a way of making the subsequent derivation easier to follow for those who prefer to see the loss upfront . However , not everyone appears to agree with this approach , and we have now moved the final loss to after the derivation . The updated paper also includes pseudocode for the algorithm , which is simpler than is perhaps suggested by the derivation ! The stop-gradient is a way to implement coordinate descent while optimizing a single loss . We have attempted to clarify this in the updated paper as it is an important practical detail . \u201c The evaluation clearly focuses on achieving the best performance , and does a good job in that regard . However , a good evaluation should also help in understanding the mechanics of V-MPO . How does k ( in top-k ) affect the performance ? How well are the constraints met during optimization ? How does V-MPO compare to related on-policy methods ( e.g.TRPO/PPO ) on slightly more computational constrained settings ( eg rllab/mujoco with < 1e7 steps ) ? \u201d In Figure 7 of the updated Appendix we now show an example of the different KL constraints during training . We note that not only are the constraints well satisfied , but they are saturated , which is desirable . We found the top-k selection to be more important in some tasks than others . In Figure 9 of the updated Appendix we show an example of multi-task DMLab-30 trained with all advantages included in the E-step , showing no appreciable difference in the final performance at 10B environment steps . In some continuous control tasks , however , agents learned less reliably without it . We therefore chose top-k so that this single setting could be applied to all experiments . We would also like to take this opportunity to note that top-k is not the only possible choice . For instance , in Fig.8 of the updated Appendix we have included an example of agents trained by * uniformly * weighting the top-k samples . Many such choices are possible , and different choices may be suitable for different applications . In this light , top-k is just one choice that worked well across the experiments in this paper , but this may not be true in other settings . We acknowledge that in the data regime of < 1e7 steps mentioned by the reviewer , the current implementation and hyperparameters would not perform as well as TRPO/PPO . We note that even in multi-task DMLab-30 and Atari-57 , it is evident that with the current set of hyperparameters V-MPO underperforms IMPALA early in training but eventually reaches substantially higher scores later ."}, {"review_id": "SylOlp4FvH-2", "review_text": "This paper proposes a new approximate policy iteration algorithm that is based on the previous method, called MPO, which formulates policy optimization (PO) as a probabilistic inference problem. The adaptation makes MPO become an on-policy method. One key modification made to MPO is to use advantage functions instead of Q-value function. Overall, the paper follows an interesting topic in policy optimization as inference. It follows MPO to formulate PO as an inference problem using a similar principle. It proposes some extensions to MPO. The experiments show a lot of promising results. I have some concerns as follows. - As V-MPO is developed based on MPO, however, the background on MPO is missing. This makes the reader hard to judge the novelty and difference of V-MPO against MPO. The discussion on the difference is very little. - V-MPO extends MPO to on-policy setting, so the policy evaluation is key for this modification. Besides this modification, the E-step and M-step's derivations and objectives look quite similar to those of MPO. Can the authors comment on this? - In addition, V-MPO is said to work for both discrete and continuous domains? It would be clearer if the authors discuss which parts in their algorithm enable this ability? - It would also be great if the algorithmic description is included. The overall algorithm contains multiple optimization steps and hyperparameters, e.g. number of updates, how Lagrangian multiplier adapted etc.. - Continuous tasks: The comparisons with low sample-efficient approaches look unfair. Are there ablations that show performance comparisons of all when set with an equal level of samples? - Can the author elaborate on the comment \"These must be consistent between the maximum likelihood weights in Eq. 3 and the temperature loss in Eq. 4\". ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their thoughtful questions and comments , and we appreciate their thorough understanding of the paper . \u201c As V-MPO is developed based on MPO , however , the background on MPO is missing . This makes the reader hard to judge the novelty and difference of V-MPO against MPO . The discussion on the difference is very little. \u201d We kept this discussion to a minimum due to the space constraints in the initial submission , but in the revised paper the Related Works section has been expanded . \u201c V-MPO extends MPO to on-policy setting , so the policy evaluation is key for this modification . Besides this modification , the E-step and M-step 's derivations and objectives look quite similar to those of MPO . Can the authors comment on this ? \u201d This is true - to an extent . This is precisely why we chose the name V-MPO , to make it clear that conceptually , V-MPO arises from the same essential formalism as MPO . Demonstrating that this results in a highly competitive algorithm in practice is indeed the main contribution of this work . \u201c In addition , V-MPO is said to work for both discrete and continuous domains ? It would be clearer if the authors discuss which parts in their algorithm enable this ability ? \u201d There exist other algorithms , such as PPO , that work well for both discrete and continuous domains , so perhaps the question is , rather , why some do not . In general , any policy gradient algorithm using the likelihood-ratio trick to estimate gradients can , in theory , be applied to both continuous and discrete settings . However , two common problems are : 1 ) finding the right regularization of the policy that avoids collapse in high-dimensional continuous domains , and 2 ) dealing with the variance of the policy gradient estimate . V-MPO does not use the likelihood-ratio trick but separates the RL step from policy-fitting . The fitting step then does not require differentiating through observed returns , has low-variance , and is invariant to the scale of the returns ( which makes finding the right regularization , e.g. , the KL constraint here , somewhat easier ) . \u201c It would also be great if the algorithmic description is included . The overall algorithm contains multiple optimization steps and hyperparameters , e.g.number of updates , how Lagrangian multiplier adapted etc. \u201d The updated paper now includes pseudocode of the algorithm , which is simpler than is perhaps suggested by the derivation ! \u201c Continuous tasks : The comparisons with low sample-efficient approaches look unfair . Are there ablations that show performance comparisons of all when set with an equal level of samples ? \u201d Our implementation and hyperparameters were optimized to achieve the highest scores possible in the high-data regime ; our goal was to show that these long-studied tasks have solutions with far higher returns than has previously been reported , and to demonstrate the existence of an algorithm , namely V-MPO , that can reliably reach them . We think this is interesting in and of itself . That said , we acknowledge that this on-policy version is optimized to reach high performance but does not outperform the other algorithms at the specified number of steps . Even in DMLab-30 and Atari-57 , it is evident that with the current set of hyperparameters V-MPO underperforms IMPALA early on but eventually reaches substantially higher scores later . \u201c Can the author elaborate on the comment `` These must be consistent between the maximum likelihood weights in Eq.3 and the temperature loss in Eq.4 '' . \u201d In the derivation of V-MPO the same policy improvement probability p ( I|s , a ) enters in the weights and the temperature loss . Thus if the chosen p ( I|s , a ) includes top-k advantages only then it will appear in both the weights and the temperature loss . We have clarified this further in the updated paper ."}], "0": {"review_id": "SylOlp4FvH-0", "review_text": "Summary: This paper presents the V-MPO algorithm for on-policy reinforcement learning that can handle both continuous/discrete control, single/multi-task learning and use both low dimensional states and pixels. V-MPO adapts MPO, a recent off-policy deep reinforcement learning algorithm, to the on-policy setting with the following changes: (1) In policy evaluation step, instead of using state-action value Q estimated from off-policy replay data, the authors use on-policy data to estimate the state value V; (2) In E-step of policy learning, construct the target distribution q(a|s) with estimated advantages; and (3) in M-step, rewrite the KL divergence constraints with Lagrangian relaxation and alternate between optimising policy and the multiplier alpha. The experiments are two-fold: (1) For discrete control, a multi-task problem setting is considered (DMLab-30 and Atari-57). V-MPO shows improved performance compared to IMPALA and R2D2, in terms of asymptotic scores. The result also suggest good stability for different hyper-parameters. (2) In continuous problems, the experiments follow the single-task problem setting, including Humanoids tasks and OpenAI Gym tasks. V-MPO achieves higher asymptotic returns compared with standard Deep RL algorithms (MPO, SAC, PPO). However, it has a lower sample-efficiency, especially compared to off-policy algorithms (MPO and SAC). Pros: + This paper demonstrates successful adaption of MPO to the on-policy problem setting and achieves better asymptotic score comparing to baseline methods. ++ Results are achieved in a relatively hyper-parameter insensitive manner. And some commonly used tricks like entropy regularisation are not necessary. + Along with the previous results of MPO, this demonstrates an alternative framework of policy gradient-based RL: first construct a nonparametric target behavioural distribution and then move the parametric policy towards this distribution. Cons and Questions: 1. The comparison between V-MPO with other single-task deep RL methods is non-standard because: (1) V-MPO is trained with far more samples, and (2) only asymptotic score is reported for baselines. It would be more informative to add the learning curve of baselines methods to show sample-efficiency and convergence properties. One may wonder if the baselines would provide better performance than reported if provided with a comparable number of steps in an appropriate way (E.g., seed search, hyper parameter sweeps). 2. The proposed adaption of MPO from off-policy to on-policy applies to both discrete and continuous problem settings. However, the designed experiments include only the discrete multi-task setting and continuous single-task setting. How does V-MPO compare to other methods in single-task discrete problems? This may enable comparison to a wider set of prior work. 3. Only top 50% advantages samples are used for generating target distribution in E-step, and all samples are used for policy updates in M-step. Does this mismatch between E-step and M-step sample pool make a difference in optimisation? 4. Some small tricks we used like PopArt, Top-K advantage. It\u2019s not clear if these are things that (were/could-also-be used) in competitor methods, and how important these are to achieve the good results shown here. At least an ablation study on their impact for V-MPO would be nice. Minor: - The mixture use of subscripts \u201cold\u201d and \u201ctarget\u201d is rather confusing, for example, in equation (5) and (15).", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their thoughtful questions and comments , and we appreciate their thorough understanding of the paper . \u201c The comparison between V-MPO with other single-task deep RL methods is non-standard because : ( 1 ) V-MPO is trained with far more samples , and ( 2 ) only asymptotic score is reported for baselines . It would be more informative to add the learning curve of baselines methods to show sample-efficiency and convergence properties . One may wonder if the baselines would provide better performance than reported if provided with a comparable number of steps in an appropriate way ( E.g. , seed search , hyper parameter sweeps ) . \u201d We have wondered this ourselves ; however , it is difficult to do justice to other algorithms ( see , for example , the discussion in https : //openreview.net/forum ? id=H1gdF34FvS ) , including all of the tricks used by other works , so we have made the baseline numbers explicit and encourage others to show that these numbers could be improved if optimized for the high-data regime . On the other hand , we acknowledge that this version of the algorithm is likely to be outperformed by other methods in the extremely low data regime . Even in DMLab-30 and Atari-57 , it is evident that with the current set of hyperparameters V-MPO underperforms IMPALA early on but eventually reaches substantially higher scores later . Our goal was to show that these long-studied tasks have solutions with far higher returns than has previously been reported , and to demonstrate the existence of an algorithm , namely V-MPO , that can reliably reach them . We think this is interesting to know . It is unclear to us whether other algorithms , if run for longer , could find these solutions - we hope others will try ! \u201c However , the designed experiments include only the discrete multi-task setting and continuous single-task setting . How does V-MPO compare to other methods in single-task discrete problems ? This may enable comparison to a wider set of prior work. \u201d Precisely for this reason , Fig.3 ( in the updated numbering ) shows V-MPO trained on example * single * tasks in DMLab , and Fig.4 shows V-MPO applied to example * single * tasks in Atari . In hindsight this is not so clear in the caption and we have changed this in the updated paper . \u201c Only top 50 % advantages samples are used for generating target distribution in E-step , and all samples are used for policy updates in M-step . Does this mismatch between E-step and M-step sample pool make a difference in optimisation ? \u201d To clarify , only the samples corresponding to the top 50 % of advantages are also used for the weighted maximum likelihood fitting in the M-step , as these are the weights generated by the E-step and the bottom half of samples get zero weight . The KL prior in the M-step does use all samples , but this is not a problem as this parametric prior is separate from the nonparametric target distribution proposed in the E-step ."}, "1": {"review_id": "SylOlp4FvH-1", "review_text": "The paper proposes an online variant of MPO, V-MPO. Compared to MPO, the main difference seems to be in the E-step. Instead of optimizing the non-parametric distribution towards a parameterized Q-function, V-MPO learns the V-function and updates the non-parametric distribution towards the advantages, which can be estimated on the samples of the last roll-outs based on the empirical returns and the learned V-function. Of course, updating towards (exponentiated and normalized) Q- or A-function does not make a difference. There are also some minor changes (which might still be crucial) such as an entropy constraint in the M-step and using only the top-k advantages during the E-Step (an option that was discussed in Abdolmaleki, et al. 2018a). V-MPO is evaluated on DLM, ALE and two humanoid tasks from the DeepMind Control Suite. In most of these tasks V-MPO achieves returns that are to the best of my knowledge higher than any previously reported ones. However, the experiments also use a very large number of system interactions (in the order of billions). Contribution / Significance: I think that there will be relatively high interest in the paper due to the reported performances. The technical contribution seems a bit incremental compared to MPO. Also, by learning a value function V-MPO gets closer to REPS. The submission lists the use of top-k samples and the M-step KL bound as the main differences to REPS. However, the former is not evaluated in the submission and the latter, albeit crucial, seems to be a relatively small modification. I do think that there are more differences to REPS, most important probably in the way of learning the value function and the corresponding differences in the derivations. However, I think that the differences abd similarities to MPO and REPS need to be discussed more thoroughly. Soundness: The derivation of V-MPO is relatively sound. The optimization of the KL constraints seems very approximate, although it seems to work well in practice. Clarity: I do not like the way the algorithm is presented. The submission specifies the complete loss function already at the beginning of the \"Method\"-Section and derives/motivates the individual terms in hindsight. The spaghetti-code like structure unnecessarily forces the reader to jump between pages or keeps the reader in the dark. I also do not like the \"stop-gradient\" notation which in my opinion puts the focus on low-level implementation details at the cost of not properly explaining what the optimization actually does. I think that paper is well-written in general, but the structure needs to be improved. Experiments: The evaluation clearly focuses on achieving the best performance, and does a good job in that regard. However, a good evaluation should also help in understanding the mechanics of V-MPO. How does k (in top-k) affect the performance? How well are the constraints met during optimization? How does V-MPO compare to related on-policy methods (e.g. TRPO/PPO) on slightly more computational constrainted settings (eg rllab/mujoco with < 1e7 steps)? Questions: - Did you experiment with controlled entropy reduction akin to MORE, instead of using fixed epsilon eta? - Can you give a rough estimate of the computational time required to perform these experiments on a standard desktop pc? It really is difficult for me to even roughly estimate it. Assessment: Currently, I am leaning to accept because I think that V-MPO is overall a nice work. However I do think that submission needs to be revised. I mainly think that the structure needs to be improved and that V-MPO needs to better related to closely related work. I listed some additional experiments that would significantly improve the submission in my opinion.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their thoughtful questions and comments , and we appreciate their thorough understanding of the paper . We kept the discussion of similarities to MPO and REPS to a minimum due to space constraints in the initial submission , but in the revised paper the Related Works section has been expanded . It is additionally worth noting , though the reviewer is probably already aware , that in REPS the value function is a linear function of a learned feature representation whose features are learned by matching the feature distributions under the policy \u2019 s stationary state distribution . In contrast , in V-MPO the nonlinear neural network value function is learned directly from n-step returns . This is now noted in the Related Works section . \u201c I do not like the way the algorithm is presented . The submission specifies the complete loss function already at the beginning of the `` Method '' -Section and derives/motivates the individual terms in hindsight . The spaghetti-code like structure unnecessarily forces the reader to jump between pages or keeps the reader in the dark . I also do not like the `` stop-gradient '' notation which in my opinion puts the focus on low-level implementation details at the cost of not properly explaining what the optimization actually does . I think that paper is well-written in general , but the structure needs to be improved. \u201d Presenting the complete loss at the beginning was suggested to us by an earlier reader of the paper as a way of making the subsequent derivation easier to follow for those who prefer to see the loss upfront . However , not everyone appears to agree with this approach , and we have now moved the final loss to after the derivation . The updated paper also includes pseudocode for the algorithm , which is simpler than is perhaps suggested by the derivation ! The stop-gradient is a way to implement coordinate descent while optimizing a single loss . We have attempted to clarify this in the updated paper as it is an important practical detail . \u201c The evaluation clearly focuses on achieving the best performance , and does a good job in that regard . However , a good evaluation should also help in understanding the mechanics of V-MPO . How does k ( in top-k ) affect the performance ? How well are the constraints met during optimization ? How does V-MPO compare to related on-policy methods ( e.g.TRPO/PPO ) on slightly more computational constrained settings ( eg rllab/mujoco with < 1e7 steps ) ? \u201d In Figure 7 of the updated Appendix we now show an example of the different KL constraints during training . We note that not only are the constraints well satisfied , but they are saturated , which is desirable . We found the top-k selection to be more important in some tasks than others . In Figure 9 of the updated Appendix we show an example of multi-task DMLab-30 trained with all advantages included in the E-step , showing no appreciable difference in the final performance at 10B environment steps . In some continuous control tasks , however , agents learned less reliably without it . We therefore chose top-k so that this single setting could be applied to all experiments . We would also like to take this opportunity to note that top-k is not the only possible choice . For instance , in Fig.8 of the updated Appendix we have included an example of agents trained by * uniformly * weighting the top-k samples . Many such choices are possible , and different choices may be suitable for different applications . In this light , top-k is just one choice that worked well across the experiments in this paper , but this may not be true in other settings . We acknowledge that in the data regime of < 1e7 steps mentioned by the reviewer , the current implementation and hyperparameters would not perform as well as TRPO/PPO . We note that even in multi-task DMLab-30 and Atari-57 , it is evident that with the current set of hyperparameters V-MPO underperforms IMPALA early in training but eventually reaches substantially higher scores later ."}, "2": {"review_id": "SylOlp4FvH-2", "review_text": "This paper proposes a new approximate policy iteration algorithm that is based on the previous method, called MPO, which formulates policy optimization (PO) as a probabilistic inference problem. The adaptation makes MPO become an on-policy method. One key modification made to MPO is to use advantage functions instead of Q-value function. Overall, the paper follows an interesting topic in policy optimization as inference. It follows MPO to formulate PO as an inference problem using a similar principle. It proposes some extensions to MPO. The experiments show a lot of promising results. I have some concerns as follows. - As V-MPO is developed based on MPO, however, the background on MPO is missing. This makes the reader hard to judge the novelty and difference of V-MPO against MPO. The discussion on the difference is very little. - V-MPO extends MPO to on-policy setting, so the policy evaluation is key for this modification. Besides this modification, the E-step and M-step's derivations and objectives look quite similar to those of MPO. Can the authors comment on this? - In addition, V-MPO is said to work for both discrete and continuous domains? It would be clearer if the authors discuss which parts in their algorithm enable this ability? - It would also be great if the algorithmic description is included. The overall algorithm contains multiple optimization steps and hyperparameters, e.g. number of updates, how Lagrangian multiplier adapted etc.. - Continuous tasks: The comparisons with low sample-efficient approaches look unfair. Are there ablations that show performance comparisons of all when set with an equal level of samples? - Can the author elaborate on the comment \"These must be consistent between the maximum likelihood weights in Eq. 3 and the temperature loss in Eq. 4\". ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their thoughtful questions and comments , and we appreciate their thorough understanding of the paper . \u201c As V-MPO is developed based on MPO , however , the background on MPO is missing . This makes the reader hard to judge the novelty and difference of V-MPO against MPO . The discussion on the difference is very little. \u201d We kept this discussion to a minimum due to the space constraints in the initial submission , but in the revised paper the Related Works section has been expanded . \u201c V-MPO extends MPO to on-policy setting , so the policy evaluation is key for this modification . Besides this modification , the E-step and M-step 's derivations and objectives look quite similar to those of MPO . Can the authors comment on this ? \u201d This is true - to an extent . This is precisely why we chose the name V-MPO , to make it clear that conceptually , V-MPO arises from the same essential formalism as MPO . Demonstrating that this results in a highly competitive algorithm in practice is indeed the main contribution of this work . \u201c In addition , V-MPO is said to work for both discrete and continuous domains ? It would be clearer if the authors discuss which parts in their algorithm enable this ability ? \u201d There exist other algorithms , such as PPO , that work well for both discrete and continuous domains , so perhaps the question is , rather , why some do not . In general , any policy gradient algorithm using the likelihood-ratio trick to estimate gradients can , in theory , be applied to both continuous and discrete settings . However , two common problems are : 1 ) finding the right regularization of the policy that avoids collapse in high-dimensional continuous domains , and 2 ) dealing with the variance of the policy gradient estimate . V-MPO does not use the likelihood-ratio trick but separates the RL step from policy-fitting . The fitting step then does not require differentiating through observed returns , has low-variance , and is invariant to the scale of the returns ( which makes finding the right regularization , e.g. , the KL constraint here , somewhat easier ) . \u201c It would also be great if the algorithmic description is included . The overall algorithm contains multiple optimization steps and hyperparameters , e.g.number of updates , how Lagrangian multiplier adapted etc. \u201d The updated paper now includes pseudocode of the algorithm , which is simpler than is perhaps suggested by the derivation ! \u201c Continuous tasks : The comparisons with low sample-efficient approaches look unfair . Are there ablations that show performance comparisons of all when set with an equal level of samples ? \u201d Our implementation and hyperparameters were optimized to achieve the highest scores possible in the high-data regime ; our goal was to show that these long-studied tasks have solutions with far higher returns than has previously been reported , and to demonstrate the existence of an algorithm , namely V-MPO , that can reliably reach them . We think this is interesting in and of itself . That said , we acknowledge that this on-policy version is optimized to reach high performance but does not outperform the other algorithms at the specified number of steps . Even in DMLab-30 and Atari-57 , it is evident that with the current set of hyperparameters V-MPO underperforms IMPALA early on but eventually reaches substantially higher scores later . \u201c Can the author elaborate on the comment `` These must be consistent between the maximum likelihood weights in Eq.3 and the temperature loss in Eq.4 '' . \u201d In the derivation of V-MPO the same policy improvement probability p ( I|s , a ) enters in the weights and the temperature loss . Thus if the chosen p ( I|s , a ) includes top-k advantages only then it will appear in both the weights and the temperature loss . We have clarified this further in the updated paper ."}}