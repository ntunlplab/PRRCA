{"year": "2019", "forum": "HyxBpoR5tm", "title": "Adversarially Robust Training through Structured Gradient Regularization", "decision": "Reject", "meta_review": "Reviewers are in a consensus and recommended to reject after engaging with the authors. Further, many additional questions raised in the discussion should be addressed in the submission to improve clarity. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.\n", "reviews": [{"review_id": "HyxBpoR5tm-0", "review_text": "The authors propose a new defense against adversarial examples that relies on a data-dependent regularization (instead of adversarial training). They then benchmark the performance of this new defense against popular white-box and transfer attacks, as well as propose a new long range correlated adversarial attack. Comments: I find the premise of this paper interesting - developing regularization strategies to help with generalization to adversarial perturbations. For instance, it is well known that state-of-the-art defenses such as PGD have generalization gaps as large as 50% between robust train and test accuracies. It has also been previously hypothesized that this could be due to a data scarcity problem [Schmidt et al., 2018]. The authors here propose to tackle this problem using a new data-dependent regularization technique. My primary issue with this paper is that the authors do not clearly illustrate what the advantage of their method over standard methods is - The problem this paper aims to solve is overfitting to a specific attack/virtual adversarial examples presented during adversarial training by using regularization instead. However, the authors do not actually illustrate that their technique reduces overfitting. For instance, the authors do not contrast the robust train-test accuracies using their method to other standard methods. Thus it is not clear that this paper met the objectives laid out in the introduction. - The claim in this paper is that SGR helps against attacks with long range dependencies. However, in their experiments (e.g., in Figure 3), the authors do not evaluate other standard defenses. It is thus unclear whether other standard methods are already robust to such attacks. In fact, based on the results of Table 1, it doesn\u2019t seem like attacks from SGR are able to reduce the robustness of PGD/FGSM trained models. Because of these two points, along with the lower robustness to various attacks (in Table 1) as compared to approaches such as PGD, it is not really clear to me what the real merit of this new approach is. Ultimately, having a defense which is more robust to a particular attack is not very meaningful if there exists an alternative attack that reduces the robustness of the defense. I am also surprised that the authors chose to use this regularization as an alternative to adversarial training instead of complementary to it. I would be interested to see if such regularization could actually help to bridge the generalization gap observed while using adversarial training. The paper is at times is poorly written and confusing. For instance, the description of CovFun is hard to parse. The authors should make this explanation more clear. The authors also do not state what their attack model is - Linf vs L2 perturbations. They also choose to evaluate attacks differently, using an average accuracy over different epsilons rather than reporting individual accuracies. This does make the results harder to compare to other work. The authors should include a full table of individual accuracies (at least in the appendix) to make the numbers easier to parse and compare. In the derivation in Section 3.1, the authors use the assumption that the robust classifier is almost equal to the Bayes optimal classifier to justify dropping terms corresponding to the Hessian(\\phi_y). I am not sure how realistic this assumption is in the adversarial setting - one can construct simple distributions for which the Bayes optimal classifier is not the robust classifier. With regards to Figure 3, the authors state - \u201cAs the decay length goes to zero, the synthetic covariance matrix converges to the identity matrix and SGR performance approaches GN performance\u201d Could the authors clarify why this is obvious? After all these two models are trained very differently. The plot in Figure 3 and the results in Table 1 seems to illustrate that SGR is no better than GN as you can find an attack where they perform as well/badly. The authors say that this is due to the short-range nature of current attacks. I do not understand this rationale though - the goal of the defenses should be to be more robust to all attacks, both short range and long range. Thus arguing that there may be an attack under which their model performs better is not sufficient. I do agree that finding long range attacks that can break current SOTA robust models would be interesting, however the authors do not seem to achieve that in this work. I find the observation on transfer attacks interesting - PGD attacks from SGR/GN models are better than PGD models. Do the authors have any insight as to why this is the case? In general, my concern about gradient regularization based defenses is that they only give a very local picture of the landscape and thus can only protect against small eps attacks. This could probably explain why the SGR/GN models are less robust than PGD. As mentioned previously, it would be valuable to see accuracies against individual eps values (rather than averaged) to understand this better. If this is the case, this regularization would not provide any additional benefits when combined with adversarial training either. References: Schmidt, Ludwig, et al. \"Adversarially Robust Generalization Requires More Data.\" arXiv preprint arXiv:1804.11285 (2018).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for his/her valuable feedback . - Merits of structured gradient regularization - SGR has several conceptual merits : Firstly , one of the main contributions of our work is to * * derive structured gradient regularization * * as a tractable approximation to training with correlated perturbations . SGR is a generalization of gradient norm ( GN ) regularization : while GN provides an approximation to training with white noise , SGR provides an approximation to training with arbitrarily correlated noise . This is in line with a large body of work on the equivalence between regularization and robust optimization . See our reply titled `` Our work is a strict generalization of previous work and regularization was proven to be equivalent to robust optimization in certain settings . '' for a list of references . Secondly , while robust optimization aims at approximating the worst-case distribution , we propose to efficiently approximate expectations over corrupted distributions through structure-informed regularization . Conceptually , rather than perturbing each data point individually , our starting point is to learn a corruption model , i.e.to use a generative mechanism to learn adversarial perturbations from examples . In practice , we propose to approximate such a corruption model by adaptively learning the structure of adversarial perturbations . Thirdly , SGR can leverage the fact that adversarial examples might live in low-dimensional subspaces . Quoting from [ Moosavi-Dezfooli et al , \u201c Universal adversarial perturbations \u201d , 2017 ] : \u201c We hypothesize that the existence of universal perturbations fooling most natural images is partly due to the existence of such a low-dimensional subspace that captures the correlations among different regions of the decision boundary. \u201d SGR can leverage this by penalizing gradients that lie within such a subspace . - Combining SGR with adversarial training - It has certainly occurred to us to combine SGR with adversarial training . However , in the interest of transparency , we believe it is more clear to benchmark and compare regularization and adversarial training individually . Nevertheless , we will investigate combining them . - Covariance function - The covariance function is just a simple parametrization of the covariance matrix in terms of the displacement between pixels , as is well-known in computer vision . We apologize for omitting to specify that the PGD attack was L_infty constrained . - Attack accuracy vs area under the attack curve - Reporting area under the attack curve serves two purposes . Firstly , it addresses the potential danger of overfitting to a specific attack epsilon . Secondly , it mimics the realistic scenario in which the attacker tries to fool the classifier with as small a perturbation as possible . That said , we believe that an even more realistic performance measure would give less weight to larger perturbations that are easier to detect and give relatively more weight to smaller ones that are harder to detect . ( Note , the numbers we currently report give equal weight to different perturbation strengths . ) - Cancellation of Laplacian terms - The underlying assumption is that Eq . ( 10 ) and Eq . ( 5 ) coincide to order O ( ||\\xi ||^3 ) at the Bayes optimum , which is within the precision to which we truncate . This assumption is rather common in the literature , see e.g . [ Bishop.Training with noise is equivalent to tikhonov regularization. , 1995 ] or [ An , G. The Effects of Adding Noise During Backpropagation Training on a Generalization Performance . 1996 ] .Alternatively , Eq . ( 10 ) can also be seen as a Levenberg-Marquart approximation of Eq . ( 5 ) , if one does not want to invoke the Bayes optimality argument , see Section 5.4.1 in Bishop \u2019 s Pattern Recognition and Machine Learning book . - Long-range correlated noise attack - We do not claim that the LRC attack can break existing methods . The purpose of the LRC attack experiment is solely to establish whether there is a potential benefit in using a structured covariance matrix in the SGR regularizer versus using an \u201c unstructured \u201d diagonal covariance ( corresponding to gradient-norm regularization ) in the presence of long-range correlated noise . In other words , this experiment simply tests whether the SGR regularizer extracts useful information about the long-range correlation structure of the perturbations , which it indeed does . - Decay length approaching zero - The quoted statement is indeed trivial : if SGR is trained from scratch with a covariance matrix that is close to the identity matrix ( i.e.the covariance matrix has a decay length close to zero ) , its performance will be similar to that of GN , as shown in Figure 3 . Note , that each data point in Figure 3 corresponds to ( an average of five ) networks that have been trained from scratch with a covariance matrix of the given decay length ."}, {"review_id": "HyxBpoR5tm-1", "review_text": "Summary of the paper: This paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. \"structured\" means that instead of just minimizing the L2 norm of the gradients, a \"mahalanobis norm\" is minimized. The covariance matrix is updated continuously to track the \"structure\" of gradients/perturbations. Whitebox attack and blackbox attack The paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting. However, I believe the paper has major flaws in several aspects. The whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. https://github.com/MadryLab/mnist_challenge I noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST. In my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because \"intrinsic\" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated. So if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners. There're also a few problems in the motivation / analysis. \"\"\"A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations.\"\"\" The adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss. \"\"\"Thus, under the assumption that \\phi \\approx \\phi^* and of small perturbations (such that we can ignore higher order terms.\"\"\" The Bayes optimal assumption seems to be arbitrary to me. If \\phi is nearly Bayes-optimal, why would we worry about adversarial examples? Other relatively minor problems In the caption of Figure 1, \"\"\"Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure.\"\"\" PGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse? In Section 3.1, the paper talks about both centered and uncentered adversarial examples. I assumed that the authors mean that the distribution of perturbations are centered? First, I think this the authors should make this more explicit. Second, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered? Figure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper. I don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved? Figure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare.", "rating": "3: Clear rejection", "reply_text": "We would like to thank the reviewer for his/her valuable feedback . - PGD attack iterations - Regarding PGD iterations , we would like to quote [ Madry A. et al.Towards deep learning models resistant to adversarial attacks , 2017 . ] , who reported whitebox attack accuracies for PGD with * * 7 iterations * * ( see Table 2 ) : \u201c For the CIFAR10 dataset , [ ... ] we trained the network against a PGD adversary with l_infty projected gradient descent again , this time using 7 steps of size 2 , and a total \u03b5 = 8. \u201d That said , we don \u2019 t think that more iterations and random restarts would change the qualitative picture of our evaluations . - Attack accuracy vs area under the attack curve - Reporting area under the attack curve serves two purposes : Firstly , it addresses the potential danger of overfitting to a specific epsilon attack . Secondly , it mimics the realistic scenario in which the attacker tries to fool the classifier with as small a perturbation as possible . That said , we believe that an even more realistic performance measure would give less weight to larger perturbations that are easier to detect and give relatively more weight to smaller ones that are harder to detect . - Robustness under the strongest whitebox attack should be the benchmark - We disagree with this statement for two reasons . Firstly , without reference to an attack \u201c budget \u201d , more precisely a ( distributional ) uncertainty set as well as an upper bound on computational resources to search for worst case perturbations , the notion of \u201c strongest \u201d is ill-defined . Even if we agree on a computational budget , the question remains of how to define or measure the strength of perturbations - norm-based , perceptually similar , etc . Secondly , robustness comes at a price : rather than aiming for robustness against the strongest attack , we believe that one should aim for an optimal trade-off between robustness and clean accuracy . In that sense , it is debatable whether training methods that considerably reduce clean accuracy even deserve to be called robust . It is worth noting that this latter point has long been understood in the statistics community , see for instance P.J . Huber \u2019 s book on Robust Statistics . - Adversarial robustness via integrating over perturbations - We propose to efficiently approximate expectations over corrupted distributions through structure-informed regularization , as outlined in Section 2.2 ( see Equation 3 ) and Section 3 . Conceptually , our starting point is to learn a corruption model , i.e.to use a generative mechanism to learn adversarial perturbations from examples . * * Integrating over these corruptions is not the same as integrating over the neighborhood , however * * . The intuition is that if the model is robust against the entire distribution of perturbations , it should also be robust against point-wise perturbations ( from which the corruption model was learned ) . In practice , we propose to approximate such a corruption model by adaptively learning the structure of adversarial perturbations . One of the main contributions of our work is to * * derive structured gradient regularization * * as a tractable approximation to training with correlated perturbations . This is in line with a large body of work on the equivalence between regularization and robust optimization . See our reply titled `` Our work is a strict generalization of previous work and regularization was proven to be equivalent to robust optimization in certain settings . '' for a list of references . - Bayes optimal classifier - The underlying assumption is that Eq . ( 10 ) and Eq . ( 5 ) coincide to order O ( ||\\xi ||^3 ) at the optimum , which is within the precision to which we truncate . This assumption is rather common in the literature , see e.g . [ Bishop.Training with noise is equivalent to tikhonov regularization. , 1995 ] or [ An , G. The Effects of Adding Noise During Backpropagation Training on a Generalization Performance . 1996 ] .Alternatively , Eq . ( 10 ) can also be seen as a Levenberg-Marquart approximation of Eq . ( 5 ) , if one does not want to invoke the Bayes optimality argument , see Section 5.4.1 in Bishop \u2019 s Pattern Recognition and Machine Learning book . - Covariance structure too coarse as a measure of attack power ? - Whether or not covariance structure is a good measure to distinguish different attacks depends on the entirety of attacks under consideration . It could be that both PGD and FGSM are members of the same \u201c universality class \u201d of adversarial attacks . After all , if we compare those two attacks with the entirety of all imaginable attacks , they are probably rather similar compared to other , e.g.gradient-free attacks . Nevertheless , we agree that it would be interesting to further explore the connection between covariance structure and attack power ."}, {"review_id": "HyxBpoR5tm-2", "review_text": "Short paper summary: This work proposes a novel method of gradient regularization (SGR) which utilizes the covariance structure of adversarial examples generated during training. The authors propose simple techniques to reduce the computational overhead of SGR. Empirically, the authors compare their method to standard adversarial training and gradient norm regularization. Brief review summary: There are some interesting ideas in this work but I feel that the some practical aspects lack formal justification and the comparison to existing work is inconclusive. Detailed comments: In addition to some minor comments, I have two concerns. First, with the SGR algorithm itself. And second with the empirical analysis. While I suspect that the first concern may be clarified with discussion I think that the second is more serious and is the primary factor behind my review score. 1) As the SGR algorithm is written I wonder whether the regularization term may be computed more efficiently using something like a Hutchinson trace estimation trick. I suspect that if the random vector used to estimate the trace was the xi from Algorithm 1 then the same Mahalanobis gradient norm would be recovered. This would hold only in the case beta=1, bringing me to my second point. 2) What is the purpose of the running average of the covariance? A relatively small beta value is used in practice but I do not see any strong justification for this. Is there a good reason why we do not want the covariance matrix to be a close approximation for the local gradient landscape? This seems like an important part of the algorithm, especially as it may shed light on my next note. 3) In practice, Algorithm 1 uses adversarial attack schemes to generate the perturbations. In simple cases like FGM, this would give the covariance of the input-output gradient which seems that it would have a direct interpretation as a form of classical gradient regularization. To this extent, I also wonder how the SGR algorithm could be related to interpretations of adversarial training as gradient smoothing (when using small perturbations). I recognize that the above points are (so far as I could tell) not directly addressed in the work, and some may be fairly considered out of scope. However, due to the direct comparison to adversarial training later and the need to tie SGR to adversarial attacks I feel that it would be important to distinguish these cases. Overall, I felt that the first three sections did well to introduce the motivation and techniques used and were was easy to follow. The derivation of the SGR algorithm was clear and concise but I believe that some of the practical details (covariance running average, computational efficiency [at first glance, it looks like the full Jacobian must be computed, but practically the sum over K reduces this to a single backprop call]) could have been elaborated on. For the empirical evaluation the authors provided ample detail on the experimental set up and have performed a fairly thorough investigation in terms of existing defenses and attacks. I felt that the bulk of the study which is contained in Table 1 is fairly inconclusive or at the very least, difficult to interpret completely. Additional comments: 4) I felt that Figure 1 and 2 are a little difficult to interpret at first. It would help to clearly define what is meant by short- and long-range signal corruptions. However, they do suggest some interesting findings. As these covariance matrices depend directly on the model itself, I think it is worth investigate (or commenting on) how this structure may change when introducing things like SGR (or GN). The authors claim that unregularized classifiers give too much weight to short range correlations but they should show that SGN (or other methods) correct this. 5) My biggest concern with this work is with the results presented in Table 1. In terms of how they are presented: first I think that the fool column requires further explanation, or perhaps more simply the column could show accuracy instead of the average perturbation size. Second, I am not sure why the reported accuracies are averaged over attack strengths in a range. So far as I am aware, this is not standard and makes it difficult to interpret the performance of the models in this way. Figure 4 in the appendix does a better job of describing the behavior over a range of attack strengths. 6) From the table, it is not obvious to me that SGR provides any improvements to robustness over existing techniques. Indeed, the authors write that SGR achieves white-box accuracies which are between those of the clean and adversarially trained models and claim that SGR improves on the clean accuracy for CIFAR-10. But in the table the gap between FGSM and GN/SGR clean accuracies seem fairly small with FGSM providing better robustness (for most source attacks). Even more concerning, is the fact that GN seems to outperform SGR. I do not find these results substantial enough to motivate SGR as a robustness defense compared with adversarial training (or even GN), especially as SGR has the same computational limitations involved with expensive adversarial perturbations. I felt that the study into the covariance structure of adversarial perturbations was interesting but as it stands was not complete enough to be informative in general. In the conclusion the authors write that they provide evidence that current adversarial attacks act by perturbing the short-range correlations of signals but this has only been confirmed for unregularized classifiers. Despite these issues, I thought that the paper was well written and hope that the empirical study can be improved and clarified. Minor comments: - Section 2.1, set of transformations only introduced briefly then forgotten. Leaving output invariant confused me, as this does not apply to adversarial examples. - Section 2.3, second paragraph l3: In Maaten et al. should be citet. - Section 3.1, should make clear that derivative is with respect to the data. - Section 3.1, define delta as the Hessian clearly (it is used for the simplex in the previous section). Though this is easy to figure out. - Section 7.1, starts with (iii), is this intentional? Perhaps an introductory sentence could make this clearer. - Section 7.3, for label leaking, I'm not convinced by this argument alone. Assuming the covariance structure is still computed from a particular adversarial example, I see no compelling reason that this would not occur. Clarity: The paper is very clearly written and is easy to follow.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for his/her valuable feedback . 1 ) Hutchinson trace estimation trick The Hutchinson trace estimation trick doesn \u2019 t seem to be relevant for our regularizer : we are not primarily concerned with the problem of estimating the trace of the covariance matrix , but we are rather interested in leveraging the sparseness of the covariance-gradient matrix-vector product . Irrespective of that , we can already efficiently aggregate batch estimates for the covariance structure in our regularizer , as the input gradient of the per-sample cross-entropy loss is often available as a highly optimized callable operation in modern deep learning frameworks . Nevertheless , it is an interesting suggestion which we would be happy to investigate further . 2 ) What is the purpose of the running average in the covariance ? The decay rate \u03b2 allows us to trade off weighting between current ( \u03b2 \u2192 1 ) and past ( \u03b2 \u2192 0 ) batch averages . The idea of using smaller decay rates is that this should avoid overfitting to a specific attack : the more of the history we take into account ( i.e.the more momentum ) , the less likely the model is to overfit on specific perturbations . Our choice of \u03b2=0.1 was inspired by momentum-based adaptive optimization algorithms like Adam , which also by default gives a weight of 0.1 to current gradients and a weight of 0.9 to past gradients . We did not observe a big difference in our experiments for other values of \u03b2 . 3 ) SGR algorithm vs. adversarial training as gradient smoothing Our regularizer is informed by the covariance structure of adversarial perturbations , which for simple perturbations , like FGM , is indeed given by the covariance of the input-output gradient . That said , it seems well worth exploring whether adversarial training can be interpreted as gradient smoothing and how this is connected to SGR regularization . 4 ) Covariance structure of adversarial perturbations and how it might change The decay length is defined as the displacement over which the covariance function decays to 1/e of its value . The covariance function is just a simple parametrization of the covariance matrix in terms of the displacement between pixels , as is well-known in computer vision . Based on the observation that unregularized/undefended classifiers are vulnerable to short-range structured corruptions , we thus conjecture that they give too much weight to short-range correlations ( high-frequency patterns ) and not enough weight to long-range ones ( globally relevant low-frequency features ) . The question of how this structure may change when robustifying the model through adversarial training or SGR regularization is indeed interesting . What makes this analysis complicated , however , is the fact that the * * covariance structure not only depends on the model but also on the attack algorithm * * . So , if the model becomes more robust to short-range correlated perturbations , the following two things can happen ( potentially both ) : ( i ) new perturbations become less effective and thus more random , in which case the decay-length of the covariance function becomes even shorter . Or ( ii ) the attack will adapt to perturb the long-range ( low-frequency ) content of the signal , if it is powerful enough . Assessing the covariance function change therefore seems rather non-trivial , as one would need to separate the effect of model robustness from attack algorithm adaptivity/non-adaptivity . We did not observe meaningful changes of the covariance structure in our experiments , which is not a negative result due to the above points however . 5 ) White-box and transfer attack accuracy results The DeepFool attack is unconstrained : if it is run for sufficiently many iterations , it should always reduce the accuracy of the classifier to below chance . This is why the Fool column in Table 1 reports the magnitudes of the perturbations required to cross the decision boundary ( normalized by the magnitude of the unperturbed data point ) , according to Equation 2 ( or its empirical counterpart in Equation 15 ) in [ Moosavi-Dezfooli et al , DeepFool : A Simple and Accurate Method to Fool Deep Neural Networks , 2016 ] . Reporting area under the attack curve serves two purposes . Firstly , it addresses the potential danger of overfitting to a specific attack epsilon . Secondly , it mimics the realistic scenario in which the attacker tries to fool the classifier with as small a perturbation as possible . That said , we believe that an even more realistic performance measure would give less weight to larger perturbations that are easier to detect and give relatively more weight to smaller ones that are harder to detect . ( Note , the numbers we report give equal weight to different perturbation strengths . )"}], "0": {"review_id": "HyxBpoR5tm-0", "review_text": "The authors propose a new defense against adversarial examples that relies on a data-dependent regularization (instead of adversarial training). They then benchmark the performance of this new defense against popular white-box and transfer attacks, as well as propose a new long range correlated adversarial attack. Comments: I find the premise of this paper interesting - developing regularization strategies to help with generalization to adversarial perturbations. For instance, it is well known that state-of-the-art defenses such as PGD have generalization gaps as large as 50% between robust train and test accuracies. It has also been previously hypothesized that this could be due to a data scarcity problem [Schmidt et al., 2018]. The authors here propose to tackle this problem using a new data-dependent regularization technique. My primary issue with this paper is that the authors do not clearly illustrate what the advantage of their method over standard methods is - The problem this paper aims to solve is overfitting to a specific attack/virtual adversarial examples presented during adversarial training by using regularization instead. However, the authors do not actually illustrate that their technique reduces overfitting. For instance, the authors do not contrast the robust train-test accuracies using their method to other standard methods. Thus it is not clear that this paper met the objectives laid out in the introduction. - The claim in this paper is that SGR helps against attacks with long range dependencies. However, in their experiments (e.g., in Figure 3), the authors do not evaluate other standard defenses. It is thus unclear whether other standard methods are already robust to such attacks. In fact, based on the results of Table 1, it doesn\u2019t seem like attacks from SGR are able to reduce the robustness of PGD/FGSM trained models. Because of these two points, along with the lower robustness to various attacks (in Table 1) as compared to approaches such as PGD, it is not really clear to me what the real merit of this new approach is. Ultimately, having a defense which is more robust to a particular attack is not very meaningful if there exists an alternative attack that reduces the robustness of the defense. I am also surprised that the authors chose to use this regularization as an alternative to adversarial training instead of complementary to it. I would be interested to see if such regularization could actually help to bridge the generalization gap observed while using adversarial training. The paper is at times is poorly written and confusing. For instance, the description of CovFun is hard to parse. The authors should make this explanation more clear. The authors also do not state what their attack model is - Linf vs L2 perturbations. They also choose to evaluate attacks differently, using an average accuracy over different epsilons rather than reporting individual accuracies. This does make the results harder to compare to other work. The authors should include a full table of individual accuracies (at least in the appendix) to make the numbers easier to parse and compare. In the derivation in Section 3.1, the authors use the assumption that the robust classifier is almost equal to the Bayes optimal classifier to justify dropping terms corresponding to the Hessian(\\phi_y). I am not sure how realistic this assumption is in the adversarial setting - one can construct simple distributions for which the Bayes optimal classifier is not the robust classifier. With regards to Figure 3, the authors state - \u201cAs the decay length goes to zero, the synthetic covariance matrix converges to the identity matrix and SGR performance approaches GN performance\u201d Could the authors clarify why this is obvious? After all these two models are trained very differently. The plot in Figure 3 and the results in Table 1 seems to illustrate that SGR is no better than GN as you can find an attack where they perform as well/badly. The authors say that this is due to the short-range nature of current attacks. I do not understand this rationale though - the goal of the defenses should be to be more robust to all attacks, both short range and long range. Thus arguing that there may be an attack under which their model performs better is not sufficient. I do agree that finding long range attacks that can break current SOTA robust models would be interesting, however the authors do not seem to achieve that in this work. I find the observation on transfer attacks interesting - PGD attacks from SGR/GN models are better than PGD models. Do the authors have any insight as to why this is the case? In general, my concern about gradient regularization based defenses is that they only give a very local picture of the landscape and thus can only protect against small eps attacks. This could probably explain why the SGR/GN models are less robust than PGD. As mentioned previously, it would be valuable to see accuracies against individual eps values (rather than averaged) to understand this better. If this is the case, this regularization would not provide any additional benefits when combined with adversarial training either. References: Schmidt, Ludwig, et al. \"Adversarially Robust Generalization Requires More Data.\" arXiv preprint arXiv:1804.11285 (2018).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for his/her valuable feedback . - Merits of structured gradient regularization - SGR has several conceptual merits : Firstly , one of the main contributions of our work is to * * derive structured gradient regularization * * as a tractable approximation to training with correlated perturbations . SGR is a generalization of gradient norm ( GN ) regularization : while GN provides an approximation to training with white noise , SGR provides an approximation to training with arbitrarily correlated noise . This is in line with a large body of work on the equivalence between regularization and robust optimization . See our reply titled `` Our work is a strict generalization of previous work and regularization was proven to be equivalent to robust optimization in certain settings . '' for a list of references . Secondly , while robust optimization aims at approximating the worst-case distribution , we propose to efficiently approximate expectations over corrupted distributions through structure-informed regularization . Conceptually , rather than perturbing each data point individually , our starting point is to learn a corruption model , i.e.to use a generative mechanism to learn adversarial perturbations from examples . In practice , we propose to approximate such a corruption model by adaptively learning the structure of adversarial perturbations . Thirdly , SGR can leverage the fact that adversarial examples might live in low-dimensional subspaces . Quoting from [ Moosavi-Dezfooli et al , \u201c Universal adversarial perturbations \u201d , 2017 ] : \u201c We hypothesize that the existence of universal perturbations fooling most natural images is partly due to the existence of such a low-dimensional subspace that captures the correlations among different regions of the decision boundary. \u201d SGR can leverage this by penalizing gradients that lie within such a subspace . - Combining SGR with adversarial training - It has certainly occurred to us to combine SGR with adversarial training . However , in the interest of transparency , we believe it is more clear to benchmark and compare regularization and adversarial training individually . Nevertheless , we will investigate combining them . - Covariance function - The covariance function is just a simple parametrization of the covariance matrix in terms of the displacement between pixels , as is well-known in computer vision . We apologize for omitting to specify that the PGD attack was L_infty constrained . - Attack accuracy vs area under the attack curve - Reporting area under the attack curve serves two purposes . Firstly , it addresses the potential danger of overfitting to a specific attack epsilon . Secondly , it mimics the realistic scenario in which the attacker tries to fool the classifier with as small a perturbation as possible . That said , we believe that an even more realistic performance measure would give less weight to larger perturbations that are easier to detect and give relatively more weight to smaller ones that are harder to detect . ( Note , the numbers we currently report give equal weight to different perturbation strengths . ) - Cancellation of Laplacian terms - The underlying assumption is that Eq . ( 10 ) and Eq . ( 5 ) coincide to order O ( ||\\xi ||^3 ) at the Bayes optimum , which is within the precision to which we truncate . This assumption is rather common in the literature , see e.g . [ Bishop.Training with noise is equivalent to tikhonov regularization. , 1995 ] or [ An , G. The Effects of Adding Noise During Backpropagation Training on a Generalization Performance . 1996 ] .Alternatively , Eq . ( 10 ) can also be seen as a Levenberg-Marquart approximation of Eq . ( 5 ) , if one does not want to invoke the Bayes optimality argument , see Section 5.4.1 in Bishop \u2019 s Pattern Recognition and Machine Learning book . - Long-range correlated noise attack - We do not claim that the LRC attack can break existing methods . The purpose of the LRC attack experiment is solely to establish whether there is a potential benefit in using a structured covariance matrix in the SGR regularizer versus using an \u201c unstructured \u201d diagonal covariance ( corresponding to gradient-norm regularization ) in the presence of long-range correlated noise . In other words , this experiment simply tests whether the SGR regularizer extracts useful information about the long-range correlation structure of the perturbations , which it indeed does . - Decay length approaching zero - The quoted statement is indeed trivial : if SGR is trained from scratch with a covariance matrix that is close to the identity matrix ( i.e.the covariance matrix has a decay length close to zero ) , its performance will be similar to that of GN , as shown in Figure 3 . Note , that each data point in Figure 3 corresponds to ( an average of five ) networks that have been trained from scratch with a covariance matrix of the given decay length ."}, "1": {"review_id": "HyxBpoR5tm-1", "review_text": "Summary of the paper: This paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. \"structured\" means that instead of just minimizing the L2 norm of the gradients, a \"mahalanobis norm\" is minimized. The covariance matrix is updated continuously to track the \"structure\" of gradients/perturbations. Whitebox attack and blackbox attack The paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting. However, I believe the paper has major flaws in several aspects. The whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. https://github.com/MadryLab/mnist_challenge I noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST. In my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because \"intrinsic\" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated. So if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners. There're also a few problems in the motivation / analysis. \"\"\"A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations.\"\"\" The adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss. \"\"\"Thus, under the assumption that \\phi \\approx \\phi^* and of small perturbations (such that we can ignore higher order terms.\"\"\" The Bayes optimal assumption seems to be arbitrary to me. If \\phi is nearly Bayes-optimal, why would we worry about adversarial examples? Other relatively minor problems In the caption of Figure 1, \"\"\"Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure.\"\"\" PGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse? In Section 3.1, the paper talks about both centered and uncentered adversarial examples. I assumed that the authors mean that the distribution of perturbations are centered? First, I think this the authors should make this more explicit. Second, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered? Figure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper. I don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved? Figure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare.", "rating": "3: Clear rejection", "reply_text": "We would like to thank the reviewer for his/her valuable feedback . - PGD attack iterations - Regarding PGD iterations , we would like to quote [ Madry A. et al.Towards deep learning models resistant to adversarial attacks , 2017 . ] , who reported whitebox attack accuracies for PGD with * * 7 iterations * * ( see Table 2 ) : \u201c For the CIFAR10 dataset , [ ... ] we trained the network against a PGD adversary with l_infty projected gradient descent again , this time using 7 steps of size 2 , and a total \u03b5 = 8. \u201d That said , we don \u2019 t think that more iterations and random restarts would change the qualitative picture of our evaluations . - Attack accuracy vs area under the attack curve - Reporting area under the attack curve serves two purposes : Firstly , it addresses the potential danger of overfitting to a specific epsilon attack . Secondly , it mimics the realistic scenario in which the attacker tries to fool the classifier with as small a perturbation as possible . That said , we believe that an even more realistic performance measure would give less weight to larger perturbations that are easier to detect and give relatively more weight to smaller ones that are harder to detect . - Robustness under the strongest whitebox attack should be the benchmark - We disagree with this statement for two reasons . Firstly , without reference to an attack \u201c budget \u201d , more precisely a ( distributional ) uncertainty set as well as an upper bound on computational resources to search for worst case perturbations , the notion of \u201c strongest \u201d is ill-defined . Even if we agree on a computational budget , the question remains of how to define or measure the strength of perturbations - norm-based , perceptually similar , etc . Secondly , robustness comes at a price : rather than aiming for robustness against the strongest attack , we believe that one should aim for an optimal trade-off between robustness and clean accuracy . In that sense , it is debatable whether training methods that considerably reduce clean accuracy even deserve to be called robust . It is worth noting that this latter point has long been understood in the statistics community , see for instance P.J . Huber \u2019 s book on Robust Statistics . - Adversarial robustness via integrating over perturbations - We propose to efficiently approximate expectations over corrupted distributions through structure-informed regularization , as outlined in Section 2.2 ( see Equation 3 ) and Section 3 . Conceptually , our starting point is to learn a corruption model , i.e.to use a generative mechanism to learn adversarial perturbations from examples . * * Integrating over these corruptions is not the same as integrating over the neighborhood , however * * . The intuition is that if the model is robust against the entire distribution of perturbations , it should also be robust against point-wise perturbations ( from which the corruption model was learned ) . In practice , we propose to approximate such a corruption model by adaptively learning the structure of adversarial perturbations . One of the main contributions of our work is to * * derive structured gradient regularization * * as a tractable approximation to training with correlated perturbations . This is in line with a large body of work on the equivalence between regularization and robust optimization . See our reply titled `` Our work is a strict generalization of previous work and regularization was proven to be equivalent to robust optimization in certain settings . '' for a list of references . - Bayes optimal classifier - The underlying assumption is that Eq . ( 10 ) and Eq . ( 5 ) coincide to order O ( ||\\xi ||^3 ) at the optimum , which is within the precision to which we truncate . This assumption is rather common in the literature , see e.g . [ Bishop.Training with noise is equivalent to tikhonov regularization. , 1995 ] or [ An , G. The Effects of Adding Noise During Backpropagation Training on a Generalization Performance . 1996 ] .Alternatively , Eq . ( 10 ) can also be seen as a Levenberg-Marquart approximation of Eq . ( 5 ) , if one does not want to invoke the Bayes optimality argument , see Section 5.4.1 in Bishop \u2019 s Pattern Recognition and Machine Learning book . - Covariance structure too coarse as a measure of attack power ? - Whether or not covariance structure is a good measure to distinguish different attacks depends on the entirety of attacks under consideration . It could be that both PGD and FGSM are members of the same \u201c universality class \u201d of adversarial attacks . After all , if we compare those two attacks with the entirety of all imaginable attacks , they are probably rather similar compared to other , e.g.gradient-free attacks . Nevertheless , we agree that it would be interesting to further explore the connection between covariance structure and attack power ."}, "2": {"review_id": "HyxBpoR5tm-2", "review_text": "Short paper summary: This work proposes a novel method of gradient regularization (SGR) which utilizes the covariance structure of adversarial examples generated during training. The authors propose simple techniques to reduce the computational overhead of SGR. Empirically, the authors compare their method to standard adversarial training and gradient norm regularization. Brief review summary: There are some interesting ideas in this work but I feel that the some practical aspects lack formal justification and the comparison to existing work is inconclusive. Detailed comments: In addition to some minor comments, I have two concerns. First, with the SGR algorithm itself. And second with the empirical analysis. While I suspect that the first concern may be clarified with discussion I think that the second is more serious and is the primary factor behind my review score. 1) As the SGR algorithm is written I wonder whether the regularization term may be computed more efficiently using something like a Hutchinson trace estimation trick. I suspect that if the random vector used to estimate the trace was the xi from Algorithm 1 then the same Mahalanobis gradient norm would be recovered. This would hold only in the case beta=1, bringing me to my second point. 2) What is the purpose of the running average of the covariance? A relatively small beta value is used in practice but I do not see any strong justification for this. Is there a good reason why we do not want the covariance matrix to be a close approximation for the local gradient landscape? This seems like an important part of the algorithm, especially as it may shed light on my next note. 3) In practice, Algorithm 1 uses adversarial attack schemes to generate the perturbations. In simple cases like FGM, this would give the covariance of the input-output gradient which seems that it would have a direct interpretation as a form of classical gradient regularization. To this extent, I also wonder how the SGR algorithm could be related to interpretations of adversarial training as gradient smoothing (when using small perturbations). I recognize that the above points are (so far as I could tell) not directly addressed in the work, and some may be fairly considered out of scope. However, due to the direct comparison to adversarial training later and the need to tie SGR to adversarial attacks I feel that it would be important to distinguish these cases. Overall, I felt that the first three sections did well to introduce the motivation and techniques used and were was easy to follow. The derivation of the SGR algorithm was clear and concise but I believe that some of the practical details (covariance running average, computational efficiency [at first glance, it looks like the full Jacobian must be computed, but practically the sum over K reduces this to a single backprop call]) could have been elaborated on. For the empirical evaluation the authors provided ample detail on the experimental set up and have performed a fairly thorough investigation in terms of existing defenses and attacks. I felt that the bulk of the study which is contained in Table 1 is fairly inconclusive or at the very least, difficult to interpret completely. Additional comments: 4) I felt that Figure 1 and 2 are a little difficult to interpret at first. It would help to clearly define what is meant by short- and long-range signal corruptions. However, they do suggest some interesting findings. As these covariance matrices depend directly on the model itself, I think it is worth investigate (or commenting on) how this structure may change when introducing things like SGR (or GN). The authors claim that unregularized classifiers give too much weight to short range correlations but they should show that SGN (or other methods) correct this. 5) My biggest concern with this work is with the results presented in Table 1. In terms of how they are presented: first I think that the fool column requires further explanation, or perhaps more simply the column could show accuracy instead of the average perturbation size. Second, I am not sure why the reported accuracies are averaged over attack strengths in a range. So far as I am aware, this is not standard and makes it difficult to interpret the performance of the models in this way. Figure 4 in the appendix does a better job of describing the behavior over a range of attack strengths. 6) From the table, it is not obvious to me that SGR provides any improvements to robustness over existing techniques. Indeed, the authors write that SGR achieves white-box accuracies which are between those of the clean and adversarially trained models and claim that SGR improves on the clean accuracy for CIFAR-10. But in the table the gap between FGSM and GN/SGR clean accuracies seem fairly small with FGSM providing better robustness (for most source attacks). Even more concerning, is the fact that GN seems to outperform SGR. I do not find these results substantial enough to motivate SGR as a robustness defense compared with adversarial training (or even GN), especially as SGR has the same computational limitations involved with expensive adversarial perturbations. I felt that the study into the covariance structure of adversarial perturbations was interesting but as it stands was not complete enough to be informative in general. In the conclusion the authors write that they provide evidence that current adversarial attacks act by perturbing the short-range correlations of signals but this has only been confirmed for unregularized classifiers. Despite these issues, I thought that the paper was well written and hope that the empirical study can be improved and clarified. Minor comments: - Section 2.1, set of transformations only introduced briefly then forgotten. Leaving output invariant confused me, as this does not apply to adversarial examples. - Section 2.3, second paragraph l3: In Maaten et al. should be citet. - Section 3.1, should make clear that derivative is with respect to the data. - Section 3.1, define delta as the Hessian clearly (it is used for the simplex in the previous section). Though this is easy to figure out. - Section 7.1, starts with (iii), is this intentional? Perhaps an introductory sentence could make this clearer. - Section 7.3, for label leaking, I'm not convinced by this argument alone. Assuming the covariance structure is still computed from a particular adversarial example, I see no compelling reason that this would not occur. Clarity: The paper is very clearly written and is easy to follow.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for his/her valuable feedback . 1 ) Hutchinson trace estimation trick The Hutchinson trace estimation trick doesn \u2019 t seem to be relevant for our regularizer : we are not primarily concerned with the problem of estimating the trace of the covariance matrix , but we are rather interested in leveraging the sparseness of the covariance-gradient matrix-vector product . Irrespective of that , we can already efficiently aggregate batch estimates for the covariance structure in our regularizer , as the input gradient of the per-sample cross-entropy loss is often available as a highly optimized callable operation in modern deep learning frameworks . Nevertheless , it is an interesting suggestion which we would be happy to investigate further . 2 ) What is the purpose of the running average in the covariance ? The decay rate \u03b2 allows us to trade off weighting between current ( \u03b2 \u2192 1 ) and past ( \u03b2 \u2192 0 ) batch averages . The idea of using smaller decay rates is that this should avoid overfitting to a specific attack : the more of the history we take into account ( i.e.the more momentum ) , the less likely the model is to overfit on specific perturbations . Our choice of \u03b2=0.1 was inspired by momentum-based adaptive optimization algorithms like Adam , which also by default gives a weight of 0.1 to current gradients and a weight of 0.9 to past gradients . We did not observe a big difference in our experiments for other values of \u03b2 . 3 ) SGR algorithm vs. adversarial training as gradient smoothing Our regularizer is informed by the covariance structure of adversarial perturbations , which for simple perturbations , like FGM , is indeed given by the covariance of the input-output gradient . That said , it seems well worth exploring whether adversarial training can be interpreted as gradient smoothing and how this is connected to SGR regularization . 4 ) Covariance structure of adversarial perturbations and how it might change The decay length is defined as the displacement over which the covariance function decays to 1/e of its value . The covariance function is just a simple parametrization of the covariance matrix in terms of the displacement between pixels , as is well-known in computer vision . Based on the observation that unregularized/undefended classifiers are vulnerable to short-range structured corruptions , we thus conjecture that they give too much weight to short-range correlations ( high-frequency patterns ) and not enough weight to long-range ones ( globally relevant low-frequency features ) . The question of how this structure may change when robustifying the model through adversarial training or SGR regularization is indeed interesting . What makes this analysis complicated , however , is the fact that the * * covariance structure not only depends on the model but also on the attack algorithm * * . So , if the model becomes more robust to short-range correlated perturbations , the following two things can happen ( potentially both ) : ( i ) new perturbations become less effective and thus more random , in which case the decay-length of the covariance function becomes even shorter . Or ( ii ) the attack will adapt to perturb the long-range ( low-frequency ) content of the signal , if it is powerful enough . Assessing the covariance function change therefore seems rather non-trivial , as one would need to separate the effect of model robustness from attack algorithm adaptivity/non-adaptivity . We did not observe meaningful changes of the covariance structure in our experiments , which is not a negative result due to the above points however . 5 ) White-box and transfer attack accuracy results The DeepFool attack is unconstrained : if it is run for sufficiently many iterations , it should always reduce the accuracy of the classifier to below chance . This is why the Fool column in Table 1 reports the magnitudes of the perturbations required to cross the decision boundary ( normalized by the magnitude of the unperturbed data point ) , according to Equation 2 ( or its empirical counterpart in Equation 15 ) in [ Moosavi-Dezfooli et al , DeepFool : A Simple and Accurate Method to Fool Deep Neural Networks , 2016 ] . Reporting area under the attack curve serves two purposes . Firstly , it addresses the potential danger of overfitting to a specific attack epsilon . Secondly , it mimics the realistic scenario in which the attacker tries to fool the classifier with as small a perturbation as possible . That said , we believe that an even more realistic performance measure would give less weight to larger perturbations that are easier to detect and give relatively more weight to smaller ones that are harder to detect . ( Note , the numbers we report give equal weight to different perturbation strengths . )"}}