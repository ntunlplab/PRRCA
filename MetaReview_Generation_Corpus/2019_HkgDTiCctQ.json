{"year": "2019", "forum": "HkgDTiCctQ", "title": "Knowledge Distillation from Few Samples", "decision": "Reject", "meta_review": "The paper considers the problem of knowledge distillation from a few samples. The proposed solution is to align feature representations of the student network with the teacher by adding 1x1 convolutions to each student block, and learning only the parameters of those layers. As noted by Reviewers 1 and 2, the performance of the proposed method is rather poor in absolute terms, and the use case considered (distillation from a few samples) is not motivated well enough. Reviewers also note the method is quite simplistic and incremental.", "reviews": [{"review_id": "HkgDTiCctQ-0", "review_text": "This paper proposes a framework for few-sample knowledge distillation of convolution neural networks. The basic idea is to fit the output of the student network and that of the teacher network layer-wisely. Such a regression problem is parameterized by a 1x1 point-wise convolution per layer (i.e. minimizing the fitting objective over the parameters of 1x1 convolutions). The author claims such an approach, called FSKD, is much more sample-efficient than previous works on knowledge distillation. Besides, it is also fast to finish the alignment procedure as the number of parameters is smaller than that in previous works. The sample efficiency is confirmed in the experiments on CIFAR-10, CIFAR-100 and ImageNet with various pruning techniques. In particular, FSKD outperforms the FitNet and fine-tuning by non-trivial margins if only small amount of samples are provided (e.g. 100). Here are some comments: 1. What exactly does \u201cabsorb\u201d mean? Is it formally defined in the paper? 2. \u201cwe do not optimize this loss all together using SGD due to that too much hyper-parameters need tuning in SGD\u201d. I don\u2019t understand (1) why does SGD require \u201ctoo much\u201d hyper-parameters tuning and (2) if not SGD, what algorithm do you use? 3. According to the illustration in 3.3, the algorithm looks like a coordinate decent that optimizing L over one Q_j at a time, with the rest fixed. However, the sentence \u201cuntil we reach the last block in the student-net\u201d means the algorithm only runs one iteration, which I suspect might not be sufficient to converge. 4. It is also confusing to use the notation SGD+FSKD v.s. FitNet+FSKD, as it seems SGD and FitNet are referring to the same type of terminology. However, SGD is an algorithm, while FitNet is an approach for neural network distillation. 5. While I understand the training of student network with FSKD should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient? 6. I assume the Top-1 accuracies of teacher networks in Figure 4 are the same as table 2 and 3, i.e. 93.38% and 72.08% for CIFAR-10 and CIFAR-100 respectively. Then the student networks have much worse performance (~85% for CIFAR-10 and ~48% for CIFAR-100) than the teachers. So does it mean FSKD is not good for redesigned student networks? 7. While most of the experiments are on CIFAR10 and CIFAR100, the abstract and conclusion only mention the results of ImageNet. Why?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the valuable comments and suggestions . We give detailed responses to each item below . 1.Meaning of \u201c absorb \u201d We add a 1x1 conv-layer Q \\in R^ { n_o \u2019 * n_o * 1 * 1 } to student-net after the conv-layer W\\in R^ { n_o * n_i * k * k } before non-linear layer , \u201c absorb \u201d here means Q can be merged into W to obtain a new conv-layer W \u2019 \\in R^ { n_o \u2019 * n_i * k * k } . If Q is squared ( n_o \u2019 =n_o ) , then W \u2019 \\in R^ { n_o * n_i * k * k } has the same size as W. Previously we put this information at appendix-A , and now we revise the description of Theorem-1 to include the information . 2 & 3 : why not SGD , why use one-step block coordinate descent ? Yes , what we use is in fact one-step block coordinate descent ( BCD ) algorithm . We add a description of our BCD algorithm in the appendix-B , also include experiment comparison to FSKD-SGD ( total loss optimization with SGD on all added 1x1 convs \u2019 parameters together ) and FSKD-BCD in the experiments on filter-pruning . The experiments show that FSKD-BCD clearly outperforms FSKD-SGD in all cases . The advantages of the BCD algorithm are also listed in the revision . One major reason is that BCD each time handles few parameters ( one block ) which can be solved with limited samples , while SGD always takes all added 1x1 convs \u2019 parameters into consideration , thus requires more data in the optimization . Our experiments do not show benefit from more iterations of BCD . This may be due to the fact that the added 1x1 conv-layer is before non-linear activations so that one-step linear estimation is accurate enough to get exact minimization . Hong et al [ 1 ] show BCD can reach sublinear convergence when each block is exactly minimized , which is consistent with our experiments . We also add Fig3b to illustrate the accuracy improvement along with block alignment sequentially . [ 1 ] Hong , Mingyi , et al . `` Iteration complexity analysis of block coordinate descent methods . '' Mathematical Programming 163.1-2 ( 2017 ) : 85-114 . 4.Confusing on SGD+FSKD and FitNet+FSKD We denote SGD as optimization without using teacher-net info . For the zero-net experiment , SGD+FSKD first uses SGD to initialize the student-net on few samples without using teacher-net info , then uses FSKD to further improve the performance with teacher-net info . While the Fitnet+FSKD first uses FitNet to initialize the student-net on few-samples with teacher-net guidance , then uses FSKD to further improve the performance with teacher-net info from our FSKD perspective . We clarify this in our revision . 5.Why FSKD is sample efficient ? As is known , fewer parameters tend to require fewer samples for estimation . The BCD algorithm considers each block separately , thus there are much fewer parameters in each block , so that we could use much fewer samples for the block-level estimation . Our experiments also verify this point when comparing FSKD-BCD to FSKD-SGD in Figure-2 , especially when samples < 100 . 6.Not good on redesigned student-net ? Yes , in Figure-4 , the student-net accuracy is about 83 % on CIFAR-10 and about 47 % on CIFAR-100 . We should emphasize that this result is obtained with a very limited number of training samples and without data augmentation . If data augmentation is enabled , about 5 % accuracy improvement could be achieved with FSKD . We design this experiments to demonstrate the effectiveness of FSKD over FitNet and SGD on the few-sample settings , and we do not compare this result with full-data training . 7.Only mention results on ImageNet in abstract and conclusion . We have revised our abstract and conclusion accordingly , even though the results on ImageNet sounds more significant to us ."}, {"review_id": "HkgDTiCctQ-1", "review_text": "Model distillation can be tricky and in my own experience can take a lot of samples (albeit unlabeled, so cheaper and more readily available), as well as time to train. This simple trick seems to be doing quite well at training students quickly with few samples. However, it departs from most student-teacher training that find its primary purpose by actually outperforming students trained from scratch (on the full dataset without time constraints). This trick does not outperform this baseline, so its emphasis is entirely on quick and cheap. However, it's unclear to me how often that is actually necessary and I don't think the paper makes a compelling case in this regard. I am borderline on this work and could probably be swayed either way. Strengths: - It's a very simple and fast technique. As I will cover in a later bullet point (under weaknesses), the paper does not make it clear why this type of model distillation is that useful (since it doesn't improve the student model over full fine-tuning, unlike most student-teacher work). However, the reason why I do see some potential for this paper is because there might be a use case in quickly being able to adapt a pretrained network. It is very common to start from a pretrained model and then attach a new loss and fine-tune. Under this paradigm, it is harder to make architectural adjustments, since you are starting from a finite set of pretrained models made available by other folks (or accept the cost of re-training one yourself). However, it is unclear how careful one needs to treat the pretrained model if more fine-tuning is going to occur. If for instance you could just remove layers, drop some channels, glue it all together, and then that model would still be reasonable as a pretrained model since the fine-tuning stage could tidy everything up, then this method would not be useful in this situation. - The fact that least squares solvers can be used at each stage, without the need for a final end-to-end fine-tune is interesting. - It is good that the paper demonstrates improvements coupled with three separate compression techniques (Li et al., Liu et al., Guo et al.). - The paper is technically thorough. - It's good that the method is evaluated on different styles of networks (VGG, ResNet, DenseNet). Weaknesses: - Limited application because it only makes the distillation faster and cheaper. The primary goal of student-teacher training in literature is to outperform a student trained from scratch by the wisdom of the teacher. It ties into this notion that networks are grossly over-parameterized, but perhaps that is where the training magic comes from. Student-teacher training acknowledges this and tries to find a way to benefit from the over-parameterized training and still end up with a small model. I think the same motivation is used for work in low-rank decomposition and many other network compression methods. However, in Table 1 the \"full fine-tune\" model is actually the clear winner and presented almost as an upper bound here, so the only benefit this paper presents is quick and cheap model distillation, not better models. Because of this, I think this paper needs to spend more time making a case for why this is so important. - Since this technique doesn't outperform full fine-tuning, the goal of this work is much more focused on pure model compression. This could put emphasis on reducing model size, RAM usage reduction, or FLOPS reduction. The paper focuses on the last one, which is an important one as it correlates fairly well with power (the biggest constraint in most on-device scenarios). However, it would be great if the paper gave a broader comparison with compression technique that may have slightly different focus, such as low-rank decomposition. Size and memory usage could be included as columns in tables like 1, along with a few of these methods. - Does it work for aggressive compression? The paper presents mostly modest reductions (30-50%). I thin even if accuracy takes a hit, it could still work to various degrees. From what I can see, the biggest reduction is in Table 4, but FSKD is used throughout this table, so there is no comparison for aggressive compression with other techniques. - The method requires appropriate blocks to line up. If you completely re-design a network, it is not as straightforward as regular student-teacher training. Even the zero-student method requires the same number of channels at certain block ends and it is unclear from the experiments how robust this is. Actually, a bit more analysis into the zero student would be great. For instance, it's very interesting how you randomly initialize (let's say 3x3) kernels, and then the final kernels are actually just linear combinations of these - so, will they look random or will they look fairly good? What if this was done at the initial layer where we can visualize the filters, will they look smooth or not? Other comments: - A comparison with \"Deep Mutual Learning\" might be relevant (Zhang et al.). I think there are also some papers on gradually adjusting neural network architectures (by adding/dropping layers/channels) that are not addressed but seem relevant. I didn't read this recently, but perhaps \"Gradual DropIn of Layers to Train Very Deep Neural Networks\" could be relevant. There is at least one more like this that I've seen that I can't seem to find now. - It could be more clear in the tables exactly what cited method is. For instance, in Table 1, does \"Fine-tuning\" (without FitNet/FSKD) correspond to the work of Li et al. (2016)? I think this should be made more clear, for instance by including a citation in the table for the correct row. Right now, at a glance, it would seem that these results are only comparing against prior work when it compares to FitNet, but as I read further, I understood that's not the case. - The paper could use a visual aid for explaining pruning/slimming/decoupling. Minor comments: - page 4, \"due to that too much hyper-parameters\" - page 4, \"each of the M term\" -> \"terms\" - page 6, methods like FitNet provides\" -> \"provide\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and suggestions , we are happy to address your concerns . Q : Limited applications We agree that teacher-student framework may make student-net has better accuracy than training the student-net from scratch with classification loss ( like cross-entropy loss ) . Our target is fast knowledge distillation from few samples , which has many potential situations for application : First , on-device learning to compression when the device has resource constraints which require cheap knowledge distillation solutions . Second , when software/hardware vendors want to use knowledge distillation for model compression while the full data is not available due to privacy and confidential issues . Third , there is a strict time budget for model optimization so that full training or fine-tuning is not allowed . Thank you for your suggestions and we will articulate these reasons in the revision . Q : Resource usages in the Tables ? Thanks for your suggestions , we added the resources usages for all our experiments ( Table-1~4 ) . Note the parameter pruned ratio is usually higher than the FLOPs reduced ratio . Q : Aggressive compression case In Table1~3 , previously we only show the FLOPs reduction ratio , which may make people confused that our compression ratio is relatively low . However , when we add the compression ratio , we could find that in some cases , the compression ratio is also very high ( > =85 % ) not just for Table-4 . Nevertheless , we introduce a novel \u201c iteratively pruning and FSKD \u201d procedure similar to that in filter-pruning and network slimming , in Appendix-D . This procedure can produce even larger compression ratio ( 88 % ) , as shown in the experiment on CIFAR-10 . Q : Visualize the filter before and after FSKD for zero student net ? The 3 * 3 kernel is too small to be visualized . However , based on the network decoupling theory , any regular convolution could be decoupled into a sum of depthwise separable convolution blocks , where each block consists of a depthwise conv-layer followed by a pointwise ( 1 * 1 ) conv-layer . The pointwise layer is just a linear combination of channels from the depthwise layer . We then visualize the pointwise conv-layer before SGD , after SGD , after SGD + FSKD for the zero-student case in Appendix-F . The experiment shows FSKD improves the smoothness of pointwise convolution tensors . Q : Cite and comparison with two papers ? Thanks for pointing us to these two papers . We have cited and compared with them in our revision . Q : Cite for fine-tuning ? Yes , the fine-tuning for filter-pruning/network slimming is the same as these two papers did . We have cited them in the revision . Q : Illustrate the FSKD case for filter-pruning and network decoupling We have illustrated these two cases in Figure-5 and Figure-6 in Appendix-C . Please see the revised paper for more details . Q : Why Q should be squared ? Squared Q ensures model compression and connectable to the next block ( output channel number matches to the input channel number of next block ) . We have mentioned this in our revision after Theorem-1 ."}, {"review_id": "HkgDTiCctQ-2", "review_text": "In this paper, an efficient re-training algorithm for neural networks is proposed. The essence is like Hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. The core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. Since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. The proposed method named FKSD is simple yet achieves good performance. Also, it performs well with a few samples, which is desirable in terms of time complexity. The downside of this paper is that there is no clear explanation of why the FKSD method goes well. For me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. Of course, learning 1x1 conv is easier than learning original conv because of a few parameters. However, it also restricts the representation power so we cannot say which one is always better. Do you have any hypothesis of why 1x1 conv works so well? Minor: The operator * in (1) is undefined. What does the boldface in tables of the experiments mean? I was confused because, in Table 1, the accuracy achieved by FKSD is in bold but is not the highest one. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and suggestions , we provide our response as follows . Q : Learning 1 * 1-conv restrict the representation power , any hypothesis why the added 1x1 works so well ? As the added 1 * 1 conv-layer is finally absorbed into the decoupled 1 * 1 conv-layer , we here hypothesize that pointwise ( 1 * 1 ) convolution is more critical for performance than the depthwise convolution since it occupies > =80 % parameters of the decoupled network . We design an experiment in Appendix-E to verify this hypothesis by comparing the full training to only training the 1 * 1 conv-layer with 3 * 3 initialized to be orthogonal from random data , on CIFAR-10/100 with VGG16 and ResNet50 . The results show that full training works noticeably worse than our designed case . This interesting result verifies our hypothesis , and may inspire more researches to further understand CNN training optimization . Q : * in Eq-1/2 is undefined . Thanks for pointing out this problem . We have added the definition in the revision . Q : Confused bold-face in the table . Thanks for point out this problem . The boldface just wants to show the best results by FSKD , which may be not the best for all the cases . We remove the bold-face in the table in our revision ."}], "0": {"review_id": "HkgDTiCctQ-0", "review_text": "This paper proposes a framework for few-sample knowledge distillation of convolution neural networks. The basic idea is to fit the output of the student network and that of the teacher network layer-wisely. Such a regression problem is parameterized by a 1x1 point-wise convolution per layer (i.e. minimizing the fitting objective over the parameters of 1x1 convolutions). The author claims such an approach, called FSKD, is much more sample-efficient than previous works on knowledge distillation. Besides, it is also fast to finish the alignment procedure as the number of parameters is smaller than that in previous works. The sample efficiency is confirmed in the experiments on CIFAR-10, CIFAR-100 and ImageNet with various pruning techniques. In particular, FSKD outperforms the FitNet and fine-tuning by non-trivial margins if only small amount of samples are provided (e.g. 100). Here are some comments: 1. What exactly does \u201cabsorb\u201d mean? Is it formally defined in the paper? 2. \u201cwe do not optimize this loss all together using SGD due to that too much hyper-parameters need tuning in SGD\u201d. I don\u2019t understand (1) why does SGD require \u201ctoo much\u201d hyper-parameters tuning and (2) if not SGD, what algorithm do you use? 3. According to the illustration in 3.3, the algorithm looks like a coordinate decent that optimizing L over one Q_j at a time, with the rest fixed. However, the sentence \u201cuntil we reach the last block in the student-net\u201d means the algorithm only runs one iteration, which I suspect might not be sufficient to converge. 4. It is also confusing to use the notation SGD+FSKD v.s. FitNet+FSKD, as it seems SGD and FitNet are referring to the same type of terminology. However, SGD is an algorithm, while FitNet is an approach for neural network distillation. 5. While I understand the training of student network with FSKD should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient? 6. I assume the Top-1 accuracies of teacher networks in Figure 4 are the same as table 2 and 3, i.e. 93.38% and 72.08% for CIFAR-10 and CIFAR-100 respectively. Then the student networks have much worse performance (~85% for CIFAR-10 and ~48% for CIFAR-100) than the teachers. So does it mean FSKD is not good for redesigned student networks? 7. While most of the experiments are on CIFAR10 and CIFAR100, the abstract and conclusion only mention the results of ImageNet. Why?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the valuable comments and suggestions . We give detailed responses to each item below . 1.Meaning of \u201c absorb \u201d We add a 1x1 conv-layer Q \\in R^ { n_o \u2019 * n_o * 1 * 1 } to student-net after the conv-layer W\\in R^ { n_o * n_i * k * k } before non-linear layer , \u201c absorb \u201d here means Q can be merged into W to obtain a new conv-layer W \u2019 \\in R^ { n_o \u2019 * n_i * k * k } . If Q is squared ( n_o \u2019 =n_o ) , then W \u2019 \\in R^ { n_o * n_i * k * k } has the same size as W. Previously we put this information at appendix-A , and now we revise the description of Theorem-1 to include the information . 2 & 3 : why not SGD , why use one-step block coordinate descent ? Yes , what we use is in fact one-step block coordinate descent ( BCD ) algorithm . We add a description of our BCD algorithm in the appendix-B , also include experiment comparison to FSKD-SGD ( total loss optimization with SGD on all added 1x1 convs \u2019 parameters together ) and FSKD-BCD in the experiments on filter-pruning . The experiments show that FSKD-BCD clearly outperforms FSKD-SGD in all cases . The advantages of the BCD algorithm are also listed in the revision . One major reason is that BCD each time handles few parameters ( one block ) which can be solved with limited samples , while SGD always takes all added 1x1 convs \u2019 parameters into consideration , thus requires more data in the optimization . Our experiments do not show benefit from more iterations of BCD . This may be due to the fact that the added 1x1 conv-layer is before non-linear activations so that one-step linear estimation is accurate enough to get exact minimization . Hong et al [ 1 ] show BCD can reach sublinear convergence when each block is exactly minimized , which is consistent with our experiments . We also add Fig3b to illustrate the accuracy improvement along with block alignment sequentially . [ 1 ] Hong , Mingyi , et al . `` Iteration complexity analysis of block coordinate descent methods . '' Mathematical Programming 163.1-2 ( 2017 ) : 85-114 . 4.Confusing on SGD+FSKD and FitNet+FSKD We denote SGD as optimization without using teacher-net info . For the zero-net experiment , SGD+FSKD first uses SGD to initialize the student-net on few samples without using teacher-net info , then uses FSKD to further improve the performance with teacher-net info . While the Fitnet+FSKD first uses FitNet to initialize the student-net on few-samples with teacher-net guidance , then uses FSKD to further improve the performance with teacher-net info from our FSKD perspective . We clarify this in our revision . 5.Why FSKD is sample efficient ? As is known , fewer parameters tend to require fewer samples for estimation . The BCD algorithm considers each block separately , thus there are much fewer parameters in each block , so that we could use much fewer samples for the block-level estimation . Our experiments also verify this point when comparing FSKD-BCD to FSKD-SGD in Figure-2 , especially when samples < 100 . 6.Not good on redesigned student-net ? Yes , in Figure-4 , the student-net accuracy is about 83 % on CIFAR-10 and about 47 % on CIFAR-100 . We should emphasize that this result is obtained with a very limited number of training samples and without data augmentation . If data augmentation is enabled , about 5 % accuracy improvement could be achieved with FSKD . We design this experiments to demonstrate the effectiveness of FSKD over FitNet and SGD on the few-sample settings , and we do not compare this result with full-data training . 7.Only mention results on ImageNet in abstract and conclusion . We have revised our abstract and conclusion accordingly , even though the results on ImageNet sounds more significant to us ."}, "1": {"review_id": "HkgDTiCctQ-1", "review_text": "Model distillation can be tricky and in my own experience can take a lot of samples (albeit unlabeled, so cheaper and more readily available), as well as time to train. This simple trick seems to be doing quite well at training students quickly with few samples. However, it departs from most student-teacher training that find its primary purpose by actually outperforming students trained from scratch (on the full dataset without time constraints). This trick does not outperform this baseline, so its emphasis is entirely on quick and cheap. However, it's unclear to me how often that is actually necessary and I don't think the paper makes a compelling case in this regard. I am borderline on this work and could probably be swayed either way. Strengths: - It's a very simple and fast technique. As I will cover in a later bullet point (under weaknesses), the paper does not make it clear why this type of model distillation is that useful (since it doesn't improve the student model over full fine-tuning, unlike most student-teacher work). However, the reason why I do see some potential for this paper is because there might be a use case in quickly being able to adapt a pretrained network. It is very common to start from a pretrained model and then attach a new loss and fine-tune. Under this paradigm, it is harder to make architectural adjustments, since you are starting from a finite set of pretrained models made available by other folks (or accept the cost of re-training one yourself). However, it is unclear how careful one needs to treat the pretrained model if more fine-tuning is going to occur. If for instance you could just remove layers, drop some channels, glue it all together, and then that model would still be reasonable as a pretrained model since the fine-tuning stage could tidy everything up, then this method would not be useful in this situation. - The fact that least squares solvers can be used at each stage, without the need for a final end-to-end fine-tune is interesting. - It is good that the paper demonstrates improvements coupled with three separate compression techniques (Li et al., Liu et al., Guo et al.). - The paper is technically thorough. - It's good that the method is evaluated on different styles of networks (VGG, ResNet, DenseNet). Weaknesses: - Limited application because it only makes the distillation faster and cheaper. The primary goal of student-teacher training in literature is to outperform a student trained from scratch by the wisdom of the teacher. It ties into this notion that networks are grossly over-parameterized, but perhaps that is where the training magic comes from. Student-teacher training acknowledges this and tries to find a way to benefit from the over-parameterized training and still end up with a small model. I think the same motivation is used for work in low-rank decomposition and many other network compression methods. However, in Table 1 the \"full fine-tune\" model is actually the clear winner and presented almost as an upper bound here, so the only benefit this paper presents is quick and cheap model distillation, not better models. Because of this, I think this paper needs to spend more time making a case for why this is so important. - Since this technique doesn't outperform full fine-tuning, the goal of this work is much more focused on pure model compression. This could put emphasis on reducing model size, RAM usage reduction, or FLOPS reduction. The paper focuses on the last one, which is an important one as it correlates fairly well with power (the biggest constraint in most on-device scenarios). However, it would be great if the paper gave a broader comparison with compression technique that may have slightly different focus, such as low-rank decomposition. Size and memory usage could be included as columns in tables like 1, along with a few of these methods. - Does it work for aggressive compression? The paper presents mostly modest reductions (30-50%). I thin even if accuracy takes a hit, it could still work to various degrees. From what I can see, the biggest reduction is in Table 4, but FSKD is used throughout this table, so there is no comparison for aggressive compression with other techniques. - The method requires appropriate blocks to line up. If you completely re-design a network, it is not as straightforward as regular student-teacher training. Even the zero-student method requires the same number of channels at certain block ends and it is unclear from the experiments how robust this is. Actually, a bit more analysis into the zero student would be great. For instance, it's very interesting how you randomly initialize (let's say 3x3) kernels, and then the final kernels are actually just linear combinations of these - so, will they look random or will they look fairly good? What if this was done at the initial layer where we can visualize the filters, will they look smooth or not? Other comments: - A comparison with \"Deep Mutual Learning\" might be relevant (Zhang et al.). I think there are also some papers on gradually adjusting neural network architectures (by adding/dropping layers/channels) that are not addressed but seem relevant. I didn't read this recently, but perhaps \"Gradual DropIn of Layers to Train Very Deep Neural Networks\" could be relevant. There is at least one more like this that I've seen that I can't seem to find now. - It could be more clear in the tables exactly what cited method is. For instance, in Table 1, does \"Fine-tuning\" (without FitNet/FSKD) correspond to the work of Li et al. (2016)? I think this should be made more clear, for instance by including a citation in the table for the correct row. Right now, at a glance, it would seem that these results are only comparing against prior work when it compares to FitNet, but as I read further, I understood that's not the case. - The paper could use a visual aid for explaining pruning/slimming/decoupling. Minor comments: - page 4, \"due to that too much hyper-parameters\" - page 4, \"each of the M term\" -> \"terms\" - page 6, methods like FitNet provides\" -> \"provide\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and suggestions , we are happy to address your concerns . Q : Limited applications We agree that teacher-student framework may make student-net has better accuracy than training the student-net from scratch with classification loss ( like cross-entropy loss ) . Our target is fast knowledge distillation from few samples , which has many potential situations for application : First , on-device learning to compression when the device has resource constraints which require cheap knowledge distillation solutions . Second , when software/hardware vendors want to use knowledge distillation for model compression while the full data is not available due to privacy and confidential issues . Third , there is a strict time budget for model optimization so that full training or fine-tuning is not allowed . Thank you for your suggestions and we will articulate these reasons in the revision . Q : Resource usages in the Tables ? Thanks for your suggestions , we added the resources usages for all our experiments ( Table-1~4 ) . Note the parameter pruned ratio is usually higher than the FLOPs reduced ratio . Q : Aggressive compression case In Table1~3 , previously we only show the FLOPs reduction ratio , which may make people confused that our compression ratio is relatively low . However , when we add the compression ratio , we could find that in some cases , the compression ratio is also very high ( > =85 % ) not just for Table-4 . Nevertheless , we introduce a novel \u201c iteratively pruning and FSKD \u201d procedure similar to that in filter-pruning and network slimming , in Appendix-D . This procedure can produce even larger compression ratio ( 88 % ) , as shown in the experiment on CIFAR-10 . Q : Visualize the filter before and after FSKD for zero student net ? The 3 * 3 kernel is too small to be visualized . However , based on the network decoupling theory , any regular convolution could be decoupled into a sum of depthwise separable convolution blocks , where each block consists of a depthwise conv-layer followed by a pointwise ( 1 * 1 ) conv-layer . The pointwise layer is just a linear combination of channels from the depthwise layer . We then visualize the pointwise conv-layer before SGD , after SGD , after SGD + FSKD for the zero-student case in Appendix-F . The experiment shows FSKD improves the smoothness of pointwise convolution tensors . Q : Cite and comparison with two papers ? Thanks for pointing us to these two papers . We have cited and compared with them in our revision . Q : Cite for fine-tuning ? Yes , the fine-tuning for filter-pruning/network slimming is the same as these two papers did . We have cited them in the revision . Q : Illustrate the FSKD case for filter-pruning and network decoupling We have illustrated these two cases in Figure-5 and Figure-6 in Appendix-C . Please see the revised paper for more details . Q : Why Q should be squared ? Squared Q ensures model compression and connectable to the next block ( output channel number matches to the input channel number of next block ) . We have mentioned this in our revision after Theorem-1 ."}, "2": {"review_id": "HkgDTiCctQ-2", "review_text": "In this paper, an efficient re-training algorithm for neural networks is proposed. The essence is like Hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. The core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. Since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. The proposed method named FKSD is simple yet achieves good performance. Also, it performs well with a few samples, which is desirable in terms of time complexity. The downside of this paper is that there is no clear explanation of why the FKSD method goes well. For me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. Of course, learning 1x1 conv is easier than learning original conv because of a few parameters. However, it also restricts the representation power so we cannot say which one is always better. Do you have any hypothesis of why 1x1 conv works so well? Minor: The operator * in (1) is undefined. What does the boldface in tables of the experiments mean? I was confused because, in Table 1, the accuracy achieved by FKSD is in bold but is not the highest one. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and suggestions , we provide our response as follows . Q : Learning 1 * 1-conv restrict the representation power , any hypothesis why the added 1x1 works so well ? As the added 1 * 1 conv-layer is finally absorbed into the decoupled 1 * 1 conv-layer , we here hypothesize that pointwise ( 1 * 1 ) convolution is more critical for performance than the depthwise convolution since it occupies > =80 % parameters of the decoupled network . We design an experiment in Appendix-E to verify this hypothesis by comparing the full training to only training the 1 * 1 conv-layer with 3 * 3 initialized to be orthogonal from random data , on CIFAR-10/100 with VGG16 and ResNet50 . The results show that full training works noticeably worse than our designed case . This interesting result verifies our hypothesis , and may inspire more researches to further understand CNN training optimization . Q : * in Eq-1/2 is undefined . Thanks for pointing out this problem . We have added the definition in the revision . Q : Confused bold-face in the table . Thanks for point out this problem . The boldface just wants to show the best results by FSKD , which may be not the best for all the cases . We remove the bold-face in the table in our revision ."}}