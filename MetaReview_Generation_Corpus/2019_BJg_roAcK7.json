{"year": "2019", "forum": "BJg_roAcK7", "title": "INVASE: Instance-wise Variable Selection using Neural Networks", "decision": "Accept (Poster)", "meta_review": "This manuscript proposes a new algorithm for instance-wise feature selection. To this end, the selection is achieved by combining three neural networks trained via an actor-critic methodology. The manuscript highlight that beyond prior work, this strategy enables the selection of a different number of features for each example. Encouraging results are provided on simulated data in comparison to related work, and on real data.\n\nThe reviewers and AC note issues with the evaluation of the proposed method. In particular, the evaluation of computer vision and natural language processing datasets may have further highlighted the performance of the proposed method. Further, while technically innovative, the approach is closely related to prior work (L2X) -- limiting the novelty. \n\nThe paper presents a promising new algorithm for training generative adversarial networks. The mathematical foundation for the method is novel and thoroughly motivated, the theoretical results are non-trivial and correct, and the experimental evaluation shows a substantial improvement over the state of the art.", "reviews": [{"review_id": "BJg_roAcK7-0", "review_text": "This paper proposes an instance-wise feature selection method, which chooses relevant features for each individual sample. The basic idea is to minimize the KL divergence between the distribution p(Y|X) and p(Y|X^{(s)}). The authors consider the classification problem and construct three frameworks: 1) a selector network to calculate the selection probability of each feature; 2) a baseline network for classification on all features; 3) a predictor network for classification on selected features. The goal is to minimize the difference between the baseline loss and predictor loss. The motivation of the paper is clear and the presentation is easy to follow. However, I have some questions on the model and experiments: 1. How is Eq. (5) formulated? As the selector network does not impact the baseline network, an intuition regarding Eq. (5) is to maximize the predictor loss, which seems not reasonable. It seems more appropriate to use an absolute value of the difference in Eq. (5). Some explanation for the formulation of Eq. (5) would be helpful. 2. The model introduces an extra hyper-parameter, $\\lambda$, to adjust the sparsity of selected features. I was curious how sensitive is the performance w.r.t. this hyper-parameter. How is $\\lambda$ determined in the experiments? 3. After the selector network is constructed, how are the features selected on testing data? Is the selection conducted by sampling from the Bernoulli distribution as in training or by directly cutting off the features with lower probabilities? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the insightful comments . A1 : Equation ( 5 ) is the difference between the cross-entropies of the predictor and baseline networks . The first term ( -sum_y log f_i^\\phi ( x^ ( s ) , s ) ) is the cross-entropy of the predictor network and the second term ( -sum y log f_i^\\gamma ( x ) ) is the cross-entropy of the baseline network . The loss in equation ( 5 ) is defined as the \u201c first term \u2013 second term \u201d . The selector network is trained to minimize this , not maximize it . Note that the baseline network is introduced to reduce the variance of this quantity , and not as a term that the selector network can change ( this is a standard technique used in the actor-critic literature ) . Also note that if the baseline network term ( the second term ) in equation ( 5 ) is removed , then we simply end up with the predictor loss defined in the \u201c Predictor Network \u201d section ( l_1 ) . If instead we were to use absolute value , then when the baseline network loss is larger than the predictor network loss , the method would actually be trying to maximise the predictor network loss ( which we do not want ) . It is important to note that we are not trying to minimize the difference between the predictor and baseline losses - we are using the baseline to reduce the variance of the overall loss and we are simply trying to minimize the predictor loss . A2 : As can be seen in page 13 ( subsection \u201c Details of INVASE \u201d ) , we explain that \u201c We use cross-validation to select lambda among { 0.1,0.3,0.5,1,2,5,10 } \u201d . We select the lambda which maximizes the predictor accuracy in terms of AUROC . We will clarify this in the revised manuscript . Below , we give the results for various values of lambda in the Syn4 , Syn5 , and Syn6 settings . More detailed results will be added to the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Datasets | Syn4 | Syn5 | Syn6 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Lambda / Metrics ( % ) | TPR | FDR | TPR | FDR | TPR | FDR | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.1 | 98.0 | 94.3 | 90.0 | 93.4 | 99.2 | 92.3 | 0.3 ( see line 225 in INVASE-.py and line 274 in INVASE.py ) We will clarify this in the revised manuscript ."}, {"review_id": "BJg_roAcK7-1", "review_text": "This paper proposes a new instance-wise feature selection method, INVASE. It is closely related to the prior work L2X (Learning to Explain). There are three differences compared to L2X. The most important difference is about how to backpropagate through subset sampling to select features. L2X use the Gumbel-softmax trick and this paper uses actor-critic models. The paper is written well. It is easy to follow the paper. The contribution of this paper is that it provides a new way, compared to L2X, to backpropagate through subset sampling in order to select features. The authors compare INVASE with L2X and several other approaches on synthetic data and show outperforming results. In the real-world experiments, the authors do not compare INVASE with other approaches. Regarding experiments, instance-wise feature selection is often applied on computer vision or natural language process applications, where global feature selection is not enough. This paper lacks experiments on CV or NLP applications. For the MAGGIC dataset, I expect to see subgroup patterns. The patterns that authors show in Figure 2 are very different for all randomly selected 20 patients. The authors do not explain why it is preferred to see very different feature patterns for all patients instead of subgroup patterns. I have questions about other two differences from L2X, pointed by the authors. First, the selector function outputs a probability for selecting each feature \\hat{S}^\\theta(x). In the paper of L2X, it also produces a weight vector w_\\theta(x) as described in section 3.4. I think the \\hat{S}^\\theta(x) has similar meaning as w_\\theta(x) in L2X. In the synthetic data experiment, the authors fix the number of selected features for L2X so that it forces to overselect or underselect features in the example of Syn4. Did the author try to relax this constraint for L2X and use w_\\theta(x) in L2X to select features as using \\hat{S}^\\theta(x) in INVASE? Second, I agree with the authors that L2X is inspired by maximizing mutual information between Y and X_S and INVASE is inspired by minimizing KL divergence between Y|X and Y|X_S. Both intuitions lead to similar objective functions that INVASE has an extra term \\log p(y|x) and \\lambda ||S(x)||. INVASE is able to add a l_0 penalty on S(x) since it uses the actor-critic models. For the \\log p(y|x) term, as the author mentioned, it helps to reduce the variance in actor-critic models. This \\log p(y|x) term is a constant in the optimization of S(x). In Algorithm 1, 12, the updates of \\gamma does not depend on other parameters related to the predictor network and selector network. Could the authors first train a baseline network and use it as a fixed function in Algorithm 1? I don't understand the meaning of updates for \\gamma iteratively with other parameters since it does not depend on the learning of other parameters. Does this constant term \\log p(y|x) have other benefits besides reducing variance in actor-critic models? I have another minor question about scaling. How does the scaling of X affect the feature importance learned by INVASE? Note: I have another concern about the experiments. Previous instance-wise variable selection methods are often tested on CV or NLP applications, could the authors present those experiments as previous works?", "rating": "6: Marginally above acceptance threshold", "reply_text": "A4 : The main problem in doing this for L2X is in the training stage ( not the testing stage ) . As can be seen in the equations to compute V values ( page 4 end of the left column of L2X paper ) , they must provide some k to train with which is , in general , unknown in real-world datasets ( because we don \u2019 t know how many features are relevant in the real-world datasets ) . The weights w ( X ) are optimized according to a specific feature selection strategy during training , using them in a different strategy during testing would not make sense , as they are no longer optimized for this strategy . While intuitively possible , consider that , due to the way they \u2019 ve been trained , the weights w ( X ) are expected to \u201c spit out \u201d k features . Because of this , it might be that the weights for the unselected k features are essentially random ( but lower than the selected k features ) . We have no reason to believe that the weights beyond the selected k features would be meaningful ( since during training the method only ever selected precisely k features ) . We have , however , conducted an experiment in the Syn4 and Syn5 settings with 100 featurs in which we directly use w ( X ) and threshold it to select features . As can be seen below , the results are significantly worse than for INVASE and the large increase in FDR is indeed consistent with the fact that the weights beyond the top k are not well-disciplined . We will clarify this in the revised manuscript . Note that the published code of the L2X paper is also forced to select k features in both the training and testing stages . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Datasets | Syn4 | Syn5 | Thresholds | TPR | FDR | TPR | FDR | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- L2X | 0.1 | 87.4 | 93.5 | 79.5 | 95.3 | L2X | 0.3 | 69.9 | 83.8 | 77.2 | 77.1 | L2X | 0.5 | 69.8 | 64.1 | 66.4 | 84.6 | L2X | 0.7 | 59.1 | 61.2 | 54.4 | 65.7 | L2X | 0.9 | 52.7 | 44.8 | 51.2 | 50.5 | INVASE | 66.3 | 40.5 | 73.2 | 23.7 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- A5 : The baseline network does not have to be trained iteratively with the other networks , however in actor-critic models it typically is . This is because the baseline is used in some sense to \u201c normalize \u201d the predictor network . For this reason , it is therefore good to have the baseline and predictor at a similar \u201c level of convergence \u201d . However , the performance differences are marginal between the two methods , and so we found that it was not important which training method we used . A6 : The scaling of X is not important . At no point do we multiply the feature vector ( X ) by the \u201c importance weights \u201d . The weights are used to obtain a binary mask vector which is then multiplied ( element-wise ) with the feature vector . As such , the unselected features end up being 0 and the selected features retain their original value ."}, {"review_id": "BJg_roAcK7-2", "review_text": "In the paper, the authors proposed a new algorithm for instance-wise feature selection. In the proposed algorithm, we prepare three DNNs, which are predictor network, baseline network, and selector network. The predictor network and the baseline networks are trained so that it fits the data well, where the predictor network uses only selected features sampled from the selector network. The selector network is trained to minimize the KL-divergence between the predictor network and the baseline network. In this way, one can train the selector network that select different feature sets for each of given instances. I think the idea is quite simple: the use of three DNNs and the proposed loss functions seem to be reasonable. The experimental results also look promising. I have a concern on the scheduling of training. Too fast training of the predictor network can lead to the subotpimal selection network. I have checked the implementations in github, and found that all the networks used Adam with the same learning rates. Is there any issue of training instability? And, if so, how we can confirm that good selector network has trained? My another concern is on the implementations in github. The repository originally had INVASE.py. In the middle of the reviewing period, I found that INVASE+.py has added. I am not sure which implementations is used for this manuscript. It seems that INVASE.py contains only two networks, while INVASE+.py contains three networks. I therefore think the latter is the implementation used for this manuscript. If this is the case, what INVASE.py is for? I am also not sure if it is appropriate to \"communicate\" through external repositories during the reviewing period.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the insightful comments . A1 : It is not true that fast training of the predictor network can lead to a suboptimal selector network . Even when the predictor network is fully trained after each selector network update , the selector network can converge optimally . However , because the input distribution of the predictor network changes with each update of the selector network , the predictor network will have to update after each selector update . It is therefore not possible for the predictor network to converge until after the selector network has converged . Therefore , there are no stability issues caused by using the same learning rates for each network . A2 : INVASE+.py is the code corresponding to the implementation found in this paper . INVASE.py corresponds to the same implementation but without the baseline ( i.e.just the selector and predictor networks ) . In practice we found both to perform similarly , but the derivation of INVASE+ is a little more natural , and as such we used it for the paper . We have since changed the names in the repository to INVASE and INVASE- ( so that now INVASE is indeed the implemented method and INVASE- is the method without the baseline ) . We hope this alleviates the confusion ."}], "0": {"review_id": "BJg_roAcK7-0", "review_text": "This paper proposes an instance-wise feature selection method, which chooses relevant features for each individual sample. The basic idea is to minimize the KL divergence between the distribution p(Y|X) and p(Y|X^{(s)}). The authors consider the classification problem and construct three frameworks: 1) a selector network to calculate the selection probability of each feature; 2) a baseline network for classification on all features; 3) a predictor network for classification on selected features. The goal is to minimize the difference between the baseline loss and predictor loss. The motivation of the paper is clear and the presentation is easy to follow. However, I have some questions on the model and experiments: 1. How is Eq. (5) formulated? As the selector network does not impact the baseline network, an intuition regarding Eq. (5) is to maximize the predictor loss, which seems not reasonable. It seems more appropriate to use an absolute value of the difference in Eq. (5). Some explanation for the formulation of Eq. (5) would be helpful. 2. The model introduces an extra hyper-parameter, $\\lambda$, to adjust the sparsity of selected features. I was curious how sensitive is the performance w.r.t. this hyper-parameter. How is $\\lambda$ determined in the experiments? 3. After the selector network is constructed, how are the features selected on testing data? Is the selection conducted by sampling from the Bernoulli distribution as in training or by directly cutting off the features with lower probabilities? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the insightful comments . A1 : Equation ( 5 ) is the difference between the cross-entropies of the predictor and baseline networks . The first term ( -sum_y log f_i^\\phi ( x^ ( s ) , s ) ) is the cross-entropy of the predictor network and the second term ( -sum y log f_i^\\gamma ( x ) ) is the cross-entropy of the baseline network . The loss in equation ( 5 ) is defined as the \u201c first term \u2013 second term \u201d . The selector network is trained to minimize this , not maximize it . Note that the baseline network is introduced to reduce the variance of this quantity , and not as a term that the selector network can change ( this is a standard technique used in the actor-critic literature ) . Also note that if the baseline network term ( the second term ) in equation ( 5 ) is removed , then we simply end up with the predictor loss defined in the \u201c Predictor Network \u201d section ( l_1 ) . If instead we were to use absolute value , then when the baseline network loss is larger than the predictor network loss , the method would actually be trying to maximise the predictor network loss ( which we do not want ) . It is important to note that we are not trying to minimize the difference between the predictor and baseline losses - we are using the baseline to reduce the variance of the overall loss and we are simply trying to minimize the predictor loss . A2 : As can be seen in page 13 ( subsection \u201c Details of INVASE \u201d ) , we explain that \u201c We use cross-validation to select lambda among { 0.1,0.3,0.5,1,2,5,10 } \u201d . We select the lambda which maximizes the predictor accuracy in terms of AUROC . We will clarify this in the revised manuscript . Below , we give the results for various values of lambda in the Syn4 , Syn5 , and Syn6 settings . More detailed results will be added to the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Datasets | Syn4 | Syn5 | Syn6 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Lambda / Metrics ( % ) | TPR | FDR | TPR | FDR | TPR | FDR | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.1 | 98.0 | 94.3 | 90.0 | 93.4 | 99.2 | 92.3 | 0.3 ( see line 225 in INVASE-.py and line 274 in INVASE.py ) We will clarify this in the revised manuscript ."}, "1": {"review_id": "BJg_roAcK7-1", "review_text": "This paper proposes a new instance-wise feature selection method, INVASE. It is closely related to the prior work L2X (Learning to Explain). There are three differences compared to L2X. The most important difference is about how to backpropagate through subset sampling to select features. L2X use the Gumbel-softmax trick and this paper uses actor-critic models. The paper is written well. It is easy to follow the paper. The contribution of this paper is that it provides a new way, compared to L2X, to backpropagate through subset sampling in order to select features. The authors compare INVASE with L2X and several other approaches on synthetic data and show outperforming results. In the real-world experiments, the authors do not compare INVASE with other approaches. Regarding experiments, instance-wise feature selection is often applied on computer vision or natural language process applications, where global feature selection is not enough. This paper lacks experiments on CV or NLP applications. For the MAGGIC dataset, I expect to see subgroup patterns. The patterns that authors show in Figure 2 are very different for all randomly selected 20 patients. The authors do not explain why it is preferred to see very different feature patterns for all patients instead of subgroup patterns. I have questions about other two differences from L2X, pointed by the authors. First, the selector function outputs a probability for selecting each feature \\hat{S}^\\theta(x). In the paper of L2X, it also produces a weight vector w_\\theta(x) as described in section 3.4. I think the \\hat{S}^\\theta(x) has similar meaning as w_\\theta(x) in L2X. In the synthetic data experiment, the authors fix the number of selected features for L2X so that it forces to overselect or underselect features in the example of Syn4. Did the author try to relax this constraint for L2X and use w_\\theta(x) in L2X to select features as using \\hat{S}^\\theta(x) in INVASE? Second, I agree with the authors that L2X is inspired by maximizing mutual information between Y and X_S and INVASE is inspired by minimizing KL divergence between Y|X and Y|X_S. Both intuitions lead to similar objective functions that INVASE has an extra term \\log p(y|x) and \\lambda ||S(x)||. INVASE is able to add a l_0 penalty on S(x) since it uses the actor-critic models. For the \\log p(y|x) term, as the author mentioned, it helps to reduce the variance in actor-critic models. This \\log p(y|x) term is a constant in the optimization of S(x). In Algorithm 1, 12, the updates of \\gamma does not depend on other parameters related to the predictor network and selector network. Could the authors first train a baseline network and use it as a fixed function in Algorithm 1? I don't understand the meaning of updates for \\gamma iteratively with other parameters since it does not depend on the learning of other parameters. Does this constant term \\log p(y|x) have other benefits besides reducing variance in actor-critic models? I have another minor question about scaling. How does the scaling of X affect the feature importance learned by INVASE? Note: I have another concern about the experiments. Previous instance-wise variable selection methods are often tested on CV or NLP applications, could the authors present those experiments as previous works?", "rating": "6: Marginally above acceptance threshold", "reply_text": "A4 : The main problem in doing this for L2X is in the training stage ( not the testing stage ) . As can be seen in the equations to compute V values ( page 4 end of the left column of L2X paper ) , they must provide some k to train with which is , in general , unknown in real-world datasets ( because we don \u2019 t know how many features are relevant in the real-world datasets ) . The weights w ( X ) are optimized according to a specific feature selection strategy during training , using them in a different strategy during testing would not make sense , as they are no longer optimized for this strategy . While intuitively possible , consider that , due to the way they \u2019 ve been trained , the weights w ( X ) are expected to \u201c spit out \u201d k features . Because of this , it might be that the weights for the unselected k features are essentially random ( but lower than the selected k features ) . We have no reason to believe that the weights beyond the selected k features would be meaningful ( since during training the method only ever selected precisely k features ) . We have , however , conducted an experiment in the Syn4 and Syn5 settings with 100 featurs in which we directly use w ( X ) and threshold it to select features . As can be seen below , the results are significantly worse than for INVASE and the large increase in FDR is indeed consistent with the fact that the weights beyond the top k are not well-disciplined . We will clarify this in the revised manuscript . Note that the published code of the L2X paper is also forced to select k features in both the training and testing stages . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Datasets | Syn4 | Syn5 | Thresholds | TPR | FDR | TPR | FDR | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- L2X | 0.1 | 87.4 | 93.5 | 79.5 | 95.3 | L2X | 0.3 | 69.9 | 83.8 | 77.2 | 77.1 | L2X | 0.5 | 69.8 | 64.1 | 66.4 | 84.6 | L2X | 0.7 | 59.1 | 61.2 | 54.4 | 65.7 | L2X | 0.9 | 52.7 | 44.8 | 51.2 | 50.5 | INVASE | 66.3 | 40.5 | 73.2 | 23.7 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- A5 : The baseline network does not have to be trained iteratively with the other networks , however in actor-critic models it typically is . This is because the baseline is used in some sense to \u201c normalize \u201d the predictor network . For this reason , it is therefore good to have the baseline and predictor at a similar \u201c level of convergence \u201d . However , the performance differences are marginal between the two methods , and so we found that it was not important which training method we used . A6 : The scaling of X is not important . At no point do we multiply the feature vector ( X ) by the \u201c importance weights \u201d . The weights are used to obtain a binary mask vector which is then multiplied ( element-wise ) with the feature vector . As such , the unselected features end up being 0 and the selected features retain their original value ."}, "2": {"review_id": "BJg_roAcK7-2", "review_text": "In the paper, the authors proposed a new algorithm for instance-wise feature selection. In the proposed algorithm, we prepare three DNNs, which are predictor network, baseline network, and selector network. The predictor network and the baseline networks are trained so that it fits the data well, where the predictor network uses only selected features sampled from the selector network. The selector network is trained to minimize the KL-divergence between the predictor network and the baseline network. In this way, one can train the selector network that select different feature sets for each of given instances. I think the idea is quite simple: the use of three DNNs and the proposed loss functions seem to be reasonable. The experimental results also look promising. I have a concern on the scheduling of training. Too fast training of the predictor network can lead to the subotpimal selection network. I have checked the implementations in github, and found that all the networks used Adam with the same learning rates. Is there any issue of training instability? And, if so, how we can confirm that good selector network has trained? My another concern is on the implementations in github. The repository originally had INVASE.py. In the middle of the reviewing period, I found that INVASE+.py has added. I am not sure which implementations is used for this manuscript. It seems that INVASE.py contains only two networks, while INVASE+.py contains three networks. I therefore think the latter is the implementation used for this manuscript. If this is the case, what INVASE.py is for? I am also not sure if it is appropriate to \"communicate\" through external repositories during the reviewing period.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the insightful comments . A1 : It is not true that fast training of the predictor network can lead to a suboptimal selector network . Even when the predictor network is fully trained after each selector network update , the selector network can converge optimally . However , because the input distribution of the predictor network changes with each update of the selector network , the predictor network will have to update after each selector update . It is therefore not possible for the predictor network to converge until after the selector network has converged . Therefore , there are no stability issues caused by using the same learning rates for each network . A2 : INVASE+.py is the code corresponding to the implementation found in this paper . INVASE.py corresponds to the same implementation but without the baseline ( i.e.just the selector and predictor networks ) . In practice we found both to perform similarly , but the derivation of INVASE+ is a little more natural , and as such we used it for the paper . We have since changed the names in the repository to INVASE and INVASE- ( so that now INVASE is indeed the implemented method and INVASE- is the method without the baseline ) . We hope this alleviates the confusion ."}}