{"year": "2020", "forum": "r1nSxrKPH", "title": "Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks", "decision": "Reject", "meta_review": "The submission proposes a complex, hierarchical architecture for continuous control RL that combines Hindsight Experience Replay, vision-based planning with privileged information, and low-level control policy learning. The authors demonstrate that the approach can achieve transfer of the different control levels between different bodies in a single environment.\n\nThe reviewers were initially all negative, but 2 were persuaded towards weak acceptance by the improvements to the paper and the authors' rebuttal. The discussion focused on remaining limitations: the use of a single maze environment for evaluation, as well as whether the baselines were fair (HAC in particular). After reading the paper, I believe that these limitations are substantial. In particular, this is not a general approach and its relevance is severely limited unless the authors demonstrate that it will work as well in a more general control setting, which is in their future work already. \n\nThus I recommend rejection at this time.", "reviews": [{"review_id": "r1nSxrKPH-0", "review_text": "The paper proposes a neat framework for creating HRL framework that will be able to generalize its application to slightly different environment layout. This is done via an image-based top-down from as input to the high level. An intermediate layer is used to help create more fine-grained goal specification for a final goal-based control layer. These layers are trained together using HAC. Overall, the method shows promise but there needs to be more analysis to understand which parts of this combination of ideas are the most important. The results are also only shown for a single environment. Last, the generalization analysis in the paper does not appear to be overly thorough. It would be good to perform this on more than one type of environment also the random environment is not very random. More detailed comments: - The results seem very similar to some of the work in \"Universal Planning Networks\" that did not need a more complex HRL design to achieve subgoal specification via images. This should be discussed more in the paper. - The authors point out that the use of relative goal positions \"ensures generalization to new environments\", this is a rather strong statement. The use of relative goal specification my help improve generalization but that can only be shown empirically. - The demonstration to show that the method generalizes to other configuration after being trained on a fixed environment should be evaluated over many randomly generated environments so that we have a non-biased estimate of the true generalization performance. In Table 2, it is rather surprising that the HiDe trained model does better on the \"Random\" environment vs the \"FOrward\" environment it is trained on. Can more details be provided on how the \"Random\" environment is created? Are the locations of the walls randomized? Is the initial position of the agent and goal randomize? - The video seems to contradict the ordering of operations for training the planning network. The video suggests that first it is learned with the Ant then transferred to the ball which is less complex to control. - At the beginning of the prior work section, it is noted that many other methods require prior knowledge of the environment I would say this method also requires certain kinds of prior knowledge about the task. For example, a top-down view of the environment is needed which is not often feasible. - HIRO and HAC use a more proprioceptive state space but I don't think the sharing of global states is intentional. I am not convinced that this choice, in particular, is what makes the approaches prone to overfitting. - You show a comparison to the \"windows\" created from your method vs a fixed neighbourhood. Do you perform any empirical evidence that your introduced methods provide an improvement over this fixed window? - It is mentioned at the end of section 4.1 that MVProp is differential so it can be trained with the Bellman error objective. Because many policies are being trained concurrently does the MVProp attention model need to be recomputed after every sub-policy update? Does the frequency of updates have a large effect on performance? - Section 4.2 introduces an interface layer that is not a very common practice. It would be good to include an ablation study of the effects of this introduced layer. - In section 4.3 it says that the control layer is the only layer with access to the agent's proprioceptive state. Would it not be good to at least include the agent facing direction or current average velocity to higher layers to improve the attention mask estimation? - In figure 5 it says HIRO converges the fastest because it has dense rewards. Can you be more specific? Also, If different agents are using different reward signals I am not sure this evaluation is a fair comparison. - Are tables 2 and 3 just for the HiDe algorithm? Is it possible to include data for the other algorithms? - You perform an experiment to train HiDe with random initial and goal locations for comparison. I think running this comparison for HIRO and HAC would be a good additional point of comparison. This would help the reader know if the generalization is not biased to the particular initial environment configuration for Maze Forward. - In the generalization analysis for the paper, how is the analysis performed? There are percentages for the success of the policy, where does the randomness come from is the agent state and goal are always fixed? Are these averaged because the agent has a stochastic policy during evaluation? If this is the case how many random trajectories are collected to compute these statistics? - It would be very helpful to have a description of the algorithm in the paper. How the algorithm works is not very clear and some details about how the goal and states are passed to the different policies would be very helpful if anyone wanted to reimplement this work. Updated comments: - The addition of more results and added analysis helps show the improvement of this method over the most related baselines.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the extensive feedback . We hope we can address most of the reviewer \u2019 s concerns . * * * Comparison to Universal Planning Networks * * * Comment : The results seem very similar to some of the work in `` Universal Planning Networks '' that did not need a more complex HRL design to achieve subgoal specification via images . This should be discussed more in the paper . We have added a more detailed comparison between our work and Universal Planning Networks [ 1 ] to the updated version of the paper . While we agree that UPN shows similar results in navigation domains , we argue that the problem we are solving is different . Our agents can learn solely from a sparse binary reward , which means it only receives 0 if it reaches the final goal and -1 in all other cases . Hence , the agent does not rely on a shaped reward function to facilitate learning or demonstrations as a guidance signal . Contrarily , UPN optimizes a supervised imitation objective , i.e. , uses demonstrations to help agents learn a task . Acquiring the necessary ground-truth data may not always be feasible . * * * Relative Goal Positions * * * Comment : The authors point out that the use of relative goal positions `` ensures generalization to new environments '' , this is a rather strong statement . The use of relative goal specification my help improve generalization but that can only be shown empirically . We rephrase the rather strong statement of \u201c ensures generalization \u201d to \u201c encourages generalization \u201d . To empirically show this , we provide an ablation study comparing absolute against relative positions ( see general comment for the results and discussion ) . * * * Comments about the Random Environment * * * 1 . Comment : Can more details be provided on how the `` Random '' environment is created ? Are the locations of the walls randomized ? Is the initial position of the agent and goal randomize ? 2.Comment : The demonstration to show that the method generalizes to other configuration after being trained on a fixed environment should be evaluated over many randomly generated environments so that we have a non-biased estimate of the true generalization performance . We refer the reviewer to appendix A.2 of the initial submission , where the procedure of creating the environments is described in detail . To summarize , we create 500 randomly generated environments with randomly sampled wall locations , start and goal positions . We then test on the same 500 random environments for all seeds to ensure a fair comparison , so it is an unbiased estimate of the true generalization performance ( addressing comment 2 . ) . 3.Comment : In Table 2 , it is rather surprising that the HiDe trained model does better on the `` Random '' environment vs the `` Forward '' environment it is trained on . The reason why the results are ( sometimes ) better compared to the forward task is because the start and goal positions can essentially be closer together due to the random sampling , while in the forward task the agent always has to move around the whole map to reach the goal . * * * Transfer Experiment Videos * * * Comment : The video seems to contradict the ordering of operations for training the planning network . The video suggests that first it is learned with the Ant then transferred to the ball which is less complex to control . In the video as well as in the results of the initial submission ( cf.Table 3 ) , we provide both the transfer of the planning layer from the ball to the more complex ant agent ( https : //drive.google.com/file/d/1mJhZPmGsr1n-JjSNBn42ExUzVjz41Nek/view ? t=1m45s ) and vice-versa ( https : //drive.google.com/file/d/1mJhZPmGsr1n-JjSNBn42ExUzVjz41Nek/view ? t=1m27s ) . * * * Prior Knowledge * * * Comment : It is noted that many other methods require prior knowledge of the environment I would say this method also requires certain kinds of prior knowledge about the task . For example , a top-down view of the environment is needed which is not often feasible . We agree on this and therefore have removed the part-sentence about prior knowledge from the related work section . Moreover , we now provide results that compare our approach against [ 3 ] , which is follow-up work of HIRO that also uses the top-down view image of the environment ( see general comment for the results and discussion ) ."}, {"review_id": "r1nSxrKPH-1", "review_text": "This paper addresses hierarchical deep reinforcement learning (RL), an important problem in control learning and RL. Based on my understanding of this paper and recent prior work, the most important difference between the proposed approach (HiDe) and other recent approaches, such as HIRO and HAC, is that the top-level goal proposal policy uses a learned planner based on VIN and a learned attention mask to decide on a subgoal. There are also other differences, e.g., this policy outputs a goal position that is relative to the agent's position, rather than an absolute position. HiDe seems to demonstrate impressive transferability to both unseen mazes and new agent embodiments, which are important problems to address for hierarchical RL. Introducing learned planning and attention into the top-level policy seems to also introduce additional assumptions into the method. For example, it is my understanding that HIRO and HAC use the same state representation, e.g., joint positions and velocities of the agent, as input to their top-level policies. In contrast, HiDe uses a top down view of the maze and the x y position of the agent, which certainly is more privileged information. If this is correct, first, this should be discussed more thoroughly and directly in the paper. Second, the experimental setup should be elaborated on: is HIRO or HAC modified to include the same information for the top-level policy? Or can HiDe somehow be extended to not require this information? I do not think that requiring this information is egregious, but currently the experimental comparison is not clear in this regard. The experiments are arguably the strongest part of the paper, and the transfer results and videos are quite nice. But there is still room for improvement. Table 1 and Figure 5 seem disconnected. In particular, the numbers reported in Table 1 are clearly not achieved in Figure 5. Is the figure cut off early? Furthermore, an additional experiment on a more complicated domain would greatly strengthen the paper. A humanoid agent, for example, seems easy to test for the current method. Another option would be the movable blocks tested in HIRO, though it is unclear if this readily fits into the current method's assumptions. In my opinion, as the paper currently rests heavily on the results, this section should be further improved. Doing so would also improve my rating of the paper. ------ Edit after author response: I appreciate the authors' efforts in providing extensive responses to all of the reviewers' concerns as well as a significant general response detailing what seems to be a large amount of additional experimental work. I think that all of this warrants a change to my score, and seeing that the authors have more or less addressed my concerns, I am bumping up to a weak accept.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive feedback . We hope we are able to address all concerns accurately . * * * Comparison to State-of-the-art * * * Comment : HiDe uses a top down view of the maze and the x y position of the agent , which certainly is more privileged information . If this is correct , first , this should be discussed more thoroughly and directly in the paper . Second , the experimental setup should be elaborated on : is HIRO or HAC modified to include the same information for the top-level policy ? Or can HiDe somehow be extended to not require this information ? HiDe has indeed access to further information as it receives a top-down image , while HAC [ 1 ] and HIRO [ 2 ] do not get such information . We mention this more explicitly in the updated version of the paper . Moreover , we now provide results that compare our approach against [ 3 ] , which is follow-up work of HIRO that uses a simplified top-down view image as well ( see general comment for the results and discussion ) . * * * Experiment Section Improvements * * * 1 . Comment : Table 1 and Figure 5 seem disconnected . In particular , the numbers reported in Table 1 are clearly not achieved in Figure 5 . Is the figure cut off early ? Table 1 and Figure 5 may seem disconnected . For the evaluation shown in Table 1 , we selected the checkpoint for each seed where the validation success rate during training was highest . Since these checkpoints are taken at different timestamps , the averaged score of Figure 5 may yield the impression of not achieving these scores . We have added a clearer description of this process in the revised version of the paper and apologize for any possible misunderstandings . 2.Comment : Furthermore , an additional experiment on a more complicated domain would greatly strengthen the paper . A humanoid agent , for example , seems easy to test for the current method . We have added a humanoid agent as a proof of concept . We thereby highlight that our method allows training the planning layer with a simplistic agent such as a ball and transferring it to a very complex agent such as the humanoid ( contribution 3 ) . 3.Comment : In my opinion , as the paper currently rests heavily on the results , this section should be further improved . We try to address this by adding further ablation studies to our experiment sections . More specifically , we compare relative and absolute positions , fixed and learned window sizes for the planning layer , and HiDe with and without interface layer . * * * References * * * [ 1 ] Andrew Levy , Robert Platt , and Kate Saenko . Learning Multi-Level Hierarchies with Hindsight . InInternational Conference on Learning Representations , 2019 . [ 2 ] Ofir Nachum , Shixiang Shane Gu , Honglak Lee , and Sergey Levine . Data-efficient Hierarchical Reinforcement Learning . In Advances in Neural Information Processing Systems , pp . 3303\u20133313,2018 . [ 3 ] Ofir Nachum , Shixiang Gu , Honglak Lee , and Sergey Levine . Near-Optimal Representation Learning for Hierarchical Reinforcement Learning . InInternational Conference on Learning Representations , 2019"}, {"review_id": "r1nSxrKPH-2", "review_text": "The submission proposes a novel method for explicit decomposition of hierarchical policies for long-horizon navigation tasks. The approach proposes to separate a policy into 3 modules, high-level planner, intermediate planner and low-level control. The evaluation shows that explicit decomposition is well suited for generalisation across a limited set of RL domains. The proposed method integrates aspects from a variety of recent work including planning layers from value propagation networks, hindsight training paradigms from hierarchical actor critic and hindsight experience replay and related techniques. The variety of different techniques combined instead of a single main contribution renders it challenging to follow all aspects and in particular to trace relevant contributions to performance - which is rendered harder by a limited evaluation section. While the approach shows good performance against a couple of start of the art methods, it is necessary to provide sufficient ablations to enable long-term insights for the community. The submission high level goal (explicit decomposition and information asymmetry) is clear, the execution involves the combination of many existing techniques plus variations such that it is hard to make solid statements about the relevance of any part. It is commendable that the authors have introduced adaptations and improvements to their baselines for a stronger and fairer comparison but the evaluation remains very limited. I suggest to run different domains as given by other domains from OpenAI gym or DeepMind control suite. But more importantly I suggest to run further ablations without the intermediate planning layer & with absolute goal positions. Furthermore, since HAC seems to perform worse when combined with the proposed low and mid level policies (RelHAC), it would make sense to compare to the proposed high-level policy using low and mid level from HAC instead. The submission provides an overall interesting perspective but makes it hard to narrow down on contribution and important insights by being unclear in formulation and providing only very limited ablations. Minor issues include: - Missing description of the mid level policy - what encourages the proposal of closer short-term goals. - Missing literature on information asymmetry in RL (e.g. see Tirumala et al 2019 \u2018Exploiting Hierarchy for Learning and Transfer in KL-regularized RL\u2019) - Unclear description of how models that get attached to existing planning layers have been trained (Sec 5.3). - Additional unclear description in the description of the experiments and method sections (4.2, 4.3)", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the constructive comments . We have tried to address the reviewer \u2019 s concerns about insufficient ablations and clarifications about our contributions . * * * Ablation Studies * * * Comment : I suggest to run further ablations without the intermediate planning layer & with absolute goal positions 1 . We conduct an ablation study to empirically show that the mid-layer is a necessary part of the architecture ( see general comment for the results and discussion ) . 2.We provide an ablation study with absolute against relative positions , showing that relative positions are an important aspect of our method in order to both solving the task and achieving generalization to other environments ( see general comment for the results and discussion ) . * * * Additional Domain Experiment * * * Comment : I suggest to run different domains as given by other domains from OpenAI gym . We have added a humanoid agent as an additional domain for a proof of concept of our method . We thereby highlight that our method allows training the planning layer with a simplistic agent such as a ball and transferring it to a very complex agent such as the humanoid . * * * Combining our Planning Layer with original HAC * * * Comment : Since HAC seems to perform worse when combined with the proposed low and mid level policies ( RelHAC ) , it would make sense to compare to the proposed high-level policy using low and mid level from HAC instead . The low and mid level from original HAC can not be directly combined with our planning layer , since the goal space in HAC is equal to the state space , such that the subgoals given to layers below consist not only of position goals , but also goals for the proprioceptive state of the agent , such as the joint angles and joint velocities . Contrarily , our planner is decoupled from such a task and therefore only learns to give position goals to the mid level . * * * Minor Comments * * * 1 . Comment : Missing description of the mid level policy - what encourages the proposal of closer short-term goals . The interface layer is motivated by results from previous work [ 1 ] . Furthermore , it is also claimed by [ 2 ] that the use of more layers leads to better exploration . To empirically show that the mid-layer is a necessary part in the architecture , we ran an ablation study ( see general comment for the results and discussion ) . 2.Comment : Missing literature on information asymmetry in RL ( e.g.see Tirumala et al 2019 \u2018 Exploiting Hierarchy for Learning and Transfer in KL-regularized RL \u2019 ) We have added the missing literature [ 3 ] to the revised version of the paper . 3.Comment : Unclear description of how models that get attached to existing planning layers have been trained ( Sec 5.3 ) . We have updated and clarified the description of how the transferred models get trained . More specifically , we evaluate the functional decomposition of HiDe by testing the compatibility of layers trained by different agents . We trained HiDe with either the Ball or the Ant agent . Then , we use the planning layer from such an agent and transfer it onto another agent that was trained using RelHAC , i.e. , HiDe without our proposed planner on the top layer . The planning layer agent is trained in the more complex environment of experiment 2 and the second agent is trained in the easier environment from experiment 1 . Despite being trained in different environments and with different agents , the planner is transferable . Moreover , our estimate indicates that training the planner with a Ball and then transferring it to a more complex agent is as much 3 to 4 times faster than training HiDe with the Ant or Humanoid from scratch . 4.Comment : Additional unclear description in the description of the experiments and method sections ( 4.2 , 4.3 ) Could the reviewer be a bit more specific about what we can clarify/improve in Sections 4.2 and 4.3 ? We will gladly apply such changes . * * * References * * * [ 1 ] Andrew Levy , Robert Platt , and Kate Saenko . Learning Multi-Level Hierarchies with Hindsight . InInternational Conference on Learning Representations , 2019 . [ 2 ] Anonymous Authors , Why Does Hierarchy ( Sometimes ) Work So Well in Reinforcement Learning ? , under double blind review for ICLR 2020 , https : //openreview.net/forum ? id=rJgSk04tDH [ 3 ] Dhruva Tirumala , Hyeonwoo Noh , Alexandre Galashov , Leonard Hasenclever , Arun Ahuja , Greg Wayne , Razvan Pascanu , Yee Whye Teh , and Nicolas Heess . Exploiting Hierarchy For Learning and Transfer in KL-regularized RL . CoRR , abs/1903.07438 , 2019"}], "0": {"review_id": "r1nSxrKPH-0", "review_text": "The paper proposes a neat framework for creating HRL framework that will be able to generalize its application to slightly different environment layout. This is done via an image-based top-down from as input to the high level. An intermediate layer is used to help create more fine-grained goal specification for a final goal-based control layer. These layers are trained together using HAC. Overall, the method shows promise but there needs to be more analysis to understand which parts of this combination of ideas are the most important. The results are also only shown for a single environment. Last, the generalization analysis in the paper does not appear to be overly thorough. It would be good to perform this on more than one type of environment also the random environment is not very random. More detailed comments: - The results seem very similar to some of the work in \"Universal Planning Networks\" that did not need a more complex HRL design to achieve subgoal specification via images. This should be discussed more in the paper. - The authors point out that the use of relative goal positions \"ensures generalization to new environments\", this is a rather strong statement. The use of relative goal specification my help improve generalization but that can only be shown empirically. - The demonstration to show that the method generalizes to other configuration after being trained on a fixed environment should be evaluated over many randomly generated environments so that we have a non-biased estimate of the true generalization performance. In Table 2, it is rather surprising that the HiDe trained model does better on the \"Random\" environment vs the \"FOrward\" environment it is trained on. Can more details be provided on how the \"Random\" environment is created? Are the locations of the walls randomized? Is the initial position of the agent and goal randomize? - The video seems to contradict the ordering of operations for training the planning network. The video suggests that first it is learned with the Ant then transferred to the ball which is less complex to control. - At the beginning of the prior work section, it is noted that many other methods require prior knowledge of the environment I would say this method also requires certain kinds of prior knowledge about the task. For example, a top-down view of the environment is needed which is not often feasible. - HIRO and HAC use a more proprioceptive state space but I don't think the sharing of global states is intentional. I am not convinced that this choice, in particular, is what makes the approaches prone to overfitting. - You show a comparison to the \"windows\" created from your method vs a fixed neighbourhood. Do you perform any empirical evidence that your introduced methods provide an improvement over this fixed window? - It is mentioned at the end of section 4.1 that MVProp is differential so it can be trained with the Bellman error objective. Because many policies are being trained concurrently does the MVProp attention model need to be recomputed after every sub-policy update? Does the frequency of updates have a large effect on performance? - Section 4.2 introduces an interface layer that is not a very common practice. It would be good to include an ablation study of the effects of this introduced layer. - In section 4.3 it says that the control layer is the only layer with access to the agent's proprioceptive state. Would it not be good to at least include the agent facing direction or current average velocity to higher layers to improve the attention mask estimation? - In figure 5 it says HIRO converges the fastest because it has dense rewards. Can you be more specific? Also, If different agents are using different reward signals I am not sure this evaluation is a fair comparison. - Are tables 2 and 3 just for the HiDe algorithm? Is it possible to include data for the other algorithms? - You perform an experiment to train HiDe with random initial and goal locations for comparison. I think running this comparison for HIRO and HAC would be a good additional point of comparison. This would help the reader know if the generalization is not biased to the particular initial environment configuration for Maze Forward. - In the generalization analysis for the paper, how is the analysis performed? There are percentages for the success of the policy, where does the randomness come from is the agent state and goal are always fixed? Are these averaged because the agent has a stochastic policy during evaluation? If this is the case how many random trajectories are collected to compute these statistics? - It would be very helpful to have a description of the algorithm in the paper. How the algorithm works is not very clear and some details about how the goal and states are passed to the different policies would be very helpful if anyone wanted to reimplement this work. Updated comments: - The addition of more results and added analysis helps show the improvement of this method over the most related baselines.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the extensive feedback . We hope we can address most of the reviewer \u2019 s concerns . * * * Comparison to Universal Planning Networks * * * Comment : The results seem very similar to some of the work in `` Universal Planning Networks '' that did not need a more complex HRL design to achieve subgoal specification via images . This should be discussed more in the paper . We have added a more detailed comparison between our work and Universal Planning Networks [ 1 ] to the updated version of the paper . While we agree that UPN shows similar results in navigation domains , we argue that the problem we are solving is different . Our agents can learn solely from a sparse binary reward , which means it only receives 0 if it reaches the final goal and -1 in all other cases . Hence , the agent does not rely on a shaped reward function to facilitate learning or demonstrations as a guidance signal . Contrarily , UPN optimizes a supervised imitation objective , i.e. , uses demonstrations to help agents learn a task . Acquiring the necessary ground-truth data may not always be feasible . * * * Relative Goal Positions * * * Comment : The authors point out that the use of relative goal positions `` ensures generalization to new environments '' , this is a rather strong statement . The use of relative goal specification my help improve generalization but that can only be shown empirically . We rephrase the rather strong statement of \u201c ensures generalization \u201d to \u201c encourages generalization \u201d . To empirically show this , we provide an ablation study comparing absolute against relative positions ( see general comment for the results and discussion ) . * * * Comments about the Random Environment * * * 1 . Comment : Can more details be provided on how the `` Random '' environment is created ? Are the locations of the walls randomized ? Is the initial position of the agent and goal randomize ? 2.Comment : The demonstration to show that the method generalizes to other configuration after being trained on a fixed environment should be evaluated over many randomly generated environments so that we have a non-biased estimate of the true generalization performance . We refer the reviewer to appendix A.2 of the initial submission , where the procedure of creating the environments is described in detail . To summarize , we create 500 randomly generated environments with randomly sampled wall locations , start and goal positions . We then test on the same 500 random environments for all seeds to ensure a fair comparison , so it is an unbiased estimate of the true generalization performance ( addressing comment 2 . ) . 3.Comment : In Table 2 , it is rather surprising that the HiDe trained model does better on the `` Random '' environment vs the `` Forward '' environment it is trained on . The reason why the results are ( sometimes ) better compared to the forward task is because the start and goal positions can essentially be closer together due to the random sampling , while in the forward task the agent always has to move around the whole map to reach the goal . * * * Transfer Experiment Videos * * * Comment : The video seems to contradict the ordering of operations for training the planning network . The video suggests that first it is learned with the Ant then transferred to the ball which is less complex to control . In the video as well as in the results of the initial submission ( cf.Table 3 ) , we provide both the transfer of the planning layer from the ball to the more complex ant agent ( https : //drive.google.com/file/d/1mJhZPmGsr1n-JjSNBn42ExUzVjz41Nek/view ? t=1m45s ) and vice-versa ( https : //drive.google.com/file/d/1mJhZPmGsr1n-JjSNBn42ExUzVjz41Nek/view ? t=1m27s ) . * * * Prior Knowledge * * * Comment : It is noted that many other methods require prior knowledge of the environment I would say this method also requires certain kinds of prior knowledge about the task . For example , a top-down view of the environment is needed which is not often feasible . We agree on this and therefore have removed the part-sentence about prior knowledge from the related work section . Moreover , we now provide results that compare our approach against [ 3 ] , which is follow-up work of HIRO that also uses the top-down view image of the environment ( see general comment for the results and discussion ) ."}, "1": {"review_id": "r1nSxrKPH-1", "review_text": "This paper addresses hierarchical deep reinforcement learning (RL), an important problem in control learning and RL. Based on my understanding of this paper and recent prior work, the most important difference between the proposed approach (HiDe) and other recent approaches, such as HIRO and HAC, is that the top-level goal proposal policy uses a learned planner based on VIN and a learned attention mask to decide on a subgoal. There are also other differences, e.g., this policy outputs a goal position that is relative to the agent's position, rather than an absolute position. HiDe seems to demonstrate impressive transferability to both unseen mazes and new agent embodiments, which are important problems to address for hierarchical RL. Introducing learned planning and attention into the top-level policy seems to also introduce additional assumptions into the method. For example, it is my understanding that HIRO and HAC use the same state representation, e.g., joint positions and velocities of the agent, as input to their top-level policies. In contrast, HiDe uses a top down view of the maze and the x y position of the agent, which certainly is more privileged information. If this is correct, first, this should be discussed more thoroughly and directly in the paper. Second, the experimental setup should be elaborated on: is HIRO or HAC modified to include the same information for the top-level policy? Or can HiDe somehow be extended to not require this information? I do not think that requiring this information is egregious, but currently the experimental comparison is not clear in this regard. The experiments are arguably the strongest part of the paper, and the transfer results and videos are quite nice. But there is still room for improvement. Table 1 and Figure 5 seem disconnected. In particular, the numbers reported in Table 1 are clearly not achieved in Figure 5. Is the figure cut off early? Furthermore, an additional experiment on a more complicated domain would greatly strengthen the paper. A humanoid agent, for example, seems easy to test for the current method. Another option would be the movable blocks tested in HIRO, though it is unclear if this readily fits into the current method's assumptions. In my opinion, as the paper currently rests heavily on the results, this section should be further improved. Doing so would also improve my rating of the paper. ------ Edit after author response: I appreciate the authors' efforts in providing extensive responses to all of the reviewers' concerns as well as a significant general response detailing what seems to be a large amount of additional experimental work. I think that all of this warrants a change to my score, and seeing that the authors have more or less addressed my concerns, I am bumping up to a weak accept.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive feedback . We hope we are able to address all concerns accurately . * * * Comparison to State-of-the-art * * * Comment : HiDe uses a top down view of the maze and the x y position of the agent , which certainly is more privileged information . If this is correct , first , this should be discussed more thoroughly and directly in the paper . Second , the experimental setup should be elaborated on : is HIRO or HAC modified to include the same information for the top-level policy ? Or can HiDe somehow be extended to not require this information ? HiDe has indeed access to further information as it receives a top-down image , while HAC [ 1 ] and HIRO [ 2 ] do not get such information . We mention this more explicitly in the updated version of the paper . Moreover , we now provide results that compare our approach against [ 3 ] , which is follow-up work of HIRO that uses a simplified top-down view image as well ( see general comment for the results and discussion ) . * * * Experiment Section Improvements * * * 1 . Comment : Table 1 and Figure 5 seem disconnected . In particular , the numbers reported in Table 1 are clearly not achieved in Figure 5 . Is the figure cut off early ? Table 1 and Figure 5 may seem disconnected . For the evaluation shown in Table 1 , we selected the checkpoint for each seed where the validation success rate during training was highest . Since these checkpoints are taken at different timestamps , the averaged score of Figure 5 may yield the impression of not achieving these scores . We have added a clearer description of this process in the revised version of the paper and apologize for any possible misunderstandings . 2.Comment : Furthermore , an additional experiment on a more complicated domain would greatly strengthen the paper . A humanoid agent , for example , seems easy to test for the current method . We have added a humanoid agent as a proof of concept . We thereby highlight that our method allows training the planning layer with a simplistic agent such as a ball and transferring it to a very complex agent such as the humanoid ( contribution 3 ) . 3.Comment : In my opinion , as the paper currently rests heavily on the results , this section should be further improved . We try to address this by adding further ablation studies to our experiment sections . More specifically , we compare relative and absolute positions , fixed and learned window sizes for the planning layer , and HiDe with and without interface layer . * * * References * * * [ 1 ] Andrew Levy , Robert Platt , and Kate Saenko . Learning Multi-Level Hierarchies with Hindsight . InInternational Conference on Learning Representations , 2019 . [ 2 ] Ofir Nachum , Shixiang Shane Gu , Honglak Lee , and Sergey Levine . Data-efficient Hierarchical Reinforcement Learning . In Advances in Neural Information Processing Systems , pp . 3303\u20133313,2018 . [ 3 ] Ofir Nachum , Shixiang Gu , Honglak Lee , and Sergey Levine . Near-Optimal Representation Learning for Hierarchical Reinforcement Learning . InInternational Conference on Learning Representations , 2019"}, "2": {"review_id": "r1nSxrKPH-2", "review_text": "The submission proposes a novel method for explicit decomposition of hierarchical policies for long-horizon navigation tasks. The approach proposes to separate a policy into 3 modules, high-level planner, intermediate planner and low-level control. The evaluation shows that explicit decomposition is well suited for generalisation across a limited set of RL domains. The proposed method integrates aspects from a variety of recent work including planning layers from value propagation networks, hindsight training paradigms from hierarchical actor critic and hindsight experience replay and related techniques. The variety of different techniques combined instead of a single main contribution renders it challenging to follow all aspects and in particular to trace relevant contributions to performance - which is rendered harder by a limited evaluation section. While the approach shows good performance against a couple of start of the art methods, it is necessary to provide sufficient ablations to enable long-term insights for the community. The submission high level goal (explicit decomposition and information asymmetry) is clear, the execution involves the combination of many existing techniques plus variations such that it is hard to make solid statements about the relevance of any part. It is commendable that the authors have introduced adaptations and improvements to their baselines for a stronger and fairer comparison but the evaluation remains very limited. I suggest to run different domains as given by other domains from OpenAI gym or DeepMind control suite. But more importantly I suggest to run further ablations without the intermediate planning layer & with absolute goal positions. Furthermore, since HAC seems to perform worse when combined with the proposed low and mid level policies (RelHAC), it would make sense to compare to the proposed high-level policy using low and mid level from HAC instead. The submission provides an overall interesting perspective but makes it hard to narrow down on contribution and important insights by being unclear in formulation and providing only very limited ablations. Minor issues include: - Missing description of the mid level policy - what encourages the proposal of closer short-term goals. - Missing literature on information asymmetry in RL (e.g. see Tirumala et al 2019 \u2018Exploiting Hierarchy for Learning and Transfer in KL-regularized RL\u2019) - Unclear description of how models that get attached to existing planning layers have been trained (Sec 5.3). - Additional unclear description in the description of the experiments and method sections (4.2, 4.3)", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the constructive comments . We have tried to address the reviewer \u2019 s concerns about insufficient ablations and clarifications about our contributions . * * * Ablation Studies * * * Comment : I suggest to run further ablations without the intermediate planning layer & with absolute goal positions 1 . We conduct an ablation study to empirically show that the mid-layer is a necessary part of the architecture ( see general comment for the results and discussion ) . 2.We provide an ablation study with absolute against relative positions , showing that relative positions are an important aspect of our method in order to both solving the task and achieving generalization to other environments ( see general comment for the results and discussion ) . * * * Additional Domain Experiment * * * Comment : I suggest to run different domains as given by other domains from OpenAI gym . We have added a humanoid agent as an additional domain for a proof of concept of our method . We thereby highlight that our method allows training the planning layer with a simplistic agent such as a ball and transferring it to a very complex agent such as the humanoid . * * * Combining our Planning Layer with original HAC * * * Comment : Since HAC seems to perform worse when combined with the proposed low and mid level policies ( RelHAC ) , it would make sense to compare to the proposed high-level policy using low and mid level from HAC instead . The low and mid level from original HAC can not be directly combined with our planning layer , since the goal space in HAC is equal to the state space , such that the subgoals given to layers below consist not only of position goals , but also goals for the proprioceptive state of the agent , such as the joint angles and joint velocities . Contrarily , our planner is decoupled from such a task and therefore only learns to give position goals to the mid level . * * * Minor Comments * * * 1 . Comment : Missing description of the mid level policy - what encourages the proposal of closer short-term goals . The interface layer is motivated by results from previous work [ 1 ] . Furthermore , it is also claimed by [ 2 ] that the use of more layers leads to better exploration . To empirically show that the mid-layer is a necessary part in the architecture , we ran an ablation study ( see general comment for the results and discussion ) . 2.Comment : Missing literature on information asymmetry in RL ( e.g.see Tirumala et al 2019 \u2018 Exploiting Hierarchy for Learning and Transfer in KL-regularized RL \u2019 ) We have added the missing literature [ 3 ] to the revised version of the paper . 3.Comment : Unclear description of how models that get attached to existing planning layers have been trained ( Sec 5.3 ) . We have updated and clarified the description of how the transferred models get trained . More specifically , we evaluate the functional decomposition of HiDe by testing the compatibility of layers trained by different agents . We trained HiDe with either the Ball or the Ant agent . Then , we use the planning layer from such an agent and transfer it onto another agent that was trained using RelHAC , i.e. , HiDe without our proposed planner on the top layer . The planning layer agent is trained in the more complex environment of experiment 2 and the second agent is trained in the easier environment from experiment 1 . Despite being trained in different environments and with different agents , the planner is transferable . Moreover , our estimate indicates that training the planner with a Ball and then transferring it to a more complex agent is as much 3 to 4 times faster than training HiDe with the Ant or Humanoid from scratch . 4.Comment : Additional unclear description in the description of the experiments and method sections ( 4.2 , 4.3 ) Could the reviewer be a bit more specific about what we can clarify/improve in Sections 4.2 and 4.3 ? We will gladly apply such changes . * * * References * * * [ 1 ] Andrew Levy , Robert Platt , and Kate Saenko . Learning Multi-Level Hierarchies with Hindsight . InInternational Conference on Learning Representations , 2019 . [ 2 ] Anonymous Authors , Why Does Hierarchy ( Sometimes ) Work So Well in Reinforcement Learning ? , under double blind review for ICLR 2020 , https : //openreview.net/forum ? id=rJgSk04tDH [ 3 ] Dhruva Tirumala , Hyeonwoo Noh , Alexandre Galashov , Leonard Hasenclever , Arun Ahuja , Greg Wayne , Razvan Pascanu , Yee Whye Teh , and Nicolas Heess . Exploiting Hierarchy For Learning and Transfer in KL-regularized RL . CoRR , abs/1903.07438 , 2019"}}