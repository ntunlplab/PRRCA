{"year": "2018", "forum": "HyrCWeWCb", "title": "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control", "decision": "Accept (Poster)", "meta_review": "This paper adapts (Nachum et al 2017) to continuous control via TRPO.   The work is incremental (not in the dirty sense of the word popular amongst researchers, but rather in the sense of \"building atop a closely related work\"), nontrivial,  and shows empirical promise.    The reviewers would like more exploration of the sensitivity of the hyper-parameters.", "reviews": [{"review_id": "HyrCWeWCb-0", "review_text": "Clarity The paper is well-written and clear. Originality The paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy. The paper leverages a novel method in determining the coefficient of relative entropy. Significance - Trust-PCL achieves overall competitive with state-of-the-art external implementations. - Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance. - Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant. - Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful. Pros: - The paper is well-written and clear. - Competitive with state-of-the-art external implementations - Significant empirical advantage over TRPO. - Open source codes. Cons: - No ablation studies. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for carefully reading the details of the paper ; we greatly appreciate it . R1 : `` Even though the paper claims Trust-PCL ( on-policy ) is close to TRPO , the initial performance of TRPO looks better in HalfCheetah , Hopper , Walker2d and Ant . '' Trust-PCL ( on-policy ) achieves equal or better final reward compared to TRPO , but TRPO has a better initial performance . The results of Trust-PCL ( off-policy ) are the main point of the paper , showing that we can get both stability and sample-efficiency at the same time in a single algorithm . The presentation of the results for Trust-PCL ( on-policy ) is to convey the advantage of using off-policy data . R1 : `` Some ablation studies ( e.g. , on entropy regularization and relative entropy ) and sensitivity analysis on parameters ( e.g.\\alpha and update frequency on \\phi ) would be helpful . '' Section 5.2.1 of the the paper shows the effect of changing \\epsilon on the performance . As discussed in Section 4.3 , the value of \\epsilon directly determines \\lambda , the coefficient of relative entropy . The main contribution of the paper is stabilizing off-policy training via a suitable trust region constraint and hence , \\epsilon and \\lambda are the key hyper-parameters . However , we have expanded Section 5.2.1 to include anecdotal experience regarding the values of \\tau and the degree of off/on-policy ( determined by \\beta , \\alpha , P ) ."}, {"review_id": "HyrCWeWCb-1", "review_text": "This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. The major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing. In experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. What is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. Another issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.", "rating": "5: Marginally below acceptance threshold", "reply_text": "R2 : `` This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time\u2026 `` Our paper does not present a policy gradient method . Rather , we show that the optimal policy for an expected reward objective regularized with entropy and relative entropy satisfies a set of path-wise consistencies . Then , we propose an off-policy algorithm to implicitly train towards this objective . R2 : `` The major weakness of this paper is the unclear presentation . For example , the algorithm is never fully described , though a handful variants are discussed . How the off-policy version is implemented is missing . '' To improve the clarity of the presentation , we have updated the paper and included a pseudo-code in Appendix C. Moreover , we included the implementation details in Appendix B , and we have released an open-source package with all of the variants of the algorithm for completeness ( see footnote 1 ; the link will become available after the blind review ) . R2 : `` In experiments , why the off-policy version of TRPO is not compared . '' Unfortunately , TRPO is restricted to the use of on-policy data . This is the major limitation of TRPO . We address this limitation by introducing Trust-PCL , which optimizes a trust region objective using off-policy data . This is the major contribution of the paper R2 : `` Comparing the on-policy results , PCL does not show a significant advantage over TRPO . '' The results of Trust-PCL ( off-policy ) are the key takeaway of the paper , showing that we obtain both stability and sample-efficiency in a single algorithm , significantly outperforming TRPO . We present the results of Trust-PCL ( on-policy ) for completeness , to give a curious reader a sense of the performance loss when only on-policy data is used . We expect practitioners to only use off-policy Trust-PCL . R2 : `` the curves of TRPO is so unstable , which is a bit uncommon . '' Our TRPO implementation obtains similar performance compared with other implementations by J Schulman and rllab . In Table 1 , we compare against a number of externally available implementations . We also find the stability of our TRPO curves to be qualitatively similar to those appearing externally . R2 : `` What is the exploration strategy in the experiments ? '' All of the algorithms in the paper have a model of the policy \\pi_\\theta during training ( parameterized as a unimodal Gaussian , as is standard for continuous control ) . Accordingly , this policy is used to sample actions . Thus , there is no additional exploration injected . This is standard for continuous control RL algorithms like TRPO . R2 : `` I guess it was softmax probability . However , in many cases , softmax does not perform a good exploration , even if the entropy regularization is added . '' Please note that the multinomial distribution ( so-called softmax probability ) is standard in * discrete * control to parametrize the policy , but we are mostly considering continuous control problems in this paper . Our policy is parameterized by a unimodal Gaussian , as is standard in the continuous control benchmarks we evaluate . R2 : `` Another issue is the discussion of the entropy regularization in the objective function . This regularization , while helping exploration , do changes the original objective . When a policy is required to pass through a very narrow tunnel of states , the regularization that forces a wide action distribution could not have a good performance . '' Augmenting the expected reward objective using entropy regularization is standard in reinforcement learning . Often the multiplier of entropy is annealed to zero by the end of training to enable learning concentrated policies . R2 : `` Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids . '' We included 6 standard benchmarks in the paper , including : Acrobot , Half Cheetah , Swimmer , Hopper , Walker2D , and Ant . On all of the environments our Trust-PCL ( off-policy ) algorithm outperforms TRPO in both final reward and sample efficiency . We believe these experiments are enough to demonstrate the promise of the approach ."}, {"review_id": "HyrCWeWCb-2", "review_text": "The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. I am not an expert in this area. It is hard to judge the significance of this extension. The paper largely follows the work of Nachum et al 2017. The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training. However, the relative entropy term added seems like a marginal modification. Authors claimed that it satisfies the multi-step path consistency but the derivation is missing. I am a bit confused about the way trust region method is used in the paper. Initially, problem is written as a constrained optimization problem (12). It is then converted into a penalty form for softmax consistency. Finally, the Lagrange parameter is estimated from the trust region method. In addition, how do you get the Lagrange parameter from epsilon? The pseudo code of the algorithm is missing. It would be much clearer if a detailed description of the algorithmic procedure is given. How is the performance of Trust-PCL compared to PCL? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "R3 : `` The paper largely follows the work of Nachum et al 2017 . The differences ( i.e. , the claimed novelty ) from that work are the relative entropy and trust region method for training . However , the relative entropy term added seems like a marginal modification . '' The extension of the work of Nachum et al.by including relative entropy is novel and significant because it enables applying softmax consistency to difficult continuous control tasks . Nachum et al ( 2017 ) only evaluated PCL on simple discrete control tasks , and without including the additional trust region term , we were not able to obtain promising results . Our results achieve state-of-the-art in continuous control by substantially outperforming TRPO . Other than the introduction of relative entropy as an implicit trust region constraint , the technique described in Section 4.3 is novel and plays a key role in the success of Trust-PCL . R3 : `` Authors claimed that it satisfies the multi-step path consistency but the derivation is missing . '' We apologize for the lack of clarity . We have updated the paper to expand the derivation of the multi-step consistency over several equations ( see Eqs.16-21 ) .R3 : `` I am a bit confused about the way trust region method is used in the paper . Initially , problem is written as a constrained optimization problem ( 12 ) . It is then converted into a penalty form for softmax consistency . Finally , the Lagrange parameter is estimated from the trust region method . In addition , how do you get the Lagrange parameter from epsilon ? '' Trust-PCL trains towards a trust region objective ( Eq.12 or equivalently Eq.14 ) implicity by training a policy and a value function to satisfy a set of path-wise consistencies on off-policy data ( Eq.21 ) .The Lagrange multiplier \\lambda is easier to work with to formulate the path-wise consistencies , but \\lambda is not constant for a fixed \\epsilon , and \\epsilon is easier and more intuitive to tune . Hence , we describe a technique in Section 4.3 to adjust \\lambda for a given \\epsilon , and in the paper we switch between the constraint and Lagrangian form . R3 : `` The pseudo code of the algorithm is missing . It would be much clearer if a detailed description of the algorithmic procedure is given . '' Good suggestion . We have updated the paper to include a pseudo code of the algorithm in Appendix C. The link to the source code will become available after the blind review as well ( footnote 1 ) . R3 : `` How is the performance of Trust-PCL compared to PCL ? \u201d PCL is equivalent to Trust-PCL with \\epsilon = infinity or \\lambda = 0 . Section 5.2.1 shows the effect of different values of \\epsilon on the results of Trust-PCL . It is clear that as \\epsilon increases , the solution quality of Trust-PCL quickly degrades . We found that PCL ( corresponding to an even larger \\epsilon ) is largely ineffective on the difficult continuous control tasks considered in the paper . This shows the significance of the new technique over the original PCL ."}], "0": {"review_id": "HyrCWeWCb-0", "review_text": "Clarity The paper is well-written and clear. Originality The paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy. The paper leverages a novel method in determining the coefficient of relative entropy. Significance - Trust-PCL achieves overall competitive with state-of-the-art external implementations. - Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance. - Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant. - Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful. Pros: - The paper is well-written and clear. - Competitive with state-of-the-art external implementations - Significant empirical advantage over TRPO. - Open source codes. Cons: - No ablation studies. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for carefully reading the details of the paper ; we greatly appreciate it . R1 : `` Even though the paper claims Trust-PCL ( on-policy ) is close to TRPO , the initial performance of TRPO looks better in HalfCheetah , Hopper , Walker2d and Ant . '' Trust-PCL ( on-policy ) achieves equal or better final reward compared to TRPO , but TRPO has a better initial performance . The results of Trust-PCL ( off-policy ) are the main point of the paper , showing that we can get both stability and sample-efficiency at the same time in a single algorithm . The presentation of the results for Trust-PCL ( on-policy ) is to convey the advantage of using off-policy data . R1 : `` Some ablation studies ( e.g. , on entropy regularization and relative entropy ) and sensitivity analysis on parameters ( e.g.\\alpha and update frequency on \\phi ) would be helpful . '' Section 5.2.1 of the the paper shows the effect of changing \\epsilon on the performance . As discussed in Section 4.3 , the value of \\epsilon directly determines \\lambda , the coefficient of relative entropy . The main contribution of the paper is stabilizing off-policy training via a suitable trust region constraint and hence , \\epsilon and \\lambda are the key hyper-parameters . However , we have expanded Section 5.2.1 to include anecdotal experience regarding the values of \\tau and the degree of off/on-policy ( determined by \\beta , \\alpha , P ) ."}, "1": {"review_id": "HyrCWeWCb-1", "review_text": "This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. The major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing. In experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. What is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. Another issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.", "rating": "5: Marginally below acceptance threshold", "reply_text": "R2 : `` This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time\u2026 `` Our paper does not present a policy gradient method . Rather , we show that the optimal policy for an expected reward objective regularized with entropy and relative entropy satisfies a set of path-wise consistencies . Then , we propose an off-policy algorithm to implicitly train towards this objective . R2 : `` The major weakness of this paper is the unclear presentation . For example , the algorithm is never fully described , though a handful variants are discussed . How the off-policy version is implemented is missing . '' To improve the clarity of the presentation , we have updated the paper and included a pseudo-code in Appendix C. Moreover , we included the implementation details in Appendix B , and we have released an open-source package with all of the variants of the algorithm for completeness ( see footnote 1 ; the link will become available after the blind review ) . R2 : `` In experiments , why the off-policy version of TRPO is not compared . '' Unfortunately , TRPO is restricted to the use of on-policy data . This is the major limitation of TRPO . We address this limitation by introducing Trust-PCL , which optimizes a trust region objective using off-policy data . This is the major contribution of the paper R2 : `` Comparing the on-policy results , PCL does not show a significant advantage over TRPO . '' The results of Trust-PCL ( off-policy ) are the key takeaway of the paper , showing that we obtain both stability and sample-efficiency in a single algorithm , significantly outperforming TRPO . We present the results of Trust-PCL ( on-policy ) for completeness , to give a curious reader a sense of the performance loss when only on-policy data is used . We expect practitioners to only use off-policy Trust-PCL . R2 : `` the curves of TRPO is so unstable , which is a bit uncommon . '' Our TRPO implementation obtains similar performance compared with other implementations by J Schulman and rllab . In Table 1 , we compare against a number of externally available implementations . We also find the stability of our TRPO curves to be qualitatively similar to those appearing externally . R2 : `` What is the exploration strategy in the experiments ? '' All of the algorithms in the paper have a model of the policy \\pi_\\theta during training ( parameterized as a unimodal Gaussian , as is standard for continuous control ) . Accordingly , this policy is used to sample actions . Thus , there is no additional exploration injected . This is standard for continuous control RL algorithms like TRPO . R2 : `` I guess it was softmax probability . However , in many cases , softmax does not perform a good exploration , even if the entropy regularization is added . '' Please note that the multinomial distribution ( so-called softmax probability ) is standard in * discrete * control to parametrize the policy , but we are mostly considering continuous control problems in this paper . Our policy is parameterized by a unimodal Gaussian , as is standard in the continuous control benchmarks we evaluate . R2 : `` Another issue is the discussion of the entropy regularization in the objective function . This regularization , while helping exploration , do changes the original objective . When a policy is required to pass through a very narrow tunnel of states , the regularization that forces a wide action distribution could not have a good performance . '' Augmenting the expected reward objective using entropy regularization is standard in reinforcement learning . Often the multiplier of entropy is annealed to zero by the end of training to enable learning concentrated policies . R2 : `` Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids . '' We included 6 standard benchmarks in the paper , including : Acrobot , Half Cheetah , Swimmer , Hopper , Walker2D , and Ant . On all of the environments our Trust-PCL ( off-policy ) algorithm outperforms TRPO in both final reward and sample efficiency . We believe these experiments are enough to demonstrate the promise of the approach ."}, "2": {"review_id": "HyrCWeWCb-2", "review_text": "The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. I am not an expert in this area. It is hard to judge the significance of this extension. The paper largely follows the work of Nachum et al 2017. The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training. However, the relative entropy term added seems like a marginal modification. Authors claimed that it satisfies the multi-step path consistency but the derivation is missing. I am a bit confused about the way trust region method is used in the paper. Initially, problem is written as a constrained optimization problem (12). It is then converted into a penalty form for softmax consistency. Finally, the Lagrange parameter is estimated from the trust region method. In addition, how do you get the Lagrange parameter from epsilon? The pseudo code of the algorithm is missing. It would be much clearer if a detailed description of the algorithmic procedure is given. How is the performance of Trust-PCL compared to PCL? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "R3 : `` The paper largely follows the work of Nachum et al 2017 . The differences ( i.e. , the claimed novelty ) from that work are the relative entropy and trust region method for training . However , the relative entropy term added seems like a marginal modification . '' The extension of the work of Nachum et al.by including relative entropy is novel and significant because it enables applying softmax consistency to difficult continuous control tasks . Nachum et al ( 2017 ) only evaluated PCL on simple discrete control tasks , and without including the additional trust region term , we were not able to obtain promising results . Our results achieve state-of-the-art in continuous control by substantially outperforming TRPO . Other than the introduction of relative entropy as an implicit trust region constraint , the technique described in Section 4.3 is novel and plays a key role in the success of Trust-PCL . R3 : `` Authors claimed that it satisfies the multi-step path consistency but the derivation is missing . '' We apologize for the lack of clarity . We have updated the paper to expand the derivation of the multi-step consistency over several equations ( see Eqs.16-21 ) .R3 : `` I am a bit confused about the way trust region method is used in the paper . Initially , problem is written as a constrained optimization problem ( 12 ) . It is then converted into a penalty form for softmax consistency . Finally , the Lagrange parameter is estimated from the trust region method . In addition , how do you get the Lagrange parameter from epsilon ? '' Trust-PCL trains towards a trust region objective ( Eq.12 or equivalently Eq.14 ) implicity by training a policy and a value function to satisfy a set of path-wise consistencies on off-policy data ( Eq.21 ) .The Lagrange multiplier \\lambda is easier to work with to formulate the path-wise consistencies , but \\lambda is not constant for a fixed \\epsilon , and \\epsilon is easier and more intuitive to tune . Hence , we describe a technique in Section 4.3 to adjust \\lambda for a given \\epsilon , and in the paper we switch between the constraint and Lagrangian form . R3 : `` The pseudo code of the algorithm is missing . It would be much clearer if a detailed description of the algorithmic procedure is given . '' Good suggestion . We have updated the paper to include a pseudo code of the algorithm in Appendix C. The link to the source code will become available after the blind review as well ( footnote 1 ) . R3 : `` How is the performance of Trust-PCL compared to PCL ? \u201d PCL is equivalent to Trust-PCL with \\epsilon = infinity or \\lambda = 0 . Section 5.2.1 shows the effect of different values of \\epsilon on the results of Trust-PCL . It is clear that as \\epsilon increases , the solution quality of Trust-PCL quickly degrades . We found that PCL ( corresponding to an even larger \\epsilon ) is largely ineffective on the difficult continuous control tasks considered in the paper . This shows the significance of the new technique over the original PCL ."}}