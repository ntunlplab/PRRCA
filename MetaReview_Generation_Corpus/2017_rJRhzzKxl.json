{"year": "2017", "forum": "rJRhzzKxl", "title": "Knowledge Adaptation: Teaching to Adapt", "decision": "Reject", "meta_review": "The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction). \n \n + the results on the sentiment dataset are strong\n + the paper is easy to follow\n \n - relatively straightforward\n - the novel aspects are a bit heuristic\n - extra evaluation is needed", "reviews": [{"review_id": "rJRhzzKxl-0", "review_text": "Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis. What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better? One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope. Authors shall refer in the related-work to similar ideas proposed in the related field of acoustic modelling (and adaptation of acoustic models), in particular, works of Yu et al. [1] and the follow up work of Li et al. [2] which to an extent are a prior the work on knowledge distillation. Reasonably related is also work on deep relationship networks [3], where MT generative approach is proposed to avoid negative transfers, something of central role in this paper. Minors: The student S similarly has an output probability -> models an output probability [1] KL-Divergence Regularized Deep Neural Network Adaptation For Improved Large Vocabulary Speech Recognition, Dong Yu, Kaisheng Yao, Hang Su, Gang Li, Frank Seide [2] Learning Small-Size DNN with Output-Distribution-Based Criteria, Jinyu Li, Rui Zhao, Jui-Ting Huang and Yifan Gong [3] Learning Multiple Tasks with Deep Relationship Networks, Mingsheng Long, Jianmin Wang ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you a lot for your time and for your valuable feedback , which will be very helpful in improving the paper . re the learned representation h for the MCD measure : Yes , we use the top level pre-softmax activations in this case as this is the only hidden layer of our model . We are using a one hidden layer-MLP in our experiments to ensure comparability to previous work ( cf . [ 1 ] ) .For a deeper model , we would likely prefer to use intermediate ones in line with previous work . We will evaluate this in a future experiment . re scope of approach : We are not sure we fully understand your comment . We do not assume that the source and target vocabularies are exactly the same , but we assume some overlap in words is present ( as do most approaches to domain adaptation in NLP ) . As we only require the models ' output probabilities for training , the student and teacher are free to create different representations from the inputs . This might be a problem if we chose to constrain intermediate layers as in [ 2 ] . In our experiments , teacher and student models use different vocabularies ( as the teacher has not seen some target domain words during training ) and the student achieves state-of-the-art in this scenario . Please clarify if you are referring to a different setting . re related work : Thanks a lot for pointing us to the very relevant references . All of these are very valuable and they will be added to the related works section . [ 1 ] Glorot , X. , Bordes , A. , & Bengio , Y . ( 2011 ) .Domain Adaptation for Large-Scale Sentiment Classification : A Deep Learning Approach . Proceedings of the 28th International Conference on Machine Learning , ( 1 ) , 513\u2013520 . Retrieved from http : //www.icml-2011.org/papers/342_icmlpaper.pdf [ 2 ] Romero , A. , Ballas , N. , Kahou , S. E. , Chassang , A. , Gatta , C. , & Bengio , Y . ( 2015 ) .Fitnets : Hints for Thin Deep Nets . ICLR , 1\u201313 . Retrieved from http : //arxiv.org/pdf/1412.6550.pdf"}, {"review_id": "rJRhzzKxl-1", "review_text": "This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset. The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful. I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix). I think this paper would make an interesting ICLR paper. ", "rating": "7: Good paper, accept", "reply_text": "Thank you a lot for your time and for your feedback ! We are glad our responses helped to clarify unclear points . We really appreciate your suggestion . We have updated the paper with an appendix containing arguments of this thread . We have also clarified unclear points that were raised in previous questions and incorporated the relevant related work mentioned by AnonReviewer3 . We are looking forward to more fruitful discussions that will allow us to improve other aspects of the paper ."}, {"review_id": "rJRhzzKxl-2", "review_text": "The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source models, and performs comparable to the proposed method. Questions: 1. Why did you choose to use different combination schemes in equation (3) and (5)? For example, in equation (5), what if minimizing H( (1-\\lambda) y_teacher + \\lambda P_t, P_s) instead? 2. how will you extend the MCD definition to multi-class settings? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you a lot for your time and for your valuable feedback ! - As far as we are aware , all existing Deep Learning-based approaches to domain adaptation as well as most non-Deep Learning-based approaches require joint training on source and target data . Our method outperforms existing Deep Learning techniques as well as the state-of-the-art and does so without re-training . Would you mind referencing the papers on feature learning techniques for domain adaptation that require no joint training so that we can compare against them , as we are not sure to which you are referring ? re 1 : That is a good point . The combination schemes in equations ( 3 ) and ( 5 ) are indeed equivalent . We uploaded the derivation here ( https : //postimg.org/image/shwj3x0zj/ ) for ease of use . We will update equation ( 5 ) in the paper with the scheme of equation ( 3 ) for better readability . re 2 : We can extend MCD to the multi-class setting using pair-wise cluster differences as follows : For $ n $ classes , we compute $ n $ cluster centroids for the clusters whose members have been assigned the same class by our model . We then create a set containing all $ n ( n-1 ) /2 $ unique pairs of cluster centroids . Finally , we compute the sum of pair-wise differences of the model 's representations with regard to the cluster centroid pairs : $ MCD_ { multi } = \\sum_ { c_1 , c_2 \\in \\text { c_pairs } } | cos ( c_1 , h ) - cos ( c_2 , h ) | $ ."}], "0": {"review_id": "rJRhzzKxl-0", "review_text": "Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis. What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better? One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope. Authors shall refer in the related-work to similar ideas proposed in the related field of acoustic modelling (and adaptation of acoustic models), in particular, works of Yu et al. [1] and the follow up work of Li et al. [2] which to an extent are a prior the work on knowledge distillation. Reasonably related is also work on deep relationship networks [3], where MT generative approach is proposed to avoid negative transfers, something of central role in this paper. Minors: The student S similarly has an output probability -> models an output probability [1] KL-Divergence Regularized Deep Neural Network Adaptation For Improved Large Vocabulary Speech Recognition, Dong Yu, Kaisheng Yao, Hang Su, Gang Li, Frank Seide [2] Learning Small-Size DNN with Output-Distribution-Based Criteria, Jinyu Li, Rui Zhao, Jui-Ting Huang and Yifan Gong [3] Learning Multiple Tasks with Deep Relationship Networks, Mingsheng Long, Jianmin Wang ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you a lot for your time and for your valuable feedback , which will be very helpful in improving the paper . re the learned representation h for the MCD measure : Yes , we use the top level pre-softmax activations in this case as this is the only hidden layer of our model . We are using a one hidden layer-MLP in our experiments to ensure comparability to previous work ( cf . [ 1 ] ) .For a deeper model , we would likely prefer to use intermediate ones in line with previous work . We will evaluate this in a future experiment . re scope of approach : We are not sure we fully understand your comment . We do not assume that the source and target vocabularies are exactly the same , but we assume some overlap in words is present ( as do most approaches to domain adaptation in NLP ) . As we only require the models ' output probabilities for training , the student and teacher are free to create different representations from the inputs . This might be a problem if we chose to constrain intermediate layers as in [ 2 ] . In our experiments , teacher and student models use different vocabularies ( as the teacher has not seen some target domain words during training ) and the student achieves state-of-the-art in this scenario . Please clarify if you are referring to a different setting . re related work : Thanks a lot for pointing us to the very relevant references . All of these are very valuable and they will be added to the related works section . [ 1 ] Glorot , X. , Bordes , A. , & Bengio , Y . ( 2011 ) .Domain Adaptation for Large-Scale Sentiment Classification : A Deep Learning Approach . Proceedings of the 28th International Conference on Machine Learning , ( 1 ) , 513\u2013520 . Retrieved from http : //www.icml-2011.org/papers/342_icmlpaper.pdf [ 2 ] Romero , A. , Ballas , N. , Kahou , S. E. , Chassang , A. , Gatta , C. , & Bengio , Y . ( 2015 ) .Fitnets : Hints for Thin Deep Nets . ICLR , 1\u201313 . Retrieved from http : //arxiv.org/pdf/1412.6550.pdf"}, "1": {"review_id": "rJRhzzKxl-1", "review_text": "This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset. The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful. I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix). I think this paper would make an interesting ICLR paper. ", "rating": "7: Good paper, accept", "reply_text": "Thank you a lot for your time and for your feedback ! We are glad our responses helped to clarify unclear points . We really appreciate your suggestion . We have updated the paper with an appendix containing arguments of this thread . We have also clarified unclear points that were raised in previous questions and incorporated the relevant related work mentioned by AnonReviewer3 . We are looking forward to more fruitful discussions that will allow us to improve other aspects of the paper ."}, "2": {"review_id": "rJRhzzKxl-2", "review_text": "The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source models, and performs comparable to the proposed method. Questions: 1. Why did you choose to use different combination schemes in equation (3) and (5)? For example, in equation (5), what if minimizing H( (1-\\lambda) y_teacher + \\lambda P_t, P_s) instead? 2. how will you extend the MCD definition to multi-class settings? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you a lot for your time and for your valuable feedback ! - As far as we are aware , all existing Deep Learning-based approaches to domain adaptation as well as most non-Deep Learning-based approaches require joint training on source and target data . Our method outperforms existing Deep Learning techniques as well as the state-of-the-art and does so without re-training . Would you mind referencing the papers on feature learning techniques for domain adaptation that require no joint training so that we can compare against them , as we are not sure to which you are referring ? re 1 : That is a good point . The combination schemes in equations ( 3 ) and ( 5 ) are indeed equivalent . We uploaded the derivation here ( https : //postimg.org/image/shwj3x0zj/ ) for ease of use . We will update equation ( 5 ) in the paper with the scheme of equation ( 3 ) for better readability . re 2 : We can extend MCD to the multi-class setting using pair-wise cluster differences as follows : For $ n $ classes , we compute $ n $ cluster centroids for the clusters whose members have been assigned the same class by our model . We then create a set containing all $ n ( n-1 ) /2 $ unique pairs of cluster centroids . Finally , we compute the sum of pair-wise differences of the model 's representations with regard to the cluster centroid pairs : $ MCD_ { multi } = \\sum_ { c_1 , c_2 \\in \\text { c_pairs } } | cos ( c_1 , h ) - cos ( c_2 , h ) | $ ."}}