{"year": "2021", "forum": "TR-Nj6nFx42", "title": "A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper gives a new PAC-Bayesian generalization error bound for graph neural networks (GCN and MPGNN). The bound improves the previously known Rademacher complexity based bound given by Garg et al. (2020). In particular, its dependency on the maximum node degree and the maximum hidden dimension is improved.\n\nThis paper gives an interesting improvement on the generalization analysis of GNNs. The writing is clear, where its connection to existing work and its technical contribution are well discussed. \nThe biggest concern is its technical novelty. Indeed, the proof follows the out-line of Neyshabur et al. (2017). Given that the technical novelty would be a bit limited, however, the analysis should properly deal with the complicated structure specific to GNNs which makes the analysis more difficult than usual CNN/MLP and requires subtle and careful manipulations. \nIn addition to that, the improvement of the generalization bound is valuable for the literature (while the improvement seems a bit minor for graphs with small maximum degree). \n\nFor these reasons, I recommend acceptance for this paper.", "reviews": [{"review_id": "TR-Nj6nFx42-0", "review_text": "This paper , by PAC-Bayesian approach , proves the generalization bounds for the two primary classes of graph neural networks\u2014graph convolutional networks and message passing GNNs . By experiments on four datasets , it shows that the generalization bound in this paper is tighter than the existing Rademacher complexity bound . This is an interesting question about generalization bound , especially , such a discussion by PAC-bayesian approach for graph neural networks is lacking . However , there are a few issues/comments with the work : 1.The proof techniques in this paper are mainly from Neyshabur et al. , 2017 . The theoretical contribution and novelty are limited ; 2.For the generalization bounds , they are exponential dependence on depth . So for $ d > 1 $ , it means that the deeper it is , the worse it is for the graphical neural network . Is that so ? 3.It will be better to use more empirical experiments to verify the relationship between the generalization bounds , node degrees , and depth ; 4.For Assumption 4 in this paper , it assumes that \u201c no loop \u201d , for the practical application of graph neural networks , loop generally exists . This will limit the potential application of this theory . in this paper ; 5.According to the theory established in this paper and the # graphs and max # nodes of the datasets provided in APPENDIX , PROTEINS and IMDB-BINARY should have more similar log bound value than IMDB-BINARY and MDB-MULTI . In Figure 1 , it seems to be the opposite , why ? Is it because the feature dimension also plays an important role in the log bound value ? 6.The format of mathematical equations is inconsistent , for example , equation ( 1 ) without punctuation , but equation ( 3 ) with punctuation . Overall , I think this is an interesting piece of work that might interest researchers to explore the questions around graph neural networks . However , I think the results need to be analyzed more carefully , especially on the depth . Moreover , the novelty of the technology is relatively limited .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : The proof techniques in this paper are mainly from Neyshabur et al. , 2017 . The theoretical contribution and novelty are limited . > A1 : Please refer to A1 in the common response . Q2 : For the generalization bounds , they are exponential dependence on depth . So for d > 1 , it means that the deeper it is , the worse it is for the graphical neural network . Is that so ? > A2 : The dependency on the depth is indeed exponential which is also the case for the recent Rademacher complexity results [ 1 ] . This indicates the deeper the network is , the looser the bound is for d > 1 . But it does not necessarily mean the worse the GNN performs since the uniform convergence bounds hold for all models in the class thus including the worst case ( so does our perturbation analysis ) . As we discussed in the paper , the current bound is vacuous and can not fully explain the practical behaviors of GNNs . > > > [ 1 ] Garg , V.K. , Jegelka , S. and Jaakkola , T. , 2020 . Generalization and representational limits of graph neural networks . In ICML.Q3 : It will be better to use more empirical experiments to verify the relationship between the generalization bounds , node degrees , and depth . > A3 : Thanks for the suggestion . We added new experiments with varying random graph families , node degrees , and depth . Please refer to A2 in the common response and the updated paper . Q4.For Assumption 4 in this paper , it assumes that \u201c no loop \u201d , for the practical application of graph neural networks , loop generally exists . This will limit the potential application of this theory in this paper . > A4 : Sorry that we should have clarified the terminology . We use the graph theory definition , i.e. , a loop means an edge that connects a vertex to itself ( sometimes called self-loop ) . This is easily confused with cycles in graph theory which many people in practice also refer to as loops . We clarify them in the new version . Therefore , our results hold for graphs with \u201c loops \u201d . Moreover , self-loops can be easily handled in the perturbation analysis and they only affect the max node degree \u201c d-1 \u201d in our bound . But multi-edges may require more subtle treatment . Q5 : According to the theory established in this paper and the # graphs and max # nodes of the datasets provided in APPENDIX , PROTEINS and IMDB-BINARY should have more similar log bound value than IMDB-BINARY and IMDB-MULTI . In Figure 1 , it seems to be the opposite , why ? Is it because the feature dimension also plays an important role in the log bound value ? > A5 : We want to clarify that 1 ) our bounds do not depend on max # nodes ; 2 ) our bounds depend on the feature dimension h in $ O ( \\sqrt { h \\log h } ) $ ; 3 ) the most significant term in our bounds is the max node degree . If we rank datasets ascendingly by the max node degree , they are PROTEINS , IMDB-MULTI , IMDB-BINARY , and COLLAB . This order mostly matches what we observed in terms of the ascending ranking of bound values in Figure 1 . Q6 : The format of mathematical equations is inconsistent , for example , equation ( 1 ) without punctuation , but equation ( 3 ) with punctuation . > A6 : Thanks for pointing this out . We have corrected them in the new version ."}, {"review_id": "TR-Nj6nFx42-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper provides results of generalization bounds for two types of GNNs : GCN and MPGNN . The presented analysis follows the framework of Neyshabur 2017 to construct posterior by adding random perturbations so that the PAC-Bayesian technique can be applied . The main contributions are the perturbation analysis for GCN and MPGNN , which results in a bound depending on the graph statistics . The paper compares the derived bounds with existing results , and examines the bounds numerically through experiments . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I find the paper well-written and overall technically sound . I vote for accept . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strength : This paper studies an important problem , as GNNs are popular learning models . While the overall idea follows directly from Neyshabur 2017 , the presented perturbation analysis is not trivial , making the current work a good advance . This paper is well-organized , with nice discussions on the state-of-the-art as well as on the overall technical review . It is appreciated that the paper is self-contained , and the preliminary results are provided in the appendix ( though they can be found in earlier papers ) . The paper provides a detailed comparison between the derived bounds with the existing bounds . The results of this paper are natural in the sense they generalize that the results in Neyshabur 2017 for MLPs . Some concerns : For the proofs of Theorem 3.2 and 3.4 , I would suggest decomposing it into several lemmas , by first identifying the covering ( and the weight-independent variance ) and then plugging the values into Lemma 3.3 ( and Lemma 3.1 ) and further into Lemma 2.2 To avoid ambiguity , using D_KL ( u+w||P ) is better than D_KL ( Q||P ) in this paper . Given the heavy notations , having a notation table in the appendix could be very helpful . What are the key difficulties in applying the proposed framework to other GNN architectures ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : For the proofs of Theorem 3.2 and 3.4 , I would suggest decomposing it into several lemmas , by first identifying the covering ( and the weight-independent variance ) and then plugging the values into Lemma 3.3 ( and Lemma 3.1 ) and further into Lemma 2.2 . To avoid ambiguity , using D_KL ( u+w||P ) is better than D_KL ( Q||P ) in this paper . Given the heavy notations , having a notation table in the appendix could be very helpful . > A1 : Thanks for your great suggestions ! We improved the presentation and added the notation table in the new version . We will reorganize the proof to make it more clear . Q2 : What are the key difficulties in applying the proposed framework to other GNN architectures ? > A2 : Given a new GNN architecture , there are mainly two difficulties in applying the framework . > > > First , developing the perturbation analysis . Some nonlinear mechanisms within GNNs like attention or spectral filters may raise some challenges in obtaining a tight perturbation bound . One may need to resort to more advanced inequalities or techniques . > > > Second , once the perturbation bound was established , the challenge is to construct a proper quantity of weights w so that 1 ) it makes all conditions on the random perturbations hold , e.g. , the ones in Lemma 2.2 , 3.1 , and 3.3 ; 2 ) it induces a finite covering of its range . Since this part highly depends on the form of the perturbation bound and the network architecture , there seems to be no general recipe on how to construct such a quantity . Please refer to A1 in the common response for more discussion on this point ."}, {"review_id": "TR-Nj6nFx42-2", "review_text": "In this paper , the authors propose generalization bounds for GNNs , both convolutional and standard message passing variants . The result is a generalization of those for CNN/MLP architectures with relu activation functions . The analysis method closely follows those established in the former settings as well . The specific setting they consider is where each sample in the dataset is a graph . The proof relies on ensuring small perturbations in the GNN weights do n't cause large deviations in output distributions . The resulting PAC-Bayes bound is shown to be tighter than corresponding Rademacher Complexity bounds . The paper is well written and the results look reasonable ( though I did n't check the proofs ) . A couple of minor comments/questions : 1 . Does assumption A4 cause things to be a lot looser than needed ? Consider a star graph , then d ~ number of nodes in the graph . But this is just for one node . 2.How would things change if your data is actually not iid ~ D , but the entire dataset is the graph ( so you just have one graph ) , and you need to classify the nodes ? I 'm guessing there 's parts in the proof where the iid assumption is necessary .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : Does assumption A4 cause things to be a lot looser than needed ? Consider a star graph , then d ~ number of nodes in the graph . But this is just for one node . > A1 : The assumption A4 is needed since our perturbation analysis is in the worst-case sense , i.e. , the change of any node representation is upper bounded under certain perturbations on the weights of GNN . If the distribution of node degrees is assumed to be known , then we can perform a probabilistic perturbation analysis which could overcome this issue . Alternatively , if the readout function only relies on a subgraph , e.g. , applying some sort of hard attention mechanism , then this issue could also be alleviated . Q2 : How would things change if your data is actually not iid ~ D , but the entire dataset is the graph ( so you just have one graph ) , and you need to classify the nodes ? I 'm guessing there 's parts in the proof where the iid assumption is necessary . > A2 : Thanks for raising this good point ! We did think about the semi-supervised node classification scenario as you mentioned . However , we did not investigate it further due to the space constraint . Intuitively , GNNs unroll a computation tree ( depth corresponds to the message passing step / graph convolution layer ) from a node perspective . Our perturbation analysis already bounds the change of the root representation of any such computation tree . So we could reuse this part . But in order to establish the full PAC-Bayes analysis , you still need an iid assumption of some form , e.g. , computation trees of all nodes within this graph are iid . This is due to the fact that general PAC-Bayes results ( Theorem 2.1 and Lemma 2.2 ) require such an assumption ."}, {"review_id": "TR-Nj6nFx42-3", "review_text": "# Summary The paper presents PAC-Bayesian generalization bounds for two classes of graph neural networks : graph convolutional neural networks and message passing graph neural networks . The paper essentially adapts Neyshabur et al . ( 2017 ) PAC-Bayesian margin bounds for neural networks to graph neural networks and expectedly the bounds contain terms that depend on the degree of the underlying graph . The main technical contribution of the paper is a perturbation bound for GNNs from which the main results follow . # Strengths 1 . The paper presents the first PAC-Bayesian generalization bound for GNNs and the authors show that their bounds are tighter than the Rademacher based generalization bounds developed by Garg et al ( 2020 ) ignoring constants . # Weakness 1 . The bounds do not necessarily give important insights into generalization performance of GNNs . Bounds are still vacuous : bounds become exponentially large with number of layers and degree . 2.For a theory paper that purely focuses on obtaining generalization bounds for GNNs , the technical contributions are weak . The only technical contribution is the derivation of perturbation bounds for GNNs which are pretty easy to obtain . If this is not the case , then this needs to be highlighted . 3.It is not sufficient to compare bounds on real world data at some fixed sample size for empirical comparison with Rademacher based bounds , especially given that constants are dropped . Synthetic experiments with varying samples , graph families ( e.g.Erdos Renyi graphs ) and different degree distribution are needed to show how the bounds compare against Rademacher based bounds . # Justification for rating The generalization bounds themselves provide very limited insights into generalization performance of GNNs especially given recent results that show that uniform convergence bounds may not be able to explain generalization of deep neural networks . While this is an issue with existing theoretical results for deep neural networks , the paper does not significantly improve the state-of-the-art in theoretical understanding of GNNs in terms of new tools and proof techniques . For a theory paper that purely focuses on generalization bounds , this is a significant shortcoming . # Other comments 1 . Misleading use of the word `` statistics '' throughout the paper . Statistics are quantities that can be computed only from the data . The paper repeatedly refers to functionals of parameters as `` statistics '' . 2.What do you mean by : `` actual posterior distribution induced by learning process may be very different from Gaussians '' ? PAC-Bayesian analysis is done for Gibbs classifiers and Neyshabur et al.provide a way to convert these convergence bounds for deterministic classifiers . The learning process does not induce a posterior distribution over weights ( assuming deterministic initialization and removing randomness like dropout ) . 3.D is overloaded to denote both data distribution and diagonal degree matrix .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : The bounds do not necessarily give important insights into generalization performance of GNNs especially given recent results that show that uniform convergence bounds may not be able to explain generalization of deep neural networks . Bounds are still vacuous : bounds become exponentially large with the number of layers and degree . > A1 : Please refer to A1 in the common response . Q2 : The paper does not significantly improve the state-of-the-art in theoretical understanding of GNNs in terms of new tools and proof techniques . For a theory paper that purely focuses on obtaining generalization bounds for GNNs , the technical contributions are weak . The only technical contribution is the derivation of perturbation bounds for GNNs which are pretty easy to obtain . If this is not the case , then this needs to be highlighted . > A2 : We appreciate your high standard in evaluating technical contributions . Please refer to A1 in the common response . Q3 : It is not sufficient to compare bounds on real-world data at some fixed sample size for empirical comparison with Rademacher based bounds , especially given that constants are dropped . Synthetic experiments with varying samples , graph families ( e.g.Erdos Renyi graphs ) and different degree distributions are needed to show how the bounds compare against Rademacher based bounds . > A3 : Thanks for the suggestion . We added experiments which consider the constants in bound evaluations and include 6 synthetic random graph families in the new version . Please refer to A2 in the common response , the updated paper , and the updated appendix . Q4 : Misleading use of the word `` statistics '' throughout the paper . Statistics are quantities that can be computed only from the data . The paper repeatedly refers to functionals of parameters as `` statistics '' . > A4 : We appreciate your rigorousness w.r.t.terminologies . For clarification , we use the term statistic to refer to some function of both data and weights , e.g. , the maximum node representations . Similar usage has become common in machine learning , e.g. , \u201c batch statistics \u201d such as the mean of hidden representations in batch normalization [ 1 ] are functions of both data and weights . But we agree that it is confusing as people typically map the terminology weights in machine learning to parameters in statistical inference . We modified our wording in the new version . > > [ 1 ] Ioffe , S. and Szegedy , C. , 2015 . Batch normalization : Accelerating deep network training by reducing internal covariate shift . In ICML.Q5 : What do you mean by : `` actual posterior distribution induced by the learning process may be very different from Gaussians '' ? PAC-Bayesian analysis is done for Gibbs classifiers and Neyshabur et al.provide a way to convert these convergence bounds for deterministic classifiers . The learning process does not induce a posterior distribution over weights ( assuming deterministic initialization and removing randomness like dropout ) . > A5 : Sorry for the confusion as we did not have space to expand the argument in detail in the submission . The results of Neyshabur et al.indeed provide a way to convert these convergence bounds for deterministic classifiers ( their result is actually a generalization of an earlier result by David McAllester ) . They achieve it by adding Gaussian perturbations on the learned weights of the deterministic model . > > > Here we refer to another stochastic view of a deterministic model where people in practice often randomly initialize weights following a prior distribution , e.g. , Gaussian or uniform , before learning . If we collect the learned weights returned by the learning process under different initial weights drawn from the prior distribution , then we can view them as drawn from a posterior distribution induced by the prior and the learning process . This posterior may be quite non-Gaussian , e.g. , they may be a multimodal distribution since the learned weights may land in different local minima with different initialized weights . Therefore , the Gaussian perturbation may hardly capture this phenomenon . > > > This view follows more closely to how people actually train deterministic models in practice . Therefore , it may be worthwhile to investigate whether one can develop a PAC-Bayes result under this stochastic view . Of course , there are many challenges , e.g. , the density of the induced distribution is unknown so that one can not obtain the KL divergence . Q6 : D is overloaded to denote both data distribution and diagonal degree matrix . > A6 : Thanks for pointing it out . We have corrected it in the new version ."}], "0": {"review_id": "TR-Nj6nFx42-0", "review_text": "This paper , by PAC-Bayesian approach , proves the generalization bounds for the two primary classes of graph neural networks\u2014graph convolutional networks and message passing GNNs . By experiments on four datasets , it shows that the generalization bound in this paper is tighter than the existing Rademacher complexity bound . This is an interesting question about generalization bound , especially , such a discussion by PAC-bayesian approach for graph neural networks is lacking . However , there are a few issues/comments with the work : 1.The proof techniques in this paper are mainly from Neyshabur et al. , 2017 . The theoretical contribution and novelty are limited ; 2.For the generalization bounds , they are exponential dependence on depth . So for $ d > 1 $ , it means that the deeper it is , the worse it is for the graphical neural network . Is that so ? 3.It will be better to use more empirical experiments to verify the relationship between the generalization bounds , node degrees , and depth ; 4.For Assumption 4 in this paper , it assumes that \u201c no loop \u201d , for the practical application of graph neural networks , loop generally exists . This will limit the potential application of this theory . in this paper ; 5.According to the theory established in this paper and the # graphs and max # nodes of the datasets provided in APPENDIX , PROTEINS and IMDB-BINARY should have more similar log bound value than IMDB-BINARY and MDB-MULTI . In Figure 1 , it seems to be the opposite , why ? Is it because the feature dimension also plays an important role in the log bound value ? 6.The format of mathematical equations is inconsistent , for example , equation ( 1 ) without punctuation , but equation ( 3 ) with punctuation . Overall , I think this is an interesting piece of work that might interest researchers to explore the questions around graph neural networks . However , I think the results need to be analyzed more carefully , especially on the depth . Moreover , the novelty of the technology is relatively limited .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : The proof techniques in this paper are mainly from Neyshabur et al. , 2017 . The theoretical contribution and novelty are limited . > A1 : Please refer to A1 in the common response . Q2 : For the generalization bounds , they are exponential dependence on depth . So for d > 1 , it means that the deeper it is , the worse it is for the graphical neural network . Is that so ? > A2 : The dependency on the depth is indeed exponential which is also the case for the recent Rademacher complexity results [ 1 ] . This indicates the deeper the network is , the looser the bound is for d > 1 . But it does not necessarily mean the worse the GNN performs since the uniform convergence bounds hold for all models in the class thus including the worst case ( so does our perturbation analysis ) . As we discussed in the paper , the current bound is vacuous and can not fully explain the practical behaviors of GNNs . > > > [ 1 ] Garg , V.K. , Jegelka , S. and Jaakkola , T. , 2020 . Generalization and representational limits of graph neural networks . In ICML.Q3 : It will be better to use more empirical experiments to verify the relationship between the generalization bounds , node degrees , and depth . > A3 : Thanks for the suggestion . We added new experiments with varying random graph families , node degrees , and depth . Please refer to A2 in the common response and the updated paper . Q4.For Assumption 4 in this paper , it assumes that \u201c no loop \u201d , for the practical application of graph neural networks , loop generally exists . This will limit the potential application of this theory in this paper . > A4 : Sorry that we should have clarified the terminology . We use the graph theory definition , i.e. , a loop means an edge that connects a vertex to itself ( sometimes called self-loop ) . This is easily confused with cycles in graph theory which many people in practice also refer to as loops . We clarify them in the new version . Therefore , our results hold for graphs with \u201c loops \u201d . Moreover , self-loops can be easily handled in the perturbation analysis and they only affect the max node degree \u201c d-1 \u201d in our bound . But multi-edges may require more subtle treatment . Q5 : According to the theory established in this paper and the # graphs and max # nodes of the datasets provided in APPENDIX , PROTEINS and IMDB-BINARY should have more similar log bound value than IMDB-BINARY and IMDB-MULTI . In Figure 1 , it seems to be the opposite , why ? Is it because the feature dimension also plays an important role in the log bound value ? > A5 : We want to clarify that 1 ) our bounds do not depend on max # nodes ; 2 ) our bounds depend on the feature dimension h in $ O ( \\sqrt { h \\log h } ) $ ; 3 ) the most significant term in our bounds is the max node degree . If we rank datasets ascendingly by the max node degree , they are PROTEINS , IMDB-MULTI , IMDB-BINARY , and COLLAB . This order mostly matches what we observed in terms of the ascending ranking of bound values in Figure 1 . Q6 : The format of mathematical equations is inconsistent , for example , equation ( 1 ) without punctuation , but equation ( 3 ) with punctuation . > A6 : Thanks for pointing this out . We have corrected them in the new version ."}, "1": {"review_id": "TR-Nj6nFx42-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper provides results of generalization bounds for two types of GNNs : GCN and MPGNN . The presented analysis follows the framework of Neyshabur 2017 to construct posterior by adding random perturbations so that the PAC-Bayesian technique can be applied . The main contributions are the perturbation analysis for GCN and MPGNN , which results in a bound depending on the graph statistics . The paper compares the derived bounds with existing results , and examines the bounds numerically through experiments . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I find the paper well-written and overall technically sound . I vote for accept . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strength : This paper studies an important problem , as GNNs are popular learning models . While the overall idea follows directly from Neyshabur 2017 , the presented perturbation analysis is not trivial , making the current work a good advance . This paper is well-organized , with nice discussions on the state-of-the-art as well as on the overall technical review . It is appreciated that the paper is self-contained , and the preliminary results are provided in the appendix ( though they can be found in earlier papers ) . The paper provides a detailed comparison between the derived bounds with the existing bounds . The results of this paper are natural in the sense they generalize that the results in Neyshabur 2017 for MLPs . Some concerns : For the proofs of Theorem 3.2 and 3.4 , I would suggest decomposing it into several lemmas , by first identifying the covering ( and the weight-independent variance ) and then plugging the values into Lemma 3.3 ( and Lemma 3.1 ) and further into Lemma 2.2 To avoid ambiguity , using D_KL ( u+w||P ) is better than D_KL ( Q||P ) in this paper . Given the heavy notations , having a notation table in the appendix could be very helpful . What are the key difficulties in applying the proposed framework to other GNN architectures ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : For the proofs of Theorem 3.2 and 3.4 , I would suggest decomposing it into several lemmas , by first identifying the covering ( and the weight-independent variance ) and then plugging the values into Lemma 3.3 ( and Lemma 3.1 ) and further into Lemma 2.2 . To avoid ambiguity , using D_KL ( u+w||P ) is better than D_KL ( Q||P ) in this paper . Given the heavy notations , having a notation table in the appendix could be very helpful . > A1 : Thanks for your great suggestions ! We improved the presentation and added the notation table in the new version . We will reorganize the proof to make it more clear . Q2 : What are the key difficulties in applying the proposed framework to other GNN architectures ? > A2 : Given a new GNN architecture , there are mainly two difficulties in applying the framework . > > > First , developing the perturbation analysis . Some nonlinear mechanisms within GNNs like attention or spectral filters may raise some challenges in obtaining a tight perturbation bound . One may need to resort to more advanced inequalities or techniques . > > > Second , once the perturbation bound was established , the challenge is to construct a proper quantity of weights w so that 1 ) it makes all conditions on the random perturbations hold , e.g. , the ones in Lemma 2.2 , 3.1 , and 3.3 ; 2 ) it induces a finite covering of its range . Since this part highly depends on the form of the perturbation bound and the network architecture , there seems to be no general recipe on how to construct such a quantity . Please refer to A1 in the common response for more discussion on this point ."}, "2": {"review_id": "TR-Nj6nFx42-2", "review_text": "In this paper , the authors propose generalization bounds for GNNs , both convolutional and standard message passing variants . The result is a generalization of those for CNN/MLP architectures with relu activation functions . The analysis method closely follows those established in the former settings as well . The specific setting they consider is where each sample in the dataset is a graph . The proof relies on ensuring small perturbations in the GNN weights do n't cause large deviations in output distributions . The resulting PAC-Bayes bound is shown to be tighter than corresponding Rademacher Complexity bounds . The paper is well written and the results look reasonable ( though I did n't check the proofs ) . A couple of minor comments/questions : 1 . Does assumption A4 cause things to be a lot looser than needed ? Consider a star graph , then d ~ number of nodes in the graph . But this is just for one node . 2.How would things change if your data is actually not iid ~ D , but the entire dataset is the graph ( so you just have one graph ) , and you need to classify the nodes ? I 'm guessing there 's parts in the proof where the iid assumption is necessary .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : Does assumption A4 cause things to be a lot looser than needed ? Consider a star graph , then d ~ number of nodes in the graph . But this is just for one node . > A1 : The assumption A4 is needed since our perturbation analysis is in the worst-case sense , i.e. , the change of any node representation is upper bounded under certain perturbations on the weights of GNN . If the distribution of node degrees is assumed to be known , then we can perform a probabilistic perturbation analysis which could overcome this issue . Alternatively , if the readout function only relies on a subgraph , e.g. , applying some sort of hard attention mechanism , then this issue could also be alleviated . Q2 : How would things change if your data is actually not iid ~ D , but the entire dataset is the graph ( so you just have one graph ) , and you need to classify the nodes ? I 'm guessing there 's parts in the proof where the iid assumption is necessary . > A2 : Thanks for raising this good point ! We did think about the semi-supervised node classification scenario as you mentioned . However , we did not investigate it further due to the space constraint . Intuitively , GNNs unroll a computation tree ( depth corresponds to the message passing step / graph convolution layer ) from a node perspective . Our perturbation analysis already bounds the change of the root representation of any such computation tree . So we could reuse this part . But in order to establish the full PAC-Bayes analysis , you still need an iid assumption of some form , e.g. , computation trees of all nodes within this graph are iid . This is due to the fact that general PAC-Bayes results ( Theorem 2.1 and Lemma 2.2 ) require such an assumption ."}, "3": {"review_id": "TR-Nj6nFx42-3", "review_text": "# Summary The paper presents PAC-Bayesian generalization bounds for two classes of graph neural networks : graph convolutional neural networks and message passing graph neural networks . The paper essentially adapts Neyshabur et al . ( 2017 ) PAC-Bayesian margin bounds for neural networks to graph neural networks and expectedly the bounds contain terms that depend on the degree of the underlying graph . The main technical contribution of the paper is a perturbation bound for GNNs from which the main results follow . # Strengths 1 . The paper presents the first PAC-Bayesian generalization bound for GNNs and the authors show that their bounds are tighter than the Rademacher based generalization bounds developed by Garg et al ( 2020 ) ignoring constants . # Weakness 1 . The bounds do not necessarily give important insights into generalization performance of GNNs . Bounds are still vacuous : bounds become exponentially large with number of layers and degree . 2.For a theory paper that purely focuses on obtaining generalization bounds for GNNs , the technical contributions are weak . The only technical contribution is the derivation of perturbation bounds for GNNs which are pretty easy to obtain . If this is not the case , then this needs to be highlighted . 3.It is not sufficient to compare bounds on real world data at some fixed sample size for empirical comparison with Rademacher based bounds , especially given that constants are dropped . Synthetic experiments with varying samples , graph families ( e.g.Erdos Renyi graphs ) and different degree distribution are needed to show how the bounds compare against Rademacher based bounds . # Justification for rating The generalization bounds themselves provide very limited insights into generalization performance of GNNs especially given recent results that show that uniform convergence bounds may not be able to explain generalization of deep neural networks . While this is an issue with existing theoretical results for deep neural networks , the paper does not significantly improve the state-of-the-art in theoretical understanding of GNNs in terms of new tools and proof techniques . For a theory paper that purely focuses on generalization bounds , this is a significant shortcoming . # Other comments 1 . Misleading use of the word `` statistics '' throughout the paper . Statistics are quantities that can be computed only from the data . The paper repeatedly refers to functionals of parameters as `` statistics '' . 2.What do you mean by : `` actual posterior distribution induced by learning process may be very different from Gaussians '' ? PAC-Bayesian analysis is done for Gibbs classifiers and Neyshabur et al.provide a way to convert these convergence bounds for deterministic classifiers . The learning process does not induce a posterior distribution over weights ( assuming deterministic initialization and removing randomness like dropout ) . 3.D is overloaded to denote both data distribution and diagonal degree matrix .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We respond to the individual questions as below . Q1 : The bounds do not necessarily give important insights into generalization performance of GNNs especially given recent results that show that uniform convergence bounds may not be able to explain generalization of deep neural networks . Bounds are still vacuous : bounds become exponentially large with the number of layers and degree . > A1 : Please refer to A1 in the common response . Q2 : The paper does not significantly improve the state-of-the-art in theoretical understanding of GNNs in terms of new tools and proof techniques . For a theory paper that purely focuses on obtaining generalization bounds for GNNs , the technical contributions are weak . The only technical contribution is the derivation of perturbation bounds for GNNs which are pretty easy to obtain . If this is not the case , then this needs to be highlighted . > A2 : We appreciate your high standard in evaluating technical contributions . Please refer to A1 in the common response . Q3 : It is not sufficient to compare bounds on real-world data at some fixed sample size for empirical comparison with Rademacher based bounds , especially given that constants are dropped . Synthetic experiments with varying samples , graph families ( e.g.Erdos Renyi graphs ) and different degree distributions are needed to show how the bounds compare against Rademacher based bounds . > A3 : Thanks for the suggestion . We added experiments which consider the constants in bound evaluations and include 6 synthetic random graph families in the new version . Please refer to A2 in the common response , the updated paper , and the updated appendix . Q4 : Misleading use of the word `` statistics '' throughout the paper . Statistics are quantities that can be computed only from the data . The paper repeatedly refers to functionals of parameters as `` statistics '' . > A4 : We appreciate your rigorousness w.r.t.terminologies . For clarification , we use the term statistic to refer to some function of both data and weights , e.g. , the maximum node representations . Similar usage has become common in machine learning , e.g. , \u201c batch statistics \u201d such as the mean of hidden representations in batch normalization [ 1 ] are functions of both data and weights . But we agree that it is confusing as people typically map the terminology weights in machine learning to parameters in statistical inference . We modified our wording in the new version . > > [ 1 ] Ioffe , S. and Szegedy , C. , 2015 . Batch normalization : Accelerating deep network training by reducing internal covariate shift . In ICML.Q5 : What do you mean by : `` actual posterior distribution induced by the learning process may be very different from Gaussians '' ? PAC-Bayesian analysis is done for Gibbs classifiers and Neyshabur et al.provide a way to convert these convergence bounds for deterministic classifiers . The learning process does not induce a posterior distribution over weights ( assuming deterministic initialization and removing randomness like dropout ) . > A5 : Sorry for the confusion as we did not have space to expand the argument in detail in the submission . The results of Neyshabur et al.indeed provide a way to convert these convergence bounds for deterministic classifiers ( their result is actually a generalization of an earlier result by David McAllester ) . They achieve it by adding Gaussian perturbations on the learned weights of the deterministic model . > > > Here we refer to another stochastic view of a deterministic model where people in practice often randomly initialize weights following a prior distribution , e.g. , Gaussian or uniform , before learning . If we collect the learned weights returned by the learning process under different initial weights drawn from the prior distribution , then we can view them as drawn from a posterior distribution induced by the prior and the learning process . This posterior may be quite non-Gaussian , e.g. , they may be a multimodal distribution since the learned weights may land in different local minima with different initialized weights . Therefore , the Gaussian perturbation may hardly capture this phenomenon . > > > This view follows more closely to how people actually train deterministic models in practice . Therefore , it may be worthwhile to investigate whether one can develop a PAC-Bayes result under this stochastic view . Of course , there are many challenges , e.g. , the density of the induced distribution is unknown so that one can not obtain the KL divergence . Q6 : D is overloaded to denote both data distribution and diagonal degree matrix . > A6 : Thanks for pointing it out . We have corrected it in the new version ."}}