{"year": "2018", "forum": "SyX0IeWAW", "title": "META LEARNING SHARED HIERARCHIES", "decision": "Accept (Poster)", "meta_review": "This paper presents a fairly straightforward algorithm for learning a set of sub-controllers that can be re-used between tasks.  The development of these concepts in a relatively clear way is a nice contribution.  However, the real problem is how niche the setup is.  However, it's over the bar in general.", "reviews": [{"review_id": "SyX0IeWAW-0", "review_text": "Please see my detailed comments in the \"official comment\" The extensive revisions addressed most of my concerns Quality ====== The idea is interesting, the theory is hand-wavy at best (ADDRESSED but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (ADDRESSED). It is also unclear how much tuning is involved (ADDRESSED). Clarity ===== The paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (ADDRESSED) leaving many details up the the reader's best guess (ADDRESSED). Originality ========= The idea looks original. Significance ========== If it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution. Pros and Cons ============ + interesting idea - we do everything asynchronously and in parallel and it magically works (ADDRESSED) - many open questions / missing details (ADDRESSED)", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hey , thanks for the feedback . We \u2019 ve addressed some clarifications in the response to your official comment , titled `` Proposed Changes '' . We hope that these ideas clear up misunderstandings , and fill in details that may have been explained unclearly ."}, {"review_id": "SyX0IeWAW-1", "review_text": "This paper considers the reinforcement learning problem setup in which an agent must solve not one, but a set of tasks in some domain, in which the state space and action space are fixed. The authors consider the problem of learning a useful set of \u2018sub policies\u2019 that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. I found the paper to be generally well written and the key ideas easy to understand on first pass. The authors should be commended for this. Aside from a few minor grammatical issues (e.g. missing articles here and there), the writing cannot be too strongly faulted. The problem setup is of general interest to the community. Metalearning in the multitask setup seems to be gaining attention and is certainly a necessary step towards building rapidly adaptable agents. While the concepts were clearly introduced, I think the authors need to make, much more strongly, the case that the method is actually valuable. In that vein, I would have liked to see more work done on elucidating how this method works \u2018under the hood\u2019. For example, it is not at all clear how the number of sub policies affects performance (one would imagine that there is a clear trade off), nor how this number should be chosen. It seems obvious that this choice would also affect the subtle dynamics between holding the master policy constant while updating the sub policies and vice versa. While the authors briefly touch on some of these issues in the rationale section, I found these arguments largely unsubstantiated. Moreover, this leads to a number of unjustified hyper-parameters in the method which I suspect would affect the training catastrophically without significant fine-tuning. There are also obvious avenues to be followed to check/bolster the intuitions behind the method. By way of example, my sense is that the procedure described in the paper uncovers a set of sub policies that form a `good\u2019 cover for the task space - if so simply plotting out what they policies look like (or better yet how they adapt in time) would be very insightful (the rooms domain is perhaps a good candidate for this). While the key ideas are clearly articulated the practical value of the procedure is insufficiently motivated. The paper would benefit hugely from additional analysis.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Hey , appreciate the feedback . To address your concern about how performance depends on hyperparameters , we ran additional experiments comparing the effects of various parameter adjustments . See the graph ( https : //imgur.com/a/TLyQv ) , which we have added in Fig.9 on the current revision . ( Default parameters are a sub-policy count of 2 , and a warmup duration of 20 ) . As long as a few minimums are met ( at least 2 sub-policies ) , performance is not overly dependent on fine-tuned parameters . The parameters we describe in the paper can be seen as a \u201c baseline minimum \u201d of parameters to reach a strong solution on the various tasks . Regarding displaying the behavior of sub-policies , we show a decomposition of the three sub-policies discovered in the Maze task in Figure 6 : moving up , right , and down . We display how the policies adapt over time in our supplemental videos , linked on the first page ( https : //sites.google.com/site/mlshsupplementals/ , specifically https : //www.youtube.com/watch ? v=9nvjy9aJi50 ) ."}, {"review_id": "SyX0IeWAW-2", "review_text": "This paper proposes a novel hierarchical reinforcement learning method for a fairly particular setting. The setting is one where the agent must solve some task for many episodes in a sequence, after which the task will change and the process repeats. The proposed solution method splits the agent into two components, a master policy which is reset to random initial weights for each new task, and several sub-policies (motor primitives) that are selected between by the master policy every N steps and whose weights are not reset on task switches. The core idea is that the master policy is given a relatively easy learning task of selecting between useful motor primitives and this can be efficiently learned from scratch on each new task, whereas learning the motor primitives occurs slowly over many different tasks. To push this motivation into the learning process, the master policy is updated always but the sub-policies are only updated after an extended warmup period (called the joint-update or training period). This experiments include both small domains (moving to 2D goals and four-rooms) and more complex physics simulations (4-legged ants and humanoids). In both the simple and complex domains, the proposed method (MLSH) is able to robustly achieve good performance. This approach to obtaining complex structured behavior appears impressive despite the amount of temporal structure that must be provided to the method (the choice of N, the warmup period, and the joint-update period). Relying on the temporal structure for the hierarchy, and forcing the master policy to be relearned from scratch for each new task may be problematic in general, but this work shows that in some complex settings, a simple temporal decomposition may be sufficient to encourage the development of reusable motor primitives and to also enable quick learning of meta-policies over these motor-primitives. Moreover, the results show that these temporal hierarchies are helpful in these domains, as the corresponding non-hierarchical methods failed on the more challenging tasks. The paper could be improved in some places (e.g. unclear aliases of joint-update or training periods, describing how the parameters were chosen, and describing what kinds of sub-policies are learned in these domains when different parameter choices are made). ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the response . We \u2019 ll fix the typo of \u201c training period \u201d - > \u201c joint-update period \u201d in the next version . We \u2019 ll also clean up the intuition behind parameter choice ( see our response \u201c Analysis on Parameter Choice \u201d ) ."}], "0": {"review_id": "SyX0IeWAW-0", "review_text": "Please see my detailed comments in the \"official comment\" The extensive revisions addressed most of my concerns Quality ====== The idea is interesting, the theory is hand-wavy at best (ADDRESSED but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (ADDRESSED). It is also unclear how much tuning is involved (ADDRESSED). Clarity ===== The paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (ADDRESSED) leaving many details up the the reader's best guess (ADDRESSED). Originality ========= The idea looks original. Significance ========== If it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution. Pros and Cons ============ + interesting idea - we do everything asynchronously and in parallel and it magically works (ADDRESSED) - many open questions / missing details (ADDRESSED)", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hey , thanks for the feedback . We \u2019 ve addressed some clarifications in the response to your official comment , titled `` Proposed Changes '' . We hope that these ideas clear up misunderstandings , and fill in details that may have been explained unclearly ."}, "1": {"review_id": "SyX0IeWAW-1", "review_text": "This paper considers the reinforcement learning problem setup in which an agent must solve not one, but a set of tasks in some domain, in which the state space and action space are fixed. The authors consider the problem of learning a useful set of \u2018sub policies\u2019 that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. I found the paper to be generally well written and the key ideas easy to understand on first pass. The authors should be commended for this. Aside from a few minor grammatical issues (e.g. missing articles here and there), the writing cannot be too strongly faulted. The problem setup is of general interest to the community. Metalearning in the multitask setup seems to be gaining attention and is certainly a necessary step towards building rapidly adaptable agents. While the concepts were clearly introduced, I think the authors need to make, much more strongly, the case that the method is actually valuable. In that vein, I would have liked to see more work done on elucidating how this method works \u2018under the hood\u2019. For example, it is not at all clear how the number of sub policies affects performance (one would imagine that there is a clear trade off), nor how this number should be chosen. It seems obvious that this choice would also affect the subtle dynamics between holding the master policy constant while updating the sub policies and vice versa. While the authors briefly touch on some of these issues in the rationale section, I found these arguments largely unsubstantiated. Moreover, this leads to a number of unjustified hyper-parameters in the method which I suspect would affect the training catastrophically without significant fine-tuning. There are also obvious avenues to be followed to check/bolster the intuitions behind the method. By way of example, my sense is that the procedure described in the paper uncovers a set of sub policies that form a `good\u2019 cover for the task space - if so simply plotting out what they policies look like (or better yet how they adapt in time) would be very insightful (the rooms domain is perhaps a good candidate for this). While the key ideas are clearly articulated the practical value of the procedure is insufficiently motivated. The paper would benefit hugely from additional analysis.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Hey , appreciate the feedback . To address your concern about how performance depends on hyperparameters , we ran additional experiments comparing the effects of various parameter adjustments . See the graph ( https : //imgur.com/a/TLyQv ) , which we have added in Fig.9 on the current revision . ( Default parameters are a sub-policy count of 2 , and a warmup duration of 20 ) . As long as a few minimums are met ( at least 2 sub-policies ) , performance is not overly dependent on fine-tuned parameters . The parameters we describe in the paper can be seen as a \u201c baseline minimum \u201d of parameters to reach a strong solution on the various tasks . Regarding displaying the behavior of sub-policies , we show a decomposition of the three sub-policies discovered in the Maze task in Figure 6 : moving up , right , and down . We display how the policies adapt over time in our supplemental videos , linked on the first page ( https : //sites.google.com/site/mlshsupplementals/ , specifically https : //www.youtube.com/watch ? v=9nvjy9aJi50 ) ."}, "2": {"review_id": "SyX0IeWAW-2", "review_text": "This paper proposes a novel hierarchical reinforcement learning method for a fairly particular setting. The setting is one where the agent must solve some task for many episodes in a sequence, after which the task will change and the process repeats. The proposed solution method splits the agent into two components, a master policy which is reset to random initial weights for each new task, and several sub-policies (motor primitives) that are selected between by the master policy every N steps and whose weights are not reset on task switches. The core idea is that the master policy is given a relatively easy learning task of selecting between useful motor primitives and this can be efficiently learned from scratch on each new task, whereas learning the motor primitives occurs slowly over many different tasks. To push this motivation into the learning process, the master policy is updated always but the sub-policies are only updated after an extended warmup period (called the joint-update or training period). This experiments include both small domains (moving to 2D goals and four-rooms) and more complex physics simulations (4-legged ants and humanoids). In both the simple and complex domains, the proposed method (MLSH) is able to robustly achieve good performance. This approach to obtaining complex structured behavior appears impressive despite the amount of temporal structure that must be provided to the method (the choice of N, the warmup period, and the joint-update period). Relying on the temporal structure for the hierarchy, and forcing the master policy to be relearned from scratch for each new task may be problematic in general, but this work shows that in some complex settings, a simple temporal decomposition may be sufficient to encourage the development of reusable motor primitives and to also enable quick learning of meta-policies over these motor-primitives. Moreover, the results show that these temporal hierarchies are helpful in these domains, as the corresponding non-hierarchical methods failed on the more challenging tasks. The paper could be improved in some places (e.g. unclear aliases of joint-update or training periods, describing how the parameters were chosen, and describing what kinds of sub-policies are learned in these domains when different parameter choices are made). ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the response . We \u2019 ll fix the typo of \u201c training period \u201d - > \u201c joint-update period \u201d in the next version . We \u2019 ll also clean up the intuition behind parameter choice ( see our response \u201c Analysis on Parameter Choice \u201d ) ."}}