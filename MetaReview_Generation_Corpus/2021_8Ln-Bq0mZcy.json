{"year": "2021", "forum": "8Ln-Bq0mZcy", "title": "On the Critical Role of Conventions in Adaptive Human-AI Collaboration", "decision": "Accept (Poster)", "meta_review": "This paper proposes a new paradigm for learning to perform cooperative tasks with partners, which factors the problem into two components: how to perform the task and how to coordinate with the partner according to conventions. The setting is new and the reviewers are excited about the paper. A clear accept.", "reviews": [{"review_id": "8Ln-Bq0mZcy-0", "review_text": "This paper makes the observation that when performing cooperative tasks with partners , there are two components to learn : how to perform the task , and how to coordinate with the partner according to conventions . Therefore , it proposes to separate these two components via a modular architecture , which learns a task-specific action distribution that attempts to marginalize out all possible partner conventions , and a series of partner-specific action distributions . The agent 's own policy is the product of these two distributions . When coordinating with a new partner , the partner-specific component is learned from scratch using a pre-trained task-specific component , and vice versa . The paper goes after an ambitious and useful problem ( rapid adaptation to coordinating with novel partners on new tasks ) , and proposes a novel technique for doing so . A weakness is that the paper does not use reasonable baselines , and effectively compares only to ablations of their own model . Why not compare to a meta-learning technique ? Or compare to some of the existing SOTA methods for Hanabi ? In general the paper is well written , but it could be made significantly clearer by providing further details on how the partner action distributions g^p_i ( a|s ) are obtained . Given the explanation in the beginning of Section 4 , I was initially under the impression that these represented the partner 's policy distribution produced by its Q-values , or perhaps the partner 's actual action frequencies obtained from observing trajectories . However , it seems that the model is learned entirely end-to-end , and so these distributions actually represent how the agent 's own policy should be modified according to which partner it is playing with . Is this correct ? If so , this explanation should be added to the paper to make it more clear how the technique can apply beyond simple domains like the contextual bandit , in which agents must choose the * same * actions as the partner . The fact that the partner module must be re-initialized and learned from scratch for each new partner is a weakness of the method . Why not learn some type of partner embedding that would enable generalization to new partners at test time that use similar conventions to training partners ? The experiments section of the paper felt rushed and lacking in explanation compared to the first 6 pages . The clarity/impact could be enhanced by explaining the experiments in more detail . In particular , the block placing task is not explained ( do agents place blocks separately ? do they have to place a block together at the same time ? ) . Also , the need for `` hand-designed '' partners is not explained , nor is what they are hand-designed to do . Since the paper collects a human user study on conventions , why not test how well the trained models are able to coordinate with humans ? This would significantly enhance the impact of the paper . Other suggestions : - Figure 2 caption does not include the explanation that agents must choose the same action to get a reward - A legend should be added to Figures 7 , 8 , and 9 . - Figure 7 is interesting in that even without the Wasserstein distance penalty ( when lambda=0 ) , the Wasserstein distance to the ground truth marginal best response is still low , suggesting the model is learning some level of task-specific representation just due to the architecture . This could be explained further in the text . Edit : I have updated my score based on the new experiments added during the rebuttal process .", "rating": "7: Good paper, accept", "reply_text": "* * A weakness is that the paper does not use reasonable baselines , and effectively compares only to ablations of their own model . Why not compare to a meta-learning technique ? Or compare to some of the existing SOTA methods for Hanabi ? * * Thanks for the suggestion . We looked at first-order MAML ( FOMAML ) / Reptile , and have added them as baselines for comparison . We used the hyperparameters from [ On First-Order Meta-Learning Algorithms ] , with k=4 steps in each inner-loop and step-size 0.25 , where they showed that FOMAML has similar performance to MAML in the supervised learning setting . We added the results to the plots in Experiments , and the hyperparameters to the Appendix . In general FOMAML seem to be comparable to BaselineAgg ( which is equivalent to using k=1 steps in each inner loop ) . In some of the plots ( e.g.6a and 6d ) it appears to start off strong but slightly worsen after many steps , perhaps because it is optimizing to only \u201c look ahead \u201d a few ( k=4 ) number of steps . We are not aware of existing methods that target adaptation to new partners in Hanabi . Current SOTA approaches for Hanabi ( e.g.Hanabi-SAD ) rely on centralized training , which is not applicable to our setting . * * In general the paper is well written , but it could be made significantly clearer by providing further details on how the partner action distributions g^p_i ( a|s ) are obtained .... these distributions actually represent how the agent 's own policy should be modified according to which partner it is playing with . Is this correct ? If so , this explanation should be added to the paper to make it more clear how the technique can apply beyond simple domains like the contextual bandit , in which agents must choose the same actions as the partner . * * That is correct . Section 4 describes how the partners actions are determined . Roughly , a partner in our setting can be characterized as a tie-breaking function at states with symmetries ( as determined by the Q values ) . The g^p_i ( a|s ) , on the other hand , is indeed \u201c how the agent \u2019 s own policy should be modified according to which partner it is playing with \u201d ( i.e.how should the ego agent respond to the partner ) . As you point out , in the contextual bandit task , symmetries in the partner \u2019 s action space are equivalent to symmetries in the ego agent \u2019 s action space , but this is not true in general ( e.g.block game or hanabi ) . The distributions g^p_i ( a|s ) is targeting the latter case of breaking symmetries from the perspective of the ego agent \u2019 s action space , so our approach can handle blocks/hanabi tasks where the symmetries are not \u201c mirrored \u201d . * * The fact that the partner module must be re-initialized and learned from scratch for each new partner is a weakness of the method . Why not learn some type of partner embedding that would enable generalization to new partners at test time that use similar conventions to training partners ? * * We did have this idea in mind , which is why the partner modules take in the latents z as input instead of the state s directly , so that the task module can pass along only the features that are relevant to different conventions . We did not try more sophisticated methods of learning the partner embedding in the original version . Based on your suggestion , we added new experiments where we bottleneck the dimensionality of z to encourage the task module to learn better embeddings for \u201c generalization to new partners at test time \u201d . We think this is perhaps the simplest and more natural form of embedding , and that perhaps the range of human interactions can be modeled in low-dimensional space . We \u2019 ve added new curves for this low-dimensional z setting to Figure 6 and 9 . But , in general using the low-dimensional z on top of the task module regularization did not seem to do better than the task module regularization alone . It is likely that we need to impose additional structure in the latent space to learn a useful embedding , and this would be an interesting topic for future research ."}, {"review_id": "8Ln-Bq0mZcy-1", "review_text": "= On the Critical Role of Conventions in Adaptive Human-AI Collaboration = # # # # # Summary # # # # # This paper studies how artificial agents can be endowed with the human-like capability to , one the one hand , retain behaviors that best fit the task environment ( s ) when no equally good alternatives exist and , on the other , transfer arbitrary partner-specific conventions to other tasks . Addressing this challenge is important . Chiefly , it promises to improve the number of iterations needed to converge on optimal behavior for cases where analogous strategies were already converged on in a different task / with the same partner . This work proposes to achieve this by combining two separate learned representations . One for partner-specific behaviors . The other for the task itself . The latter module , reused across partners , ends up regulating behavior for cases where only one optimal actions exists . When multiple optimal actions exist there is some slack for players to explore and converge on an arbitrary optimal action . In this sense , agents are endowed with the ability to reuse optimal action policies that will work across agents as well as to reuse optimal partner-specific policies when faced with a context with multiple solutions . # # # # # Reasons for score # # # # # I ultimately decided for rejection . This work has many merits : the topic is very important ; it is of relevance to many fields ; the approach is sound and the experiments interesting . However , I fail to see how much this work advances our understanding of the interplay between partner-specific and task general behavior . The main reason is that , while sound and straightforward to follow , it leaves open a lot of crucial questions ( e.g. , How do we recognize what a task/context is ? Is the separation of the modules motivated ? If so , how ? What do we learn from the human experiment that we did n't before ? See `` Cons '' below for details ) . I 'm very happy to be convinced otherwise but I do n't think that these concerns can be addressed in the present submission . # # # # # Pros # # # # # + Interesting and important topic that is relevant to many fields + Technically sound approach and clear exposition . + Comprehensive experimentation . I really liked that the approach was put to the test across agents , human and artificial , and tasks ( Contexual Bandit , Block Placing ; Hanabi ) # # # # # Cons # # # # # - Goals : My main issue is that the goals of this study are unclear . This work would be greatly enhanced by clarifying what they are and what is ultimately achieved . I do not think its true or fair to state that , as the authors put it in the abstract , `` current approaches have not attempted to distinguish a task and the conventions used by a partner , and [ that ] more generally there has been little focus on leveraging conventions for adapting to new settings '' . To name just one of many examples , in the linguistics literature this distinction is standardly made and has long been studied ( e.g. , by Clark & Wilkes-Gibbs , 1986 ; Clark 1996 ; Hawkins et al.2017 , all cited in this work ) . This is also how semantics vs. pragmatics is defined in game-theoretic approaches to language use & dialog ( e.g. , Franke 2009 , `` Signal to Act '' ; or Brochhagen 2017 , `` Signalling under Uncertainty : Interpretative Alignment without a Common Prior '' ) , as well as how it is generally understood within Gricean pragmatics . In other words , the distinction has been made and rests on a long philosophical tradition . I therefore do n't think that taking this separation seriously alone is enough to motivate this investigation . - Further motivations : In a similar vein to the point above , there 's a lack of detail on what sets this work apart from previous investigations . For instance , the critique that `` [ ... ] '' all these frameworks [ ... ] quickly become intractable '' is not very strong in light of the existence of approximations and solutions for the models mentioned ( e.g. , Monroe 2018 's `` Learning in the Rational Speech Acts Model '' , which also uses neural networks to model rule-dependent behavior ) . - Clarification : In what sense are Train and Test different tasks ? ( Section 5 ) - Analysis : The experiment in Section 5.1 averages across participants . I 'd suggest looking at / reporting individual-level variation . These kinds of experiments usually vary a lot from individual to individual ( see , e.g. , Kanwal et al.2017 , `` Zipf \u2019 s Law of Abbreviation and the Principle of Least Effort '' ) . Population-level averages can therefore inadvertently hide or misrepresent what subjects are actually doing . - On page 4 , the authors state : `` we assume that behavior at different states are uncorrelated : a choice of action at one state tells us nothing about actions at other states . '' I take it that this assumption is made for simplicity 's sake . However , is n't this also an integral part of human-like abilities ? If I realize my partner 's behavior accords -- or does not accord -- to some conventions I had already established , I might as well behave accordingly . For instance , if I 'm playing a game of chess with someone I might match their level of expertise ( e.g. , avoid castling with a complete beginner ) ; and if I 'm speaking with someone at a conference , I might change my phrasing based on theirs based on what I believe to be their background to be . # # # # # Questions during rebuttal period # # # # # See cons above # # # # # Updated reviewer # # # # # The authors have addressed my main concerns satisfactorily , in a clear and concise matter . I have updated my recommendation to reflect this . # # # # # Minor comments # # # # # - Figure 6 has a different y-axis across plots . This is confusing at a fast glance , and makes the subplots ' comparison quite hard .", "rating": "7: Good paper, accept", "reply_text": "* * how much this work advances our understanding of the interplay between partner-specific and task general behavior . * * We agree that many works in the linguistics literature have studied the \u201c interplay between partner-specific and task general behavior \u201d . Our work is largely inspired by these works in a language setting , and we are expanding this to games without explicit communication , and to the deep-RL setting . To our knowledge , there has not been as much work considering the implicit communication setting , so we are bringing ideas of conventions from the linguistic literature to coordination games that don \u2019 t have an explicit communication channel . Moreover , our focus is on providing a concrete deep-RL approach to learning the two types of partner-specific and task-general behavior . * * How do we recognize what a task/context is ? * * In our problem formulation , the ego agent knows the identity of the task ( which task it is solving ) but does not know the exact reward function of the task . The agent observes the state ( context ) of the environment at each step . This is typically the setup used in similar problem settings such as meta-learning or multi-task learning . The bigger question of how to recognize when the task has changed and what constitutes a task in the real world is very interesting , and we would be curious to hear thoughts you may have on this . * * Is the separation of the modules motivated ? If so , how ? * * Intuitively , the task/rules determine the set of equally optimal solutions , and each partner separately chooses which solution based on the developed conventions . We consider this explicit factorization of tasks and partners in the problem setup in Section 4 , so we think that the problem of adapting to new partner and task combinations in this setting naturally motivates the separation of modules . We also believe the partner module should be \u201c on top \u201d of the task module for the following reasoning ( copied from our response to Reviewer 1 ) : We want the task module to \u2018 featurize \u2019 the raw observations into latent space z , so that the partner module has an easier learning task : for example , loosely speaking , the partner module only has to choose between equally optimal actions instead of all possible actions . For the same reason , we also think it makes sense to have the modules in this order ( instead of task module on top ) , since we want to first extract the optimal actions , and then second have the partners choose one of the optimal actions second . * * What do we learn from the human experiment that we did n't before ? * * Our intuition that coordinating with a partner on an earlier task will help us coordinate with the same partner on a new task relies \u201c on the hypothesis that our partners will carry over the same conventions developed on an earlier task \u201d ( section 5.1 ) . Since the users had no external communication , it is not clear to what extent this hypothesis holds true . Our human experiments show that when the Q-value is exactly the same ( in context C of Train/Test ) , they are generally able to carry over conventions . When the Q-values are different in context B of Train/Test , but they could conceivably have mapped Train action 3 to Test action 1 and Train action 4 to Test action 2 and carried over the conventions , but our users were generally not able to carry over their convention in this way . * * the goals of this study are unclear * * The goal of our work is to provide a concrete \u201c learning framework that teases apart rule-dependent representation from convention-dependent representation in a principled way \u201d ( from the abstract ) to enable faster adaptation to new tasks and partners . We focus on this goal throughout the paper , in the environment setup ( section 4 ) where there is a factorization of partner and task , the architecture ( section 5 ) that separates representations with modular networks , and in the human study where we test if the hypothesis of carrying over convention-dependent representation is true . In the experiments we test on a variety of tasks and environments that require coordination with implicit communication , where we show how our method performs when adapting to new partners and tasks . We hope this helps clarify the goal of our work ."}, {"review_id": "8Ln-Bq0mZcy-2", "review_text": "This paper proposes that in human-AI collaboration using deep neural nets , the AI agents we train should separate learning the _rules_ of the environment from the _conventions_ used to coordinate with humans in that environment . It proposes a simple method to do so : learn a single task-specific module that is always used , as well as many partner-specific modules that are used with specific partners . Intuitively , when trained with multiple partners , the task-specific module should learn heuristics that work across partners ( the environment-specific rules ) while the partner-specific model should learn personalized heuristics for each partner ( the conventions ) . They further incentivize this by regularizing the task-specific module towards the average of the partner policies . Quality : The one qualm I have is that the environments studied are relatively simple ( though even 1-color Hanabi is a fairly challenging coordination problem ) . Other than that the paper is high quality . I especially appreciated the user study . Clarity : I found the paper reasonably clear , though some parts took some time to understand . Originality : To my knowledge , this is the first paper exploring the distinction between rules and conventions within the deep RL paradigm , and it shows good results both in simulation and with real humans in a user study ( albeit in a very simple toy domain ) . Significance : Human-AI collaboration is clearly important and significant , and the application of deep RL to human-AI collaboration has grown in the last 2-3 years . This paper extends this field with an important contribution . The main weakness of the approach I see is that it doesn \u2019 t have an obvious way to handle the fact that humans will typically _adapt_ to whatever policy the robot plays . This may not happen in the simple environments considered in this paper , but definitely does happen in larger environments . Perhaps this technique would work anyway : arguably , an adaptive human just means that the convention changes , and simply continuing to train the partner module could be enough for the robot to adapt to this change in the human \u2019 s convention . Regardless , I think even the contribution of how to deal with multiple different non-adaptive humans is significant and relevant to ICLR . Questions for the authors : 1 . Why does the partner module operate \u201c on top of \u201d the task module ? Why not instead have both modules take in the state as an input and produce a probability distribution over actions , that are then multiplied together ? 2.How might this extend to collaboration with adaptive humans ? Typos / nitpicks : The phrase \u201c ego agent \u201d was confusing to me , and I think it wasn \u2019 t explained anywhere ? I did eventually figure it out though .", "rating": "7: Good paper, accept", "reply_text": "* * The one qualm I have is that the environments studied are relatively simple ( though even 1-color Hanabi is a fairly challenging coordination problem ) . Other than that the paper is high quality . I especially appreciated the user study . * * We agree that it would be interesting to try on even more environments . However we would like to emphasize that \u201c even 1-color Hanabi is a fairly challenging coordination problem \u201d . The block placing task is also challenging too -- it \u2019 s difficult to coordinate when there \u2019 s no external communication . * * The main weakness of the approach I see is that it doesn \u2019 t have an obvious way to handle the fact that humans will typically adapt to whatever policy the robot plays .... perhaps this technique would work anyway ... * * Yes , we currently do not focus on humans adapting to the robot policy . This would be interesting to study , but since training with human-in-the-loop is too costly , a big question is how to realistically model human adaptation . We think this is an intriguing next step for future work . We are currently looking into studying the problem of conventions with adaptive AI partners ( where data collection is not a problem ) , and we 're working on developing techniques that could be applied in such settings with the goal of moving to adaptive human-AI interaction in the future . We also agree that a modular architecture may also be the right approach for adaptive partners too . * * Why does the partner module operate \u201c on top of \u201d the task module ? Why not instead have both modules take in the state as an input and produce a probability distribution over actions , that are then multiplied together ? * * We want the task module to \u2018 featurize \u2019 the raw observations into latent space z , so that the partner module has an easier learning task : for example , loosely speaking , the partner module only has to choose between equally optimal actions instead of all possible actions . For the same reason , we also think it makes sense to have the modules in this order ( instead of task module on top ) , since we want to first extract the optimal actions , and then second have the partners choose one of the optimal actions second . * * How might this extend to collaboration with adaptive humans ? * * Perhaps fine-tuning the partner module to adapt to adaptive partners would be a potential approach . We would need a good grasp on how we think humans adapt , which is an interesting question on its own . * * The phrase \u201c ego agent \u201d was confusing to me , and I think it wasn \u2019 t explained anywhere ? I did eventually figure it out though . * * Thank you for bringing this up . We have clarified it in Section 3 ."}, {"review_id": "8Ln-Bq0mZcy-3", "review_text": "The paper proposes an interesting model to study multi-agent interactions , in uncertain environments . In s nutshell , the model proposed consists of a MDP ( Finite state , action , horizon ) and two players playing simultaneously ( the turn-by-turn model can be subsumed by the simultaneous move model as stated in the paper ) . In the absence of learning , the finite MDP has an optimal solution . The key contribution of the paper is to focus on instances when the optimal solution is not unique . In a two-player model , this requires symmetry breaking in order for a sample path of the MDP to track the optimal trajectory . The above is when the MDP and rewards are all well known . The setting in the paper concerns a learning situation where some or all of the components of the MDP is unknown . In this case , the agents must learn the MDP , while breaking symmetry ( coordinate ) in converging on a sample path closest to an optimal one . The problem , is very interesting and the authors propose a nice formulation to study these questions . I have a few high-level suggestions to the authors . 1.The model description is mathematically imprecise . Are the agents aiming to optimize the total reward collected , in presence of unknown model and partner ? What information about the partners are known to the agent ? ( Is the agent distribution common information ? ) Are the partners assumed to know the underlying MDP , or they are also simultaneously learning ? If the partner are also simultaneously learning , is their `` state of knowledge '' at the beginning known to the ego agent ? I understand that having a robust solution to all of the above problems is perhaps too hard . Nevertheless , clarifying the precise setup mathematically will greatly aid the reader . 2.The human experiment , were the users able to communicate to each other in any way ? Were they total strangers to each other , or known acquaintances ? Clearly specifying the `` conditions of the experiment '' , can help take the results in context . Overall , I believe the paper is attempting to study an interesting ( and hard ) problem . But my ( low ) rating is based on the clarity of the presentation .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Are the agents aiming to optimize the total reward collected , in presence of unknown model and partner ? What information about the partners are known to the agent ? ( Is the agent distribution common information ? ) * * Our agent is optimizing the total reward , and knows the identity of the task and partner . Our agent does not know the exact MDP of the task or the policy of the partner , but collects experience with ( same task / different partner ) or ( different task / same partner ) combinations during training time . The agent distribution is not explicitly known but we have access to samples ( i.e.training partners ) , and testing partners are drawn ( i.i.d ) from the same distribution . We have clarified this in Section 5 . * * Are the partners assumed to know the underlying MDP , or they are also simultaneously learning ? If the partner are also simultaneously learning , is their `` state of knowledge '' at the beginning known to the ego agent ? * * No , the partners are not also simultaneously learning , they are assumed to have converged to a fixed convention . The partners do not know the underlying MDP when learning to converge to a convention but have access to samples from the environment / can explore the environment . The \u2018 state of knowledge \u2019 of the partners is not known to the ego agent ; the ego agent only observes the state , joint action , and rewards at the end of each episode . * * The human experiment , were the users able to communicate to each other in any way ? Were they total strangers to each other , or known acquaintances ? Clearly specifying the `` conditions of the experiment '' , can help take the results in context . * * The users were not able to communicate to each other in any way . The only signal they received was the revelation of their partner \u2019 s actions and the reward at the end of each try . The users were not total strangers -- they were typically pairs of students or staff . Some more details : the experiment was conducted using an online interface . Our users were placed in different rooms before we gave them the task instructions , so there was no way to coordinate \u201c outside \u201d of the game . We have updated the PDF in Section 5.1 to clarify this ."}], "0": {"review_id": "8Ln-Bq0mZcy-0", "review_text": "This paper makes the observation that when performing cooperative tasks with partners , there are two components to learn : how to perform the task , and how to coordinate with the partner according to conventions . Therefore , it proposes to separate these two components via a modular architecture , which learns a task-specific action distribution that attempts to marginalize out all possible partner conventions , and a series of partner-specific action distributions . The agent 's own policy is the product of these two distributions . When coordinating with a new partner , the partner-specific component is learned from scratch using a pre-trained task-specific component , and vice versa . The paper goes after an ambitious and useful problem ( rapid adaptation to coordinating with novel partners on new tasks ) , and proposes a novel technique for doing so . A weakness is that the paper does not use reasonable baselines , and effectively compares only to ablations of their own model . Why not compare to a meta-learning technique ? Or compare to some of the existing SOTA methods for Hanabi ? In general the paper is well written , but it could be made significantly clearer by providing further details on how the partner action distributions g^p_i ( a|s ) are obtained . Given the explanation in the beginning of Section 4 , I was initially under the impression that these represented the partner 's policy distribution produced by its Q-values , or perhaps the partner 's actual action frequencies obtained from observing trajectories . However , it seems that the model is learned entirely end-to-end , and so these distributions actually represent how the agent 's own policy should be modified according to which partner it is playing with . Is this correct ? If so , this explanation should be added to the paper to make it more clear how the technique can apply beyond simple domains like the contextual bandit , in which agents must choose the * same * actions as the partner . The fact that the partner module must be re-initialized and learned from scratch for each new partner is a weakness of the method . Why not learn some type of partner embedding that would enable generalization to new partners at test time that use similar conventions to training partners ? The experiments section of the paper felt rushed and lacking in explanation compared to the first 6 pages . The clarity/impact could be enhanced by explaining the experiments in more detail . In particular , the block placing task is not explained ( do agents place blocks separately ? do they have to place a block together at the same time ? ) . Also , the need for `` hand-designed '' partners is not explained , nor is what they are hand-designed to do . Since the paper collects a human user study on conventions , why not test how well the trained models are able to coordinate with humans ? This would significantly enhance the impact of the paper . Other suggestions : - Figure 2 caption does not include the explanation that agents must choose the same action to get a reward - A legend should be added to Figures 7 , 8 , and 9 . - Figure 7 is interesting in that even without the Wasserstein distance penalty ( when lambda=0 ) , the Wasserstein distance to the ground truth marginal best response is still low , suggesting the model is learning some level of task-specific representation just due to the architecture . This could be explained further in the text . Edit : I have updated my score based on the new experiments added during the rebuttal process .", "rating": "7: Good paper, accept", "reply_text": "* * A weakness is that the paper does not use reasonable baselines , and effectively compares only to ablations of their own model . Why not compare to a meta-learning technique ? Or compare to some of the existing SOTA methods for Hanabi ? * * Thanks for the suggestion . We looked at first-order MAML ( FOMAML ) / Reptile , and have added them as baselines for comparison . We used the hyperparameters from [ On First-Order Meta-Learning Algorithms ] , with k=4 steps in each inner-loop and step-size 0.25 , where they showed that FOMAML has similar performance to MAML in the supervised learning setting . We added the results to the plots in Experiments , and the hyperparameters to the Appendix . In general FOMAML seem to be comparable to BaselineAgg ( which is equivalent to using k=1 steps in each inner loop ) . In some of the plots ( e.g.6a and 6d ) it appears to start off strong but slightly worsen after many steps , perhaps because it is optimizing to only \u201c look ahead \u201d a few ( k=4 ) number of steps . We are not aware of existing methods that target adaptation to new partners in Hanabi . Current SOTA approaches for Hanabi ( e.g.Hanabi-SAD ) rely on centralized training , which is not applicable to our setting . * * In general the paper is well written , but it could be made significantly clearer by providing further details on how the partner action distributions g^p_i ( a|s ) are obtained .... these distributions actually represent how the agent 's own policy should be modified according to which partner it is playing with . Is this correct ? If so , this explanation should be added to the paper to make it more clear how the technique can apply beyond simple domains like the contextual bandit , in which agents must choose the same actions as the partner . * * That is correct . Section 4 describes how the partners actions are determined . Roughly , a partner in our setting can be characterized as a tie-breaking function at states with symmetries ( as determined by the Q values ) . The g^p_i ( a|s ) , on the other hand , is indeed \u201c how the agent \u2019 s own policy should be modified according to which partner it is playing with \u201d ( i.e.how should the ego agent respond to the partner ) . As you point out , in the contextual bandit task , symmetries in the partner \u2019 s action space are equivalent to symmetries in the ego agent \u2019 s action space , but this is not true in general ( e.g.block game or hanabi ) . The distributions g^p_i ( a|s ) is targeting the latter case of breaking symmetries from the perspective of the ego agent \u2019 s action space , so our approach can handle blocks/hanabi tasks where the symmetries are not \u201c mirrored \u201d . * * The fact that the partner module must be re-initialized and learned from scratch for each new partner is a weakness of the method . Why not learn some type of partner embedding that would enable generalization to new partners at test time that use similar conventions to training partners ? * * We did have this idea in mind , which is why the partner modules take in the latents z as input instead of the state s directly , so that the task module can pass along only the features that are relevant to different conventions . We did not try more sophisticated methods of learning the partner embedding in the original version . Based on your suggestion , we added new experiments where we bottleneck the dimensionality of z to encourage the task module to learn better embeddings for \u201c generalization to new partners at test time \u201d . We think this is perhaps the simplest and more natural form of embedding , and that perhaps the range of human interactions can be modeled in low-dimensional space . We \u2019 ve added new curves for this low-dimensional z setting to Figure 6 and 9 . But , in general using the low-dimensional z on top of the task module regularization did not seem to do better than the task module regularization alone . It is likely that we need to impose additional structure in the latent space to learn a useful embedding , and this would be an interesting topic for future research ."}, "1": {"review_id": "8Ln-Bq0mZcy-1", "review_text": "= On the Critical Role of Conventions in Adaptive Human-AI Collaboration = # # # # # Summary # # # # # This paper studies how artificial agents can be endowed with the human-like capability to , one the one hand , retain behaviors that best fit the task environment ( s ) when no equally good alternatives exist and , on the other , transfer arbitrary partner-specific conventions to other tasks . Addressing this challenge is important . Chiefly , it promises to improve the number of iterations needed to converge on optimal behavior for cases where analogous strategies were already converged on in a different task / with the same partner . This work proposes to achieve this by combining two separate learned representations . One for partner-specific behaviors . The other for the task itself . The latter module , reused across partners , ends up regulating behavior for cases where only one optimal actions exists . When multiple optimal actions exist there is some slack for players to explore and converge on an arbitrary optimal action . In this sense , agents are endowed with the ability to reuse optimal action policies that will work across agents as well as to reuse optimal partner-specific policies when faced with a context with multiple solutions . # # # # # Reasons for score # # # # # I ultimately decided for rejection . This work has many merits : the topic is very important ; it is of relevance to many fields ; the approach is sound and the experiments interesting . However , I fail to see how much this work advances our understanding of the interplay between partner-specific and task general behavior . The main reason is that , while sound and straightforward to follow , it leaves open a lot of crucial questions ( e.g. , How do we recognize what a task/context is ? Is the separation of the modules motivated ? If so , how ? What do we learn from the human experiment that we did n't before ? See `` Cons '' below for details ) . I 'm very happy to be convinced otherwise but I do n't think that these concerns can be addressed in the present submission . # # # # # Pros # # # # # + Interesting and important topic that is relevant to many fields + Technically sound approach and clear exposition . + Comprehensive experimentation . I really liked that the approach was put to the test across agents , human and artificial , and tasks ( Contexual Bandit , Block Placing ; Hanabi ) # # # # # Cons # # # # # - Goals : My main issue is that the goals of this study are unclear . This work would be greatly enhanced by clarifying what they are and what is ultimately achieved . I do not think its true or fair to state that , as the authors put it in the abstract , `` current approaches have not attempted to distinguish a task and the conventions used by a partner , and [ that ] more generally there has been little focus on leveraging conventions for adapting to new settings '' . To name just one of many examples , in the linguistics literature this distinction is standardly made and has long been studied ( e.g. , by Clark & Wilkes-Gibbs , 1986 ; Clark 1996 ; Hawkins et al.2017 , all cited in this work ) . This is also how semantics vs. pragmatics is defined in game-theoretic approaches to language use & dialog ( e.g. , Franke 2009 , `` Signal to Act '' ; or Brochhagen 2017 , `` Signalling under Uncertainty : Interpretative Alignment without a Common Prior '' ) , as well as how it is generally understood within Gricean pragmatics . In other words , the distinction has been made and rests on a long philosophical tradition . I therefore do n't think that taking this separation seriously alone is enough to motivate this investigation . - Further motivations : In a similar vein to the point above , there 's a lack of detail on what sets this work apart from previous investigations . For instance , the critique that `` [ ... ] '' all these frameworks [ ... ] quickly become intractable '' is not very strong in light of the existence of approximations and solutions for the models mentioned ( e.g. , Monroe 2018 's `` Learning in the Rational Speech Acts Model '' , which also uses neural networks to model rule-dependent behavior ) . - Clarification : In what sense are Train and Test different tasks ? ( Section 5 ) - Analysis : The experiment in Section 5.1 averages across participants . I 'd suggest looking at / reporting individual-level variation . These kinds of experiments usually vary a lot from individual to individual ( see , e.g. , Kanwal et al.2017 , `` Zipf \u2019 s Law of Abbreviation and the Principle of Least Effort '' ) . Population-level averages can therefore inadvertently hide or misrepresent what subjects are actually doing . - On page 4 , the authors state : `` we assume that behavior at different states are uncorrelated : a choice of action at one state tells us nothing about actions at other states . '' I take it that this assumption is made for simplicity 's sake . However , is n't this also an integral part of human-like abilities ? If I realize my partner 's behavior accords -- or does not accord -- to some conventions I had already established , I might as well behave accordingly . For instance , if I 'm playing a game of chess with someone I might match their level of expertise ( e.g. , avoid castling with a complete beginner ) ; and if I 'm speaking with someone at a conference , I might change my phrasing based on theirs based on what I believe to be their background to be . # # # # # Questions during rebuttal period # # # # # See cons above # # # # # Updated reviewer # # # # # The authors have addressed my main concerns satisfactorily , in a clear and concise matter . I have updated my recommendation to reflect this . # # # # # Minor comments # # # # # - Figure 6 has a different y-axis across plots . This is confusing at a fast glance , and makes the subplots ' comparison quite hard .", "rating": "7: Good paper, accept", "reply_text": "* * how much this work advances our understanding of the interplay between partner-specific and task general behavior . * * We agree that many works in the linguistics literature have studied the \u201c interplay between partner-specific and task general behavior \u201d . Our work is largely inspired by these works in a language setting , and we are expanding this to games without explicit communication , and to the deep-RL setting . To our knowledge , there has not been as much work considering the implicit communication setting , so we are bringing ideas of conventions from the linguistic literature to coordination games that don \u2019 t have an explicit communication channel . Moreover , our focus is on providing a concrete deep-RL approach to learning the two types of partner-specific and task-general behavior . * * How do we recognize what a task/context is ? * * In our problem formulation , the ego agent knows the identity of the task ( which task it is solving ) but does not know the exact reward function of the task . The agent observes the state ( context ) of the environment at each step . This is typically the setup used in similar problem settings such as meta-learning or multi-task learning . The bigger question of how to recognize when the task has changed and what constitutes a task in the real world is very interesting , and we would be curious to hear thoughts you may have on this . * * Is the separation of the modules motivated ? If so , how ? * * Intuitively , the task/rules determine the set of equally optimal solutions , and each partner separately chooses which solution based on the developed conventions . We consider this explicit factorization of tasks and partners in the problem setup in Section 4 , so we think that the problem of adapting to new partner and task combinations in this setting naturally motivates the separation of modules . We also believe the partner module should be \u201c on top \u201d of the task module for the following reasoning ( copied from our response to Reviewer 1 ) : We want the task module to \u2018 featurize \u2019 the raw observations into latent space z , so that the partner module has an easier learning task : for example , loosely speaking , the partner module only has to choose between equally optimal actions instead of all possible actions . For the same reason , we also think it makes sense to have the modules in this order ( instead of task module on top ) , since we want to first extract the optimal actions , and then second have the partners choose one of the optimal actions second . * * What do we learn from the human experiment that we did n't before ? * * Our intuition that coordinating with a partner on an earlier task will help us coordinate with the same partner on a new task relies \u201c on the hypothesis that our partners will carry over the same conventions developed on an earlier task \u201d ( section 5.1 ) . Since the users had no external communication , it is not clear to what extent this hypothesis holds true . Our human experiments show that when the Q-value is exactly the same ( in context C of Train/Test ) , they are generally able to carry over conventions . When the Q-values are different in context B of Train/Test , but they could conceivably have mapped Train action 3 to Test action 1 and Train action 4 to Test action 2 and carried over the conventions , but our users were generally not able to carry over their convention in this way . * * the goals of this study are unclear * * The goal of our work is to provide a concrete \u201c learning framework that teases apart rule-dependent representation from convention-dependent representation in a principled way \u201d ( from the abstract ) to enable faster adaptation to new tasks and partners . We focus on this goal throughout the paper , in the environment setup ( section 4 ) where there is a factorization of partner and task , the architecture ( section 5 ) that separates representations with modular networks , and in the human study where we test if the hypothesis of carrying over convention-dependent representation is true . In the experiments we test on a variety of tasks and environments that require coordination with implicit communication , where we show how our method performs when adapting to new partners and tasks . We hope this helps clarify the goal of our work ."}, "2": {"review_id": "8Ln-Bq0mZcy-2", "review_text": "This paper proposes that in human-AI collaboration using deep neural nets , the AI agents we train should separate learning the _rules_ of the environment from the _conventions_ used to coordinate with humans in that environment . It proposes a simple method to do so : learn a single task-specific module that is always used , as well as many partner-specific modules that are used with specific partners . Intuitively , when trained with multiple partners , the task-specific module should learn heuristics that work across partners ( the environment-specific rules ) while the partner-specific model should learn personalized heuristics for each partner ( the conventions ) . They further incentivize this by regularizing the task-specific module towards the average of the partner policies . Quality : The one qualm I have is that the environments studied are relatively simple ( though even 1-color Hanabi is a fairly challenging coordination problem ) . Other than that the paper is high quality . I especially appreciated the user study . Clarity : I found the paper reasonably clear , though some parts took some time to understand . Originality : To my knowledge , this is the first paper exploring the distinction between rules and conventions within the deep RL paradigm , and it shows good results both in simulation and with real humans in a user study ( albeit in a very simple toy domain ) . Significance : Human-AI collaboration is clearly important and significant , and the application of deep RL to human-AI collaboration has grown in the last 2-3 years . This paper extends this field with an important contribution . The main weakness of the approach I see is that it doesn \u2019 t have an obvious way to handle the fact that humans will typically _adapt_ to whatever policy the robot plays . This may not happen in the simple environments considered in this paper , but definitely does happen in larger environments . Perhaps this technique would work anyway : arguably , an adaptive human just means that the convention changes , and simply continuing to train the partner module could be enough for the robot to adapt to this change in the human \u2019 s convention . Regardless , I think even the contribution of how to deal with multiple different non-adaptive humans is significant and relevant to ICLR . Questions for the authors : 1 . Why does the partner module operate \u201c on top of \u201d the task module ? Why not instead have both modules take in the state as an input and produce a probability distribution over actions , that are then multiplied together ? 2.How might this extend to collaboration with adaptive humans ? Typos / nitpicks : The phrase \u201c ego agent \u201d was confusing to me , and I think it wasn \u2019 t explained anywhere ? I did eventually figure it out though .", "rating": "7: Good paper, accept", "reply_text": "* * The one qualm I have is that the environments studied are relatively simple ( though even 1-color Hanabi is a fairly challenging coordination problem ) . Other than that the paper is high quality . I especially appreciated the user study . * * We agree that it would be interesting to try on even more environments . However we would like to emphasize that \u201c even 1-color Hanabi is a fairly challenging coordination problem \u201d . The block placing task is also challenging too -- it \u2019 s difficult to coordinate when there \u2019 s no external communication . * * The main weakness of the approach I see is that it doesn \u2019 t have an obvious way to handle the fact that humans will typically adapt to whatever policy the robot plays .... perhaps this technique would work anyway ... * * Yes , we currently do not focus on humans adapting to the robot policy . This would be interesting to study , but since training with human-in-the-loop is too costly , a big question is how to realistically model human adaptation . We think this is an intriguing next step for future work . We are currently looking into studying the problem of conventions with adaptive AI partners ( where data collection is not a problem ) , and we 're working on developing techniques that could be applied in such settings with the goal of moving to adaptive human-AI interaction in the future . We also agree that a modular architecture may also be the right approach for adaptive partners too . * * Why does the partner module operate \u201c on top of \u201d the task module ? Why not instead have both modules take in the state as an input and produce a probability distribution over actions , that are then multiplied together ? * * We want the task module to \u2018 featurize \u2019 the raw observations into latent space z , so that the partner module has an easier learning task : for example , loosely speaking , the partner module only has to choose between equally optimal actions instead of all possible actions . For the same reason , we also think it makes sense to have the modules in this order ( instead of task module on top ) , since we want to first extract the optimal actions , and then second have the partners choose one of the optimal actions second . * * How might this extend to collaboration with adaptive humans ? * * Perhaps fine-tuning the partner module to adapt to adaptive partners would be a potential approach . We would need a good grasp on how we think humans adapt , which is an interesting question on its own . * * The phrase \u201c ego agent \u201d was confusing to me , and I think it wasn \u2019 t explained anywhere ? I did eventually figure it out though . * * Thank you for bringing this up . We have clarified it in Section 3 ."}, "3": {"review_id": "8Ln-Bq0mZcy-3", "review_text": "The paper proposes an interesting model to study multi-agent interactions , in uncertain environments . In s nutshell , the model proposed consists of a MDP ( Finite state , action , horizon ) and two players playing simultaneously ( the turn-by-turn model can be subsumed by the simultaneous move model as stated in the paper ) . In the absence of learning , the finite MDP has an optimal solution . The key contribution of the paper is to focus on instances when the optimal solution is not unique . In a two-player model , this requires symmetry breaking in order for a sample path of the MDP to track the optimal trajectory . The above is when the MDP and rewards are all well known . The setting in the paper concerns a learning situation where some or all of the components of the MDP is unknown . In this case , the agents must learn the MDP , while breaking symmetry ( coordinate ) in converging on a sample path closest to an optimal one . The problem , is very interesting and the authors propose a nice formulation to study these questions . I have a few high-level suggestions to the authors . 1.The model description is mathematically imprecise . Are the agents aiming to optimize the total reward collected , in presence of unknown model and partner ? What information about the partners are known to the agent ? ( Is the agent distribution common information ? ) Are the partners assumed to know the underlying MDP , or they are also simultaneously learning ? If the partner are also simultaneously learning , is their `` state of knowledge '' at the beginning known to the ego agent ? I understand that having a robust solution to all of the above problems is perhaps too hard . Nevertheless , clarifying the precise setup mathematically will greatly aid the reader . 2.The human experiment , were the users able to communicate to each other in any way ? Were they total strangers to each other , or known acquaintances ? Clearly specifying the `` conditions of the experiment '' , can help take the results in context . Overall , I believe the paper is attempting to study an interesting ( and hard ) problem . But my ( low ) rating is based on the clarity of the presentation .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Are the agents aiming to optimize the total reward collected , in presence of unknown model and partner ? What information about the partners are known to the agent ? ( Is the agent distribution common information ? ) * * Our agent is optimizing the total reward , and knows the identity of the task and partner . Our agent does not know the exact MDP of the task or the policy of the partner , but collects experience with ( same task / different partner ) or ( different task / same partner ) combinations during training time . The agent distribution is not explicitly known but we have access to samples ( i.e.training partners ) , and testing partners are drawn ( i.i.d ) from the same distribution . We have clarified this in Section 5 . * * Are the partners assumed to know the underlying MDP , or they are also simultaneously learning ? If the partner are also simultaneously learning , is their `` state of knowledge '' at the beginning known to the ego agent ? * * No , the partners are not also simultaneously learning , they are assumed to have converged to a fixed convention . The partners do not know the underlying MDP when learning to converge to a convention but have access to samples from the environment / can explore the environment . The \u2018 state of knowledge \u2019 of the partners is not known to the ego agent ; the ego agent only observes the state , joint action , and rewards at the end of each episode . * * The human experiment , were the users able to communicate to each other in any way ? Were they total strangers to each other , or known acquaintances ? Clearly specifying the `` conditions of the experiment '' , can help take the results in context . * * The users were not able to communicate to each other in any way . The only signal they received was the revelation of their partner \u2019 s actions and the reward at the end of each try . The users were not total strangers -- they were typically pairs of students or staff . Some more details : the experiment was conducted using an online interface . Our users were placed in different rooms before we gave them the task instructions , so there was no way to coordinate \u201c outside \u201d of the game . We have updated the PDF in Section 5.1 to clarify this ."}}