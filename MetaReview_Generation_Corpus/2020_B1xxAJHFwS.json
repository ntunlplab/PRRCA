{"year": "2020", "forum": "B1xxAJHFwS", "title": "A Finite-Time Analysis of  Q-Learning with Neural Network Function Approximation", "decision": "Reject", "meta_review": "This was an extremely difficult paper to decide, as it attracted significant commentary (and controversy) that led to non-trivial corrections in the results.  One of the main criticisms is that the work is an incremental combination of existing results.  A potentially bigger concern is that of correctness: the main convergence rate was changed from 1/T to 1/sqrt{T} during the rebuttal and revision process.  Such a change is not trivial and essentially proves the initial submission was incorrect.  In general, it is not prudent to accept a hastily revised theory paper without a proper assessment of correctness in its modified form.  Therefore, I think it would be premature to accept this paper without a full review cycle that assessed the revised form.  There also appear to be technical challenges from the discussion that remain unaddressed.  Any resubmission will also have to highlight significance and make a stronger case for the novelty of the results.", "reviews": [{"review_id": "B1xxAJHFwS-0", "review_text": "[Summary] This paper studies the convergence of Q-Learning when a wide multi-layer network in the Neural Tangent Kernel (NTK) regime is used as the function approximator. Concretely, it shows that Q-learning converges with rate O(1/T) with data sampled from a single trajectory (non-i.i.d.) of an infinite-horizon MDP + a certain (stationary) exploration policy. [Pros] The results in this paper improve upon recent work on the same topic. It is able to handle multi-layer neural nets as opposed to two-layer, prove a faster rate O(1/T), and handle non-iid data (as opposed to iid data where (s,a,r,s\u2019) are sampled freshly from the beginning at each step.) The paper is generally well-written. The results and proof sketches are well presented and easy to follow. The proof seems correct to me from my check, including the indicator issue pointed out in the comments which I think can be easily fixed (by explicitly writing out the indicator and thus the Cauchy-Schwarz will still apply.) [Cons] The result in this paper seems more or less like a direct combination of existing techniques, and thus may be limited in bringing in new techniques / messages. Key technical bottlenecks that are assumed out in prior work are still assumed out in this paper with potentially different forms but essentially the same thing. More concretely, the proof of the main theorem (Thm 5.4) seems to rely critically on Lemmas 6.2 and 6.3, both of which are rather straightforward adaptations of prior work: Lemma 6.2 (concentration of stochastic gradients on linearized problem): Seems to me like almost the same as [Bhandari et al. 2018], expect that now the network is an affine function---rather than a linear function---of \\theta, where the additional constant term f(\\theta_0; s, a) depends on (s, a). Lemma 6.3 (good landscape of linearized problem): Comparing with prior work (Theorem 6.3, Cai et al. 2019), this Lemma works by directly assuming out the property of the arg-max operator in Assumption 5.3, which has a slightly different form from, but is essentially the same thing as (Assumption 6.1, Cai et al. 2019). To be fair, the paper has to deal with the linearization error of a multi-layer net, which is dealt with in Lemma 6.1 and should be valued. But still I tend to think the above adaptations are rather straightforward and technically not quite novel. [Potential improvements] I would like to hear more from the authors about the technical novelty in this paper, specifically how Lemma 6.1 - 6.3 compare with prior work. I would be willing to improve my evaluation if this can be addressed. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your insightful comments . We would like to clarify that our paper is not a direct combination of existing results . We highlight that our main contributions are to provide ( 1 ) the first finite-time analysis of Q-learning with multi-layer neural network function approximation , and ( 2 ) the first finite-time analysis of neural Q-learning with non-i.i.d.data assumption . We agree with the reviewer 's comment that our analysis is built on previous work of Bhandari et al . ( 2018 ) and Cai et al . ( 2019 ) .However , the analysis for deep Q-learning with non-i.i.d.data is by no means trivial , as is shown in our following responses to your comments on each technical lemma . Overall , our proof of Theorem 5.4 was decomposed into three parts ( the bounds of terms $ I_1 $ , $ I_2 $ and $ I_3 $ in equation ( 6.5 ) of our paper ) , which are bounded using Lemmas 6.1 , 6.2 and 6.3 respectively . In Lemma 6.1 , we upper bound the difference between $ \\mathbf { g } _t $ and $ \\mathbf { m } _t $ , where $ \\mathbf { g } _t $ is defined based on the Bellman residual error and the multi-layer neural network function , and $ \\mathbf { m } _t $ is defined on the same Bellman residual error but with a linearized function at the initial point $ \\theta_0 $ . This lemma requires a careful calculation of the bound on the temporal difference $ \\Delta_t ( s_t , a_t , s_ { t+1 } ; \\theta_t ) $ and the linearization error , which is not presented in previous work . In Lemma 6.2 , we characterize the bias of the stochastic gradient $ \\mathbf { m } _t ( \\cdot ) $ and its idealized version $ \\overline { \\mathbf { m } } ( \\cdot ) $ whose definition does not depend on the Markov data trajectory . As we mentioned at the beginning of the proof of Lemma 6.2 , our proof was indeed adapted from that in Bhandari et al . ( 2018 ) .However , there are a few differences between their proof and ours . First , $ \\mathbf { m } _t ( \\cdot ) $ and $ \\overline { \\mathbf { m } } ( \\cdot ) $ in our paper are defined based on a neural network function and its gradient , and thus the Lipschitz condition and the gradient norm bound are not trivial to derive . Second , the proof in Bhandari et al . ( 2018 ) is for TD learning , which does not directly apply to neural Q-learning . In Lemma 6.3 , we used a slightly different assumption ( Assumption 5.3 in the revision ) from that of Cai et al . ( 2018 ) .It is worth noting that our Assumption 5.3 follows the same idea of Melo et al . ( 2008 ) , Zou et al . ( 2019 ) and Chen et al . ( 2019 ) , which can be interpreted as the advantage of the greedy policy over the learning policy . In contrast , Assumption 6.1 in Cai et al . ( 2019 ) directly imposes the condition on the difference between action value functions at two different policies . Therefore , our proof is based on bounding the eigenvalue of the difference between two covariance matrices ( i.e. , $ \\hat\\Sigma_ { \\pi } $ and $ \\hat\\Sigma_ { \\pi } ^ * ( \\theta ) $ ) , which is different from that of Cai et al . ( 2019 ) ."}, {"review_id": "B1xxAJHFwS-1", "review_text": "This paper provides a finite-time analysis of a neural Q-learning algorithm, where the data are generated from a Markov decision process and the action-value function is approximated by a deep ReLU neural network. When the neural function is sufficiently over-parameterized, the O(1/T) convergence rate is attained. Pros: This paper makes theoretical contribution to the understanding of neural Q-learning. This is an important but difficult task. The recent finite-time analysis on Q-learning either assumes a linear function approximation or an i.i.d. setting in the neural Q-learning. This paper makes a first attempt to study the neural Q-learning with Markovian noise. Overall, this paper is very easy to follow. Cons: In spite of its theoretical contributions, this paper has a few major issues. 1. The projection step relies on a parameter $\\omega$ which is unknown in practice. In theorem, $\\omega = C m^{-1/2}$ for some unknown constant $C$. It would be of practical interests to seek other proof techniques to avoid such projection step. For instance, Srikant and Yang (2019) and Chen et al. (2019) removed this projection step in the finite-time analysis of Q-learning with a linear function approximation. 2. Assumption 5.3 is problematic for the considered neural Q-learning setting. The matrix $\\hat{\\Sigma}_{\\pi}$ is of a very large dimension in the order of $O(m^2) * O(m^2)$ where the width of the neural network $m$ is assumed to diverge in the Theorem 5.4 for the over-parameterization purpose. Given the diverging dimension scenario, it is problematic to ensure Assumption 5.3. Moreover, it is unclear how to verify this condition in practice. In the literature, Melo et al. (2008) and Zou et al. (2019b) assumed a similar condition, which is OK because in the Q-learning with linear function approximation, this matrix reduces to the covariance matrix of the feature vector. 3. The error rate in Theorem 5.4 is an increasing function of the layer $L$ in DNN, which is counterintuitive. A typically practical observation is that a larger $L$ is better. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your helpful comments . We address them point by point as follows . Q1 : `` The projection step relies on a parameter $ \\omega $ which is unknown in practice . It would be of practical interests to seek other proof techniques to avoid such projection step . ... '' A1 : The unknown constant $ C $ is often treated as a hyperparameter and can be tuned using grid search in practice . We agree that it would be interesting to explore the possibility of removing the projection step as what Srikant and Ying ( 2019 ) and Chen et al . ( 2019 ) did in the linear approximation setting . We have discussed their methods in the related work section . However , adapting their proof techniques would completely change our algorithm and our current analysis framework . So we will investigate the projection-free version of our algorithm in the future work . Q2 : `` Assumption 5.3 is problematic for the considered neural Q-learning setting . The matrix $ \\hat { \\Sigma } _ { \\pi } $ is of a very large dimension in the order of $ O ( m^2 ) * O ( m^2 ) $ where the width of the neural network $ m $ is assumed to diverge in Theorem 5.4 for the over-parameterization purpose . ... '' A2 : In our previous submission , we did not require $ m $ to go to infinity and thus the matrix $ \\hat\\Sigma_ { \\pi } $ is well defined and positive definite . However , the minimum eigenvalue of $ \\hat\\Sigma_ { \\pi } $ could be very small and hence $ \\alpha $ ( the minimum eigenvalue of $ \\hat\\Sigma_ { \\pi } -\\gamma^2\\hat\\Sigma_ { \\pi } ^ * ( \\theta ) $ ) would be a very small quantity which can slow down the convergence rate . Based on this observation , we agree that the previous assumption is too restrictive and we have removed the assumption on the minimum eigenvalue in the revision . In particular , we relax the previous assumption to a much milder one where we only require the difference between the two matrices ( $ \\hat\\Sigma_ { \\pi } $ and $ \\hat\\Sigma_ { \\pi } ^ * ( \\theta ) $ ) to be positive definite ( see Assumption 5.3 in the revision ) . Under this milder assumption , we proved that neural Q-learning converges with an $ O ( 1/\\sqrt { T } ) $ rate . This result matches the convergence rate of neural Q-learning in Cai et al . ( 2019 ) where only a two-layer neural network approximator is used and the data are assumed to be i.i.d.generated . Q3 : `` The error rate in Theorem 5.4 is an increasing function of the layer $ L $ in DNN , which is counterintuitive . A typically practical observation is that a larger $ L $ is better . '' A3 : The dependence on $ L $ in the error rate in Theorem 5.4 comes from Lemma 6.1 which characterizes the approximation error between the linearized gradient $ \\mathbf { m } _t $ and the gradient term $ \\mathbf { g } _t $ . The dependency of $ L $ can be removed by choosing a smaller $ \\omega=C_0m^ { -1/2 } L^ { -9/4 } $ . Please see the updated Theorem 5.4 in the revision ."}, {"review_id": "B1xxAJHFwS-2", "review_text": "This paper introduces a finite time analysis of Q-learning with neural network function approximators across multiple layers and no iid assumption. [Pros] + Provides a novel way to analyze Q learning with nn function approximators that can be applied to other algorithms (notably in my mind, TD in actor critic where iid assumptions are often violated). [Cons] + The novelty is a bit unclear other than the non-iid assumption. We note that modern Q-learning tends to use batching so doesn't require much of an iid assumption anyways, but this allows for more robust proofs in TD settings with non-iid training. + The paper was a bit dense and hard to follow, we suggest reducing p.8 to have more discussion with references to proofs in the Appendix as in Chen2019. + As the authors admit in open commentary, there is a mistake to be fixed which needs to be reviewed before acceptance. I think there is value to this work, however, would require seeing the change to assess a revision. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive comments . We address your questions as follows . Q1 : `` The novelty is a bit unclear other than the non-iid assumption . We note that modern Q-learning tends to use batching so does n't require much of an iid assumption anyways , but this allows for more robust proofs in TD settings with non-iid training . '' A1 : Existing work on neural Q-learning ( Cai et al , 2019 ) requires to resample a new pair of data $ ( s , a , s ' ) $ at every iteration from the initial data distribution . This is not efficient in practice since one step along the trajectory may not give a good prediction of the policy . In contrast , our paper study the case where data $ ( s_t , a_t , s_ { t+1 } ) $ is drawn from a consecutive trajectory generated by the learning policy . Apart from the non-i.i.d.data generation , another contribution of our paper is to study the convergence of Q-learning with multi-layer neural network approximation . The extension from the two layer case in Cai et al . ( 2019 ) to our multi-layer case is not easy since the linearization error can not be calculated directly . Q2 : `` The paper was a bit dense and hard to follow , we suggest reducing p.8 to have more discussion with references to proofs in the Appendix as in Chen2019 . '' A2 : Thank you for the suggestion . We have added additional discussions and more details of the proof in Section 6 to make the proof easier to follow . In order to make the proof coherent , we did not divide the proof into several parts and move some parts of the proof to the appendix . Please let us know if you have any further suggestion . Q3 : `` As the authors admit in open commentary , there is a mistake to be fixed which needs to be reviewed before acceptance . I think there is value to this work , however , would require seeing the change to assess a revision . '' A3 : We have fixed the problem of the indicator function used in the proof of Lemma 6.3 . In particular , we chose to modify the definition of $ b_ { \\max } $ in ( 5.6 ) to be $ b_ { \\max } ( \\theta ) =\\arg\\max_ { b\\in\\mathcal { A } } |\\langle\\nabla_ { \\theta } f ( \\theta ; s , b ) , \\theta\\rangle| $ which is similar to the definition used in Chen et al . ( 2019 ) ( Note that their paper is for linear function approximation and thus $ b_ { \\max } ( \\theta ) =\\arg\\max_ { b\\in\\mathcal { A } } |\\phi ( s , b ) ^ { \\top } \\theta| $ ) . This does not change the result of Lemma 6.3 . See page 5 and pages 17-18 of the revision for the details ."}], "0": {"review_id": "B1xxAJHFwS-0", "review_text": "[Summary] This paper studies the convergence of Q-Learning when a wide multi-layer network in the Neural Tangent Kernel (NTK) regime is used as the function approximator. Concretely, it shows that Q-learning converges with rate O(1/T) with data sampled from a single trajectory (non-i.i.d.) of an infinite-horizon MDP + a certain (stationary) exploration policy. [Pros] The results in this paper improve upon recent work on the same topic. It is able to handle multi-layer neural nets as opposed to two-layer, prove a faster rate O(1/T), and handle non-iid data (as opposed to iid data where (s,a,r,s\u2019) are sampled freshly from the beginning at each step.) The paper is generally well-written. The results and proof sketches are well presented and easy to follow. The proof seems correct to me from my check, including the indicator issue pointed out in the comments which I think can be easily fixed (by explicitly writing out the indicator and thus the Cauchy-Schwarz will still apply.) [Cons] The result in this paper seems more or less like a direct combination of existing techniques, and thus may be limited in bringing in new techniques / messages. Key technical bottlenecks that are assumed out in prior work are still assumed out in this paper with potentially different forms but essentially the same thing. More concretely, the proof of the main theorem (Thm 5.4) seems to rely critically on Lemmas 6.2 and 6.3, both of which are rather straightforward adaptations of prior work: Lemma 6.2 (concentration of stochastic gradients on linearized problem): Seems to me like almost the same as [Bhandari et al. 2018], expect that now the network is an affine function---rather than a linear function---of \\theta, where the additional constant term f(\\theta_0; s, a) depends on (s, a). Lemma 6.3 (good landscape of linearized problem): Comparing with prior work (Theorem 6.3, Cai et al. 2019), this Lemma works by directly assuming out the property of the arg-max operator in Assumption 5.3, which has a slightly different form from, but is essentially the same thing as (Assumption 6.1, Cai et al. 2019). To be fair, the paper has to deal with the linearization error of a multi-layer net, which is dealt with in Lemma 6.1 and should be valued. But still I tend to think the above adaptations are rather straightforward and technically not quite novel. [Potential improvements] I would like to hear more from the authors about the technical novelty in this paper, specifically how Lemma 6.1 - 6.3 compare with prior work. I would be willing to improve my evaluation if this can be addressed. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your insightful comments . We would like to clarify that our paper is not a direct combination of existing results . We highlight that our main contributions are to provide ( 1 ) the first finite-time analysis of Q-learning with multi-layer neural network function approximation , and ( 2 ) the first finite-time analysis of neural Q-learning with non-i.i.d.data assumption . We agree with the reviewer 's comment that our analysis is built on previous work of Bhandari et al . ( 2018 ) and Cai et al . ( 2019 ) .However , the analysis for deep Q-learning with non-i.i.d.data is by no means trivial , as is shown in our following responses to your comments on each technical lemma . Overall , our proof of Theorem 5.4 was decomposed into three parts ( the bounds of terms $ I_1 $ , $ I_2 $ and $ I_3 $ in equation ( 6.5 ) of our paper ) , which are bounded using Lemmas 6.1 , 6.2 and 6.3 respectively . In Lemma 6.1 , we upper bound the difference between $ \\mathbf { g } _t $ and $ \\mathbf { m } _t $ , where $ \\mathbf { g } _t $ is defined based on the Bellman residual error and the multi-layer neural network function , and $ \\mathbf { m } _t $ is defined on the same Bellman residual error but with a linearized function at the initial point $ \\theta_0 $ . This lemma requires a careful calculation of the bound on the temporal difference $ \\Delta_t ( s_t , a_t , s_ { t+1 } ; \\theta_t ) $ and the linearization error , which is not presented in previous work . In Lemma 6.2 , we characterize the bias of the stochastic gradient $ \\mathbf { m } _t ( \\cdot ) $ and its idealized version $ \\overline { \\mathbf { m } } ( \\cdot ) $ whose definition does not depend on the Markov data trajectory . As we mentioned at the beginning of the proof of Lemma 6.2 , our proof was indeed adapted from that in Bhandari et al . ( 2018 ) .However , there are a few differences between their proof and ours . First , $ \\mathbf { m } _t ( \\cdot ) $ and $ \\overline { \\mathbf { m } } ( \\cdot ) $ in our paper are defined based on a neural network function and its gradient , and thus the Lipschitz condition and the gradient norm bound are not trivial to derive . Second , the proof in Bhandari et al . ( 2018 ) is for TD learning , which does not directly apply to neural Q-learning . In Lemma 6.3 , we used a slightly different assumption ( Assumption 5.3 in the revision ) from that of Cai et al . ( 2018 ) .It is worth noting that our Assumption 5.3 follows the same idea of Melo et al . ( 2008 ) , Zou et al . ( 2019 ) and Chen et al . ( 2019 ) , which can be interpreted as the advantage of the greedy policy over the learning policy . In contrast , Assumption 6.1 in Cai et al . ( 2019 ) directly imposes the condition on the difference between action value functions at two different policies . Therefore , our proof is based on bounding the eigenvalue of the difference between two covariance matrices ( i.e. , $ \\hat\\Sigma_ { \\pi } $ and $ \\hat\\Sigma_ { \\pi } ^ * ( \\theta ) $ ) , which is different from that of Cai et al . ( 2019 ) ."}, "1": {"review_id": "B1xxAJHFwS-1", "review_text": "This paper provides a finite-time analysis of a neural Q-learning algorithm, where the data are generated from a Markov decision process and the action-value function is approximated by a deep ReLU neural network. When the neural function is sufficiently over-parameterized, the O(1/T) convergence rate is attained. Pros: This paper makes theoretical contribution to the understanding of neural Q-learning. This is an important but difficult task. The recent finite-time analysis on Q-learning either assumes a linear function approximation or an i.i.d. setting in the neural Q-learning. This paper makes a first attempt to study the neural Q-learning with Markovian noise. Overall, this paper is very easy to follow. Cons: In spite of its theoretical contributions, this paper has a few major issues. 1. The projection step relies on a parameter $\\omega$ which is unknown in practice. In theorem, $\\omega = C m^{-1/2}$ for some unknown constant $C$. It would be of practical interests to seek other proof techniques to avoid such projection step. For instance, Srikant and Yang (2019) and Chen et al. (2019) removed this projection step in the finite-time analysis of Q-learning with a linear function approximation. 2. Assumption 5.3 is problematic for the considered neural Q-learning setting. The matrix $\\hat{\\Sigma}_{\\pi}$ is of a very large dimension in the order of $O(m^2) * O(m^2)$ where the width of the neural network $m$ is assumed to diverge in the Theorem 5.4 for the over-parameterization purpose. Given the diverging dimension scenario, it is problematic to ensure Assumption 5.3. Moreover, it is unclear how to verify this condition in practice. In the literature, Melo et al. (2008) and Zou et al. (2019b) assumed a similar condition, which is OK because in the Q-learning with linear function approximation, this matrix reduces to the covariance matrix of the feature vector. 3. The error rate in Theorem 5.4 is an increasing function of the layer $L$ in DNN, which is counterintuitive. A typically practical observation is that a larger $L$ is better. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your helpful comments . We address them point by point as follows . Q1 : `` The projection step relies on a parameter $ \\omega $ which is unknown in practice . It would be of practical interests to seek other proof techniques to avoid such projection step . ... '' A1 : The unknown constant $ C $ is often treated as a hyperparameter and can be tuned using grid search in practice . We agree that it would be interesting to explore the possibility of removing the projection step as what Srikant and Ying ( 2019 ) and Chen et al . ( 2019 ) did in the linear approximation setting . We have discussed their methods in the related work section . However , adapting their proof techniques would completely change our algorithm and our current analysis framework . So we will investigate the projection-free version of our algorithm in the future work . Q2 : `` Assumption 5.3 is problematic for the considered neural Q-learning setting . The matrix $ \\hat { \\Sigma } _ { \\pi } $ is of a very large dimension in the order of $ O ( m^2 ) * O ( m^2 ) $ where the width of the neural network $ m $ is assumed to diverge in Theorem 5.4 for the over-parameterization purpose . ... '' A2 : In our previous submission , we did not require $ m $ to go to infinity and thus the matrix $ \\hat\\Sigma_ { \\pi } $ is well defined and positive definite . However , the minimum eigenvalue of $ \\hat\\Sigma_ { \\pi } $ could be very small and hence $ \\alpha $ ( the minimum eigenvalue of $ \\hat\\Sigma_ { \\pi } -\\gamma^2\\hat\\Sigma_ { \\pi } ^ * ( \\theta ) $ ) would be a very small quantity which can slow down the convergence rate . Based on this observation , we agree that the previous assumption is too restrictive and we have removed the assumption on the minimum eigenvalue in the revision . In particular , we relax the previous assumption to a much milder one where we only require the difference between the two matrices ( $ \\hat\\Sigma_ { \\pi } $ and $ \\hat\\Sigma_ { \\pi } ^ * ( \\theta ) $ ) to be positive definite ( see Assumption 5.3 in the revision ) . Under this milder assumption , we proved that neural Q-learning converges with an $ O ( 1/\\sqrt { T } ) $ rate . This result matches the convergence rate of neural Q-learning in Cai et al . ( 2019 ) where only a two-layer neural network approximator is used and the data are assumed to be i.i.d.generated . Q3 : `` The error rate in Theorem 5.4 is an increasing function of the layer $ L $ in DNN , which is counterintuitive . A typically practical observation is that a larger $ L $ is better . '' A3 : The dependence on $ L $ in the error rate in Theorem 5.4 comes from Lemma 6.1 which characterizes the approximation error between the linearized gradient $ \\mathbf { m } _t $ and the gradient term $ \\mathbf { g } _t $ . The dependency of $ L $ can be removed by choosing a smaller $ \\omega=C_0m^ { -1/2 } L^ { -9/4 } $ . Please see the updated Theorem 5.4 in the revision ."}, "2": {"review_id": "B1xxAJHFwS-2", "review_text": "This paper introduces a finite time analysis of Q-learning with neural network function approximators across multiple layers and no iid assumption. [Pros] + Provides a novel way to analyze Q learning with nn function approximators that can be applied to other algorithms (notably in my mind, TD in actor critic where iid assumptions are often violated). [Cons] + The novelty is a bit unclear other than the non-iid assumption. We note that modern Q-learning tends to use batching so doesn't require much of an iid assumption anyways, but this allows for more robust proofs in TD settings with non-iid training. + The paper was a bit dense and hard to follow, we suggest reducing p.8 to have more discussion with references to proofs in the Appendix as in Chen2019. + As the authors admit in open commentary, there is a mistake to be fixed which needs to be reviewed before acceptance. I think there is value to this work, however, would require seeing the change to assess a revision. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive comments . We address your questions as follows . Q1 : `` The novelty is a bit unclear other than the non-iid assumption . We note that modern Q-learning tends to use batching so does n't require much of an iid assumption anyways , but this allows for more robust proofs in TD settings with non-iid training . '' A1 : Existing work on neural Q-learning ( Cai et al , 2019 ) requires to resample a new pair of data $ ( s , a , s ' ) $ at every iteration from the initial data distribution . This is not efficient in practice since one step along the trajectory may not give a good prediction of the policy . In contrast , our paper study the case where data $ ( s_t , a_t , s_ { t+1 } ) $ is drawn from a consecutive trajectory generated by the learning policy . Apart from the non-i.i.d.data generation , another contribution of our paper is to study the convergence of Q-learning with multi-layer neural network approximation . The extension from the two layer case in Cai et al . ( 2019 ) to our multi-layer case is not easy since the linearization error can not be calculated directly . Q2 : `` The paper was a bit dense and hard to follow , we suggest reducing p.8 to have more discussion with references to proofs in the Appendix as in Chen2019 . '' A2 : Thank you for the suggestion . We have added additional discussions and more details of the proof in Section 6 to make the proof easier to follow . In order to make the proof coherent , we did not divide the proof into several parts and move some parts of the proof to the appendix . Please let us know if you have any further suggestion . Q3 : `` As the authors admit in open commentary , there is a mistake to be fixed which needs to be reviewed before acceptance . I think there is value to this work , however , would require seeing the change to assess a revision . '' A3 : We have fixed the problem of the indicator function used in the proof of Lemma 6.3 . In particular , we chose to modify the definition of $ b_ { \\max } $ in ( 5.6 ) to be $ b_ { \\max } ( \\theta ) =\\arg\\max_ { b\\in\\mathcal { A } } |\\langle\\nabla_ { \\theta } f ( \\theta ; s , b ) , \\theta\\rangle| $ which is similar to the definition used in Chen et al . ( 2019 ) ( Note that their paper is for linear function approximation and thus $ b_ { \\max } ( \\theta ) =\\arg\\max_ { b\\in\\mathcal { A } } |\\phi ( s , b ) ^ { \\top } \\theta| $ ) . This does not change the result of Lemma 6.3 . See page 5 and pages 17-18 of the revision for the details ."}}