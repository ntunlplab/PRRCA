{"year": "2019", "forum": "HkxLXnAcFQ", "title": "A Closer Look at Few-shot Classification", "decision": "Accept (Poster)", "meta_review": "This paper provides a number of interesting experiments for few-shot learning using the CUB and miniImagenet datasets. One of the especially intriguing experiments is the analysis of backbone depth in the architecture, as it relates to few-shot performance. The strong performance of the baseline and baseline++ are quite surprising. Overall the reviewers agree that this paper raises a number of questions about current few-shot learning approaches, especially how they relate to architecture and dataset characteristics.\n\nA few minor comments:\n- In table 1, matching nets are mistakenly attributed to Ravi and Larochelle. Should be Vinyals et al.\n- The notation for cosine similarity in section 3.2 is odd. It looks like you\u2019re computing some cosine function of two vectors which doesn\u2019t make sense. Please clarify this.\n- There are a few results that were promised after the revision deadline, please be sure to include these in the final draft.\n", "reviews": [{"review_id": "HkxLXnAcFQ-0", "review_text": "The paper tried to propose a systematic/consistent way for evaluating meta-learning algorithms. I believe this is a great direction of research as the meta-learning community is growing quickly. However, my question is if a relatively simple modification could improve the baselines, are there simple modifications available to other meta-learning algorithms being investigated? If the other algorithms are not as good as they claimed, can you give any insights on why and what to improve?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments ! Our responses are as follow : Q1 : If a relatively simple modification could improve the baselines , are there simple modifications available to other meta-learning algorithms being investigated ? A1 : The simple modification we made for the baseline approach is to replace the softmax layer with a distance-based classifier . However , among other meta-learning algorithms , only the MAML method is applicable to this modification . Both ProtoNet and MatchingNet already use distance-based classifier in their algorithm . RelationNet has its own relation module so is not applicable for this modification . While MAML could adopt this strategy , we did not include it into our experiment since our primary goal is not to improve one specific method . Q2 : If the other algorithms are not as good as they claimed , can you give any insights on why and what to improve ? A2 : Meta-learning for few-shot classification algorithms are not as good as they claimed because of the following two aspects : First , in the CUB setting , the gap among each algorithm diminished when using a deeper backbone . That is , with a deeper feature backbone , the improvement from different meta-learning algorithm become less significant . Our results suggest that both deeper backbones and meta-learning algorithms both aim to reduce intra-class variation for improving few-classification accuracy . Consequently , when intra-class variation has been dramatically reduced using a deeper backbone , the contribution from meta-learning becomes less significant . Second , in the CUB - > mini-ImageNet setting where a larger domain shift exists , the Baseline method outperforms all meta-learning algorithms . That is , existing meta-learning algorithms are not robust to larger domain shift . As discussed in section 4.4 , while meta-learning methods learn to learn from the support set during the meta-training stage , all of the base support sets are still within the same dataset . Thus , these algorithms did not learn how to learn from a support set with large domain shift . With our results , we encourage the community to tackle the challenge of potential domain shifts in the context of few-shot learning . We will release the source code and evaluation setting that will facilitate future research directions ."}, {"review_id": "HkxLXnAcFQ-1", "review_text": "This paper gives a nice overview of existing works on few-shot learning. It groups them into some intuitive categories and meanwhile distills a common framework (Figure 2) employed by the methods. Moreover, the authors selected four of them, along with two baselines, to experimentally compare their performances under a cleaned experiment protocol. The experiments cover three few-shot learning scenarios respectively for generic object recognition, fine-grained classification, and cross-domain adaptation. While I do *not* think the third scenario is \u201cmore practical\u201d, it is certainly nice to have it included in the experiments. The experiment setup is unfortunately questionable. Since there is a validation set, one should use it to determine the free parameters (e.g., the number of epochs, learning rates, etc.). However, it seems like the same set of free parameters are used for different methods, making the comparison unfair because this set may favor some methods and yet hurt the others. The results of RelationNet are missing in Table 4. Another concern is that the same number of novel classes is used in the training and the testing stage. A more practical application of the learned meta model is to use it to handle different testing scenarios. There could be five novel classes in one scenario, 10 novel classes in another, and 100 in the third, etc. The number of labeled examples per class may also vary from one testing scenario to anther. It is misleading by the following: \u201cVery recently, Motiian et al. (2017) addresses the few-shot domain adaptation problem.\u201d There are a few variations in domain adaptation (DA). The learner has access to the fully labeled source domain and a small set of labeled target examples in supervised DA, to the source domain, a couple of labeled target examples, and many unlabeled target examples in semi-supervised DA, and to the source domain and many unlabeled target data points in the unsupervised DA. These have been studied long before (Motiian et al., 2017), for instance the works of Saenko et al. (2010) and Gong et al. (2013). [ref] Saenko K, Kulis B, Fritz M, Darrell T. Adapting visual category models to new domains. InEuropean conference on computer vision 2010 Sep 5 (pp. 213-226). Springer, Berlin, Heidelberg. [ref] Gong B, Grauman K, Sha F. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. InInternational Conference on Machine Learning 2013 Feb 13 (pp. 222-230). Overall, the paper is well written and may serve as a nice survey of existing works on few-shot learning. The unified experiment setup can facilitate the future research for fair comparisons, along with the three testing scenarios. However, I have some concerns as above about the experiment setups and hence also the conclusions. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q3 : Another concern is that the same number of novel classes is used in the training and the testing stage . A more practical application of the learned meta model is to use it to handle different testing scenarios . A3 : Thanks for pointing this out . As suggested , we conduct the experiments of 5-way meta-training and N-way meta-testing ( where we vary the number of N to be 5 , 10 , and 20 ) to examine the effect of handling testing scenarios that are different from training . We compare the methods Baseline , Baseline++ , MatchingNet , ProtoNet , and RelationNet . Note that we are unable to apply the MAML method as MAML learns the initialization for the classifier and can thus only be updated to classify the same number of classes . We show the experimental results on mini-ImageNet with 5-shot meta-training as follows . Backbone : Conv4 5-way test 10-way test 20-way test Baseline 62.53 % +- 0.69 % 46.44 % +- 0.41 % 32.27 % +- 0.24 % Baseline++ 66.43 % +- 0.63 % * 52.26 % +- 0.40 % * * 38.03 % +- 0.24 % * MatchingNet 63.48 % +- 0.66 % 47.61 % +- 0.44 % 33.97 % +- 0.24 % ProtoNet 64.24 % +- 0.68 % 48.77 % +- 0.45 % 34.58 % +- 0.23 % RelationNet * 66.60 % +- 0.69 % * 47.77 % +- 0.43 % 33.72 % +- 0.22 % Backbone : ResNet18 5-way test 10-way test 20-way test Baseline 74.27 % +- 0.63 % 55.00 % +- 0.46 % 42.03 % +- 0.25 % Baseline++ * 75.68 % +- 0.63 % * * 63.40 % +- 0.44 % * * 50.85 % +- 0.25 % * MatchingNet 68.88 % +- 0.69 % 52.27 % +- 0.46 % 36.78 % +- 0.25 % ProtoNet 73.68 % +- 0.65 % 59.22 % +- 0.44 % 44.96 % +- 0.26 % RelationNet 69.83 % +- 0.68 % 53.88 % +- 0.48 % 39.17 % +- 0.25 % Our results show that for classification with a larger-way ( e.g. , 10 or 20-way ) in the meta-testing stage , the proposed Baseline++ compares favorably against other methods in both shallow or deeper backbone settings . We attribute the results to two reasons . 1 ) To perform well in a larger N-way classification setting , one needs to further reduce the intra-class variation to avoid misclassification . Thus , in both shallow and deeper backbone settings , Baseline++ has better performance than Baseline . 2 ) As meta-learning algorithms were trained to perform 5-way classification in the meta-training stage , the performance of these algorithms may drop significantly when increasing the N-way in the meta-testing stage because the tasks of 10-way or 20-way classification are harder than that of 5-way classification . One may address this issue by performing a larger N-way classification in the meta-training stage ( as suggested in [ Snell et al.NIPS 2017 ] ) . However , this may encounter the issue of memory constraint . For example , to perform a 20-way classification with 5 support images and 15 query images in each class , we need to fit a batch size of 400 ( 20 x ( 5 + 15 ) ) that must fit in the GPUs . Without special hardware parallelization , the large batch size may prevent us from training models with deeper backbones such as ResNet . We have include the result in the appendix of the revised paper . Q4 : It is misleading by the following : \u201c Very recently , Motiian et al . ( 2017 ) addresses the few-shot domain adaptation problem. \u201d ... A4 : Thanks for the correction . Indeed , both Saenko et al.Gong et al.address the supervised domain adaptation problem with only a few labeled instances prior to [ Motiian et al. , NIPS 2017 ] . On the other hand , we would like to point out another research direction . Very recently , the method in [ Dong et al.ECML-PKDD 2018 ] addresses the few-shot problem where both the domain * and * the categories change . This work is more related to our setting , as we also consider novel category accuracy in few-shot classification under domain differences . We have corrected the statement in the revised paper ."}, {"review_id": "HkxLXnAcFQ-2", "review_text": "There are a few things I like about the paper. Firstly, it makes interesting observations about the evaluation of the few-shot learning approaches, e.g. the underestimated baselines, and compares multiple methods in the same conditions. In fact, one of the reasons for accepting this paper would be to get a unified and, hopefully, well-written implementation of those methods. Secondly, I like the domain shift experiments, but I have the following question. The description of the CUB says that there is an overlap between CUB and ImageNet. Is there an overlap between CUB and mini-ImageNet? If so, then domain shift experiments might be too optimistic or even then it is not a big deal? One thing I don\u2019t like is that, in my opinion, the paper includes much redundant information which could go to the appendix in order to not weary the reader. For instance, everything related to Table 1. There is also some overlap between Section 2 and 3.3, while MAML, for instance, is still not well explained. Also, tables with too many numbers are difficult to read, e.g. Table 4. ---- Other notes ----- Many of the few-shot learning papers use Omniglot, so I think it would be a valuable addition to the appendix. Moreover, there exists a cross-domain scenario with Omniglot-> MNIST which I would also like to see in the appendix. In the Matching Nets paper, there is a good baseline classifier based on k-NNs. Do you know how does that one compares to Baseline and Baseline++ models if used with the same architecture for the feature extractor? The conclusion from the network depth experiments is that \u201cgaps among different methods diminish as the backbone gets deeper\u201d. However, in a 5-shot mini-ImageNet case, this is not what the plot shows. Quite the opposite: the gap increased. Did I misunderstand something? Could you please comment on that? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q4 : In the Matching Nets paper , there is a good baseline classifier based on k-NNs . Do you know how does that one compares to Baseline and Baseline++ models if used with the same architecture for the feature extractor ? A4 : Here we show our 1-shot and 5-shot accuracy of Baseline and Baseline++ with the softmax and 1-NN classifier on the mini-ImageNet dataset with a Conv4 backbone . We only include the result of k = 1 with cosine distance to match the setting of Matching Nets paper . 1-shot softmax 1-NN ( cosine distance ) Baseline 42.11 % +- 0.71 % 44.18 % +- 0.69 % Baseline++ 48.24 % +- 0.75 % 49.57 % +- 0.73 % 5-shot softmax 1-NN ( cosine distance ) Baseline 62.53 % +- 0.69 % 56.68 % +- 0.67 % Baseline++ 66.43 % +- 0.63 % 61.93 % +- 0.65 % As shown above , using 1-NN classifier has better performance than that of using the softmax classifier in 1-shot setting , but softmax classifier is better in 5-shot setting instead . We note that that the number presented here are not directly comparable to the results reported in the Matching Nets paper because we use a different \u201c mini-ImageNet \u201d separation . In this paper , we follow the data split provided by [ Ravi et al.ICLR 2017 ] , which is used in most few-shot papers . We have included the result in the appendix of the revised paper . Q5 : The conclusion from the network depth experiments is that \u201c gaps among different methods diminish as the backbone gets deeper \u201d . However , in a 5-shot mini-ImageNet case , this is not what the plot shows . Quite the opposite : the gap increased . Did I misunderstand something ? Could you please comment on that ? A5 : Sorry for the confusion . As addressed in 4.3 , gaps among different methods diminish as the backbone gets deeper * in the CUB dataset * . In the mini-ImageNet dataset , the results are more complicated due to the domain difference . We further discuss this phenomenon in Section 4.4 and 4.5 . We have clarified related texts in the revised paper ."}], "0": {"review_id": "HkxLXnAcFQ-0", "review_text": "The paper tried to propose a systematic/consistent way for evaluating meta-learning algorithms. I believe this is a great direction of research as the meta-learning community is growing quickly. However, my question is if a relatively simple modification could improve the baselines, are there simple modifications available to other meta-learning algorithms being investigated? If the other algorithms are not as good as they claimed, can you give any insights on why and what to improve?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments ! Our responses are as follow : Q1 : If a relatively simple modification could improve the baselines , are there simple modifications available to other meta-learning algorithms being investigated ? A1 : The simple modification we made for the baseline approach is to replace the softmax layer with a distance-based classifier . However , among other meta-learning algorithms , only the MAML method is applicable to this modification . Both ProtoNet and MatchingNet already use distance-based classifier in their algorithm . RelationNet has its own relation module so is not applicable for this modification . While MAML could adopt this strategy , we did not include it into our experiment since our primary goal is not to improve one specific method . Q2 : If the other algorithms are not as good as they claimed , can you give any insights on why and what to improve ? A2 : Meta-learning for few-shot classification algorithms are not as good as they claimed because of the following two aspects : First , in the CUB setting , the gap among each algorithm diminished when using a deeper backbone . That is , with a deeper feature backbone , the improvement from different meta-learning algorithm become less significant . Our results suggest that both deeper backbones and meta-learning algorithms both aim to reduce intra-class variation for improving few-classification accuracy . Consequently , when intra-class variation has been dramatically reduced using a deeper backbone , the contribution from meta-learning becomes less significant . Second , in the CUB - > mini-ImageNet setting where a larger domain shift exists , the Baseline method outperforms all meta-learning algorithms . That is , existing meta-learning algorithms are not robust to larger domain shift . As discussed in section 4.4 , while meta-learning methods learn to learn from the support set during the meta-training stage , all of the base support sets are still within the same dataset . Thus , these algorithms did not learn how to learn from a support set with large domain shift . With our results , we encourage the community to tackle the challenge of potential domain shifts in the context of few-shot learning . We will release the source code and evaluation setting that will facilitate future research directions ."}, "1": {"review_id": "HkxLXnAcFQ-1", "review_text": "This paper gives a nice overview of existing works on few-shot learning. It groups them into some intuitive categories and meanwhile distills a common framework (Figure 2) employed by the methods. Moreover, the authors selected four of them, along with two baselines, to experimentally compare their performances under a cleaned experiment protocol. The experiments cover three few-shot learning scenarios respectively for generic object recognition, fine-grained classification, and cross-domain adaptation. While I do *not* think the third scenario is \u201cmore practical\u201d, it is certainly nice to have it included in the experiments. The experiment setup is unfortunately questionable. Since there is a validation set, one should use it to determine the free parameters (e.g., the number of epochs, learning rates, etc.). However, it seems like the same set of free parameters are used for different methods, making the comparison unfair because this set may favor some methods and yet hurt the others. The results of RelationNet are missing in Table 4. Another concern is that the same number of novel classes is used in the training and the testing stage. A more practical application of the learned meta model is to use it to handle different testing scenarios. There could be five novel classes in one scenario, 10 novel classes in another, and 100 in the third, etc. The number of labeled examples per class may also vary from one testing scenario to anther. It is misleading by the following: \u201cVery recently, Motiian et al. (2017) addresses the few-shot domain adaptation problem.\u201d There are a few variations in domain adaptation (DA). The learner has access to the fully labeled source domain and a small set of labeled target examples in supervised DA, to the source domain, a couple of labeled target examples, and many unlabeled target examples in semi-supervised DA, and to the source domain and many unlabeled target data points in the unsupervised DA. These have been studied long before (Motiian et al., 2017), for instance the works of Saenko et al. (2010) and Gong et al. (2013). [ref] Saenko K, Kulis B, Fritz M, Darrell T. Adapting visual category models to new domains. InEuropean conference on computer vision 2010 Sep 5 (pp. 213-226). Springer, Berlin, Heidelberg. [ref] Gong B, Grauman K, Sha F. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. InInternational Conference on Machine Learning 2013 Feb 13 (pp. 222-230). Overall, the paper is well written and may serve as a nice survey of existing works on few-shot learning. The unified experiment setup can facilitate the future research for fair comparisons, along with the three testing scenarios. However, I have some concerns as above about the experiment setups and hence also the conclusions. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q3 : Another concern is that the same number of novel classes is used in the training and the testing stage . A more practical application of the learned meta model is to use it to handle different testing scenarios . A3 : Thanks for pointing this out . As suggested , we conduct the experiments of 5-way meta-training and N-way meta-testing ( where we vary the number of N to be 5 , 10 , and 20 ) to examine the effect of handling testing scenarios that are different from training . We compare the methods Baseline , Baseline++ , MatchingNet , ProtoNet , and RelationNet . Note that we are unable to apply the MAML method as MAML learns the initialization for the classifier and can thus only be updated to classify the same number of classes . We show the experimental results on mini-ImageNet with 5-shot meta-training as follows . Backbone : Conv4 5-way test 10-way test 20-way test Baseline 62.53 % +- 0.69 % 46.44 % +- 0.41 % 32.27 % +- 0.24 % Baseline++ 66.43 % +- 0.63 % * 52.26 % +- 0.40 % * * 38.03 % +- 0.24 % * MatchingNet 63.48 % +- 0.66 % 47.61 % +- 0.44 % 33.97 % +- 0.24 % ProtoNet 64.24 % +- 0.68 % 48.77 % +- 0.45 % 34.58 % +- 0.23 % RelationNet * 66.60 % +- 0.69 % * 47.77 % +- 0.43 % 33.72 % +- 0.22 % Backbone : ResNet18 5-way test 10-way test 20-way test Baseline 74.27 % +- 0.63 % 55.00 % +- 0.46 % 42.03 % +- 0.25 % Baseline++ * 75.68 % +- 0.63 % * * 63.40 % +- 0.44 % * * 50.85 % +- 0.25 % * MatchingNet 68.88 % +- 0.69 % 52.27 % +- 0.46 % 36.78 % +- 0.25 % ProtoNet 73.68 % +- 0.65 % 59.22 % +- 0.44 % 44.96 % +- 0.26 % RelationNet 69.83 % +- 0.68 % 53.88 % +- 0.48 % 39.17 % +- 0.25 % Our results show that for classification with a larger-way ( e.g. , 10 or 20-way ) in the meta-testing stage , the proposed Baseline++ compares favorably against other methods in both shallow or deeper backbone settings . We attribute the results to two reasons . 1 ) To perform well in a larger N-way classification setting , one needs to further reduce the intra-class variation to avoid misclassification . Thus , in both shallow and deeper backbone settings , Baseline++ has better performance than Baseline . 2 ) As meta-learning algorithms were trained to perform 5-way classification in the meta-training stage , the performance of these algorithms may drop significantly when increasing the N-way in the meta-testing stage because the tasks of 10-way or 20-way classification are harder than that of 5-way classification . One may address this issue by performing a larger N-way classification in the meta-training stage ( as suggested in [ Snell et al.NIPS 2017 ] ) . However , this may encounter the issue of memory constraint . For example , to perform a 20-way classification with 5 support images and 15 query images in each class , we need to fit a batch size of 400 ( 20 x ( 5 + 15 ) ) that must fit in the GPUs . Without special hardware parallelization , the large batch size may prevent us from training models with deeper backbones such as ResNet . We have include the result in the appendix of the revised paper . Q4 : It is misleading by the following : \u201c Very recently , Motiian et al . ( 2017 ) addresses the few-shot domain adaptation problem. \u201d ... A4 : Thanks for the correction . Indeed , both Saenko et al.Gong et al.address the supervised domain adaptation problem with only a few labeled instances prior to [ Motiian et al. , NIPS 2017 ] . On the other hand , we would like to point out another research direction . Very recently , the method in [ Dong et al.ECML-PKDD 2018 ] addresses the few-shot problem where both the domain * and * the categories change . This work is more related to our setting , as we also consider novel category accuracy in few-shot classification under domain differences . We have corrected the statement in the revised paper ."}, "2": {"review_id": "HkxLXnAcFQ-2", "review_text": "There are a few things I like about the paper. Firstly, it makes interesting observations about the evaluation of the few-shot learning approaches, e.g. the underestimated baselines, and compares multiple methods in the same conditions. In fact, one of the reasons for accepting this paper would be to get a unified and, hopefully, well-written implementation of those methods. Secondly, I like the domain shift experiments, but I have the following question. The description of the CUB says that there is an overlap between CUB and ImageNet. Is there an overlap between CUB and mini-ImageNet? If so, then domain shift experiments might be too optimistic or even then it is not a big deal? One thing I don\u2019t like is that, in my opinion, the paper includes much redundant information which could go to the appendix in order to not weary the reader. For instance, everything related to Table 1. There is also some overlap between Section 2 and 3.3, while MAML, for instance, is still not well explained. Also, tables with too many numbers are difficult to read, e.g. Table 4. ---- Other notes ----- Many of the few-shot learning papers use Omniglot, so I think it would be a valuable addition to the appendix. Moreover, there exists a cross-domain scenario with Omniglot-> MNIST which I would also like to see in the appendix. In the Matching Nets paper, there is a good baseline classifier based on k-NNs. Do you know how does that one compares to Baseline and Baseline++ models if used with the same architecture for the feature extractor? The conclusion from the network depth experiments is that \u201cgaps among different methods diminish as the backbone gets deeper\u201d. However, in a 5-shot mini-ImageNet case, this is not what the plot shows. Quite the opposite: the gap increased. Did I misunderstand something? Could you please comment on that? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q4 : In the Matching Nets paper , there is a good baseline classifier based on k-NNs . Do you know how does that one compares to Baseline and Baseline++ models if used with the same architecture for the feature extractor ? A4 : Here we show our 1-shot and 5-shot accuracy of Baseline and Baseline++ with the softmax and 1-NN classifier on the mini-ImageNet dataset with a Conv4 backbone . We only include the result of k = 1 with cosine distance to match the setting of Matching Nets paper . 1-shot softmax 1-NN ( cosine distance ) Baseline 42.11 % +- 0.71 % 44.18 % +- 0.69 % Baseline++ 48.24 % +- 0.75 % 49.57 % +- 0.73 % 5-shot softmax 1-NN ( cosine distance ) Baseline 62.53 % +- 0.69 % 56.68 % +- 0.67 % Baseline++ 66.43 % +- 0.63 % 61.93 % +- 0.65 % As shown above , using 1-NN classifier has better performance than that of using the softmax classifier in 1-shot setting , but softmax classifier is better in 5-shot setting instead . We note that that the number presented here are not directly comparable to the results reported in the Matching Nets paper because we use a different \u201c mini-ImageNet \u201d separation . In this paper , we follow the data split provided by [ Ravi et al.ICLR 2017 ] , which is used in most few-shot papers . We have included the result in the appendix of the revised paper . Q5 : The conclusion from the network depth experiments is that \u201c gaps among different methods diminish as the backbone gets deeper \u201d . However , in a 5-shot mini-ImageNet case , this is not what the plot shows . Quite the opposite : the gap increased . Did I misunderstand something ? Could you please comment on that ? A5 : Sorry for the confusion . As addressed in 4.3 , gaps among different methods diminish as the backbone gets deeper * in the CUB dataset * . In the mini-ImageNet dataset , the results are more complicated due to the domain difference . We further discuss this phenomenon in Section 4.4 and 4.5 . We have clarified related texts in the revised paper ."}}