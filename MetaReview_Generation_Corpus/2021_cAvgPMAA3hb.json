{"year": "2021", "forum": "cAvgPMAA3hb", "title": "GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering", "decision": "Reject", "meta_review": "The paper presents an extension of recent implicit representations for view synthesis, such as NeRF. The presented formulation accepts an image set as input at test time, and can thus in principle be applied to new scenes. The idea is sound, but reviewers had concerns with the presentation and the experimental results. The work is primarily evaluated on the simplistic ShapeNet domain, which a number of reviewers found unconvincing. Concerns remain even after the authors' responses, and the AC agrees that the work can benefit from further investment before it is published.", "reviews": [{"review_id": "cAvgPMAA3hb-0", "review_text": "This paper presents a new neural learner for learning generic scene radiance function . Compared with existing methods such as NeRf which are scene-specific requiring per scene based training , the proposed new method is potentially able to generalize to novel unseen scenes or objects . Specifically , the method represents scene as a neural radiance filed in terms of spatial coordinates of camera centre and a query point . An attention model is used for aggregating back-projected image features at every query location , and the aggregated feature vectors are then used for predicting its RGB-alpha radiance . The work is well motivated , aiming to relax the restriction of existing per-scene based radiance field learning methods ( NeRF , for example ) . Experiments show some improved performance in novel view/radiance field synthesis , however I do no f find there are convincing tests provided or conducted to validate the `` better generalizability '' claim . Overall , the current results are insufficient to validate the method 's generality to novel scenes .More comments are given below : Pros : + The work is well motivated . + The idea of using attention model to address multi-view consistency issue is interesting , which appears to be sound and promising . + extensive experimental study , lots of ablation studies and comparisons . Cons : -One key intention of this paper is to improve the network 's generalibility to unseen , novel scenes . However , throughout the reported experiments , I do not find adequate experimental evidences to support this claim . From the paper it seems the proposed method has only been tested on some unseen scenes in the ShapeNet dataset , however with very similarly-looking objects in simple and similar poses . What are the results on the other two datasets , and what are the training-testing split on unfamiliar scenes ? Since the major motivation of this paper is to handle complex novel unseen scenes , I would suggest the authors conduct experiments on outdoor scenarios , such as the 'Tanks ' and 'Temples ' dataset commonly used in recent related work on neural novel view scene rendering . - It is mentioned that two different attention models ( AttSets and Slot Attention ) are used depending on training dataset . However it is unclear which model was used for testing which datasets . How did you compare in PSNR/SSIM metrics on each dataset ? Also , there is no description of how these attention mechanisms help the process . In particular , since 3D point visibility of source view features under target view is one of the key problems for any novel view synthesis work . How the employed attention module solves this issue is unclear . In short , I did not see how this attention module handles the visibility problem , other than ( as the authors said ) it helps to `` aggregate multiple source view features '' -- which seems to me to be an obvious outcome . - The authors also claimed that the attention module is invariant to the input view number and orders ; Yet , there is not any evidence ( either theoretic analysis , or experimental ) provided in the paper . Is it an inherent property of the Attention modules ? - From reading Table-1 , it seems SRNs sometimes outperforms GRF . The advantage of the proposed GRF is not entirely clear . - overall , a well motivated paper , but the execution of the ideas is not convincing .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Concern 1 * * \\ One key intention of this paper is to improve the network 's generalibility to unseen , novel scenes . However , throughout the reported experiments , I do not find adequate experimental evidences to support this claim . From the paper it seems the proposed method has only been tested on some unseen scenes in the ShapeNet dataset , however with very similarly-looking objects in simple and similar poses . What are the results on the other two datasets , and what are the training-testing split on unfamiliar scenes ? Since the major motivation of this paper is to handle complex novel unseen scenes , I would suggest the authors conduct experiments on outdoor scenarios , such as the 'Tanks ' and 'Temples ' dataset commonly used in recent related work on neural novel view scene rendering . * * Response * * : \\ These points are highly valuable for our paper . As requested , we conducted additional experiments on the Synthetic-NeRF dataset to thoroughly evaluate the generalization capability of our GRF across novel scenes . In particular , we train a single GRF model on four different scenes , and then directly test it on the remaining completely different four scenes . The results show that our GRF can indeed generalize well across novel scenes . In addition , we conduct extra experiments to demonstrate that the learned GRF can greatly benefit the single-scene learning , significantly better than the NeRF trained from scratch given a sparse amount of training signals . All the details are presented in Section 4.2 , page 7 and 8 . As to the suggested large-scale 3D scenes ( Tanks and Temples ) , it would be interesting to explore . However , it is non-trivial to directly evaluate our GRF on it , because of the complexity of geometry and the expensive computation required . In the revised paper , this is fully discussed in section 4.3 , the last paragraph of page 8 , and we leave it for future exploration . * * Concern 2 * * \\ It is mentioned that two different attention models ( AttSets and Slot Attention ) are used depending on training dataset . However it is unclear which model was used for testing which datasets . How did you compare in PSNR/SSIM metrics on each dataset ? \\ * * Response * * : \\ As noted in the appendix , we use AttSets for the ShapeNet dataset , and Slot Attention for the synthetic/real-world datasets . We use AttSets in the former because it can quickly aggregate information from multiple views , allowing the rendering of the millions of images needed for the ShapeNet benchmark . We also empirically find that the models for synthetic/real-world datasets converge quicker with Slot Attention . Comparison of the PSNR/SSIM metrics all strictly follows the baselines such as NeRF . * * Concern 3 * * \\ Also , there is no description of how these attention mechanisms help the process . In particular , since 3D point visibility of source view features under target view is one of the key problems for any novel view synthesis work . How the employed attention module solves this issue is unclear . In short , I did not see how this attention module handles the visibility problem , other than ( as the authors said ) it helps to \\ '' aggregate multiple source view features\\ '' -- which seems to me to be an obvious outcome . * * Response * * : \\ These are insightful questions . To thoroughly investigate how the attention module aids the network to deal with the visual occlusion , we conduct additional experiments and analyze in depth in the appendix ( A.4 ) . In particular , we track the largest attention score of the pixel patch and visualize the maximal attention score map for every pixel of a rendered image . Our results show that the attention module can indeed deal with the visual occlusion as it is expected . * * Concern 4 * * \\ The authors also claimed that the attention module is invariant to the input view number and orders ; Yet , there is not any evidence ( either theoretic analysis , or experimental ) provided in the paper . Is it an inherent property of the Attention modules ? * * Response * * : \\ The property of being invariant to the input views is theoretically analyzed in the papers AttSets and Slot-Att . To avoid the confusion , we rephrased the sentence in the revised paper . * * Concern 5 * * \\ From reading Table-1 , it seems SRNs sometimes outperforms GRF . The advantage of the proposed GRF is not entirely clear . * * Response * * : \\ In the revised paper , we have added additional results in section 4.1 , page 6 and 7 . We show that SRNs can not generalize at all without retraining on novel scenes , while our GRF can ."}, {"review_id": "cAvgPMAA3hb-1", "review_text": "Summary : The paper proposed an extension on Neural radiance field for better generalization across novel scenes by introducing multi-view pixel aligned features as additional input to NeRF . To fuse multi-view information and implicit reason about occlusion , an attention aggregation module is applied . Strengths : + The idea of using pixel aligned feature for making NeRF generalize to novel scenes is interesting . Concerns : 1 ) the term of implicit representation is interchangeably used to describe implicit function and explicit 3D representation parameterized by MLPs . 2 ) Comparison between GRF and GRAF : In GRAF , the radiance field is also conditioned on a shape code as well as an appearance code . But in the related work section , the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim . 3 ) Missing details about volumetric rendering : The paper did not talk about how rendering is performed ( possibly volumetric rendering ) . And both in NeRF and NSVF , sampling strategy and volumetric rendering both play important roles on achieving high-fidelity rendering results . It is unclear here how ray marching is formulated . 4 ) Experiments on ShapeNetV2 : the author just reported SRN 's result here , but did n't compare with some other method like NVS which is also directly applicable . Besides , other methods like NeRF could be modified to train on multiple objects like including a conditional embedding to NeRF jsut like GRAF or like in SRN use hyper-networks . Lack results here could be potentially undermining the claim of GRF being more general and robust . 5 ) Experiments on real-world complex scenes : The training setup here is a bit unclear to me . From just the description there , it is hard for me to tell whether GRF is training one model on all those scenes or training separate models for different scenes . It would be great if the author could make that clear . Minors : In the last paragraph of section 3.4 , very query 3D point - > every query 3D point ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Concern 1 * * \\ The term of implicit representation is interchangeably used to describe implicit function and explicit 3D representation parameterized by MLPs.\\ * * Response * * : \\ We use implicit function to describe the network architecture itself , while implicit representation to describe the learned features in general . More information about these terms may be found in the paper `` Local Deep Implicit Functions for 3D Shape , CVPR'20 '' . In the revised paper , we have carefully checked the consistency of the terms . * * Concern 2 * * \\ Comparison between GRF and GRAF : In GRAF , the radiance field is also conditioned on a shape code as well as an appearance code . But in the related work section , the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim.\\ * * Response * * : \\ Thanks for the advice . We have rephrased the sentences accordingly . * * Concern 3 * * \\ Missing details about volumetric rendering : The paper did not talk about how rendering is performed ( possibly volumetric rendering ) . And both in NeRF and NSVF , sampling strategy and volumetric rendering both play important roles on achieving high-fidelity rendering results . It is unclear here how ray marching is formulated.\\ * * Response * * : \\ In the revised paper , we have updated the section 3.5 , clearly specifying that we strictly follow NeRF for the sampling strategy and volumetric rendering . * * Concern 4 * * \\ Experiments on ShapeNetV2 : the author just reported SRN 's result here , but did n't compare with some other method like NVS which is also directly applicable . Besides , other methods like NeRF could be modified to train on multiple objects like including a conditional embedding to NeRF jsut like GRAF or like in SRN use hyper-networks . Lack results here could be potentially undermining the claim of GRF being more general and robust.\\ * * Response * * : \\ These are helpful points . On the ShapeNetV2 ( cars and chairs ) dataset , SRNs is currently the state-of-the-art approach for novel view generation . To the best of our knowledge , there is no other NVS ( novel view synthesis ) method reporting results on this particular dataset . Although some methods may be trained from scratch on this dataset , it is non-trivial to conduct the experiments considering the extremely expensive computation . It is insightful to point out that the conditional embedding to NeRF can be used for comparison . In fact , in our ablation study , we analyze that the removal of Point Local Features is a baseline of \\ '' Conditional embedding + NeRF\\ '' . In the revised paper , we update the section 4.4 accordingly . * * Concern 5 * * \\ Experiments on real-world complex scenes : The training setup here is a bit unclear to me . From just the description there , it is hard for me to tell whether GRF is training one model on all those scenes or training separate models for different scenes . It would be great if the author could make that clear.\\ * * Response * * : \\ We thank the reviewer for pointing out this ambiguity . We have changed the language to make it explicit where single-scene learning is happening , and where multi-scene learning is happening ."}, {"review_id": "cAvgPMAA3hb-2", "review_text": "Pros : + This paper proposes a method for synthesizing 3D scenes in novel view by treating the network as a general radiance field . It seems that the method is more like traditional manner based on multiple view geometry . It enables the generalization ability of rendering unseen test data in contrast to the most related work , NeRF ( Mildenhall et al. , 2020 ) . + The proposed GRF can empirically generalize to novel scenes . Experimental results show that the proposed method achieves better performance on several large-scale datasets . + In general , this paper is well-written and easy to read . Concerns : - The novelty is limited as the concept of this paper is highly similar to NeRF , despite a significant improvement is the general feature for novel view rendering . Yet I have few questions about general features for 3D points . 1.As depicted , the aggregator that assembles the 2D features of each 3D point from multiple views handles the visual occlusion implicitly via an attention process without depth scans . How does it work on unseen scenes ? The network may have no knowledge about the structure of the novel scenes . Are there any failure examples of such cases ? 2.Rendering a query 3D point p requires the ( r_p , g_p , b_p ) and the density d_p . As described in Eq . ( 6 ) , the color channels are estimated through MLPs with learned feature \\bar { F } _p and the query viewpoint \\mathcal { V } _p as input . Nonetheless , the density function in Eq . ( 5 ) does not require the query viewpoint as input . I think the density of 3D points for volume rendering should be dependent on the viewing ray as well . How to explain the difference between Eq . ( 5 ) and Eq . ( 6 ) ? - The evaluation of generalization for the novel scene of the proposed model is still not convincing enough , which needs improvements . 1.In section 4.1 , the performance of SRN is better than the proposed methods in almost all situations . The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code . I suggest that the authors should evaluate the performance of SRN on novel scenes without retraining process to validate the argument . 2.The experiments only conduct on the novel scene of the same category which share the similar features . I consider that SRN can also handle this situation by the learned latent code and hypernetwork . The authors are suggested to train the proposed GRF on a large amount of data with different categories of objects to learn the general feature for attention mechanism , and evaluate the model on the new object to validate the generalization .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Concern 1 * * \\ \\ As depicted , the aggregator that assembles the 2D features of each 3D point from multiple views handles the visual occlusion implicitly via an attention process without depth scans . How does it work on unseen scenes ? The network may have no knowledge about the structure of the novel scenes . Are there any failure examples of such cases ? \\ \\ * * Response * * : \\ This is a fundamental question . Given sparse views of an unseen scene , the CNN module of our trained GRF can extract hierarchical pixel features including the pixel local patterns and the possible larger shape information . These features are usually believed to be common and shared across different objects and scenes . This allows our model to have a certain level of generalization capability across unseen scenes . Basically , the attention mechanism is designed to select the most important pixel features among many for rendering a novel pixel . This module is likely to assign higher attention scores to visually similar and visible pixel patches . In order to evaluate the effectiveness of the attention module , we conduct an experiment to visualize and analyze the learned attention scores on novel objects . Details are presented in the appendix ( A.4 ) . It shows that the attention module can truly drive the network to focus on the visible pixel local features among the multiple intersected light rays . This result is also added into the section Ablation Study . * * Concern 2 * * \\ Rendering a query 3D point p requires the ( r_p , g_p , b_p ) and the density d_p . As described in Eq . ( 6 ) , the color channels are estimated through MLPs with learned feature $ \\bar { F } _p $ and the query viewpoint $ \\mathcal { V } _p $ as input . Nonetheless , the density function in Eq . ( 5 ) does not require the query viewpoint as input . I think the density of 3D points for volume rendering should be dependent on the viewing ray as well . How to explain the difference between Eq . ( 5 ) and Eq. ( 6 ) ? * * Response * * : \\ Here is the clarification . The volumetric density $ d_p $ is modeled as a function of the point features $ \\mathbf { \\bar { F } } _p $ only , because this allows the estimated density $ d_p $ at the query point $ p $ to remain unchanged under different query viewpoints , thus maintaining consistent geometry for each point . We update the paper accordingly in section 3.5 , the last paragraph of page 5 . * * Concern 3 * * \\ In section 4.1 , the performance of SRN is better than the proposed methods in almost all situations . The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code . I suggest that the authors should evaluate the performance of SRN on novel scenes without retraining process to validate the argument.\\ * * Response * * : \\ Thanks for the valuable suggestion . In the revised paper , we conducted additional experiments as suggested in section 4.1 , the last paragraph of page 6 . The qualitative results are presented in Figure 4 , clearly demonstrating the advantage of our GRF over SRNs when directly being tested on novel objects . * * Concern 4 * * \\ The evaluation of generalization for the novel scene of the proposed model is still not convincing enough , which needs improvements . The experiments only conduct on the novel scene of the same category which share the similar features . I consider that SRN can also handle this situation by the learned latent code and hypernetwork . The authors are suggested to train the proposed GRF on a large amount of data with different categories of objects to learn the general feature for attention mechanism , and evaluate the model on the new object to validate the generalization . * * Response * * : \\ We agree with the reviewer that our GRF can be trained on a large amount of objects belonging to different categories . However , we empirically find that such experiments would take up to a few months to finish on a modern GPU . Alternatively , we conduct additional experiments on the Synthetic-NeRF dataset to evaluate the generalization across novel scenes . In particular , our GRF is trained on four different scenes , and then directly tested on the remaining completely different four scenes . The results show that our GRF can indeed generalize well across novel scenes . The details are presented in the revised paper , the section 4.2 page 7 and 8 ."}, {"review_id": "cAvgPMAA3hb-3", "review_text": "This paper presents an extension of NeRF . The key idea is to represent a 3D scene as a collection of K `` posed '' images . A network is trained to take these images , viewing information and a query point as input and output density ( soft occupancy ) and RGB reflectance color . In contrast to NeRF , this methods learns a a mapping from the input images to feature vectors and thus promises to generalize across inputs ( whereas NeRF uses weight-encoding ) . On a detailed level , this method also uses multiview consistency during training , though there is not an ablation I could find that demonstrates the effectiveness of this delta . I recommend accepting this paper to ICLR . I like the idea of using the K input images as parameters controling the scene . My main criticisms are in the exposition of the paper and the experiments . I hope the exposition can be improved in revisions and am comfortable leaving the experimental shortcomings to future work . My main source of confusion reading this paper centers around the input `` viewpoint '' . The following comments are made in order of the paper 's exposition and hopefully highlight the path of my confusion : Please define what 's meant by `` posed 2D images '' . Are the viewpoints V1 , ... Vk corresponding to the K input images really just the 3D position of the camera focal point ? Or does this also include the view direction and camera intrinsics ? If it does include other information then perhaps use a different symbol for the query viewpoint . Near Figure 2 , the discussion of `` viewpoint '' for the image is really confusing . Do all pixels of an input share the same viewpoint position ( seems to be the case.This seems awkward , but best matches the written explanation . Or is the image `` placed '' into the 3D scene according to a pinhole camera 's transformation matrix , so that each pixel gets a unique 3d position associated to it ? This seems more appropriate , but would not match the text or explanation . The reprojection step appears to assume access to more than the camera center . Much later the paper says Vk `` including extrinsics and intrinsics '' . This is a fairly abusive notation . This should be clarified early on and the confusion issue with the image augmentation with viewpoint position remains . If the intro/abstract made it clear that the input is K images and full camera information and a different symbol was used for camera info and query viewpoint , much of this confusion would go away . In the motivation of this paper , there is special emphasis on representing `` scenes '' , yet the experiments are on `` single objects '' , just as other methods are criticized for being limited to . Can the proposed method be used to represented a full indoor room ( not just a 360\u00b0 video ; but view from anywhere in any direction ) ? The examples in the appendix are more like photos+depth type examples than what I would consider a full `` scene '' . Perhaps tone down/clarify what is claimed in the introduction . I would also like to get a sense of how stable / gracefully degrading this method is to the K input images . For example , if I only input images of the front of an object , what will happen when viewing the back ? What if most of the images are from one direction , will this bias affect views elsewhere ? Does the method generalize to scenarios with \u2260 K input images ? I suppose one could use the zero vectors trick for the < K images , but I wonder about degradation . What about > K ? It is not really fair to write that `` mose methods ... require ground truth 3D geometry for supervision '' . NERF/IDR/SIREN and when considering noisy point clouds SAL/SAL++ do not need ground truth 3D geometry . The paper writes `` This simple design of GRF follows the principle of classic multi-view geometry ( Hartley & Zisserman , 2004 ) , therefore guaranteeing the learned implicit representations meaningful and multi-view consistent . '' I do not see how this setup provides any formal guarantee . In the appendix it appears that following NeRF this paper predicts the density from the query position and not the viewing direction . Is that simply inherited here ? The multiview aspects during training simply harmonize with this choice but do n't provide any guarantee that I can understand . `` for very query '' -- > `` for every query ''", "rating": "7: Good paper, accept", "reply_text": "* * Concern 1 * * \\ Notational inconsistency and clarification relating to 'viewpoint ' and 'posed images'\\ \\ * * Response * * : \\ We thank the reviewer for pointing out the inconsistency of `` viewpoint '' and the symbols . In the revised paper , we made the following modifications : 1 ) The `` viewpoint '' is clearly defined as the camera location xyz . 2 ) All `` posed 2D images '' are replaced by `` 2D images with camera poses and intrinsics '' . 3 ) Near Figure 2 , the discussion about viewpoint is updated with clear specifications . 4 ) Equation 1 , Figure 1 and the symbols are all updated with clear definition . The whole paper now has consistent meanings for the viewpoint . * * Concern 2 * * \\ In the motivation of this paper , there is special emphasis on representing \\ '' scenes\\ '' , yet the experiments are on \\ '' single objects\\ '' , just as other methods are criticized for being limited to . Can the proposed method be used to represented a full indoor room ( not just a 360\u00b0 video ; but view from anywhere in any direction ) ? * * Response * * : \\ This is a good point . In principle , our GRF formulates the 3D structure via per 3D query point , which is agnostic to the complexity of scenes . The success on both Synthetic-NeRF and the real-world datasets also demonstrates that our GRF can indeed represent a certain level of complex 3D scenes . However , it is still challenging to directly evaluate on large-scale 3D scenes such as full indoor rooms or extreme large outdoor space . In order to avoid the confusion , we made the following changes in the revised paper : 1 ) In Introduction , the first paragraph of page 2 , we specify \\ '' the complex 3D scenes\\ '' as \\ '' multiple objects with cluttered background\\ '' . 2 ) In Section 4.3 , the last paragraph of page 8 , we analyse the difficulties of recovering large-scale 3D scenes and it is left for future exploration . * * Concern 3 * * \\ I would also like to get a sense of how stable / gracefully degrading this method is to the K input images . For example , if I only input images of the front of an object , what will happen when viewing the back ? What if most of the images are from one direction , will this bias affect views elsewhere ? \\ \\ * * Response * * : \\ This is an interesting question . We conduct an experiment accordingly and the results are presented in the appendix ( A.5 ) . 1 ) In the extreme case , i.e. , 1-view reconstruction , our GRF is still able to recover the general 3D shape of the unseen object , including the visually occluded parts , primarily because our CNN model learns the hierarchical features including the high-level shapes . 2 ) Given more input views , the originally occluded parts tend to be observed from some viewing angles , and then these parts can be reconstructed better and better . This shows that our GRF is indeed able to effectively identify the corresponding useful pixel features for more accurately recovering shape and appearance . * * Concern 4 * * \\ Does the method generalize to scenarios with $ \\neq $ K input images ? \\ \\ * * Response * * : \\ Since the used attention modules ( AttSets or Slot-Att ) are able to aggregate an arbitrary number of feature vectors , our GRF can naturally take any number of input images without needing the zero vector trick . We have conducted four groups of experiments for K = ( 1,2,5,10 ) in testing , where the model is trained with 5 images per object . Details are shown in the appendix ( A.5 ) . The results show that with less input images , the quality of shapes indeed degrades , especially the visually occluded parts . * * Concern 5 * * \\ It is not really fair to write that \\ '' mose methods \\ ... require ground truth 3D geometry for supervision\\ '' . NERF/IDR/SIREN and when considering noisy point clouds SAL/SAL++ do not need ground truth 3D geometry.\\ \\ * * Response * * : \\ We thank the reviewer for pointing out the related work . In the revised paper , we rephrased the sentence and briefly discussed SAL/SAL++ in the section Related Work . * * Concern 6 * * \\ The paper writes \\ '' This simple design of GRF follows the principle of classic multi-view geometry ( Hartley & Zisserman , 2004 ) , therefore guaranteeing the learned implicit representations meaningful and multi-view consistent.\\ '' I do not see how this setup provides any formal guarantee . \\ '' for very query\\ '' -- $ > $ \\ '' for every query\\ '' \\ \\ * * Response * * : \\ We agree that the word `` guaranteeing '' is not appropriate . It is replaced by more suitable words such as `` empirically remaining '' and \\ '' leading to\\ '' throughout the paper to avoid the confusion . In addition , we further discuss the reasons in section 3.1 , the last paragraph of page 3 . Typos are corrected ."}], "0": {"review_id": "cAvgPMAA3hb-0", "review_text": "This paper presents a new neural learner for learning generic scene radiance function . Compared with existing methods such as NeRf which are scene-specific requiring per scene based training , the proposed new method is potentially able to generalize to novel unseen scenes or objects . Specifically , the method represents scene as a neural radiance filed in terms of spatial coordinates of camera centre and a query point . An attention model is used for aggregating back-projected image features at every query location , and the aggregated feature vectors are then used for predicting its RGB-alpha radiance . The work is well motivated , aiming to relax the restriction of existing per-scene based radiance field learning methods ( NeRF , for example ) . Experiments show some improved performance in novel view/radiance field synthesis , however I do no f find there are convincing tests provided or conducted to validate the `` better generalizability '' claim . Overall , the current results are insufficient to validate the method 's generality to novel scenes .More comments are given below : Pros : + The work is well motivated . + The idea of using attention model to address multi-view consistency issue is interesting , which appears to be sound and promising . + extensive experimental study , lots of ablation studies and comparisons . Cons : -One key intention of this paper is to improve the network 's generalibility to unseen , novel scenes . However , throughout the reported experiments , I do not find adequate experimental evidences to support this claim . From the paper it seems the proposed method has only been tested on some unseen scenes in the ShapeNet dataset , however with very similarly-looking objects in simple and similar poses . What are the results on the other two datasets , and what are the training-testing split on unfamiliar scenes ? Since the major motivation of this paper is to handle complex novel unseen scenes , I would suggest the authors conduct experiments on outdoor scenarios , such as the 'Tanks ' and 'Temples ' dataset commonly used in recent related work on neural novel view scene rendering . - It is mentioned that two different attention models ( AttSets and Slot Attention ) are used depending on training dataset . However it is unclear which model was used for testing which datasets . How did you compare in PSNR/SSIM metrics on each dataset ? Also , there is no description of how these attention mechanisms help the process . In particular , since 3D point visibility of source view features under target view is one of the key problems for any novel view synthesis work . How the employed attention module solves this issue is unclear . In short , I did not see how this attention module handles the visibility problem , other than ( as the authors said ) it helps to `` aggregate multiple source view features '' -- which seems to me to be an obvious outcome . - The authors also claimed that the attention module is invariant to the input view number and orders ; Yet , there is not any evidence ( either theoretic analysis , or experimental ) provided in the paper . Is it an inherent property of the Attention modules ? - From reading Table-1 , it seems SRNs sometimes outperforms GRF . The advantage of the proposed GRF is not entirely clear . - overall , a well motivated paper , but the execution of the ideas is not convincing .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Concern 1 * * \\ One key intention of this paper is to improve the network 's generalibility to unseen , novel scenes . However , throughout the reported experiments , I do not find adequate experimental evidences to support this claim . From the paper it seems the proposed method has only been tested on some unseen scenes in the ShapeNet dataset , however with very similarly-looking objects in simple and similar poses . What are the results on the other two datasets , and what are the training-testing split on unfamiliar scenes ? Since the major motivation of this paper is to handle complex novel unseen scenes , I would suggest the authors conduct experiments on outdoor scenarios , such as the 'Tanks ' and 'Temples ' dataset commonly used in recent related work on neural novel view scene rendering . * * Response * * : \\ These points are highly valuable for our paper . As requested , we conducted additional experiments on the Synthetic-NeRF dataset to thoroughly evaluate the generalization capability of our GRF across novel scenes . In particular , we train a single GRF model on four different scenes , and then directly test it on the remaining completely different four scenes . The results show that our GRF can indeed generalize well across novel scenes . In addition , we conduct extra experiments to demonstrate that the learned GRF can greatly benefit the single-scene learning , significantly better than the NeRF trained from scratch given a sparse amount of training signals . All the details are presented in Section 4.2 , page 7 and 8 . As to the suggested large-scale 3D scenes ( Tanks and Temples ) , it would be interesting to explore . However , it is non-trivial to directly evaluate our GRF on it , because of the complexity of geometry and the expensive computation required . In the revised paper , this is fully discussed in section 4.3 , the last paragraph of page 8 , and we leave it for future exploration . * * Concern 2 * * \\ It is mentioned that two different attention models ( AttSets and Slot Attention ) are used depending on training dataset . However it is unclear which model was used for testing which datasets . How did you compare in PSNR/SSIM metrics on each dataset ? \\ * * Response * * : \\ As noted in the appendix , we use AttSets for the ShapeNet dataset , and Slot Attention for the synthetic/real-world datasets . We use AttSets in the former because it can quickly aggregate information from multiple views , allowing the rendering of the millions of images needed for the ShapeNet benchmark . We also empirically find that the models for synthetic/real-world datasets converge quicker with Slot Attention . Comparison of the PSNR/SSIM metrics all strictly follows the baselines such as NeRF . * * Concern 3 * * \\ Also , there is no description of how these attention mechanisms help the process . In particular , since 3D point visibility of source view features under target view is one of the key problems for any novel view synthesis work . How the employed attention module solves this issue is unclear . In short , I did not see how this attention module handles the visibility problem , other than ( as the authors said ) it helps to \\ '' aggregate multiple source view features\\ '' -- which seems to me to be an obvious outcome . * * Response * * : \\ These are insightful questions . To thoroughly investigate how the attention module aids the network to deal with the visual occlusion , we conduct additional experiments and analyze in depth in the appendix ( A.4 ) . In particular , we track the largest attention score of the pixel patch and visualize the maximal attention score map for every pixel of a rendered image . Our results show that the attention module can indeed deal with the visual occlusion as it is expected . * * Concern 4 * * \\ The authors also claimed that the attention module is invariant to the input view number and orders ; Yet , there is not any evidence ( either theoretic analysis , or experimental ) provided in the paper . Is it an inherent property of the Attention modules ? * * Response * * : \\ The property of being invariant to the input views is theoretically analyzed in the papers AttSets and Slot-Att . To avoid the confusion , we rephrased the sentence in the revised paper . * * Concern 5 * * \\ From reading Table-1 , it seems SRNs sometimes outperforms GRF . The advantage of the proposed GRF is not entirely clear . * * Response * * : \\ In the revised paper , we have added additional results in section 4.1 , page 6 and 7 . We show that SRNs can not generalize at all without retraining on novel scenes , while our GRF can ."}, "1": {"review_id": "cAvgPMAA3hb-1", "review_text": "Summary : The paper proposed an extension on Neural radiance field for better generalization across novel scenes by introducing multi-view pixel aligned features as additional input to NeRF . To fuse multi-view information and implicit reason about occlusion , an attention aggregation module is applied . Strengths : + The idea of using pixel aligned feature for making NeRF generalize to novel scenes is interesting . Concerns : 1 ) the term of implicit representation is interchangeably used to describe implicit function and explicit 3D representation parameterized by MLPs . 2 ) Comparison between GRF and GRAF : In GRAF , the radiance field is also conditioned on a shape code as well as an appearance code . But in the related work section , the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim . 3 ) Missing details about volumetric rendering : The paper did not talk about how rendering is performed ( possibly volumetric rendering ) . And both in NeRF and NSVF , sampling strategy and volumetric rendering both play important roles on achieving high-fidelity rendering results . It is unclear here how ray marching is formulated . 4 ) Experiments on ShapeNetV2 : the author just reported SRN 's result here , but did n't compare with some other method like NVS which is also directly applicable . Besides , other methods like NeRF could be modified to train on multiple objects like including a conditional embedding to NeRF jsut like GRAF or like in SRN use hyper-networks . Lack results here could be potentially undermining the claim of GRF being more general and robust . 5 ) Experiments on real-world complex scenes : The training setup here is a bit unclear to me . From just the description there , it is hard for me to tell whether GRF is training one model on all those scenes or training separate models for different scenes . It would be great if the author could make that clear . Minors : In the last paragraph of section 3.4 , very query 3D point - > every query 3D point ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Concern 1 * * \\ The term of implicit representation is interchangeably used to describe implicit function and explicit 3D representation parameterized by MLPs.\\ * * Response * * : \\ We use implicit function to describe the network architecture itself , while implicit representation to describe the learned features in general . More information about these terms may be found in the paper `` Local Deep Implicit Functions for 3D Shape , CVPR'20 '' . In the revised paper , we have carefully checked the consistency of the terms . * * Concern 2 * * \\ Comparison between GRF and GRAF : In GRAF , the radiance field is also conditioned on a shape code as well as an appearance code . But in the related work section , the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim.\\ * * Response * * : \\ Thanks for the advice . We have rephrased the sentences accordingly . * * Concern 3 * * \\ Missing details about volumetric rendering : The paper did not talk about how rendering is performed ( possibly volumetric rendering ) . And both in NeRF and NSVF , sampling strategy and volumetric rendering both play important roles on achieving high-fidelity rendering results . It is unclear here how ray marching is formulated.\\ * * Response * * : \\ In the revised paper , we have updated the section 3.5 , clearly specifying that we strictly follow NeRF for the sampling strategy and volumetric rendering . * * Concern 4 * * \\ Experiments on ShapeNetV2 : the author just reported SRN 's result here , but did n't compare with some other method like NVS which is also directly applicable . Besides , other methods like NeRF could be modified to train on multiple objects like including a conditional embedding to NeRF jsut like GRAF or like in SRN use hyper-networks . Lack results here could be potentially undermining the claim of GRF being more general and robust.\\ * * Response * * : \\ These are helpful points . On the ShapeNetV2 ( cars and chairs ) dataset , SRNs is currently the state-of-the-art approach for novel view generation . To the best of our knowledge , there is no other NVS ( novel view synthesis ) method reporting results on this particular dataset . Although some methods may be trained from scratch on this dataset , it is non-trivial to conduct the experiments considering the extremely expensive computation . It is insightful to point out that the conditional embedding to NeRF can be used for comparison . In fact , in our ablation study , we analyze that the removal of Point Local Features is a baseline of \\ '' Conditional embedding + NeRF\\ '' . In the revised paper , we update the section 4.4 accordingly . * * Concern 5 * * \\ Experiments on real-world complex scenes : The training setup here is a bit unclear to me . From just the description there , it is hard for me to tell whether GRF is training one model on all those scenes or training separate models for different scenes . It would be great if the author could make that clear.\\ * * Response * * : \\ We thank the reviewer for pointing out this ambiguity . We have changed the language to make it explicit where single-scene learning is happening , and where multi-scene learning is happening ."}, "2": {"review_id": "cAvgPMAA3hb-2", "review_text": "Pros : + This paper proposes a method for synthesizing 3D scenes in novel view by treating the network as a general radiance field . It seems that the method is more like traditional manner based on multiple view geometry . It enables the generalization ability of rendering unseen test data in contrast to the most related work , NeRF ( Mildenhall et al. , 2020 ) . + The proposed GRF can empirically generalize to novel scenes . Experimental results show that the proposed method achieves better performance on several large-scale datasets . + In general , this paper is well-written and easy to read . Concerns : - The novelty is limited as the concept of this paper is highly similar to NeRF , despite a significant improvement is the general feature for novel view rendering . Yet I have few questions about general features for 3D points . 1.As depicted , the aggregator that assembles the 2D features of each 3D point from multiple views handles the visual occlusion implicitly via an attention process without depth scans . How does it work on unseen scenes ? The network may have no knowledge about the structure of the novel scenes . Are there any failure examples of such cases ? 2.Rendering a query 3D point p requires the ( r_p , g_p , b_p ) and the density d_p . As described in Eq . ( 6 ) , the color channels are estimated through MLPs with learned feature \\bar { F } _p and the query viewpoint \\mathcal { V } _p as input . Nonetheless , the density function in Eq . ( 5 ) does not require the query viewpoint as input . I think the density of 3D points for volume rendering should be dependent on the viewing ray as well . How to explain the difference between Eq . ( 5 ) and Eq . ( 6 ) ? - The evaluation of generalization for the novel scene of the proposed model is still not convincing enough , which needs improvements . 1.In section 4.1 , the performance of SRN is better than the proposed methods in almost all situations . The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code . I suggest that the authors should evaluate the performance of SRN on novel scenes without retraining process to validate the argument . 2.The experiments only conduct on the novel scene of the same category which share the similar features . I consider that SRN can also handle this situation by the learned latent code and hypernetwork . The authors are suggested to train the proposed GRF on a large amount of data with different categories of objects to learn the general feature for attention mechanism , and evaluate the model on the new object to validate the generalization .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Concern 1 * * \\ \\ As depicted , the aggregator that assembles the 2D features of each 3D point from multiple views handles the visual occlusion implicitly via an attention process without depth scans . How does it work on unseen scenes ? The network may have no knowledge about the structure of the novel scenes . Are there any failure examples of such cases ? \\ \\ * * Response * * : \\ This is a fundamental question . Given sparse views of an unseen scene , the CNN module of our trained GRF can extract hierarchical pixel features including the pixel local patterns and the possible larger shape information . These features are usually believed to be common and shared across different objects and scenes . This allows our model to have a certain level of generalization capability across unseen scenes . Basically , the attention mechanism is designed to select the most important pixel features among many for rendering a novel pixel . This module is likely to assign higher attention scores to visually similar and visible pixel patches . In order to evaluate the effectiveness of the attention module , we conduct an experiment to visualize and analyze the learned attention scores on novel objects . Details are presented in the appendix ( A.4 ) . It shows that the attention module can truly drive the network to focus on the visible pixel local features among the multiple intersected light rays . This result is also added into the section Ablation Study . * * Concern 2 * * \\ Rendering a query 3D point p requires the ( r_p , g_p , b_p ) and the density d_p . As described in Eq . ( 6 ) , the color channels are estimated through MLPs with learned feature $ \\bar { F } _p $ and the query viewpoint $ \\mathcal { V } _p $ as input . Nonetheless , the density function in Eq . ( 5 ) does not require the query viewpoint as input . I think the density of 3D points for volume rendering should be dependent on the viewing ray as well . How to explain the difference between Eq . ( 5 ) and Eq. ( 6 ) ? * * Response * * : \\ Here is the clarification . The volumetric density $ d_p $ is modeled as a function of the point features $ \\mathbf { \\bar { F } } _p $ only , because this allows the estimated density $ d_p $ at the query point $ p $ to remain unchanged under different query viewpoints , thus maintaining consistent geometry for each point . We update the paper accordingly in section 3.5 , the last paragraph of page 5 . * * Concern 3 * * \\ In section 4.1 , the performance of SRN is better than the proposed methods in almost all situations . The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code . I suggest that the authors should evaluate the performance of SRN on novel scenes without retraining process to validate the argument.\\ * * Response * * : \\ Thanks for the valuable suggestion . In the revised paper , we conducted additional experiments as suggested in section 4.1 , the last paragraph of page 6 . The qualitative results are presented in Figure 4 , clearly demonstrating the advantage of our GRF over SRNs when directly being tested on novel objects . * * Concern 4 * * \\ The evaluation of generalization for the novel scene of the proposed model is still not convincing enough , which needs improvements . The experiments only conduct on the novel scene of the same category which share the similar features . I consider that SRN can also handle this situation by the learned latent code and hypernetwork . The authors are suggested to train the proposed GRF on a large amount of data with different categories of objects to learn the general feature for attention mechanism , and evaluate the model on the new object to validate the generalization . * * Response * * : \\ We agree with the reviewer that our GRF can be trained on a large amount of objects belonging to different categories . However , we empirically find that such experiments would take up to a few months to finish on a modern GPU . Alternatively , we conduct additional experiments on the Synthetic-NeRF dataset to evaluate the generalization across novel scenes . In particular , our GRF is trained on four different scenes , and then directly tested on the remaining completely different four scenes . The results show that our GRF can indeed generalize well across novel scenes . The details are presented in the revised paper , the section 4.2 page 7 and 8 ."}, "3": {"review_id": "cAvgPMAA3hb-3", "review_text": "This paper presents an extension of NeRF . The key idea is to represent a 3D scene as a collection of K `` posed '' images . A network is trained to take these images , viewing information and a query point as input and output density ( soft occupancy ) and RGB reflectance color . In contrast to NeRF , this methods learns a a mapping from the input images to feature vectors and thus promises to generalize across inputs ( whereas NeRF uses weight-encoding ) . On a detailed level , this method also uses multiview consistency during training , though there is not an ablation I could find that demonstrates the effectiveness of this delta . I recommend accepting this paper to ICLR . I like the idea of using the K input images as parameters controling the scene . My main criticisms are in the exposition of the paper and the experiments . I hope the exposition can be improved in revisions and am comfortable leaving the experimental shortcomings to future work . My main source of confusion reading this paper centers around the input `` viewpoint '' . The following comments are made in order of the paper 's exposition and hopefully highlight the path of my confusion : Please define what 's meant by `` posed 2D images '' . Are the viewpoints V1 , ... Vk corresponding to the K input images really just the 3D position of the camera focal point ? Or does this also include the view direction and camera intrinsics ? If it does include other information then perhaps use a different symbol for the query viewpoint . Near Figure 2 , the discussion of `` viewpoint '' for the image is really confusing . Do all pixels of an input share the same viewpoint position ( seems to be the case.This seems awkward , but best matches the written explanation . Or is the image `` placed '' into the 3D scene according to a pinhole camera 's transformation matrix , so that each pixel gets a unique 3d position associated to it ? This seems more appropriate , but would not match the text or explanation . The reprojection step appears to assume access to more than the camera center . Much later the paper says Vk `` including extrinsics and intrinsics '' . This is a fairly abusive notation . This should be clarified early on and the confusion issue with the image augmentation with viewpoint position remains . If the intro/abstract made it clear that the input is K images and full camera information and a different symbol was used for camera info and query viewpoint , much of this confusion would go away . In the motivation of this paper , there is special emphasis on representing `` scenes '' , yet the experiments are on `` single objects '' , just as other methods are criticized for being limited to . Can the proposed method be used to represented a full indoor room ( not just a 360\u00b0 video ; but view from anywhere in any direction ) ? The examples in the appendix are more like photos+depth type examples than what I would consider a full `` scene '' . Perhaps tone down/clarify what is claimed in the introduction . I would also like to get a sense of how stable / gracefully degrading this method is to the K input images . For example , if I only input images of the front of an object , what will happen when viewing the back ? What if most of the images are from one direction , will this bias affect views elsewhere ? Does the method generalize to scenarios with \u2260 K input images ? I suppose one could use the zero vectors trick for the < K images , but I wonder about degradation . What about > K ? It is not really fair to write that `` mose methods ... require ground truth 3D geometry for supervision '' . NERF/IDR/SIREN and when considering noisy point clouds SAL/SAL++ do not need ground truth 3D geometry . The paper writes `` This simple design of GRF follows the principle of classic multi-view geometry ( Hartley & Zisserman , 2004 ) , therefore guaranteeing the learned implicit representations meaningful and multi-view consistent . '' I do not see how this setup provides any formal guarantee . In the appendix it appears that following NeRF this paper predicts the density from the query position and not the viewing direction . Is that simply inherited here ? The multiview aspects during training simply harmonize with this choice but do n't provide any guarantee that I can understand . `` for very query '' -- > `` for every query ''", "rating": "7: Good paper, accept", "reply_text": "* * Concern 1 * * \\ Notational inconsistency and clarification relating to 'viewpoint ' and 'posed images'\\ \\ * * Response * * : \\ We thank the reviewer for pointing out the inconsistency of `` viewpoint '' and the symbols . In the revised paper , we made the following modifications : 1 ) The `` viewpoint '' is clearly defined as the camera location xyz . 2 ) All `` posed 2D images '' are replaced by `` 2D images with camera poses and intrinsics '' . 3 ) Near Figure 2 , the discussion about viewpoint is updated with clear specifications . 4 ) Equation 1 , Figure 1 and the symbols are all updated with clear definition . The whole paper now has consistent meanings for the viewpoint . * * Concern 2 * * \\ In the motivation of this paper , there is special emphasis on representing \\ '' scenes\\ '' , yet the experiments are on \\ '' single objects\\ '' , just as other methods are criticized for being limited to . Can the proposed method be used to represented a full indoor room ( not just a 360\u00b0 video ; but view from anywhere in any direction ) ? * * Response * * : \\ This is a good point . In principle , our GRF formulates the 3D structure via per 3D query point , which is agnostic to the complexity of scenes . The success on both Synthetic-NeRF and the real-world datasets also demonstrates that our GRF can indeed represent a certain level of complex 3D scenes . However , it is still challenging to directly evaluate on large-scale 3D scenes such as full indoor rooms or extreme large outdoor space . In order to avoid the confusion , we made the following changes in the revised paper : 1 ) In Introduction , the first paragraph of page 2 , we specify \\ '' the complex 3D scenes\\ '' as \\ '' multiple objects with cluttered background\\ '' . 2 ) In Section 4.3 , the last paragraph of page 8 , we analyse the difficulties of recovering large-scale 3D scenes and it is left for future exploration . * * Concern 3 * * \\ I would also like to get a sense of how stable / gracefully degrading this method is to the K input images . For example , if I only input images of the front of an object , what will happen when viewing the back ? What if most of the images are from one direction , will this bias affect views elsewhere ? \\ \\ * * Response * * : \\ This is an interesting question . We conduct an experiment accordingly and the results are presented in the appendix ( A.5 ) . 1 ) In the extreme case , i.e. , 1-view reconstruction , our GRF is still able to recover the general 3D shape of the unseen object , including the visually occluded parts , primarily because our CNN model learns the hierarchical features including the high-level shapes . 2 ) Given more input views , the originally occluded parts tend to be observed from some viewing angles , and then these parts can be reconstructed better and better . This shows that our GRF is indeed able to effectively identify the corresponding useful pixel features for more accurately recovering shape and appearance . * * Concern 4 * * \\ Does the method generalize to scenarios with $ \\neq $ K input images ? \\ \\ * * Response * * : \\ Since the used attention modules ( AttSets or Slot-Att ) are able to aggregate an arbitrary number of feature vectors , our GRF can naturally take any number of input images without needing the zero vector trick . We have conducted four groups of experiments for K = ( 1,2,5,10 ) in testing , where the model is trained with 5 images per object . Details are shown in the appendix ( A.5 ) . The results show that with less input images , the quality of shapes indeed degrades , especially the visually occluded parts . * * Concern 5 * * \\ It is not really fair to write that \\ '' mose methods \\ ... require ground truth 3D geometry for supervision\\ '' . NERF/IDR/SIREN and when considering noisy point clouds SAL/SAL++ do not need ground truth 3D geometry.\\ \\ * * Response * * : \\ We thank the reviewer for pointing out the related work . In the revised paper , we rephrased the sentence and briefly discussed SAL/SAL++ in the section Related Work . * * Concern 6 * * \\ The paper writes \\ '' This simple design of GRF follows the principle of classic multi-view geometry ( Hartley & Zisserman , 2004 ) , therefore guaranteeing the learned implicit representations meaningful and multi-view consistent.\\ '' I do not see how this setup provides any formal guarantee . \\ '' for very query\\ '' -- $ > $ \\ '' for every query\\ '' \\ \\ * * Response * * : \\ We agree that the word `` guaranteeing '' is not appropriate . It is replaced by more suitable words such as `` empirically remaining '' and \\ '' leading to\\ '' throughout the paper to avoid the confusion . In addition , we further discuss the reasons in section 3.1 , the last paragraph of page 3 . Typos are corrected ."}}