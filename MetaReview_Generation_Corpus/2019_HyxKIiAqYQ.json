{"year": "2019", "forum": "HyxKIiAqYQ", "title": "Context-adaptive Entropy Model for End-to-end Optimized Image Compression", "decision": "Accept (Poster)", "meta_review": "This paper proposes an algorithm for end-to-end image compression outperforming previously proposed ANN-based techniques and typical image compression standards like JPEG.\n\nStrengths\n- All reviewers agreed that this a well written paper, with careful analysis and results.\n\nWeaknesses\n- One of the points raised during the review process was that 2 very recent publications propose very similar algorithms. Since these works appeared very close to ICLR paper submission deadline (within 30 days), the program committee decided to treat this as concurrent work.\n\nThe authors also clarified the differences and similarities with prior work, and included additional experiments to clarify some of the concerns raised during the review process. Overall the paper is a solid contribution towards improving image compression, and is therefore recommended to be accepted.\n", "reviews": [{"review_id": "HyxKIiAqYQ-0", "review_text": "Update: I have updated my review to mention that we should accept this work as being concurrent with the two papers that are discussed below. Original review: This paper is very similar to two previously published papers (as pointed by David Minnen before the review period was opened): \"Learning a Code-Space Predictor by Exploiting Intra-Image-Dependencies\" (Klopp et al.) from BMVC 2018, and \"Joint Autoregressive and Hierarchical Priors for Learned Image Compression\" (Minnen et al.) from NIPS 2018. The authors have already tried to address these similarities and have provided a list in their reply, and my summary of the differences is as follows (dear authors: please comment if I am misrepresenting what you said): (1) the context model is slightly different (2) parametric model for hyperprior vs non-parametric (3) this point is highly debatable to be considered as a difference because the distinction between using noisy outputs vs quantized outputs is a very tiny detail (any any practitioner would probably try both and test which works better). (4) this is not really a difference. The fact that you provide details about the method should be a default! I want all the papers I read to have enough details to be able to implement them. (5+) not relevant for the discussion here. If the results were significantly different from previous work, these differences would indeed be interesting to discuss, but they didn't seem to change much vs. previously published work. If the other papers didn't exist, this would be an excellent paper on its own. However, I think the overlap is definitely there and as you can see from the summary above, it's not really clear to me whether this should be an ICLR paper or not. I am on the fence because I would expect more from a paper to be accepted to this venue (i.e., more than an incremental update to an existing set of models, which have already been covered in two papers). ", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer 1 , We appreciate your comments . As discussed in the separate thread , the most important issue is to clarify the criteria on prior works . To deal with this issue , we officially requested the decision of AC/PCs , and currently we \u2019 re waiting for it . We agree with most of your comments , but please understand that the reason we described the differences was to emphasize that our work was independently conducted . One more thing we \u2019 d like to emphasize is that our results were significantly superior from Klopp et al . ( 2018 ) 's approach . As we described in other postings , our work is more than 10 % superior in compression performance . As we have described in the response to reviewer 2 \u2019 s comments , occasionally there exist concurrently conducted studies , such as DiscoGAN and CycleGAN . Although decision on prior works will be made by AC/PCs , we would also be grateful if you view our work from a generous perspective for mutual progress of technologies . Regards , authors"}, {"review_id": "HyxKIiAqYQ-1", "review_text": "The authors present their own take on a variational image compression model based on Ball\u00e9 et al. (2018), with some interesting extensions/modifications: - The combination of an autoregressive and a hierarchical approach to define the prior, as in Klopp et al. (2018) and Minnen et al. (2018). - A simplified hyperprior, replacing the flow-based density model with a simpler Gaussian. - Breaking the strict separation of stochastic variables (denoted with a tilde, and used during training) and deterministic variables (denoted with a hat, used during evaluation), and instead conditioning some of the distributions on the quantized variables directly during training, in an effort to reduce potential training biases. The paper is written in clear language, and generally well presented with a great attention to detail. It is unfortunate that, as noted in the comments above, two prior, peer-reviewed studies have already explored extensions of the prior by introducing an autoregressive component, obtaining similar results. As far as I can see, this reduces the novelty of the present paper to the latter two modifications. The bit-free vs. bit-consuming terminology is simply another way of presenting the same concept. In my opinion, it is not sufficiently novel to consider acceptance of this work into the paper track at ICLR. The authors should consider to build on their work further and consider publication at a later time, possibly highlighting the latter modifications. However, the paper would need to be rewritten with a different set of claims. Update: Incorporating the AC/PC decision to treat the paper as concurrent work.", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer 2 , We appreciate your comments . As discussed in the separate thread , the most important issue is to clarify the criteria on prior works . To deal with this issue , we officially requested the decision of AC/PCs , and currently we \u2019 re waiting for it . Although the current prior work issue depends on the chairs \u2019 decision , we \u2019 d like to show one similar example , the case of DiscoGAN ( https : //arxiv.org/abs/1703.05192 ) and CycleGAN ( https : //arxiv.org/abs/1703.10593 ) . Both were opened to public via arXiv March 2017 ( 15 days of time difference ) . In spite of their very similar concepts and structures , they were accepted by ICML2017 and ICCV2017 , respectively . In addition , from the technical point of view , our approach has clear difference from Klopp et al . ( 2018 ) 's approach in performance ( our approach is more than 10 % superior in compression performance ) and paper composition ( we provide more comprehensive and concrete models along with a detailed implementation and training methods . ) We think that around 10 % of performance improvement has value enough to be reported to public in the compression research field . Regards , authors"}, {"review_id": "HyxKIiAqYQ-2", "review_text": "Summary. The paper is an improvement over (Balle et al 2018) for end-to-end image compression using deep neural networks. It relies on a generalized entropy model and some modifications in the training algorithm. Experimentals results on the Kodak PhotoCD dataset show improvements over the BPG format in terms of the peak signal-to-noise ratio (PSNR). It is not said whether the code will be made available. Pros. * Deep image compression is an active field of research of interest for ICLR. The paper is a step forward w.r.t. (Balle et al 2018). * The paper is well written. * Experimental results are promising. Cons. * Differences with (Balle et al 2018) should be emphasized. It is not easy to see where the improvements come from: from the new entropy model or from modifications in the training phase (using discrete representations on the conditions). * I am surprised that there is no discussion on the choice of the hyperparameter \\lambda: what are the optimal values in the experiments? Are the results varying a lot depending on the choice? Is there a strategy for an a priori choice? * Also is one dataset enough to draw conclusions on the proposed method? Evaluation. As a non expert in deep learning compression, I have a positive opinion on the paper but the paper seems more a fine tuning of the method of (Balle et al 2018). Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results. Some details. Typos: the p1, the their p2 and p10, while whereas p3, and and figure 2 p8: lower configurations, higher configurations, R-D configurations ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer 3 , [ Authors ] First of all , we really appreciate your careful comments . Please understand our late response due to an additional experiments to resolve your concerns . Attached please find the revised version . We address your comments as below : ______ Cons . * Differences with ( Balle et al 2018 ) should be emphasized . It is not easy to see where the improvements come from : from the new entropy model or from modifications in the training phase ( using discrete representations on the conditions ) . ______ [ Authors ] We agree with your comments and we also think it is a really important point that needs to be clarified . To clarify this , we conducted an additional experiments ( appendix 6.2 in the revised version ) on the network trained using the noisy representations as inputs of g_s and h_s . From the results , we found that the performance improvement comes from both new context adaptive entropy model and replacing the noisy representations with the discrete representations . Compared with ( Balle et al 2018 ) \u2019 s approach , our network trained with the noisy representations is 11.97 % better in compression performance , whereas the same trained with the discrete representations is 7.2 % better . ______ * I am surprised that there is no discussion on the choice of the hyperparameter \\lambda : what are the optimal values in the experiments ? Are the results varying a lot depending on the choice ? Is there a strategy for an a priori choice ? ______ [ Authors ] As your comments , \\lambda is very important parameter for training , which determines which to focus on between rate and distortion . However , \\lambda is not an optimization target , but a given condition for optimization . Therefore , several networks were trained , each of which was trained with a specific value of \\lambda . In figure 5 , illustrating the evaluation results , each point represents a result of one single network trained under a specific \\lambda , so one line of our approach represents results of nine trained networks . We described the range of \\lambda values in section 4.2 , from 0.01 to 0.5 . As the lambda increases , the gain of the bit amount side is increased , but the loss of the image quality side is also increased . The exact values that we used are 0.5 , 0.4 , 0.3 , 0.2 , 0.1 , 0.06 , 0.03 , 0.017 , and 0.01 , in order of from rate-centric condition to distortion-centric condition . To clarify the purpose of using \\lambda , we have added more description about \\lambda in the revised version , before equation ( 2 ) . ______ * Also is one dataset enough to draw conclusions on the proposed method ? ______ [ Authors ] One dataset may not be enough for completely evaluating one method . However , Kodak photo CD image set has served as a reference test set for many studies . We guess that the reason many studies have used this set is to make comparison between approaches easier , and to ensure the objectivity of the comparison results . Instead of adding more evaluation results over other image sets , we will add an URL link to our test code repository if publication is decided . Our methods could be evaluated over any kind of image sets with the test code . ______ Evaluation . As a non expert in deep learning compression , I have a positive opinion on the paper but the paper seems more a fine tuning of the method of ( Balle et al 2018 ) . Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results . ___ [ Authors ] ( Balle et al 2018 ) successfully captures spatial dependencies of natural images by estimating scales of representation , in an input-adaptive manner . To further remove the spatial dependency , we proposed a model that can sequentially predict each value ( mean ) of representations , as well as standard deviation values as in ( Balle et al 2018 ) . We believe that this autoregression using the two types of contexts is essential component to achieve higher compression performance . It has been just two years since two great papers , which become a basis of entropy model based image compression , were poposed by ( Balle et al.2017 ) and ( Theis et al.2017 ) , and currently context utilization within latent space is at the very beginning phase . We believe that a variety of context utilization methods will be studied , and hope our work will serve as a stepping stone for future studies utilizing various types of bit-free and bit-consuming contexts . ______ Some details . Typos : the p1 , the their p2 and p10 , while whereas p3 , and and figure 2 [ Authors ] We \u2019 ve fixed the typos . Thank you for pointing out . p8 : lower configurations , higher configurations , R-D configurations [ Authors ] We \u2019 ve changed the phrases to make them clear , as follows : \\lambda configurations for lower bit-rates , \\lambda configurations for higher bit-rates , \\lambda configurations Thank you very much for your insightful comments again ! Regards , Authors"}], "0": {"review_id": "HyxKIiAqYQ-0", "review_text": "Update: I have updated my review to mention that we should accept this work as being concurrent with the two papers that are discussed below. Original review: This paper is very similar to two previously published papers (as pointed by David Minnen before the review period was opened): \"Learning a Code-Space Predictor by Exploiting Intra-Image-Dependencies\" (Klopp et al.) from BMVC 2018, and \"Joint Autoregressive and Hierarchical Priors for Learned Image Compression\" (Minnen et al.) from NIPS 2018. The authors have already tried to address these similarities and have provided a list in their reply, and my summary of the differences is as follows (dear authors: please comment if I am misrepresenting what you said): (1) the context model is slightly different (2) parametric model for hyperprior vs non-parametric (3) this point is highly debatable to be considered as a difference because the distinction between using noisy outputs vs quantized outputs is a very tiny detail (any any practitioner would probably try both and test which works better). (4) this is not really a difference. The fact that you provide details about the method should be a default! I want all the papers I read to have enough details to be able to implement them. (5+) not relevant for the discussion here. If the results were significantly different from previous work, these differences would indeed be interesting to discuss, but they didn't seem to change much vs. previously published work. If the other papers didn't exist, this would be an excellent paper on its own. However, I think the overlap is definitely there and as you can see from the summary above, it's not really clear to me whether this should be an ICLR paper or not. I am on the fence because I would expect more from a paper to be accepted to this venue (i.e., more than an incremental update to an existing set of models, which have already been covered in two papers). ", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer 1 , We appreciate your comments . As discussed in the separate thread , the most important issue is to clarify the criteria on prior works . To deal with this issue , we officially requested the decision of AC/PCs , and currently we \u2019 re waiting for it . We agree with most of your comments , but please understand that the reason we described the differences was to emphasize that our work was independently conducted . One more thing we \u2019 d like to emphasize is that our results were significantly superior from Klopp et al . ( 2018 ) 's approach . As we described in other postings , our work is more than 10 % superior in compression performance . As we have described in the response to reviewer 2 \u2019 s comments , occasionally there exist concurrently conducted studies , such as DiscoGAN and CycleGAN . Although decision on prior works will be made by AC/PCs , we would also be grateful if you view our work from a generous perspective for mutual progress of technologies . Regards , authors"}, "1": {"review_id": "HyxKIiAqYQ-1", "review_text": "The authors present their own take on a variational image compression model based on Ball\u00e9 et al. (2018), with some interesting extensions/modifications: - The combination of an autoregressive and a hierarchical approach to define the prior, as in Klopp et al. (2018) and Minnen et al. (2018). - A simplified hyperprior, replacing the flow-based density model with a simpler Gaussian. - Breaking the strict separation of stochastic variables (denoted with a tilde, and used during training) and deterministic variables (denoted with a hat, used during evaluation), and instead conditioning some of the distributions on the quantized variables directly during training, in an effort to reduce potential training biases. The paper is written in clear language, and generally well presented with a great attention to detail. It is unfortunate that, as noted in the comments above, two prior, peer-reviewed studies have already explored extensions of the prior by introducing an autoregressive component, obtaining similar results. As far as I can see, this reduces the novelty of the present paper to the latter two modifications. The bit-free vs. bit-consuming terminology is simply another way of presenting the same concept. In my opinion, it is not sufficiently novel to consider acceptance of this work into the paper track at ICLR. The authors should consider to build on their work further and consider publication at a later time, possibly highlighting the latter modifications. However, the paper would need to be rewritten with a different set of claims. Update: Incorporating the AC/PC decision to treat the paper as concurrent work.", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer 2 , We appreciate your comments . As discussed in the separate thread , the most important issue is to clarify the criteria on prior works . To deal with this issue , we officially requested the decision of AC/PCs , and currently we \u2019 re waiting for it . Although the current prior work issue depends on the chairs \u2019 decision , we \u2019 d like to show one similar example , the case of DiscoGAN ( https : //arxiv.org/abs/1703.05192 ) and CycleGAN ( https : //arxiv.org/abs/1703.10593 ) . Both were opened to public via arXiv March 2017 ( 15 days of time difference ) . In spite of their very similar concepts and structures , they were accepted by ICML2017 and ICCV2017 , respectively . In addition , from the technical point of view , our approach has clear difference from Klopp et al . ( 2018 ) 's approach in performance ( our approach is more than 10 % superior in compression performance ) and paper composition ( we provide more comprehensive and concrete models along with a detailed implementation and training methods . ) We think that around 10 % of performance improvement has value enough to be reported to public in the compression research field . Regards , authors"}, "2": {"review_id": "HyxKIiAqYQ-2", "review_text": "Summary. The paper is an improvement over (Balle et al 2018) for end-to-end image compression using deep neural networks. It relies on a generalized entropy model and some modifications in the training algorithm. Experimentals results on the Kodak PhotoCD dataset show improvements over the BPG format in terms of the peak signal-to-noise ratio (PSNR). It is not said whether the code will be made available. Pros. * Deep image compression is an active field of research of interest for ICLR. The paper is a step forward w.r.t. (Balle et al 2018). * The paper is well written. * Experimental results are promising. Cons. * Differences with (Balle et al 2018) should be emphasized. It is not easy to see where the improvements come from: from the new entropy model or from modifications in the training phase (using discrete representations on the conditions). * I am surprised that there is no discussion on the choice of the hyperparameter \\lambda: what are the optimal values in the experiments? Are the results varying a lot depending on the choice? Is there a strategy for an a priori choice? * Also is one dataset enough to draw conclusions on the proposed method? Evaluation. As a non expert in deep learning compression, I have a positive opinion on the paper but the paper seems more a fine tuning of the method of (Balle et al 2018). Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results. Some details. Typos: the p1, the their p2 and p10, while whereas p3, and and figure 2 p8: lower configurations, higher configurations, R-D configurations ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer 3 , [ Authors ] First of all , we really appreciate your careful comments . Please understand our late response due to an additional experiments to resolve your concerns . Attached please find the revised version . We address your comments as below : ______ Cons . * Differences with ( Balle et al 2018 ) should be emphasized . It is not easy to see where the improvements come from : from the new entropy model or from modifications in the training phase ( using discrete representations on the conditions ) . ______ [ Authors ] We agree with your comments and we also think it is a really important point that needs to be clarified . To clarify this , we conducted an additional experiments ( appendix 6.2 in the revised version ) on the network trained using the noisy representations as inputs of g_s and h_s . From the results , we found that the performance improvement comes from both new context adaptive entropy model and replacing the noisy representations with the discrete representations . Compared with ( Balle et al 2018 ) \u2019 s approach , our network trained with the noisy representations is 11.97 % better in compression performance , whereas the same trained with the discrete representations is 7.2 % better . ______ * I am surprised that there is no discussion on the choice of the hyperparameter \\lambda : what are the optimal values in the experiments ? Are the results varying a lot depending on the choice ? Is there a strategy for an a priori choice ? ______ [ Authors ] As your comments , \\lambda is very important parameter for training , which determines which to focus on between rate and distortion . However , \\lambda is not an optimization target , but a given condition for optimization . Therefore , several networks were trained , each of which was trained with a specific value of \\lambda . In figure 5 , illustrating the evaluation results , each point represents a result of one single network trained under a specific \\lambda , so one line of our approach represents results of nine trained networks . We described the range of \\lambda values in section 4.2 , from 0.01 to 0.5 . As the lambda increases , the gain of the bit amount side is increased , but the loss of the image quality side is also increased . The exact values that we used are 0.5 , 0.4 , 0.3 , 0.2 , 0.1 , 0.06 , 0.03 , 0.017 , and 0.01 , in order of from rate-centric condition to distortion-centric condition . To clarify the purpose of using \\lambda , we have added more description about \\lambda in the revised version , before equation ( 2 ) . ______ * Also is one dataset enough to draw conclusions on the proposed method ? ______ [ Authors ] One dataset may not be enough for completely evaluating one method . However , Kodak photo CD image set has served as a reference test set for many studies . We guess that the reason many studies have used this set is to make comparison between approaches easier , and to ensure the objectivity of the comparison results . Instead of adding more evaluation results over other image sets , we will add an URL link to our test code repository if publication is decided . Our methods could be evaluated over any kind of image sets with the test code . ______ Evaluation . As a non expert in deep learning compression , I have a positive opinion on the paper but the paper seems more a fine tuning of the method of ( Balle et al 2018 ) . Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results . ___ [ Authors ] ( Balle et al 2018 ) successfully captures spatial dependencies of natural images by estimating scales of representation , in an input-adaptive manner . To further remove the spatial dependency , we proposed a model that can sequentially predict each value ( mean ) of representations , as well as standard deviation values as in ( Balle et al 2018 ) . We believe that this autoregression using the two types of contexts is essential component to achieve higher compression performance . It has been just two years since two great papers , which become a basis of entropy model based image compression , were poposed by ( Balle et al.2017 ) and ( Theis et al.2017 ) , and currently context utilization within latent space is at the very beginning phase . We believe that a variety of context utilization methods will be studied , and hope our work will serve as a stepping stone for future studies utilizing various types of bit-free and bit-consuming contexts . ______ Some details . Typos : the p1 , the their p2 and p10 , while whereas p3 , and and figure 2 [ Authors ] We \u2019 ve fixed the typos . Thank you for pointing out . p8 : lower configurations , higher configurations , R-D configurations [ Authors ] We \u2019 ve changed the phrases to make them clear , as follows : \\lambda configurations for lower bit-rates , \\lambda configurations for higher bit-rates , \\lambda configurations Thank you very much for your insightful comments again ! Regards , Authors"}}