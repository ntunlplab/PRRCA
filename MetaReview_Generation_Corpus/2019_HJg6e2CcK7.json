{"year": "2019", "forum": "HJg6e2CcK7", "title": "Clean-Label Backdoor Attacks", "decision": "Reject", "meta_review": "The present work proposes to improve backdoor poisoning attacks by only using \"clean-label\" images (images whose label would be judged correct by a human), with the motivation that this would make them harder to detect. It considers two approaches to this, one based on GANs and one based on adversarial examples, and shows that the latter works better (and is in general quite effective). It also identifies an interesting phenomenon---that simply using existing back-door attacks with clean labels is substantially less effective than with incorrect labels, because the network does not need to modify itself to accommodate these additional correctly-labeled examples.\n\nThe strengths of this paper are that it has a detailed empirical evaluation with multiple interesting insights (described above). It also considers efficacy against some basic defense measures based on random pre-processing.\n\nA weakness of the paper is that the justification for clean-label attacks is somewhat heuristic, based on the claim that dirty-label attacks can be recognized by hand. There is additional justification that dirty labels tend to be correlated with low confidence, but this correlation (as shown in Figure 2) is actually quite weak. On the other hand, natural defense strategies against the adversarial examples based attack (such as detecting and removing points with large loss at intermediate stages of training) are not considered. This might be fine, as we often assume that the attacker can react to the defender, but it is unclear why we should reject dirty-label attacks on the basis that they can be recognized by one detection mechanism but not give the defender the benefit of other simple detection mechanisms for clean-label attacks.\n\nA separate concern was brought up that the attack is too similar to that of Guo et al., and that the method was not run on large-scale datasets. The Guo et al. paper does somewhat diminish the novelty of the present work, but not in a way that I consider problematic; there are definitely new results in this paper, especially the interesting empirical finding that the Guo et al. attack crucially relies on dirty labels. I do not agree with the criticism about large-scale datasets; in general, not all authors have the resources to test on ImageNet, and it is not clear why this should be required unless there is a specific hypothesis that running on ImageNet would test. It is true that the GAN-based method might work more poorly on ImageNet than on CIFAR, but the adversarial attack method (which is in any case the stronger method) seems unlikely to run into scaling issues.\n\nOverall, this paper is right on the borderline of acceptance. There are interesting results, and none of the weaknesses are critical. It was unfortunately the case that there wasn't room in the program this year, so the paper was ultimately rejected. However, I think this could be a strong piece of work (and a clear accept) with some additional development. Here are some ideas that might help:\n\n(1) Further investigate the phenomenon that adding data points that are too easy to fit do not succeed in data poisoning. This is a fairly interesting point but is not emphasized in the paper.\n(2) Investigate natural defense mechanisms in the clean-label setting (such as filtering by loss or other such strategies). I do not think it is crucial that the clean-label attack bypasses every simple defense, but considering such defenses can provide more insight into how the attack works--e.g., does it in fact lead to substantially higher loss during training? And if so, at what stage does this occur? If not, how does it succeed in altering the model without inducing high loss?", "reviews": [{"review_id": "HJg6e2CcK7-0", "review_text": " This work explores backdoor attacks -- attacks that alter a fraction of training examples which can alter inference -- while ensuring that the poisoned inputs are consisten t with their labels. These attacks are attained through either a GAN mechanism or using adversarial perturbations. The ideas proposed (i.e. GAN mechanism and adversarial mechanism) are interesting additions to this literaature. I found the observation of greater effectiveness of adversa rial mechanism particularly interesting. The paper also does a good job of investigating effectiveness of the attack under data augmentation and propooses a limited solution. Main criticism: there are a number of typos that need fixing. ~ ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the kind comments . We have updated the manuscript to fix typos ."}, {"review_id": "HJg6e2CcK7-1", "review_text": "Overall I am positive about this manuscript: - I find the motivation is clear and valid. As far as I know, this is a novel contribution (my confidence is not very high on that one though - I might be unaware of related work). - The paper is well-written and organized. - Experiments are conducted systematically, although certain parts could be better explained (see my questions below). I think this paper adds an original and valuable angle to the existing literature on data poisoning attacks. I don't see any major flaws, therefore I think it should be accepted. A few points which might need clarification: - How exactly is \"attack success\" being measured? - Which model is used to generate the adversarial samples? Is this an (adversarially) pretrained model? (If that's the case, then what is the model architecture?) Or are adversarial samples generated on the fly using the currently trained/poisoned model? - At the end of Section 4.4: if the images with larger noise rely more on the backdoor, why does this have an adverse effect? Shouldn't it increase the effectiveness of the attack? - Was the data augmentation (flips, crops etc) performed before or after the poisoning pattern was applied? Minor comments: - definition of the encoding at the bottom of page 4: this should be argmax instead of max - typo in Sec. 5.1: \"to evaluate the uat a wide variety\" - repetitive sentence in Sec. 5.2: \"we find that images generated with $\\tau \\leq 0.2$ remain [fairly] plausible\" ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the kind comments and helpful suggestions . We will address points raised below : - The attack success rate ( ASR ) is computed as the fraction of inputs that are _not_ labeled with the target class but are classified as the target class after the backdoor pattern is applied ( Beginning of Section 5 ) . We have edited the manuscript to make this definition appear more prominently earlier in the paper and edited the relevant captions . - We use adversarially trained models trained with the publicly available code from https : //github.com/MadryLab/cifar10_challenge ( we train the non-wide variant both with L2 and Linf ) . The adversarial examples are generated once using this pre-trained network . Since our threat model only allows us to add examples to the training set , we can not compute these adversarial perturbations on the fly . We have edited the manuscript to incorporate this discussion . - We were also surprised initially but we believe that there is a fairly simple explanation ( outlined in Section 4.4 ) . On noisy images , the classifier learns to predict by relying on the backdoor * in the absence of strong image signal * ( since the salient image features are fairly corrupted ) . However , when evaluated on the test set with a backdoor applied , the image itself will have a strong signal ( since it will not be noisy ) that can overcome the backdoor pattern . Therefore , it is necessary for the classifier to learn to predict the backdoor even when the salient image characteristics are present . As a result , random noise is not very effective at injecting backdoors . We have updated Section 4.4 to better reflect this argument . - Since we do not have access to the training procedure , the pattern is applied before any data augmentation . This is the reason why this setting is challenging -- data augmentation might obscure the pattern . We have updated the manuscript to incorporate the other comments ."}, {"review_id": "HJg6e2CcK7-2", "review_text": "This paper investigates an interesting problem, backdoor attack against neural networks. The main idea is to add a watermark pattern to the corners of the training images, so that the classifier is guided to leverage the watermark as a discriminative cue as opposed to the real content of the image. At the test stage, one can hence manipulate the classifier\u2019s predictions by adding the watermark to the test images. This paper is heavily built upon Gu et al. (2017)\u2019s work. It shows that Gu et al. (2017)\u2019s method can be easily defended by a data sanitization algorithm. To improve Gu et al.\u2019s work, the authors propose to add watermark patterns to the adversarial examples or examples interpolated in GAN\u2019s latent space. The intuition is that these examples are adversarial and hard to learn, forcing the classifier to focus on the watermark pattern instead. It is an interesting idea and an intuitive improvement over (Gu et al. 2017). However, the implementation of the idea could be improved. This paper does not propose any new attack algorithms. Instead, it investigates an existing adversarial attack method and the GAN based interpolation for the purpose of backdoor attack. As experiments are conducted on small-scale datasets, it is unclear how effective the improved backdoor attack is. Moreover, one of the main disadvantages of the proposed attack method is that simple data augmentation techniques, especially random cropping, can successfully defend against the attack. The quality of the paper writing could be improved. I had to read the paper more than twice and check the references now and then in order to understand some claims of the paper. The paper\u2019s lack of clarity was actually also raised by probably one of the coauthors of the paper; see the comment \u201cDimitris: clarify this point\u201d on Page 11. Please find some concrete suggestions below. - Figure 1 is visually not appealing at all. Perhaps find better illustrative examples. - It is worth considering to add a separate section/paragraph to describe the details of Gu et al. (2017)\u2019s method, given that this paper is heavily built upon Gu et al. (2017)\u2019s work. - It was unclear what the \u201creduced amplitude backdoor trigger\u201d means until Section 4. If a context-dependent term has to be used in the introduction, explain it or refer the readers to the right place of the paper. - Merge Sections 4.3\u20144.5 with the experiment section (Section 5). The results of Section 4.3\u20134.5 are out of context without any explanation about the experiment setups. I have some concerns about Section 3, which is the main motivation of this work. As the authors noted in Appendix A that Gu et al.\u2019s method works well with as few as 75 poised examples, the proposed sanitization algorithm would not be able to fail Gu et al.\u2019s method by only identifying 20 out of 100 poised examples. How to control the parameter $\\tau$ so that the perturbation appears plausible to humans? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful comments . We will address comments raised below . - On the novelty of our attacks . We believe that the main conceptual contribution of our work is the formulation of the clean-label attack problem and showing how these attacks can be made successful by modifying samples to be `` harder '' . The two attacks investigated are meant as proof-of-concept that this approach works with existing methods . We agree with the reviewer that designing specialized attacks for this task is a valuable research direction that could lead to even more successful attacks . - On the scale of our datasets . We do not have the resources to run an equally comprehensive study on ImageNet-scale datasets . Hence , we decided to perform more experiments on a small dataset rather than fewer experiments on a larger dataset . Note that the plots in Figure 3 and 4 involved training 50 models each . Does the reviewer have concrete concerns about the applicability of our approach to large-scale datasets ? - On the resistance of our approach to data augmentation . We have demonstrated ( Appendix B ) that simply modifying the pattern to appear in all 4 corners is already sufficient to make the attack significantly more resistant to data augmentation . Thus , we do n't consider data augmentation to be a fundamental obstacle to our attack . We believe that future work investigating different backdoor triggers can further increase the resistance of our attack to data augmentation . We thank the reviewer for concrete suggestions on improving our manuscript . We incorporated the following changes : - We replaced Figure 1 with more illustrative examples . - We modified the second paragraph of Section 2 to better explain the original Gu et al . ( 2017 ) attack . - We changed the wording of \u201c reduced amplitude backdoor trigger \u201d to `` less conspicuous backdoor trigger '' which should be clear without any further context . - The goal of Sections 4.3 - 4.5 is to provide a reader with an overview of our results before going into the experimental details . We modified these Sections to be more self-contained . - On concerns about Section 3 . We do not argue that manual inspection will find all the poisoned examples ( or enough to render the attack ineffective ) . We rather argue that if manual inspection of 300 images reveals 20 * clearly mislabelled * images , then the attack will very likely be detected leading to additional investigation and filtering . This argument illustrates a broader point -- if poisoned inputs appear suspicious upon human inspection , the attack is not truly insidious and can always be detected by more advanced filtering . This is why we believe our proposed attack is powerful : even if the samples are identified as potential outliers , they will not appear suspicious upon human inspection . We modified the text to better explain our argument . - We chose the parameter tau by manually inspecting different values of \\tau on a 100 images ."}], "0": {"review_id": "HJg6e2CcK7-0", "review_text": " This work explores backdoor attacks -- attacks that alter a fraction of training examples which can alter inference -- while ensuring that the poisoned inputs are consisten t with their labels. These attacks are attained through either a GAN mechanism or using adversarial perturbations. The ideas proposed (i.e. GAN mechanism and adversarial mechanism) are interesting additions to this literaature. I found the observation of greater effectiveness of adversa rial mechanism particularly interesting. The paper also does a good job of investigating effectiveness of the attack under data augmentation and propooses a limited solution. Main criticism: there are a number of typos that need fixing. ~ ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the kind comments . We have updated the manuscript to fix typos ."}, "1": {"review_id": "HJg6e2CcK7-1", "review_text": "Overall I am positive about this manuscript: - I find the motivation is clear and valid. As far as I know, this is a novel contribution (my confidence is not very high on that one though - I might be unaware of related work). - The paper is well-written and organized. - Experiments are conducted systematically, although certain parts could be better explained (see my questions below). I think this paper adds an original and valuable angle to the existing literature on data poisoning attacks. I don't see any major flaws, therefore I think it should be accepted. A few points which might need clarification: - How exactly is \"attack success\" being measured? - Which model is used to generate the adversarial samples? Is this an (adversarially) pretrained model? (If that's the case, then what is the model architecture?) Or are adversarial samples generated on the fly using the currently trained/poisoned model? - At the end of Section 4.4: if the images with larger noise rely more on the backdoor, why does this have an adverse effect? Shouldn't it increase the effectiveness of the attack? - Was the data augmentation (flips, crops etc) performed before or after the poisoning pattern was applied? Minor comments: - definition of the encoding at the bottom of page 4: this should be argmax instead of max - typo in Sec. 5.1: \"to evaluate the uat a wide variety\" - repetitive sentence in Sec. 5.2: \"we find that images generated with $\\tau \\leq 0.2$ remain [fairly] plausible\" ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the kind comments and helpful suggestions . We will address points raised below : - The attack success rate ( ASR ) is computed as the fraction of inputs that are _not_ labeled with the target class but are classified as the target class after the backdoor pattern is applied ( Beginning of Section 5 ) . We have edited the manuscript to make this definition appear more prominently earlier in the paper and edited the relevant captions . - We use adversarially trained models trained with the publicly available code from https : //github.com/MadryLab/cifar10_challenge ( we train the non-wide variant both with L2 and Linf ) . The adversarial examples are generated once using this pre-trained network . Since our threat model only allows us to add examples to the training set , we can not compute these adversarial perturbations on the fly . We have edited the manuscript to incorporate this discussion . - We were also surprised initially but we believe that there is a fairly simple explanation ( outlined in Section 4.4 ) . On noisy images , the classifier learns to predict by relying on the backdoor * in the absence of strong image signal * ( since the salient image features are fairly corrupted ) . However , when evaluated on the test set with a backdoor applied , the image itself will have a strong signal ( since it will not be noisy ) that can overcome the backdoor pattern . Therefore , it is necessary for the classifier to learn to predict the backdoor even when the salient image characteristics are present . As a result , random noise is not very effective at injecting backdoors . We have updated Section 4.4 to better reflect this argument . - Since we do not have access to the training procedure , the pattern is applied before any data augmentation . This is the reason why this setting is challenging -- data augmentation might obscure the pattern . We have updated the manuscript to incorporate the other comments ."}, "2": {"review_id": "HJg6e2CcK7-2", "review_text": "This paper investigates an interesting problem, backdoor attack against neural networks. The main idea is to add a watermark pattern to the corners of the training images, so that the classifier is guided to leverage the watermark as a discriminative cue as opposed to the real content of the image. At the test stage, one can hence manipulate the classifier\u2019s predictions by adding the watermark to the test images. This paper is heavily built upon Gu et al. (2017)\u2019s work. It shows that Gu et al. (2017)\u2019s method can be easily defended by a data sanitization algorithm. To improve Gu et al.\u2019s work, the authors propose to add watermark patterns to the adversarial examples or examples interpolated in GAN\u2019s latent space. The intuition is that these examples are adversarial and hard to learn, forcing the classifier to focus on the watermark pattern instead. It is an interesting idea and an intuitive improvement over (Gu et al. 2017). However, the implementation of the idea could be improved. This paper does not propose any new attack algorithms. Instead, it investigates an existing adversarial attack method and the GAN based interpolation for the purpose of backdoor attack. As experiments are conducted on small-scale datasets, it is unclear how effective the improved backdoor attack is. Moreover, one of the main disadvantages of the proposed attack method is that simple data augmentation techniques, especially random cropping, can successfully defend against the attack. The quality of the paper writing could be improved. I had to read the paper more than twice and check the references now and then in order to understand some claims of the paper. The paper\u2019s lack of clarity was actually also raised by probably one of the coauthors of the paper; see the comment \u201cDimitris: clarify this point\u201d on Page 11. Please find some concrete suggestions below. - Figure 1 is visually not appealing at all. Perhaps find better illustrative examples. - It is worth considering to add a separate section/paragraph to describe the details of Gu et al. (2017)\u2019s method, given that this paper is heavily built upon Gu et al. (2017)\u2019s work. - It was unclear what the \u201creduced amplitude backdoor trigger\u201d means until Section 4. If a context-dependent term has to be used in the introduction, explain it or refer the readers to the right place of the paper. - Merge Sections 4.3\u20144.5 with the experiment section (Section 5). The results of Section 4.3\u20134.5 are out of context without any explanation about the experiment setups. I have some concerns about Section 3, which is the main motivation of this work. As the authors noted in Appendix A that Gu et al.\u2019s method works well with as few as 75 poised examples, the proposed sanitization algorithm would not be able to fail Gu et al.\u2019s method by only identifying 20 out of 100 poised examples. How to control the parameter $\\tau$ so that the perturbation appears plausible to humans? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful comments . We will address comments raised below . - On the novelty of our attacks . We believe that the main conceptual contribution of our work is the formulation of the clean-label attack problem and showing how these attacks can be made successful by modifying samples to be `` harder '' . The two attacks investigated are meant as proof-of-concept that this approach works with existing methods . We agree with the reviewer that designing specialized attacks for this task is a valuable research direction that could lead to even more successful attacks . - On the scale of our datasets . We do not have the resources to run an equally comprehensive study on ImageNet-scale datasets . Hence , we decided to perform more experiments on a small dataset rather than fewer experiments on a larger dataset . Note that the plots in Figure 3 and 4 involved training 50 models each . Does the reviewer have concrete concerns about the applicability of our approach to large-scale datasets ? - On the resistance of our approach to data augmentation . We have demonstrated ( Appendix B ) that simply modifying the pattern to appear in all 4 corners is already sufficient to make the attack significantly more resistant to data augmentation . Thus , we do n't consider data augmentation to be a fundamental obstacle to our attack . We believe that future work investigating different backdoor triggers can further increase the resistance of our attack to data augmentation . We thank the reviewer for concrete suggestions on improving our manuscript . We incorporated the following changes : - We replaced Figure 1 with more illustrative examples . - We modified the second paragraph of Section 2 to better explain the original Gu et al . ( 2017 ) attack . - We changed the wording of \u201c reduced amplitude backdoor trigger \u201d to `` less conspicuous backdoor trigger '' which should be clear without any further context . - The goal of Sections 4.3 - 4.5 is to provide a reader with an overview of our results before going into the experimental details . We modified these Sections to be more self-contained . - On concerns about Section 3 . We do not argue that manual inspection will find all the poisoned examples ( or enough to render the attack ineffective ) . We rather argue that if manual inspection of 300 images reveals 20 * clearly mislabelled * images , then the attack will very likely be detected leading to additional investigation and filtering . This argument illustrates a broader point -- if poisoned inputs appear suspicious upon human inspection , the attack is not truly insidious and can always be detected by more advanced filtering . This is why we believe our proposed attack is powerful : even if the samples are identified as potential outliers , they will not appear suspicious upon human inspection . We modified the text to better explain our argument . - We chose the parameter tau by manually inspecting different values of \\tau on a 100 images ."}}