{"year": "2018", "forum": "SyhRVm-Rb", "title": "Automatic Goal Generation for Reinforcement Learning Agents", "decision": "Reject", "meta_review": "In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable \"goals\" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high-dimensional state space, which seems to be be whole raison d'etre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments  (a la Figure 2) showing how this method performs on complicated tasks.\n\nI encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases).", "reviews": [{"review_id": "SyhRVm-Rb-0", "review_text": "In general I find this to be a good paper and vote for acceptance. The paper is well-written and easy to follow. The proposed approach is a useful addition to existing literature. Besides that I have not much to say except one point I would like to discuss: In 4.2 I am not fully convinced of using an adversial model for goal generation. RL algorithms generally suffer from poor stability and GANs themselves can have convergence issues. This imposes another layer of possible instability. Besides, generating useful reward function, while not trivial, can be seen as easier than solving the full RL problem. Can the authors argue why this model class was chosen over other, more simple, generative models? Furthermore, did the authors do experiments with simpler models? Related: \"We found that the LSGAN works better than other forms of GAN for our problem.\" Was this improvement minor, or major, or didn't even work with other GAN types? This question is important, because for me the big question is if this model is universal and stable in a lot of applications or requires careful fine-tuning and monitoring. --- Update: The authors addressed the major point of criticism in my review. I am now more convinced in the quality of the proposed work, and have updated my review score accordingly.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for recognizing the contribution in this paper . We agree that care must be taken to ensure stability for training the GAN . Still , our experiments show that our method outperforms the competing approaches on this problem . We chose to use a GAN rather than another generative model due to a GAN \u2019 s demonstrated ability to generate samples in high-dimensional spaces ( such as images ) , thus giving our method the potential to scale up to high-dimensional goal spaces . We did not experiment with other generative models for these tasks . Regarding a comparison of different GAN types : in our experiments , using a WGAN ( Arjovsky et al.2017 ) led to significantly more stable training than a vanilla GAN ( as in Goodfellow et al. , 2014 ) , and using an LSGAN improved the training stability even further , but not quite as dramatically . We have added these observations in the paper without additional details as it is not the focus of our work . As is stated in Section 4.2 , all results shown in our paper , across a number of different environments , use the LSGAN with the original hyperparameters reported in Mao et al.2017.In general , we \u2019 ve found GANs to be much more stable in lower dimensional state spaces than in image spaces , and many of the well known convergence issues did not happen . Therefore , no considerable fine-tuning and monitoring was needed . In future work we hope to extend our model to an even greater number of environments ."}, {"review_id": "SyhRVm-Rb-1", "review_text": "Summary: This paper proposes to use a GAN to generate goals to implement a form of curriculum learning. A goal is defined as a subset of the state space. The authors claim that this model can discover all \"goals\" in the environment and their 'difficulty', which can be measured by the success rate / reward of the policy. Hence the goal network could learn a form of curriculum, where a goal is 'good' if it is a state that the policy can reach after a (small) improvement of the current policy. Training the goal GAN is done via labels, which are states together with the achieved reward by the policy that is being learned. The benchmark problems are whether the GAN generates goals that allow the agent to reach the end of a U-maze, and a point-mass task. Authors compare GAN goal generation vs uniformly choosing a goal and 2 other methods. My overall impression is that this work addresses an interesting question, but the experimental setup / results are not clearly worked out. More broadly, the paper does not address how one can combine RL and training a goal GAN in a stable way. Pro: - Developing hierarchical learning methods to improve the sample complexity of RL is an important problem. - The paper shows that the U-maze can be 'solved' using a variety of methods that generate goals in a non-uniform way. Con: - It is not clear to me how the asymmetric self-play and SAGG-RIAC are implemented and why they are natural baselines. - It is not clear to me what the 'goals' are in the point mass experiment. This entire experiment should be explained much more clearly (+image). - It is not clear how this method compares qualitatively vs baselines (differences in goals etc). - This method doesn't seem to always outperform the asymm-selfplay baseline. The text mentions that baseline is less efficient, but this doesn't make the graph very interpretable. - The curriculum in the maze-case consists of regions that just progress along the maze, and hence is a 1-dimensional space. Hence using a manually defined set of goals should work quite well. It would be better to include such a baseline as well. - The experimental maze-setting and point-mass have a simple state / goal structure. How can this method generalize to harder problems? -- The entire method is quite complicated (e.g. training GANs can be highly unstable). How do we stabilize / balance training the GAN vs the RL problem? -- I don't see how this method could generalize to problems where the goals / subregions of space do not have a simple distribution as in the maze problem, e.g. if there are multiple ways of navigating a maze towards some final goal state. In that case, to discover a good solution, the generated goals should focus on one alternative and hence the GAN should have a unimodal distribution. How do you force the GAN in a principled way to focus on one goal in this case? How could you combine RL and training the GAN stably in that case? Detailed: - (2) is a bit strange: shouldn't the indicator say: 1( \\exists t: s_t \\in S^g )? Surely not all states in the rollout (s_0 ... s_t) are in the goal subspace: the indicator does not factorize over the union. Same for other formulas that use \\union. - Are goals overlapping or non-overlapping subsets of the state space? Definition around (1) basically says it's non-overlapping, yet the goal GAN seems to predict goals in a 2d space, hence the predicted goals are overlapping? - What are the goals that the non-uniform baselines predict? Does the GAN produce better goals? - Generating goal labels is - Paper should discuss literature on hierarchical methods that use goals learned from data and via variational methods: 1. Strategic Attentive Writer (STRAW), V. Mnih et al, NIPS 2016 2. Generating Long-term Trajectories Using Deep Hierarchical Networks. S. Zheng et al, NIPS 2016", "rating": "4: Ok but not good enough - rejection", "reply_text": "Our implementation of \u201c Asymmetric Self-Play \u201d follows directly from the description of their method from their publication . In Asymmetric Self-play , \u201c Alice \u201d proposes goals ( exactly what our Goal GAN does ) for the agent \u201c Bob \u201d to try to achieve , and Alice and Bob are both trained with reinforcement learning ( we use TRPO , with the same parameters as for our method ) . We use the \u201c repeat \u201d version of asymmetric self-play in which \u201c Bob \u201d must then learn to reach the goal that \u201c Alice \u201d proposed . In the Asymmetric Self-play paper , training is alternated between a \u201c multi-goal \u201d setup and a single \u201c target task \u201d setup . In our case we do not alternate because our \u201c target task \u201d setup is the same as the \u201c multi-goal \u201d one : we desire to train an agent that can achieve many target tasks , which is already done by the multi-goal setup ; thus we only need the \u201c multi-goal \u201d training portion of their method . Their multi-goal training method , if successful , would result in a policy in which \u201c Bob \u201d learns to achieve many goals . Since this is also the objective of our method ( described in equation 3 of our paper ) , Asymmetric Self-play is an appropriate baseline for our task . Regarding SAGG-RIAC , details of our implementation of this method can be found in Appendix E.2 . The objective of SAGG-RIAC is the same as the objective of our method , although SAGG-RIAC is usually used to train a model-based agent whereas our method also works with an agent trained in a model-free setting . Regardless , since SAGG-RIAC likewise attempts to train an agent to achieve many goals , it is also a natural baseline to compare against ."}, {"review_id": "SyhRVm-Rb-2", "review_text": "This paper proposed a method for automatic curriculum generation that allow an agent to learn to reach multiple goals in an environment with considerable sample efficiency. They use a generator network to propose tasks for the agent accomplish. The generator network is trained with GAN. In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. They compare the Goal GAN method with four baselines, including Uniform sampling, Asymmetric Self-play, SAGG-RIAC, and Rejection sampling. The proposed method is tested on two environments: Free Ant and Maze Ant. The empirical study shows that the proposed method is able to improve policies\u2019 training efficiency comparing to these baselines. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below. 1. it is unclear to me why the proposed method is able to solve tasks with sparse rewards? Is it because of the horizons of the problems considered are not long enough? The author should provide more insight for this contribution. 2. It is unclear to me how R_min and R_max as hyperparameters are obtained and how their settings affect the performance. 3. Another concern I have is regarding the generalizability of the proposed method. One of the assumption is \u201cA policy trained on a sufficient number of goals in some area of the goal-space will learn to interpolate to other goals within that area\u201d. This seems to mean that the area is convex. It might be better if some quantitative analysis can be provided to illustrate geometry of goal space (given complex motor coordination) that is feasible for the proposed method. 4. It is difficult to understand the plots in Figure 4 without more details. Do you assume for every episode, the agent starts from the same state? 5. For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough. 6. According to the experimental details, three rollouts are performed to estimate the empirical return. It there any justification why three rollouts are enough? 7. Minor comments Achieve tasks -> achieve goals or accomplish/solve tasks A variation of to -> variation of Allows a policy to quickly learn to reach \u2026-> allow an agent to be quickly learn a policy to reach\u2026 \u2026the difficulty of the generated goals -> \u2026 the difficulty of reaching ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the thorough analysis and insightful comments . In the following we answer one by one the questions , and we detail the clarifications made in the paper wherever needed . 1.Our proposed method is able to solve tasks with sparse rewards without modifying the reward function by automatically generating a curriculum over tasks . As our Problem Definition ( Sec.3 ) states in the Overall Objective ( Sec.3.2 ) , we are seeking a policy $ \\pi^ * ( \\cdot | s_t , g ) $ that can succeed at many goals $ g $ , each goal corresponding to a different task with its own sparse reward $ r^g ( s_t , a_t , s_ { t+1 } ) $ . But , although all tasks have a sparse reward , they are not all of the same difficulty ! In particular , reaching a goal nearby the starting position is very easy , and can be performed even by the randomly initialized policy . Then , once the policy has learned to reach the nearby goals ( in our navigation settings , it implies having learned some basic locomotor skills ) , it can bootstrap this acquired knowledge to attempt more complex ( further away ) goals . As explained in our Goal Labeling ( Sec 4.1 ) , our method strives to sample goals always of \u201c intermediate difficulty \u201d $ g : R_ { \\min } \\leq R^g ( \\pi_i ) \\leq R_ { \\max } $ . This means that our method will always be sampling goals such that training on them is efficient ( i.e.our policy is able to receive a sufficient amount of reward such that it can improve its performance ) , despite their sparse reward structure . If no curriculum is applied , a prohibitively long time-horizon would be needed for the policy to learn to reach the far away goals . Furthermore , many goals are actually infeasible , and no matter the time-horizon they always receive a reward of 0 . Our method minimizes wasting rollouts trying to reach such goals because they do not satisfy our condition $ R_ { \\min } \\leq R^g ( \\pi_i ) $ . 2.The hyperparameters R_min and R_max have a very clear probabilistic interpretation given in Sec.4.1 , based on analyzing Eq . ( 2 ) .R_min is the minimum success probability required to start training on a particular goal . R_max is the maximum success probability above which we prefer to concentrate training on new goals . In practice , as explained in Sec.4.3 and Appendix C , we estimate $ R^g ( \\pi_i ) $ with the rollouts collected by our RL algorithm . Therefore , each estimation is an average over two to five binary rewards ( whether the rollout succeeded or not ) , meaning that the lowest numbers it can get are 0 or \u2155 and the highest are \u2158 or 1 . In all our experiments we used R_min = 0.1 and R_max = 0.9 , but given the above analysis any $ R_min \\in ] 0 , 0.2 [ $ and $ R_max \\in ] 0.8 , 1 [ $ would have yield exactly the same result . We have not experimented with values outside this range because it might not be of practical interest to not train on goals that are already achieved more than 20 % of the time or have a policy succeeding less than 80 % of the time on the goals it is given . 3.Our assumptions do not imply convexity of the goal space . For example , we do provide quantitative analysis for the Ant-Maze environment , where we report an efficient learning of our method despite the geometry of the feasible goal space being U-shaped , as seen in Fig.4 ( we have updated the legend to more clearly identify the feasible goal space ) . Rather , the interpolation statement refers to the smoothness of the goal space with respect to the policy , i.e.the policy for reaching a specific goal that has not been sampled during training can be inferred from sampling a sufficient number nearby goals in the continuous goal space . The extrapolation statement should be understood along the lines of the explanation given in our point 1. of this rebuttal : \u201c once the training policy is able to reach the nearby goals ... it can bootstrap this acquired knowledge to attempt more complex ( further away ) goals \u201d . This is a very reasonable assumption in many learning systems , robotics in particular ."}], "0": {"review_id": "SyhRVm-Rb-0", "review_text": "In general I find this to be a good paper and vote for acceptance. The paper is well-written and easy to follow. The proposed approach is a useful addition to existing literature. Besides that I have not much to say except one point I would like to discuss: In 4.2 I am not fully convinced of using an adversial model for goal generation. RL algorithms generally suffer from poor stability and GANs themselves can have convergence issues. This imposes another layer of possible instability. Besides, generating useful reward function, while not trivial, can be seen as easier than solving the full RL problem. Can the authors argue why this model class was chosen over other, more simple, generative models? Furthermore, did the authors do experiments with simpler models? Related: \"We found that the LSGAN works better than other forms of GAN for our problem.\" Was this improvement minor, or major, or didn't even work with other GAN types? This question is important, because for me the big question is if this model is universal and stable in a lot of applications or requires careful fine-tuning and monitoring. --- Update: The authors addressed the major point of criticism in my review. I am now more convinced in the quality of the proposed work, and have updated my review score accordingly.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for recognizing the contribution in this paper . We agree that care must be taken to ensure stability for training the GAN . Still , our experiments show that our method outperforms the competing approaches on this problem . We chose to use a GAN rather than another generative model due to a GAN \u2019 s demonstrated ability to generate samples in high-dimensional spaces ( such as images ) , thus giving our method the potential to scale up to high-dimensional goal spaces . We did not experiment with other generative models for these tasks . Regarding a comparison of different GAN types : in our experiments , using a WGAN ( Arjovsky et al.2017 ) led to significantly more stable training than a vanilla GAN ( as in Goodfellow et al. , 2014 ) , and using an LSGAN improved the training stability even further , but not quite as dramatically . We have added these observations in the paper without additional details as it is not the focus of our work . As is stated in Section 4.2 , all results shown in our paper , across a number of different environments , use the LSGAN with the original hyperparameters reported in Mao et al.2017.In general , we \u2019 ve found GANs to be much more stable in lower dimensional state spaces than in image spaces , and many of the well known convergence issues did not happen . Therefore , no considerable fine-tuning and monitoring was needed . In future work we hope to extend our model to an even greater number of environments ."}, "1": {"review_id": "SyhRVm-Rb-1", "review_text": "Summary: This paper proposes to use a GAN to generate goals to implement a form of curriculum learning. A goal is defined as a subset of the state space. The authors claim that this model can discover all \"goals\" in the environment and their 'difficulty', which can be measured by the success rate / reward of the policy. Hence the goal network could learn a form of curriculum, where a goal is 'good' if it is a state that the policy can reach after a (small) improvement of the current policy. Training the goal GAN is done via labels, which are states together with the achieved reward by the policy that is being learned. The benchmark problems are whether the GAN generates goals that allow the agent to reach the end of a U-maze, and a point-mass task. Authors compare GAN goal generation vs uniformly choosing a goal and 2 other methods. My overall impression is that this work addresses an interesting question, but the experimental setup / results are not clearly worked out. More broadly, the paper does not address how one can combine RL and training a goal GAN in a stable way. Pro: - Developing hierarchical learning methods to improve the sample complexity of RL is an important problem. - The paper shows that the U-maze can be 'solved' using a variety of methods that generate goals in a non-uniform way. Con: - It is not clear to me how the asymmetric self-play and SAGG-RIAC are implemented and why they are natural baselines. - It is not clear to me what the 'goals' are in the point mass experiment. This entire experiment should be explained much more clearly (+image). - It is not clear how this method compares qualitatively vs baselines (differences in goals etc). - This method doesn't seem to always outperform the asymm-selfplay baseline. The text mentions that baseline is less efficient, but this doesn't make the graph very interpretable. - The curriculum in the maze-case consists of regions that just progress along the maze, and hence is a 1-dimensional space. Hence using a manually defined set of goals should work quite well. It would be better to include such a baseline as well. - The experimental maze-setting and point-mass have a simple state / goal structure. How can this method generalize to harder problems? -- The entire method is quite complicated (e.g. training GANs can be highly unstable). How do we stabilize / balance training the GAN vs the RL problem? -- I don't see how this method could generalize to problems where the goals / subregions of space do not have a simple distribution as in the maze problem, e.g. if there are multiple ways of navigating a maze towards some final goal state. In that case, to discover a good solution, the generated goals should focus on one alternative and hence the GAN should have a unimodal distribution. How do you force the GAN in a principled way to focus on one goal in this case? How could you combine RL and training the GAN stably in that case? Detailed: - (2) is a bit strange: shouldn't the indicator say: 1( \\exists t: s_t \\in S^g )? Surely not all states in the rollout (s_0 ... s_t) are in the goal subspace: the indicator does not factorize over the union. Same for other formulas that use \\union. - Are goals overlapping or non-overlapping subsets of the state space? Definition around (1) basically says it's non-overlapping, yet the goal GAN seems to predict goals in a 2d space, hence the predicted goals are overlapping? - What are the goals that the non-uniform baselines predict? Does the GAN produce better goals? - Generating goal labels is - Paper should discuss literature on hierarchical methods that use goals learned from data and via variational methods: 1. Strategic Attentive Writer (STRAW), V. Mnih et al, NIPS 2016 2. Generating Long-term Trajectories Using Deep Hierarchical Networks. S. Zheng et al, NIPS 2016", "rating": "4: Ok but not good enough - rejection", "reply_text": "Our implementation of \u201c Asymmetric Self-Play \u201d follows directly from the description of their method from their publication . In Asymmetric Self-play , \u201c Alice \u201d proposes goals ( exactly what our Goal GAN does ) for the agent \u201c Bob \u201d to try to achieve , and Alice and Bob are both trained with reinforcement learning ( we use TRPO , with the same parameters as for our method ) . We use the \u201c repeat \u201d version of asymmetric self-play in which \u201c Bob \u201d must then learn to reach the goal that \u201c Alice \u201d proposed . In the Asymmetric Self-play paper , training is alternated between a \u201c multi-goal \u201d setup and a single \u201c target task \u201d setup . In our case we do not alternate because our \u201c target task \u201d setup is the same as the \u201c multi-goal \u201d one : we desire to train an agent that can achieve many target tasks , which is already done by the multi-goal setup ; thus we only need the \u201c multi-goal \u201d training portion of their method . Their multi-goal training method , if successful , would result in a policy in which \u201c Bob \u201d learns to achieve many goals . Since this is also the objective of our method ( described in equation 3 of our paper ) , Asymmetric Self-play is an appropriate baseline for our task . Regarding SAGG-RIAC , details of our implementation of this method can be found in Appendix E.2 . The objective of SAGG-RIAC is the same as the objective of our method , although SAGG-RIAC is usually used to train a model-based agent whereas our method also works with an agent trained in a model-free setting . Regardless , since SAGG-RIAC likewise attempts to train an agent to achieve many goals , it is also a natural baseline to compare against ."}, "2": {"review_id": "SyhRVm-Rb-2", "review_text": "This paper proposed a method for automatic curriculum generation that allow an agent to learn to reach multiple goals in an environment with considerable sample efficiency. They use a generator network to propose tasks for the agent accomplish. The generator network is trained with GAN. In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. They compare the Goal GAN method with four baselines, including Uniform sampling, Asymmetric Self-play, SAGG-RIAC, and Rejection sampling. The proposed method is tested on two environments: Free Ant and Maze Ant. The empirical study shows that the proposed method is able to improve policies\u2019 training efficiency comparing to these baselines. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below. 1. it is unclear to me why the proposed method is able to solve tasks with sparse rewards? Is it because of the horizons of the problems considered are not long enough? The author should provide more insight for this contribution. 2. It is unclear to me how R_min and R_max as hyperparameters are obtained and how their settings affect the performance. 3. Another concern I have is regarding the generalizability of the proposed method. One of the assumption is \u201cA policy trained on a sufficient number of goals in some area of the goal-space will learn to interpolate to other goals within that area\u201d. This seems to mean that the area is convex. It might be better if some quantitative analysis can be provided to illustrate geometry of goal space (given complex motor coordination) that is feasible for the proposed method. 4. It is difficult to understand the plots in Figure 4 without more details. Do you assume for every episode, the agent starts from the same state? 5. For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough. 6. According to the experimental details, three rollouts are performed to estimate the empirical return. It there any justification why three rollouts are enough? 7. Minor comments Achieve tasks -> achieve goals or accomplish/solve tasks A variation of to -> variation of Allows a policy to quickly learn to reach \u2026-> allow an agent to be quickly learn a policy to reach\u2026 \u2026the difficulty of the generated goals -> \u2026 the difficulty of reaching ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the thorough analysis and insightful comments . In the following we answer one by one the questions , and we detail the clarifications made in the paper wherever needed . 1.Our proposed method is able to solve tasks with sparse rewards without modifying the reward function by automatically generating a curriculum over tasks . As our Problem Definition ( Sec.3 ) states in the Overall Objective ( Sec.3.2 ) , we are seeking a policy $ \\pi^ * ( \\cdot | s_t , g ) $ that can succeed at many goals $ g $ , each goal corresponding to a different task with its own sparse reward $ r^g ( s_t , a_t , s_ { t+1 } ) $ . But , although all tasks have a sparse reward , they are not all of the same difficulty ! In particular , reaching a goal nearby the starting position is very easy , and can be performed even by the randomly initialized policy . Then , once the policy has learned to reach the nearby goals ( in our navigation settings , it implies having learned some basic locomotor skills ) , it can bootstrap this acquired knowledge to attempt more complex ( further away ) goals . As explained in our Goal Labeling ( Sec 4.1 ) , our method strives to sample goals always of \u201c intermediate difficulty \u201d $ g : R_ { \\min } \\leq R^g ( \\pi_i ) \\leq R_ { \\max } $ . This means that our method will always be sampling goals such that training on them is efficient ( i.e.our policy is able to receive a sufficient amount of reward such that it can improve its performance ) , despite their sparse reward structure . If no curriculum is applied , a prohibitively long time-horizon would be needed for the policy to learn to reach the far away goals . Furthermore , many goals are actually infeasible , and no matter the time-horizon they always receive a reward of 0 . Our method minimizes wasting rollouts trying to reach such goals because they do not satisfy our condition $ R_ { \\min } \\leq R^g ( \\pi_i ) $ . 2.The hyperparameters R_min and R_max have a very clear probabilistic interpretation given in Sec.4.1 , based on analyzing Eq . ( 2 ) .R_min is the minimum success probability required to start training on a particular goal . R_max is the maximum success probability above which we prefer to concentrate training on new goals . In practice , as explained in Sec.4.3 and Appendix C , we estimate $ R^g ( \\pi_i ) $ with the rollouts collected by our RL algorithm . Therefore , each estimation is an average over two to five binary rewards ( whether the rollout succeeded or not ) , meaning that the lowest numbers it can get are 0 or \u2155 and the highest are \u2158 or 1 . In all our experiments we used R_min = 0.1 and R_max = 0.9 , but given the above analysis any $ R_min \\in ] 0 , 0.2 [ $ and $ R_max \\in ] 0.8 , 1 [ $ would have yield exactly the same result . We have not experimented with values outside this range because it might not be of practical interest to not train on goals that are already achieved more than 20 % of the time or have a policy succeeding less than 80 % of the time on the goals it is given . 3.Our assumptions do not imply convexity of the goal space . For example , we do provide quantitative analysis for the Ant-Maze environment , where we report an efficient learning of our method despite the geometry of the feasible goal space being U-shaped , as seen in Fig.4 ( we have updated the legend to more clearly identify the feasible goal space ) . Rather , the interpolation statement refers to the smoothness of the goal space with respect to the policy , i.e.the policy for reaching a specific goal that has not been sampled during training can be inferred from sampling a sufficient number nearby goals in the continuous goal space . The extrapolation statement should be understood along the lines of the explanation given in our point 1. of this rebuttal : \u201c once the training policy is able to reach the nearby goals ... it can bootstrap this acquired knowledge to attempt more complex ( further away ) goals \u201d . This is a very reasonable assumption in many learning systems , robotics in particular ."}}