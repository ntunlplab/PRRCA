{"year": "2018", "forum": "rkHywl-A-", "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "The AIRL is presented as a scalable inverse reinforcement learning algorithm. A key idea is to produce \"disentangled rewards\", which are invariant to changing dynamics; this is done by having the rewards depend only on the current state. There are some similarities with GAIL and the authors argue that this is effectively a concrete implementation of GAN-GCL that actually works.  The results look promising to me and the portability aspect is neat and useful!\n\nIn general, the reviewers found this paper and its results interesting and I think the rebuttal addressed many of the concerns. I am happy that the reproducibility report is positive which helped me put this otherwise potentially borderline paper into the 'accept' bucket.", "reviews": [{"review_id": "rkHywl-A--0", "review_text": "The paper provides an approach to learning reward functions in high-dimensional domains, showing that it performs comparably to other recent approaches to this problem in the imitation-learning setting. It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state-action or state-action-state. It uses this property to produce \"disentangled rewards\", demonstrating that they transfer well to the same task under different transition dynamics. The need for \"state-only\" rewards is a useful insight and is covered fairly well in the paper. The need for an \"adversarial\" approach is not justified as fully, but perhaps is a consequence of recent work. The experiments are thorough, although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak. Detailed feedback: \"deployed in at test-time on environments\" -> \"deployed at test time in environments\"? \"which can effectively recover disentangle the goals\" -> \"which can effectively disentangle the goals\"? \"it allows for sub-optimality in demonstrations, and removes ambiguity between demonstrations and the expert policy\": I am not certain what is being described here and it doesn't appear to come up again in the paper. Perhaps remove it? \"r high-dimensional (Finn et al., 2016b) Wulfmeier\" -> \"r high-dimensional (Finn et al., 2016b). Wulfmeier\". \"also consider learning cost function with\" -> \"also consider learning cost functions with\"? \"o learn nonlinear cost function have\" -> \"o learn nonlinear cost functions have\". \" are not robust the environment changes\" -> \" are not robust to environment changes\"? \"We present a short proof sketch\": It is unclear to me what is being proven here. Please state the theorem. \"In the method presented in Section 4, we cannot learn a state-only reward function\": I'm not seeing that. Or, maybe I'm confused between rewards depending on s vs. s,a vs. s,a,s'. Again, an explicit theorem statement might remove some confusion here. \"AIRLperforms\" -> \"AIRL performs\". Figure 2: The blue and green colors look very similar to me. I'd recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret. \"must reach to goal\" -> \"must reach the goal\"? \"pointmass\" -> \"point mass\". (Multiple times.) Amin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.) Perhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the detailed feedback . We have included all of the typo corrections and clarifications , as well as included state-only runs in the imitation learning experiments ( Section 7.3 ) . As detailed below , we believe that we have addressed all of the issues raised in your review , but we would appreciate any further feedback you might offer . > The need for an `` adversarial '' approach is not justified as fully , but perhaps is a consequence of recent work . Adversarial approaches are an inherent consequence of using sampling-based methods for training energy-based models , and we \u2019 ve edited Section 2 , paragraph 2 to make this more clear . There is in fact no other ( known ) choice for doing this : any method that does maxent IRL and generates samples ( rather than assuming known dynamics ) must be adversarial in nature , as shown by Finn16a . Traditional methods like tabular MaxEnt IRL [ Ziebart 08 ] have an adversarial nature as they must alternate between an inner-loop RL problem ( the sampler ) and updating the reward function ( the discriminator ) . > Although the connection to the motivation in the abstract ( wanting to avoid reward engineering ) is weak . We \u2019 ve slightly modified the paragraph before section 7.1 to make this connection more clear . We use environments where a reward function is available for the purpose of easily collecting demonstrations ( otherwise we would need to resort to motion capture or teleoperation ) . However the experimental setup after demo collection is exactly the same as one would encounter while using IRL when a ground truth reward is not available . > Amin , Jiang , and Singh 's work on efficiently learning a transferable reward function seems relevant here . ( Although , it might not be published yet : https : //arxiv.org/pdf/1705.05427.pdf . ) Amin , Jian & Singh \u2019 s work is indeed relevant and we have also included it in the related work section . > Perhaps the final experiment should have included state-only runs . I 'm guessing that they did n't work out too well , but it would still be good to know how they compare . We \u2019 ve included these in the experiments . State-only runs perform slightly worse as expected , since the true reward has torque penalty terms which depend on the action , and can not be captured by the model . However the performance isn \u2019 t so bad that the agent fails to solve the task ."}, {"review_id": "rkHywl-A--1", "review_text": "SUMMARY: This paper considers the Inverse Reinforcement Learning (IRL) problem, and particularly suggests a method that obtains a reward function that is robust to the change of dynamics of the MDP. It starts from formulating the problem within the MaxEnt IRL framework of Ziebart et al. (2008). The challenge of MaxEnt IRL is the computation of a partition function. Guided Cost Learning (GCL) of Finn et al. (2016b) is an approximation of MaxEnt IRL that uses an adaptive importance sampler to estimate the partition function. This can be shown to be a form of GAN, obtained by using a specific discriminator [Finn et al. (2016a)]. If the discriminator directly works with trajectories tau, the result would be GAN-GCL. But this leads to high variance estimates, so the paper suggests using a single state-action formulation, in which the discriminator f_theta(s,a) is a function of (s,a) instead of the trajectory. The optimal solution of this discriminator is to have f(s,a) = A(s,a) \u2014 the advantage function. The paper, however, argues that the advantage function is \u201centangled\u201d with the dynamics, and this is undesirable. So it modified the discriminator to learn a function that is a combination of two terms, one only depends on state-action and the other depends on state, and has the form of shaped reward transformation. EVALUATION: This is an interesting paper with good empirical results. As I am not very familiar with the work of Finn et al. (2016a) and Finn et al. (2016b), I have not verified the detail of derivations of this new paper very closely. That being said, I have some comments and questions: * The MaxEnt IRL formulation of this work, which assumes that p_theta(tau) is proportional to exp( r_theta (tau) ), comes from [Ziebart et al., 2008] and assumes a deterministic dynamics. Ziebart\u2019s PhD dissertation [Ziebart, 2010] or the following paper show that the formulation is different for stochastic dynamics: Ziebart, Bagnell, Dey, \u201cThe Principle of Maximum Causal Entropy for Estimating Interacting Processes,\u201d IEEE Trans. on IT, 2013. Is it still a reasonable thing to develop based on this earlier, an inaccurate, formulation? * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants. It is suggested that since the only items on both sides of the equation on top of p. 13 depend on s\u2019 are h* and V, they should be equal. This would be true if s\u2019 could be chosen arbitrararily. But s\u2019 would be uniquely determined by s for a deterministic dynamics. In that case, this conclusion is not obvious anymore. Consider the state space to be integers 0, 1, 2, 3, \u2026 . Suppose the dynamics is that whenever we are at state s (which is an integer), at the next time step the state decreases toward 1, that is s\u2019 = phi(s,a) = s - 1; unless s = 0, which we just stay at s\u2019 = s = 0. This is independent of actions. Also define r(s) = 1/s for s>=1 and r(0) = 0. Suppose the discount factor is gamma = 1 (note that in Appendix B.1, the undiscounted case is studied, so I assume gamma = 1 is acceptable). With this choices, the value function V(s) = 1/s + 1/(s-1) + \u2026 + 1/1 = H_s, i.e., the Harmonic function. The advantage function is zero. So we can choose g*(s) = 0, and h*(s) = h*(s\u2019) = 1. This is in contrast to the conclusion that h*(s\u2019) = V(s\u2019) + c, which would be H_s + c, and g*(s) = r(s) = 1/s. (In fact, nothing is special about this choice of reward and dynamics.) Am I missing something obvious here? Also please discuss how ergodicity leads to the conclusion that spaces of s\u2019 and s are identical. What does \u201cspace of s\u201d mean? Do you mean the support of s? Please make the argument more rigorous. * Please make the argument of Section 5.1 more rigorous.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the constructive feedback . We \u2019 ve incorporated your comments and clarified certain points of the paper below . Please let us know if there are other additional issues which need clarification . > The MaxEnt IRL formulation of this work , which assumes that p_theta ( tau ) is proportional to exp ( r_theta ( tau ) ) , comes from [ Ziebart et al. , 2008 ] and assumes a deterministic dynamics . Ziebart \u2019 s PhD dissertation [ Ziebart , 2010 ] or the following paper show that the formulation is different for stochastic dynamics . Is it still a reasonable thing to develop based on this earlier , an inaccurate , formulation ? We have updated the background ( section 3 ) and appendix ( section A ) to use the maximum causal entropy framework rather than the earlier maximum entropy framework of [ Ziebart 08 ] . Our algorithm requires no changes since the causal entropy framework more accurately describes what we were doing in the first place ( our old derivations were valid in the deterministic case , where MaxEnt and MaxCausalEnt are identical , but in the stochastic case , our approach in fact matches MaxCausalEnt ) . > * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants . Also please discuss how ergodicity leads to the conclusion that spaces of s \u2019 and s are identical . What does \u201c space of s \u201d mean ? Do you mean the support of s ? Please make the argument more rigorous . * Please make the argument of Section 5.1 more rigorous . We \u2019 ve provided more formal proofs for Section 5 and the appendix . In order to fix the statements , we \u2019 ve changed the condition on the dynamics - a major component is that it requires that each state be reachable from > 1 other state within one step . Ergodicity is neither a sufficient nor necessary condition on the dynamics , but special cases such as an ergodic MDP with self-transitions at each state satisfies the new condition ( though the minimum necessary conditions are less restrictive ) ."}, {"review_id": "rkHywl-A--2", "review_text": "This paper revisits the generative adversarial network guided cost learning (GAN-GCL) algorithm presented last year. The authors argue learning rewards from sampled trajectories has a high variance. Instead, they propose to learn a generative model wherein actions are sampled as a function of states. The same energy model is used for sampling actions: the probability of an action is proportional to the exponential of its reward. To avoid overfitting the expert's demonstrations (by mimicking the actions directly instead of learning a reward that can be generalized to different dynamics), the authors propose to learn rewards that depend only on states, and not on actions. Also, the proposed reward function includes a shaping term, in order to cover all possible transformations of the reward function that could have been behind the expert's actions. The authors argue formally that this is necessary to disentangle the reward function from the dynamics. Th paper also demonstrates this argument empirically (e.g. Figure 1). This paper is well-written and technically sound. The empirical evaluations seem to be supporting the main claims of the paper. The paper lacks a little bit in novelty since it is basically a variante of GAN-GCL, but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments. The empirical evaluations could also be strengthened with experiments in higher-dimensional systems (like video games). \"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" This is not an assumption, it's the form of the solution we get by maximizing the entropy (for regularization). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful feedback . We \u2019 ve incorporated the suggestions to the best of our ability , and clarified portions of the paper , as described below . > `` Under maximum entropy IRL , we assume the demonstrations are drawn from an optimal policy p ( \\tau ) \\propto exp ( r ( tau ) ) '' This is not an assumption , it 's the form of the solution we get by maximizing the entropy ( for regularization ) . We \u2019 ve modified Section 3 to remove this ambiguity ( note that we \u2019 ve also modified the section to use the causal entropy framework as requested by another reviewer ) . This statement was referring to the fact that we are assuming the expert is drawing samples from the distribution p ( tau ) , not the fact that p ( tau ) \\propto exp ( r ( tau ) ) . > `` The paper lacks a little bit in novelty since it is basically a variant of GAN-GCL , but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments . '' In regard to GAN-GCL , we would note that , although the method draws heavily on the theory in this workshop paper , it is unpublished and does not describe an implementation of any actual algorithm -- the GAN-GCL paper simply describes a theoretical connection between GANs and IRL . Our implementation of the algorithm that is closest to the one suggested by the theory in the GAN-GCL workshop paper does not perform very well in practice ( Section 7.3 ) ."}], "0": {"review_id": "rkHywl-A--0", "review_text": "The paper provides an approach to learning reward functions in high-dimensional domains, showing that it performs comparably to other recent approaches to this problem in the imitation-learning setting. It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state-action or state-action-state. It uses this property to produce \"disentangled rewards\", demonstrating that they transfer well to the same task under different transition dynamics. The need for \"state-only\" rewards is a useful insight and is covered fairly well in the paper. The need for an \"adversarial\" approach is not justified as fully, but perhaps is a consequence of recent work. The experiments are thorough, although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak. Detailed feedback: \"deployed in at test-time on environments\" -> \"deployed at test time in environments\"? \"which can effectively recover disentangle the goals\" -> \"which can effectively disentangle the goals\"? \"it allows for sub-optimality in demonstrations, and removes ambiguity between demonstrations and the expert policy\": I am not certain what is being described here and it doesn't appear to come up again in the paper. Perhaps remove it? \"r high-dimensional (Finn et al., 2016b) Wulfmeier\" -> \"r high-dimensional (Finn et al., 2016b). Wulfmeier\". \"also consider learning cost function with\" -> \"also consider learning cost functions with\"? \"o learn nonlinear cost function have\" -> \"o learn nonlinear cost functions have\". \" are not robust the environment changes\" -> \" are not robust to environment changes\"? \"We present a short proof sketch\": It is unclear to me what is being proven here. Please state the theorem. \"In the method presented in Section 4, we cannot learn a state-only reward function\": I'm not seeing that. Or, maybe I'm confused between rewards depending on s vs. s,a vs. s,a,s'. Again, an explicit theorem statement might remove some confusion here. \"AIRLperforms\" -> \"AIRL performs\". Figure 2: The blue and green colors look very similar to me. I'd recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret. \"must reach to goal\" -> \"must reach the goal\"? \"pointmass\" -> \"point mass\". (Multiple times.) Amin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.) Perhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the detailed feedback . We have included all of the typo corrections and clarifications , as well as included state-only runs in the imitation learning experiments ( Section 7.3 ) . As detailed below , we believe that we have addressed all of the issues raised in your review , but we would appreciate any further feedback you might offer . > The need for an `` adversarial '' approach is not justified as fully , but perhaps is a consequence of recent work . Adversarial approaches are an inherent consequence of using sampling-based methods for training energy-based models , and we \u2019 ve edited Section 2 , paragraph 2 to make this more clear . There is in fact no other ( known ) choice for doing this : any method that does maxent IRL and generates samples ( rather than assuming known dynamics ) must be adversarial in nature , as shown by Finn16a . Traditional methods like tabular MaxEnt IRL [ Ziebart 08 ] have an adversarial nature as they must alternate between an inner-loop RL problem ( the sampler ) and updating the reward function ( the discriminator ) . > Although the connection to the motivation in the abstract ( wanting to avoid reward engineering ) is weak . We \u2019 ve slightly modified the paragraph before section 7.1 to make this connection more clear . We use environments where a reward function is available for the purpose of easily collecting demonstrations ( otherwise we would need to resort to motion capture or teleoperation ) . However the experimental setup after demo collection is exactly the same as one would encounter while using IRL when a ground truth reward is not available . > Amin , Jiang , and Singh 's work on efficiently learning a transferable reward function seems relevant here . ( Although , it might not be published yet : https : //arxiv.org/pdf/1705.05427.pdf . ) Amin , Jian & Singh \u2019 s work is indeed relevant and we have also included it in the related work section . > Perhaps the final experiment should have included state-only runs . I 'm guessing that they did n't work out too well , but it would still be good to know how they compare . We \u2019 ve included these in the experiments . State-only runs perform slightly worse as expected , since the true reward has torque penalty terms which depend on the action , and can not be captured by the model . However the performance isn \u2019 t so bad that the agent fails to solve the task ."}, "1": {"review_id": "rkHywl-A--1", "review_text": "SUMMARY: This paper considers the Inverse Reinforcement Learning (IRL) problem, and particularly suggests a method that obtains a reward function that is robust to the change of dynamics of the MDP. It starts from formulating the problem within the MaxEnt IRL framework of Ziebart et al. (2008). The challenge of MaxEnt IRL is the computation of a partition function. Guided Cost Learning (GCL) of Finn et al. (2016b) is an approximation of MaxEnt IRL that uses an adaptive importance sampler to estimate the partition function. This can be shown to be a form of GAN, obtained by using a specific discriminator [Finn et al. (2016a)]. If the discriminator directly works with trajectories tau, the result would be GAN-GCL. But this leads to high variance estimates, so the paper suggests using a single state-action formulation, in which the discriminator f_theta(s,a) is a function of (s,a) instead of the trajectory. The optimal solution of this discriminator is to have f(s,a) = A(s,a) \u2014 the advantage function. The paper, however, argues that the advantage function is \u201centangled\u201d with the dynamics, and this is undesirable. So it modified the discriminator to learn a function that is a combination of two terms, one only depends on state-action and the other depends on state, and has the form of shaped reward transformation. EVALUATION: This is an interesting paper with good empirical results. As I am not very familiar with the work of Finn et al. (2016a) and Finn et al. (2016b), I have not verified the detail of derivations of this new paper very closely. That being said, I have some comments and questions: * The MaxEnt IRL formulation of this work, which assumes that p_theta(tau) is proportional to exp( r_theta (tau) ), comes from [Ziebart et al., 2008] and assumes a deterministic dynamics. Ziebart\u2019s PhD dissertation [Ziebart, 2010] or the following paper show that the formulation is different for stochastic dynamics: Ziebart, Bagnell, Dey, \u201cThe Principle of Maximum Causal Entropy for Estimating Interacting Processes,\u201d IEEE Trans. on IT, 2013. Is it still a reasonable thing to develop based on this earlier, an inaccurate, formulation? * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants. It is suggested that since the only items on both sides of the equation on top of p. 13 depend on s\u2019 are h* and V, they should be equal. This would be true if s\u2019 could be chosen arbitrararily. But s\u2019 would be uniquely determined by s for a deterministic dynamics. In that case, this conclusion is not obvious anymore. Consider the state space to be integers 0, 1, 2, 3, \u2026 . Suppose the dynamics is that whenever we are at state s (which is an integer), at the next time step the state decreases toward 1, that is s\u2019 = phi(s,a) = s - 1; unless s = 0, which we just stay at s\u2019 = s = 0. This is independent of actions. Also define r(s) = 1/s for s>=1 and r(0) = 0. Suppose the discount factor is gamma = 1 (note that in Appendix B.1, the undiscounted case is studied, so I assume gamma = 1 is acceptable). With this choices, the value function V(s) = 1/s + 1/(s-1) + \u2026 + 1/1 = H_s, i.e., the Harmonic function. The advantage function is zero. So we can choose g*(s) = 0, and h*(s) = h*(s\u2019) = 1. This is in contrast to the conclusion that h*(s\u2019) = V(s\u2019) + c, which would be H_s + c, and g*(s) = r(s) = 1/s. (In fact, nothing is special about this choice of reward and dynamics.) Am I missing something obvious here? Also please discuss how ergodicity leads to the conclusion that spaces of s\u2019 and s are identical. What does \u201cspace of s\u201d mean? Do you mean the support of s? Please make the argument more rigorous. * Please make the argument of Section 5.1 more rigorous.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the constructive feedback . We \u2019 ve incorporated your comments and clarified certain points of the paper below . Please let us know if there are other additional issues which need clarification . > The MaxEnt IRL formulation of this work , which assumes that p_theta ( tau ) is proportional to exp ( r_theta ( tau ) ) , comes from [ Ziebart et al. , 2008 ] and assumes a deterministic dynamics . Ziebart \u2019 s PhD dissertation [ Ziebart , 2010 ] or the following paper show that the formulation is different for stochastic dynamics . Is it still a reasonable thing to develop based on this earlier , an inaccurate , formulation ? We have updated the background ( section 3 ) and appendix ( section A ) to use the maximum causal entropy framework rather than the earlier maximum entropy framework of [ Ziebart 08 ] . Our algorithm requires no changes since the causal entropy framework more accurately describes what we were doing in the first place ( our old derivations were valid in the deterministic case , where MaxEnt and MaxCausalEnt are identical , but in the stochastic case , our approach in fact matches MaxCausalEnt ) . > * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants . Also please discuss how ergodicity leads to the conclusion that spaces of s \u2019 and s are identical . What does \u201c space of s \u201d mean ? Do you mean the support of s ? Please make the argument more rigorous . * Please make the argument of Section 5.1 more rigorous . We \u2019 ve provided more formal proofs for Section 5 and the appendix . In order to fix the statements , we \u2019 ve changed the condition on the dynamics - a major component is that it requires that each state be reachable from > 1 other state within one step . Ergodicity is neither a sufficient nor necessary condition on the dynamics , but special cases such as an ergodic MDP with self-transitions at each state satisfies the new condition ( though the minimum necessary conditions are less restrictive ) ."}, "2": {"review_id": "rkHywl-A--2", "review_text": "This paper revisits the generative adversarial network guided cost learning (GAN-GCL) algorithm presented last year. The authors argue learning rewards from sampled trajectories has a high variance. Instead, they propose to learn a generative model wherein actions are sampled as a function of states. The same energy model is used for sampling actions: the probability of an action is proportional to the exponential of its reward. To avoid overfitting the expert's demonstrations (by mimicking the actions directly instead of learning a reward that can be generalized to different dynamics), the authors propose to learn rewards that depend only on states, and not on actions. Also, the proposed reward function includes a shaping term, in order to cover all possible transformations of the reward function that could have been behind the expert's actions. The authors argue formally that this is necessary to disentangle the reward function from the dynamics. Th paper also demonstrates this argument empirically (e.g. Figure 1). This paper is well-written and technically sound. The empirical evaluations seem to be supporting the main claims of the paper. The paper lacks a little bit in novelty since it is basically a variante of GAN-GCL, but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments. The empirical evaluations could also be strengthened with experiments in higher-dimensional systems (like video games). \"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" This is not an assumption, it's the form of the solution we get by maximizing the entropy (for regularization). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful feedback . We \u2019 ve incorporated the suggestions to the best of our ability , and clarified portions of the paper , as described below . > `` Under maximum entropy IRL , we assume the demonstrations are drawn from an optimal policy p ( \\tau ) \\propto exp ( r ( tau ) ) '' This is not an assumption , it 's the form of the solution we get by maximizing the entropy ( for regularization ) . We \u2019 ve modified Section 3 to remove this ambiguity ( note that we \u2019 ve also modified the section to use the causal entropy framework as requested by another reviewer ) . This statement was referring to the fact that we are assuming the expert is drawing samples from the distribution p ( tau ) , not the fact that p ( tau ) \\propto exp ( r ( tau ) ) . > `` The paper lacks a little bit in novelty since it is basically a variant of GAN-GCL , but it makes it up with the inclusion of a shaping term in the rewards and with the related formal arguments . '' In regard to GAN-GCL , we would note that , although the method draws heavily on the theory in this workshop paper , it is unpublished and does not describe an implementation of any actual algorithm -- the GAN-GCL paper simply describes a theoretical connection between GANs and IRL . Our implementation of the algorithm that is closest to the one suggested by the theory in the GAN-GCL workshop paper does not perform very well in practice ( Section 7.3 ) ."}}