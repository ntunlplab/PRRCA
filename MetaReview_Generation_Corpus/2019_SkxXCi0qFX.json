{"year": "2019", "forum": "SkxXCi0qFX", "title": "ProMP: Proximal Meta-Policy Search", "decision": "Accept (Poster)", "meta_review": "The paper studies the credit assignment problem in meta-RL, proposes a new algorithm that computes the right gradient, and demonstrates its superior empirical performance over others.  The paper is well written, and all reviewers agree the work is a solid contribution to an important problem.", "reviews": [{"review_id": "SkxXCi0qFX-0", "review_text": " In this paper, the authors investigate the gradient calculation in the original MAML (Finn et al. 2017) and E-MAML (Al-Shedivat et al. 2018). By comparing the differences in the gradients of these two algorithms, the authors demonstrate the advantages of the original MAML in taking the casual dependence into account. To obtain the correct estimation of the gradient through auto-differentiation, the authors exploit the DiCE formulation. Considering the variance in the DiCE objective formulation, the authors finally propose an objective which leads to low-variance but biased gradient. The authors verify the proposed methods in meta-RL tasks and achieves comparable performances to MAML and E-MAML. Although the ultimate algorithm proposed by this paper is not far away from MAML and E-MAML, they did a quite good job in clarify the differences in the existing variants of MAML from the gradient computation perspective and reveal the potential error due to the auto-differentiation. The proposed new objective and the surrogate is well-motivated from such observation and the trade-off between variance and bias. My major concern is how big the effect is if we use (3) comparing to (4) in calculate the gradient. As the authors showed, the only difference between (3) and (4) is the weights in front of the term \\nabla_\\theta\\log\\pi_\\theta: the E-MAML is a fixed weight and the MAML is using a adaptive through the inner product. Whether the final difference in Figure 4 between MAML and E-MAML is all caused by such difference in gradient estimation is not clearly. In fact, based on the other large-scale high-dimension empirical experiments in Figure 2, it seems the difference in gradient estimator (3) and (4) does not induced too much difference in final performances between MAML and E-MAML. Based on such observation, I was wondering the consistent better performance of the proposed algorithm might not because the corrected gradient computation from the proposed objective. It might because the clip operation or other components in the algorithm. To make a more convincing argument, it will be better if the authors can evaluate different gradient within the same updates. I am willing to raise my score if the author can address the question. minor: The gradients calculation in Eq (2) and (3) are not consistent with the Algorithm and the appendix. The notation is not consistent with common usage: \\nabla^2 is actually used for denoting the Laplace operator, i.e., \\nabla^2 = \\nabla \\cdot \\nable, which is a scalar. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable feedback . The main concern of the reviewer is that the difference in performance between using equation ( 4 ) and ( 3 ) is not as significant as we claim . First , we want to clarify what might be a misunderstanding . The results labeled as MAML in Fig.2 are obtained using the original MAML implementation , which , due to the use of a normal score function estimator , computes the wrong meta-gradient instead of the one given by Eq.3 ( you can find a discussion on this in section 5 , further elaborated in the appendix ) . We have added some further explanations in section 5 to further clarify this . Overall , here is a legend for what each name refers to : : MAML : no pre-adaptation credit assignment , i.e.\\nabla J = \\nabla J_post , i.e.how MAML was implemented for the original MAML paper ( but this actually doesn \u2019 t follow the math correctly ) E-MAML : naive pre-adaptation credit assignment as in Eq.3 DiCE : ( unbiased but high variance ) credit assignment as in Eq.4 LVC : ( slightly biased but low variance ) credit assignment as in Eq.4 ProMP : our final method ( described in section 6 ) With the nomenclature clarified , let us highlight how our experiments showcase the difference w.r.t the credit assignment . First , we have added a plot showing the effect of each formulation when the same optimizer is used ( see Figure 3 ) . We performed this experiment as an ablation study in order eliminate possible influences of the outer optimizer , i.e.PPO and TRPO . These results consistently show the superior performance of the low variance version of Eq.4 ( LVC-VPG ) when compared with Eq.3 ( E-MAML-VPG ) . Due to the high variance nature of Eq.4 ( DiCE-VPG ) its performance saturates below the other formulations . This effect is discussed in section 7.2 . Second , the experiment in Fig.5 illustrates the differences w.r.t.the meta-learned pre-adaptation policy behavior . Since MAML does not assign any credit to the pre-adaptation policy , it fails so solve the task . Though , E-MAML ( Eq.3 ) is able to solve the task , it does not learn an effective task identification policy since it can only assign credit to batches of pre-adaptation trajectories . In contrast , LVC ( Eq.4 ) can assign credit to individual pre-adaptation trajectories which is reflected by its superior task identification behavior . Third , the fact that the results of MAML and E-MAML in Fig.2 and Fig.3 are comparable underpins the ineffectiveness of the naive credit assignment : there is little difference between zero pre-adaptation credit assignment and the credit assignment of E-MAML ( discussed in section 4 ) . Finally , the experiment in Fig.6 depicts computed gradients and convergence properties corresponding to Eq.3 and Eq.4 in a simple toy environment . Once more , this experiment shows the advantage of formulation I over formulation II . We have clarified this in the experiment sections of the paper , and will be happy to add further clarifications if the reviewer requests it ."}, {"review_id": "SkxXCi0qFX-1", "review_text": "In this paper, the author proposed an efficient surrogate loss for estimating Hessian in the setting of Meta-reinforcement learning (Finn.et al, 2017), which significantly reduce the variance while introducing small bias. The author verified their proposed method with other meta-learning algorithms on the Mujoco benchmarks. The author also compared with unbiased higher order gradient estimation method-DiCE in terms of gradient variance and average return. The work is essentially important due to the need for second-order gradient estimation for meta-learning (Finn et al., 2017) and other related work such as multi-agent RL. The results look promising and the method is easy to implement. I have two detail questions about the experiment: 1) As the author states, the new proposed method introduces bias while reducing variance significantly. It is necessary to examine the MSE, Bias, Variance of the gradient estimatorsquantitatively for the proposed and related baseline methods (including MAML, E-MAML-TRPO, LVC-VPG, etc). If the bias is not a big issue empirically, the proposed method is good to use in practice. 2) The author should add DiCE in the benchmark in section 7.1, which will verify its advantage over DiCE thoroughly. Overall this is a good paper and I vote for acceptance. Finn, Chelsea, et al. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017. Foerster, Jakob, et al. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" ICML 2018.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the valuable feedback provided . As suggested by the reviewer , we have added more experiments in order to underpin the advantage of the LVC over the DiCE estimator . In particular , we have included DiCE-VPG in the benchmark section 7.1 . Results in all environments demonstrate that the learning performance of DiCE is inferior to LVC . In many of the environments , DiCE learns very slowly when compared to the other methods . We ascribe the poor learning performance of DiCE to the high variance of its meta-gradient estimates . To further strengthen this hypothesis , we have extended the meta-gradient variance experiments to more environments and more training iterations . All in all , the bias introduced by LVC seems to make the learning a little bit more unstable when VPG is used as outer optimizer . However , the gains in data-efficiency substantially outwage this disadvantage . Ultimately , the mechanisms in ProMP that ensure proximity w.r.t.to the policy \u2019 s KL-divergence may counteract these instabilities during training , giving us a stable and efficient meta-learning algorithm . We hope that the experiments and discussions , added to the paper , further substantiate the soundness of our claims ."}, {"review_id": "SkxXCi0qFX-2", "review_text": "The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. Thought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC. Quality: strong, clarity:strong, originality:strong, significance: strong, Pros: - The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow. - The proposed method is able to improve in sample complexity, speed and convergence over past methods. - The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update. - Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory. Cons: - It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour) ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for the valuable feedback . Indeed , the LVC-VPG learning curves exhibit high noise . We believe that the bias introduced by LVC makes the learning less stable when VPG is used as outer optimizer . However , the mechanisms in ProMP that ensure proximity w.r.t.to the policy \u2019 s KL-divergence may counteract these instabilities , explaining why ProMP works so well in practice . Following the suggestion of the reviewer we have extended our comparison and the analysis of the variance . In particular , we added learning curves for DiCE-VPG to the benchmarks in section 7.1 . Furthermore , we have extended the analysis of the variance to more environments and and more training iterations . The result show that LVC has a substantially higher data-efficiency and its meta-gradients consistently exhibit a lower variance than DiCE . We hope that this results further underpin the soundness of our claims and show the importance of our method ."}], "0": {"review_id": "SkxXCi0qFX-0", "review_text": " In this paper, the authors investigate the gradient calculation in the original MAML (Finn et al. 2017) and E-MAML (Al-Shedivat et al. 2018). By comparing the differences in the gradients of these two algorithms, the authors demonstrate the advantages of the original MAML in taking the casual dependence into account. To obtain the correct estimation of the gradient through auto-differentiation, the authors exploit the DiCE formulation. Considering the variance in the DiCE objective formulation, the authors finally propose an objective which leads to low-variance but biased gradient. The authors verify the proposed methods in meta-RL tasks and achieves comparable performances to MAML and E-MAML. Although the ultimate algorithm proposed by this paper is not far away from MAML and E-MAML, they did a quite good job in clarify the differences in the existing variants of MAML from the gradient computation perspective and reveal the potential error due to the auto-differentiation. The proposed new objective and the surrogate is well-motivated from such observation and the trade-off between variance and bias. My major concern is how big the effect is if we use (3) comparing to (4) in calculate the gradient. As the authors showed, the only difference between (3) and (4) is the weights in front of the term \\nabla_\\theta\\log\\pi_\\theta: the E-MAML is a fixed weight and the MAML is using a adaptive through the inner product. Whether the final difference in Figure 4 between MAML and E-MAML is all caused by such difference in gradient estimation is not clearly. In fact, based on the other large-scale high-dimension empirical experiments in Figure 2, it seems the difference in gradient estimator (3) and (4) does not induced too much difference in final performances between MAML and E-MAML. Based on such observation, I was wondering the consistent better performance of the proposed algorithm might not because the corrected gradient computation from the proposed objective. It might because the clip operation or other components in the algorithm. To make a more convincing argument, it will be better if the authors can evaluate different gradient within the same updates. I am willing to raise my score if the author can address the question. minor: The gradients calculation in Eq (2) and (3) are not consistent with the Algorithm and the appendix. The notation is not consistent with common usage: \\nabla^2 is actually used for denoting the Laplace operator, i.e., \\nabla^2 = \\nabla \\cdot \\nable, which is a scalar. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable feedback . The main concern of the reviewer is that the difference in performance between using equation ( 4 ) and ( 3 ) is not as significant as we claim . First , we want to clarify what might be a misunderstanding . The results labeled as MAML in Fig.2 are obtained using the original MAML implementation , which , due to the use of a normal score function estimator , computes the wrong meta-gradient instead of the one given by Eq.3 ( you can find a discussion on this in section 5 , further elaborated in the appendix ) . We have added some further explanations in section 5 to further clarify this . Overall , here is a legend for what each name refers to : : MAML : no pre-adaptation credit assignment , i.e.\\nabla J = \\nabla J_post , i.e.how MAML was implemented for the original MAML paper ( but this actually doesn \u2019 t follow the math correctly ) E-MAML : naive pre-adaptation credit assignment as in Eq.3 DiCE : ( unbiased but high variance ) credit assignment as in Eq.4 LVC : ( slightly biased but low variance ) credit assignment as in Eq.4 ProMP : our final method ( described in section 6 ) With the nomenclature clarified , let us highlight how our experiments showcase the difference w.r.t the credit assignment . First , we have added a plot showing the effect of each formulation when the same optimizer is used ( see Figure 3 ) . We performed this experiment as an ablation study in order eliminate possible influences of the outer optimizer , i.e.PPO and TRPO . These results consistently show the superior performance of the low variance version of Eq.4 ( LVC-VPG ) when compared with Eq.3 ( E-MAML-VPG ) . Due to the high variance nature of Eq.4 ( DiCE-VPG ) its performance saturates below the other formulations . This effect is discussed in section 7.2 . Second , the experiment in Fig.5 illustrates the differences w.r.t.the meta-learned pre-adaptation policy behavior . Since MAML does not assign any credit to the pre-adaptation policy , it fails so solve the task . Though , E-MAML ( Eq.3 ) is able to solve the task , it does not learn an effective task identification policy since it can only assign credit to batches of pre-adaptation trajectories . In contrast , LVC ( Eq.4 ) can assign credit to individual pre-adaptation trajectories which is reflected by its superior task identification behavior . Third , the fact that the results of MAML and E-MAML in Fig.2 and Fig.3 are comparable underpins the ineffectiveness of the naive credit assignment : there is little difference between zero pre-adaptation credit assignment and the credit assignment of E-MAML ( discussed in section 4 ) . Finally , the experiment in Fig.6 depicts computed gradients and convergence properties corresponding to Eq.3 and Eq.4 in a simple toy environment . Once more , this experiment shows the advantage of formulation I over formulation II . We have clarified this in the experiment sections of the paper , and will be happy to add further clarifications if the reviewer requests it ."}, "1": {"review_id": "SkxXCi0qFX-1", "review_text": "In this paper, the author proposed an efficient surrogate loss for estimating Hessian in the setting of Meta-reinforcement learning (Finn.et al, 2017), which significantly reduce the variance while introducing small bias. The author verified their proposed method with other meta-learning algorithms on the Mujoco benchmarks. The author also compared with unbiased higher order gradient estimation method-DiCE in terms of gradient variance and average return. The work is essentially important due to the need for second-order gradient estimation for meta-learning (Finn et al., 2017) and other related work such as multi-agent RL. The results look promising and the method is easy to implement. I have two detail questions about the experiment: 1) As the author states, the new proposed method introduces bias while reducing variance significantly. It is necessary to examine the MSE, Bias, Variance of the gradient estimatorsquantitatively for the proposed and related baseline methods (including MAML, E-MAML-TRPO, LVC-VPG, etc). If the bias is not a big issue empirically, the proposed method is good to use in practice. 2) The author should add DiCE in the benchmark in section 7.1, which will verify its advantage over DiCE thoroughly. Overall this is a good paper and I vote for acceptance. Finn, Chelsea, et al. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017. Foerster, Jakob, et al. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" ICML 2018.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the valuable feedback provided . As suggested by the reviewer , we have added more experiments in order to underpin the advantage of the LVC over the DiCE estimator . In particular , we have included DiCE-VPG in the benchmark section 7.1 . Results in all environments demonstrate that the learning performance of DiCE is inferior to LVC . In many of the environments , DiCE learns very slowly when compared to the other methods . We ascribe the poor learning performance of DiCE to the high variance of its meta-gradient estimates . To further strengthen this hypothesis , we have extended the meta-gradient variance experiments to more environments and more training iterations . All in all , the bias introduced by LVC seems to make the learning a little bit more unstable when VPG is used as outer optimizer . However , the gains in data-efficiency substantially outwage this disadvantage . Ultimately , the mechanisms in ProMP that ensure proximity w.r.t.to the policy \u2019 s KL-divergence may counteract these instabilities during training , giving us a stable and efficient meta-learning algorithm . We hope that the experiments and discussions , added to the paper , further substantiate the soundness of our claims ."}, "2": {"review_id": "SkxXCi0qFX-2", "review_text": "The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. Thought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC. Quality: strong, clarity:strong, originality:strong, significance: strong, Pros: - The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow. - The proposed method is able to improve in sample complexity, speed and convergence over past methods. - The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update. - Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory. Cons: - It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour) ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for the valuable feedback . Indeed , the LVC-VPG learning curves exhibit high noise . We believe that the bias introduced by LVC makes the learning less stable when VPG is used as outer optimizer . However , the mechanisms in ProMP that ensure proximity w.r.t.to the policy \u2019 s KL-divergence may counteract these instabilities , explaining why ProMP works so well in practice . Following the suggestion of the reviewer we have extended our comparison and the analysis of the variance . In particular , we added learning curves for DiCE-VPG to the benchmarks in section 7.1 . Furthermore , we have extended the analysis of the variance to more environments and and more training iterations . The result show that LVC has a substantially higher data-efficiency and its meta-gradients consistently exhibit a lower variance than DiCE . We hope that this results further underpin the soundness of our claims and show the importance of our method ."}}