{"year": "2019", "forum": "ByxGSsR9FQ", "title": "L2-Nonexpansive Neural Networks", "decision": "Accept (Poster)", "meta_review": "\n* Strengths\n\nThis paper studies adversarial robustness to perturbations that are bounded in the L2 norm. It is motivated by a theoretical sufficient condition (non-expansiveness) but rather than trying to formally verify robustness, it uses this condition as inspiration, modifying standard network architectures in several ways to encourage non-expansiveness while mostly preserving computational efficiency and accuracy. This \u201ctheory-inspired practically-focused\u201d hybrid is a rare perspective in this area and could fruitfully inspire further improvements. Finally, the paper came under substantial scrutiny during the review period (there are 65 comments on the page) and the authors have convincingly answered a number of technical criticisms.\n\n* Weaknesses\n\nOne reviewer and some commenters were concerned that the L2 norm is not a realistic norm to measure adversarial attacks in. There were also concerns that the empirical level of robustness of the network was too weak to be meaningful. In addition, while some parts of the experiments were thorough and some parts of the paper were well-presented, the quality was not uniform throughout. Finally, while the proposed changes improve adversarial robustness, they also decrease the accuracy of the network on clean examples (this is to be expected but may be an issue in practice).\n\n* Discussion\n\nThere was substantial disagreement on whether to accept the paper. On the one hand, there has been limited progress on robustness to adversarial examples (even under simple norms such as the L2 norm) and most methods that do work are based on formal verification and therefore quite computationally expensive. On the other hand, simple norms such as the L2 norm are somewhat contrived and mainly chosen for convenience (although doing well in the L2 norm is a necessary condition for being robust to more general attacks). Moreover, the empirical results are currently too weak to confer meaningful robustness even under the L2 norm.\n\n* Decision\n\nWhile I agree with the reviewers and commenters who are skeptical of the L2 norm model (and would very much like to see approaches that consider more realistic threat models), I decided to accept the paper for two reasons: first, doing well in L2 is a necessary condition to doing well in more general models, and the ideas and approach here are simple enough that they might provide inspiration in these more general models as well. Additionally, this was one of the strongest adversarial defense papers at ICLR this year in terms of credibility of the claims (certainly the strongest in my pile) and contains several useful ideas as well as novel empirical findings (such as the increased success of attacks up to 1 million iterations).", "reviews": [{"review_id": "ByxGSsR9FQ-0", "review_text": "This paper presents a combination of methods that, together, yield neural networks that are robust to small changes in L2 distance. The main idea is to ensure that changing the input by a bounded L2 distance never changes the output by more than the same L2 distance. Then, the difference between the highest-scoring class and the second-highest scoring class provides a bound on how much the input must change. The trivial way to do this is to rescale the final output layer so that all of the magnitudes are very small; however, this would give no additional robustness at all. To counteract this, the paper introduces several additional heuristics for increasing the gap between the highest-scoring class and the second-highest scoring one. Adversarial training can be used to make the models even more robust. Experimental results on MNIST and CIFAR look impressive, although most are in terms of L2 distance, while most previous work optimizes L_infinity distance. The methods described by this paper are similar to max-margin training, which is already known to be optimally robust to L2 perturbations for linear models (e.g., Xu et al. (2009)). This paper would be stronger with more discussion and analysis of this connection, although that might be work for a future paper. Although the method relies heavily on heuristics, the empirical results are promising. The analysis of the contribution of the heuristics is fairly thorough as well. The MNIST results are strong. The CIFAR results show improved robustness, though at reduced accuracy on natural images. A combination of robust and non-robust classifiers improves the accuracy somewhat. Overall, this is interesting work with promising empirical results. The biggest weaknesses are: - Limited theory. The loss function is particularly strange. - The majority of the comparisons focus on L2-robustness, but are comparing to a model optimized for L_infinity-robustness. (Thankfully, the authors also do some comparisons on L_infinity-robustness.) - Robustness comes at a cost in accuracy, though this is not uncommon for adversarial training. The biggest strengths are: - Strong empirical robustness - Analysis of combinations of methods and their interactions: different loss function, different architecture, different weight constraints, and adversarial training are all evaluated together and separately. - Wide variety of experiments, including generalization on training data with noisy labels and analysis of the confidence gaps. Questions for the authors: - For equation (4) in the loss function, why would rescaling the layers in the middle of the network be equivalent to a linear transformation (u1, u2, ..., u_K) of the output? - In equation (6), what is the average averaging over? - The connection between confidence gap and robustness is discussed empirically, as a correlation, rather than theoretically, as a bound. Doesn't the confidence gap give a lower bound on the minimum perturbation to change the predicted class? --------- EDIT: After the author response, I remain positive about this paper. In addition to addressing my concerns, I admire the authors' patience in answering the concerns of other reviewers and commenters. I think that this is a solid paper that makes a good contribution to the literature on adversarial machine learning.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "4 ) Regarding the robustness-accuracy tradeoff . There is indeed a robustness versus nominal accuracy trade-off . We reported the trade-off in the second to last paragraph of Section 3.1 , and we revisited the topic in the second to last paragraph of Section 3.3 to state a hypothesis . As the reviewer pointed out , we are not the only defense work that face this trade-off . It remains an open question whether such trade-off is a necessary part of life . Our hypothesis on L2NNN 's trade-off is stated at end of Section 3.3 : by having a second goal of enlarging confidence gaps , L2NNN 's parameter training automatically and selectively misclassify certain training data in exchange for larger gaps . In the context of original training labels , this implies that some original labels are ignored and that leads to lower nominal accuracy . Although by looking at examples in Figure 4 , one could argue that some of the original labels are better ignored . If this hypothesis is true , this trade-off mechanism is a double-edged sword , as it both costs us nominal accuracy in Tables 1 and 2 and helps us in dealing with noisy data in Table 5 . This is only a hypothesis , and we may have a better answer in future work . 5 ) Regarding max-margin training . Thank you very much for the suggestion . Do you mean this paper http : //www.jmlr.org/papers/volume10/xu09b/xu09b.pdf ? Please advise . We will study the connection for future work , and we also want to see if it is appropriate to cite in this paper . Please let us know if we have missed anything and we 'd be happy to continue the discussion ."}, {"review_id": "ByxGSsR9FQ-1", "review_text": "Summary: The paper presents techniques for training a non expansive network, which keeps the Lipchitz constant of all layers lower than 1. While being non-expansive, means are taken to preserve distance information better than standard networks. The architectural changes required w.r.t standard networks are minor, and the most interesting changes are made to the loss minimized. The main claim of the paper is that the method is robust against adversarial attacks of a certain kind. However, the results presented show that a) such robustness comes at a high cost of accuracy for standard examples, and b) even though the network is preferable to a previous alternative in combating adversarial examples, the accuracy obtained in the face of adversarial attacks is too low to be of practical value. Other properties of the networks, explored empirically, are that the confidence of the prediction is indicative of robustness (to adversarial attacks) and that the networks learn better in the presence of high label noise. In short, this paper may be of interest to a sub-community interested in defense against certain types of adversarial attacks, even when the defense level is much too low to be practical. I am not part of this community, hence did not find this part very interesting. I believe the regularization results are of wider interest. However, to present this as the main contribution of L2NNN more work is required to find configuration which are resilient to overfit yet enable high training accuracy, and more diverse experiments are required. Pros: + the idea of non expansive network is interesting and important + results indicate some advantages in fighting adversarial examples and label noise Cons: - the results for fighting adversarial examples are not significant from a practical perspective - the results for copying with label noise are preliminary and require expansion with more experiments. - the method has costs in accuracy, which is lower than standard networks and this issue is not faced with enough attention - presentation clarity is medium: proofs for claims are missing, as well as relevant background on the relevant adversarial attacks. The choice to place the related work at the end also reduces presentation clarity. More detailed comments: Pages 1-3: In many places, small proofs are left to the reader as \u2018straightforward\u2019. Examples are: the claim in the introduction, in eq. 2, in section 2.2, section 2.3\u2019 last line of page 3, etc.. While the claim are true (in the cases I tried to verify them long enough), this makes reading difficult and not fluent. For some of these claims I do not see the argument behind them. In general, I think proofs should be brought for claims, and short proofs (preferably) should be brought for small claims. Leaving every proof to the reader as an exercise is not a convenient strategy. Page 4: The loss is complex and its terms utility require empirical evidence. The third term is shown to be clearly useful, enabling a trade off between train accuracy and margin. However, the utility of terms 4) and 5) is not verified. Do we really need both these terms? Cannot we just stay with one? The main claim is robustness w.r.t \u201cwhite-box non targeted L2-bounded attacks\u201d. This seems to be a very specific attack type, and it is not explained at all in the text. Hence it is hard to judge the value of this robustness. Explanation of adversarial attack kinds, and specifically of \u201cwhite-box non targeted L2-bounded attacks\u201d is required for this paper to be a stand alone readable paper. Similarly \u2018L_\\infty\u2019-bounded attacks, for which results are shown, should be explained. Table 1,2: First, the model architecture used in these experiments is not stated. Second, the accuracy of the \u2018natural\u2019 baseline classifier, at least in the MNist case, is somewhat low \u2013 much better results can be obtained with CNN on MNist. Third, the accuracies of the suggested robust models are very low compared to what can be obtained on these datatsets. Forth, while the accuracies under attack of the proposed method are better than those of Madri et al., both are quite poor and indicate that the classifier is not useful under attack (from a practical perspective). Page 6: The classifiers which share the work between an L2NNN network and a regular more accurate network may be interesting, as the accuracies reported for them are significantly higher than the L2NNN networks. However, the robustness scores are not reported for these classifiers, so it is not possible to judge if they lead to a practical and effective strategy. Page 7: For me, the results with partially random labels are the most interesting in the paper. The resistance of L2NNN to overfit and its ability to learn with very noisy data are considerably better than the suggested alternatives. Relevant work not mentioned \u201cSpectral Norm Regularization for Improving the Generalizability of Deep Learning\u201d - Yuichi Yoshida and Takeru Miyato, Arxiv, 2017. I have read the rebuttal. The discussion was interesting, but I do not see a need to change my assessment. The example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this. I do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "3 ) Regarding the robustness-accuracy trade-off . There is indeed a robustness versus nominal accuracy trade-off . We reported the trade-off in the second to last paragraph of Section 3.1 , and we revisited the topic in the second to last paragraph of Section 3.3 to state a hypothesis . We are not the only defense work that face this trade-off . As AnonReviewer2 pointed out , adversarial training has a similar trade-off . It also can be seen in the adversarial polytope work of https : //arxiv.org/abs/1805.12514 . It remains an open question whether such trade-off is a necessary part of life . Our hypothesis on L2NNN 's trade-off is stated at end of Section 3.3 : by having a second goal of enlarging confidence gap , L2NNN 's parameter training automatically and selectively misclassify certain training data in exchange for larger gaps . In the context of original training labels , this implies that some original labels are ignored and that leads to lower nominal accuracy . Although by looking at examples in Figure 4 , one could argue that some of the original labels are better ignored . This is only a hypothesis . We agree with the reviewer that this trade-off is an important subject to study , we may have a better answer in future work . 4 ) Regarding omitted proofs . We thank the reviewer for the suggestions and we are updating the appendix to add proofs , and will post the revision soon . 5 ) Regarding loss terms ( 4 ) and ( 5 ) . Removing one of these two terms would not result in as much degradation as in Table 3 . If to choose one of the two , it makes sense to use ( 5 ) and the end result would be slight degradation in nominal accuracy compared with the current results in Table 1 and 2 . 6 ) Regarding architecture . Our models 3 and 4 in Table 1 and 2 all use convolution layers followed by fully connected layers , some of which are split layers with stacks dedicated to individual logits . These are conventional architecture choices , and our unconventional elements are two-sided ReLU and norm-pooling . By the way , they are all available for download at the dropbox link on page 4 . For the scrambled-label experiments , as detailed in Tables 7 and 8 , we want to be fair and build ordinary networks with two different architectures , one shallow and one deep . Then the ordinary-network section of Table 5 is entry-wise max of Tables 7 and 8 . The L2NNNs use the same architecture for MNIST throughout this paper . 7 ) Regarding hybrid models reported at end of Section 3.2 . The following are measurements of the said hybrid models under same settings in Tables 1 and 2 , after 1000 iterations : MNIST 62.9 % , CIFAR-10 6.4 % . Please note that the base for comparison is Models 3 in Tables 1 and 2 . The CIFAR-10 number is in line with expectation , and 6.4 % is a degradation from 10.1 % . The MNIST number , however , is an artifact of that the CW attack code was not designed for ensemble models and Carlini & Wagner can likely do much better if they know and take advantage of the hybrid mechanism . The real MNIST number ought to be somewhat below 20.1 % . 8 ) Regarding missing reference . Thank you and we will added the reference . Please let us know if we have missed anything and we 'd be happy to continue the discussion ."}, {"review_id": "ByxGSsR9FQ-2", "review_text": "I read this paper with some excitement. The authors propose a very sensible idea: simultaneously maximizing the confidence gap and constraining the Lipschitz constant of the network, thus achieving a guarantee that no L2-bounded perturbation can alter the prediction so long as the perturbation is bounded by some function of the confidence gap. The main idea consists of three parts: 1) smooth networks (fixed, low Lipschitz constant) (2) loss function that explicitly maximizes the confidence gap (distance between largest and second-largest logits). (3) \u201cthe network architecture restricts confidence gaps as little as possible. We will elaborate.\u201d The first two conditions make plain sense. The third condition and subsequent elaborations are far too vague. What precisely is the property of restricting confidence gaps? At first glance this seems akin to the smoothness sought in property one. Even in the bulleted list, the authors owe the reader a clearer explanation. The proposed model, denoted L2-nonexpansive neural networks (L2NNNs) and consists of a sensible form of Lipschitz-constant-enforcing weight regularization, a loss function that penalizes the confidence gap. To address the third condition, the authors say only \u201cwe adapt various layers in new ways for the third condition, for example norm-pooling and two-sided ReLU, which will be presented later\u201d which is far too vacuous. At this point the reader is exposed to the third condition for the second time and yet it remains shrouded in mystery. The authors should elaborate here and describe what precisely, if anything, this third condition consists of. If it is not rigorously defined but only a heuristic notion, that would be fine, but this should be communicated clearly to the reader. A following paragraph introduces the notion of \u201cpreserving distance\u201d. However, what follows is too informal a discussion, and the rigorous definition never materializes. The authors say in one place \u201ca network that maximizes confidence gaps well must be one that preserves distance well\u201d. In this case, why do we need the third condition at all if the second condition appears to be sufficient? In the next sections the authors describe the methods in greater detail and summarize their results. I have placed some more specific nittier comments in the ***small issues*** section below. But comment hear on the empirical findings. One undersold finding here is that the existing methods (including the widely-believed-to-be-robust method due to MAdry 2017) that appear robust under FGSM attacks break badly under iterated attacks, and that the attacks go stronger up to 1M iterations, bringing accuracy below 10%. In contrast the proposed method reaches 24% accuracy, which isn\u2019t magnificent, but does appear to outperform the model due to Madry. A comparison against the method due to Kolter & Wong seems in order. The authors do not implement methods based on the adversarial polytope due to their present un-scalability, but that argument would be better supported if the authors were addressing larger models on harder datasets (vs MNIST and CIFAR10). In short, I like the main ideas in this paper although some more empirical elbow grease is in order, the third condition needs to be discussed more rigorously or discarded. Additionally the choice of loss function should be better justified. Why do we need the original cross-entropy objective at all. Why not directly optimize the confidence gap? Did the authors try this? Did it work? Apologies if I missed this detail. Overall, I am interested in the development of this paper and would like to give it a higher vote but believe the authors have a bit more work to do to make this an easier decision. Looking forward to reading the rebuttal. ***Small issues*** Page 1 \u201cnonexpansive neural networks (L2NNN)\u201d for agreement on pluralization, should be \u201cL2NNNs\u201d \u201cThey generalize better from noisy training labels than ordinary networks: for example, when 75% of MNIST training labels are randomized, an L2NNN still achieves 93.1% accuracy on the test set\u201d When you make a claim about accuracy of a proposed model, it must be made in reference to a standard model, even in the intro. It\u2019s well-known in general that DNNs perform well even under large amounts of label noise. Hard to say without reference if 93.1% represents a significant improvement. Repeated phrase on page 2: \u201cHow to adapt subtleties like recursion and splitting-reconvergence is included in the appendix.\u201d \u201cDiscussions on splitting-reconvergence, recursion and normalization are in the appendix.\u201d Inputs to softmax cross-entropy should be both a set of logits and the label -- here the way the function is used in the notation does not match the proper function signature Figure --- do not put \u201cModel1, Model2, Model3, Model4\u201d. This is unreadable. Put some shortname and then define it in the caption. Once one knows the abbreviations, they should be able to look at the figure and understand it without constantly referencing the caption. Table 1-4 should be at the top of the page and arranged in a grid. This wrapfigure floating in the middle of the page, while purely a cosmetic issue that should not bear on our deliberations, tortures the template unnecessarily, turning the middle 80% of page 5 into a one-column page unnecessarily. Table 4 should show comparison to Madry model. Also this is why you need a shortname in the legend. In order to understand table 4, the reader has to consult the caption for tables 1 and 2. \u201cIt is an important property that an L2NNN has an easily accessible measurement on how robust its decisions are\u201d I AGREE!", "rating": "6: Marginally above acceptance threshold", "reply_text": "4 ) Regarding presentation issues . Thank you very much for the suggestions and we agree with most . We are making some of the changes and will post a revision soon , and will do more if this paper is accepted and more space is allowed in final version . One thing we want to point out that that L2NNN 's 93.1 % performance from 75 % -random training labels is significantly higher than the best of ordinary networks , see Tables 5,7,8 . Our measurements of L_inf defense of Madry et al . ( 2017 ) are close to those reported in their paper . Because we use the same L_inf epsilon values , the numbers in Table 4 can be directly compared against those reported in Madry et al . ( 2017 ) , Kolter & Wong ( 2017 ) and Raghunathan et al . ( 2018 ) .As we acknowledged at end of section 3.1 , Madry et al . 's MNIST L_inf result is still the best , while for CIFAR-10 we are on par . 5 ) We agree with the reviewer that pointing out that MNIST is not a solved problem is important to the field as well . And we thank the reviewer for appreciating that L2NNNs have an easily accessible measure of robustness . Please let us know if we have missed anything and we 'd be happy to continue the discussion ."}], "0": {"review_id": "ByxGSsR9FQ-0", "review_text": "This paper presents a combination of methods that, together, yield neural networks that are robust to small changes in L2 distance. The main idea is to ensure that changing the input by a bounded L2 distance never changes the output by more than the same L2 distance. Then, the difference between the highest-scoring class and the second-highest scoring class provides a bound on how much the input must change. The trivial way to do this is to rescale the final output layer so that all of the magnitudes are very small; however, this would give no additional robustness at all. To counteract this, the paper introduces several additional heuristics for increasing the gap between the highest-scoring class and the second-highest scoring one. Adversarial training can be used to make the models even more robust. Experimental results on MNIST and CIFAR look impressive, although most are in terms of L2 distance, while most previous work optimizes L_infinity distance. The methods described by this paper are similar to max-margin training, which is already known to be optimally robust to L2 perturbations for linear models (e.g., Xu et al. (2009)). This paper would be stronger with more discussion and analysis of this connection, although that might be work for a future paper. Although the method relies heavily on heuristics, the empirical results are promising. The analysis of the contribution of the heuristics is fairly thorough as well. The MNIST results are strong. The CIFAR results show improved robustness, though at reduced accuracy on natural images. A combination of robust and non-robust classifiers improves the accuracy somewhat. Overall, this is interesting work with promising empirical results. The biggest weaknesses are: - Limited theory. The loss function is particularly strange. - The majority of the comparisons focus on L2-robustness, but are comparing to a model optimized for L_infinity-robustness. (Thankfully, the authors also do some comparisons on L_infinity-robustness.) - Robustness comes at a cost in accuracy, though this is not uncommon for adversarial training. The biggest strengths are: - Strong empirical robustness - Analysis of combinations of methods and their interactions: different loss function, different architecture, different weight constraints, and adversarial training are all evaluated together and separately. - Wide variety of experiments, including generalization on training data with noisy labels and analysis of the confidence gaps. Questions for the authors: - For equation (4) in the loss function, why would rescaling the layers in the middle of the network be equivalent to a linear transformation (u1, u2, ..., u_K) of the output? - In equation (6), what is the average averaging over? - The connection between confidence gap and robustness is discussed empirically, as a correlation, rather than theoretically, as a bound. Doesn't the confidence gap give a lower bound on the minimum perturbation to change the predicted class? --------- EDIT: After the author response, I remain positive about this paper. In addition to addressing my concerns, I admire the authors' patience in answering the concerns of other reviewers and commenters. I think that this is a solid paper that makes a good contribution to the literature on adversarial machine learning.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "4 ) Regarding the robustness-accuracy tradeoff . There is indeed a robustness versus nominal accuracy trade-off . We reported the trade-off in the second to last paragraph of Section 3.1 , and we revisited the topic in the second to last paragraph of Section 3.3 to state a hypothesis . As the reviewer pointed out , we are not the only defense work that face this trade-off . It remains an open question whether such trade-off is a necessary part of life . Our hypothesis on L2NNN 's trade-off is stated at end of Section 3.3 : by having a second goal of enlarging confidence gaps , L2NNN 's parameter training automatically and selectively misclassify certain training data in exchange for larger gaps . In the context of original training labels , this implies that some original labels are ignored and that leads to lower nominal accuracy . Although by looking at examples in Figure 4 , one could argue that some of the original labels are better ignored . If this hypothesis is true , this trade-off mechanism is a double-edged sword , as it both costs us nominal accuracy in Tables 1 and 2 and helps us in dealing with noisy data in Table 5 . This is only a hypothesis , and we may have a better answer in future work . 5 ) Regarding max-margin training . Thank you very much for the suggestion . Do you mean this paper http : //www.jmlr.org/papers/volume10/xu09b/xu09b.pdf ? Please advise . We will study the connection for future work , and we also want to see if it is appropriate to cite in this paper . Please let us know if we have missed anything and we 'd be happy to continue the discussion ."}, "1": {"review_id": "ByxGSsR9FQ-1", "review_text": "Summary: The paper presents techniques for training a non expansive network, which keeps the Lipchitz constant of all layers lower than 1. While being non-expansive, means are taken to preserve distance information better than standard networks. The architectural changes required w.r.t standard networks are minor, and the most interesting changes are made to the loss minimized. The main claim of the paper is that the method is robust against adversarial attacks of a certain kind. However, the results presented show that a) such robustness comes at a high cost of accuracy for standard examples, and b) even though the network is preferable to a previous alternative in combating adversarial examples, the accuracy obtained in the face of adversarial attacks is too low to be of practical value. Other properties of the networks, explored empirically, are that the confidence of the prediction is indicative of robustness (to adversarial attacks) and that the networks learn better in the presence of high label noise. In short, this paper may be of interest to a sub-community interested in defense against certain types of adversarial attacks, even when the defense level is much too low to be practical. I am not part of this community, hence did not find this part very interesting. I believe the regularization results are of wider interest. However, to present this as the main contribution of L2NNN more work is required to find configuration which are resilient to overfit yet enable high training accuracy, and more diverse experiments are required. Pros: + the idea of non expansive network is interesting and important + results indicate some advantages in fighting adversarial examples and label noise Cons: - the results for fighting adversarial examples are not significant from a practical perspective - the results for copying with label noise are preliminary and require expansion with more experiments. - the method has costs in accuracy, which is lower than standard networks and this issue is not faced with enough attention - presentation clarity is medium: proofs for claims are missing, as well as relevant background on the relevant adversarial attacks. The choice to place the related work at the end also reduces presentation clarity. More detailed comments: Pages 1-3: In many places, small proofs are left to the reader as \u2018straightforward\u2019. Examples are: the claim in the introduction, in eq. 2, in section 2.2, section 2.3\u2019 last line of page 3, etc.. While the claim are true (in the cases I tried to verify them long enough), this makes reading difficult and not fluent. For some of these claims I do not see the argument behind them. In general, I think proofs should be brought for claims, and short proofs (preferably) should be brought for small claims. Leaving every proof to the reader as an exercise is not a convenient strategy. Page 4: The loss is complex and its terms utility require empirical evidence. The third term is shown to be clearly useful, enabling a trade off between train accuracy and margin. However, the utility of terms 4) and 5) is not verified. Do we really need both these terms? Cannot we just stay with one? The main claim is robustness w.r.t \u201cwhite-box non targeted L2-bounded attacks\u201d. This seems to be a very specific attack type, and it is not explained at all in the text. Hence it is hard to judge the value of this robustness. Explanation of adversarial attack kinds, and specifically of \u201cwhite-box non targeted L2-bounded attacks\u201d is required for this paper to be a stand alone readable paper. Similarly \u2018L_\\infty\u2019-bounded attacks, for which results are shown, should be explained. Table 1,2: First, the model architecture used in these experiments is not stated. Second, the accuracy of the \u2018natural\u2019 baseline classifier, at least in the MNist case, is somewhat low \u2013 much better results can be obtained with CNN on MNist. Third, the accuracies of the suggested robust models are very low compared to what can be obtained on these datatsets. Forth, while the accuracies under attack of the proposed method are better than those of Madri et al., both are quite poor and indicate that the classifier is not useful under attack (from a practical perspective). Page 6: The classifiers which share the work between an L2NNN network and a regular more accurate network may be interesting, as the accuracies reported for them are significantly higher than the L2NNN networks. However, the robustness scores are not reported for these classifiers, so it is not possible to judge if they lead to a practical and effective strategy. Page 7: For me, the results with partially random labels are the most interesting in the paper. The resistance of L2NNN to overfit and its ability to learn with very noisy data are considerably better than the suggested alternatives. Relevant work not mentioned \u201cSpectral Norm Regularization for Improving the Generalizability of Deep Learning\u201d - Yuichi Yoshida and Takeru Miyato, Arxiv, 2017. I have read the rebuttal. The discussion was interesting, but I do not see a need to change my assessment. The example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this. I do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "3 ) Regarding the robustness-accuracy trade-off . There is indeed a robustness versus nominal accuracy trade-off . We reported the trade-off in the second to last paragraph of Section 3.1 , and we revisited the topic in the second to last paragraph of Section 3.3 to state a hypothesis . We are not the only defense work that face this trade-off . As AnonReviewer2 pointed out , adversarial training has a similar trade-off . It also can be seen in the adversarial polytope work of https : //arxiv.org/abs/1805.12514 . It remains an open question whether such trade-off is a necessary part of life . Our hypothesis on L2NNN 's trade-off is stated at end of Section 3.3 : by having a second goal of enlarging confidence gap , L2NNN 's parameter training automatically and selectively misclassify certain training data in exchange for larger gaps . In the context of original training labels , this implies that some original labels are ignored and that leads to lower nominal accuracy . Although by looking at examples in Figure 4 , one could argue that some of the original labels are better ignored . This is only a hypothesis . We agree with the reviewer that this trade-off is an important subject to study , we may have a better answer in future work . 4 ) Regarding omitted proofs . We thank the reviewer for the suggestions and we are updating the appendix to add proofs , and will post the revision soon . 5 ) Regarding loss terms ( 4 ) and ( 5 ) . Removing one of these two terms would not result in as much degradation as in Table 3 . If to choose one of the two , it makes sense to use ( 5 ) and the end result would be slight degradation in nominal accuracy compared with the current results in Table 1 and 2 . 6 ) Regarding architecture . Our models 3 and 4 in Table 1 and 2 all use convolution layers followed by fully connected layers , some of which are split layers with stacks dedicated to individual logits . These are conventional architecture choices , and our unconventional elements are two-sided ReLU and norm-pooling . By the way , they are all available for download at the dropbox link on page 4 . For the scrambled-label experiments , as detailed in Tables 7 and 8 , we want to be fair and build ordinary networks with two different architectures , one shallow and one deep . Then the ordinary-network section of Table 5 is entry-wise max of Tables 7 and 8 . The L2NNNs use the same architecture for MNIST throughout this paper . 7 ) Regarding hybrid models reported at end of Section 3.2 . The following are measurements of the said hybrid models under same settings in Tables 1 and 2 , after 1000 iterations : MNIST 62.9 % , CIFAR-10 6.4 % . Please note that the base for comparison is Models 3 in Tables 1 and 2 . The CIFAR-10 number is in line with expectation , and 6.4 % is a degradation from 10.1 % . The MNIST number , however , is an artifact of that the CW attack code was not designed for ensemble models and Carlini & Wagner can likely do much better if they know and take advantage of the hybrid mechanism . The real MNIST number ought to be somewhat below 20.1 % . 8 ) Regarding missing reference . Thank you and we will added the reference . Please let us know if we have missed anything and we 'd be happy to continue the discussion ."}, "2": {"review_id": "ByxGSsR9FQ-2", "review_text": "I read this paper with some excitement. The authors propose a very sensible idea: simultaneously maximizing the confidence gap and constraining the Lipschitz constant of the network, thus achieving a guarantee that no L2-bounded perturbation can alter the prediction so long as the perturbation is bounded by some function of the confidence gap. The main idea consists of three parts: 1) smooth networks (fixed, low Lipschitz constant) (2) loss function that explicitly maximizes the confidence gap (distance between largest and second-largest logits). (3) \u201cthe network architecture restricts confidence gaps as little as possible. We will elaborate.\u201d The first two conditions make plain sense. The third condition and subsequent elaborations are far too vague. What precisely is the property of restricting confidence gaps? At first glance this seems akin to the smoothness sought in property one. Even in the bulleted list, the authors owe the reader a clearer explanation. The proposed model, denoted L2-nonexpansive neural networks (L2NNNs) and consists of a sensible form of Lipschitz-constant-enforcing weight regularization, a loss function that penalizes the confidence gap. To address the third condition, the authors say only \u201cwe adapt various layers in new ways for the third condition, for example norm-pooling and two-sided ReLU, which will be presented later\u201d which is far too vacuous. At this point the reader is exposed to the third condition for the second time and yet it remains shrouded in mystery. The authors should elaborate here and describe what precisely, if anything, this third condition consists of. If it is not rigorously defined but only a heuristic notion, that would be fine, but this should be communicated clearly to the reader. A following paragraph introduces the notion of \u201cpreserving distance\u201d. However, what follows is too informal a discussion, and the rigorous definition never materializes. The authors say in one place \u201ca network that maximizes confidence gaps well must be one that preserves distance well\u201d. In this case, why do we need the third condition at all if the second condition appears to be sufficient? In the next sections the authors describe the methods in greater detail and summarize their results. I have placed some more specific nittier comments in the ***small issues*** section below. But comment hear on the empirical findings. One undersold finding here is that the existing methods (including the widely-believed-to-be-robust method due to MAdry 2017) that appear robust under FGSM attacks break badly under iterated attacks, and that the attacks go stronger up to 1M iterations, bringing accuracy below 10%. In contrast the proposed method reaches 24% accuracy, which isn\u2019t magnificent, but does appear to outperform the model due to Madry. A comparison against the method due to Kolter & Wong seems in order. The authors do not implement methods based on the adversarial polytope due to their present un-scalability, but that argument would be better supported if the authors were addressing larger models on harder datasets (vs MNIST and CIFAR10). In short, I like the main ideas in this paper although some more empirical elbow grease is in order, the third condition needs to be discussed more rigorously or discarded. Additionally the choice of loss function should be better justified. Why do we need the original cross-entropy objective at all. Why not directly optimize the confidence gap? Did the authors try this? Did it work? Apologies if I missed this detail. Overall, I am interested in the development of this paper and would like to give it a higher vote but believe the authors have a bit more work to do to make this an easier decision. Looking forward to reading the rebuttal. ***Small issues*** Page 1 \u201cnonexpansive neural networks (L2NNN)\u201d for agreement on pluralization, should be \u201cL2NNNs\u201d \u201cThey generalize better from noisy training labels than ordinary networks: for example, when 75% of MNIST training labels are randomized, an L2NNN still achieves 93.1% accuracy on the test set\u201d When you make a claim about accuracy of a proposed model, it must be made in reference to a standard model, even in the intro. It\u2019s well-known in general that DNNs perform well even under large amounts of label noise. Hard to say without reference if 93.1% represents a significant improvement. Repeated phrase on page 2: \u201cHow to adapt subtleties like recursion and splitting-reconvergence is included in the appendix.\u201d \u201cDiscussions on splitting-reconvergence, recursion and normalization are in the appendix.\u201d Inputs to softmax cross-entropy should be both a set of logits and the label -- here the way the function is used in the notation does not match the proper function signature Figure --- do not put \u201cModel1, Model2, Model3, Model4\u201d. This is unreadable. Put some shortname and then define it in the caption. Once one knows the abbreviations, they should be able to look at the figure and understand it without constantly referencing the caption. Table 1-4 should be at the top of the page and arranged in a grid. This wrapfigure floating in the middle of the page, while purely a cosmetic issue that should not bear on our deliberations, tortures the template unnecessarily, turning the middle 80% of page 5 into a one-column page unnecessarily. Table 4 should show comparison to Madry model. Also this is why you need a shortname in the legend. In order to understand table 4, the reader has to consult the caption for tables 1 and 2. \u201cIt is an important property that an L2NNN has an easily accessible measurement on how robust its decisions are\u201d I AGREE!", "rating": "6: Marginally above acceptance threshold", "reply_text": "4 ) Regarding presentation issues . Thank you very much for the suggestions and we agree with most . We are making some of the changes and will post a revision soon , and will do more if this paper is accepted and more space is allowed in final version . One thing we want to point out that that L2NNN 's 93.1 % performance from 75 % -random training labels is significantly higher than the best of ordinary networks , see Tables 5,7,8 . Our measurements of L_inf defense of Madry et al . ( 2017 ) are close to those reported in their paper . Because we use the same L_inf epsilon values , the numbers in Table 4 can be directly compared against those reported in Madry et al . ( 2017 ) , Kolter & Wong ( 2017 ) and Raghunathan et al . ( 2018 ) .As we acknowledged at end of section 3.1 , Madry et al . 's MNIST L_inf result is still the best , while for CIFAR-10 we are on par . 5 ) We agree with the reviewer that pointing out that MNIST is not a solved problem is important to the field as well . And we thank the reviewer for appreciating that L2NNNs have an easily accessible measure of robustness . Please let us know if we have missed anything and we 'd be happy to continue the discussion ."}}