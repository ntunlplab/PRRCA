{"year": "2017", "forum": "BybtVK9lg", "title": "Autoencoding Variational Inference For Topic Models", "decision": "Accept (Poster)", "meta_review": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.", "reviews": [{"review_id": "BybtVK9lg-0", "review_text": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . Here are our clarifications and answers . Our results do not conflate modelling and inference . It \u2019 s true that we introduce both a new model ProdLDA and a new inference method , but we evaluate them separately . First we compare the new inference method to old inference methods on an existing model , LDA . Then we compare the new model , ProdLDA , to the old model , LDA , with the same inference method . We included the comparison to prodLDA as an example to show how easy it is to apply our method to difficult models . ProdLDA is an interesting model for this approach precisely because it is difficult to get good baselines with other inference methods . As far as we are aware , prodLDA is a novel topic model related to exponential family harmonium like NVDM . Figure 1 and Sparsity : theta are sampled from the posterior p ( theta|doc , alpha ) . alpha is the concentration parameter of the prior on theta . Sparsity is shown through the amount of probability mass that the different components are allocated under the different choices of prior . For gaussian prior , it seems that all components are getting roughly the same mass , whereas , for dirichlet , fewer components get more mass and the rest of the components get hardly any . This is in fact also the motivation for the choice of dirichlet priors in the original LDA model . Hyper-parameter selection : For other LDA inference methods we used the built in hyperparameter optimization option in Mallet or scikit-learn , respectively . For NVDM we used the author \u2019 s code and because our method is so fast , it \u2019 s easy to embed them within an optimization scheme for alpha , like is used in mallet . So we used BO for HP selection and used it for all the experiments . Uni-modal : No , Dirichlet is unimodal in the softmax basis . See Philipp Hennig , David H Stern , Ralf Herbrich , and Thore Graepel . Kernel topic models . In AISTATS , pp . 511\u2013519 , 2012 , Section 3.3 and David JC MacKay . Choice of basis for laplace approximation . Machine learning , 33 ( 1 ) :77\u201386 , 1998 , Section 2 . Statistical significance : Yes the results are statistically significant . Across 40 different random initializations , the standard deviation is very low ."}, {"review_id": "BybtVK9lg-1", "review_text": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine. Minor comments: Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. A typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers? In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this? How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1? It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue). Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. [1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914 [2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "rating": "7: Good paper, accept", "reply_text": "Thanks for the comments , which we \u2019 ll incorporate . Here are the clarifications to your questions : 1 . Reason for not waiting for DMFVI to finish : In order to make sure the reported results were statistically significant we ran every method for multiple ( 40 ) times . This would have not been practically possible for DMFVI which was taking more than 24 hours for a single run . 2.Perplexity vs Topic Coherence : J. Chang et al , ( 2009 ) in Reading tea leaves : How humans interpret topic models show that in fact perplexity correlates negatively with topic interpretability . Our results are in line with their findings . 3.We agree and recently we found that a BN inspired reparametrization of the topic matrix helps a great deal in improving the topic coherence for NVLDA as well so we are in the process of extending the training section of the paper with the effect of normalization , learning rate and momentum scheduling on the latent topics ."}, {"review_id": "BybtVK9lg-2", "review_text": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf Section 4.1: error in the equation. The last term should be Prod_i exp(delta*_r_i) * exp((1-delta)*s_i). Last paragraph 4.1. The increment relative to NVDM seems small: approximating the Dirichlet with a Gaussian and high momentum training. While these aspects may be important in practice they are somewhat incremental. I couldn\u2019t find the size of the vocabularies of the datasets in the paper. Does this method work well for very high dimensional sparse document representations? The comment on page 8 that the method is very sensitive to optimization tricks like very high momentum in ADAM and batch normalization is a bit worrying to me. In the end, it\u2019s a useful paper to read, but it\u2019s not going to be the highlight of the conference. The relative increment is somewhat small and seems to heavily rely optimization tricks. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your feedback and the correction in eq 4.1 . Here are a few points that we would like to clarify to make our contributions more specific . 1.Sensitivity to optimization tricks : VAEs and GANs are notorious for being sensitive to optimization parameters . One of the main contributions of our paper is to work out which optimization tricks are necessary for VAEs to work with LDA ( and other mixed-membership methods ) . Given how important a probabilistic model LDA is , we feel that this is an important contribution . Once the normalizations have been applied before softmax non-linearities the training method does not seem to be sensitive to the numerical parameters of the optimization scheme . In fact , we used the same learning rate and momentum across both data set and both models . 2.Another key highlight is the speed of training which is known to be quite a problem area for VAEs . The proposed method drastically speeds up inference in one of the most celebrated ML models . On 20newsgroup dataset , it takes 46 seconds compared to several minutes and hours of inference time needed for DMFVI and collapsed gibbs . In addition , our proposed model increases the topic quality three times over the state-of-art in the same 46 seconds of inference time , which in our opinion is both highly useful and exciting . 4.Simple exponential family : While proLDA can be seen as an extension to SePCA ( with non-gaussian priors ) , that would be true of several other mixed membership models simply because SePCA is fairly general architecture . Secondly , it is not clear how would one extend the proposed inference for SePCA to prodLDA . 5.Vocabulary Size : 20newsgroup = 2000 whereas RCV1 = 10,000 . The method works just as well for higher dimensional sparse representations without any change or additional adjustments . 6.Batch-Normalization : We have recently been able to improve the NVLDA learning via a re-parameterization scheme for the topic matrix that is inspired by batch normalization .Now it performs significantly better than NVDM and Gibbs Sampler ( in some cases ) in terms of the topic coherence ."}], "0": {"review_id": "BybtVK9lg-0", "review_text": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . Here are our clarifications and answers . Our results do not conflate modelling and inference . It \u2019 s true that we introduce both a new model ProdLDA and a new inference method , but we evaluate them separately . First we compare the new inference method to old inference methods on an existing model , LDA . Then we compare the new model , ProdLDA , to the old model , LDA , with the same inference method . We included the comparison to prodLDA as an example to show how easy it is to apply our method to difficult models . ProdLDA is an interesting model for this approach precisely because it is difficult to get good baselines with other inference methods . As far as we are aware , prodLDA is a novel topic model related to exponential family harmonium like NVDM . Figure 1 and Sparsity : theta are sampled from the posterior p ( theta|doc , alpha ) . alpha is the concentration parameter of the prior on theta . Sparsity is shown through the amount of probability mass that the different components are allocated under the different choices of prior . For gaussian prior , it seems that all components are getting roughly the same mass , whereas , for dirichlet , fewer components get more mass and the rest of the components get hardly any . This is in fact also the motivation for the choice of dirichlet priors in the original LDA model . Hyper-parameter selection : For other LDA inference methods we used the built in hyperparameter optimization option in Mallet or scikit-learn , respectively . For NVDM we used the author \u2019 s code and because our method is so fast , it \u2019 s easy to embed them within an optimization scheme for alpha , like is used in mallet . So we used BO for HP selection and used it for all the experiments . Uni-modal : No , Dirichlet is unimodal in the softmax basis . See Philipp Hennig , David H Stern , Ralf Herbrich , and Thore Graepel . Kernel topic models . In AISTATS , pp . 511\u2013519 , 2012 , Section 3.3 and David JC MacKay . Choice of basis for laplace approximation . Machine learning , 33 ( 1 ) :77\u201386 , 1998 , Section 2 . Statistical significance : Yes the results are statistically significant . Across 40 different random initializations , the standard deviation is very low ."}, "1": {"review_id": "BybtVK9lg-1", "review_text": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine. Minor comments: Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. A typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers? In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this? How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1? It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue). Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. [1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914 [2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "rating": "7: Good paper, accept", "reply_text": "Thanks for the comments , which we \u2019 ll incorporate . Here are the clarifications to your questions : 1 . Reason for not waiting for DMFVI to finish : In order to make sure the reported results were statistically significant we ran every method for multiple ( 40 ) times . This would have not been practically possible for DMFVI which was taking more than 24 hours for a single run . 2.Perplexity vs Topic Coherence : J. Chang et al , ( 2009 ) in Reading tea leaves : How humans interpret topic models show that in fact perplexity correlates negatively with topic interpretability . Our results are in line with their findings . 3.We agree and recently we found that a BN inspired reparametrization of the topic matrix helps a great deal in improving the topic coherence for NVLDA as well so we are in the process of extending the training section of the paper with the effect of normalization , learning rate and momentum scheduling on the latent topics ."}, "2": {"review_id": "BybtVK9lg-2", "review_text": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf Section 4.1: error in the equation. The last term should be Prod_i exp(delta*_r_i) * exp((1-delta)*s_i). Last paragraph 4.1. The increment relative to NVDM seems small: approximating the Dirichlet with a Gaussian and high momentum training. While these aspects may be important in practice they are somewhat incremental. I couldn\u2019t find the size of the vocabularies of the datasets in the paper. Does this method work well for very high dimensional sparse document representations? The comment on page 8 that the method is very sensitive to optimization tricks like very high momentum in ADAM and batch normalization is a bit worrying to me. In the end, it\u2019s a useful paper to read, but it\u2019s not going to be the highlight of the conference. The relative increment is somewhat small and seems to heavily rely optimization tricks. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your feedback and the correction in eq 4.1 . Here are a few points that we would like to clarify to make our contributions more specific . 1.Sensitivity to optimization tricks : VAEs and GANs are notorious for being sensitive to optimization parameters . One of the main contributions of our paper is to work out which optimization tricks are necessary for VAEs to work with LDA ( and other mixed-membership methods ) . Given how important a probabilistic model LDA is , we feel that this is an important contribution . Once the normalizations have been applied before softmax non-linearities the training method does not seem to be sensitive to the numerical parameters of the optimization scheme . In fact , we used the same learning rate and momentum across both data set and both models . 2.Another key highlight is the speed of training which is known to be quite a problem area for VAEs . The proposed method drastically speeds up inference in one of the most celebrated ML models . On 20newsgroup dataset , it takes 46 seconds compared to several minutes and hours of inference time needed for DMFVI and collapsed gibbs . In addition , our proposed model increases the topic quality three times over the state-of-art in the same 46 seconds of inference time , which in our opinion is both highly useful and exciting . 4.Simple exponential family : While proLDA can be seen as an extension to SePCA ( with non-gaussian priors ) , that would be true of several other mixed membership models simply because SePCA is fairly general architecture . Secondly , it is not clear how would one extend the proposed inference for SePCA to prodLDA . 5.Vocabulary Size : 20newsgroup = 2000 whereas RCV1 = 10,000 . The method works just as well for higher dimensional sparse representations without any change or additional adjustments . 6.Batch-Normalization : We have recently been able to improve the NVLDA learning via a re-parameterization scheme for the topic matrix that is inspired by batch normalization .Now it performs significantly better than NVDM and Gibbs Sampler ( in some cases ) in terms of the topic coherence ."}}