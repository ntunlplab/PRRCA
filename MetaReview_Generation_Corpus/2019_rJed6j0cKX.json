{"year": "2019", "forum": "rJed6j0cKX", "title": "Analyzing Inverse Problems with Invertible Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper proposes a framework for using invertible neural networks to study inverse problems, e.g., recover hidden states or parameters of a system from measurements. This is an important and well-motivated topic, and the solution proposed is novel although somewhat incremental. The paper is generally well written. Some theoretical analysis is provided, giving conditions under which the proposed approach recovers the true posterior. Empirically, the approach is tested on synthetic data and real world problems from medicine and astronomy, where it is shown to compared favorably to ABC and conditional VAEs. Adding additional baselines (Bayesian MCMC and Stein methods) would be good. There are some potential issues regarding MMD scalability to high dimensional spaces, but overall the paper makes a solid contribution and all the reviewers agree it should be accepted for publication.", "reviews": [{"review_id": "rJed6j0cKX-0", "review_text": "1) Summary The authors propose to use invertible networks to solve ambiguous inverse problems. This is done by training one group of Real-NVP output variables supervised while training the other group via maximum likelihood under a Gaussian prior as done in the standard Real-NVP. Further, the authors suggest to not only train the forward model, but also the inverse model with an MMD critic, similar to previous works that used a more flexible GAN critic [1]. 2) Clarity The paper is easy to understand and the main idea is well-motivated. 3) Significance The main contribution of this work is of conceptual nature and illustrates how invertible networks are a promising framework for many inverse problems. I really like the main idea and think it is inspiring. However, the experiments and technical contributions are rather limited. Theoretical / ML contribution: Using an MMD to factorize groups of latent variables is well-known and combining flow-based maximum likelihood training in the forward model with GAN-like objectives in the inverse model has been done before as well. Experimental contribution: I am not fully convinced by the experiments. The inverse kinematics experiment shows that the posterior collapses from large uncertainty to almost a point for the right-most joint. This seems like a negative result to me. The medical experiment also seems rather limited, because if I understand correctly the tissue data is artificial and the proposed INN only outperforms competitors (despite ABC) on two out of three measurements. Further, the authors should have explained the experimental setup of the tissue experiment better, as it is not a standard task in the field. In the astronomy experiment figure 4 shows strong correlations between some of the z variables, the authors claim that this is a feature of their method, but I argue that they should not be present if training with the factorial prior was successful. It would be good to show the correlation between y and z variables as well if they show high dependencies, learning was not very successful. Simply eyeballing the shape of the posterior is not enough to conclude independence. In summary, even though interesting, the significance of the experimental results is hard to judge and I am a bit worried that if the proposed model is making some strange mistakes on artificial toy-data, how well it will perform on challenging realistic problems. 4) Main Concerns The authors claim that specifying a prior/posterior distribution in density modeling is complicated and typically the chosen distributions are too simplistic. This argument is, of course, valid, but they also have the same problem and specify z to be factorial Gaussian. So the same \"hen-and-egg\" problem applies here. The authors also seem to suggest that they are the first to train flow-based models in forward and inverse direction, but this has already been done in the flow-GAN paper [1]. MMD does not easily scale to high-dimensional problems, this is not a problem here as all artificial problems considered are very low-dimensional. But when applying the proposed algorithm in realistic settings, one will likely need extensions of MMD, like used in MMD GANs, which would introduce min/max games on both sides of the network. This will likely be hard to train and constitutes a fundamental limitation of the approach that needs to be discussed. 5) Minor Concerns - Some basic citations on normalizing flows seem to be missing, e.g. [2,3]. - How does one guarantee that padded regions are actually zero on output when padding input with zeros? Small variance in those dimensions could potentially code important information. Is this considered as part of y or z? - The authors require the existence of inverse and set this equal to bijectivity, but injectivity would be sufficient. - The authors mention that z is conditioned on y, but in their notation, the conditional density p(z|y) never shows up explicitly. It should be made clear, that p(z)=p(z|y) is a consequence of their additional MMD penalty and only holds at convergence. [1] Grover et al., \"Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models\" [2] Tabak and Turner, \"Density estimation by dual ascent of the log-likelihood\" [3] Deco and Brauer, \"Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures\"", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your time , and your constructive comments , we are looking forward to further discussions ! We answer your questions and concerns in the following . Note that we split the response into two comments , due to the 5000 character limit . > `` The inverse kinematics experiment shows that the posterior collapses from large uncertainty to almost a point for the right-most joint . This seems like a negative result to me . '' This comment made us realize that the description/illustration of experiment 2 may not have been clear enough . The rightmost circle marker is not a joint , but the end effector ( \u2018 hand \u2019 ) of the arm . The conditioning variable y is the position of this hand . Therefore , having the hand located on or near the gray cross is the desired outcome of the experiment , not a failure . The thick contour line does not represent the posterior p ( x|y ) , but indicates the re-simulation error : It is the 97 % -confidence region of the model \u2019 s end-point distribution p ( y|y_target ) = integral p ( y|x ) p ( x|y_target ) dx and should be as small as possible ( ideally , a delta ( y - y_target ) is desired ) . The ABC result ( leftmost panel ) is essentially the ground truth posterior . We will replace Fig.3 with the following improved illustration , to clarify the setup and show what the arm \u2019 s degrees of freedom are : https : //i.imgur.com/nNMdwPA.png > `` The medical experiment also seems rather limited , because if I understand correctly the tissue data is artificial and the proposed INN only outperforms competitors ( despite ABC ) on two out of three measurements. `` Concerning the artificial nature of the medical experiment : Medical researchers must resort to simulation , because so far there is no way to create real training data from living tissue . These simulations are sufficiently realistic that they are currently used in clinical trials during actual surgery , albeit only with point estimate methods . The medical scientists consider our approach a major leap forward , because our full posteriors allow them to quantify uncertainty reliably and efficiently for the first time , especially regarding possible ambiguities arising from multi-modal posteriors . Concerning the performance measures : To compare posteriors , the calibration errors reported in Sec.4.2 ( \u201c Quantitative results \u201d ) and Appendix Sec.6 are the most meaningful performance metrics , and the INN has a clear lead here . We will add these numbers to Table 1 to emphasize their importance . The numbers in the current Table 1 refer to MAP estimate accuracy , where alternative methods may be competitive , even if their estimated posteriors or uncertainties are inferior . > `` In the astronomy experiment figure 4 shows strong correlations between some of the z variables , the authors claim that this is a feature of their method , but I argue that they should not be present if training with the factorial prior was successful . It would be good to show the correlation between y and z variables as well if they show high dependencies , learning was not very successful . '' There seems to be a misunderstanding , the paper does not show the correlation matrix of the latent z variables . Instead , the matrices in Figs . 4 and 5 ( right ) show the correlation of the x-variables for some fixed y . It is a distinguishing feature of our method that we can uncover correlations in the posterior p ( x|y ) , which are not visible in the marginals p ( x_i|y ) or a mean-field approximation . We verify correctness of the correlations in Fig.4 via comparison to ( expensive ) ABC . > `` The authors also seem to suggest that they are the first to train flow-based models in forward and inverse direction , but this has already been done in the flow-GAN paper [ 1 ] . `` Thank you for pointing out that their \u2018 hybrid \u2019 strategy is equivalent to bi-directional training . We will change the related work and Sec.3.3 , to properly appreciate their pioneering contributions . Note that we did not make any claims to be the first to use bi-directional training ."}, {"review_id": "rJed6j0cKX-1", "review_text": "The authors propose in this paper an approach for learning models with tractable approximate posterior inference. The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described. From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC). Concerning the experimental section: - The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution. However, I do not understand how are the *discrete* output y is handled. Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice? - The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem. - For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn\u2019t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better. I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments. The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method. However, the real-world experiments are not necessarily the easiest to read. EDIT: the concerns were mostly addressed in the revision. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your time , and your constructive comments , we are looking forward to further discussions ! We answer your questions and concerns in the following . > `` However , I do not understand how are the * discrete * output y is handled . '' For this toy problem , we represent labels y by standard one-hot encoding , and we directly regress one-hot vectors using squared loss instead of softmax . This allows us to input one-hot vectors into the inverted network to generate conditional x-samples . > `` I \u2019 m not sure I understand what we are supposed to learn from the astrophysics experiments . '' We included this experiment to demonstrate that we are able to find multi-modal posteriors in a second real-world setting relevant to natural science . > `` INN outperforms other methods [ ... ] over some metrics such as the error on parameters recovery ( Table 1 ) , calibration error , and does indeed have a approximate posterior which seems to correspond to the ABC solution better '' We indeed consider the calibration errors ( reported in Sec.4.2 ( \u201c Quantitative results \u201d ) and Appendix Sec.6 ) the most meaningful of these comparisons , because they directly measure the quality of the estimated posterior distributions , and INNs have a clear lead here . We will add these numbers to Table 1 to emphasize their importance . > `` However , the real-world experiments are not necessarily the easiest to read . '' We understand , although we tried our best to condense the complicated nature of these applications . For the astrophysics setting , we provide more information in the appendix , Sec.5 , and for the medical application we refer to [ 1 ] for full details . [ 1 ] Wirkert et al . : Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression . International Journal of Computer Assisted Radiology and Surgery , 2016 . ( https : //link.springer.com/article/10.1007/s11548-016-1376-5 )"}, {"review_id": "rJed6j0cKX-2", "review_text": "While the invertible model structure itself is essentially the same as Real-NVP, the use of observation variables in the framework with theoretically sound bidirectional training for safe use of the seemingly na\u00efve inclusion of y (i.e., y and z can be independent). Its abilities to model the posterior distributions of the inputs are supported by both quantitative and qualitative experiments. The demonstration on practical examples is a plus. The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE. This is an interesting paper overall, so I am looking forward for further discussions. Pros: 1. Extensive analyses of the possibility of modeling posterior distributions with an INN have been shown. Detailed experiment setups are provided in the appendix. 2. The theoretical guarantee (with some assumptions) of the true posterior might be beneficial in practice for relatively low-dimensional or less complex tasks. Comments/Questions: 1. From the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted \u201ccGAN\u2026often lack satisfactory diversity in practice\u201d. Also, can cGAN be used estimate the density of X (posterior or not)? 2. For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)? This question comes from my observation that the nature of the losses, especially for L_y vs. L_y,L_x (i.e., SL vs. USL) seem to be different. 3. \u201cwe find it advantageous to pad both the in- and output of the network with equal number of zeros\u201d: Is this to effectively increase the intermediate network dimensions? Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero? It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z). 4. It seems that most of the experiments are done in relatively small dimensional data. This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your time , and your constructive comments , we are looking forward to further discussions ! We answer your questions and concerns in the following . > `` The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE . '' It is indeed possible to adapt other network types to the task of predicting conditional posteriors . We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper . In the present paper , we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs . Concerning the comments/questions : 1 . > `` could the authors elaborate on the comparison against cGAN '' cGAN generators are at an inherent disadvantage relative to INNs , because they never see ground-truth pairs ( x , y ) directly -- they are only informed about them indirectly via discriminator gradients . This it not a problem for simple relationships , e.g.between images x and attributes y , and cGANs work very well there . However , it makes learning of complicated forward processes much harder and may cause the resulting posteriors to be inaccurate . Moreover , INNs are forced to embed every training point x somewhere in the latent space , whereas cGAN generators may fail to allocate latent space for some x , because this is never explicitly penalized . This can lead to mode collapse and insufficient diversity . > `` Can cGAN be used to estimate the density of X ( posterior or not ) ? '' cGANs can in principle do this by choosing a generator architecture with tractable Jacobian ( using e.g.coupling layers or autoregressive flow ) , but we are not aware of published results about this possibility . 2. > `` For the bidirectional training , did the ratios of the losses ( L_z , L_y , L_x ) have to be changed , or the iterations of forward/backward trainings have to be changed ( e.g. , 1 forward , 1 backward vs. 2 forward , 1 backward ) ? '' Yes , the weights of the losses are considered as hyperparameters , because the magnitude of MMD-based losses depends on the chosen kernel function . Hyperparameter optimization suggested an up-weighting of MMD-based losses by a factor of 5 , to give them approximately equal impact as the supervised loss . For the iterations , we accumulated gradients over one forward and one inverse network execution before each parameter update . We also tried alternating parameter updates after each forward and backward pass , which resulted in equal accuracy , but was a bit slower . We did not experiment with other ratios than 1:1 . 3. > `` Is this to effectively increase the intermediate network dimensions ? '' This is precisely the reason : It improves the representational power of the INN , as mentioned in Sec.3.2 and discussed in our response to reviewer 1 . At present , we find this is only necessary for the toy problem in Fig.2. > `` It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests ( x , y and z ) . '' This is correct . We explicitly prevent information from being hidden in the padding dimensions in the following way : A squared loss ensures that the amplitudes are close to zero . In an additional inverse training pass , we overwrite the padding dimensions with noise of the same amplitude , and minimize their effect via a reconstruction loss . We will add this to the relevant paragraph in the paper . 4. > `` I am curious if this model could succeed on higher dimensional data '' Works such as [ 1 , 2 , 3 ] ( also cited in our paper ) have shown that the coupling layer architecture in general works well with images . These works use maximum likelihood training , i.e.exploit the tractable Jacobians to maximize the likelihood of the data embedding in latent space . To scale-up our approach , we may need to replace MMD loss with maximum likelihood as well , and first experiments with this show promising results , see https : //i.imgur.com/ft09Pk9.png . [ 1 ] Laurent Dinh , Jascha Sohl-Dickstein , and Samy Bengio . Density estimation using Real NVP . arXiv:1605.08803 , 2016 . [ 2 ] Diederik P Kingma and Prafulla Dhariwal . Glow : Generative flow with invertible 1x1 convolutions . arXiv:1807.03039 , 2018 [ 3 ] Schirrmeister , Robin Tibor , et al . `` Generative Reversible Networks . '' arXiv:1806.01610 , 2018"}], "0": {"review_id": "rJed6j0cKX-0", "review_text": "1) Summary The authors propose to use invertible networks to solve ambiguous inverse problems. This is done by training one group of Real-NVP output variables supervised while training the other group via maximum likelihood under a Gaussian prior as done in the standard Real-NVP. Further, the authors suggest to not only train the forward model, but also the inverse model with an MMD critic, similar to previous works that used a more flexible GAN critic [1]. 2) Clarity The paper is easy to understand and the main idea is well-motivated. 3) Significance The main contribution of this work is of conceptual nature and illustrates how invertible networks are a promising framework for many inverse problems. I really like the main idea and think it is inspiring. However, the experiments and technical contributions are rather limited. Theoretical / ML contribution: Using an MMD to factorize groups of latent variables is well-known and combining flow-based maximum likelihood training in the forward model with GAN-like objectives in the inverse model has been done before as well. Experimental contribution: I am not fully convinced by the experiments. The inverse kinematics experiment shows that the posterior collapses from large uncertainty to almost a point for the right-most joint. This seems like a negative result to me. The medical experiment also seems rather limited, because if I understand correctly the tissue data is artificial and the proposed INN only outperforms competitors (despite ABC) on two out of three measurements. Further, the authors should have explained the experimental setup of the tissue experiment better, as it is not a standard task in the field. In the astronomy experiment figure 4 shows strong correlations between some of the z variables, the authors claim that this is a feature of their method, but I argue that they should not be present if training with the factorial prior was successful. It would be good to show the correlation between y and z variables as well if they show high dependencies, learning was not very successful. Simply eyeballing the shape of the posterior is not enough to conclude independence. In summary, even though interesting, the significance of the experimental results is hard to judge and I am a bit worried that if the proposed model is making some strange mistakes on artificial toy-data, how well it will perform on challenging realistic problems. 4) Main Concerns The authors claim that specifying a prior/posterior distribution in density modeling is complicated and typically the chosen distributions are too simplistic. This argument is, of course, valid, but they also have the same problem and specify z to be factorial Gaussian. So the same \"hen-and-egg\" problem applies here. The authors also seem to suggest that they are the first to train flow-based models in forward and inverse direction, but this has already been done in the flow-GAN paper [1]. MMD does not easily scale to high-dimensional problems, this is not a problem here as all artificial problems considered are very low-dimensional. But when applying the proposed algorithm in realistic settings, one will likely need extensions of MMD, like used in MMD GANs, which would introduce min/max games on both sides of the network. This will likely be hard to train and constitutes a fundamental limitation of the approach that needs to be discussed. 5) Minor Concerns - Some basic citations on normalizing flows seem to be missing, e.g. [2,3]. - How does one guarantee that padded regions are actually zero on output when padding input with zeros? Small variance in those dimensions could potentially code important information. Is this considered as part of y or z? - The authors require the existence of inverse and set this equal to bijectivity, but injectivity would be sufficient. - The authors mention that z is conditioned on y, but in their notation, the conditional density p(z|y) never shows up explicitly. It should be made clear, that p(z)=p(z|y) is a consequence of their additional MMD penalty and only holds at convergence. [1] Grover et al., \"Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models\" [2] Tabak and Turner, \"Density estimation by dual ascent of the log-likelihood\" [3] Deco and Brauer, \"Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures\"", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your time , and your constructive comments , we are looking forward to further discussions ! We answer your questions and concerns in the following . Note that we split the response into two comments , due to the 5000 character limit . > `` The inverse kinematics experiment shows that the posterior collapses from large uncertainty to almost a point for the right-most joint . This seems like a negative result to me . '' This comment made us realize that the description/illustration of experiment 2 may not have been clear enough . The rightmost circle marker is not a joint , but the end effector ( \u2018 hand \u2019 ) of the arm . The conditioning variable y is the position of this hand . Therefore , having the hand located on or near the gray cross is the desired outcome of the experiment , not a failure . The thick contour line does not represent the posterior p ( x|y ) , but indicates the re-simulation error : It is the 97 % -confidence region of the model \u2019 s end-point distribution p ( y|y_target ) = integral p ( y|x ) p ( x|y_target ) dx and should be as small as possible ( ideally , a delta ( y - y_target ) is desired ) . The ABC result ( leftmost panel ) is essentially the ground truth posterior . We will replace Fig.3 with the following improved illustration , to clarify the setup and show what the arm \u2019 s degrees of freedom are : https : //i.imgur.com/nNMdwPA.png > `` The medical experiment also seems rather limited , because if I understand correctly the tissue data is artificial and the proposed INN only outperforms competitors ( despite ABC ) on two out of three measurements. `` Concerning the artificial nature of the medical experiment : Medical researchers must resort to simulation , because so far there is no way to create real training data from living tissue . These simulations are sufficiently realistic that they are currently used in clinical trials during actual surgery , albeit only with point estimate methods . The medical scientists consider our approach a major leap forward , because our full posteriors allow them to quantify uncertainty reliably and efficiently for the first time , especially regarding possible ambiguities arising from multi-modal posteriors . Concerning the performance measures : To compare posteriors , the calibration errors reported in Sec.4.2 ( \u201c Quantitative results \u201d ) and Appendix Sec.6 are the most meaningful performance metrics , and the INN has a clear lead here . We will add these numbers to Table 1 to emphasize their importance . The numbers in the current Table 1 refer to MAP estimate accuracy , where alternative methods may be competitive , even if their estimated posteriors or uncertainties are inferior . > `` In the astronomy experiment figure 4 shows strong correlations between some of the z variables , the authors claim that this is a feature of their method , but I argue that they should not be present if training with the factorial prior was successful . It would be good to show the correlation between y and z variables as well if they show high dependencies , learning was not very successful . '' There seems to be a misunderstanding , the paper does not show the correlation matrix of the latent z variables . Instead , the matrices in Figs . 4 and 5 ( right ) show the correlation of the x-variables for some fixed y . It is a distinguishing feature of our method that we can uncover correlations in the posterior p ( x|y ) , which are not visible in the marginals p ( x_i|y ) or a mean-field approximation . We verify correctness of the correlations in Fig.4 via comparison to ( expensive ) ABC . > `` The authors also seem to suggest that they are the first to train flow-based models in forward and inverse direction , but this has already been done in the flow-GAN paper [ 1 ] . `` Thank you for pointing out that their \u2018 hybrid \u2019 strategy is equivalent to bi-directional training . We will change the related work and Sec.3.3 , to properly appreciate their pioneering contributions . Note that we did not make any claims to be the first to use bi-directional training ."}, "1": {"review_id": "rJed6j0cKX-1", "review_text": "The authors propose in this paper an approach for learning models with tractable approximate posterior inference. The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described. From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC). Concerning the experimental section: - The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution. However, I do not understand how are the *discrete* output y is handled. Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice? - The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem. - For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn\u2019t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better. I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments. The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method. However, the real-world experiments are not necessarily the easiest to read. EDIT: the concerns were mostly addressed in the revision. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your time , and your constructive comments , we are looking forward to further discussions ! We answer your questions and concerns in the following . > `` However , I do not understand how are the * discrete * output y is handled . '' For this toy problem , we represent labels y by standard one-hot encoding , and we directly regress one-hot vectors using squared loss instead of softmax . This allows us to input one-hot vectors into the inverted network to generate conditional x-samples . > `` I \u2019 m not sure I understand what we are supposed to learn from the astrophysics experiments . '' We included this experiment to demonstrate that we are able to find multi-modal posteriors in a second real-world setting relevant to natural science . > `` INN outperforms other methods [ ... ] over some metrics such as the error on parameters recovery ( Table 1 ) , calibration error , and does indeed have a approximate posterior which seems to correspond to the ABC solution better '' We indeed consider the calibration errors ( reported in Sec.4.2 ( \u201c Quantitative results \u201d ) and Appendix Sec.6 ) the most meaningful of these comparisons , because they directly measure the quality of the estimated posterior distributions , and INNs have a clear lead here . We will add these numbers to Table 1 to emphasize their importance . > `` However , the real-world experiments are not necessarily the easiest to read . '' We understand , although we tried our best to condense the complicated nature of these applications . For the astrophysics setting , we provide more information in the appendix , Sec.5 , and for the medical application we refer to [ 1 ] for full details . [ 1 ] Wirkert et al . : Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression . International Journal of Computer Assisted Radiology and Surgery , 2016 . ( https : //link.springer.com/article/10.1007/s11548-016-1376-5 )"}, "2": {"review_id": "rJed6j0cKX-2", "review_text": "While the invertible model structure itself is essentially the same as Real-NVP, the use of observation variables in the framework with theoretically sound bidirectional training for safe use of the seemingly na\u00efve inclusion of y (i.e., y and z can be independent). Its abilities to model the posterior distributions of the inputs are supported by both quantitative and qualitative experiments. The demonstration on practical examples is a plus. The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE. This is an interesting paper overall, so I am looking forward for further discussions. Pros: 1. Extensive analyses of the possibility of modeling posterior distributions with an INN have been shown. Detailed experiment setups are provided in the appendix. 2. The theoretical guarantee (with some assumptions) of the true posterior might be beneficial in practice for relatively low-dimensional or less complex tasks. Comments/Questions: 1. From the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted \u201ccGAN\u2026often lack satisfactory diversity in practice\u201d. Also, can cGAN be used estimate the density of X (posterior or not)? 2. For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)? This question comes from my observation that the nature of the losses, especially for L_y vs. L_y,L_x (i.e., SL vs. USL) seem to be different. 3. \u201cwe find it advantageous to pad both the in- and output of the network with equal number of zeros\u201d: Is this to effectively increase the intermediate network dimensions? Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero? It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z). 4. It seems that most of the experiments are done in relatively small dimensional data. This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your time , and your constructive comments , we are looking forward to further discussions ! We answer your questions and concerns in the following . > `` The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE . '' It is indeed possible to adapt other network types to the task of predicting conditional posteriors . We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper . In the present paper , we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs . Concerning the comments/questions : 1 . > `` could the authors elaborate on the comparison against cGAN '' cGAN generators are at an inherent disadvantage relative to INNs , because they never see ground-truth pairs ( x , y ) directly -- they are only informed about them indirectly via discriminator gradients . This it not a problem for simple relationships , e.g.between images x and attributes y , and cGANs work very well there . However , it makes learning of complicated forward processes much harder and may cause the resulting posteriors to be inaccurate . Moreover , INNs are forced to embed every training point x somewhere in the latent space , whereas cGAN generators may fail to allocate latent space for some x , because this is never explicitly penalized . This can lead to mode collapse and insufficient diversity . > `` Can cGAN be used to estimate the density of X ( posterior or not ) ? '' cGANs can in principle do this by choosing a generator architecture with tractable Jacobian ( using e.g.coupling layers or autoregressive flow ) , but we are not aware of published results about this possibility . 2. > `` For the bidirectional training , did the ratios of the losses ( L_z , L_y , L_x ) have to be changed , or the iterations of forward/backward trainings have to be changed ( e.g. , 1 forward , 1 backward vs. 2 forward , 1 backward ) ? '' Yes , the weights of the losses are considered as hyperparameters , because the magnitude of MMD-based losses depends on the chosen kernel function . Hyperparameter optimization suggested an up-weighting of MMD-based losses by a factor of 5 , to give them approximately equal impact as the supervised loss . For the iterations , we accumulated gradients over one forward and one inverse network execution before each parameter update . We also tried alternating parameter updates after each forward and backward pass , which resulted in equal accuracy , but was a bit slower . We did not experiment with other ratios than 1:1 . 3. > `` Is this to effectively increase the intermediate network dimensions ? '' This is precisely the reason : It improves the representational power of the INN , as mentioned in Sec.3.2 and discussed in our response to reviewer 1 . At present , we find this is only necessary for the toy problem in Fig.2. > `` It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests ( x , y and z ) . '' This is correct . We explicitly prevent information from being hidden in the padding dimensions in the following way : A squared loss ensures that the amplitudes are close to zero . In an additional inverse training pass , we overwrite the padding dimensions with noise of the same amplitude , and minimize their effect via a reconstruction loss . We will add this to the relevant paragraph in the paper . 4. > `` I am curious if this model could succeed on higher dimensional data '' Works such as [ 1 , 2 , 3 ] ( also cited in our paper ) have shown that the coupling layer architecture in general works well with images . These works use maximum likelihood training , i.e.exploit the tractable Jacobians to maximize the likelihood of the data embedding in latent space . To scale-up our approach , we may need to replace MMD loss with maximum likelihood as well , and first experiments with this show promising results , see https : //i.imgur.com/ft09Pk9.png . [ 1 ] Laurent Dinh , Jascha Sohl-Dickstein , and Samy Bengio . Density estimation using Real NVP . arXiv:1605.08803 , 2016 . [ 2 ] Diederik P Kingma and Prafulla Dhariwal . Glow : Generative flow with invertible 1x1 convolutions . arXiv:1807.03039 , 2018 [ 3 ] Schirrmeister , Robin Tibor , et al . `` Generative Reversible Networks . '' arXiv:1806.01610 , 2018"}}