{"year": "2020", "forum": "r1xwA34KDB", "title": "Learning Invariants through Soft Unification", "decision": "Reject", "meta_review": "The main concern raised by reviewers is the limited experiments, which are on simple tasks and missing some baselines to state-of-the-art methods. While the overall approach is interesting, the reviewers found the empirical evidence to be fairly unconvincing. ", "reviews": [{"review_id": "r1xwA34KDB-0", "review_text": "This paper explores a very interesting idea: can a model learn what variables are and how to use them? Unfortunately, the paper doesn't seem quite ready: the model description was very hard to follow and it's not clear the approach has found a compelling use case. I read the paper carefully three times, and try as I might, I simply can't get my head around the entire architecture. The modeling section jumps straight into a series of definitions, without trying to build intuition or provide a worked example. There is an example in Figure 2, but it isn't really explained and I didn't find it helpful. Unification seems to be implemented as a form of attention (or self-attention) where the model can control the degree to which a symbol acts as variable. But the relationship between soft unification and attention isn't really spelled out -- what's the same, what's different? Ultimately it's not clear to me what the model is attending over during soft unification. There are various other aspects of the paper that aren't clear: - strong vs. weak supervision - comparison models DMN and IMA are not introduced at all, and include no references - the logical reasoning experiment is not clearly described - there is only a cursory conclusion I am not sure the model has found a compelling use case. On bAbi with weak supervision, the model is worse than the comparison models. It only slightly beats out memory networks with strong supervision. For logical reasoning, it's not clear what it is compared against or if the comparison is fair. The clearest win over standard networks is on the simple synthetic experiments. Finally, the authors mention the paper has a cognitive science motivation, in that \"Humans learn what variables are and how to use then at a young age\" or that \"symbolic thought with variables is learned...\", taking a strong \"nurture\" stance on the origin of variables. But variables could very well be innate and simply early emerging. Any discussion of the origin of variables in the mind requires more nuance. I am excited about this research direction, and it could ultimately be a very nice contribution as the work matures. I don't think the paper is ready in its current form. ", "rating": "1: Reject", "reply_text": "Dear reviewer , thank you for your feedback . We are excited that you find the idea interesting and important . To answer your questions and comments : \u201c But the relationship between soft unification and attention is n't really spelled out -- what 's the same , what 's different ? \u201d - The difference / similarity is mentioned multiple times in the paper , firstly in the introduction : \u201c we consider unification a selection of the most appropriate value .. , we can reframe it as a form of attention. \u201d ; secondly in Section 2 : \u201c For example , \u2026 $ \\phi_V ( X : s_d ) $ would become a weighted sum of symbol embeddings as in conventional attention models \u201d , and finally in Definition 3 where soft unification is defined as a dot product attention , equation 3 . Hence , soft unification is implemented as a form of dot product attention . \u201c Ultimately it 's not clear to me what the model is attending over during soft unification. \u201d - Following equation 3 , soft unification attends over the symbols present in the example K. This K is another example from the dataset and could be a sequence , grid , a story or a logic program as setup in the datasets section and detailed for each architecture in Section 3 . \u201c strong vs. weak supervision \u201d - We mention the strongly supervised experiments in Section 5 : \u201c in the strongly supervised cases , the negative log-likelihood for the context attentions are also added to the objective function. \u201d In the memory networks literature surrounding the bAbI dataset , this refers to using the supporting facts . We do not supervise the soft unification mechanism in any of the experiments . \u201c comparison models DMN and IMA are not introduced at all , and include no references \u201d - We do not provide detailed previous work to reduce clutter , distinguish our work and adhere to space constraints . The reference for them are in the caption of Table 3 : \u201c and DMN , IMA by Cingillioglu & Russo ( 2019 ) . \u201d We ask the readers to refer to the cited paper for further details . Similarly N2N , GN2N , EntNet are not details and we ask the readers to refer to the citations . \u201c the logical reasoning experiment is not clearly described \u201d - We use an existing data generation procedure as mentioned , \u201c using the procedure by Cingillioglu & Russo ( 2019 ) . \u201c and only give details of the specific settings we used to generate the data such as the arity and size of the data . Similar to the bAbI dataset , for the logical reasoning dataset we ask readers to refer to the original papers that introduce the individual tasks . \u201c there is only a cursory conclusion \u201d - This is quite subjective as the conclusion of the paper clearly states a novel approach to incorporating variables to neural network architectures and learning invariants . We present the concrete output of this approach : the invariants learnt by analysing soft unification mechanism implemented as an attention . \u201c I am not sure the model has found a compelling use case. \u201d - This is interesting as the judgement seems to be made on the final accuracy based performance of the model in the bAbI and the logical reasoning dataset . The objective is \u201c learning invariants \u201d rather than to lower error rates further . We urge the readers to consider the qualitative novel output of our approach instead of a win or lose against other models in certain datasets . Our approach is flexible in the network architecture ( UMLP , UCNN , UMN ) as well as the tasks it can solve , in some cases better or as good as other models . \u201c But variables could very well be innate and simply early emerging. \u201d - This is also very interesting , it may very well be . We followed the line of work cited in the introduction and the related work to establish our argument showing evidence such as pretend play etc . as to why the notion of a variable could be learned . The discussion about whether it could be innate is more appropriate in the field of developmental psychology that is outside the scope and focus of this work . We would be happy to incorporate references showing evidence for the innateness of variables in human reasoning . Please note that , Reviewer 4 supports the motivation of a cognitive background and Reviewer 1 points out the paper is well written , structured and clear . These seem to counter your two main concerns . We fail to find in the review any further scientific or technical grounds for disputing the validity or novelty of the work to hamper its publication . We believe we have addressed your questions and comments and highlighted the novelty and contribution this work brings ."}, {"review_id": "r1xwA34KDB-1", "review_text": "This paper presents a novel approach for learning invariants that can capture underlying patterns in the tasks through Unification Networks. This effectively allows the machine to learn the notion of `variable`, which is a symbol that can take on different values. Pros: The authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification. They further performed analysis on the learned invariants, and verified the sensibility. The paper overall is well written and structured. Cons: Despite its superiority over plain baseline, the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks. Some of the technical details regarding the choice of hyperparameters are missing. For example: In section 6, what\u2019s the rationale of setting $t$ differently for bAbI solely? In Equation 5, how is the sparsity regularization parameter $\\tau$ chosen optimally for a particular task? A bit more discussion on these choices would be helpful. Overall, this paper presents a seemingly promising architecture capable of learning and using variables, with the caveat for lack of experiments and comparison with other state-of-the-art methods. ", "rating": "3: Weak Reject", "reply_text": "Dear reviewer , thank you for your feedback and we are glad you have found the paper well written and structured . To answer your questions and comments : \u201c the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks \u201d - If we are referring to the bAbI and the logical reasoning tasks as reasoning related tasks , we provide a detailed comparison of each task with all of the state-of-the-art models in Appendix D Table 7 and 8 . We present similar ( related to our memory network architecture ) memory based architectures in the main body of the paper and the rest of the state-of-the-art models in the appendix due to space limitations . \u201c In section 6 , what \u2019 s the rationale of setting $ t $ - threshold differently for bAbI solely ? \u201d - This question is answered at the beginning of Section 6 , \u201c The magnitude of this threshold seems to depend on the amount of regularisation , equation 5 , and the number of training steps along with batch size all controlling how much $ \\psi $ is pushed towards 0. \u201d Thus , after training is complete there will be a lower bound on $ \\psi $ depending on those aspects which is different for bAbI from the other datasets . \u201c how is the sparsity regularization parameter $ \\tau $ chosen optimally for a particular task ? \u201d - It is not chosen optimally , we used 0.1 as a reasonable coefficient in recognising that $ \\tau $ is an L1 regularisation applied to $ \\psi $ . We haven \u2019 t performed hyper-parameter tuning . \u201c the caveat for lack of experiments and comparison with other state-of-the-art methods \u201d - We disagree with this statement as we present 4 datasets , 3 different architectures , different experimental setups ( strong vs weak , 1k vs 50 training examples ) , analysis of invariants , analysis of soft unification as well as comparison to existing state-of-the-architectures in Sections 4 , 5 and 6 respectively with detailed results and further analysis in Appendix D. We hope we have answered your questions individually and highlighted the novelty , the results of the experimental setup and comparison to the state-of-the-art models presented in this work ."}, {"review_id": "r1xwA34KDB-2", "review_text": "The authors propose a neural network approach to variable unification and reasoning by example as a way to mimic the human ability to identify invariant patterns in examples and then apply them more generally in practice. This general idea of identifying invariates and mapping new instances to them is well motivated by the authors, citing work in philosophy of mind, cognitive science, and developmental psychology. The authors go on to propose MLP, CNN and Memory Network models of unification for sequence, grid, and story reasoning tasks respectively. Experiments on the sequence and grid datasets demonstrate the data efficiency of this approach. MLP and CNN models with unification achieve near perfect performance in fewer iterations (an order of magnitude fewer in the MLP case!) than their non-unification enabled counter parts. Unification enabled models also demonstrate high performance in a reduced training set setting (using only 50 training examples). While this is encouraging, these are very simple toy tasks. I also am in doubt as to whether the representation of these problems causes some issues. In the sequence task, one question the models are trying to solve is what symbol is the head or tail of the sequence. Modeling variables over the sequence of symbols here is, in a sense, the wrong object of study. The position of the symbols would need to be represented, e.g. a b c d 1 4 3 1 where I've represented positions as a-d, and the learned invariant about head questions would be: X:a b c d Y:1 4 3 1 As is, by mapping symbols and not positions to variables, one cannot, at the variable level distinguish between the two 1s in the sequence above. My guess is that in practice the bi-GRU model that produces embedding features of the symbols in sequence is implicitly representing head/tail positioning. Similar arguments could be made about the grid example. I don't find the experiments/analysis on the bAbI dataset very convincing. For instance, in the example given in Figure 4b (reproduced below) is shown as an example of temporal reasoning, where a symbol Z is mapped to the word morning (a symbol distinguishing a time), and the question asked is where was Bill before school. If logical reasoning is being used to solve this question, surely the symbol 'before' must also be represented as a variable. Its possible that the model is instead learning a trick about mutual exclusivity, i.e. that Y:school is the only location symbol not mentioned in question but this could fail as a general strategy. this Z:morning X:bill went to the Y:school yesterday X:bill journeyed to the A:park where was X:bill before the Y:school A:park Figure 4b It would make for a much more interesting paper if the authors took examples such as these and formed counter-factuals to probe the way the models are answering the questions. E.g., transforming the question in 4b to \"where was X:bill today\" or \"where was X:bill after school.\" Because the authors use soft unification, interpretability is difficult to assess. Interpretability is crucial here because to claim that unification and reasoning by logical induction is being used to solve tasks, it becomes important to show how the neural networks make their decisions. Given the instances of extra variables and one to many mappings on the bAbI dataset it seems very likely that the models are not solving many tasks using unification as it would be possible to learn to use the symbols directly to learn to answer. As such, I think these issues are not addressed in the paper sufficiently to warrant acceptance. Minor Notes - In definition 1, the definition of Variable is a little confusing because there are two different senses of the word in use. I understand them to be (1) Variable (X) in the logical template that is intended to be learned and used in problem solving, and (2) variable (x) in the neural network model that is a soft asignment of the Variable to a default symbol s. It would be nice if this distinction could be noted or made clearer. - In the definition 2, in the phrase \"is the invariant example such as a tokenized story\" it might be worth stating that the tokens are the symbols in S. - My understanding is that each unique symbol in the invariate is a potential variable. Does this mean there are no co-referent symbols in the invariate? Would be helpful to state whether babi contains co-referent expressions and how these might affect the model. - It might be interesting to see how model architecture affects variable learning. For example, does a CNN result in more sensible variable assignments than the mlp on a flattened representation of the grid problem? - What is the strongly supervised case? These are token level annotations I think (at least for babi) but it might be good to specify in more detail what they are. - The figure and explanation of the UMN are not very clear. From the figure is does not seem that the variables interact with the memory at all. More space could be devoted to this section. Possibly Relevant Related Work Brenden Lake. Compositional generalization through metasequence-to-sequence learning. NeurIPS 2019. ", "rating": "3: Weak Reject", "reply_text": "Dear reviewer , thank you for your detailed constructive feedback . We are happy to answer your comments : \u201c these are very simple toy tasks \u201d - We must first understand how these architectures work and analyse them in a controlled environment in order to mitigate the black box effect they create . In order to analyse the invariants , it is crucial to work in a fixed setting where the data generating distribution is known for comparison . This learning task has never been attempted before and the paper demonstrates the validity of the idea and its effectiveness , enabling work towards addressing more complex tasks . \u201c Modelling variables over the sequence of symbols here is , in a sense , the wrong object of study \u201d - Our approach is indifferent to the underlying structure of the task , demonstrated by the different datasets . There is no right or wrong task with respect to learning variables since the model does not make assumptions about the data . We present 3 different structures still using the same definitions from Section 2 . The soft unification function \u201c g \u201d potentially learns different features with different structures , as you have guessed in the next comment below . \u201c My guess is that in practice the bi-GRU model that produces embedding features of the symbols in sequence is implicitly representing head/tail positioning. \u201d - Your guess is correct ! We state this in Section 2 as the unifying properties $ \\phi_U $ that can be learned . In the example you \u2019 ve provided , 1 4 3 1 , the unifying features of 1s will be different due to the bi-GRU . This also gives great capacity to the network and the ability to unify the head of a sequence with the tail of another sequence ( Appendix D ) . \u201c If logical reasoning is being used to solve this question , surely the symbol 'before ' must also be represented as a variable \u201d - We must be careful in projecting our understanding of natural language and logical reasoning to dictate what should and shouldn \u2019 t be a variable . If this was an alien language with unknown symbols , we wouldn \u2019 t be able to say a symbol should be a variable nor assume a certain logical reasoning is involved . Hence , only variations in the data can tell , as is the case in our approach , whether a symbol should be a variable . Since the model can optimise to use 1 variable where we might expect 2 , Figure 5b , it might not follow the data generating distribution exactly but still solve the task by exploiting these commonalities . We discuss this in \u201c Interpretability versus Ability \u201d , Section 6 . \u201c .. formed counter-factuals to probe the way the models are answering the questions \u201d - This is an interesting point we also make in our relevant work in Section 7 . Our objective of learning invariants to some extent uses counter-factuals as unification changes the facts of a story . Furthermore , your question is not a counter-factual as it can be answered with \u201c unknown \u201d looking at the story facts . The question \u201c where would X : bill have been before Y : school should he have gone to the garden yesterday \u201d is a counter-factual as it yields an answer of garden against the facts presented in the story . \u201c Interpretability is crucial here because to claim that unification and reasoning by logical induction is being used to solve tasks ... \u201d - We don \u2019 t claim this is logical unification , reasoning or induction . In fact , we refrain from using those terms \u201c since neither the invariant structure needs to be rule-like nor the variables carry logical semantics \u201d . The reason is because the \u201c g \u201d and the \u201c f \u201d are learned end-to-end and could learn elements of logical unification , reasoning or not ; hence , it is inappropriate to assume or claim that it is any sort of logical induction . \u201c Does this mean there are no co-referent symbols in the invariate ? \u201d Yes , each symbol is considered unique and a potential variable independently . In bAbI co-reference task,11 , \u201c He \u201d etc . are unique symbols and treated equally . Detailed results , including task 11 , are in Appendix D Table 6 . \u201c does a CNN result in more sensible variable assignments than the mlp on a flattened representation of the grid problem ? \u201d - It is a good question . It depends on what we mean by sensible . If we refer to them following the data generating distribution then the answer is similar to asking whether an MLP or a CNN solves the task better . This is because the variables are learned with respect to an upstream \u201c f \u201d and that network provides the gradients for which symbols should be variables . In either case , we observe occasional \u201c insensible \u201d variables ( Appendix D Figure 9 ) in which the invariants can still solve the task . \u201c What is the strongly supervised case ? \u201d - These experiments use the supporting facts provided in the bAbI dataset as done in literature around memory networks . This is mentioned in Section 5 : \u201c and , in the strongly supervised cases , the negative log-likelihood for the context attentions are also added to the objective function. \u201d We do not supervise soft unification nor label correct tokens ."}], "0": {"review_id": "r1xwA34KDB-0", "review_text": "This paper explores a very interesting idea: can a model learn what variables are and how to use them? Unfortunately, the paper doesn't seem quite ready: the model description was very hard to follow and it's not clear the approach has found a compelling use case. I read the paper carefully three times, and try as I might, I simply can't get my head around the entire architecture. The modeling section jumps straight into a series of definitions, without trying to build intuition or provide a worked example. There is an example in Figure 2, but it isn't really explained and I didn't find it helpful. Unification seems to be implemented as a form of attention (or self-attention) where the model can control the degree to which a symbol acts as variable. But the relationship between soft unification and attention isn't really spelled out -- what's the same, what's different? Ultimately it's not clear to me what the model is attending over during soft unification. There are various other aspects of the paper that aren't clear: - strong vs. weak supervision - comparison models DMN and IMA are not introduced at all, and include no references - the logical reasoning experiment is not clearly described - there is only a cursory conclusion I am not sure the model has found a compelling use case. On bAbi with weak supervision, the model is worse than the comparison models. It only slightly beats out memory networks with strong supervision. For logical reasoning, it's not clear what it is compared against or if the comparison is fair. The clearest win over standard networks is on the simple synthetic experiments. Finally, the authors mention the paper has a cognitive science motivation, in that \"Humans learn what variables are and how to use then at a young age\" or that \"symbolic thought with variables is learned...\", taking a strong \"nurture\" stance on the origin of variables. But variables could very well be innate and simply early emerging. Any discussion of the origin of variables in the mind requires more nuance. I am excited about this research direction, and it could ultimately be a very nice contribution as the work matures. I don't think the paper is ready in its current form. ", "rating": "1: Reject", "reply_text": "Dear reviewer , thank you for your feedback . We are excited that you find the idea interesting and important . To answer your questions and comments : \u201c But the relationship between soft unification and attention is n't really spelled out -- what 's the same , what 's different ? \u201d - The difference / similarity is mentioned multiple times in the paper , firstly in the introduction : \u201c we consider unification a selection of the most appropriate value .. , we can reframe it as a form of attention. \u201d ; secondly in Section 2 : \u201c For example , \u2026 $ \\phi_V ( X : s_d ) $ would become a weighted sum of symbol embeddings as in conventional attention models \u201d , and finally in Definition 3 where soft unification is defined as a dot product attention , equation 3 . Hence , soft unification is implemented as a form of dot product attention . \u201c Ultimately it 's not clear to me what the model is attending over during soft unification. \u201d - Following equation 3 , soft unification attends over the symbols present in the example K. This K is another example from the dataset and could be a sequence , grid , a story or a logic program as setup in the datasets section and detailed for each architecture in Section 3 . \u201c strong vs. weak supervision \u201d - We mention the strongly supervised experiments in Section 5 : \u201c in the strongly supervised cases , the negative log-likelihood for the context attentions are also added to the objective function. \u201d In the memory networks literature surrounding the bAbI dataset , this refers to using the supporting facts . We do not supervise the soft unification mechanism in any of the experiments . \u201c comparison models DMN and IMA are not introduced at all , and include no references \u201d - We do not provide detailed previous work to reduce clutter , distinguish our work and adhere to space constraints . The reference for them are in the caption of Table 3 : \u201c and DMN , IMA by Cingillioglu & Russo ( 2019 ) . \u201d We ask the readers to refer to the cited paper for further details . Similarly N2N , GN2N , EntNet are not details and we ask the readers to refer to the citations . \u201c the logical reasoning experiment is not clearly described \u201d - We use an existing data generation procedure as mentioned , \u201c using the procedure by Cingillioglu & Russo ( 2019 ) . \u201c and only give details of the specific settings we used to generate the data such as the arity and size of the data . Similar to the bAbI dataset , for the logical reasoning dataset we ask readers to refer to the original papers that introduce the individual tasks . \u201c there is only a cursory conclusion \u201d - This is quite subjective as the conclusion of the paper clearly states a novel approach to incorporating variables to neural network architectures and learning invariants . We present the concrete output of this approach : the invariants learnt by analysing soft unification mechanism implemented as an attention . \u201c I am not sure the model has found a compelling use case. \u201d - This is interesting as the judgement seems to be made on the final accuracy based performance of the model in the bAbI and the logical reasoning dataset . The objective is \u201c learning invariants \u201d rather than to lower error rates further . We urge the readers to consider the qualitative novel output of our approach instead of a win or lose against other models in certain datasets . Our approach is flexible in the network architecture ( UMLP , UCNN , UMN ) as well as the tasks it can solve , in some cases better or as good as other models . \u201c But variables could very well be innate and simply early emerging. \u201d - This is also very interesting , it may very well be . We followed the line of work cited in the introduction and the related work to establish our argument showing evidence such as pretend play etc . as to why the notion of a variable could be learned . The discussion about whether it could be innate is more appropriate in the field of developmental psychology that is outside the scope and focus of this work . We would be happy to incorporate references showing evidence for the innateness of variables in human reasoning . Please note that , Reviewer 4 supports the motivation of a cognitive background and Reviewer 1 points out the paper is well written , structured and clear . These seem to counter your two main concerns . We fail to find in the review any further scientific or technical grounds for disputing the validity or novelty of the work to hamper its publication . We believe we have addressed your questions and comments and highlighted the novelty and contribution this work brings ."}, "1": {"review_id": "r1xwA34KDB-1", "review_text": "This paper presents a novel approach for learning invariants that can capture underlying patterns in the tasks through Unification Networks. This effectively allows the machine to learn the notion of `variable`, which is a symbol that can take on different values. Pros: The authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification. They further performed analysis on the learned invariants, and verified the sensibility. The paper overall is well written and structured. Cons: Despite its superiority over plain baseline, the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks. Some of the technical details regarding the choice of hyperparameters are missing. For example: In section 6, what\u2019s the rationale of setting $t$ differently for bAbI solely? In Equation 5, how is the sparsity regularization parameter $\\tau$ chosen optimally for a particular task? A bit more discussion on these choices would be helpful. Overall, this paper presents a seemingly promising architecture capable of learning and using variables, with the caveat for lack of experiments and comparison with other state-of-the-art methods. ", "rating": "3: Weak Reject", "reply_text": "Dear reviewer , thank you for your feedback and we are glad you have found the paper well written and structured . To answer your questions and comments : \u201c the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks \u201d - If we are referring to the bAbI and the logical reasoning tasks as reasoning related tasks , we provide a detailed comparison of each task with all of the state-of-the-art models in Appendix D Table 7 and 8 . We present similar ( related to our memory network architecture ) memory based architectures in the main body of the paper and the rest of the state-of-the-art models in the appendix due to space limitations . \u201c In section 6 , what \u2019 s the rationale of setting $ t $ - threshold differently for bAbI solely ? \u201d - This question is answered at the beginning of Section 6 , \u201c The magnitude of this threshold seems to depend on the amount of regularisation , equation 5 , and the number of training steps along with batch size all controlling how much $ \\psi $ is pushed towards 0. \u201d Thus , after training is complete there will be a lower bound on $ \\psi $ depending on those aspects which is different for bAbI from the other datasets . \u201c how is the sparsity regularization parameter $ \\tau $ chosen optimally for a particular task ? \u201d - It is not chosen optimally , we used 0.1 as a reasonable coefficient in recognising that $ \\tau $ is an L1 regularisation applied to $ \\psi $ . We haven \u2019 t performed hyper-parameter tuning . \u201c the caveat for lack of experiments and comparison with other state-of-the-art methods \u201d - We disagree with this statement as we present 4 datasets , 3 different architectures , different experimental setups ( strong vs weak , 1k vs 50 training examples ) , analysis of invariants , analysis of soft unification as well as comparison to existing state-of-the-architectures in Sections 4 , 5 and 6 respectively with detailed results and further analysis in Appendix D. We hope we have answered your questions individually and highlighted the novelty , the results of the experimental setup and comparison to the state-of-the-art models presented in this work ."}, "2": {"review_id": "r1xwA34KDB-2", "review_text": "The authors propose a neural network approach to variable unification and reasoning by example as a way to mimic the human ability to identify invariant patterns in examples and then apply them more generally in practice. This general idea of identifying invariates and mapping new instances to them is well motivated by the authors, citing work in philosophy of mind, cognitive science, and developmental psychology. The authors go on to propose MLP, CNN and Memory Network models of unification for sequence, grid, and story reasoning tasks respectively. Experiments on the sequence and grid datasets demonstrate the data efficiency of this approach. MLP and CNN models with unification achieve near perfect performance in fewer iterations (an order of magnitude fewer in the MLP case!) than their non-unification enabled counter parts. Unification enabled models also demonstrate high performance in a reduced training set setting (using only 50 training examples). While this is encouraging, these are very simple toy tasks. I also am in doubt as to whether the representation of these problems causes some issues. In the sequence task, one question the models are trying to solve is what symbol is the head or tail of the sequence. Modeling variables over the sequence of symbols here is, in a sense, the wrong object of study. The position of the symbols would need to be represented, e.g. a b c d 1 4 3 1 where I've represented positions as a-d, and the learned invariant about head questions would be: X:a b c d Y:1 4 3 1 As is, by mapping symbols and not positions to variables, one cannot, at the variable level distinguish between the two 1s in the sequence above. My guess is that in practice the bi-GRU model that produces embedding features of the symbols in sequence is implicitly representing head/tail positioning. Similar arguments could be made about the grid example. I don't find the experiments/analysis on the bAbI dataset very convincing. For instance, in the example given in Figure 4b (reproduced below) is shown as an example of temporal reasoning, where a symbol Z is mapped to the word morning (a symbol distinguishing a time), and the question asked is where was Bill before school. If logical reasoning is being used to solve this question, surely the symbol 'before' must also be represented as a variable. Its possible that the model is instead learning a trick about mutual exclusivity, i.e. that Y:school is the only location symbol not mentioned in question but this could fail as a general strategy. this Z:morning X:bill went to the Y:school yesterday X:bill journeyed to the A:park where was X:bill before the Y:school A:park Figure 4b It would make for a much more interesting paper if the authors took examples such as these and formed counter-factuals to probe the way the models are answering the questions. E.g., transforming the question in 4b to \"where was X:bill today\" or \"where was X:bill after school.\" Because the authors use soft unification, interpretability is difficult to assess. Interpretability is crucial here because to claim that unification and reasoning by logical induction is being used to solve tasks, it becomes important to show how the neural networks make their decisions. Given the instances of extra variables and one to many mappings on the bAbI dataset it seems very likely that the models are not solving many tasks using unification as it would be possible to learn to use the symbols directly to learn to answer. As such, I think these issues are not addressed in the paper sufficiently to warrant acceptance. Minor Notes - In definition 1, the definition of Variable is a little confusing because there are two different senses of the word in use. I understand them to be (1) Variable (X) in the logical template that is intended to be learned and used in problem solving, and (2) variable (x) in the neural network model that is a soft asignment of the Variable to a default symbol s. It would be nice if this distinction could be noted or made clearer. - In the definition 2, in the phrase \"is the invariant example such as a tokenized story\" it might be worth stating that the tokens are the symbols in S. - My understanding is that each unique symbol in the invariate is a potential variable. Does this mean there are no co-referent symbols in the invariate? Would be helpful to state whether babi contains co-referent expressions and how these might affect the model. - It might be interesting to see how model architecture affects variable learning. For example, does a CNN result in more sensible variable assignments than the mlp on a flattened representation of the grid problem? - What is the strongly supervised case? These are token level annotations I think (at least for babi) but it might be good to specify in more detail what they are. - The figure and explanation of the UMN are not very clear. From the figure is does not seem that the variables interact with the memory at all. More space could be devoted to this section. Possibly Relevant Related Work Brenden Lake. Compositional generalization through metasequence-to-sequence learning. NeurIPS 2019. ", "rating": "3: Weak Reject", "reply_text": "Dear reviewer , thank you for your detailed constructive feedback . We are happy to answer your comments : \u201c these are very simple toy tasks \u201d - We must first understand how these architectures work and analyse them in a controlled environment in order to mitigate the black box effect they create . In order to analyse the invariants , it is crucial to work in a fixed setting where the data generating distribution is known for comparison . This learning task has never been attempted before and the paper demonstrates the validity of the idea and its effectiveness , enabling work towards addressing more complex tasks . \u201c Modelling variables over the sequence of symbols here is , in a sense , the wrong object of study \u201d - Our approach is indifferent to the underlying structure of the task , demonstrated by the different datasets . There is no right or wrong task with respect to learning variables since the model does not make assumptions about the data . We present 3 different structures still using the same definitions from Section 2 . The soft unification function \u201c g \u201d potentially learns different features with different structures , as you have guessed in the next comment below . \u201c My guess is that in practice the bi-GRU model that produces embedding features of the symbols in sequence is implicitly representing head/tail positioning. \u201d - Your guess is correct ! We state this in Section 2 as the unifying properties $ \\phi_U $ that can be learned . In the example you \u2019 ve provided , 1 4 3 1 , the unifying features of 1s will be different due to the bi-GRU . This also gives great capacity to the network and the ability to unify the head of a sequence with the tail of another sequence ( Appendix D ) . \u201c If logical reasoning is being used to solve this question , surely the symbol 'before ' must also be represented as a variable \u201d - We must be careful in projecting our understanding of natural language and logical reasoning to dictate what should and shouldn \u2019 t be a variable . If this was an alien language with unknown symbols , we wouldn \u2019 t be able to say a symbol should be a variable nor assume a certain logical reasoning is involved . Hence , only variations in the data can tell , as is the case in our approach , whether a symbol should be a variable . Since the model can optimise to use 1 variable where we might expect 2 , Figure 5b , it might not follow the data generating distribution exactly but still solve the task by exploiting these commonalities . We discuss this in \u201c Interpretability versus Ability \u201d , Section 6 . \u201c .. formed counter-factuals to probe the way the models are answering the questions \u201d - This is an interesting point we also make in our relevant work in Section 7 . Our objective of learning invariants to some extent uses counter-factuals as unification changes the facts of a story . Furthermore , your question is not a counter-factual as it can be answered with \u201c unknown \u201d looking at the story facts . The question \u201c where would X : bill have been before Y : school should he have gone to the garden yesterday \u201d is a counter-factual as it yields an answer of garden against the facts presented in the story . \u201c Interpretability is crucial here because to claim that unification and reasoning by logical induction is being used to solve tasks ... \u201d - We don \u2019 t claim this is logical unification , reasoning or induction . In fact , we refrain from using those terms \u201c since neither the invariant structure needs to be rule-like nor the variables carry logical semantics \u201d . The reason is because the \u201c g \u201d and the \u201c f \u201d are learned end-to-end and could learn elements of logical unification , reasoning or not ; hence , it is inappropriate to assume or claim that it is any sort of logical induction . \u201c Does this mean there are no co-referent symbols in the invariate ? \u201d Yes , each symbol is considered unique and a potential variable independently . In bAbI co-reference task,11 , \u201c He \u201d etc . are unique symbols and treated equally . Detailed results , including task 11 , are in Appendix D Table 6 . \u201c does a CNN result in more sensible variable assignments than the mlp on a flattened representation of the grid problem ? \u201d - It is a good question . It depends on what we mean by sensible . If we refer to them following the data generating distribution then the answer is similar to asking whether an MLP or a CNN solves the task better . This is because the variables are learned with respect to an upstream \u201c f \u201d and that network provides the gradients for which symbols should be variables . In either case , we observe occasional \u201c insensible \u201d variables ( Appendix D Figure 9 ) in which the invariants can still solve the task . \u201c What is the strongly supervised case ? \u201d - These experiments use the supporting facts provided in the bAbI dataset as done in literature around memory networks . This is mentioned in Section 5 : \u201c and , in the strongly supervised cases , the negative log-likelihood for the context attentions are also added to the objective function. \u201d We do not supervise soft unification nor label correct tokens ."}}