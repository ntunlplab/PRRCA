{"year": "2020", "forum": "ryg8WJSKPr", "title": "ConQUR: Mitigating Delusional Bias in Deep Q-Learning", "decision": "Reject", "meta_review": "While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.\n\nConcerns raised included the need for better motivation of the practicality of the approach, versus its computational cost. The need for improved evaluations was also raised.", "reviews": [{"review_id": "ryg8WJSKPr-0", "review_text": "A recent paper by Lu et al introduced delusional bias in Q-learning, an error due to the max in the Bellman backup not being consistent with the policy representation implied by the greedy operator applied to the approximated value function. That work proposed a consistent algorithm for small and finite state spaces, which essentially enumerates over realizable policies. This paper proposes an algorithm for overcoming delusional bias in large state spaces. The idea is to add to the Q-learning objective a smooth penalty term that induces approximate consistency, and search over possible Q-function approximators. Several heuristic methods are proposed for this search, and results are demonstrated in Atari domains. I found the topic of the paper very interesting - delusional bias is an intriguing aspect of Q learning, and the approach of Lu et al is severely limited to discrete and small state spaces. Thus, tackling the large state space problem is worthy and definitely not trivial. The authors\u2019 proposed solution of combining a smooth penalty for approximate consistency and search over regressors makes sense. The implementation of the search (Sec 3.4) is not trivial, and builds on a number of heuristics, but given the difficulty of the problem, I expect that the first proposed solution will not be straightforward. I am, however, concerned with the evaluation of the method and its practicality, as reflected by the following issues: 1. The method has many hyper parameters. The most salient one, \\lambda, the penalty coefficient, is changed between 0.25 to 2 on the consistency penalty experiment, and between 1 to 1000 in the full ConQUR experiments. I did not understand the order of magnitude change between the experiments, and more importantly, how can one know a reasonable \\lambda, and an annealing schedule for it in advance. 2. I do not understand the statistical significance of the results. For example, with the constant \\lambda=0.5, the authors report beating the baseline in 11 out of 19 games. That\u2019s probably not statistically significant enough to claim improvement. Also, only one run is performed for each game; adding more runs might make the results clearer. 3. The claim that with the best \\lambda for each game, the method outperforms the baseline in 16 out 19 games seems more significant, but testing an optimal hyper parameter for each game is not fair. Statistically speaking, *even if the parameter \\lambda was set to a constant zero* for the 5 runs that the method is tested on, and the best performing run was taken for evaluation against the baseline, that would have given a strong advantage to the proposed method over the baseline\u2026. 4. For the full ConQUR, there are many more hyper parameters, which I did not understand the intuition how to choose. Again, I do not understand how the results establish any statistically significant claim. For example, what does: \u201cCONQUR wins by at least a 10% margin in 20 games, while 22 games see improvements of 1\u201310% and 8 games show little effect (plus/minus 1%) and 7 games show a decline of greater than 1% (most are 1\u20136% with the exception of Centipede at -12% and IceHockey at -86%)\u201d mean? How can I understand from this that ConQUR is really better? Establishing a clearer evaluation metric, and using well-accepted statistical tests would greatly help the paper. At the minimum, add error bars to the figures! 5. While evaluating on Atari shows applicability to large state spaces, it is hard to understand from it whether the (claimed) advantage of the method is due to the delusional bias effect, or some other factor (like implicit regularization due to the penalty term in the loss). In addition, it is hard to understand the different approximations in the method. For example, how does the proposed consistency penalty approximate the true consistency? These could all be evaluated on the simple MDP example of Lu et al. I strongly advise the authors to add such an evaluation, which is easy to implement, and will show exactly how the approximations in the approach deal with delusional bias. It will also be easier to demonstrate the effects of the different hyper parameters in a toy domain. ", "rating": "3: Weak Reject", "reply_text": "[ STATISTICAL SIGNIFICANCE TESTING ] While it is not standard in the Atari RL literature to perform statistical significance tests , we ran a Welch \u2019 s t-test on performance from iteration 40 to 100 , and obtained the following results : With lambda=10 : out of 59 games , 40 games give statistically significant difference ( p-val < 0.05 ) between ConQUR and the baseline . In 31 games , ConQUR is significantly better , while in 9 games the baseline is . With lambda=1 : 31 give statistically significant difference ( p-val < 0.05 ) between ConQUR and the baseline . In 28 games , ConQUR is significantly better , while in 3 games the baseline is . We also ran a one-sample t-test to compare against pre-trained DQN : With lambda=10 : out of 59 games , 51 games give statistically significant difference ( p-val < 0.05 ) between ConQUR and the pre-trained DQN . In 44 games , ConQUR is significantly better , while in 7 games the pre-trained DQN is . With lambda=1 : 53 give statistically significant difference ( p-val < 0.05 ) between ConQUR and the pre-trained DQN . In 43 games , ConQUR is significantly better , while in 10 games the pre-trained DQN is . 5.Thanks for raising this point about whether our methods are providing improvements because of delusion mitigation or for other reasons . We have provided a detailed response to review 2 that addresses this point : the attribution of improved performance is effectively \u201c by definition \u201d due to the ( partial ) removal of delusional bias . We acknowledge that this point should have been made more explicitly in the paper and we will clarify in revision . Regarding \u201c approximations \u201d to consistency : we assume this refers to the soft-consistency penalty only ( there are no other approximations other than limiting the search to a subset of possible action assignments ) . In the paper we explain that the soft consistency penalty measures the degree to which consistency constraints are satisfied : full consistency incurs no penalty while the penalty increases linearly in the degree of violation ( other penalty functions are possible of course ) . Your question also suggests that understanding how much more or less stringent consistency enforcement impacts induced policy quality is important -- -we agree fully . Our experiments with different values of lambda get partly at this . Evaluating soft consistency vs. * exact * consistency is more challenging in larger domains like Atari due to runtime bottlenecks ( large linear programs for linear approximators and solving NP-hard classification problems for DNNs ) . But we can do so on smaller toy domains ( see next paragraph ) . We do have results on the simple MDP of Lu et al.with a simplified ConQUR algorithm ( with exact consistency checking ) and can include these in the paper ( we will do so in an appendix ) . Your suggestion to test how ( different degrees of ) soft consistency impact the final result vis-a-vis exact consistency is a very nice one , and we will test and explicate this on the simple MDP of Lu et al , or another small example ."}, {"review_id": "ryg8WJSKPr-1", "review_text": "This paper presents a solution to tackling the problem of delusional bias in Deep Q-learning, building upon Lu et.al. (NeuRIPS 2018). Delusional bias arises because independently choosing maximizing actions at a state may be inconsistent as the backed-up values may not be realizable by any policy. They encourage non-delusional Q-functions by adding a penalty term that enforces that the max_a in Q-learning chooses actions that do not give rise to actions outside the realizable policy class. Further, in order to keep track of all consistent assignments, they pose a search problem and propose heuristics to approximately perform this search. The heuristics are based on sampling using exponentiated Q-values and scoring possible children using scores like Bellman error, and returns of the greedy policy. Their final algorithm is evaluated on a DQN and DDQN, where they observe some improvement from both components (consistency penalty and approximate search). I would lean towards being slightly negative towards accepting this paper. However, I am not sure if the paper provides enough evidence that delusional bias is a very relevant problem with DQNs, when using high-capacity neural net approximators. Further, would the problem go away, if we perform policy iteration, in the sense of performing policy iteration instead of max Q-learning (atleast in practice)? Maybe, the paper benefits with some evidence answering this question. To summarize, I am mainly concerned about the marginal benefit at the cost of added complexity and computation for this paper. I would appreciate more evidence justifying the significance of this problem in practice. Another comment about experiments is that the paper uses pre-trained DQN for the ConQur results, where only the last linear layer of the Q-network is trained with ConQur. I think this setting might hide some properties which arise through the learning process without initial pre-training, which might be more interesting. Also, how would other auxilliary losses compare in practice, for example, losses explored in the Reinforcement Learning with Auxilliary Tasks (Jaderberg et.al.) paper? ", "rating": "3: Weak Reject", "reply_text": "Thank you for the constructive feedback and for raising some important questions . Some brief responses to specific points/questions you raise . [ DOES DELUSION ARISE IN PRACTICE ? ] The purpose of the experiments is to show that mitigating delusional bias , even with high-capacity NNs , can offer improvements . We believe the experiments show that delusional bias does occur in practice since the pre-trained Q-regressors upon which we improve are Dopamine-trained DQNs/DDQNs . Our methods differ only from ( say ) DQN in the use of the soft-consistency penalty ( plus the use of search to explore multiple assignments against which to apply this penalty ) . We claim that this tackles only \u201c policy inconsistency \u201d ( i.e. , delusion ) . Because we obtain improvements over the pre-trained DQNs , our conclusion is that delusion does , indeed , arise in practice . In retrospect , we should have made this important point much more explicit in the paper -- -our apologies for not doing so originally -- -and we will do so in revision . Our experiments , we believe , do not demonstrate the full power of removing delusion , since we only retrain the last FC layer of the pre-trained DQN , which in fact limits our performance opportunities vs. training on all layers -- -this alone results in significantly better greedy policies in many instances . [ WILL DELUSION ARISE IN POLICY ITERATION ? ] This is a good point , and while it may depend on the implementation , generally policy iteration will not have delusional bias . However , our contribution is focused on improving \u201c pure \u201d value-based methods like Q-learning ( and related methods like DDQN ) . These are widely used algorithms , that researchers and practitioners often have strong reasons to use -- -our focus is to mitigate delusional bias to extract maximum value from such methods whenever they are used . [ WHY USE PRE-TRAINED NETWORKS ] ? The rationale for improving with pre-trained DQNs is three-fold . First , it demonstrates that delusion actually causes problems in practice ( as discussed above , we will articulate this point much more explicitly in revision ) . In some sense , by freezing the feature representation learned by DQN , and demonstrating that a \u201c linear \u201d value function over those same features can be trained in ( partially ) non-delusional fashion to extract improvements gives more of a focus on non-delusional training ( as opposed to novel \u201c feature discovery \u201d ) . The second reason is a practical one -- -it allowed us to scale our experiments to cover a range of hyperparameters and run the entire Atari suite ( rather than selecting just a few high-performing games ) . We completely agree with your broader point about experimenting with our methods with full network training ( i.e. , from scratch ) to understand their performance . In some sense , this paper provides a ( we hope , compelling ) first exploration of these ideas . Third , from a practical point of view , this \u201c linear tuning \u201d approach offers a relatively inexpensive alternative to extract improvements from a model learned using classic techniques ( e.g. , linear tuning requires many fewer training samples ) . We also note that , if the full application of ConQUR is too expensive in some settings , adding a our simple consistency penalty can sometimes provide a lift ( and rarely hurts ) , as shown by the experiments in Section 4.1 . This requires no major changes to standard DQN , DDQN or the like and adds no significant implementation complexity or computational cost . [ AUXILIARY LOSSES ] Thanks for making the reference to auxiliary losses , this is an interesting question . While our penalty term focuses on consistency for a single task and auxiliary losses help accelerate learning for the main task , we can imagine applying a consistency penalty for each auxiliary objective ( in addition to minimizing task \u2019 s Bellman error ) . This direction is interesting to explore , which we will cite in the revised paper ."}, {"review_id": "ryg8WJSKPr-2", "review_text": "This paper focuses on addressing the delusional bias problem in deep Q-learning, and propose a general framework (ConQUR) for integrating policy-consistent backups with regression-based function approximation for Q-learning and for managing the search through the space of possible regressors. Specifically, it proposes a soft consistency penalty to alleviate the delusional bias problem while avoiding the expensive exact consistency testing. This penalty encourages the parameters of the model to satisfy the consistency condition when solving the MSBE problem. Pros: The soft penalization itself is already shown to be effective in further improving any regression-based Q-learning algorithms (e.g., DQN and DDQN). When combining the soft consistency penalty and the search procedure, it is shown to significantly outperform the DQN and DDQN baselines, which is impressive. This work presents novel idea and solid supporting experiments. Cons: The major experimental results of the paper are in Table 4 of Appendix D. This is not a good effort to save space by moving the most important results into appendix. At least a selected set of results should be presented in the main paper rather than in appendices. One alternative approach is the authors plot a bar figure to demonstrate the performance of different algorithms on different Atari games. Other comments: \u2022 Fig. 2 two is a bit hard to read due to too many curves. \u2022 Standard baseline results other than DQN and DDQN should also be listed, in order to demonstrate that solving delusional bias could make Q-learning more competitive than alternatives (e.g., A3C, PPO, TRPO). ", "rating": "6: Weak Accept", "reply_text": "Thank you for the constructive feedback . Some brief responses : We will try to fit in the results of Table 4 into the main text -- -if this is not feasible due to space constraints , we \u2019 ll include a comprehensive summary as you suggest . With respect to other algorithms ( A3C , PPO , TRPO , etc . ) we will add a brief comparison to such algorithms in the literature , thanks for the suggestion . While we don \u2019 t make a broad claim that removing delusion will make Q-learning competitive universally -- -we believe some domains may be better suited to policy-based or actor-critic-style algorithms -- -it does in a number of the cases examined here . However , there are many instances where Q-learning is desirable for other reasons , and our primary aim is to try to extract maximum performance from Q-learning itself . We will make this part of our motivation more explicit . We will make Fig.2 more readable -- -apologies for that !"}], "0": {"review_id": "ryg8WJSKPr-0", "review_text": "A recent paper by Lu et al introduced delusional bias in Q-learning, an error due to the max in the Bellman backup not being consistent with the policy representation implied by the greedy operator applied to the approximated value function. That work proposed a consistent algorithm for small and finite state spaces, which essentially enumerates over realizable policies. This paper proposes an algorithm for overcoming delusional bias in large state spaces. The idea is to add to the Q-learning objective a smooth penalty term that induces approximate consistency, and search over possible Q-function approximators. Several heuristic methods are proposed for this search, and results are demonstrated in Atari domains. I found the topic of the paper very interesting - delusional bias is an intriguing aspect of Q learning, and the approach of Lu et al is severely limited to discrete and small state spaces. Thus, tackling the large state space problem is worthy and definitely not trivial. The authors\u2019 proposed solution of combining a smooth penalty for approximate consistency and search over regressors makes sense. The implementation of the search (Sec 3.4) is not trivial, and builds on a number of heuristics, but given the difficulty of the problem, I expect that the first proposed solution will not be straightforward. I am, however, concerned with the evaluation of the method and its practicality, as reflected by the following issues: 1. The method has many hyper parameters. The most salient one, \\lambda, the penalty coefficient, is changed between 0.25 to 2 on the consistency penalty experiment, and between 1 to 1000 in the full ConQUR experiments. I did not understand the order of magnitude change between the experiments, and more importantly, how can one know a reasonable \\lambda, and an annealing schedule for it in advance. 2. I do not understand the statistical significance of the results. For example, with the constant \\lambda=0.5, the authors report beating the baseline in 11 out of 19 games. That\u2019s probably not statistically significant enough to claim improvement. Also, only one run is performed for each game; adding more runs might make the results clearer. 3. The claim that with the best \\lambda for each game, the method outperforms the baseline in 16 out 19 games seems more significant, but testing an optimal hyper parameter for each game is not fair. Statistically speaking, *even if the parameter \\lambda was set to a constant zero* for the 5 runs that the method is tested on, and the best performing run was taken for evaluation against the baseline, that would have given a strong advantage to the proposed method over the baseline\u2026. 4. For the full ConQUR, there are many more hyper parameters, which I did not understand the intuition how to choose. Again, I do not understand how the results establish any statistically significant claim. For example, what does: \u201cCONQUR wins by at least a 10% margin in 20 games, while 22 games see improvements of 1\u201310% and 8 games show little effect (plus/minus 1%) and 7 games show a decline of greater than 1% (most are 1\u20136% with the exception of Centipede at -12% and IceHockey at -86%)\u201d mean? How can I understand from this that ConQUR is really better? Establishing a clearer evaluation metric, and using well-accepted statistical tests would greatly help the paper. At the minimum, add error bars to the figures! 5. While evaluating on Atari shows applicability to large state spaces, it is hard to understand from it whether the (claimed) advantage of the method is due to the delusional bias effect, or some other factor (like implicit regularization due to the penalty term in the loss). In addition, it is hard to understand the different approximations in the method. For example, how does the proposed consistency penalty approximate the true consistency? These could all be evaluated on the simple MDP example of Lu et al. I strongly advise the authors to add such an evaluation, which is easy to implement, and will show exactly how the approximations in the approach deal with delusional bias. It will also be easier to demonstrate the effects of the different hyper parameters in a toy domain. ", "rating": "3: Weak Reject", "reply_text": "[ STATISTICAL SIGNIFICANCE TESTING ] While it is not standard in the Atari RL literature to perform statistical significance tests , we ran a Welch \u2019 s t-test on performance from iteration 40 to 100 , and obtained the following results : With lambda=10 : out of 59 games , 40 games give statistically significant difference ( p-val < 0.05 ) between ConQUR and the baseline . In 31 games , ConQUR is significantly better , while in 9 games the baseline is . With lambda=1 : 31 give statistically significant difference ( p-val < 0.05 ) between ConQUR and the baseline . In 28 games , ConQUR is significantly better , while in 3 games the baseline is . We also ran a one-sample t-test to compare against pre-trained DQN : With lambda=10 : out of 59 games , 51 games give statistically significant difference ( p-val < 0.05 ) between ConQUR and the pre-trained DQN . In 44 games , ConQUR is significantly better , while in 7 games the pre-trained DQN is . With lambda=1 : 53 give statistically significant difference ( p-val < 0.05 ) between ConQUR and the pre-trained DQN . In 43 games , ConQUR is significantly better , while in 10 games the pre-trained DQN is . 5.Thanks for raising this point about whether our methods are providing improvements because of delusion mitigation or for other reasons . We have provided a detailed response to review 2 that addresses this point : the attribution of improved performance is effectively \u201c by definition \u201d due to the ( partial ) removal of delusional bias . We acknowledge that this point should have been made more explicitly in the paper and we will clarify in revision . Regarding \u201c approximations \u201d to consistency : we assume this refers to the soft-consistency penalty only ( there are no other approximations other than limiting the search to a subset of possible action assignments ) . In the paper we explain that the soft consistency penalty measures the degree to which consistency constraints are satisfied : full consistency incurs no penalty while the penalty increases linearly in the degree of violation ( other penalty functions are possible of course ) . Your question also suggests that understanding how much more or less stringent consistency enforcement impacts induced policy quality is important -- -we agree fully . Our experiments with different values of lambda get partly at this . Evaluating soft consistency vs. * exact * consistency is more challenging in larger domains like Atari due to runtime bottlenecks ( large linear programs for linear approximators and solving NP-hard classification problems for DNNs ) . But we can do so on smaller toy domains ( see next paragraph ) . We do have results on the simple MDP of Lu et al.with a simplified ConQUR algorithm ( with exact consistency checking ) and can include these in the paper ( we will do so in an appendix ) . Your suggestion to test how ( different degrees of ) soft consistency impact the final result vis-a-vis exact consistency is a very nice one , and we will test and explicate this on the simple MDP of Lu et al , or another small example ."}, "1": {"review_id": "ryg8WJSKPr-1", "review_text": "This paper presents a solution to tackling the problem of delusional bias in Deep Q-learning, building upon Lu et.al. (NeuRIPS 2018). Delusional bias arises because independently choosing maximizing actions at a state may be inconsistent as the backed-up values may not be realizable by any policy. They encourage non-delusional Q-functions by adding a penalty term that enforces that the max_a in Q-learning chooses actions that do not give rise to actions outside the realizable policy class. Further, in order to keep track of all consistent assignments, they pose a search problem and propose heuristics to approximately perform this search. The heuristics are based on sampling using exponentiated Q-values and scoring possible children using scores like Bellman error, and returns of the greedy policy. Their final algorithm is evaluated on a DQN and DDQN, where they observe some improvement from both components (consistency penalty and approximate search). I would lean towards being slightly negative towards accepting this paper. However, I am not sure if the paper provides enough evidence that delusional bias is a very relevant problem with DQNs, when using high-capacity neural net approximators. Further, would the problem go away, if we perform policy iteration, in the sense of performing policy iteration instead of max Q-learning (atleast in practice)? Maybe, the paper benefits with some evidence answering this question. To summarize, I am mainly concerned about the marginal benefit at the cost of added complexity and computation for this paper. I would appreciate more evidence justifying the significance of this problem in practice. Another comment about experiments is that the paper uses pre-trained DQN for the ConQur results, where only the last linear layer of the Q-network is trained with ConQur. I think this setting might hide some properties which arise through the learning process without initial pre-training, which might be more interesting. Also, how would other auxilliary losses compare in practice, for example, losses explored in the Reinforcement Learning with Auxilliary Tasks (Jaderberg et.al.) paper? ", "rating": "3: Weak Reject", "reply_text": "Thank you for the constructive feedback and for raising some important questions . Some brief responses to specific points/questions you raise . [ DOES DELUSION ARISE IN PRACTICE ? ] The purpose of the experiments is to show that mitigating delusional bias , even with high-capacity NNs , can offer improvements . We believe the experiments show that delusional bias does occur in practice since the pre-trained Q-regressors upon which we improve are Dopamine-trained DQNs/DDQNs . Our methods differ only from ( say ) DQN in the use of the soft-consistency penalty ( plus the use of search to explore multiple assignments against which to apply this penalty ) . We claim that this tackles only \u201c policy inconsistency \u201d ( i.e. , delusion ) . Because we obtain improvements over the pre-trained DQNs , our conclusion is that delusion does , indeed , arise in practice . In retrospect , we should have made this important point much more explicit in the paper -- -our apologies for not doing so originally -- -and we will do so in revision . Our experiments , we believe , do not demonstrate the full power of removing delusion , since we only retrain the last FC layer of the pre-trained DQN , which in fact limits our performance opportunities vs. training on all layers -- -this alone results in significantly better greedy policies in many instances . [ WILL DELUSION ARISE IN POLICY ITERATION ? ] This is a good point , and while it may depend on the implementation , generally policy iteration will not have delusional bias . However , our contribution is focused on improving \u201c pure \u201d value-based methods like Q-learning ( and related methods like DDQN ) . These are widely used algorithms , that researchers and practitioners often have strong reasons to use -- -our focus is to mitigate delusional bias to extract maximum value from such methods whenever they are used . [ WHY USE PRE-TRAINED NETWORKS ] ? The rationale for improving with pre-trained DQNs is three-fold . First , it demonstrates that delusion actually causes problems in practice ( as discussed above , we will articulate this point much more explicitly in revision ) . In some sense , by freezing the feature representation learned by DQN , and demonstrating that a \u201c linear \u201d value function over those same features can be trained in ( partially ) non-delusional fashion to extract improvements gives more of a focus on non-delusional training ( as opposed to novel \u201c feature discovery \u201d ) . The second reason is a practical one -- -it allowed us to scale our experiments to cover a range of hyperparameters and run the entire Atari suite ( rather than selecting just a few high-performing games ) . We completely agree with your broader point about experimenting with our methods with full network training ( i.e. , from scratch ) to understand their performance . In some sense , this paper provides a ( we hope , compelling ) first exploration of these ideas . Third , from a practical point of view , this \u201c linear tuning \u201d approach offers a relatively inexpensive alternative to extract improvements from a model learned using classic techniques ( e.g. , linear tuning requires many fewer training samples ) . We also note that , if the full application of ConQUR is too expensive in some settings , adding a our simple consistency penalty can sometimes provide a lift ( and rarely hurts ) , as shown by the experiments in Section 4.1 . This requires no major changes to standard DQN , DDQN or the like and adds no significant implementation complexity or computational cost . [ AUXILIARY LOSSES ] Thanks for making the reference to auxiliary losses , this is an interesting question . While our penalty term focuses on consistency for a single task and auxiliary losses help accelerate learning for the main task , we can imagine applying a consistency penalty for each auxiliary objective ( in addition to minimizing task \u2019 s Bellman error ) . This direction is interesting to explore , which we will cite in the revised paper ."}, "2": {"review_id": "ryg8WJSKPr-2", "review_text": "This paper focuses on addressing the delusional bias problem in deep Q-learning, and propose a general framework (ConQUR) for integrating policy-consistent backups with regression-based function approximation for Q-learning and for managing the search through the space of possible regressors. Specifically, it proposes a soft consistency penalty to alleviate the delusional bias problem while avoiding the expensive exact consistency testing. This penalty encourages the parameters of the model to satisfy the consistency condition when solving the MSBE problem. Pros: The soft penalization itself is already shown to be effective in further improving any regression-based Q-learning algorithms (e.g., DQN and DDQN). When combining the soft consistency penalty and the search procedure, it is shown to significantly outperform the DQN and DDQN baselines, which is impressive. This work presents novel idea and solid supporting experiments. Cons: The major experimental results of the paper are in Table 4 of Appendix D. This is not a good effort to save space by moving the most important results into appendix. At least a selected set of results should be presented in the main paper rather than in appendices. One alternative approach is the authors plot a bar figure to demonstrate the performance of different algorithms on different Atari games. Other comments: \u2022 Fig. 2 two is a bit hard to read due to too many curves. \u2022 Standard baseline results other than DQN and DDQN should also be listed, in order to demonstrate that solving delusional bias could make Q-learning more competitive than alternatives (e.g., A3C, PPO, TRPO). ", "rating": "6: Weak Accept", "reply_text": "Thank you for the constructive feedback . Some brief responses : We will try to fit in the results of Table 4 into the main text -- -if this is not feasible due to space constraints , we \u2019 ll include a comprehensive summary as you suggest . With respect to other algorithms ( A3C , PPO , TRPO , etc . ) we will add a brief comparison to such algorithms in the literature , thanks for the suggestion . While we don \u2019 t make a broad claim that removing delusion will make Q-learning competitive universally -- -we believe some domains may be better suited to policy-based or actor-critic-style algorithms -- -it does in a number of the cases examined here . However , there are many instances where Q-learning is desirable for other reasons , and our primary aim is to try to extract maximum performance from Q-learning itself . We will make this part of our motivation more explicit . We will make Fig.2 more readable -- -apologies for that !"}}