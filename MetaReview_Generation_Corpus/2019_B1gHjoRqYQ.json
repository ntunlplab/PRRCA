{"year": "2019", "forum": "B1gHjoRqYQ", "title": "An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack", "decision": "Reject", "meta_review": "The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods. The authors provide theoretical guarantees and a broad set of experiments. \n\nIn the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches. The authors mention that the lines are too thick to reveal the difference. It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods. AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side. \n\nThe comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion. Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence. \n\nThe referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold.  \n\nAlthough the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time. ", "reviews": [{"review_id": "B1gHjoRqYQ-0", "review_text": "i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted. The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible. This is clearly an intractable problem. Thus all attacks make some kind of approximation, including this paper. I am still a bit confused about the difference between \"zero-confidence attacks\" and those that don't fall into that category such as PGD. Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help. The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced. The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case. What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized. This was not done. This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations. Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation. Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack. So clarifying the above question will help to judge the paper's novelty. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack . https : //docs.google.com/viewer ? url=https : //raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf As can be seen , the zero-confidence attack finds the closest point on the decision boundary ; while fix perturbation-attack finds adversarial samples within a fix perturbation . Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level . However , we will be better off with zero-confidence attacks if we want to 1 ) Compute the margin of each individual example ; and 2 ) Probe and study the decision boundary of a classifier Of course , we can also measure the margin of each example using a fix-perturbation attack , for example PGD , by binary searching over the perturbation levels . However , the computation cost will significantly increase . Consider , for example , the CIFAR-10 dataset . Since for our model , most margins fall within 10 , so let \u2019 s assume the binary search range is 10 ( for adversarially trained models this number will be much higher ) . If we want to achieve a accuracy of 0.1 , then we need at least 7 binary search steps . In other words , the computation complexity increases by 7 times . In fact , CW applies a similar binary search idea to achieve zero-confidence attack , and that is why its computation cost is high . The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently , but it at least provides a perspective on why zero-confidence attack is challenging , and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable ."}, {"review_id": "B1gHjoRqYQ-1", "review_text": "This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. Under a set of conditions, the authors proved convergence of the proposed attack algorithm. My main concern about this paper is why this algorithm has a better performance than CW attack? I would suggest comparing with CW attack under different sets of hyper-parameters. Minor comment: The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding your first concern on the comparison with CW : In short , MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time . The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference . To show this point clearly , we would like to refer you to the results in our response to reviewer 3 , where we scanned through the number of binary search steps and measure the success rate and running time . As can be seen , MarginAttack has a higher success rate than all the versions of CW . There is a success-rate-efficiency tradeoff in CW , as a smaller binary search step number leads to a lower success rate . However , even with 10 binary search steps , CW is still unable to outperform MarginAttack in terms of success rate . On the other hand , with very small numbers of binary search steps , CW still runs slower than MarginAttack . Hope these results will clarify your major concern . Regarding your minor concern : In the theorem , we did not assume convexity . The assumption with the name 'convexity ' is saying that the constraint set should not be 'too concave ' . Please check the following figure where we listed what decision boundaries are permitted by our theorem and what not . https : //docs.google.com/viewer ? url=https : //raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf As can be seen , the convexity assumption permits a wide variety of decision boundaries . Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does . In this case , the critical point becomes a local maximum rather than a local minimum ."}, {"review_id": "B1gHjoRqYQ-2", "review_text": "The authors propose a new method for constructing adversarial examples called MarginAttack. The method is inspired by Rosen's algorithm, a classical algorithm in constrained optimization. At its core, Rosen's algorithm (instantiated for adversarial examples) alternates between moving towards the set of misclassified points and moving towards the original data point (while ensuring that we do not move too far away from the set of misclassified points). The authors provide theoretical guarantees (local convergence) and a broad set of experiments. The experiments show that MarginAttack finds adversarial examples with small distortion (as good as the baselines or slightly better), and that the algorithm runs faster than the Carlini-Wagner (CW) baseline (but slower than other methods). The authors make a distinction between \"fixed perturbation\" attacks and \"zero confidence\" attacks. The former finds the strongest attack within a given constrained set, while the latter finds the smallest perturbation that leads to a misclassification. Method such as projected gradient descent fall into the \"fixed perturbation\" category, while MarginAttack and CW belong to the \"zero confidence\" category. The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. Indeed, their results show that MarginAttack is 3x - 5x faster than CW and sometimes achieves smaller perturbations. First of all, I would like to emphasize that the authors conducted a thorough experimental study on multiple datasets using multiple baseline algorithms. Unfortunately, the comparison to CW and PGD still leaves some questions in my opinion: - The authors state that CW does an internal binary search over the Lagrangian multiplier, and that this search goes for up to 10 steps. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters. To the best of my knowledge, the CW implementation in Cleverhans is specifically set up so that the user does not need to tune a large number of hyperparameters (the implementation accepts a running time overhead to achieve this). Since MarginAttack also contains multiple hyperparameters (see Table 4), it would be interesting to see how the running time of MarginAttack compares to that of a tuned CW implementation without the binary search. - The authors explicitly state that the step sizes for CW were tuned for best performance, but do not mention this for PGD. For a fair comparison, the step sizes used for PGD should also be (approximately) tuned. Moreover, it is not clear why PGD is only used for an l_inf comparison and not a l_2 comparison. - In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). I would be grateful if the authors could provide their view on these points. Until then, I will assign a rating of 5 since tuning the parameters of optimization algorithms is crucial for a fair comparison. Additional comments: - In the introduction, the authors equate white-box attacks with access to gradient information. But generally a white-box attack is understood as an attack that has arbitrary access to the target network. It may be helpful for the reader to clarify this. - In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. So it is not clear that there is a large gap in difficulty. Moreover, the authors state that fixed perturbation attacks often come with theoretical guarantees. But to the best of my knowledge, there is no comprehensive theory that describes when a fixed perturbation attack should be expected to succeed in attacking a commonly used neural network. - On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting. Why is that? - The authors state that JSMA (Papernot et al., 2016) is one of the earliest works that use gradient information for constructing adversarial examples. However, L-BFGS as employed by Szegedy et al., 2013 also uses gradient information. Moreover, the authors may want to cite the work of Biggio et al. from 2013 (see the survey https://arxiv.org/abs/1712.03141). - Since all distances referred to by d(x, y) seem to be norms (and the paper relies on the existence of dual norms), it may be more clear for the reader to use the norm notation || . || from the beginning.", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , regarding the white box attack definition . Yes , the white box attack is understood as having access to all network information , including structure and parameters . So it is possible to compute gradient information . Black box attacks only have access to logits or only decision , so it is not possible to accurately compute the gradient information . We will make this distinction clearer . Second , the PGD convergence guarantee we meant is only about local convergence . Under mild assumptions , PGD is able to converge to a critical point of the PGD loss function , where no feasible direction can increase the loss function . We will clarify this in our updated version . Third , by a \u2018 more realistic attack \u2019 , we meant that under a true attack setting , an attacker would not confine himself to a fix perturbation , but is more likely to keep attacking until success , while minimizing perturbation . Fourth , we will correct our statement about the earliest work that incorporates gradient information into adversarial attack . Finally , we will change the norm notation ."}], "0": {"review_id": "B1gHjoRqYQ-0", "review_text": "i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted. The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible. This is clearly an intractable problem. Thus all attacks make some kind of approximation, including this paper. I am still a bit confused about the difference between \"zero-confidence attacks\" and those that don't fall into that category such as PGD. Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help. The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced. The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case. What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized. This was not done. This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations. Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation. Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack. So clarifying the above question will help to judge the paper's novelty. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack . https : //docs.google.com/viewer ? url=https : //raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf As can be seen , the zero-confidence attack finds the closest point on the decision boundary ; while fix perturbation-attack finds adversarial samples within a fix perturbation . Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level . However , we will be better off with zero-confidence attacks if we want to 1 ) Compute the margin of each individual example ; and 2 ) Probe and study the decision boundary of a classifier Of course , we can also measure the margin of each example using a fix-perturbation attack , for example PGD , by binary searching over the perturbation levels . However , the computation cost will significantly increase . Consider , for example , the CIFAR-10 dataset . Since for our model , most margins fall within 10 , so let \u2019 s assume the binary search range is 10 ( for adversarially trained models this number will be much higher ) . If we want to achieve a accuracy of 0.1 , then we need at least 7 binary search steps . In other words , the computation complexity increases by 7 times . In fact , CW applies a similar binary search idea to achieve zero-confidence attack , and that is why its computation cost is high . The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently , but it at least provides a perspective on why zero-confidence attack is challenging , and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable ."}, "1": {"review_id": "B1gHjoRqYQ-1", "review_text": "This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. Under a set of conditions, the authors proved convergence of the proposed attack algorithm. My main concern about this paper is why this algorithm has a better performance than CW attack? I would suggest comparing with CW attack under different sets of hyper-parameters. Minor comment: The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding your first concern on the comparison with CW : In short , MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time . The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference . To show this point clearly , we would like to refer you to the results in our response to reviewer 3 , where we scanned through the number of binary search steps and measure the success rate and running time . As can be seen , MarginAttack has a higher success rate than all the versions of CW . There is a success-rate-efficiency tradeoff in CW , as a smaller binary search step number leads to a lower success rate . However , even with 10 binary search steps , CW is still unable to outperform MarginAttack in terms of success rate . On the other hand , with very small numbers of binary search steps , CW still runs slower than MarginAttack . Hope these results will clarify your major concern . Regarding your minor concern : In the theorem , we did not assume convexity . The assumption with the name 'convexity ' is saying that the constraint set should not be 'too concave ' . Please check the following figure where we listed what decision boundaries are permitted by our theorem and what not . https : //docs.google.com/viewer ? url=https : //raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf As can be seen , the convexity assumption permits a wide variety of decision boundaries . Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does . In this case , the critical point becomes a local maximum rather than a local minimum ."}, "2": {"review_id": "B1gHjoRqYQ-2", "review_text": "The authors propose a new method for constructing adversarial examples called MarginAttack. The method is inspired by Rosen's algorithm, a classical algorithm in constrained optimization. At its core, Rosen's algorithm (instantiated for adversarial examples) alternates between moving towards the set of misclassified points and moving towards the original data point (while ensuring that we do not move too far away from the set of misclassified points). The authors provide theoretical guarantees (local convergence) and a broad set of experiments. The experiments show that MarginAttack finds adversarial examples with small distortion (as good as the baselines or slightly better), and that the algorithm runs faster than the Carlini-Wagner (CW) baseline (but slower than other methods). The authors make a distinction between \"fixed perturbation\" attacks and \"zero confidence\" attacks. The former finds the strongest attack within a given constrained set, while the latter finds the smallest perturbation that leads to a misclassification. Method such as projected gradient descent fall into the \"fixed perturbation\" category, while MarginAttack and CW belong to the \"zero confidence\" category. The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. Indeed, their results show that MarginAttack is 3x - 5x faster than CW and sometimes achieves smaller perturbations. First of all, I would like to emphasize that the authors conducted a thorough experimental study on multiple datasets using multiple baseline algorithms. Unfortunately, the comparison to CW and PGD still leaves some questions in my opinion: - The authors state that CW does an internal binary search over the Lagrangian multiplier, and that this search goes for up to 10 steps. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters. To the best of my knowledge, the CW implementation in Cleverhans is specifically set up so that the user does not need to tune a large number of hyperparameters (the implementation accepts a running time overhead to achieve this). Since MarginAttack also contains multiple hyperparameters (see Table 4), it would be interesting to see how the running time of MarginAttack compares to that of a tuned CW implementation without the binary search. - The authors explicitly state that the step sizes for CW were tuned for best performance, but do not mention this for PGD. For a fair comparison, the step sizes used for PGD should also be (approximately) tuned. Moreover, it is not clear why PGD is only used for an l_inf comparison and not a l_2 comparison. - In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above). I would be grateful if the authors could provide their view on these points. Until then, I will assign a rating of 5 since tuning the parameters of optimization algorithms is crucial for a fair comparison. Additional comments: - In the introduction, the authors equate white-box attacks with access to gradient information. But generally a white-box attack is understood as an attack that has arbitrary access to the target network. It may be helpful for the reader to clarify this. - In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. So it is not clear that there is a large gap in difficulty. Moreover, the authors state that fixed perturbation attacks often come with theoretical guarantees. But to the best of my knowledge, there is no comprehensive theory that describes when a fixed perturbation attack should be expected to succeed in attacking a commonly used neural network. - On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting. Why is that? - The authors state that JSMA (Papernot et al., 2016) is one of the earliest works that use gradient information for constructing adversarial examples. However, L-BFGS as employed by Szegedy et al., 2013 also uses gradient information. Moreover, the authors may want to cite the work of Biggio et al. from 2013 (see the survey https://arxiv.org/abs/1712.03141). - Since all distances referred to by d(x, y) seem to be norms (and the paper relies on the existence of dual norms), it may be more clear for the reader to use the norm notation || . || from the beginning.", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , regarding the white box attack definition . Yes , the white box attack is understood as having access to all network information , including structure and parameters . So it is possible to compute gradient information . Black box attacks only have access to logits or only decision , so it is not possible to accurately compute the gradient information . We will make this distinction clearer . Second , the PGD convergence guarantee we meant is only about local convergence . Under mild assumptions , PGD is able to converge to a critical point of the PGD loss function , where no feasible direction can increase the loss function . We will clarify this in our updated version . Third , by a \u2018 more realistic attack \u2019 , we meant that under a true attack setting , an attacker would not confine himself to a fix perturbation , but is more likely to keep attacking until success , while minimizing perturbation . Fourth , we will correct our statement about the earliest work that incorporates gradient information into adversarial attack . Finally , we will change the norm notation ."}}