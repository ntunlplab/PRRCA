{"year": "2020", "forum": "H1x-pANtDB", "title": "A closer look at network resolution for efficient network design", "decision": "Reject", "meta_review": "Main content:new training regime for multi-resolution slimmable networks. \n\nDiscussion:\nreviewer 4: believes the main contribution of mutual learning from width and resolution is a bit weak\nreviewer 1: incremental work, details/baselines missing in experimental section\nreviewer 2: (least detailed): well-written with good results\nRecommendation: I agree with reviewer 1, 4 that the experimental section could be improved. Leaning to reject. \n\n", "reviews": [{"review_id": "H1x-pANtDB-0", "review_text": "The authors propose a new training regime for multi-resolution slimmable networks. Their approach is based on US-Net training technique but in addition to sampling from different network widths they also sample from different input resolutions and show that using multi-scale inputs improves the top-1 accuracy on ImageNet comparing to US-Net or MobileNet v1/v2 within the same resource constraints. Pros: + The authors correctly identify input resolution as one of the aspects of lightweight network design that is often overlooked + They propose a practically viable training scheme that can be used to train & select networks given resource constraints + The paper is well written and includes many insightful experimental findings Con: The authors specify the mutual learning from width and resolution as their main contribution. They insist that treating input resolution independently from network structure is what distinguishes previous work from the newly suggested technique. But the paper doesn't include extensive experimental comparisons with the approaches that treat input resolution independently. Thus its claim that joint width/resolution sampling is beneficial comparing to independent approaches is somewhat unfounded. For example, the authors show that MobileNet with 1.0-224 config (no sampling from widths nor from input resolutions during training) is outperformed by their network with 1.0-224 config (which effectively samples only from input resolutions during training). This is not surprising as one can view sampling from input resolutions as an equivalent to data augmentation. The importance of data augmentation is well known, so to prove the proposed mutual learning is beneficial the authors would need to compare against the networks that were trained using this multi-scale data augmentation. Figure 5 has a similar comparison but the only multi-resolution baseline there is US-Net+ which isn't using multi-resolution images in training. The paper would greatly benefit from adding such comparisons and proving they are not marginal. On rating: I'd summarize the idea of this paper as A) US-Net + B) multi-scale data augmentation + C) selecting the best network based on both input resolution and width to achieve optimal performance within resource constraints. Although C is practical and novel contribution, it is also quite straightforward. I would like to see authors response on how their approach differs from US-Net + multi-scale data augmentation for training and how/why this works better.", "rating": "3: Weak Reject", "reply_text": "We would like to sincerely thank the reviewer for the comments . In our experiments , both US-Net and MobileNets are trained with the multi-scale data augmentation indicated by the reviewer . It is implemented by \u2018 transforms.RandomResizedCrop ( 224 , scale= ( 0.25 , 1.0 ) ) \u2019 in the codes . This means that an image of random ratio/scale ( 0.25 to 1.0 ) of the original size is cropped and then resized to a random aspect ratio ( 3/4 to 4/3 ) of the original aspect ratio . The crop is finally resized to the given size ( 224 ) . Therefore , this is a multi-scale data augmentation process and our method outperforms US-Net + multi-scale augmentation . On the other hand , our multi-resolution TRAINING is not equivalent to multi-scale data augmentation ( which can be considered as a pre-processing step ) . Our framework randomly feeds different resolution images to different sub-networks . It has the following advantages over multi-scale data augmentation . ( a ) Smaller resolutions can reduce computational cost . Therefore , we don \u2019 t have to go to an extremely small network width to meet the resource constraints ( please see the detailed explanation in the response to Reviewer 1 , Q2 ) . ( b ) Multi-resolution training helps find better width-resolution balance while US-Net trained with multi-scale data augmentation fails because there is no width-resolution learning in the multi-scale data augmentation scheme . ( c ) Since our sub-networks share weights with each other , different sub-networks can share the representations learned from different resolutions , enabling each sub-network to capture multi-scale representations as illustrated in Figure 3 . The multi-scale representation learning has been proven effective in previous works [ 1 , 2 , 3 ] and it is not contradictory or equivalent to multi-scale data augmentation . Specifically , as pointed out by the reviewer , our framework outperforms MobileNets at 1.0-224 config . As illustrated in Figure 2 , the 1.0-224 config is the full-network and is trained with the 224 resolution only , so the improvement is coming from the shared multi-scale representations with other sub-networks as discussed above . [ 1 ] Chen , Yunpeng , et al . `` Drop an octave : Reducing spatial redundancy in convolutional neural networks with octave convolution . '' arXiv preprint arXiv:1904.05049 ( 2019 ) . [ 2 ] Sun , Ke , et al . `` Deep High-Resolution Representation Learning for Human Pose Estimation . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019 . [ 3 ] Lin , Tsung-Yi , et al . `` Feature pyramid networks for object detection . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2017 ."}, {"review_id": "H1x-pANtDB-1", "review_text": "The paper explore how varying input image resolution can have a powerful effect in reducing computational complexity of Convolutional NN's to the point of outperforming state of the art efficient architectures. More in detail, the paper proposes a method of joint training of multiple resolutions networks, leveraging student/teacher/distillation from scratch. This is based on training a high resolution teacher network and a low resolution student network, as well as a number of intermediate resolutions networks sampled randomly and jointly during training. Thanks to distillation well known regularization effects, the proposed method is achieving competitive results compared to existing state of the art efficient network architectures. The authors claim, and to some extent show, that this is due to the ability of the proposed method to take into account in a optimal way multi-resolution features available in the image. The paper is well written and presented with extensive results, comparing computational complexity/accuracy curves to existing state of the art architectures, as well as results on transfer learning to show that the feature learned do indeed generalize and don't necessarily overfit to imagenet. The idea is rather simple, but the results and the execution is inspiring.", "rating": "6: Weak Accept", "reply_text": "We would like to sincerely thank you for the comments and appreciation of our work . Our work is the first to consider both network structure ( i.e. , network width ) and input resolution in a unified learning framework to achieve an adaptive network that can instantly tradeoff between accuracy and latency at run time . Since the input scale is not well explored in previous work for achieving better accuracy-efficiency tradeoffs , our work strives to close the gap between network structure and network input . The mutual learning from different network widths and input resolutions is novel and our extensive experiments demonstrate its effectiveness over different network structures , datasets as well as tasks . The generality of this framework also makes it logically ready to extend to video input and 3D neural networks , where we can leverage both spatial resolution and temporal resolution . We believe our work brings new insights for resource-adaptive network/framework design ."}, {"review_id": "H1x-pANtDB-2", "review_text": "This paper proposes a multi-resolution training scheme for a slimmable network. The proposed method provides a new regime leveraging diverse image resolutions for training sub-networks, which enables efficient network scaling. Throughout the experiments, the authors claim the proposed method shows better performance compared to US-net. Pros) (+) The idea of multi-resolution training combining to a slimmable network looks good (+) Applying a slimmable network's technique to other tasks including detection and segmentation looks good (+) The combination of multi-resolution and the slimmable network seems to be reasonable. (+) The paper is well written and looks justified well. (+) The authors provided extensive experiments. Cons) (-) There is no backups why the proposed method could outperform over US-net. (-) The proposed method is incremental and improvements are marginal. (-) Looks like there exists missing in details of the experiments. (-) The performance report of the compared methods is quite strange. Comments) - The proposed method is too straightforward, so the authors should clarify why it works over US-net. Additionally, can the authors provide advantages using a different image-scale need for training a different sub-network? - The authors should clarify the training details of US-Net used in this paper. The performance of US-Net in Figure 4 (a) looks the same as the performance of US-Net trained with [0.05, 1]x scaling in the original paper. However, in the original paper, the authors of US-net reported [0.05, 1]x scaling as the worst performance setting in the original paper. Therefore, the authors should compare their method with the best performance setting of US-Net, which is [0.25, 1]x (because the proposed method looks being used [0.25, 1]x training setting, so the comparison should be done in fair). - The scaling parameters of US-Net used in the experiments should be specified. All the results of US-Net do not contain where they come from (i.e., the training width bound in US-net). - Can the authors report the results for 0.5-224 and 0.15-224 in Figure.4(a)? Why 0.7-160 and 0.25-160 were picked? - In Table 1, the performance of EfficientNet is weird. EfficientNet-B2 has 79.8% accuracy with 1.0B FLOPs, but the reported performance in this paper of EfficientNet has 75.6% accuracy with 2.3B FLOPs. Please clarify this. - How much does KLdiv contribute to the overall performance? - All the tables are not clearly shown. Please reattach all the tables for better readability. About rating) I think the idea looks novel, but the method is quite straightforward, and the paper does not incorporate any analysis as a backup for the proposed method. The initial rating is towards reject, but I would like to see the authors' response and the other reviewers' comments. After that, the final rating might be changed.", "rating": "3: Weak Reject", "reply_text": "We would like to sincerely thank you for the detailed comments . We addressed each question as follows . We reorganize the order of these questions for better explanations . Q1.The scaling parameters of US-Net used in the experiments should be specified . A : We apologize for missing some training details in the paper . In the experiments of ImageNet classification , for MobileNet v1 backbone , our method uses width scale from [ 0.25 , 1 ] and resolutions from { 224 , 192 , 160 , 128 } while US-Net uses width scale from [ 0.05 , 1 ] . For MobileNet v2 backbone , our method uses width scale from [ 0.7 , 1 ] and resolutions from { 224 , 192 , 160 , 128 } while US-Net uses width scale from [ 0.35 , 1 ] . In transfer learning experiments , both our method and US-Net use width scale from [ 0.25 , 1 ] . Specifically , for US-Net , we adopt the officially released model on width range [ 0.25 , 1 ] and finetune it with width range [ 0.25 , 1 ] . In object detection and segmentation , we also use the same width range [ 0.25 ,1 ] for our method and US-Net . We add these details in Appendix 1 in the updated version . Q2.The authors should compare their method with the best performance setting of US-Net , which is [ 0.25 , 1 ] x . A : We would like to clarify that our experimental settings are fair . Given that both our approach and US-Net aim to train an adaptive network to meet the dynamic resource constraints at test time . We compared our approach and US-Net using the Accuracy-FLOPs curves under the same dynamic resource constraint . For MobileNet v1 backbone ( Figure 4 ( a ) ) , the dynamic FLOPs constraint is [ 13 , 569 ] MFLOPs . To meet this constraint , US-Net needs the width scale of [ 0.05 , 1 ] x . However , our approach can meet this computation constraint by balancing between width scale [ 0.25 , 1 ] x and resolutions { 224 , 192 , 160 , 128 } . In other words , by considering input resolution , we can set the training network width scale to [ 0.25 , 1 ] x ( no need to go low as 0.05x of width ) in order to meet the resource constraint ( i.e. , [ 13 , 569 ] MFLOPs ) . Similarly , for MobileNet v2 ( Figure 4 ( b ) ) , the dynamic constraint is [ 59 , 300 ] MFLOPs . US-Net needs width scale of [ 0.35 , 1 ] x , while we can meet this constraint by combining width [ 0.7 , 1 ] x and resolutions { 224 , 192 , 160 , 128 } . Therefore , integrating resolution in our framework gives more flexibility to balance network width . This is one advantage that we illustrated in Figure 1 . In our experiments , we also compared to US-MobileNet v1 trained with width [ 0.25 , 1 ] x . We even made one step further , that is to compare to our proposed US-Net+ as illustrated in Figure 5 . US-Net+ is applying different resolutions { 224 , 192 , 160 , 128 } to US-Net $ _ { [ 0.25 , 1 ] } $ during test time . We choose the best-performing width-resolution configurations under different FLOPs to get its Accuracy-FLOPs curve , which is the upper bound of US-Net $ _ { [ 0.25 , 1 ] } $ . As demonstrated in Figure 5 , our method outperforms US-Net+ because US-Net+ can not find the optimal width-resolution balance due to lack of multi-resolution learning . Our method also outperforms US-Net $ _ { [ 0.25 , 1 ] } $ and achieves wider dynamic constraints . Besides , as clarified in Q1 , for all the experiments in transfer learning , object detection and segmentation , both our method and US-Net use the same width scale [ 0.25 , 1 ] . This setting is actually in favor of US-Net , because if the original US-Net $ _ { [ 0.05 , 1 ] } $ is used , which meets the same resource constraints as ours , it performs worse . Q3.The proposed method is too straightforward , so the authors should clarify why it works over US-net . Additionally , can the authors provide advantages using a different image-scale need for training a different sub-network ? A : ( 1 ) -- about the method We would like to clarify that the proposed method is not straightforward . As discussed in Section 2 , most previous works focus on reducing computational cost only from the structure level , ignoring the importance of input scale for achieving better accuracy-efficiency tradeoffs . Recently , EfficientNet [ 1 ] points out that balancing between network depth , width and resolution can lead to better performance , but it only considers network depth , width and resolution as independent factors by simply searching over different depth-width-resolution configurations . In our work , we try to bridge the gap between network structure and network input . In the paper , we first shed light on the promising advantages of taking input resolution into account for achieving better accuracy-efficiency in Section 2 . In light of this , we proposed a unified framework to learn from different network widths and input resolutions jointly , i.e. , deep mutual learning in our framework . As illustrated in Figure 5 , trivially applying different resolutions during test time can not achieve the optimal tradeoffs , while our joint training framework consistently improves the performance on different backbones , datasets and tasks . ( continued in part 2 )"}], "0": {"review_id": "H1x-pANtDB-0", "review_text": "The authors propose a new training regime for multi-resolution slimmable networks. Their approach is based on US-Net training technique but in addition to sampling from different network widths they also sample from different input resolutions and show that using multi-scale inputs improves the top-1 accuracy on ImageNet comparing to US-Net or MobileNet v1/v2 within the same resource constraints. Pros: + The authors correctly identify input resolution as one of the aspects of lightweight network design that is often overlooked + They propose a practically viable training scheme that can be used to train & select networks given resource constraints + The paper is well written and includes many insightful experimental findings Con: The authors specify the mutual learning from width and resolution as their main contribution. They insist that treating input resolution independently from network structure is what distinguishes previous work from the newly suggested technique. But the paper doesn't include extensive experimental comparisons with the approaches that treat input resolution independently. Thus its claim that joint width/resolution sampling is beneficial comparing to independent approaches is somewhat unfounded. For example, the authors show that MobileNet with 1.0-224 config (no sampling from widths nor from input resolutions during training) is outperformed by their network with 1.0-224 config (which effectively samples only from input resolutions during training). This is not surprising as one can view sampling from input resolutions as an equivalent to data augmentation. The importance of data augmentation is well known, so to prove the proposed mutual learning is beneficial the authors would need to compare against the networks that were trained using this multi-scale data augmentation. Figure 5 has a similar comparison but the only multi-resolution baseline there is US-Net+ which isn't using multi-resolution images in training. The paper would greatly benefit from adding such comparisons and proving they are not marginal. On rating: I'd summarize the idea of this paper as A) US-Net + B) multi-scale data augmentation + C) selecting the best network based on both input resolution and width to achieve optimal performance within resource constraints. Although C is practical and novel contribution, it is also quite straightforward. I would like to see authors response on how their approach differs from US-Net + multi-scale data augmentation for training and how/why this works better.", "rating": "3: Weak Reject", "reply_text": "We would like to sincerely thank the reviewer for the comments . In our experiments , both US-Net and MobileNets are trained with the multi-scale data augmentation indicated by the reviewer . It is implemented by \u2018 transforms.RandomResizedCrop ( 224 , scale= ( 0.25 , 1.0 ) ) \u2019 in the codes . This means that an image of random ratio/scale ( 0.25 to 1.0 ) of the original size is cropped and then resized to a random aspect ratio ( 3/4 to 4/3 ) of the original aspect ratio . The crop is finally resized to the given size ( 224 ) . Therefore , this is a multi-scale data augmentation process and our method outperforms US-Net + multi-scale augmentation . On the other hand , our multi-resolution TRAINING is not equivalent to multi-scale data augmentation ( which can be considered as a pre-processing step ) . Our framework randomly feeds different resolution images to different sub-networks . It has the following advantages over multi-scale data augmentation . ( a ) Smaller resolutions can reduce computational cost . Therefore , we don \u2019 t have to go to an extremely small network width to meet the resource constraints ( please see the detailed explanation in the response to Reviewer 1 , Q2 ) . ( b ) Multi-resolution training helps find better width-resolution balance while US-Net trained with multi-scale data augmentation fails because there is no width-resolution learning in the multi-scale data augmentation scheme . ( c ) Since our sub-networks share weights with each other , different sub-networks can share the representations learned from different resolutions , enabling each sub-network to capture multi-scale representations as illustrated in Figure 3 . The multi-scale representation learning has been proven effective in previous works [ 1 , 2 , 3 ] and it is not contradictory or equivalent to multi-scale data augmentation . Specifically , as pointed out by the reviewer , our framework outperforms MobileNets at 1.0-224 config . As illustrated in Figure 2 , the 1.0-224 config is the full-network and is trained with the 224 resolution only , so the improvement is coming from the shared multi-scale representations with other sub-networks as discussed above . [ 1 ] Chen , Yunpeng , et al . `` Drop an octave : Reducing spatial redundancy in convolutional neural networks with octave convolution . '' arXiv preprint arXiv:1904.05049 ( 2019 ) . [ 2 ] Sun , Ke , et al . `` Deep High-Resolution Representation Learning for Human Pose Estimation . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019 . [ 3 ] Lin , Tsung-Yi , et al . `` Feature pyramid networks for object detection . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2017 ."}, "1": {"review_id": "H1x-pANtDB-1", "review_text": "The paper explore how varying input image resolution can have a powerful effect in reducing computational complexity of Convolutional NN's to the point of outperforming state of the art efficient architectures. More in detail, the paper proposes a method of joint training of multiple resolutions networks, leveraging student/teacher/distillation from scratch. This is based on training a high resolution teacher network and a low resolution student network, as well as a number of intermediate resolutions networks sampled randomly and jointly during training. Thanks to distillation well known regularization effects, the proposed method is achieving competitive results compared to existing state of the art efficient network architectures. The authors claim, and to some extent show, that this is due to the ability of the proposed method to take into account in a optimal way multi-resolution features available in the image. The paper is well written and presented with extensive results, comparing computational complexity/accuracy curves to existing state of the art architectures, as well as results on transfer learning to show that the feature learned do indeed generalize and don't necessarily overfit to imagenet. The idea is rather simple, but the results and the execution is inspiring.", "rating": "6: Weak Accept", "reply_text": "We would like to sincerely thank you for the comments and appreciation of our work . Our work is the first to consider both network structure ( i.e. , network width ) and input resolution in a unified learning framework to achieve an adaptive network that can instantly tradeoff between accuracy and latency at run time . Since the input scale is not well explored in previous work for achieving better accuracy-efficiency tradeoffs , our work strives to close the gap between network structure and network input . The mutual learning from different network widths and input resolutions is novel and our extensive experiments demonstrate its effectiveness over different network structures , datasets as well as tasks . The generality of this framework also makes it logically ready to extend to video input and 3D neural networks , where we can leverage both spatial resolution and temporal resolution . We believe our work brings new insights for resource-adaptive network/framework design ."}, "2": {"review_id": "H1x-pANtDB-2", "review_text": "This paper proposes a multi-resolution training scheme for a slimmable network. The proposed method provides a new regime leveraging diverse image resolutions for training sub-networks, which enables efficient network scaling. Throughout the experiments, the authors claim the proposed method shows better performance compared to US-net. Pros) (+) The idea of multi-resolution training combining to a slimmable network looks good (+) Applying a slimmable network's technique to other tasks including detection and segmentation looks good (+) The combination of multi-resolution and the slimmable network seems to be reasonable. (+) The paper is well written and looks justified well. (+) The authors provided extensive experiments. Cons) (-) There is no backups why the proposed method could outperform over US-net. (-) The proposed method is incremental and improvements are marginal. (-) Looks like there exists missing in details of the experiments. (-) The performance report of the compared methods is quite strange. Comments) - The proposed method is too straightforward, so the authors should clarify why it works over US-net. Additionally, can the authors provide advantages using a different image-scale need for training a different sub-network? - The authors should clarify the training details of US-Net used in this paper. The performance of US-Net in Figure 4 (a) looks the same as the performance of US-Net trained with [0.05, 1]x scaling in the original paper. However, in the original paper, the authors of US-net reported [0.05, 1]x scaling as the worst performance setting in the original paper. Therefore, the authors should compare their method with the best performance setting of US-Net, which is [0.25, 1]x (because the proposed method looks being used [0.25, 1]x training setting, so the comparison should be done in fair). - The scaling parameters of US-Net used in the experiments should be specified. All the results of US-Net do not contain where they come from (i.e., the training width bound in US-net). - Can the authors report the results for 0.5-224 and 0.15-224 in Figure.4(a)? Why 0.7-160 and 0.25-160 were picked? - In Table 1, the performance of EfficientNet is weird. EfficientNet-B2 has 79.8% accuracy with 1.0B FLOPs, but the reported performance in this paper of EfficientNet has 75.6% accuracy with 2.3B FLOPs. Please clarify this. - How much does KLdiv contribute to the overall performance? - All the tables are not clearly shown. Please reattach all the tables for better readability. About rating) I think the idea looks novel, but the method is quite straightforward, and the paper does not incorporate any analysis as a backup for the proposed method. The initial rating is towards reject, but I would like to see the authors' response and the other reviewers' comments. After that, the final rating might be changed.", "rating": "3: Weak Reject", "reply_text": "We would like to sincerely thank you for the detailed comments . We addressed each question as follows . We reorganize the order of these questions for better explanations . Q1.The scaling parameters of US-Net used in the experiments should be specified . A : We apologize for missing some training details in the paper . In the experiments of ImageNet classification , for MobileNet v1 backbone , our method uses width scale from [ 0.25 , 1 ] and resolutions from { 224 , 192 , 160 , 128 } while US-Net uses width scale from [ 0.05 , 1 ] . For MobileNet v2 backbone , our method uses width scale from [ 0.7 , 1 ] and resolutions from { 224 , 192 , 160 , 128 } while US-Net uses width scale from [ 0.35 , 1 ] . In transfer learning experiments , both our method and US-Net use width scale from [ 0.25 , 1 ] . Specifically , for US-Net , we adopt the officially released model on width range [ 0.25 , 1 ] and finetune it with width range [ 0.25 , 1 ] . In object detection and segmentation , we also use the same width range [ 0.25 ,1 ] for our method and US-Net . We add these details in Appendix 1 in the updated version . Q2.The authors should compare their method with the best performance setting of US-Net , which is [ 0.25 , 1 ] x . A : We would like to clarify that our experimental settings are fair . Given that both our approach and US-Net aim to train an adaptive network to meet the dynamic resource constraints at test time . We compared our approach and US-Net using the Accuracy-FLOPs curves under the same dynamic resource constraint . For MobileNet v1 backbone ( Figure 4 ( a ) ) , the dynamic FLOPs constraint is [ 13 , 569 ] MFLOPs . To meet this constraint , US-Net needs the width scale of [ 0.05 , 1 ] x . However , our approach can meet this computation constraint by balancing between width scale [ 0.25 , 1 ] x and resolutions { 224 , 192 , 160 , 128 } . In other words , by considering input resolution , we can set the training network width scale to [ 0.25 , 1 ] x ( no need to go low as 0.05x of width ) in order to meet the resource constraint ( i.e. , [ 13 , 569 ] MFLOPs ) . Similarly , for MobileNet v2 ( Figure 4 ( b ) ) , the dynamic constraint is [ 59 , 300 ] MFLOPs . US-Net needs width scale of [ 0.35 , 1 ] x , while we can meet this constraint by combining width [ 0.7 , 1 ] x and resolutions { 224 , 192 , 160 , 128 } . Therefore , integrating resolution in our framework gives more flexibility to balance network width . This is one advantage that we illustrated in Figure 1 . In our experiments , we also compared to US-MobileNet v1 trained with width [ 0.25 , 1 ] x . We even made one step further , that is to compare to our proposed US-Net+ as illustrated in Figure 5 . US-Net+ is applying different resolutions { 224 , 192 , 160 , 128 } to US-Net $ _ { [ 0.25 , 1 ] } $ during test time . We choose the best-performing width-resolution configurations under different FLOPs to get its Accuracy-FLOPs curve , which is the upper bound of US-Net $ _ { [ 0.25 , 1 ] } $ . As demonstrated in Figure 5 , our method outperforms US-Net+ because US-Net+ can not find the optimal width-resolution balance due to lack of multi-resolution learning . Our method also outperforms US-Net $ _ { [ 0.25 , 1 ] } $ and achieves wider dynamic constraints . Besides , as clarified in Q1 , for all the experiments in transfer learning , object detection and segmentation , both our method and US-Net use the same width scale [ 0.25 , 1 ] . This setting is actually in favor of US-Net , because if the original US-Net $ _ { [ 0.05 , 1 ] } $ is used , which meets the same resource constraints as ours , it performs worse . Q3.The proposed method is too straightforward , so the authors should clarify why it works over US-net . Additionally , can the authors provide advantages using a different image-scale need for training a different sub-network ? A : ( 1 ) -- about the method We would like to clarify that the proposed method is not straightforward . As discussed in Section 2 , most previous works focus on reducing computational cost only from the structure level , ignoring the importance of input scale for achieving better accuracy-efficiency tradeoffs . Recently , EfficientNet [ 1 ] points out that balancing between network depth , width and resolution can lead to better performance , but it only considers network depth , width and resolution as independent factors by simply searching over different depth-width-resolution configurations . In our work , we try to bridge the gap between network structure and network input . In the paper , we first shed light on the promising advantages of taking input resolution into account for achieving better accuracy-efficiency in Section 2 . In light of this , we proposed a unified framework to learn from different network widths and input resolutions jointly , i.e. , deep mutual learning in our framework . As illustrated in Figure 5 , trivially applying different resolutions during test time can not achieve the optimal tradeoffs , while our joint training framework consistently improves the performance on different backbones , datasets and tasks . ( continued in part 2 )"}}