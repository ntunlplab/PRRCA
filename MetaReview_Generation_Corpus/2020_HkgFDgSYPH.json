{"year": "2020", "forum": "HkgFDgSYPH", "title": "Adaptive Online Planning for Continual Lifelong Learning", "decision": "Reject", "meta_review": "A new setting for lifelong learning is analyzed and a new method, AOP, is introduced, which combines a model-free with a model-based  approach to deal with this setting.\n\nWhile the idea is interesting, the main claims are insufficiently demonstrated. A theoretical justification is missing, and the experiments alone are not rigorous enough to draw strong conclusions. The three environments are rather simplistic and there are concerns about the statistical significance, for at least some of the experiments.", "reviews": [{"review_id": "HkgFDgSYPH-0", "review_text": "The authors study continual, lifelong learning. They suggest a new algorithm, named Adaptive Online Planning (AOP) that combines model-based planning with model-free learning. AOP decides how much additional planning is needed based on the uncertainty of the model-free learner and the performance of the planner. Experiments are carried out on three tasks, i.e. Hopper, Ant and a Maze. This paper should be rejected. The main reason is that the experiments were only performed for 3 different seeds and are therefore not statistically relevant (see Henderson et al. \"Deep reinforcement learning that matters.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.). Besides the issue of significance of the results section, there are other concerns. Some of them are: - Page 2: 'The dynamics model is updated immediately at world changes.' - Is this a reasonable assumption? Where does an accurate model come from? Given a perfect model, it is not surprising that a learner that is combined with such a model achieves a superior performance. - Although the authors state that the 'ability to perform well in old tasks (backward transfer)' is important, they don't explicitly show their algorithm to achieve this goal. Backwards transfer might be included into the experimental section, but I could not find a statement that addresses this explicitly. - I would like the authors to crisply define their use of the word 'online learning'. Does online learning simply mean to process each sample as it is available or does the term include real-time? - How is \\sigma_{thres} chosen? What is the influence of this parameter? - The statement that 'AOP uses only 0 - 25% of the number of timesteps as MPC-8, but achieves generally comparable or stronger performance.' is wrong (see Fig 4, d and e). This statement is especially difficult, as results are only averaged over 3 runs. There are furthermore a few minor concerns: - the interval for \\gamma should exclude 1 in this setting, as the return would otherwise be unbounded. - In the background section, the authors confuse the definition of the return with reward. - the term 'deep exploration' is used but not defined - There are two figures between the subsection header for 4.4 and the text - this is highly confusing ", "rating": "1: Reject", "reply_text": "Thank you for taking the time to read our paper and for providing feedback ! We have added some details to the paper and hope to address some of your concerns : 1 ) Significance of results/seeds : We have updated all experiments to now include five seeds , as done in Henderson et al.2018.In general , it is difficult for us to include more seeds due to computational constraints , but we hope this is satisfactory . In general , most of the AOP experiments are low-variance , as the standard deviations presented suggest . 2 ) Dynamics model : It is true that an accurate model is not a given in real-world robotics settings . However , we think the problem is still interesting . First , we would like to clarify that we compare AOP to other algorithms that also have access to an updated and correct dynamics model -- none of the algorithms discussed lack this access . Second , we believe that there are still many unsolved challenges and interesting ideas to consider , even with a perfect model . Control is still difficult , and learning control in a way that does not impact the agent \u2019 s future ability to learn is highly nontrivial . We observe that all algorithms struggle with this , even MPC , and notably PPO . If we can not first do well in this setting with access to a model , then it would be extremely difficult to do so without one . Additionally , the idea of an agent that is not only knowledgeable about how to act , but also of when to plan , is not something that has been previously explored . Finally , some of the insights from our setting extend to other settings that are not obviously directly related : for example , in multi-agent settings , policies must be learned in a continually nonstationary environment , which we observe can be difficult with traditional methods , but can possibly be improved by strong exploration techniques -- multi-agent RL in some settings is deeply concerned with control , and not as much with learning the dynamics ( in some settings they may even give agents access to other agents ) . We have further clarified some of the insights towards policy learning in a new Section 4.4 . 3 ) Backwards transfer : We have now added an experiment demonstrating backwards transfer in the changing worlds Hopper environment : we take a trained policy from the end of AOP , and then train it in standard TD3 fashion in the initial setting ( which it has not seen since the initial time ) . We compare this to a new policy ( what is was when it first saw the world ) , and show that it adapts much more quickly , demonstrating backward transfer . Furthermore , we add some more analysis on the policies in general in Section 4.4 . 4 ) Online learning : We define online learning as : for a particular timestep , the agent first trains on its own , and then is forced to make an action , before repeating for the next timestep . We have now further clarified this in our background section . 5 ) Threshold parameters : It is reasonable that any choice for \\sigma_ { thres } and \\epsilon_ { thres } is somewhat arbitrary ; for our experiments , we picked a reasonable value around levels that the ensemble typically takes on throughout training . We have added an experiment to Appendix C.1.1 consisting of a grid search over a reasonable range of choices for these hyperparameters , and show that AOP is overall not particularly sensitive to them , so it is not especially important what we pick . In general , these parameters correspond to the algorithm \u2019 s inclination to cut planning . 6 ) Clarification on comparable performance/Ant environment : This statement refers to that , across most environments , AOP generally performs well , most of the time . We have amended this statement to clarify this . We would also like to discuss the Ant environment results in particular ( old Figure 4 d & e ) : the Ant environment is particularly difficult , as most of the time the agent never gets up/takes a long time to get up after falling over , which showcases the sharp challenge of exploration in continual lifelong learning . Safe exploration is a well-studied topic , which we do not directly tackle , but is certainly an interesting problem to consider for future work in this setting . We have added more discussion on this in the \u201c Vast Worlds \u201d commentary in Section 4.3 . * We do believe it is possible to improve this performance , and will likely post an update on it later this week . 7 ) Minor concerns : These have been corrected ; in particular , we changed \u201c deep exploration \u201d to \u201c temporally extended exploration \u201d . Again , thank you for your feedback ! Please let us know if you have other concerns , or topics you would like us to address/clarify further ."}, {"review_id": "HkgFDgSYPH-1", "review_text": "The paper presents an adaptive online planning(AOP) strategy in a model-free policy setting, a reinforcement learning method aimed to solve catastrophic forgetting problem by combining model-based planning and model-free policy learning. AOP is able extensive plan only when necessary, leading to over all average reduced computation times. AOP can be easily integrated into other reinforcement learning frameworks such as to any offline-planning reinforcement learning algorithms. The experiments demonstrate that AOP is computationally efficient compared to traditional baselines MPC-8 and MPC-3 while maintaining the performances. The algorithm is developed based on heuristic solutions to address some of the fundamental problems in reinforcement learning, and although the proposed strategies definitely seem to provide some benefits in terms of computation complexity, the solution is not very elagant or noval. It is hard to justify the computational efficiency and performance in dynamically changing environments just based on the presented results. While the improvement in computation is there, what I find lacking is the experiments demonstrating clear evidence of overcoming catastrophic forgetting problem. The paper gives off a feeling that AOP as an add-on that can increase the performance of any RL algorithm. ", "rating": "6: Weak Accept", "reply_text": "Thank you for taking the time to read our paper and for providing feedback ! We have added some details to the paper and hope to address some of your concerns : 1 ) Novelty of work : Our setting of continual lifelong learning has not been studied in the past , and is a new setup for which we analyze a new algorithm and adaptations of existing algorithms on . Additionally , past work into reducing the computation of a planner has been limited . We kindly ask that you consider our work in the broader context of our setting . 2 ) Catastrophic forgetting : We have now added an experiment demonstrating backwards transfer in the changing worlds Hopper environment : we take a trained policy from the end of AOP , and then train it in standard TD3 fashion in the initial setting ( which it has not seen since the initial time ) . We compare this to a new policy ( what is was when it first saw the world ) , and show that it adapts much more quickly , demonstrating backward transfer . Furthermore , we add some more analysis on the policies in general in Section 4.4 . 3 ) Increase in performance for any RL algorithm : There are many algorithms that can be fit into the AOP framework ; however , we think that it is important to note that the goal of AOP is not directly to increase performance , but rather primarily to reduce computation , and in some cases improve exploration . Again , thank you for your feedback ! Please let us know if you have other concerns , or topics you would like us to address/clarify further ."}, {"review_id": "HkgFDgSYPH-2", "review_text": " The work is heuristically motivated by the goal of reducing the high computation of model-based learning while achieving high performance. For achieving that, the authors propose an algorithm, Adaptive Online Planning (AOP) combining a model-free policy learning method and a model-based planner. In terms of the empirical study, they test the algorithm in 3 environments, Hopper, Ant, and Maze. They compare their algorithms with several model-based methods. From my perspective, the paper has several weaknesses for which I give a weak rejection. The motivation is interesting to me, but the authors do not provide enough justification. The authors claim that the proposed method is able to reduce high computation. However, seemingly they only intuitively illustrate how it saves energy without strong proofs, which weakens the claim. What\u2019s more, the experiment is not clear to me. What are the take-aways of Figure 3 and Figure 4 while I cannot see an improvement from them? There is no comparison in Figure 6; not clear how the plots of other models look like. The last comment is about the 3 environments that are not complex enough. Minor comments: - Some typos and grammar mistakes, e.g., \u2018planing\u2019 and \u2018(d)by\u2019 in the third last line (p.4); the second sentence in Sec. conclusion (p.8).", "rating": "3: Weak Reject", "reply_text": "Thank you for taking the time to read our paper and for providing feedback ! We have added some details to the paper and hope to address some of your concerns : 1 ) Theoretical justification of algorithm and motivation : While we do not provide theoretical justification for AOP , our main contribution is the introduction of a new problem setting , and the proposal of an initial idea to tackle it . This problem has close ties to nonstationary environments , which are broadly relevant in many settings , ex . multi-agent settings , policy learning in learned dynamics , real-world robotics where resets are costly , etc . Furthermore , we identify several challenges in such a setting , and show where previous methods fail , which can lead to insights on how to improve methods more generally . We hope you will consider our contribution as whole . 2 ) Takeaways from Figures 3/4 [ now Figures A.1 and A.2 ] ( computation/rewards ) : We agree that the graphs are difficult to see information from ; we have now summarized the information compactly , moved the detailed graphs into Appendix A , and added some clarifications on takeaways from the experiments . For the particularly interesting takeaway of policy degradation , we have kept the relevant graphs and added further discussion in Section 4.4 . We hope this is now more clearly showing the reduction in computation and the strength of performance of the model-based/model-free algorithms . 3 ) Comparisons in Figure 6 [ now Figure 5 ] ( behavior of AOP ) : We dedicate Section 4.5 to discussing the specific components of the AOP algorithm , namely individual statistics ( Bellman error and standard deviation of the value ensemble , planning horizon length , planning iterations , policy usage ) that help to give a clearer picture of what the algorithm is doing at each stage of training . Therefore , we do not plot other algorithms on the same graph . Notably , uncertainty and planning decrease as the agent progresses farther in each world . 4 ) Complexity of environments : It is true that the environments themselves are not particularly complex control environments , and have been solved adequately in the past in the offline setting . However , we show that these environments become problematic for state-of-the-art algorithms ( TD3 , PPO , POLO ) when tackled in continual lifelong learning , due to the lack of ability to reset , nonstationary dynamics , etc . Therefore , we believe that they are complex enough for our investigations , and are capable of crisply showing where existing work struggles . Again , thank you for your feedback ! Please let us know if you have other concerns , or topics you would like us to address/clarify further ."}], "0": {"review_id": "HkgFDgSYPH-0", "review_text": "The authors study continual, lifelong learning. They suggest a new algorithm, named Adaptive Online Planning (AOP) that combines model-based planning with model-free learning. AOP decides how much additional planning is needed based on the uncertainty of the model-free learner and the performance of the planner. Experiments are carried out on three tasks, i.e. Hopper, Ant and a Maze. This paper should be rejected. The main reason is that the experiments were only performed for 3 different seeds and are therefore not statistically relevant (see Henderson et al. \"Deep reinforcement learning that matters.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.). Besides the issue of significance of the results section, there are other concerns. Some of them are: - Page 2: 'The dynamics model is updated immediately at world changes.' - Is this a reasonable assumption? Where does an accurate model come from? Given a perfect model, it is not surprising that a learner that is combined with such a model achieves a superior performance. - Although the authors state that the 'ability to perform well in old tasks (backward transfer)' is important, they don't explicitly show their algorithm to achieve this goal. Backwards transfer might be included into the experimental section, but I could not find a statement that addresses this explicitly. - I would like the authors to crisply define their use of the word 'online learning'. Does online learning simply mean to process each sample as it is available or does the term include real-time? - How is \\sigma_{thres} chosen? What is the influence of this parameter? - The statement that 'AOP uses only 0 - 25% of the number of timesteps as MPC-8, but achieves generally comparable or stronger performance.' is wrong (see Fig 4, d and e). This statement is especially difficult, as results are only averaged over 3 runs. There are furthermore a few minor concerns: - the interval for \\gamma should exclude 1 in this setting, as the return would otherwise be unbounded. - In the background section, the authors confuse the definition of the return with reward. - the term 'deep exploration' is used but not defined - There are two figures between the subsection header for 4.4 and the text - this is highly confusing ", "rating": "1: Reject", "reply_text": "Thank you for taking the time to read our paper and for providing feedback ! We have added some details to the paper and hope to address some of your concerns : 1 ) Significance of results/seeds : We have updated all experiments to now include five seeds , as done in Henderson et al.2018.In general , it is difficult for us to include more seeds due to computational constraints , but we hope this is satisfactory . In general , most of the AOP experiments are low-variance , as the standard deviations presented suggest . 2 ) Dynamics model : It is true that an accurate model is not a given in real-world robotics settings . However , we think the problem is still interesting . First , we would like to clarify that we compare AOP to other algorithms that also have access to an updated and correct dynamics model -- none of the algorithms discussed lack this access . Second , we believe that there are still many unsolved challenges and interesting ideas to consider , even with a perfect model . Control is still difficult , and learning control in a way that does not impact the agent \u2019 s future ability to learn is highly nontrivial . We observe that all algorithms struggle with this , even MPC , and notably PPO . If we can not first do well in this setting with access to a model , then it would be extremely difficult to do so without one . Additionally , the idea of an agent that is not only knowledgeable about how to act , but also of when to plan , is not something that has been previously explored . Finally , some of the insights from our setting extend to other settings that are not obviously directly related : for example , in multi-agent settings , policies must be learned in a continually nonstationary environment , which we observe can be difficult with traditional methods , but can possibly be improved by strong exploration techniques -- multi-agent RL in some settings is deeply concerned with control , and not as much with learning the dynamics ( in some settings they may even give agents access to other agents ) . We have further clarified some of the insights towards policy learning in a new Section 4.4 . 3 ) Backwards transfer : We have now added an experiment demonstrating backwards transfer in the changing worlds Hopper environment : we take a trained policy from the end of AOP , and then train it in standard TD3 fashion in the initial setting ( which it has not seen since the initial time ) . We compare this to a new policy ( what is was when it first saw the world ) , and show that it adapts much more quickly , demonstrating backward transfer . Furthermore , we add some more analysis on the policies in general in Section 4.4 . 4 ) Online learning : We define online learning as : for a particular timestep , the agent first trains on its own , and then is forced to make an action , before repeating for the next timestep . We have now further clarified this in our background section . 5 ) Threshold parameters : It is reasonable that any choice for \\sigma_ { thres } and \\epsilon_ { thres } is somewhat arbitrary ; for our experiments , we picked a reasonable value around levels that the ensemble typically takes on throughout training . We have added an experiment to Appendix C.1.1 consisting of a grid search over a reasonable range of choices for these hyperparameters , and show that AOP is overall not particularly sensitive to them , so it is not especially important what we pick . In general , these parameters correspond to the algorithm \u2019 s inclination to cut planning . 6 ) Clarification on comparable performance/Ant environment : This statement refers to that , across most environments , AOP generally performs well , most of the time . We have amended this statement to clarify this . We would also like to discuss the Ant environment results in particular ( old Figure 4 d & e ) : the Ant environment is particularly difficult , as most of the time the agent never gets up/takes a long time to get up after falling over , which showcases the sharp challenge of exploration in continual lifelong learning . Safe exploration is a well-studied topic , which we do not directly tackle , but is certainly an interesting problem to consider for future work in this setting . We have added more discussion on this in the \u201c Vast Worlds \u201d commentary in Section 4.3 . * We do believe it is possible to improve this performance , and will likely post an update on it later this week . 7 ) Minor concerns : These have been corrected ; in particular , we changed \u201c deep exploration \u201d to \u201c temporally extended exploration \u201d . Again , thank you for your feedback ! Please let us know if you have other concerns , or topics you would like us to address/clarify further ."}, "1": {"review_id": "HkgFDgSYPH-1", "review_text": "The paper presents an adaptive online planning(AOP) strategy in a model-free policy setting, a reinforcement learning method aimed to solve catastrophic forgetting problem by combining model-based planning and model-free policy learning. AOP is able extensive plan only when necessary, leading to over all average reduced computation times. AOP can be easily integrated into other reinforcement learning frameworks such as to any offline-planning reinforcement learning algorithms. The experiments demonstrate that AOP is computationally efficient compared to traditional baselines MPC-8 and MPC-3 while maintaining the performances. The algorithm is developed based on heuristic solutions to address some of the fundamental problems in reinforcement learning, and although the proposed strategies definitely seem to provide some benefits in terms of computation complexity, the solution is not very elagant or noval. It is hard to justify the computational efficiency and performance in dynamically changing environments just based on the presented results. While the improvement in computation is there, what I find lacking is the experiments demonstrating clear evidence of overcoming catastrophic forgetting problem. The paper gives off a feeling that AOP as an add-on that can increase the performance of any RL algorithm. ", "rating": "6: Weak Accept", "reply_text": "Thank you for taking the time to read our paper and for providing feedback ! We have added some details to the paper and hope to address some of your concerns : 1 ) Novelty of work : Our setting of continual lifelong learning has not been studied in the past , and is a new setup for which we analyze a new algorithm and adaptations of existing algorithms on . Additionally , past work into reducing the computation of a planner has been limited . We kindly ask that you consider our work in the broader context of our setting . 2 ) Catastrophic forgetting : We have now added an experiment demonstrating backwards transfer in the changing worlds Hopper environment : we take a trained policy from the end of AOP , and then train it in standard TD3 fashion in the initial setting ( which it has not seen since the initial time ) . We compare this to a new policy ( what is was when it first saw the world ) , and show that it adapts much more quickly , demonstrating backward transfer . Furthermore , we add some more analysis on the policies in general in Section 4.4 . 3 ) Increase in performance for any RL algorithm : There are many algorithms that can be fit into the AOP framework ; however , we think that it is important to note that the goal of AOP is not directly to increase performance , but rather primarily to reduce computation , and in some cases improve exploration . Again , thank you for your feedback ! Please let us know if you have other concerns , or topics you would like us to address/clarify further ."}, "2": {"review_id": "HkgFDgSYPH-2", "review_text": " The work is heuristically motivated by the goal of reducing the high computation of model-based learning while achieving high performance. For achieving that, the authors propose an algorithm, Adaptive Online Planning (AOP) combining a model-free policy learning method and a model-based planner. In terms of the empirical study, they test the algorithm in 3 environments, Hopper, Ant, and Maze. They compare their algorithms with several model-based methods. From my perspective, the paper has several weaknesses for which I give a weak rejection. The motivation is interesting to me, but the authors do not provide enough justification. The authors claim that the proposed method is able to reduce high computation. However, seemingly they only intuitively illustrate how it saves energy without strong proofs, which weakens the claim. What\u2019s more, the experiment is not clear to me. What are the take-aways of Figure 3 and Figure 4 while I cannot see an improvement from them? There is no comparison in Figure 6; not clear how the plots of other models look like. The last comment is about the 3 environments that are not complex enough. Minor comments: - Some typos and grammar mistakes, e.g., \u2018planing\u2019 and \u2018(d)by\u2019 in the third last line (p.4); the second sentence in Sec. conclusion (p.8).", "rating": "3: Weak Reject", "reply_text": "Thank you for taking the time to read our paper and for providing feedback ! We have added some details to the paper and hope to address some of your concerns : 1 ) Theoretical justification of algorithm and motivation : While we do not provide theoretical justification for AOP , our main contribution is the introduction of a new problem setting , and the proposal of an initial idea to tackle it . This problem has close ties to nonstationary environments , which are broadly relevant in many settings , ex . multi-agent settings , policy learning in learned dynamics , real-world robotics where resets are costly , etc . Furthermore , we identify several challenges in such a setting , and show where previous methods fail , which can lead to insights on how to improve methods more generally . We hope you will consider our contribution as whole . 2 ) Takeaways from Figures 3/4 [ now Figures A.1 and A.2 ] ( computation/rewards ) : We agree that the graphs are difficult to see information from ; we have now summarized the information compactly , moved the detailed graphs into Appendix A , and added some clarifications on takeaways from the experiments . For the particularly interesting takeaway of policy degradation , we have kept the relevant graphs and added further discussion in Section 4.4 . We hope this is now more clearly showing the reduction in computation and the strength of performance of the model-based/model-free algorithms . 3 ) Comparisons in Figure 6 [ now Figure 5 ] ( behavior of AOP ) : We dedicate Section 4.5 to discussing the specific components of the AOP algorithm , namely individual statistics ( Bellman error and standard deviation of the value ensemble , planning horizon length , planning iterations , policy usage ) that help to give a clearer picture of what the algorithm is doing at each stage of training . Therefore , we do not plot other algorithms on the same graph . Notably , uncertainty and planning decrease as the agent progresses farther in each world . 4 ) Complexity of environments : It is true that the environments themselves are not particularly complex control environments , and have been solved adequately in the past in the offline setting . However , we show that these environments become problematic for state-of-the-art algorithms ( TD3 , PPO , POLO ) when tackled in continual lifelong learning , due to the lack of ability to reset , nonstationary dynamics , etc . Therefore , we believe that they are complex enough for our investigations , and are capable of crisply showing where existing work struggles . Again , thank you for your feedback ! Please let us know if you have other concerns , or topics you would like us to address/clarify further ."}}