{"year": "2020", "forum": "SklD9yrFPS", "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "decision": "Accept (Spotlight)", "meta_review": "This paper presents a software library for dealing with neural networks either in the (usual) finite limit or in the infinite limit. The latter is obtained by using the Neural Tangent Kernel theory. \n\nThere is variance in the reviewers' scores, however there has also been quite a lot of discussion, which has been facilitated by the authors' elaborate rebuttal. The main points in favor and against are clear: on the positive side, the library is demonstrated well (especially after rebuttal) and is equipped with desirable properties such as usage of GPU/TPU, scalability etc. On the other hand, a lot of the key insights build heavily on prior work of Lee et al, 2019. However, judging novelty when it comes to a software paper is more tricky to do, especially given that not many such papers appear in ICLR and therefore calibration is difficult. This has been discussed among reviewers. \n\nIt would help if some further theoretical insights were included in this paper; these insights could come by working backwards from the implementation (i.e. what more can we learn about infinite width networks now that we can experiment easily with them?).\n\nOverall, this paper should still be of interest to the ICLR community.\n", "reviews": [{"review_id": "SklD9yrFPS-0", "review_text": "POST-REBUTTAL COMMENTS I appreciate the response from the authors. I particularly like the comparison table in the response to the other reviewer and ought to be highlighted in the paper. If I were to start this line of research, I would be inclined to expand on the codebase. The contribution is significant. Hence, I am bumping up my score to accept. PRIOR FEEDBACK The contribution of this work lies in providing a library for working with the existing variants of infinite-width neural networks and avoiding the need to derive the NNGP and NT kernels for each architecture by hand. The authors have firstly shown performance comparisons between inferences between finite vs. infinitely wide neural networks. The authors then go into some implementation details with their library. The authors have provided the code and cookbook in the links provided in the abstract. On the overall, I like this effort which is timely. Some additional suggestions below: I would like to see an additional metric for performance comparison of probabilistic models, which is often used in the GP literature: mean negative log probability. It would also be interesting to see how the posterior variance (e.g., Fig. 1 right) evolves over the entire space during training. I would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3. For the summary of contributions, can you give the corresponding section number to refer to when you demonstrate each feature? For example, is the 4th feature (i.e., exploring weight space perspective) demonstrated in the paper? Can the authors elaborate on the ease of expanding their library for the new developments in this field? Minor issues: Page 1: Gaussian Procesesses? Page 4: it\u2019s infinite? Fig. 4: I would have preferred the indices to be placed as subscripts instead of superscripts. Page 8: it\u2019s order of dimensions?", "rating": "8: Accept", "reply_text": "Thank you for the careful review and great suggestions ! We believe to have addressed all your comments , and hope that you could increase the score as a result . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > I would like to see an additional metric for performance comparison of probabilistic models , which is often used in the GP literature : mean negative log probability . Thank you for the suggestion , we have added negative log likelihood ( NLL ) measurement in the updated version . With model \u2019 s marginal likelihood ( train ) we did model selection across different depths and plotted accuracy/mean squared error/marginal NLL . In the appendix ( Figure 7 ) , we included test NLL \u2019 s for fully connected and convolutional models . Since the predictive covariance of the WideResNet kernel has high condition number ( due to the pooling layers , see https : //openreview.net/pdf ? id=Bkx1mxSKvB section C ) , obtaining numerical stable NLL measures was more challenging . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > It would also be interesting to see how the posterior variance ( e.g. , Fig.1 right ) evolves over the entire space during training . Thank you for the suggestion , done in the new revision ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > I would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3 . Agreed - in the new revision , we have expanded the text with section 3.1. demonstrating the tensor-to-kernel ops translation . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > For the summary of contributions , can you give the corresponding section number to refer to when you demonstrate each feature ? For example , is the 4th feature ( i.e. , exploring weight space perspective ) demonstrated in the paper ? Great suggestion , done in the new revision . We have also added an experiment demonstrating linearization / taylor expansion ( 4th feature , section B.6 , Figure 6 ) . Please also see the existing example in ` examples/weight_space.py ` and https : //github.com/neural-tangents/neural-tangents # weight-space . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > Can the authors elaborate on the ease of expanding their library for the new developments in this field ? Thank you for the question , we have elaborated on the process of extending the library to new layers in the new revision in section B.7 ( see also new section 3.1 for the mathematical aspect of deriving new NTK/NNGP results ) . In general , we believe the process to be fairly straightforward , apart from the cases of : - Certain nonlinearities : to derive the layer kernel propagation expression , the user has to compute the covariance of the nonlinearity ( and its derivative , for NTK ) applied to correlated Gaussian variables . As discussed in section E ( new revision ) , some such nonlinearities may not have known exact expressions for these covariances , and either `` nt.empirical_kernel_fn '' or other specialized approximations need to be employed . - Weight sharing between different layers in the network is not currently supported and may require some nontrivial work , but it is on our radar . Finally , once we de-anonymize the repository , we will be using the Github issue and project tracker to inform and engage the community in the library development and planning , and provide support for users and developers ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - > > > Minor issues : > > > Page 1 : Gaussian Procesesses ? > > > Page 4 : it \u2019 s infinite ? > > > Fig.4 : I would have preferred the indices to be placed as subscripts instead of superscripts . > > > Page 8 : it \u2019 s order of dimensions ? Thank you , all fixed in the new revision except for the Figure indices : we stick to superscript usage to follow an established tradition in prior work [ 1-4 , ... ] of using superscript for layer numbers and subscript for hidden units / channels . [ 1 ] Jaehoon Lee , Yasaman Bahri , Roman Novak , Sam Schoenholz , Jeffrey Pennington , and JaschaSohl-dickstein . Deep neural networks as gaussian processes . https : //arxiv.org/pdf/1711.00165.pdf [ 2 ] Alexander G. de G. Matthews , Jiri Hron , Mark Rowland , Richard E. Turner , and Zoubin Ghahramani.Gaussian process behaviour in wide deep neural networks . https : //arxiv.org/pdf/1804.11271.pdf [ 3 ] Roman Novak , Lechao Xiao , Jaehoon Lee , Yasaman Bahri , Greg Yang , Jiri Hron , Daniel A. Abolafia , Jeffrey Pennington , and Jascha Sohl-Dickstein . Bayesian deep convolutional networks with many channels are gaussian processes . https : //arxiv.org/pdf/1810.05148 [ 4 ] Adri\u00e0 Garriga-Alonso , Carl Edward Rasmussen , and Laurence Aitchison . Deep convolutional networks as shallow gaussian processes . https : //arxiv.org/pdf/1808.05587.pdf"}, {"review_id": "SklD9yrFPS-1", "review_text": "Summary: A Jax based neural tangents kernel library is introduced, with native GPU, TPU, and XLA support. Due to the correspondences with infinite neural network kernels (NNGPs), these kernels are also able to be computed for (essentially) free. Layers which do not emit an analytical form (e.g. Tanh or Softplus) can be implemented using Monte Carlo estimates. Several engineering-based experiments are performed demonstrating the potential scalability of their library. While I really enjoyed reading the paper and believe that this library could be extremely practically useful, I vote to reject this paper because I do not feel that it has sufficient novelty to be a paper on its own in light of Lee et al, 2019. Edit: post rebuttal, I'm bumping my score to a weak reject but would have minimal qualms if this paper were to be accepted. I find the further experiments performed by the authors of very good quality overall, but I'm still not particularly satisfied by their `argument that the codebase itself is distinct enough from separate related work. It's unfortunately a bit hard, from a machine learning researcher side, to review the quality of a codebase in and of itself. Significance: Having played around with the code a bit, I find that the library itself is of very high quality and is pretty straightforward to use. I could definitely see myself using this library in the future for research work. However, my primary concern with this paper is that it\u2019s not sufficiently distinct from the previous work of Lee et al, 2019. After all, most of the experiments in that paper would have required the type of implementation that is described in greater detail in this paper. To be able to vote to accept this paper, I will have to see an experiment that is practically performed with the current library in order to distinguish it from previous work (specifically Lee et al, 2019). In recent work, Arora et al, 2019 (Note: I do not consider this reference in my review other than to be mentioned as an example of an experiment that could be run with your library) run neural tangent kernels on tabular data using kernel SVMs. One other potential example would be a kernel SVM in this manner on CIFAR-10. An alternative example would be to exploit the Gaussian process representation and test out both NTKs and NNGPs in comparison to standard kernels for GPs and NNs on UCI regression tasks. Originality: Again, a very efficient and easy to use implementation of neural tangent kernels would be a great boost to the community. This is doubly so as Jax is easy and pretty straightforward to use and is quite numpy like. Again, I am very concerned with originality in comparison to Lee et al, 2019. Even checking out the link to their codebase provides a github repo that is quite similar to this one. Given that ICLR is a venue of similar domain to NeurIPS, it\u2019s not clear to me why this paper ought to be anything other than a separate supporting tech report. If this paper had been submitted to something like SysML (edit: or JMLR MLOSS), I would see the distinctness instead. Clarity: I find the paper to be extremely well-written and easy to follow. The addition of code snippets throughout is very well done, even if it\u2019s a bit overkill. I don\u2019t know what adding a half page long description of an infinitely wide WideResNet adds to the paper when that space could be better used by another experiment. Quality: I find the experiments performed to be very well constructed. Below are a few mostly minor comments on the experiments: In Figure 1 on the right, I would have liked to have seen the posterior predictive for a NNGP with the same kernel as well. In Figure 2, why is the NNGP slower to converge to the analytic values here? Obviously, the rates of convergence are the same, but the constants seem different. In Figure 3 (and throughout the experiments), does \u201cfull Bayesian inference for CIFAR10\u201d mean that you treated the classification labels as regression targets? If so, how was classification error measured. In Section 3.1, you mention that the library \u201cleverages block-diagonal structure\u201d to invert the CIFAR10 covariance matrix for each class (still 50k x 50k). Possibly this is because I haven\u2019t had the chance to use TPUs, but I\u2019m currently struggling to see how one could form and invert (via Choleskys) matrices of this size (50k x 50k) on a standard GPU (or CPU). Could the authors please clarify how they did this (whether through iterative methods, another structure exploiting trick, lots of memory, etc.)? second edit: I was also unable to respond to the final comment about the UCI experiments in its own comment, but thank you for providing the estimated depths. These results definitely show the potential software promise of the codebase and open some interesting new research questions as a result. References: Arora, S., et al., Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks, https://arxiv.org/abs/1910.01663 Lee, J., et al., Wide Neural Networks of any Depth Evolve as Linear Models Under Gradient Descent, NeurIPS, 2019, https://arxiv.org/abs/1902.06720 ", "rating": "3: Weak Reject", "reply_text": "For your convenience , we provide in this comment the diff https : //github.com/neural-tangents/neural-tangents/compare/Lee_et_al_2019 .. master and the brief table of differences between the code released by Lee et al. , 2019 at the time of their submission , and our work . Thank you again for the careful review . We hope , having addressed your concerns regarding differences between this work and Lee et al.that you will consider increasing your score . \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Codebase \u2551 Released with Lee et al. , 2019 \u2551 Ours \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Line of code \u2551 1400+ \u2551 6600+ \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Empirical kernel \u2551 NTK \u2551 NTK/NNGP \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Weight space linearization \u2551 Yes \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Higher-order Taylor series expansion\u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Monte-carlo sampling for empirical NTK/NNGP \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Multi-device parallelization \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Dense Layer \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Nonlinearities \u2551 No \u2551 ReLU , Erf , Abs , LeakyRelu , ABReLU \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Convolution \u2551 No \u2551 Any paddings , strides , filter shapes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Average pooling \u2551 No \u2551 Global , local , any strides / shapes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Flattening \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 LayerNorm \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Skip-connection \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Global self-attention \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Finite-time inference of the posterior \u2551 NTK , Mean \u2551 Mean , covariance , NNGP/NTK \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Infinite-time inference of the posterior \u2551 No \u2551 Mean , covariance , NNGP/NTK \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Dropout \u2551 No \u2551 Coming soon \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Standard ( non-ntk ) parameterization \u2551 No \u2551 Coming soon \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d"}, {"review_id": "SklD9yrFPS-2", "review_text": "This work develops a library for working with a class of infinitely wide neural networks, in particular those corresponding to neural tangent kernels (NTKs) and neural network Gaussian processes (NNGPs). The theory for these two kernels was well developed in a series of recent papers, and this library provides an automatic way to transform any appropriate neural net architecture into its corresponding NTK and NNGP. Infinitely wide neural networks have been a popular subject of theoretical research and have been observed to have highly nontrivial performance on a variety of tasks (e.g. CIFAR-10 classification). It's really nice to see the development of such a library, which I believe could benefit the deep learning community a lot, especially for theoretical research on NTK. I appreciate this work a lot. Currently I can only give weak accept instead of accept for a couple of reasons: 1. The theory and formulae of NTKs and NNGPs were well developed. This work mostly consists of implementing and modularizing them. The research contribution is relatively low. 2. As commented in the paper and if I understand correctly, the current library cannot scale to large datasets for CNNs with pooling. This would make the computation much more expensive (and probably infeasible without additional techniques and huge computing power) as mentioned in [Novak et al. 2019] and [Arora et al. 2019]. However pooling seems extremely useful for NTKs and NNGPs on image datasets. I think this makes this work somewhat less exciting than it may sound.", "rating": "6: Weak Accept", "reply_text": "Thank you for the careful review , we \u2019 re happy you found our library useful and beneficial to the ML community . Below we believe we have addressed your concerns , and we hope you can increase your score as a result . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > 1 . The theory and formulae of NTKs and NNGPs were well developed . This work mostly consists of implementing and modularizing them . The research contribution is relatively low . ICLR explicitly calls for \u201c implementation issues , parallelization , software platforms , hardware \u201d ( https : //iclr.cc/ , bottom ) . We believe that Neural Tangents will unlock qualitatively new avenues for research by making computations on infinite networks tractable for non-experts and orders of magnitude easier for theoretical practitioners . This is increasingly true as work on infinite networks continues to attract interest from the community . On a separate note , a significant intellectual effort went into designing and efficiently implementing the library while keeping it scalable , flexible , and easy to use ( see section 3.2 [ new revision number ] ) . By means of analogy , there is an immense gap between knowing the mathematical formulae of a convolutional layer and having a general and user-/accelerator-friendly implementation . We believe this kind of gaps should not be underestimated , and the novelty of our approach is itself a research contribution . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > 2 . As commented in the paper and if I understand correctly , the current library can not scale to large datasets for CNNs with pooling . This would make the computation much more expensive ( and probably infeasible without additional techniques and huge computing power ) as mentioned in [ Novak et al.2019 ] and [ Arora et al.2019 ] .However pooling seems extremely useful for NTKs and NNGPs on image datasets . I think this makes this work somewhat less exciting than it may sound . You are correct that CNN-GPs/NTKs with pooling are _very_ compute-hungry . However , we would like to highlight that 1 ) We did successfully run experiments on 8K CIFAR10 subsets for a WideResNet with pooling in Figure 3 , and we have further run pooling experiments on the 45K CIFAR10 training set and achieved a slight improvement over the prior state of the art in [ Arora et al.2019a ] with our library ( see the table below , GAP = global average pooling , best values marked with * * ) . \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Model \u2551 NNGP acc \u2551 NTK acc \u2551 NNGP loss \u2551 NTK loss \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 WResNet-LayerNorm-depth_28 \u2551 73.7 \u2551 72.8 \u2551 0.0501 \u2551 0.0501 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 CNN-GAP-Relu-depth_10 \u2551 78.84 \u2551 * 77.84 * \u2551 0.0454 \u2551 * 0.0462 * \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 CNN-GAP-Relu-depth_20 \u2551 * 79.38 * \u2551 76.98 \u2551 * 0.0447 * \u2551 * 0.0462 * \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 CNN-GAP-Erf-depth_10 \u2551 71.32 \u2551 71.3 \u2551 0.0538 \u2551 0.054 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Arora et al.2019 [ GAP ] \u2551 - \u2551 77.43 \u2551 - \u2551 - \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Li et al.2019b [ GAP ] \u2551 78.49 \u2551 77.63 \u2551 - \u2551 - \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d [ Arora et al.2019 ] https : //arxiv.org/pdf/1904.11955v2.pdf [ Li et al.2019 ] https : //arxiv.org/pdf/1911.00809v1.pdf 2 ) Our library provides the [ parallelizable ] `` nt.monte_carlo_fn '' method to Monte-Carlo estimate compute-heavy kernels , and we have established reasonable convergence for a WideResNet in Figure 2 . The question of how good of a tradeoff between accuracy and time / memory the MC method provides is still admittedly open and left for future work . 3 ) Pooling CNN kernels is arguably an emerging field of study , and we believe that as groups with large computing power demonstrate their good performance , studying these kernels ( e.g.on small datasets ) and developing novel approximation / mimicking / \u201c inspired-by \u201d techniques will attract a lot of research attention . We believe our library will facilitate such research greatly , and serve as a platform to deliver new results to the users ."}], "0": {"review_id": "SklD9yrFPS-0", "review_text": "POST-REBUTTAL COMMENTS I appreciate the response from the authors. I particularly like the comparison table in the response to the other reviewer and ought to be highlighted in the paper. If I were to start this line of research, I would be inclined to expand on the codebase. The contribution is significant. Hence, I am bumping up my score to accept. PRIOR FEEDBACK The contribution of this work lies in providing a library for working with the existing variants of infinite-width neural networks and avoiding the need to derive the NNGP and NT kernels for each architecture by hand. The authors have firstly shown performance comparisons between inferences between finite vs. infinitely wide neural networks. The authors then go into some implementation details with their library. The authors have provided the code and cookbook in the links provided in the abstract. On the overall, I like this effort which is timely. Some additional suggestions below: I would like to see an additional metric for performance comparison of probabilistic models, which is often used in the GP literature: mean negative log probability. It would also be interesting to see how the posterior variance (e.g., Fig. 1 right) evolves over the entire space during training. I would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3. For the summary of contributions, can you give the corresponding section number to refer to when you demonstrate each feature? For example, is the 4th feature (i.e., exploring weight space perspective) demonstrated in the paper? Can the authors elaborate on the ease of expanding their library for the new developments in this field? Minor issues: Page 1: Gaussian Procesesses? Page 4: it\u2019s infinite? Fig. 4: I would have preferred the indices to be placed as subscripts instead of superscripts. Page 8: it\u2019s order of dimensions?", "rating": "8: Accept", "reply_text": "Thank you for the careful review and great suggestions ! We believe to have addressed all your comments , and hope that you could increase the score as a result . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > I would like to see an additional metric for performance comparison of probabilistic models , which is often used in the GP literature : mean negative log probability . Thank you for the suggestion , we have added negative log likelihood ( NLL ) measurement in the updated version . With model \u2019 s marginal likelihood ( train ) we did model selection across different depths and plotted accuracy/mean squared error/marginal NLL . In the appendix ( Figure 7 ) , we included test NLL \u2019 s for fully connected and convolutional models . Since the predictive covariance of the WideResNet kernel has high condition number ( due to the pooling layers , see https : //openreview.net/pdf ? id=Bkx1mxSKvB section C ) , obtaining numerical stable NLL measures was more challenging . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > It would also be interesting to see how the posterior variance ( e.g. , Fig.1 right ) evolves over the entire space during training . Thank you for the suggestion , done in the new revision ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > I would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3 . Agreed - in the new revision , we have expanded the text with section 3.1. demonstrating the tensor-to-kernel ops translation . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > For the summary of contributions , can you give the corresponding section number to refer to when you demonstrate each feature ? For example , is the 4th feature ( i.e. , exploring weight space perspective ) demonstrated in the paper ? Great suggestion , done in the new revision . We have also added an experiment demonstrating linearization / taylor expansion ( 4th feature , section B.6 , Figure 6 ) . Please also see the existing example in ` examples/weight_space.py ` and https : //github.com/neural-tangents/neural-tangents # weight-space . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > Can the authors elaborate on the ease of expanding their library for the new developments in this field ? Thank you for the question , we have elaborated on the process of extending the library to new layers in the new revision in section B.7 ( see also new section 3.1 for the mathematical aspect of deriving new NTK/NNGP results ) . In general , we believe the process to be fairly straightforward , apart from the cases of : - Certain nonlinearities : to derive the layer kernel propagation expression , the user has to compute the covariance of the nonlinearity ( and its derivative , for NTK ) applied to correlated Gaussian variables . As discussed in section E ( new revision ) , some such nonlinearities may not have known exact expressions for these covariances , and either `` nt.empirical_kernel_fn '' or other specialized approximations need to be employed . - Weight sharing between different layers in the network is not currently supported and may require some nontrivial work , but it is on our radar . Finally , once we de-anonymize the repository , we will be using the Github issue and project tracker to inform and engage the community in the library development and planning , and provide support for users and developers ! -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - > > > Minor issues : > > > Page 1 : Gaussian Procesesses ? > > > Page 4 : it \u2019 s infinite ? > > > Fig.4 : I would have preferred the indices to be placed as subscripts instead of superscripts . > > > Page 8 : it \u2019 s order of dimensions ? Thank you , all fixed in the new revision except for the Figure indices : we stick to superscript usage to follow an established tradition in prior work [ 1-4 , ... ] of using superscript for layer numbers and subscript for hidden units / channels . [ 1 ] Jaehoon Lee , Yasaman Bahri , Roman Novak , Sam Schoenholz , Jeffrey Pennington , and JaschaSohl-dickstein . Deep neural networks as gaussian processes . https : //arxiv.org/pdf/1711.00165.pdf [ 2 ] Alexander G. de G. Matthews , Jiri Hron , Mark Rowland , Richard E. Turner , and Zoubin Ghahramani.Gaussian process behaviour in wide deep neural networks . https : //arxiv.org/pdf/1804.11271.pdf [ 3 ] Roman Novak , Lechao Xiao , Jaehoon Lee , Yasaman Bahri , Greg Yang , Jiri Hron , Daniel A. Abolafia , Jeffrey Pennington , and Jascha Sohl-Dickstein . Bayesian deep convolutional networks with many channels are gaussian processes . https : //arxiv.org/pdf/1810.05148 [ 4 ] Adri\u00e0 Garriga-Alonso , Carl Edward Rasmussen , and Laurence Aitchison . Deep convolutional networks as shallow gaussian processes . https : //arxiv.org/pdf/1808.05587.pdf"}, "1": {"review_id": "SklD9yrFPS-1", "review_text": "Summary: A Jax based neural tangents kernel library is introduced, with native GPU, TPU, and XLA support. Due to the correspondences with infinite neural network kernels (NNGPs), these kernels are also able to be computed for (essentially) free. Layers which do not emit an analytical form (e.g. Tanh or Softplus) can be implemented using Monte Carlo estimates. Several engineering-based experiments are performed demonstrating the potential scalability of their library. While I really enjoyed reading the paper and believe that this library could be extremely practically useful, I vote to reject this paper because I do not feel that it has sufficient novelty to be a paper on its own in light of Lee et al, 2019. Edit: post rebuttal, I'm bumping my score to a weak reject but would have minimal qualms if this paper were to be accepted. I find the further experiments performed by the authors of very good quality overall, but I'm still not particularly satisfied by their `argument that the codebase itself is distinct enough from separate related work. It's unfortunately a bit hard, from a machine learning researcher side, to review the quality of a codebase in and of itself. Significance: Having played around with the code a bit, I find that the library itself is of very high quality and is pretty straightforward to use. I could definitely see myself using this library in the future for research work. However, my primary concern with this paper is that it\u2019s not sufficiently distinct from the previous work of Lee et al, 2019. After all, most of the experiments in that paper would have required the type of implementation that is described in greater detail in this paper. To be able to vote to accept this paper, I will have to see an experiment that is practically performed with the current library in order to distinguish it from previous work (specifically Lee et al, 2019). In recent work, Arora et al, 2019 (Note: I do not consider this reference in my review other than to be mentioned as an example of an experiment that could be run with your library) run neural tangent kernels on tabular data using kernel SVMs. One other potential example would be a kernel SVM in this manner on CIFAR-10. An alternative example would be to exploit the Gaussian process representation and test out both NTKs and NNGPs in comparison to standard kernels for GPs and NNs on UCI regression tasks. Originality: Again, a very efficient and easy to use implementation of neural tangent kernels would be a great boost to the community. This is doubly so as Jax is easy and pretty straightforward to use and is quite numpy like. Again, I am very concerned with originality in comparison to Lee et al, 2019. Even checking out the link to their codebase provides a github repo that is quite similar to this one. Given that ICLR is a venue of similar domain to NeurIPS, it\u2019s not clear to me why this paper ought to be anything other than a separate supporting tech report. If this paper had been submitted to something like SysML (edit: or JMLR MLOSS), I would see the distinctness instead. Clarity: I find the paper to be extremely well-written and easy to follow. The addition of code snippets throughout is very well done, even if it\u2019s a bit overkill. I don\u2019t know what adding a half page long description of an infinitely wide WideResNet adds to the paper when that space could be better used by another experiment. Quality: I find the experiments performed to be very well constructed. Below are a few mostly minor comments on the experiments: In Figure 1 on the right, I would have liked to have seen the posterior predictive for a NNGP with the same kernel as well. In Figure 2, why is the NNGP slower to converge to the analytic values here? Obviously, the rates of convergence are the same, but the constants seem different. In Figure 3 (and throughout the experiments), does \u201cfull Bayesian inference for CIFAR10\u201d mean that you treated the classification labels as regression targets? If so, how was classification error measured. In Section 3.1, you mention that the library \u201cleverages block-diagonal structure\u201d to invert the CIFAR10 covariance matrix for each class (still 50k x 50k). Possibly this is because I haven\u2019t had the chance to use TPUs, but I\u2019m currently struggling to see how one could form and invert (via Choleskys) matrices of this size (50k x 50k) on a standard GPU (or CPU). Could the authors please clarify how they did this (whether through iterative methods, another structure exploiting trick, lots of memory, etc.)? second edit: I was also unable to respond to the final comment about the UCI experiments in its own comment, but thank you for providing the estimated depths. These results definitely show the potential software promise of the codebase and open some interesting new research questions as a result. References: Arora, S., et al., Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks, https://arxiv.org/abs/1910.01663 Lee, J., et al., Wide Neural Networks of any Depth Evolve as Linear Models Under Gradient Descent, NeurIPS, 2019, https://arxiv.org/abs/1902.06720 ", "rating": "3: Weak Reject", "reply_text": "For your convenience , we provide in this comment the diff https : //github.com/neural-tangents/neural-tangents/compare/Lee_et_al_2019 .. master and the brief table of differences between the code released by Lee et al. , 2019 at the time of their submission , and our work . Thank you again for the careful review . We hope , having addressed your concerns regarding differences between this work and Lee et al.that you will consider increasing your score . \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Codebase \u2551 Released with Lee et al. , 2019 \u2551 Ours \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Line of code \u2551 1400+ \u2551 6600+ \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Empirical kernel \u2551 NTK \u2551 NTK/NNGP \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Weight space linearization \u2551 Yes \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Higher-order Taylor series expansion\u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Monte-carlo sampling for empirical NTK/NNGP \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Multi-device parallelization \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Dense Layer \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Nonlinearities \u2551 No \u2551 ReLU , Erf , Abs , LeakyRelu , ABReLU \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Convolution \u2551 No \u2551 Any paddings , strides , filter shapes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Average pooling \u2551 No \u2551 Global , local , any strides / shapes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Flattening \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 LayerNorm \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Skip-connection \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Global self-attention \u2551 No \u2551 Yes \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Finite-time inference of the posterior \u2551 NTK , Mean \u2551 Mean , covariance , NNGP/NTK \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Infinite-time inference of the posterior \u2551 No \u2551 Mean , covariance , NNGP/NTK \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Dropout \u2551 No \u2551 Coming soon \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Standard ( non-ntk ) parameterization \u2551 No \u2551 Coming soon \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d"}, "2": {"review_id": "SklD9yrFPS-2", "review_text": "This work develops a library for working with a class of infinitely wide neural networks, in particular those corresponding to neural tangent kernels (NTKs) and neural network Gaussian processes (NNGPs). The theory for these two kernels was well developed in a series of recent papers, and this library provides an automatic way to transform any appropriate neural net architecture into its corresponding NTK and NNGP. Infinitely wide neural networks have been a popular subject of theoretical research and have been observed to have highly nontrivial performance on a variety of tasks (e.g. CIFAR-10 classification). It's really nice to see the development of such a library, which I believe could benefit the deep learning community a lot, especially for theoretical research on NTK. I appreciate this work a lot. Currently I can only give weak accept instead of accept for a couple of reasons: 1. The theory and formulae of NTKs and NNGPs were well developed. This work mostly consists of implementing and modularizing them. The research contribution is relatively low. 2. As commented in the paper and if I understand correctly, the current library cannot scale to large datasets for CNNs with pooling. This would make the computation much more expensive (and probably infeasible without additional techniques and huge computing power) as mentioned in [Novak et al. 2019] and [Arora et al. 2019]. However pooling seems extremely useful for NTKs and NNGPs on image datasets. I think this makes this work somewhat less exciting than it may sound.", "rating": "6: Weak Accept", "reply_text": "Thank you for the careful review , we \u2019 re happy you found our library useful and beneficial to the ML community . Below we believe we have addressed your concerns , and we hope you can increase your score as a result . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > 1 . The theory and formulae of NTKs and NNGPs were well developed . This work mostly consists of implementing and modularizing them . The research contribution is relatively low . ICLR explicitly calls for \u201c implementation issues , parallelization , software platforms , hardware \u201d ( https : //iclr.cc/ , bottom ) . We believe that Neural Tangents will unlock qualitatively new avenues for research by making computations on infinite networks tractable for non-experts and orders of magnitude easier for theoretical practitioners . This is increasingly true as work on infinite networks continues to attract interest from the community . On a separate note , a significant intellectual effort went into designing and efficiently implementing the library while keeping it scalable , flexible , and easy to use ( see section 3.2 [ new revision number ] ) . By means of analogy , there is an immense gap between knowing the mathematical formulae of a convolutional layer and having a general and user-/accelerator-friendly implementation . We believe this kind of gaps should not be underestimated , and the novelty of our approach is itself a research contribution . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- > > > 2 . As commented in the paper and if I understand correctly , the current library can not scale to large datasets for CNNs with pooling . This would make the computation much more expensive ( and probably infeasible without additional techniques and huge computing power ) as mentioned in [ Novak et al.2019 ] and [ Arora et al.2019 ] .However pooling seems extremely useful for NTKs and NNGPs on image datasets . I think this makes this work somewhat less exciting than it may sound . You are correct that CNN-GPs/NTKs with pooling are _very_ compute-hungry . However , we would like to highlight that 1 ) We did successfully run experiments on 8K CIFAR10 subsets for a WideResNet with pooling in Figure 3 , and we have further run pooling experiments on the 45K CIFAR10 training set and achieved a slight improvement over the prior state of the art in [ Arora et al.2019a ] with our library ( see the table below , GAP = global average pooling , best values marked with * * ) . \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Model \u2551 NNGP acc \u2551 NTK acc \u2551 NNGP loss \u2551 NTK loss \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 WResNet-LayerNorm-depth_28 \u2551 73.7 \u2551 72.8 \u2551 0.0501 \u2551 0.0501 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 CNN-GAP-Relu-depth_10 \u2551 78.84 \u2551 * 77.84 * \u2551 0.0454 \u2551 * 0.0462 * \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 CNN-GAP-Relu-depth_20 \u2551 * 79.38 * \u2551 76.98 \u2551 * 0.0447 * \u2551 * 0.0462 * \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 CNN-GAP-Erf-depth_10 \u2551 71.32 \u2551 71.3 \u2551 0.0538 \u2551 0.054 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Arora et al.2019 [ GAP ] \u2551 - \u2551 77.43 \u2551 - \u2551 - \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 Li et al.2019b [ GAP ] \u2551 78.49 \u2551 77.63 \u2551 - \u2551 - \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d [ Arora et al.2019 ] https : //arxiv.org/pdf/1904.11955v2.pdf [ Li et al.2019 ] https : //arxiv.org/pdf/1911.00809v1.pdf 2 ) Our library provides the [ parallelizable ] `` nt.monte_carlo_fn '' method to Monte-Carlo estimate compute-heavy kernels , and we have established reasonable convergence for a WideResNet in Figure 2 . The question of how good of a tradeoff between accuracy and time / memory the MC method provides is still admittedly open and left for future work . 3 ) Pooling CNN kernels is arguably an emerging field of study , and we believe that as groups with large computing power demonstrate their good performance , studying these kernels ( e.g.on small datasets ) and developing novel approximation / mimicking / \u201c inspired-by \u201d techniques will attract a lot of research attention . We believe our library will facilitate such research greatly , and serve as a platform to deliver new results to the users ."}}