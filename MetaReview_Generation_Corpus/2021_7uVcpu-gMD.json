{"year": "2021", "forum": "7uVcpu-gMD", "title": "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks", "decision": "Accept (Poster)", "meta_review": "This is a paper that is actively discussed.  The general sentiment is that this paper aims to address an important set of questions. While the technique could be improved with more novelty, the empirical study is extensive. The concerns are about how to interpret the results, or rather whether the empirical evidence fully supports the the claim/hypothesis.  After discussion and rebuttal, the reviewers improved their scores (and one reviewer remained at \"weaker marginally above threshold\").\n\nThe AC read the paper and the discussion. One value the AC sees that the discussion threads between the authors and the reviewers provide a significant amount of scientific value -- the questions to be answered are hard and might indeed require further refinements in framing and conceptualization, better techniques,  strong power in experimental designs to rule exclusively various hypotheses.  Thus, the AC recommends acceptance.", "reviews": [{"review_id": "7uVcpu-gMD-0", "review_text": "The paper presents an empirical study of whether modularity can emerge within neural networks . It starts by proposing a novel definition of modularity that identifies modules by their functionality . To discover the module that implements a specific target functionality , the paper proposes to first pretrain the full network on the original task , then freeze the pretrained weights , and train a binary mask for each weight using Gumbel-Sigmoid . The training objective for the masks is given by the target functionality ( e.g. , a subtask of the original task ) , plus some sparsity regularization . The paper then investigates the discovered modules in terms of specialization , reusability , and compositionality . The main findings are : ( 1 ) Neural nets tend to satisfy specialization but not reusability ; ( 2 ) Weight sharing between modules tends to be affected more by whether I/O are shared than by task similarity , and there tends to be less sharing in larger networks ; ( 3 ) When trained on algorithmic tasks , neural nets fail to learn compositional rules , and thus generalize poorly ; ( 4 ) CNNs trained for image classification contain class-specific , non-shared weights in the feature detectors . Pros - The experiments are comprehensive , covering many neural net architectures and datasets . The results seem consistent across these architectures and datasets . Also , source code is provided , and the supplementary material provides sufficient details to reproduce the results . - The paper investigates natural emergence of functional modules , which is a novel perspective on modularity . Cons - Some concepts are not rigorously defined . For example , in P_ { specialize } and P_ { reuse } , when should two modules be considered the same ? The paper seems to define a module as a subset of weights . Does this imply that two different subsets ( potentially differ by only one element ) will correspond to two different modules ? If this is the case , then P_ { reuse } is extremely hard to achieve , and it can never be true if the input/output neurons of two modules are different ( i.e. , separate I/O considered in Section 3.2 ) . Hence , the experiments in Section 3.2 seem meaningless . One may argue that the weights in the input/output layer should be excluded from the module . But then the functionality of the module is not well defined . Another confusion I had is : What can be a target functionality and what can not ? Take the addition/multiplication task as an example . The task is basically to learn the function f ( a , b , s ) , where a and b are two numbers , and s is a switch indicating whether addition or multiplication should be performed for a and b . The paper suggests f ( a , b , s=+ ) and f ( a , b , s= * ) as two target functionalities , which are the original function restricted to two subsets of the input space . Is it reasonable to consider f ( a=99 , b , s ) a target functionality ? Would the result be different ? Also , all other experiments in the paper seem to construct target functionalities by similarly restricting the original functions . Are there other ways to define target functionalities ? I am somewhat skeptical about investigating properties like compositionality and generalization after network pruning . It is well known that training performance does not reflect these properties . So even if training performance only slightly drops after network pruning , the pruned network may lose some properties that the full network has . Minor Comments - It is mentioned that separate I/O biases the network at initialization time to not share weights , but to me it seems to only bias the input/output layers rather than the hidden layers . It would be interesting to see if shared I/O will also lead to unshared hidden weights . - Fig.13 did not mention which LSTM variant was used . - In Section 3.3 , why is increased sharing in the first layer undesirable ? It might not be possible to undo the permutation with only one layer .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . We \u2019 re glad that you were able to appreciate the novelty of our perspective as well as the thoroughness of our experimental study . Before we dive into individual responses to your specific queries and comments , we would first like to briefly comment on point ( 3 ) of your summary . In particular , the novelty of our work is not merely in showing that neural networks generalize poorly on algorithmic tasks ( which was previously shown in several other works [ Lake & Baroni , 2018 ; Saxton et al. , 2019 ] ) , but rather that the learned solution on these tasks is non-compositional , even for the training set . This provides counter evidence to the hypothesis that neural networks are unable to find an \u2018 analogical mapping \u2019 between the training and validation set . > `` Some concepts are not rigorously defined . For example , in $ P_ { \\text { specialize } } $ and $ P_ { \\text { reuse } } $ , when should two modules be considered the same ? The paper seems to define a module as a subset of weights . Does this imply that two different subsets ( potentially differ by only one element ) will correspond to two different modules ? '' This is an interesting question and indeed it would be problematic if two modules would be considered as \u201c entirely different \u201d were they to vary only by a single element . Therefore , in this work , we have treated $ P_ { \\text { specialize } } $ and $ P_ { \\text { reuse } } $ as a continuous quantity ranging from 0 ( no sharing ) to 1 ( completing sharing ) , which accordingly allows us to evaluate the _degree_ to which modularity emerges . We will update the next revision of the current submission to better emphasize this distinction . Further , and related to this , we note that throughout most of the work we do not rely on these quantities directly . Rather , we measure the change in performance when certain masks are applied , which is well defined and yields what we believe to be an easy to interpret metric . In this way , we decouple our findings from the specific amount of sharing that is observed , which is a quantity that is hard to interpret ( i.e.how much sharing is desirable on say CIFAR10 ? ) . On the contrary , it is clear how the performance should change as a result of applying different masks ( e.g if the same functionality is used twice , their weight should not be mutually exclusive , as we demonstrated in section 3.2 ) . We will further clarify our motivation for this metric in the paper . > \u201c Hence , the experiments in Section 3.2 seem meaningless \u201d Hopefully from our previous comment it is now clear that the experiments in Section 3.2 are highly meaningful . To summarize , we show that when performing two addition operations ( $ x=a+b $ and $ y=c+d $ ) , one becomes completely independent of the other . This is achieved by training a mask to select the subset of the network responsible for $ x=a+b $ , which performs well for $ x $ , but the performance of $ y $ is low . However , we noted that this behavior could also be observed if just a subset of weights is unshared . Hence , we also take the inverse of the mask trained on $ x $ ( which now excludes all the weights needed for $ x=a+b $ ) , and demonstrate how the resulting network performs well on $ y $ ( while performing poorly on $ x $ ) . This clearly demonstrates that two independent modules are learned : they correspond to two mutually exclusive sets of weights , which is a non-trivial finding . > \u201c One may argue that the weights in the input/output layer should be excluded from the module \u201d Ideally , one would want to exclude the input/output connections from the analysis . However , they are already performing a transformation of data , which makes it impossible to do so without architectural modifications ( and in that case the functionality of a module is not well defined as you noted ) . We combat this issue in two different ways : either we use shared input and output ( for the addition/multiplication task , SCAN , Math dataset , CNN ) , or we made sure that the I/O sharing does not affect the results ( for double addition , in Section 3.2 , we find two mutually exclusive sets of weights , each one capable of performing one , and only one of the two instances of addition ) ."}, {"review_id": "7uVcpu-gMD-1", "review_text": "This paper studies weight modularity in neural networks ( NNs ) . In particular , given a NN trained to perform a task , a subset of weights are identified which in isolation perform well on a subtask of the original task . Such subsets are inspected to understand the extent to which they are specialized or reused across different subtasks of the original task . To identify subtask specific weights , a mask is learned that minimizes loss over a subtask when applied to the original NN 's frozen weights . This process is carried out using gradient based optimization techniques ( Adam ) . Extensive experiments are performed across various datasets and architectures . The paper concludes that while NNs seem to exhibit module specialization , they fail to exhibit reuse . # # # Strengths 1 . Understanding modularity within NNs and its relation to failures of systematic generalization is an important research direction . 2.The precise masking method proposed here is novel to the best of my knowledge . 3.The functional view of modularity has clear advantages over clustering based approaches . 4.I believe this work could be extended in interesting ways . For example , it seems like the methods presented here could naturally be extended to encourage modularity , rather than measure it . # # # Weaknesses 1 . The paper addresses a well known issue , that NNs often fail to generalize systematically . Why this occurs and potential solutions are left unaddressed . While the paper does put forward a hypothesis regarding why , that the difficulty of learning routing operation in NNs may lead current approaches to fail to discover solutions which generalize systematically , little evidence is provided to support this conclusion . As such , I believe claims such as , `` our approach can also be applied in these settings to provide additional insight regarding the underlying reason for the observed failure at systematic generalization '' should be relaxed , or clarified if there is evidence for this hypothesis presented that I am failing to connect . 2.How do we know the problem is with the full model , rather than the mask ? Recent literature ( cited in the paper ) demonstrates that performant models can be obtained even when severe weight constraints are imposed [ 1 , 2 ] . Indeed , [ 2 ] demonstrates masking alone is sufficient to adapt a NN to a completely new task . Given these observations , I 'm concerned about the validity of assuming the mask optimization procedure identifies an independent functional module in the context of the computation performed by the full model . It seems plausible that the mask performs well on the subtask given the fixed model weights , but is unrelated to the function of the full model . If this was the case , I believe a number of the conclusions in the paper would not be justified . The paper does provide some evidence this may not be occurring . For example , the double-addition experiment ( Figure 2 ) shows that inverted masks perform well on the other pair . This suggests the mask and inverse mask may correspond to distinct functionality in the full model . However , Figure 10 tells a different story , where inverse masks do not perform well on the other task . Is this due to the shared IO in the addition/multiplication task ? Is there any relation between the addition mask with shared weights removed and the inverted multiplication mask with shared weights removed ? Ideally , the paper would provide more evidence to support this assumption . Are units correlated in the masked and unmasked model ? Could the analysis here be supported by explicitly constructing a model that exhibits reuse , and verifying it can be identified ? 3.The paper argues in several places that a weight level analysis is necessary for reasoning about functional modularity , e.g. , `` without considering the contribution of individual weights it is not possible to reason about * functional * modularity . '' I believe this statement conflates two separate issues ; ( 1 ) if modules should be defined in terms of units or weights , and ( 2 ) whether modules should be defined by functionality ( as done in this work ) or using techniques such as clustering . On the latter point , I agree with the statements in the paper and believe the functional approach is more closely aligned with intuitive notions of what is meant by modularity . However , I do n't believe a weight vs unit level perspective is necessarily mutually exclusive . For example , consider a simple affine layer $ h = f ( Wx ) $ . Defining a module as a subset of elements of $ h $ would be equivalent to defining a module as a subset of rows in $ W $ . 4.Figures 2 , 10 , 14 would be much clearer if presented in a tabular format . I had to write out the results this way myself to aid understanding . 5.I found the use of a non-symmetric sharing metric extremely confusing . I spent a while pondering over Figure 1 trying to figure out what the weights were shared with since both tasks are represented in the figure . I believe this issue could be easily remedied , for example , by using something like IoU ( Jaccard index ) . The paper uses this metric in Appendix B.1 to determine the amount of sharing between different masks trained on the same network and task . 6.Permuted MNIST Experiment 1 . The paper states `` it suffices to re-train a new first layer to undo the permutation so that later layers can be reused . '' This makes sense . However , given that the procedure is `` freeze the occupied weights , '' lower level weights can not perform this simple permutation by construction since they are frozen ( I 've assumed that occupied means a non-zero mask value ) . 2.Why was the choice made to `` train masks and weights simultaneously '' in this setting ? This introduces an additional source of variation from previous experiments , and I 'm unsure what the benefit is . 7.SCAN experiments 1 . The paper states `` if the masking process removes any important weights , then the solution is pattern-recognition like instead of being based on reusable rules , confirming explanation . '' This is related to point ( 2 ) above , and I 'm not convinced this is the only possible conclusion . While the mask has identified a subnetwork whose solution is pattern-recognition like , I 'm not sure that the logical jump to conclude the model as a whole performs identically is sound . 2.Regarding the output weight analysis , I 'm also unsure why it should be the case that the final layer is sufficiently powerful for unbinding bound variables . Is n't it also possible that the mask learns to ignore those weights because it is not important for the task it is trained on ? # # # Recommendation I recommend acceptance . Although I have several concerns with the paper , I found the ideas and analysis presented very interesting . I believe the community would benefit from further discussion , scrutiny , and exploration of the ideas presented . # # # Post Author Response Period Update Most of my concerns have been addressed by the additional experiments and updated language in the latest revision . I believe the techniques and analysis presented here for assessing reuse could be an important step between observations and explanations for the failure of NNs to generalize systematically . I have raised my score accordingly . # # # Minor Issues 1 . Appendix C.2 : `` We always check the * * choosen * * hyperparameters '' = > `` We always check the * * chosen * * hyperparameters '' # # # References 1 . Arun Mallya , Dillon Davis , and Svetlana Lazebnik . Piggyback : Adapting a single network to multiple tasks by learning to mask weights . In Proceedings of the European Conference on Computer Vision ( ECCV ) , pp . 67\u201382 , 2018 . 2.Adam Gaier and David Ha . Weight agnostic neural networks . In Advances in Neural Information Processing Systems , pp . 5364\u20135378 , 2019 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . We were very impressed with the thoroughness of your review and really appreciate the effort that you put into reviewing our work , which allows us to make several improvements . Below are the individual responses to the many specific queries and comments : 1 . Thank you for pointing this out , this is mostly an oversight on our part . We will relax the claims about the insights to better reflect the fact that we can not provide definitive evidence to support our hypothesis in some cases . 2. > \u201c How do we know the problem is with the full model , rather than the mask ? .. \u201d This is a good point and although we strongly believe that the problems are on the model side ( as is also evident from the inverse masking results as you point out ) , we have devised an additional experiment to demonstrate that the masking process does not change the original function performed by the network . In particular , we will attempt to evaluate a network where only the first half of the network is masked , while the second is not . If the network works like this , it should mean that the activations produced by the first , unmasked , half are compatible with the second , masked half . Alternatively , we could calculate additional statistics of the activations , or plot the activations of the same sample with and without masking , to provide further evidence to this point . > \u201c However , Figure 10 tells a different story , where inverse masks do not perform well on the other task . Is this due to the shared IO in the addition/multiplication task ? \u201d Figure 10 tells a different story for multiple reasons . First , the IO is shared as you also noted . Second , the separation of the hidden layers is not perfect . We believe that the sharing of weights is explained more by sharing the module \u2019 s inputs/outputs than by the performed function in this case . Note , however that Figure 10 is mostly there for symmetry with the double-addition experiments , which provides additional insight . > \u201d Is there any relation between the addition mask with shared weights removed and the inverted multiplication mask with shared weights removed ? \u201d The addition mask with the shared weights removed is a subset of the inverted multiplication mask with the shared weights removed . Both of them would perform poorly because the shared weights are important for the task . An additional experiment that we could perform is to modify the inverse-mask experiment on the addition/multiplication task such that the inverted mask will be unioned together with the shared weights . This would just remove the weights exclusive for one operation . The performance on that operation should decrease , but the inverse mask should perform well on the complementary task . Personally , we don \u2019 t expect that this will provide much additional insight , but if you think that this is valuable then we are happy to run this . Please let us know in this case . 3.The paper argues for both ( 1 ) and ( 2 ) , but we are also specifically advocating for ( 1 ) when mentioning weight-level analysis . We are not arguing that unit-wise analysis necessarily gives a wrong result , but rather that it is a special case of the weight-level analysis that is conducted here , and in practice probably not enough to draw meaningful conclusions . The provided example with the affine layer can demonstrate this easily : imagine x has 4k units , while h has k. X can be partitioned to 4 equal parts , a , b , c , d . The task is to perform either h = a | b or h = c & d , depending on which inputs are present . This can be done with a single affine layer . Now if we conduct a unit-wise analysis on h , all units will equally take part in both | and & operations . In contrast , a single row of W can be divided into 2 parts , where the first half performs the | and the second the & . This is why we are arguing explicitly for ( 2 ) in this case . 4.Thank you for the feedback . We are currently looking to improve this visualization and will let you know once the paper is updated with this . We will also add tabular results to the appendix . 5.Using IoU is a good idea for these figures , thank you for the suggestion . We will change the corresponding figures in the main paper accordingly ."}, {"review_id": "7uVcpu-gMD-2", "review_text": "In its current form , I feel that the paper should be disqualified because it contains some results essential to its claims in the appendix ( referenced in the second paragraph , page 5 ) . However , this can be easily addressed - thus my full review below : Summary of the paper : The paper aims to analyze if and how neural networks learn modular representations . Modularity under the paper 's definition means that the network learns representations that ( 1 ) specialize ( using different modules for different functions ) and that ( 2 ) compose in a re-usable fashion - i.e. , that functions are used in diverse tasks . To do so , the authors train different probabistic masks ( using a Gumbel-Sigmoid ) on the weights of a neural network over a series of different tasks . They incentivize these masks to be sparse by regularizing the number of `` active '' elements in a mask . They then compare masks learned for different tasks ( including simple arithmetic tasks , and permuted MNIST ) , and then compare the usage of the parameter masks over different tasks . They find that NNs learn to specialize only , without reuse . The paper continues by postulating that the reason is either that the network learned `` bad '' representations , or that the network did not learn the correct composition . They argue that results on the SCAN dataset show that the representations are of a sufficient quality , concluding that the network did not learn the correct composition . Commentary : The question the paper tries to answer is very relevant , on two levels . The first is that we do not understand how an NN learns sufficiently well . The second is that compositional modularity is a highly desireable property , and it is important to know if neural network exhibit it . Strengths : - The paper does some interesting analysis - In general , the paper is well-written , and easily understood . Unfortunately , the paper suffers from major drawbacks : - ( this is a minor point I 'm putting here to facilitate my discussion below ) The paper is not positioning itself correctly in the literature , thereby using confusing terminology While modularity is not a main focus in neural network research , there exists some meaningful research that has established some terminology . In essence , the paper talks about compositional modularity ( and combinatorial generalization ) , but does not use this terminology . This makes the paper a little difficult to follow , if one is familiar with this literature . Within this terminology , specialize would be called `` modularize '' and Preuse would be called `` compose '' , a terminology I will use in the following . - The conclusions are not convincing The core argument in the paper is that neural networks fail to modularize because they either learn insufficient representations , or because they fail to learn to compose ( to learn the `` algorithm '' required to utilize the modules correctly ) . Because the network learns re-usable representations , they modularize , and must thus fail to compose . While this is probably true , I can not help to feel a little underwhelmed . It is well known that neural networks are overparameterized ( see the `` lottery ticket '' literature ) . For this analysis , this means that it appears to be easier for the model to re-learn using the available capacity than to re-use the existing modules . This is not particularly surpsising either , because this is , at its core , overfitting : the model does not generalize ( which , in effect , is a `` softer '' way to re-use ) , but instead learns something akint to a separate function for different inputs . What the paper does not investigate is why a neural network not re-uses capabilities , even those should be a good fit for a problem . This would be a really interesting analysis , one which I would very strongly argue for acceptance in any venue . - The novelty of some parts is overstated This is particularly true for using binary masks for multi-task like learning . See `` Bengio , E. , Bacon , P. L. , Pineau , J. , & Precup , D. ( 2015 ) . Conditional computation in neural networks for faster models . arXiv preprint arXiv:1511.06297 . '' Additionally , the paper does not sufficiently relate their insights to the ( cited ) work around inducing modularity and compositionaly in networks , some of which does already come to similar results .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . Although we were initially glad to see that you found the presented analysis interesting , we were disappointed at receiving a final rating of a 3 . Ignoring your comments about results in the appendix ( which , as you also state , are easily addressed ) and terminology ( which you state is minor ) for now , it appears that there are two reasons that led you to this score : 1 . You argue that \u201c The conclusions are not convincing \u201d , since \u201c it is well known at neural networks are overparameterized \u201d , and that therefore the presented results are \u201c not surprising \u201d . You suggest that we investigate an alternative research question : \u201c why a neural network not re-uses capabilities , even those should be a good fit for a problem \u201d . 2.You argue that the \u201c novelty of some parts is overstated \u201d , which you believe to be \u201c particularly true for the binary masks for multi-task learning \u201d . Additionally , you feel that \u201c the paper does not sufficiently relate their insights to the ( cited ) work around inducing modularity and compositionality in networks , some of which does already come to similar results. \u201d Regarding ( 1 ) , we would like to emphasize the distinction between results not being \u201c convincing \u201d and them being \u201c expected \u201d . Although you state that our analysis is not \u201c convincing \u201d , you yourself point out how our findings are \u201c probably true \u201d and do not offer any concrete remarks or evidence that suggest that the presented analysis is not thorough or even invalid . As to whether the results can be \u201c expected \u201d , clearly this is debatable . When we started this work a year ago it was far from obvious that these results would follow from the \u201c lottery ticket hypothesis \u201d and arguably it still isn \u2019 t without first considering the evidence presented in this work . Indeed , to the best of our knowledge there has been no prior analysis that studies such effects in neural networks , which we argue makes this work a valid contribution , irrespective of whether , in hindsight , this may have been expected . In judging the significance of our results we therefore ask that you also consider the comments from the other three reviewers , none of which suggested that these results are obvious or logically follow , which we argue indicates that our results are significant . Further , we argue that it is important to consider the understanding of the broader community ( as opposed to expert reviewers ) regarding these issues in neural networks when judging the significance of our findings . It is our hope that you will reconsider . Regarding ( 2 ) , it was never our intention to claim that multi-task learning with binary masks is a novel approach that we propose . The sentence \u201c Typical approaches revolve around freezing used weights when a new task is added ( Fernando et al. , 2017 ; Mallya & Lazebnik , 2018 ; Golkar et al. , 2019 ) . \u201d was meant to clarify this . However , we now realize that it may not be obvious to the reader that this implies that these methods use a form of masking . This is an oversight on our part and we thank the reviewer for pointing this out . The next revision will clarify this . We also thank the reviewer for pointing out Bengio et al. , ( 2015 ) , which we were unaware of and which we will include in the next revision as related work . More generally , regarding novelty for the multi-task setting , we note that our analysis of the behavior of the multi-task learner is the novel part , which is what this experiment contributes . Regarding the second part of ( 2 ) , we believe that we have sufficiently compared our work to the many of the related works on modularity . To the best of our knowledge these works each focus on different aspects of this problem and do not not analyze modules based on their functionality , which is a defining characteristic of our work . If you disagree , could you please elaborate further on related work that we are not comparing to and that provides similar results ? We will now provide individual responses to the remaining minor comments : > \u201c In its current form , I feel that the paper should be disqualified because it contains some results essential to its claims in the appendix ( referenced in the second paragraph , page 5 ) \u201d The main results and conclusions from the second paragraph , page 5 are shown in Figure 2 . Figures 13 and 14 are only referenced to support our claims . We agree that it would be better if we can present it in the main paper , however considering the page limit , this was challenging . We believe that with the extra page , we can do a better job at this ."}, {"review_id": "7uVcpu-gMD-3", "review_text": "The paper investigates the functional modularity of neural networks using a simple yet efficient end-to-end method . The paper is well written and clearly articulates a contribution to the literature . The proposed method is intuitive and straightforward . The experimental evidence is provided for both synthetic , language , and image classification tasks . Most of the related works are cited . Concerns : The biggest concern I had is whether the conclusion reached in the paper is invariant to different neural network architectures , the size of the network , and the complexity of the task . As depicted in Figure 6 ( a ) , there is a huge difference in the relative drop in performance for simple CNN , simple CNN without drouput and ResNet-110 . It seems that a larger and more complex network tends to not sharing weights . Besides , the paper uses accuracy drop after masking the weights as the main metric , which is related to the number of masked weights . It might be better to learn the binary mask for each subtask with a certain accuracy objective ( e.g. , less than 1-2 % lower than the original network ) for each subtask and compare the learned mask of different subtasks . In addition , it is useful to see the results on a more complex task such as ImageNet . In ImageNet , there exist many similar subtasks as many categories of the images are actually belong to one big category . I am wondering whether these subtasks can share the weights . Minor comments : The paper claims the advantage of using Gumbel-Sigmoid than a simple threshold function . However , the state-of-the-art binarized neural networks are trained using a simple sign function with a straight-through estimator . Is there a significant difference ( e.g. , the stability of the training ) in training the binary mask with the Gumbel-Sigmoid and threshold function ? Reasons for score : I vote for accepting . I like the finding that most neural networks have non-overlapped functional modules . However , I still have some concerns about the generality of the conclusion , and also the number of shared weights is not calculated with respect to a uniform accuracy requirement . I would consider raising my score if the authors can address my concerns .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . We \u2019 re glad that you were able to appreciate the effectiveness of the proposed method as well as our findings . Below are the individual responses to your specific queries and comments . > \u201c The biggest concern I had is whether the conclusion reached in the paper is invariant to different neural network architectures , the size of the network , and the complexity of the task. \u201d We understand your concern . When empirically studying properties of neural networks there will always remain the question of whether they will hold on yet another dataset or using yet another architecture . Unfortunately , reality is that we can only test so many things . In the current submission we present consistent observations across what we believe to be a wide variety of neural networks : FNNs , LSTMS , CNNs , ResNets , Transformers , and data sets : synthetic data sets , datasets for image classification , algorithmic language tasks and solving math problems , which makes it reasonable to assume that similar behavior can be observed in ( slightly ) different settings . Moreover , the considered architectures contain standardized architectures that are popular across the deep learning literature , and the data sets include several standard benchmarks . Therefore , although we can not with absolute certainty say that these results will extend to other data sets ( or perhaps in other domains ) or when considering certain custom architectures , we strongly believe that the insights presented in this work will be of use to a significant fraction of the neural networks community . We appreciate that you voted for acceptance even given this concern . > \u201c It seems that a larger and more complex network tends to not sharing weights. \u201d Larger networks are certainly more susceptible to not sharing weights . This can also be seen in Figure 9 . As the network size shrinks , the network must share weights in order to have enough capacity for solving the task . However , note that sharing is not happening as a result of performing similar functionality as is evident from the findings in our paper . Moreover , in most cases , overparametrized networks tend to work better . For the SCAN and Math experiments , we used the default architecture with default hyperparameters of the original papers . The simple CNN is a reduced version of a popular architectural choice used for classifying MNIST . We emphasize that although the amount of sharing varies , our goal was not to show what fraction is _generally_ shared . Rather we demonstrated that the networks are biased against sharing based on _functionality_ , which is consistently observed . We will improve our discussion of these results in the revision to better clarify this . > \u201c Besides , the paper uses accuracy drop after masking the weights as the main metric , which is related to the number of masked weights \u201d It is indeed related , but not ( directly ) caused by it . Weight removal is the result of them not being needed to perform the target task . Our goal has always been to measure modularity in a way that depends least on the exact amount of weight sharing . For example , this quantity may cause problems when encountering alternative weight configurations , especially when used with dropout : Some weights might be marginally more important and kept in some instances and not in the others as a result of the masking process \u2019 s stochasticity . Further , for many tasks it is unclear what a good amount of weight sharing is ( see also our reply above to R3 regarding this ) . In contrast , the desired behavior when applying different masks is more easily understood . This led us to use accuracy for our experiments , since in this case if the experiments are correctly set up , the precise amount of sharing is irrelevant . The effects on accuracy are directly measurable and interpretable . We hope that this explanation helps motivate our design choices regarding the considered metric . Should you have additional concerns , then please let us know so we can clarify further . > \u201c It is useful to see the results on a more complex task such as ImageNet \u201d We briefly mentioned in the paper that we don \u2019 t expect the CNN experiments to directly transfer to more complex datasets with a higher number of classes . However , the experiment you propose sounds very interesting . We are currently working on conducting such an experiment with bigger category groups removed initially ( for example remove all dogs ) . However , we note that since Imagenet is very expensive to train on , we might not be able to finish these experiments in the rebuttal period ."}], "0": {"review_id": "7uVcpu-gMD-0", "review_text": "The paper presents an empirical study of whether modularity can emerge within neural networks . It starts by proposing a novel definition of modularity that identifies modules by their functionality . To discover the module that implements a specific target functionality , the paper proposes to first pretrain the full network on the original task , then freeze the pretrained weights , and train a binary mask for each weight using Gumbel-Sigmoid . The training objective for the masks is given by the target functionality ( e.g. , a subtask of the original task ) , plus some sparsity regularization . The paper then investigates the discovered modules in terms of specialization , reusability , and compositionality . The main findings are : ( 1 ) Neural nets tend to satisfy specialization but not reusability ; ( 2 ) Weight sharing between modules tends to be affected more by whether I/O are shared than by task similarity , and there tends to be less sharing in larger networks ; ( 3 ) When trained on algorithmic tasks , neural nets fail to learn compositional rules , and thus generalize poorly ; ( 4 ) CNNs trained for image classification contain class-specific , non-shared weights in the feature detectors . Pros - The experiments are comprehensive , covering many neural net architectures and datasets . The results seem consistent across these architectures and datasets . Also , source code is provided , and the supplementary material provides sufficient details to reproduce the results . - The paper investigates natural emergence of functional modules , which is a novel perspective on modularity . Cons - Some concepts are not rigorously defined . For example , in P_ { specialize } and P_ { reuse } , when should two modules be considered the same ? The paper seems to define a module as a subset of weights . Does this imply that two different subsets ( potentially differ by only one element ) will correspond to two different modules ? If this is the case , then P_ { reuse } is extremely hard to achieve , and it can never be true if the input/output neurons of two modules are different ( i.e. , separate I/O considered in Section 3.2 ) . Hence , the experiments in Section 3.2 seem meaningless . One may argue that the weights in the input/output layer should be excluded from the module . But then the functionality of the module is not well defined . Another confusion I had is : What can be a target functionality and what can not ? Take the addition/multiplication task as an example . The task is basically to learn the function f ( a , b , s ) , where a and b are two numbers , and s is a switch indicating whether addition or multiplication should be performed for a and b . The paper suggests f ( a , b , s=+ ) and f ( a , b , s= * ) as two target functionalities , which are the original function restricted to two subsets of the input space . Is it reasonable to consider f ( a=99 , b , s ) a target functionality ? Would the result be different ? Also , all other experiments in the paper seem to construct target functionalities by similarly restricting the original functions . Are there other ways to define target functionalities ? I am somewhat skeptical about investigating properties like compositionality and generalization after network pruning . It is well known that training performance does not reflect these properties . So even if training performance only slightly drops after network pruning , the pruned network may lose some properties that the full network has . Minor Comments - It is mentioned that separate I/O biases the network at initialization time to not share weights , but to me it seems to only bias the input/output layers rather than the hidden layers . It would be interesting to see if shared I/O will also lead to unshared hidden weights . - Fig.13 did not mention which LSTM variant was used . - In Section 3.3 , why is increased sharing in the first layer undesirable ? It might not be possible to undo the permutation with only one layer .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . We \u2019 re glad that you were able to appreciate the novelty of our perspective as well as the thoroughness of our experimental study . Before we dive into individual responses to your specific queries and comments , we would first like to briefly comment on point ( 3 ) of your summary . In particular , the novelty of our work is not merely in showing that neural networks generalize poorly on algorithmic tasks ( which was previously shown in several other works [ Lake & Baroni , 2018 ; Saxton et al. , 2019 ] ) , but rather that the learned solution on these tasks is non-compositional , even for the training set . This provides counter evidence to the hypothesis that neural networks are unable to find an \u2018 analogical mapping \u2019 between the training and validation set . > `` Some concepts are not rigorously defined . For example , in $ P_ { \\text { specialize } } $ and $ P_ { \\text { reuse } } $ , when should two modules be considered the same ? The paper seems to define a module as a subset of weights . Does this imply that two different subsets ( potentially differ by only one element ) will correspond to two different modules ? '' This is an interesting question and indeed it would be problematic if two modules would be considered as \u201c entirely different \u201d were they to vary only by a single element . Therefore , in this work , we have treated $ P_ { \\text { specialize } } $ and $ P_ { \\text { reuse } } $ as a continuous quantity ranging from 0 ( no sharing ) to 1 ( completing sharing ) , which accordingly allows us to evaluate the _degree_ to which modularity emerges . We will update the next revision of the current submission to better emphasize this distinction . Further , and related to this , we note that throughout most of the work we do not rely on these quantities directly . Rather , we measure the change in performance when certain masks are applied , which is well defined and yields what we believe to be an easy to interpret metric . In this way , we decouple our findings from the specific amount of sharing that is observed , which is a quantity that is hard to interpret ( i.e.how much sharing is desirable on say CIFAR10 ? ) . On the contrary , it is clear how the performance should change as a result of applying different masks ( e.g if the same functionality is used twice , their weight should not be mutually exclusive , as we demonstrated in section 3.2 ) . We will further clarify our motivation for this metric in the paper . > \u201c Hence , the experiments in Section 3.2 seem meaningless \u201d Hopefully from our previous comment it is now clear that the experiments in Section 3.2 are highly meaningful . To summarize , we show that when performing two addition operations ( $ x=a+b $ and $ y=c+d $ ) , one becomes completely independent of the other . This is achieved by training a mask to select the subset of the network responsible for $ x=a+b $ , which performs well for $ x $ , but the performance of $ y $ is low . However , we noted that this behavior could also be observed if just a subset of weights is unshared . Hence , we also take the inverse of the mask trained on $ x $ ( which now excludes all the weights needed for $ x=a+b $ ) , and demonstrate how the resulting network performs well on $ y $ ( while performing poorly on $ x $ ) . This clearly demonstrates that two independent modules are learned : they correspond to two mutually exclusive sets of weights , which is a non-trivial finding . > \u201c One may argue that the weights in the input/output layer should be excluded from the module \u201d Ideally , one would want to exclude the input/output connections from the analysis . However , they are already performing a transformation of data , which makes it impossible to do so without architectural modifications ( and in that case the functionality of a module is not well defined as you noted ) . We combat this issue in two different ways : either we use shared input and output ( for the addition/multiplication task , SCAN , Math dataset , CNN ) , or we made sure that the I/O sharing does not affect the results ( for double addition , in Section 3.2 , we find two mutually exclusive sets of weights , each one capable of performing one , and only one of the two instances of addition ) ."}, "1": {"review_id": "7uVcpu-gMD-1", "review_text": "This paper studies weight modularity in neural networks ( NNs ) . In particular , given a NN trained to perform a task , a subset of weights are identified which in isolation perform well on a subtask of the original task . Such subsets are inspected to understand the extent to which they are specialized or reused across different subtasks of the original task . To identify subtask specific weights , a mask is learned that minimizes loss over a subtask when applied to the original NN 's frozen weights . This process is carried out using gradient based optimization techniques ( Adam ) . Extensive experiments are performed across various datasets and architectures . The paper concludes that while NNs seem to exhibit module specialization , they fail to exhibit reuse . # # # Strengths 1 . Understanding modularity within NNs and its relation to failures of systematic generalization is an important research direction . 2.The precise masking method proposed here is novel to the best of my knowledge . 3.The functional view of modularity has clear advantages over clustering based approaches . 4.I believe this work could be extended in interesting ways . For example , it seems like the methods presented here could naturally be extended to encourage modularity , rather than measure it . # # # Weaknesses 1 . The paper addresses a well known issue , that NNs often fail to generalize systematically . Why this occurs and potential solutions are left unaddressed . While the paper does put forward a hypothesis regarding why , that the difficulty of learning routing operation in NNs may lead current approaches to fail to discover solutions which generalize systematically , little evidence is provided to support this conclusion . As such , I believe claims such as , `` our approach can also be applied in these settings to provide additional insight regarding the underlying reason for the observed failure at systematic generalization '' should be relaxed , or clarified if there is evidence for this hypothesis presented that I am failing to connect . 2.How do we know the problem is with the full model , rather than the mask ? Recent literature ( cited in the paper ) demonstrates that performant models can be obtained even when severe weight constraints are imposed [ 1 , 2 ] . Indeed , [ 2 ] demonstrates masking alone is sufficient to adapt a NN to a completely new task . Given these observations , I 'm concerned about the validity of assuming the mask optimization procedure identifies an independent functional module in the context of the computation performed by the full model . It seems plausible that the mask performs well on the subtask given the fixed model weights , but is unrelated to the function of the full model . If this was the case , I believe a number of the conclusions in the paper would not be justified . The paper does provide some evidence this may not be occurring . For example , the double-addition experiment ( Figure 2 ) shows that inverted masks perform well on the other pair . This suggests the mask and inverse mask may correspond to distinct functionality in the full model . However , Figure 10 tells a different story , where inverse masks do not perform well on the other task . Is this due to the shared IO in the addition/multiplication task ? Is there any relation between the addition mask with shared weights removed and the inverted multiplication mask with shared weights removed ? Ideally , the paper would provide more evidence to support this assumption . Are units correlated in the masked and unmasked model ? Could the analysis here be supported by explicitly constructing a model that exhibits reuse , and verifying it can be identified ? 3.The paper argues in several places that a weight level analysis is necessary for reasoning about functional modularity , e.g. , `` without considering the contribution of individual weights it is not possible to reason about * functional * modularity . '' I believe this statement conflates two separate issues ; ( 1 ) if modules should be defined in terms of units or weights , and ( 2 ) whether modules should be defined by functionality ( as done in this work ) or using techniques such as clustering . On the latter point , I agree with the statements in the paper and believe the functional approach is more closely aligned with intuitive notions of what is meant by modularity . However , I do n't believe a weight vs unit level perspective is necessarily mutually exclusive . For example , consider a simple affine layer $ h = f ( Wx ) $ . Defining a module as a subset of elements of $ h $ would be equivalent to defining a module as a subset of rows in $ W $ . 4.Figures 2 , 10 , 14 would be much clearer if presented in a tabular format . I had to write out the results this way myself to aid understanding . 5.I found the use of a non-symmetric sharing metric extremely confusing . I spent a while pondering over Figure 1 trying to figure out what the weights were shared with since both tasks are represented in the figure . I believe this issue could be easily remedied , for example , by using something like IoU ( Jaccard index ) . The paper uses this metric in Appendix B.1 to determine the amount of sharing between different masks trained on the same network and task . 6.Permuted MNIST Experiment 1 . The paper states `` it suffices to re-train a new first layer to undo the permutation so that later layers can be reused . '' This makes sense . However , given that the procedure is `` freeze the occupied weights , '' lower level weights can not perform this simple permutation by construction since they are frozen ( I 've assumed that occupied means a non-zero mask value ) . 2.Why was the choice made to `` train masks and weights simultaneously '' in this setting ? This introduces an additional source of variation from previous experiments , and I 'm unsure what the benefit is . 7.SCAN experiments 1 . The paper states `` if the masking process removes any important weights , then the solution is pattern-recognition like instead of being based on reusable rules , confirming explanation . '' This is related to point ( 2 ) above , and I 'm not convinced this is the only possible conclusion . While the mask has identified a subnetwork whose solution is pattern-recognition like , I 'm not sure that the logical jump to conclude the model as a whole performs identically is sound . 2.Regarding the output weight analysis , I 'm also unsure why it should be the case that the final layer is sufficiently powerful for unbinding bound variables . Is n't it also possible that the mask learns to ignore those weights because it is not important for the task it is trained on ? # # # Recommendation I recommend acceptance . Although I have several concerns with the paper , I found the ideas and analysis presented very interesting . I believe the community would benefit from further discussion , scrutiny , and exploration of the ideas presented . # # # Post Author Response Period Update Most of my concerns have been addressed by the additional experiments and updated language in the latest revision . I believe the techniques and analysis presented here for assessing reuse could be an important step between observations and explanations for the failure of NNs to generalize systematically . I have raised my score accordingly . # # # Minor Issues 1 . Appendix C.2 : `` We always check the * * choosen * * hyperparameters '' = > `` We always check the * * chosen * * hyperparameters '' # # # References 1 . Arun Mallya , Dillon Davis , and Svetlana Lazebnik . Piggyback : Adapting a single network to multiple tasks by learning to mask weights . In Proceedings of the European Conference on Computer Vision ( ECCV ) , pp . 67\u201382 , 2018 . 2.Adam Gaier and David Ha . Weight agnostic neural networks . In Advances in Neural Information Processing Systems , pp . 5364\u20135378 , 2019 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . We were very impressed with the thoroughness of your review and really appreciate the effort that you put into reviewing our work , which allows us to make several improvements . Below are the individual responses to the many specific queries and comments : 1 . Thank you for pointing this out , this is mostly an oversight on our part . We will relax the claims about the insights to better reflect the fact that we can not provide definitive evidence to support our hypothesis in some cases . 2. > \u201c How do we know the problem is with the full model , rather than the mask ? .. \u201d This is a good point and although we strongly believe that the problems are on the model side ( as is also evident from the inverse masking results as you point out ) , we have devised an additional experiment to demonstrate that the masking process does not change the original function performed by the network . In particular , we will attempt to evaluate a network where only the first half of the network is masked , while the second is not . If the network works like this , it should mean that the activations produced by the first , unmasked , half are compatible with the second , masked half . Alternatively , we could calculate additional statistics of the activations , or plot the activations of the same sample with and without masking , to provide further evidence to this point . > \u201c However , Figure 10 tells a different story , where inverse masks do not perform well on the other task . Is this due to the shared IO in the addition/multiplication task ? \u201d Figure 10 tells a different story for multiple reasons . First , the IO is shared as you also noted . Second , the separation of the hidden layers is not perfect . We believe that the sharing of weights is explained more by sharing the module \u2019 s inputs/outputs than by the performed function in this case . Note , however that Figure 10 is mostly there for symmetry with the double-addition experiments , which provides additional insight . > \u201d Is there any relation between the addition mask with shared weights removed and the inverted multiplication mask with shared weights removed ? \u201d The addition mask with the shared weights removed is a subset of the inverted multiplication mask with the shared weights removed . Both of them would perform poorly because the shared weights are important for the task . An additional experiment that we could perform is to modify the inverse-mask experiment on the addition/multiplication task such that the inverted mask will be unioned together with the shared weights . This would just remove the weights exclusive for one operation . The performance on that operation should decrease , but the inverse mask should perform well on the complementary task . Personally , we don \u2019 t expect that this will provide much additional insight , but if you think that this is valuable then we are happy to run this . Please let us know in this case . 3.The paper argues for both ( 1 ) and ( 2 ) , but we are also specifically advocating for ( 1 ) when mentioning weight-level analysis . We are not arguing that unit-wise analysis necessarily gives a wrong result , but rather that it is a special case of the weight-level analysis that is conducted here , and in practice probably not enough to draw meaningful conclusions . The provided example with the affine layer can demonstrate this easily : imagine x has 4k units , while h has k. X can be partitioned to 4 equal parts , a , b , c , d . The task is to perform either h = a | b or h = c & d , depending on which inputs are present . This can be done with a single affine layer . Now if we conduct a unit-wise analysis on h , all units will equally take part in both | and & operations . In contrast , a single row of W can be divided into 2 parts , where the first half performs the | and the second the & . This is why we are arguing explicitly for ( 2 ) in this case . 4.Thank you for the feedback . We are currently looking to improve this visualization and will let you know once the paper is updated with this . We will also add tabular results to the appendix . 5.Using IoU is a good idea for these figures , thank you for the suggestion . We will change the corresponding figures in the main paper accordingly ."}, "2": {"review_id": "7uVcpu-gMD-2", "review_text": "In its current form , I feel that the paper should be disqualified because it contains some results essential to its claims in the appendix ( referenced in the second paragraph , page 5 ) . However , this can be easily addressed - thus my full review below : Summary of the paper : The paper aims to analyze if and how neural networks learn modular representations . Modularity under the paper 's definition means that the network learns representations that ( 1 ) specialize ( using different modules for different functions ) and that ( 2 ) compose in a re-usable fashion - i.e. , that functions are used in diverse tasks . To do so , the authors train different probabistic masks ( using a Gumbel-Sigmoid ) on the weights of a neural network over a series of different tasks . They incentivize these masks to be sparse by regularizing the number of `` active '' elements in a mask . They then compare masks learned for different tasks ( including simple arithmetic tasks , and permuted MNIST ) , and then compare the usage of the parameter masks over different tasks . They find that NNs learn to specialize only , without reuse . The paper continues by postulating that the reason is either that the network learned `` bad '' representations , or that the network did not learn the correct composition . They argue that results on the SCAN dataset show that the representations are of a sufficient quality , concluding that the network did not learn the correct composition . Commentary : The question the paper tries to answer is very relevant , on two levels . The first is that we do not understand how an NN learns sufficiently well . The second is that compositional modularity is a highly desireable property , and it is important to know if neural network exhibit it . Strengths : - The paper does some interesting analysis - In general , the paper is well-written , and easily understood . Unfortunately , the paper suffers from major drawbacks : - ( this is a minor point I 'm putting here to facilitate my discussion below ) The paper is not positioning itself correctly in the literature , thereby using confusing terminology While modularity is not a main focus in neural network research , there exists some meaningful research that has established some terminology . In essence , the paper talks about compositional modularity ( and combinatorial generalization ) , but does not use this terminology . This makes the paper a little difficult to follow , if one is familiar with this literature . Within this terminology , specialize would be called `` modularize '' and Preuse would be called `` compose '' , a terminology I will use in the following . - The conclusions are not convincing The core argument in the paper is that neural networks fail to modularize because they either learn insufficient representations , or because they fail to learn to compose ( to learn the `` algorithm '' required to utilize the modules correctly ) . Because the network learns re-usable representations , they modularize , and must thus fail to compose . While this is probably true , I can not help to feel a little underwhelmed . It is well known that neural networks are overparameterized ( see the `` lottery ticket '' literature ) . For this analysis , this means that it appears to be easier for the model to re-learn using the available capacity than to re-use the existing modules . This is not particularly surpsising either , because this is , at its core , overfitting : the model does not generalize ( which , in effect , is a `` softer '' way to re-use ) , but instead learns something akint to a separate function for different inputs . What the paper does not investigate is why a neural network not re-uses capabilities , even those should be a good fit for a problem . This would be a really interesting analysis , one which I would very strongly argue for acceptance in any venue . - The novelty of some parts is overstated This is particularly true for using binary masks for multi-task like learning . See `` Bengio , E. , Bacon , P. L. , Pineau , J. , & Precup , D. ( 2015 ) . Conditional computation in neural networks for faster models . arXiv preprint arXiv:1511.06297 . '' Additionally , the paper does not sufficiently relate their insights to the ( cited ) work around inducing modularity and compositionaly in networks , some of which does already come to similar results .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . Although we were initially glad to see that you found the presented analysis interesting , we were disappointed at receiving a final rating of a 3 . Ignoring your comments about results in the appendix ( which , as you also state , are easily addressed ) and terminology ( which you state is minor ) for now , it appears that there are two reasons that led you to this score : 1 . You argue that \u201c The conclusions are not convincing \u201d , since \u201c it is well known at neural networks are overparameterized \u201d , and that therefore the presented results are \u201c not surprising \u201d . You suggest that we investigate an alternative research question : \u201c why a neural network not re-uses capabilities , even those should be a good fit for a problem \u201d . 2.You argue that the \u201c novelty of some parts is overstated \u201d , which you believe to be \u201c particularly true for the binary masks for multi-task learning \u201d . Additionally , you feel that \u201c the paper does not sufficiently relate their insights to the ( cited ) work around inducing modularity and compositionality in networks , some of which does already come to similar results. \u201d Regarding ( 1 ) , we would like to emphasize the distinction between results not being \u201c convincing \u201d and them being \u201c expected \u201d . Although you state that our analysis is not \u201c convincing \u201d , you yourself point out how our findings are \u201c probably true \u201d and do not offer any concrete remarks or evidence that suggest that the presented analysis is not thorough or even invalid . As to whether the results can be \u201c expected \u201d , clearly this is debatable . When we started this work a year ago it was far from obvious that these results would follow from the \u201c lottery ticket hypothesis \u201d and arguably it still isn \u2019 t without first considering the evidence presented in this work . Indeed , to the best of our knowledge there has been no prior analysis that studies such effects in neural networks , which we argue makes this work a valid contribution , irrespective of whether , in hindsight , this may have been expected . In judging the significance of our results we therefore ask that you also consider the comments from the other three reviewers , none of which suggested that these results are obvious or logically follow , which we argue indicates that our results are significant . Further , we argue that it is important to consider the understanding of the broader community ( as opposed to expert reviewers ) regarding these issues in neural networks when judging the significance of our findings . It is our hope that you will reconsider . Regarding ( 2 ) , it was never our intention to claim that multi-task learning with binary masks is a novel approach that we propose . The sentence \u201c Typical approaches revolve around freezing used weights when a new task is added ( Fernando et al. , 2017 ; Mallya & Lazebnik , 2018 ; Golkar et al. , 2019 ) . \u201d was meant to clarify this . However , we now realize that it may not be obvious to the reader that this implies that these methods use a form of masking . This is an oversight on our part and we thank the reviewer for pointing this out . The next revision will clarify this . We also thank the reviewer for pointing out Bengio et al. , ( 2015 ) , which we were unaware of and which we will include in the next revision as related work . More generally , regarding novelty for the multi-task setting , we note that our analysis of the behavior of the multi-task learner is the novel part , which is what this experiment contributes . Regarding the second part of ( 2 ) , we believe that we have sufficiently compared our work to the many of the related works on modularity . To the best of our knowledge these works each focus on different aspects of this problem and do not not analyze modules based on their functionality , which is a defining characteristic of our work . If you disagree , could you please elaborate further on related work that we are not comparing to and that provides similar results ? We will now provide individual responses to the remaining minor comments : > \u201c In its current form , I feel that the paper should be disqualified because it contains some results essential to its claims in the appendix ( referenced in the second paragraph , page 5 ) \u201d The main results and conclusions from the second paragraph , page 5 are shown in Figure 2 . Figures 13 and 14 are only referenced to support our claims . We agree that it would be better if we can present it in the main paper , however considering the page limit , this was challenging . We believe that with the extra page , we can do a better job at this ."}, "3": {"review_id": "7uVcpu-gMD-3", "review_text": "The paper investigates the functional modularity of neural networks using a simple yet efficient end-to-end method . The paper is well written and clearly articulates a contribution to the literature . The proposed method is intuitive and straightforward . The experimental evidence is provided for both synthetic , language , and image classification tasks . Most of the related works are cited . Concerns : The biggest concern I had is whether the conclusion reached in the paper is invariant to different neural network architectures , the size of the network , and the complexity of the task . As depicted in Figure 6 ( a ) , there is a huge difference in the relative drop in performance for simple CNN , simple CNN without drouput and ResNet-110 . It seems that a larger and more complex network tends to not sharing weights . Besides , the paper uses accuracy drop after masking the weights as the main metric , which is related to the number of masked weights . It might be better to learn the binary mask for each subtask with a certain accuracy objective ( e.g. , less than 1-2 % lower than the original network ) for each subtask and compare the learned mask of different subtasks . In addition , it is useful to see the results on a more complex task such as ImageNet . In ImageNet , there exist many similar subtasks as many categories of the images are actually belong to one big category . I am wondering whether these subtasks can share the weights . Minor comments : The paper claims the advantage of using Gumbel-Sigmoid than a simple threshold function . However , the state-of-the-art binarized neural networks are trained using a simple sign function with a straight-through estimator . Is there a significant difference ( e.g. , the stability of the training ) in training the binary mask with the Gumbel-Sigmoid and threshold function ? Reasons for score : I vote for accepting . I like the finding that most neural networks have non-overlapped functional modules . However , I still have some concerns about the generality of the conclusion , and also the number of shared weights is not calculated with respect to a uniform accuracy requirement . I would consider raising my score if the authors can address my concerns .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our work in detail and provide valuable feedback . We \u2019 re glad that you were able to appreciate the effectiveness of the proposed method as well as our findings . Below are the individual responses to your specific queries and comments . > \u201c The biggest concern I had is whether the conclusion reached in the paper is invariant to different neural network architectures , the size of the network , and the complexity of the task. \u201d We understand your concern . When empirically studying properties of neural networks there will always remain the question of whether they will hold on yet another dataset or using yet another architecture . Unfortunately , reality is that we can only test so many things . In the current submission we present consistent observations across what we believe to be a wide variety of neural networks : FNNs , LSTMS , CNNs , ResNets , Transformers , and data sets : synthetic data sets , datasets for image classification , algorithmic language tasks and solving math problems , which makes it reasonable to assume that similar behavior can be observed in ( slightly ) different settings . Moreover , the considered architectures contain standardized architectures that are popular across the deep learning literature , and the data sets include several standard benchmarks . Therefore , although we can not with absolute certainty say that these results will extend to other data sets ( or perhaps in other domains ) or when considering certain custom architectures , we strongly believe that the insights presented in this work will be of use to a significant fraction of the neural networks community . We appreciate that you voted for acceptance even given this concern . > \u201c It seems that a larger and more complex network tends to not sharing weights. \u201d Larger networks are certainly more susceptible to not sharing weights . This can also be seen in Figure 9 . As the network size shrinks , the network must share weights in order to have enough capacity for solving the task . However , note that sharing is not happening as a result of performing similar functionality as is evident from the findings in our paper . Moreover , in most cases , overparametrized networks tend to work better . For the SCAN and Math experiments , we used the default architecture with default hyperparameters of the original papers . The simple CNN is a reduced version of a popular architectural choice used for classifying MNIST . We emphasize that although the amount of sharing varies , our goal was not to show what fraction is _generally_ shared . Rather we demonstrated that the networks are biased against sharing based on _functionality_ , which is consistently observed . We will improve our discussion of these results in the revision to better clarify this . > \u201c Besides , the paper uses accuracy drop after masking the weights as the main metric , which is related to the number of masked weights \u201d It is indeed related , but not ( directly ) caused by it . Weight removal is the result of them not being needed to perform the target task . Our goal has always been to measure modularity in a way that depends least on the exact amount of weight sharing . For example , this quantity may cause problems when encountering alternative weight configurations , especially when used with dropout : Some weights might be marginally more important and kept in some instances and not in the others as a result of the masking process \u2019 s stochasticity . Further , for many tasks it is unclear what a good amount of weight sharing is ( see also our reply above to R3 regarding this ) . In contrast , the desired behavior when applying different masks is more easily understood . This led us to use accuracy for our experiments , since in this case if the experiments are correctly set up , the precise amount of sharing is irrelevant . The effects on accuracy are directly measurable and interpretable . We hope that this explanation helps motivate our design choices regarding the considered metric . Should you have additional concerns , then please let us know so we can clarify further . > \u201c It is useful to see the results on a more complex task such as ImageNet \u201d We briefly mentioned in the paper that we don \u2019 t expect the CNN experiments to directly transfer to more complex datasets with a higher number of classes . However , the experiment you propose sounds very interesting . We are currently working on conducting such an experiment with bigger category groups removed initially ( for example remove all dogs ) . However , we note that since Imagenet is very expensive to train on , we might not be able to finish these experiments in the rebuttal period ."}}