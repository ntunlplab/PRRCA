{"year": "2019", "forum": "rygkk305YQ", "title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "decision": "Accept (Poster)", "meta_review": "This is an ambitious paper tackling the important and timely problem of controlling non-annotated attributes in generated speech. \n\nThe reviewers had mixed opinions about the results. R1 asks for more convincing exposition of results but, nevertheless, acknowledging that it is difficult to evaluate TTS systems systematically. Besides, R2 and R3 find the results good. \n\nJudging from the reviews and previous work, this paper does not seem to be very novel, although it certainly has intriguing new elements. Furthermore, it constitutes a mature piece of work.  \n", "reviews": [{"review_id": "rygkk305YQ-0", "review_text": "The authors describe the conditioned GAN model to generate speaker conditioned Mel spectra. They augment the z-space corresponding to the identification with latent variables that allow a richer set of produced audio. In a way this is like a partially conditioned model that has \"extra\" degrees of freedom. It looks that the \"latent\" variables are just concaneted to the \"original\" set of z-values (altough with particular conditions to maximize independence). The conditioning of the z-space has originality in it and may provide interesting to the audience. Ultimately one coud think about z-space direction being totally mapped to specific features of the produced signal. Also, I am curious to know how the Mel spectra are used to produce the actual sound wave - as the phase information is not present if utilizing only the spectral amplitude. Very often this leads to suboptimal generation, and the remedy is to use the time domain like in ( https://arxiv.org/ftp/arxiv/papers/1810/1810.05319.pdf). However, in this case the audio samples show a pretty nice generation of sound. However, it is not really end to end. The manuscript has some curious decisios in its concepts - I do not see the architecture really hiearchial, nor end to end. I would prefer modifications on the paper that concentrate on the truly novel features. The paper is clear, well written and done with high ambition, from data set utilization to novel architetures to human quality panels. Results are good and interesting. NEW: The authors have addressed the concerns I had with the manuscript. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the great feedback . Below are the itemized responses regarding each comment . We will also incorporate these into the revised version . Due to the `` max 5000 characters '' limit , we break down our response into multiple comments . Re : The authors describe the conditioned GAN model to generate speaker conditioned Mel spectra . They augment the z-space corresponding to the identification with latent variables that allow a richer set of produced audio . In a way this is like a partially conditioned model that has `` extra '' degrees of freedom . It looks that the `` latent '' variables are just concaneted to the `` original '' set of z-values ( altough with particular conditions to maximize independence ) . The conditioning of the z-space has originality in it and may provide interesting to the audience . Ultimately one coud think about z-space direction being totally mapped to specific features of the produced signal . Ans : - The proposed model is not a conditional GAN , but is in fact a conditional VAE which uses a Gaussian mixture prior for latent attribute representations ( z_l ) , and a Gaussian mixture prior for speaker attribute representations ( z_o ) where each speaker corresponds to one mixture component . While the speaker identity ( y_o ) is given , the latent attribute representations and speaker attribute representations ( z_l and z_o ) are both continuous latent variables , the posteriors of which are estimated from an utterance by neural networks , denoted in Figure 8 as latent encoder and observed encoder , respectively . - We are not sure if the reviewer is referring to z_o as the \u201c original set of z-values \u201d and z_l as the \u201c latent variables. \u201d As mentioned above , during training , both z_o and z_l used for generating the target speech X are latent variables inferred directly from the spectrogram , as shown in Eq . ( 4 ) .This is different from the typical speaker embedding-based method , which uses a fixed embedding learned for each speaker stored in an embedding table . From the perspective of modeling speakers , our models has the advantage that it is capable of inferring the z_o for an unseen speaker and imitate the voice of that unseen speaker , while the speaker embedding method can not . Re : Also , I am curious to know how the Mel spectra are used to produce the actual sound wave - as the phase information is not present if utilizing only the spectral amplitude\u2026 . However , in this case the audio samples show a pretty nice generation of sound . However , it is not really end to end . Ans : - To synthesize waveforms from Mel spectrograms , we closely follow the Tacotron 2 ( Shen et al , 2018 ) framework , which trains a neural network-based vocoder to predict waveforms conditioning on the Mel-spectrograms generated by GMVAE-Tacotron . In this work , we use WaveRNN ( Kalchbrenner et al , 2018 ) as the neural vocoder , as described in Section 2.4 . We will update the text to further clarify this . - We agree with the reviewer about the wording and will delete the term \u201c end-to-end \u201d to reflect the use of neural vocoder . In addition , we would like to restate that the focus of this paper is the ability to control latent attributes , which is achieved by the GMVAE-Tacotron module , and having the WaveRNN vocoder only improves the audio fidelity , similar to the observation made in GST ( Wang et al. , 2018 ) . For example , we observed that the WADA-SNR results have no difference when using Griffin-Lim or WaveRNN for vocoding ."}, {"review_id": "rygkk305YQ-1", "review_text": "This paper proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitates fine-grained control over various attributes including noise level, speaker rate etc. Detailed comments: i) This work is closely related to Akuzawa et al. (2018). The difference is not properly discussed. ii) In the abstract, \u201cend-to-end text-to-speech\u201d is an unfortunate claim, because the proposed system has two separately trained component: 1) a text to mel-spectrogram model based on Tacotron and, 2) a WaveRNN for waveform synthesis. In ASR, it\u2019s absolutely fine to claim a spectrogram to text model as a end-to-end system, because the wave to spectrogram step is trivial. In TTS, waveform synthesis is a very crucial step and largely determines the final naturalness results. iii) In experiment, one need include the MOS score of ground truth for comparison or debiasing. iv) Did you try different values of D other than 16? When you check the meaning of different dimensions, how many dimensions are meaningful? How many are meaningless, or even just dummy? In summary, pros: - A good work with impressive results. cons: - Related work need to be properly discussed. - Doesn\u2019t include the MOS of ground truth. - Moderate novelty. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the great feedback . Below are the itemized responses regarding each comment . We will also incorporate these into the revised version . Due to the `` max 5000 characters '' limit , we break down our response into multiple comments . Re : This work is closely related to Akuzawa et al . ( 2018 ) .The difference is not properly discussed . Ans : - There are two major differences from the latent space modeling perspectives , both of which have large effects on the interpretability of the latent representation and on the ability to control individual attributes . In addition , there is a difference in terms of the adopted neural network architecture , which we think is of less importance . We will update the text to better describe these differences . -- First , we model latent attributes using a * mixture distribution * , which allows automatic discovery of latent attribute clusters . This design makes the resulting latent space much more interpretable , as data-driven clustering leads to meaningful structure . Specifically , we can 1 ) quickly analyze clusters to get a sense of what they correspond to ( similar to GST ) , as in multispeaker experiments ( Section 4.1 ) , and 2 ) easily analyze inter-cluster variance to identify most distinctive attributes , which e.g.allows us to easily identify the noise dimension ( Section 4.2.2 ) . We demonstrate how identifying this structure can be easily used to train a model on noisy found-data that can consistently synthesize clean speech , which has not been done before to the best of our knowledge . Such interpretability can not be achieved using an isotropic Gaussian prior as in Akuzawa et al. , ( 2018 ) . -- Second , we describe an extension of the proposed framework to additionally model speaker attributes with a second mixture distribution , where each speaker corresponds to one mixture component . This formulation learns disentangled speaker and latent attribute representations , which can be applied to one-shot learning to imitating the voice of speakers previously unseen during training ( Section 4.4 and Appendix G.2 ) . This functionality was not provided in Akuzawa et al . ( 2018 ) either . We will include this discussion in the next version . -- Third , regarding the neural network architecture , while Akuzawa et al . ( 2018 ) uses the VoiceLoop ( Taigman et al. , 2018 ) architecture for the synthesizer , our proposed model is based on Tacotron 2 ( Shen et al. , 2018 ) . However , we would like to emphasize that such a choice is not the major difference in terms of the ability to control latent attributes , and our approach can also be applied to the VoiceLoop architecture . - Asides from the three differences mentioned above , we would also like to point out that our model synthesizes speech with higher quality than the samples presented on the demo page from Akuzawa et al . ( 2018 ) ( https : //akuzeee.github.io/VAELoopDemo/ ) . Re : ii ) In the abstract , \u201c end-to-end text-to-speech \u201d is an unfortunate claim , because the proposed system has two separately trained component : 1 ) a text to mel-spectrogram model based on Tacotron and , 2 ) a WaveRNN for waveform synthesis . In ASR , it \u2019 s absolutely fine to claim a spectrogram to text model as a end-to-end system , because the wave to spectrogram step is trivial . In TTS , waveform synthesis is a very crucial step and largely determines the final naturalness results . Ans : We agree with the reviewer and will delete the term \u201c end-to-end \u201d to reflect that vocoding from Mel-spectrogram is done with a separate WaveRNN module . In addition , we would like to restate that the focus of this paper is the ability to control latent attributes , which is achieved by the GMVAE-Tacotron module , and having the WaveRNN vocoder only improves the audio fidelity , similar to the observation made in GST ( Wang et al. , 2018 ) . For example , we observed that the WADA-SNR metric has no difference when using Griffin-Lim or WaveRNN for vocoding . Re : iii ) In experiment , one need include the MOS score of ground truth for comparison or debiasing . Ans : We are now evaluating the MOS score of ground truth , and will reply once the evaluation is finished . EDIT : We have obtained the ground truth MOS scores . Please see the comment below ."}, {"review_id": "rygkk305YQ-2", "review_text": "Quality: This submission claims to present a model that can control non-annotated attributes such as speaking style, accent, background noise, etc. Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed. For example a reliable numerical evidence is needed on page 4 following \"We also found...\", page 5 following \"We discovered....\", page 5 following \"It clearly presents...\", page 5 following \"Drawing samples...\" evidence is given only for 1 dimension, page 6 following \"Figure 7(b)...\". Clarity: The model is simple though the exact form and nature of observed and latent class variables could be made more explicit. Including how they are computed/initialised/set. What are different modes using the proposed model? Why both negative results are in the appendix? Originality: moderately Significance: moderately ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the great feedback . Below are the itemized responses regarding each comment . We will also incorporate these into the revised version . Due to the `` max 5000 characters '' limit , we break down our response into multiple comments . Re : Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed . Ans : - Identifying reliable objective quality metrics for TTS , and indeed generative models more generally , is an open problem . This is especially true for high level style concepts such as accent , or labels related to emotional valence or arousal . It is common practice across papers on generating real data to demonstrate the ability to independently control attributes qualitatively , using generated samples , providing quantitative evaluation on few attributes that are easy to estimate due to similar difficulties , e.g.Glow ( Kingma et al. , 2018 ) . Qualitative evaluation similar to what we provide has been the standard in other recent work on controllable TTS models such as GST ( Wang et al. , 2018 ) and VAE-Loop ( Akuzawa et al. , 2018 ) . - In addition , it is difficult to provide comprehensive quantitative evaluation because ( 1 ) the set of unlabeled attributes we are able to measure is limited to those for which reliable systems exist to estimate them from the speech signal , as in e.g.pitch tracking . It is unclear how to convincingly measure the level of excitement , deepness , or roughness of a voice for style control . ( 2 ) Also , estimation of attributes such as pitch can be prone to error . For example , pitch estimation of rough voices is not accurate because the harmonic patterns are not as clear as normal voices ( See the bottom row in Figure 16 ) . Even in cases where it is straightforward to report statistics to show the attribute we claim to be affected does indeed vary , simple metrics alone do not show the whole picture since they do not verify that other attributes remain the same . However , such * independent * control of an individual attribute is easily demonstrated qualitatively . To circumvent the difficulty , we provide multiple samples on the demo page to demonstrate that control over each attribute is invariant to other decoder input . Re : page 5 following `` It clearly presents the samples ( in fact , all the samples ) drawn from components in group one were noisy , while the samples drawn from the other components were clean '' Ans : - Quantitative evaluation on the ability to control noise level is presented in Figure 5 , Figure 13 , and Table 1 . -- In Figure 5 , we plot the WADA-SNR with respect to different values for the noise level dimension , which demonstrates the ability to control noise level through that dimension . In Figure 13 , we plot the distribution of the noise level dimension for each mixture component , showing two groups of components that are well separated in this dimension . We can conclude that the group of mixture components on the right in Figure 13 generate clean speech because they have high probability of generating samples that correspond to high SNRs . In contrast , the group on the left in Figure 13 generates noisy speech . -- In Table 1 , we report WADA-SNR when conditioning on the mean of a clean component , verifying that the clean component can consistently generate clean speech ."}], "0": {"review_id": "rygkk305YQ-0", "review_text": "The authors describe the conditioned GAN model to generate speaker conditioned Mel spectra. They augment the z-space corresponding to the identification with latent variables that allow a richer set of produced audio. In a way this is like a partially conditioned model that has \"extra\" degrees of freedom. It looks that the \"latent\" variables are just concaneted to the \"original\" set of z-values (altough with particular conditions to maximize independence). The conditioning of the z-space has originality in it and may provide interesting to the audience. Ultimately one coud think about z-space direction being totally mapped to specific features of the produced signal. Also, I am curious to know how the Mel spectra are used to produce the actual sound wave - as the phase information is not present if utilizing only the spectral amplitude. Very often this leads to suboptimal generation, and the remedy is to use the time domain like in ( https://arxiv.org/ftp/arxiv/papers/1810/1810.05319.pdf). However, in this case the audio samples show a pretty nice generation of sound. However, it is not really end to end. The manuscript has some curious decisios in its concepts - I do not see the architecture really hiearchial, nor end to end. I would prefer modifications on the paper that concentrate on the truly novel features. The paper is clear, well written and done with high ambition, from data set utilization to novel architetures to human quality panels. Results are good and interesting. NEW: The authors have addressed the concerns I had with the manuscript. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the great feedback . Below are the itemized responses regarding each comment . We will also incorporate these into the revised version . Due to the `` max 5000 characters '' limit , we break down our response into multiple comments . Re : The authors describe the conditioned GAN model to generate speaker conditioned Mel spectra . They augment the z-space corresponding to the identification with latent variables that allow a richer set of produced audio . In a way this is like a partially conditioned model that has `` extra '' degrees of freedom . It looks that the `` latent '' variables are just concaneted to the `` original '' set of z-values ( altough with particular conditions to maximize independence ) . The conditioning of the z-space has originality in it and may provide interesting to the audience . Ultimately one coud think about z-space direction being totally mapped to specific features of the produced signal . Ans : - The proposed model is not a conditional GAN , but is in fact a conditional VAE which uses a Gaussian mixture prior for latent attribute representations ( z_l ) , and a Gaussian mixture prior for speaker attribute representations ( z_o ) where each speaker corresponds to one mixture component . While the speaker identity ( y_o ) is given , the latent attribute representations and speaker attribute representations ( z_l and z_o ) are both continuous latent variables , the posteriors of which are estimated from an utterance by neural networks , denoted in Figure 8 as latent encoder and observed encoder , respectively . - We are not sure if the reviewer is referring to z_o as the \u201c original set of z-values \u201d and z_l as the \u201c latent variables. \u201d As mentioned above , during training , both z_o and z_l used for generating the target speech X are latent variables inferred directly from the spectrogram , as shown in Eq . ( 4 ) .This is different from the typical speaker embedding-based method , which uses a fixed embedding learned for each speaker stored in an embedding table . From the perspective of modeling speakers , our models has the advantage that it is capable of inferring the z_o for an unseen speaker and imitate the voice of that unseen speaker , while the speaker embedding method can not . Re : Also , I am curious to know how the Mel spectra are used to produce the actual sound wave - as the phase information is not present if utilizing only the spectral amplitude\u2026 . However , in this case the audio samples show a pretty nice generation of sound . However , it is not really end to end . Ans : - To synthesize waveforms from Mel spectrograms , we closely follow the Tacotron 2 ( Shen et al , 2018 ) framework , which trains a neural network-based vocoder to predict waveforms conditioning on the Mel-spectrograms generated by GMVAE-Tacotron . In this work , we use WaveRNN ( Kalchbrenner et al , 2018 ) as the neural vocoder , as described in Section 2.4 . We will update the text to further clarify this . - We agree with the reviewer about the wording and will delete the term \u201c end-to-end \u201d to reflect the use of neural vocoder . In addition , we would like to restate that the focus of this paper is the ability to control latent attributes , which is achieved by the GMVAE-Tacotron module , and having the WaveRNN vocoder only improves the audio fidelity , similar to the observation made in GST ( Wang et al. , 2018 ) . For example , we observed that the WADA-SNR results have no difference when using Griffin-Lim or WaveRNN for vocoding ."}, "1": {"review_id": "rygkk305YQ-1", "review_text": "This paper proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitates fine-grained control over various attributes including noise level, speaker rate etc. Detailed comments: i) This work is closely related to Akuzawa et al. (2018). The difference is not properly discussed. ii) In the abstract, \u201cend-to-end text-to-speech\u201d is an unfortunate claim, because the proposed system has two separately trained component: 1) a text to mel-spectrogram model based on Tacotron and, 2) a WaveRNN for waveform synthesis. In ASR, it\u2019s absolutely fine to claim a spectrogram to text model as a end-to-end system, because the wave to spectrogram step is trivial. In TTS, waveform synthesis is a very crucial step and largely determines the final naturalness results. iii) In experiment, one need include the MOS score of ground truth for comparison or debiasing. iv) Did you try different values of D other than 16? When you check the meaning of different dimensions, how many dimensions are meaningful? How many are meaningless, or even just dummy? In summary, pros: - A good work with impressive results. cons: - Related work need to be properly discussed. - Doesn\u2019t include the MOS of ground truth. - Moderate novelty. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the great feedback . Below are the itemized responses regarding each comment . We will also incorporate these into the revised version . Due to the `` max 5000 characters '' limit , we break down our response into multiple comments . Re : This work is closely related to Akuzawa et al . ( 2018 ) .The difference is not properly discussed . Ans : - There are two major differences from the latent space modeling perspectives , both of which have large effects on the interpretability of the latent representation and on the ability to control individual attributes . In addition , there is a difference in terms of the adopted neural network architecture , which we think is of less importance . We will update the text to better describe these differences . -- First , we model latent attributes using a * mixture distribution * , which allows automatic discovery of latent attribute clusters . This design makes the resulting latent space much more interpretable , as data-driven clustering leads to meaningful structure . Specifically , we can 1 ) quickly analyze clusters to get a sense of what they correspond to ( similar to GST ) , as in multispeaker experiments ( Section 4.1 ) , and 2 ) easily analyze inter-cluster variance to identify most distinctive attributes , which e.g.allows us to easily identify the noise dimension ( Section 4.2.2 ) . We demonstrate how identifying this structure can be easily used to train a model on noisy found-data that can consistently synthesize clean speech , which has not been done before to the best of our knowledge . Such interpretability can not be achieved using an isotropic Gaussian prior as in Akuzawa et al. , ( 2018 ) . -- Second , we describe an extension of the proposed framework to additionally model speaker attributes with a second mixture distribution , where each speaker corresponds to one mixture component . This formulation learns disentangled speaker and latent attribute representations , which can be applied to one-shot learning to imitating the voice of speakers previously unseen during training ( Section 4.4 and Appendix G.2 ) . This functionality was not provided in Akuzawa et al . ( 2018 ) either . We will include this discussion in the next version . -- Third , regarding the neural network architecture , while Akuzawa et al . ( 2018 ) uses the VoiceLoop ( Taigman et al. , 2018 ) architecture for the synthesizer , our proposed model is based on Tacotron 2 ( Shen et al. , 2018 ) . However , we would like to emphasize that such a choice is not the major difference in terms of the ability to control latent attributes , and our approach can also be applied to the VoiceLoop architecture . - Asides from the three differences mentioned above , we would also like to point out that our model synthesizes speech with higher quality than the samples presented on the demo page from Akuzawa et al . ( 2018 ) ( https : //akuzeee.github.io/VAELoopDemo/ ) . Re : ii ) In the abstract , \u201c end-to-end text-to-speech \u201d is an unfortunate claim , because the proposed system has two separately trained component : 1 ) a text to mel-spectrogram model based on Tacotron and , 2 ) a WaveRNN for waveform synthesis . In ASR , it \u2019 s absolutely fine to claim a spectrogram to text model as a end-to-end system , because the wave to spectrogram step is trivial . In TTS , waveform synthesis is a very crucial step and largely determines the final naturalness results . Ans : We agree with the reviewer and will delete the term \u201c end-to-end \u201d to reflect that vocoding from Mel-spectrogram is done with a separate WaveRNN module . In addition , we would like to restate that the focus of this paper is the ability to control latent attributes , which is achieved by the GMVAE-Tacotron module , and having the WaveRNN vocoder only improves the audio fidelity , similar to the observation made in GST ( Wang et al. , 2018 ) . For example , we observed that the WADA-SNR metric has no difference when using Griffin-Lim or WaveRNN for vocoding . Re : iii ) In experiment , one need include the MOS score of ground truth for comparison or debiasing . Ans : We are now evaluating the MOS score of ground truth , and will reply once the evaluation is finished . EDIT : We have obtained the ground truth MOS scores . Please see the comment below ."}, "2": {"review_id": "rygkk305YQ-2", "review_text": "Quality: This submission claims to present a model that can control non-annotated attributes such as speaking style, accent, background noise, etc. Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed. For example a reliable numerical evidence is needed on page 4 following \"We also found...\", page 5 following \"We discovered....\", page 5 following \"It clearly presents...\", page 5 following \"Drawing samples...\" evidence is given only for 1 dimension, page 6 following \"Figure 7(b)...\". Clarity: The model is simple though the exact form and nature of observed and latent class variables could be made more explicit. Including how they are computed/initialised/set. What are different modes using the proposed model? Why both negative results are in the appendix? Originality: moderately Significance: moderately ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the great feedback . Below are the itemized responses regarding each comment . We will also incorporate these into the revised version . Due to the `` max 5000 characters '' limit , we break down our response into multiple comments . Re : Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed . Ans : - Identifying reliable objective quality metrics for TTS , and indeed generative models more generally , is an open problem . This is especially true for high level style concepts such as accent , or labels related to emotional valence or arousal . It is common practice across papers on generating real data to demonstrate the ability to independently control attributes qualitatively , using generated samples , providing quantitative evaluation on few attributes that are easy to estimate due to similar difficulties , e.g.Glow ( Kingma et al. , 2018 ) . Qualitative evaluation similar to what we provide has been the standard in other recent work on controllable TTS models such as GST ( Wang et al. , 2018 ) and VAE-Loop ( Akuzawa et al. , 2018 ) . - In addition , it is difficult to provide comprehensive quantitative evaluation because ( 1 ) the set of unlabeled attributes we are able to measure is limited to those for which reliable systems exist to estimate them from the speech signal , as in e.g.pitch tracking . It is unclear how to convincingly measure the level of excitement , deepness , or roughness of a voice for style control . ( 2 ) Also , estimation of attributes such as pitch can be prone to error . For example , pitch estimation of rough voices is not accurate because the harmonic patterns are not as clear as normal voices ( See the bottom row in Figure 16 ) . Even in cases where it is straightforward to report statistics to show the attribute we claim to be affected does indeed vary , simple metrics alone do not show the whole picture since they do not verify that other attributes remain the same . However , such * independent * control of an individual attribute is easily demonstrated qualitatively . To circumvent the difficulty , we provide multiple samples on the demo page to demonstrate that control over each attribute is invariant to other decoder input . Re : page 5 following `` It clearly presents the samples ( in fact , all the samples ) drawn from components in group one were noisy , while the samples drawn from the other components were clean '' Ans : - Quantitative evaluation on the ability to control noise level is presented in Figure 5 , Figure 13 , and Table 1 . -- In Figure 5 , we plot the WADA-SNR with respect to different values for the noise level dimension , which demonstrates the ability to control noise level through that dimension . In Figure 13 , we plot the distribution of the noise level dimension for each mixture component , showing two groups of components that are well separated in this dimension . We can conclude that the group of mixture components on the right in Figure 13 generate clean speech because they have high probability of generating samples that correspond to high SNRs . In contrast , the group on the left in Figure 13 generates noisy speech . -- In Table 1 , we report WADA-SNR when conditioning on the mean of a clean component , verifying that the clean component can consistently generate clean speech ."}}