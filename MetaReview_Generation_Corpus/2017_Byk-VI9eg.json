{"year": "2017", "forum": "Byk-VI9eg", "title": "Generative Multi-Adversarial Networks", "decision": "Accept (Poster)", "meta_review": "Using an ensemble in the discriminator portion of a GAN is a sensible idea, and it is well explored and described in this paper. Further clarification and exploration of how the multiple discriminators are combined (max versus averaging versus weighted averaging) would be good. The results are fairly strong, across a variety of datasets.", "reviews": [{"review_id": "Byk-VI9eg-0", "review_text": "The paper extends the GAN framework to accommodate multiple discriminators. The authors motivate this from two points of view: (1) Having multiple discriminators tackle the task is equivalent to optimizing the value function using random restarts, which can potentially help optimization given the nonconvexity of the value function. (2) Having multiple discriminators can help overcome the optimization problems arising when a discriminator is too harsh a critic. A generator receiving signal from multiple discriminators is less likely to be receiving poor gradient signal from all discriminators. The paper's main idea looks straightforward to implement in practice and makes for a good addition to the GAN training toolbelt. I am not very convinced by the GAM (and by extension the GMAM) evaluation metric. Without evidence that the GAN game is converging (even approximately), it is hard to make the case that the discriminators tell something meaningful about the generators with respect to the data distribution. In particular, it does not inform on mode coverage or probability mass misallocation. The learning curves (Figure 3) look more convincing to me: they provide good evidence that increasing the number of discriminators has a stabilizing effect on the learning dynamics. However, it seems like this figure along with Figure 4 also show that the unmodified generator objective is more stable even with only one discriminator. In that case, is it even necessary to have more than one discriminator to train the generator using an unmodified objective? Overall, I think the ideas presented in this paper show good potential, but I would like to see an extended analysis in the line of Figures 3 and 4 for more datasets before I think it is ready for publication. UPDATE: The rating has been revised to a 7 following discussion with the authors.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and the insights ! We agree that the GMAM metric is not an absolute score for evaluating GANs . However , we do feel that it is an appropriate method to compare GAN models against each other . It is also an unsupervised method to evaluate GANs that are not limited to the domain of images . More detailed results for CIFAR and MNIST using the GMAM metric are included in the Appendix to show this . We have also included Inception scores for the CIFAR dataset in the Appendix to show that the GMAM metric compares well with how the score evaluates Generated images . Figure 3 and Figure 4 plot the dynamics of the different models while learning . With multiple discriminators it is evident that variance in the Generator objective is reduced and the convergence to equilibrium is achieved up to twice as fast . Further , the using the original unmodified objective with a single discriminator leads to saturation early on in the training process with the generator not getting meaningful signals . Using multiple discriminators mitigates this issue as well . Figures 10 and 11 in the Appendix plot the same dynamics as Figures 3 and 4 on the CelebA dataset . Here training with the original unmodified objective and a single discriminator was not possible due to the saturation problem , while with multiple discriminators we could use the original objective . For more empirical results , we have shall include plots similar to figures 3 and 4 ( and 10 , 11 ) for the CIFAR dataset as well . We shall also include results on one more dataset soon ."}, {"review_id": "Byk-VI9eg-1", "review_text": "This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect. I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We did not compare against GAN with DAE because it is a parallel submission in this conference . However , that paper contains Inception scores for their model on CIFAR-10 which can be directly compared against our model ( SEE TABLE 6 IN APPENDIX A.3 ) ."}, {"review_id": "Byk-VI9eg-2", "review_text": "In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator. My main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a \u2019soft-discriminator\u2019. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier. Overall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and the feedback ! Figure 1 was meant to depict the idea of multiple Discriminators providing feedback to the Generator , with the caption giving an example by specifying F : = max in section 3.1 . We will modify the caption so that the complete idea is more evident and less misleading . We have also modified the layout of the paper to avoid the misunderstanding that was pointed out . We will be uploading the same soon ."}], "0": {"review_id": "Byk-VI9eg-0", "review_text": "The paper extends the GAN framework to accommodate multiple discriminators. The authors motivate this from two points of view: (1) Having multiple discriminators tackle the task is equivalent to optimizing the value function using random restarts, which can potentially help optimization given the nonconvexity of the value function. (2) Having multiple discriminators can help overcome the optimization problems arising when a discriminator is too harsh a critic. A generator receiving signal from multiple discriminators is less likely to be receiving poor gradient signal from all discriminators. The paper's main idea looks straightforward to implement in practice and makes for a good addition to the GAN training toolbelt. I am not very convinced by the GAM (and by extension the GMAM) evaluation metric. Without evidence that the GAN game is converging (even approximately), it is hard to make the case that the discriminators tell something meaningful about the generators with respect to the data distribution. In particular, it does not inform on mode coverage or probability mass misallocation. The learning curves (Figure 3) look more convincing to me: they provide good evidence that increasing the number of discriminators has a stabilizing effect on the learning dynamics. However, it seems like this figure along with Figure 4 also show that the unmodified generator objective is more stable even with only one discriminator. In that case, is it even necessary to have more than one discriminator to train the generator using an unmodified objective? Overall, I think the ideas presented in this paper show good potential, but I would like to see an extended analysis in the line of Figures 3 and 4 for more datasets before I think it is ready for publication. UPDATE: The rating has been revised to a 7 following discussion with the authors.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and the insights ! We agree that the GMAM metric is not an absolute score for evaluating GANs . However , we do feel that it is an appropriate method to compare GAN models against each other . It is also an unsupervised method to evaluate GANs that are not limited to the domain of images . More detailed results for CIFAR and MNIST using the GMAM metric are included in the Appendix to show this . We have also included Inception scores for the CIFAR dataset in the Appendix to show that the GMAM metric compares well with how the score evaluates Generated images . Figure 3 and Figure 4 plot the dynamics of the different models while learning . With multiple discriminators it is evident that variance in the Generator objective is reduced and the convergence to equilibrium is achieved up to twice as fast . Further , the using the original unmodified objective with a single discriminator leads to saturation early on in the training process with the generator not getting meaningful signals . Using multiple discriminators mitigates this issue as well . Figures 10 and 11 in the Appendix plot the same dynamics as Figures 3 and 4 on the CelebA dataset . Here training with the original unmodified objective and a single discriminator was not possible due to the saturation problem , while with multiple discriminators we could use the original objective . For more empirical results , we have shall include plots similar to figures 3 and 4 ( and 10 , 11 ) for the CIFAR dataset as well . We shall also include results on one more dataset soon ."}, "1": {"review_id": "Byk-VI9eg-1", "review_text": "This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect. I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We did not compare against GAN with DAE because it is a parallel submission in this conference . However , that paper contains Inception scores for their model on CIFAR-10 which can be directly compared against our model ( SEE TABLE 6 IN APPENDIX A.3 ) ."}, "2": {"review_id": "Byk-VI9eg-2", "review_text": "In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator. My main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a \u2019soft-discriminator\u2019. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier. Overall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and the feedback ! Figure 1 was meant to depict the idea of multiple Discriminators providing feedback to the Generator , with the caption giving an example by specifying F : = max in section 3.1 . We will modify the caption so that the complete idea is more evident and less misleading . We have also modified the layout of the paper to avoid the misunderstanding that was pointed out . We will be uploading the same soon ."}}