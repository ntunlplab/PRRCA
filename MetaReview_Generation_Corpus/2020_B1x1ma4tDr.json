{"year": "2020", "forum": "B1x1ma4tDr", "title": "DDSP: Differentiable Digital Signal Processing", "decision": "Accept (Spotlight)", "meta_review": "This paper proposes a novel differentiable digital signal processing in audio synthesis. The application is novel and interesting. All the reivewers agree to accept it. The authors are encouraged to consider the reviewer's suggestions to revise the paper.", "reviews": [{"review_id": "B1x1ma4tDr-0", "review_text": "This paper presents a model for audio generation/synthesis where the model is trained to output time-varying parameters of a vocoder/synthesiser rather than directly outputting audio samples/spectrogram frames. The model is trained by mining an L1 loss between the synthesised audio and the real training audio. Overall, I found the paper to be well written, I found the online supplementary material helpful in getting an intuition for the quality of audio samples generated and to understand the various parts of the proposed architecture. I also found the figures to be extremely informative, especially figure 2. Although, I think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar, since the description and experiments very specifically deal with audio synthesis with vocoders, even though the components might be general DSP components. I think the paper presents a reasonable alternative to current autoregressive audio generation methods and should be accepted for publication. Minor Comments 1. The reference provided for RNNs, Sutskever et. al. 2014, should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time. 2. \u201cThe bias of the natural world is to vibrate.\u201d I am not sure what exactly is meant here. Is it really a \u201cbias\u201d if every object in the universe vibrates? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors. Bias as used in the paper, seems to have several means. It would help the description if the authors cleared up this confusing use of the term. 3. In Section 1.3, the authors claim that one of the strengths of the proposed method is that the models are \u201crelatively small\u201d, however all discussion of the sizes is relegated to the appendix. It would be helpful to the reader if some model complexity comparisons are presented in the results section and a high level summary is presented at the end of Section 1. 4. In Section 3, some additional high-level description for the Harmonic plus Noise model (Serra & Smith, 1990) should be provided to motivate the discussion and experiments in the rest of the paper. 5. Section 4.2 should provide some description of the parameters counts for each of the components considered and how this compares with existing auto-regressive generation algorithms. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your time and expertise in your review . We \u2019 ve done our best to address your key points with paper revisions and the comments below . > \u201c I think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar , since the description and experiments very specifically deal with audio synthesis with vocoders , even though the components might be general DSP components. \u201d This is a very valid point and something we \u2019 ve done a lot of back and forth about . A more limited title would be more descriptive of the specific experiments performed in the paper . However , a main partner of the paper is the release of the corresponding Tensorflow library , that we hope will see use in a much broader range of applications . As the library will likely not receive its own paper in another venue , we would like this paper to serve as the initial and primary reference , which the current title accomplishes . It \u2019 s a tough call , but I think we \u2019 d like to stick with the title as it is . > \u201c 1.The reference provided for RNNs , Sutskever et . al.2014 , should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time. \u201d Agreed . We \u2019 ve added citations to a popular paper by Werber and book by Williams and Zipser from 1990 . > \u201c 2. \u201c The bias of the natural world is to vibrate. \u201d I am not sure what exactly is meant here . Is it really a \u201c bias \u201d if every object in the universe vibrates ? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors . Bias as used in the paper , seems to have several means . It would help the description if the authors cleared up this confusing use of the term. \u201d Thank you for pointing out that we \u2019 ve overloaded the term \u201c bias \u201d , which we indeed use interchangeably with structural/inductive priors . We \u2019 ve amended the paper in several places to be more explicit . For instance we \u2019 ve rewritten the paragraph in the introduction to try and be more straightforward : \u201c \u201d \u201d Objects have a natural tendency to periodically vibrate . Small shape displacements are usually restored with elastic forces that conserve energy ( similar to a canonical mass on a spring ) , leading to harmonic oscillation between kinetic and potential energy ( Smith , 2010 ) .... ... However , neural synthesis models often do not exploit this periodic structure for generation and perception . \u201c \u201d \u201d > \u201c 4.In Section 3 , some additional high-level description for the Harmonic plus Noise model ( Serra & Smith , 1990 ) should be provided to motivate the discussion and experiments in the rest of the paper. \u201d We agree this would strengthen the paper , as the model expressivity seems to have been a point of confusion for some readers . We have added a Section 3.1 with citations to better motivate the use and expressivity of the model : \u201c \u201d \u201d 3.1 SPECTRAL MODELING SYNTHESIS Here , as an example DDSP model , we implement a differentiable version of Spectral Modeling Synthesis ( SMS ) Serra & Smith ( 1990 ) . This model generates sound by combining an additive synthesizer ( adding together many sinusoids ) with a subtractive synthesizer ( filtering white noise ) . We choose SMS because , despite being parametric , it is a highly expressive model of sound , and has found widespread adoption in tasks as diverse as spectral morphing , time stretching , pitch shifting , source separation , transcription , and even as a general purpose audio codec in MPEG-4 ( Tellman et al. , 1995 ; Sanjaume , 2002 ; Klapuri et al. , 2000 ; Purnhagen & Meine , 2000 ) . As we only consider monophonic sources in these experiments , we use the Harmonic plus Noise model , that further constrains sinusoids to be integer multiples of a fundamental frequency ( Beauchamp , 2007 ) . One of the reasons that SMS is more expressive than many other parametric models because it has so many more parameters . For example , in the 4 seconds of 16kHz audio in the datasets considered here , the synthesizer coefficients actually have \u223c2.5 times more dimensions than the audio waveform itself ( ( 1 amplitude + 100 harmonics + 65 noise band magnitudes ) * 1000 timesteps = 165,000 dimensions , vs. 64,000 audio samples ) . This makes them amenable to control by a neural network , as it would be difficult to realistically specify all these parameters by hand . \u201c \u201d \u201d"}, {"review_id": "B1x1ma4tDr-1", "review_text": "This paper develops a framework for for audio generation using oscillators with differentiable neural network type learning. They showcase the usefulness and effectiveness of the approach with several examples such as timbre transfer, dereverberation, changing the room impulse response, pitch extrapolation and so on. I can imagine the proposed learnable oscillator based autoencoders in a variety of applications. I think that this suggested software library can be useful for a wide range of audio researchers, and I commend the authors for this contribution. It is very nice to see an example of research where we make use of our physical understanding of the sound medium rather than blindly throwing a neural network at the problem. I have one important question though: how susceptible do you think the system is robust with respect to f0 and loudness encoders? Have you experimented with situations where the f0 and the loudness encoders might fail (such as more non-periodic and noisy signals)? ", "rating": "8: Accept", "reply_text": "Thank you for your review and helpful comments . We have replied to your main question below . > \u201c how susceptible do you think the system is robust with respect to f0 and loudness encoders ? Have you experimented with situations where the f0 and the loudness encoders might fail ( such as more non-periodic and noisy signals ) ? \u201d For the specific harmonic+noise model we consider in this paper , accurate f0 estimation is very important . It is a strong constraint of the model by construction . We can see this when we train a DDSP autoencoder with a learnable f0 encoder . The model first reduces loss by covering the spectrogram with filtered noise , and only later replaces that noise with harmonic content when it can more accurately follow the f0 contours . The loudness conditioning signal is extracted directly from the audio , so is not a source of such variability . Due to its construction , the harmonic constraints are not appropriate for modeling non-periodic signals , and we haven \u2019 t tested on that type of data . However , there are many similar variants ( such as an unconstrained sinusoidal+residual model ) that have been shown to work well on more general-purpose sounds , and this is definitely an area we \u2019 d like to explore in future research . Beyond sinusoidal+noise modeling , we found in our experiments that waveforms can be added linearly . This means using DDSP components does not preclude using raw waveform generation . We demonstrate this in the paper by adding the output waveforms of the additive and noise synthesizers , but future work can extend this waveforms generated directly by neural networks , which may be an efficient manner of representing transients ."}, {"review_id": "B1x1ma4tDr-2", "review_text": "This very nice paper tackles the integration of of domain knowledge in signal processing within neural nets; the approach is illustrated in the domain of music. The proof of concept (audio supplementary material) is very convincing. The argument is that a \"natural\" latent space for audio is the spectro-temporal domain. Approaches working purely in the waveform, or in the frequency domains must handle the phase issues. Approaches can learn to handle these issues, at the expense of more data. A key difficulty is that the L_2 loss does not match the perception. The authors present a perceptual loss that addresses the point (Eq. 4 - clarify the difference w.r.t. Wang et al.), . A natural question thus is whether applying this loss on e.g. Wavenet, Sample RNN or Wave RNN would solve the problem. In short, the contribution is in designing a latent space that accounts for independent components of the music signal (pitch; loudness; reverberation and/or noise), using existing components (oscillators, envelopes and filters), and making them amenable to end-to-end optimization (noting that loudness can be extracted deterministically). I understand that the auto-encoder is enriched with a FIR filter at its core: the input is mapped into the time-frequency domain; convolved with the output of the neural net H_l, and the result is recovered from the time-frequency domain. Explain \"frames x_l to match the impulse responses h_l\". Care (domain knowledge and trials and errors, I suppose) is exercized in the conversion and recovery (shape and size of the window) to remove undesired effects. Overall, the approach works in two modes: one where the fundamental frequency is extracted, one where it is learned. I miss this comparison in the audio material, could you tell where to look / hear ? Questions: * NN operate at a slower frame rate (sect. 3.2): how much slower ? How sensitive to this parameter ? Detail * were, end p. 5. A word missing ? * are useful --> is useful ? * could produced, p.6 ", "rating": "8: Accept", "reply_text": "Thank you for the review and the encouraging comments . We \u2019 ve done our best to address your questions with paper revisions and the comments below . > \u201c The authors present a perceptual loss that addresses the point ( Eq.4 - clarify the difference w.r.t.Wang et al . ) , .A natural question thus is whether applying this loss on e.g.Wavenet , Sample RNN or Wave RNN would solve the problem. \u201d The loss in Equation 4 is a multi-scale spectrogram loss in both magnitude and log-magnitude . Wang et al . ( 2019 ) use a similar loss . Their loss differs in that they use different window/hop sizes than this paper , 3 scales instead of 6 , and a phase loss instead of a linear magnitude loss . As described in Section B.4 , we also use a \u201c perceptual \u201d loss for the unsupervised DDSP autoencoder ( that must jointly learn to infer f ( t ) ) , using the activations of a pretrained CREPE model . We agree that it would be great to be able to use the multi-scale spectrogram losses and pretrained model losses for autoregressive waveform models such as WaveNet , SampleRNN , or WaveRNN . Unfortunately , one of the drawbacks of these models ( as described in Section 1 ) is that they are trained with teacher forcing and do not generate samples during training ( which would be computationally infeasible ) . This prevents using any losses that actually compare target audio and generated audio , and motivates the approach taken in this paper ( and others e.g.GANSynth , NSF , etc . ) . > \u201c I understand that the auto-encoder is enriched with a FIR filter at its core : the input is mapped into the time-frequency domain ; convolved with the output of the neural net H_l , and the result is recovered from the time-frequency domain . Explain `` frames x_l to match the impulse responses h_l '' . \u201d For memory/compute efficiency , the outputs of the neural network are not at audio rate . In the paper experiments the FIR coefficients are provided every 4ms i.e.for audio at 16kHz , the FIR coefficients are provided every 64 samples . To make this concrete , the size of the coefficients tensor is then ( n_batch , n_frames=1000 , n_coefficients=65 ) , while the size of audio is ( n_batch , n_samples=64000 ) . The audio is then divided into the same number of frames , ( n_batch , n_frames=1000 , n_samples_per_frame=64 ) , using non-overlapping box windows . We then take the DFT of each frame , multiply them with IDFT ( Window ( DFT ( coefficients ) ) and take the IDFT to get the filtered audio frames ( n_batch , n_frames=1000 , n_samples_per_frame=64 ) . Finally we flatten the frames ( box window with no overlap ) to get the filtered audio ( n_batch , n_samples=64000 ) . > \u201c Overall , the approach works in two modes : one where the fundamental frequency is extracted , one where it is learned . I miss this comparison in the audio material , could you tell where to look / hear ? \u201d Good point . We have added comparisons of the reconstructions to the online supplement . > \u201c NN operate at a slower frame rate ( sect.3.2 ) : how much slower ? How sensitive to this parameter ? \u201d As we mentioned above , the network in these experiments operates at 1 frame every 4ms ( 64 samples ) . We \u2019 ve also run experiments with 1 frame every 16ms ( 256 samples ) , which are actually faster to optimize , but are limited in the temporal response of quick attacks . > \u201c were , end p. 5 . A word missing ? are useful -- > is useful ? could produced , p.6 \u201d Thanks for the help catching the typos ! We \u2019 ve fixed them in the revised version of the manuscript ."}], "0": {"review_id": "B1x1ma4tDr-0", "review_text": "This paper presents a model for audio generation/synthesis where the model is trained to output time-varying parameters of a vocoder/synthesiser rather than directly outputting audio samples/spectrogram frames. The model is trained by mining an L1 loss between the synthesised audio and the real training audio. Overall, I found the paper to be well written, I found the online supplementary material helpful in getting an intuition for the quality of audio samples generated and to understand the various parts of the proposed architecture. I also found the figures to be extremely informative, especially figure 2. Although, I think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar, since the description and experiments very specifically deal with audio synthesis with vocoders, even though the components might be general DSP components. I think the paper presents a reasonable alternative to current autoregressive audio generation methods and should be accepted for publication. Minor Comments 1. The reference provided for RNNs, Sutskever et. al. 2014, should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time. 2. \u201cThe bias of the natural world is to vibrate.\u201d I am not sure what exactly is meant here. Is it really a \u201cbias\u201d if every object in the universe vibrates? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors. Bias as used in the paper, seems to have several means. It would help the description if the authors cleared up this confusing use of the term. 3. In Section 1.3, the authors claim that one of the strengths of the proposed method is that the models are \u201crelatively small\u201d, however all discussion of the sizes is relegated to the appendix. It would be helpful to the reader if some model complexity comparisons are presented in the results section and a high level summary is presented at the end of Section 1. 4. In Section 3, some additional high-level description for the Harmonic plus Noise model (Serra & Smith, 1990) should be provided to motivate the discussion and experiments in the rest of the paper. 5. Section 4.2 should provide some description of the parameters counts for each of the components considered and how this compares with existing auto-regressive generation algorithms. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your time and expertise in your review . We \u2019 ve done our best to address your key points with paper revisions and the comments below . > \u201c I think the title of the paper could be updated to something more specific like Differentiable Vocoders or something similar , since the description and experiments very specifically deal with audio synthesis with vocoders , even though the components might be general DSP components. \u201d This is a very valid point and something we \u2019 ve done a lot of back and forth about . A more limited title would be more descriptive of the specific experiments performed in the paper . However , a main partner of the paper is the release of the corresponding Tensorflow library , that we hope will see use in a much broader range of applications . As the library will likely not receive its own paper in another venue , we would like this paper to serve as the initial and primary reference , which the current title accomplishes . It \u2019 s a tough call , but I think we \u2019 d like to stick with the title as it is . > \u201c 1.The reference provided for RNNs , Sutskever et . al.2014 , should be supplemented with older references from the 80s when RNNs were first trained with backdrop through time. \u201d Agreed . We \u2019 ve added citations to a popular paper by Werber and book by Williams and Zipser from 1990 . > \u201c 2. \u201c The bias of the natural world is to vibrate. \u201d I am not sure what exactly is meant here . Is it really a \u201c bias \u201d if every object in the universe vibrates ? I think the term bias has been overloaded in the paper and is confused with structural/inductive priors . Bias as used in the paper , seems to have several means . It would help the description if the authors cleared up this confusing use of the term. \u201d Thank you for pointing out that we \u2019 ve overloaded the term \u201c bias \u201d , which we indeed use interchangeably with structural/inductive priors . We \u2019 ve amended the paper in several places to be more explicit . For instance we \u2019 ve rewritten the paragraph in the introduction to try and be more straightforward : \u201c \u201d \u201d Objects have a natural tendency to periodically vibrate . Small shape displacements are usually restored with elastic forces that conserve energy ( similar to a canonical mass on a spring ) , leading to harmonic oscillation between kinetic and potential energy ( Smith , 2010 ) .... ... However , neural synthesis models often do not exploit this periodic structure for generation and perception . \u201c \u201d \u201d > \u201c 4.In Section 3 , some additional high-level description for the Harmonic plus Noise model ( Serra & Smith , 1990 ) should be provided to motivate the discussion and experiments in the rest of the paper. \u201d We agree this would strengthen the paper , as the model expressivity seems to have been a point of confusion for some readers . We have added a Section 3.1 with citations to better motivate the use and expressivity of the model : \u201c \u201d \u201d 3.1 SPECTRAL MODELING SYNTHESIS Here , as an example DDSP model , we implement a differentiable version of Spectral Modeling Synthesis ( SMS ) Serra & Smith ( 1990 ) . This model generates sound by combining an additive synthesizer ( adding together many sinusoids ) with a subtractive synthesizer ( filtering white noise ) . We choose SMS because , despite being parametric , it is a highly expressive model of sound , and has found widespread adoption in tasks as diverse as spectral morphing , time stretching , pitch shifting , source separation , transcription , and even as a general purpose audio codec in MPEG-4 ( Tellman et al. , 1995 ; Sanjaume , 2002 ; Klapuri et al. , 2000 ; Purnhagen & Meine , 2000 ) . As we only consider monophonic sources in these experiments , we use the Harmonic plus Noise model , that further constrains sinusoids to be integer multiples of a fundamental frequency ( Beauchamp , 2007 ) . One of the reasons that SMS is more expressive than many other parametric models because it has so many more parameters . For example , in the 4 seconds of 16kHz audio in the datasets considered here , the synthesizer coefficients actually have \u223c2.5 times more dimensions than the audio waveform itself ( ( 1 amplitude + 100 harmonics + 65 noise band magnitudes ) * 1000 timesteps = 165,000 dimensions , vs. 64,000 audio samples ) . This makes them amenable to control by a neural network , as it would be difficult to realistically specify all these parameters by hand . \u201c \u201d \u201d"}, "1": {"review_id": "B1x1ma4tDr-1", "review_text": "This paper develops a framework for for audio generation using oscillators with differentiable neural network type learning. They showcase the usefulness and effectiveness of the approach with several examples such as timbre transfer, dereverberation, changing the room impulse response, pitch extrapolation and so on. I can imagine the proposed learnable oscillator based autoencoders in a variety of applications. I think that this suggested software library can be useful for a wide range of audio researchers, and I commend the authors for this contribution. It is very nice to see an example of research where we make use of our physical understanding of the sound medium rather than blindly throwing a neural network at the problem. I have one important question though: how susceptible do you think the system is robust with respect to f0 and loudness encoders? Have you experimented with situations where the f0 and the loudness encoders might fail (such as more non-periodic and noisy signals)? ", "rating": "8: Accept", "reply_text": "Thank you for your review and helpful comments . We have replied to your main question below . > \u201c how susceptible do you think the system is robust with respect to f0 and loudness encoders ? Have you experimented with situations where the f0 and the loudness encoders might fail ( such as more non-periodic and noisy signals ) ? \u201d For the specific harmonic+noise model we consider in this paper , accurate f0 estimation is very important . It is a strong constraint of the model by construction . We can see this when we train a DDSP autoencoder with a learnable f0 encoder . The model first reduces loss by covering the spectrogram with filtered noise , and only later replaces that noise with harmonic content when it can more accurately follow the f0 contours . The loudness conditioning signal is extracted directly from the audio , so is not a source of such variability . Due to its construction , the harmonic constraints are not appropriate for modeling non-periodic signals , and we haven \u2019 t tested on that type of data . However , there are many similar variants ( such as an unconstrained sinusoidal+residual model ) that have been shown to work well on more general-purpose sounds , and this is definitely an area we \u2019 d like to explore in future research . Beyond sinusoidal+noise modeling , we found in our experiments that waveforms can be added linearly . This means using DDSP components does not preclude using raw waveform generation . We demonstrate this in the paper by adding the output waveforms of the additive and noise synthesizers , but future work can extend this waveforms generated directly by neural networks , which may be an efficient manner of representing transients ."}, "2": {"review_id": "B1x1ma4tDr-2", "review_text": "This very nice paper tackles the integration of of domain knowledge in signal processing within neural nets; the approach is illustrated in the domain of music. The proof of concept (audio supplementary material) is very convincing. The argument is that a \"natural\" latent space for audio is the spectro-temporal domain. Approaches working purely in the waveform, or in the frequency domains must handle the phase issues. Approaches can learn to handle these issues, at the expense of more data. A key difficulty is that the L_2 loss does not match the perception. The authors present a perceptual loss that addresses the point (Eq. 4 - clarify the difference w.r.t. Wang et al.), . A natural question thus is whether applying this loss on e.g. Wavenet, Sample RNN or Wave RNN would solve the problem. In short, the contribution is in designing a latent space that accounts for independent components of the music signal (pitch; loudness; reverberation and/or noise), using existing components (oscillators, envelopes and filters), and making them amenable to end-to-end optimization (noting that loudness can be extracted deterministically). I understand that the auto-encoder is enriched with a FIR filter at its core: the input is mapped into the time-frequency domain; convolved with the output of the neural net H_l, and the result is recovered from the time-frequency domain. Explain \"frames x_l to match the impulse responses h_l\". Care (domain knowledge and trials and errors, I suppose) is exercized in the conversion and recovery (shape and size of the window) to remove undesired effects. Overall, the approach works in two modes: one where the fundamental frequency is extracted, one where it is learned. I miss this comparison in the audio material, could you tell where to look / hear ? Questions: * NN operate at a slower frame rate (sect. 3.2): how much slower ? How sensitive to this parameter ? Detail * were, end p. 5. A word missing ? * are useful --> is useful ? * could produced, p.6 ", "rating": "8: Accept", "reply_text": "Thank you for the review and the encouraging comments . We \u2019 ve done our best to address your questions with paper revisions and the comments below . > \u201c The authors present a perceptual loss that addresses the point ( Eq.4 - clarify the difference w.r.t.Wang et al . ) , .A natural question thus is whether applying this loss on e.g.Wavenet , Sample RNN or Wave RNN would solve the problem. \u201d The loss in Equation 4 is a multi-scale spectrogram loss in both magnitude and log-magnitude . Wang et al . ( 2019 ) use a similar loss . Their loss differs in that they use different window/hop sizes than this paper , 3 scales instead of 6 , and a phase loss instead of a linear magnitude loss . As described in Section B.4 , we also use a \u201c perceptual \u201d loss for the unsupervised DDSP autoencoder ( that must jointly learn to infer f ( t ) ) , using the activations of a pretrained CREPE model . We agree that it would be great to be able to use the multi-scale spectrogram losses and pretrained model losses for autoregressive waveform models such as WaveNet , SampleRNN , or WaveRNN . Unfortunately , one of the drawbacks of these models ( as described in Section 1 ) is that they are trained with teacher forcing and do not generate samples during training ( which would be computationally infeasible ) . This prevents using any losses that actually compare target audio and generated audio , and motivates the approach taken in this paper ( and others e.g.GANSynth , NSF , etc . ) . > \u201c I understand that the auto-encoder is enriched with a FIR filter at its core : the input is mapped into the time-frequency domain ; convolved with the output of the neural net H_l , and the result is recovered from the time-frequency domain . Explain `` frames x_l to match the impulse responses h_l '' . \u201d For memory/compute efficiency , the outputs of the neural network are not at audio rate . In the paper experiments the FIR coefficients are provided every 4ms i.e.for audio at 16kHz , the FIR coefficients are provided every 64 samples . To make this concrete , the size of the coefficients tensor is then ( n_batch , n_frames=1000 , n_coefficients=65 ) , while the size of audio is ( n_batch , n_samples=64000 ) . The audio is then divided into the same number of frames , ( n_batch , n_frames=1000 , n_samples_per_frame=64 ) , using non-overlapping box windows . We then take the DFT of each frame , multiply them with IDFT ( Window ( DFT ( coefficients ) ) and take the IDFT to get the filtered audio frames ( n_batch , n_frames=1000 , n_samples_per_frame=64 ) . Finally we flatten the frames ( box window with no overlap ) to get the filtered audio ( n_batch , n_samples=64000 ) . > \u201c Overall , the approach works in two modes : one where the fundamental frequency is extracted , one where it is learned . I miss this comparison in the audio material , could you tell where to look / hear ? \u201d Good point . We have added comparisons of the reconstructions to the online supplement . > \u201c NN operate at a slower frame rate ( sect.3.2 ) : how much slower ? How sensitive to this parameter ? \u201d As we mentioned above , the network in these experiments operates at 1 frame every 4ms ( 64 samples ) . We \u2019 ve also run experiments with 1 frame every 16ms ( 256 samples ) , which are actually faster to optimize , but are limited in the temporal response of quick attacks . > \u201c were , end p. 5 . A word missing ? are useful -- > is useful ? could produced , p.6 \u201d Thanks for the help catching the typos ! We \u2019 ve fixed them in the revised version of the manuscript ."}}