{"year": "2017", "forum": "Sywh5KYex", "title": "Learning Identity Mappings with Residual Gates", "decision": "Reject", "meta_review": "Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.", "reviews": [{"review_id": "Sywh5KYex-0", "review_text": " This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks. It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net. More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved. The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Hi and thanks for the feedback ! - Wide Gated ResNet requires much more parameters than DenseNet ( and other Res Net variants ) for obtaining a little improvement over Dense Net Gated WideResNets are by no means the focus of our paper , but an example of how our layer augmentation works . We are not proposing a new type of ResNet , but a general tool to aid optimization of general networks . It suffices to see that our technique is also applicable to DenseNets , or nearly any other kind of network ( including ResNeXts ) . We augmented Wide ResNets because they were the current SOTA during our experiments , and we wanted to provide experimental indications that our technique could be used to improve state-of-the-art models . We feel that our technique is more closely related to general tools such as Dropout and Batch Norm : it is a general technique that can be added to nearly any model and provides advantages such as easier optimization . Our claim is not that it improves ResNets , but that it can be used to improve any layer design . Due to the linear nature of the gates , an optimization on the new dimension is purely linear , and adds no further non-linear complexity to the model . It is also easily reversible -- we actually initialize the gates in such a way that they do not impact the model -- just like BN once we consider its post-norm affine transformation . Moreover , the comparison between the Gated WideResNet and DenseNets had the goal of showing that a simple change with negligible impact on training time and number of parameters could make a model surpass the previous state-of-the-art . The main comparison , however , is the one between Gated and Non-Gated models , where the improvement due to the linear gates could be put to trial ."}, {"review_id": "Sywh5KYex-1", "review_text": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation. The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn. Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete? For the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture. For CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant. Some questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the feedback . - it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data We do mention this difference , however it goes against our main goals when designing the gates , more specifically : + Having gates which are data-dependent makes the number of parameters grow . In case we used a data-dependent scalar to control the gates , we would need a Nx1 matrix ( considering a fully-connected network ) to achieve this . This would imply in N times more parameters than our approach . + This would make the analysis extremely more complex . It would not be possible to understand as easily the exact effects of the extra dimension that each k parameter adds to the cost surface . I believe it would be actually impossible with the current theoretical tools we have , since the problem would be as hard as understanding how a new layer affects the cost function . This would result in understanding the whole cost function of a Deep Network , since it it is decomposable in a limited number of layers . Having a single data-independent scalar and a linear gate offers the huge advantage of understanding very easily its impact on the model . + One of our proposals is to use the method as a way for layers to fully deactivate . This is only feasible if the parameters that deactivate the layer are data-independent , because if they were data-dependent we would have the constraint : \\forall x , u ( x ) = x . That is , the gates would have to deactivate for every possible input . Since the number of parameters of a data-dependent layer is lower bounded by N , we would require at least N parameters to be set to zero . This would offer little advantage compared to how Residual layers behave as identity mappings for specific parameter sets . In our approach , layers truly become identity mappings , instead of just having the ability to behave as such mapping for specific data points . - Did the authors try their original design of u = g ( k ) f ( x ) + ( 1 - g ( k ) ) x where f ( x ) is a plain layer instead of a residual layer ? Based on the arguments made in the paper , this should work fine . Why was n't it tested ? If it does n't work , are the arguments incorrect or incomplete ? Such layers have already been partially explored in `` Identity Mappings in Deep Residual Networks '' , and perform similarly to Residual Networks . Unfortunately following the ~8 page limit was not possible with all discussions and experiments we had in mind . EDIT : We have realized that adding such results without having over 10 pages was doable . Therefore we will add them to the conference version in case of acceptance . - For the MNIST experiments , since the hyperparameters are fixed , the plots are misleading if any dependence on hyperparameters exists for the different models . This experiment appears to be based on Srivastava et al ( 2015 ) . If it is indeed designed to test optimization at aggressive depths , then apart from doing a hyperparameter search , the authors should not use regularization such as dropout or batch norm , which do not appear in the theoretical arguments for the architecture . We have not used Dropout for such experiments . We believe that the internal covariate shift is an orthogonal problem to the one of learning identity mappings -- even though they both negatively impact the optimization of deep networks -- , therefore we decided on using BN for all our tests . - Some questions regarding g ( ) : Was g ( ) always ReLU ? Does n't this have potential problems with g ( k ) becoming 0 and never recovering ? Does this also mean that for the wide resnet in Fig 7 , most residual blocks are zeroed out since k < 0 ? During prototyping we have tested sigmoid , linear and ReLU . Sigmoid had three major disadvantages : first , if just performed worse , second , we lost the linear interpretation of the extra dimension ( where the slices of the new dimension are linear combinations of the original model and the identity mapping , roughly speaking ) , lastly , g ( k ) could not be > 1 , which means the gate could not properly suppress the shortcut connection ( as the network does in the last layer for both experiments we ran ) . Linear and ReLU perform the same , except for when layers deactivate . We chose to use ReLU exactly for its deactivation : we propose our method as one whose layers can be fully pruned during training , hence making future epochs faster . Using the ReLU as g ( k ) is equivalent to using a linear gate where layers are removed from the model once k < 0 . Of course , nothing stops a linear function to be used as g. As for Fig 7 , thanks a lot for pointing the mistake , the values in the graph are actually the real values minus 1 . We are not currently sure what caused this , but we will be updating the paper with the correct graph soon ."}, {"review_id": "Sywh5KYex-2", "review_text": "The paper presents a layer architecture where a single parameter is used to gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments ! We have added a proper evaluation on the learned k values , and achieved interesting conclusions on the pattern of the final k parameters . For example , in Residual Networks the lowest k values are of blocks that increase the dimension of the feature maps , suggesting that these units might be less relevant to a network , and conversely just maintaining the shortcut connection ( in this case , with a 1x1 convolution ) might be a good alternative . We focused our experimental validation on understanding the effects of the k parameter . Achieving such satisfying results ( 3.65 % /18.27 % ) on CIFAR was not expected at all , and we believe that achieving these results without requiring to tune hyperparameters or to compare several potential architectures was a positive factor of the proposed technique ."}], "0": {"review_id": "Sywh5KYex-0", "review_text": " This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks. It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net. More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved. The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Hi and thanks for the feedback ! - Wide Gated ResNet requires much more parameters than DenseNet ( and other Res Net variants ) for obtaining a little improvement over Dense Net Gated WideResNets are by no means the focus of our paper , but an example of how our layer augmentation works . We are not proposing a new type of ResNet , but a general tool to aid optimization of general networks . It suffices to see that our technique is also applicable to DenseNets , or nearly any other kind of network ( including ResNeXts ) . We augmented Wide ResNets because they were the current SOTA during our experiments , and we wanted to provide experimental indications that our technique could be used to improve state-of-the-art models . We feel that our technique is more closely related to general tools such as Dropout and Batch Norm : it is a general technique that can be added to nearly any model and provides advantages such as easier optimization . Our claim is not that it improves ResNets , but that it can be used to improve any layer design . Due to the linear nature of the gates , an optimization on the new dimension is purely linear , and adds no further non-linear complexity to the model . It is also easily reversible -- we actually initialize the gates in such a way that they do not impact the model -- just like BN once we consider its post-norm affine transformation . Moreover , the comparison between the Gated WideResNet and DenseNets had the goal of showing that a simple change with negligible impact on training time and number of parameters could make a model surpass the previous state-of-the-art . The main comparison , however , is the one between Gated and Non-Gated models , where the improvement due to the linear gates could be put to trial ."}, "1": {"review_id": "Sywh5KYex-1", "review_text": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation. The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn. Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete? For the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture. For CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant. Some questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the feedback . - it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data We do mention this difference , however it goes against our main goals when designing the gates , more specifically : + Having gates which are data-dependent makes the number of parameters grow . In case we used a data-dependent scalar to control the gates , we would need a Nx1 matrix ( considering a fully-connected network ) to achieve this . This would imply in N times more parameters than our approach . + This would make the analysis extremely more complex . It would not be possible to understand as easily the exact effects of the extra dimension that each k parameter adds to the cost surface . I believe it would be actually impossible with the current theoretical tools we have , since the problem would be as hard as understanding how a new layer affects the cost function . This would result in understanding the whole cost function of a Deep Network , since it it is decomposable in a limited number of layers . Having a single data-independent scalar and a linear gate offers the huge advantage of understanding very easily its impact on the model . + One of our proposals is to use the method as a way for layers to fully deactivate . This is only feasible if the parameters that deactivate the layer are data-independent , because if they were data-dependent we would have the constraint : \\forall x , u ( x ) = x . That is , the gates would have to deactivate for every possible input . Since the number of parameters of a data-dependent layer is lower bounded by N , we would require at least N parameters to be set to zero . This would offer little advantage compared to how Residual layers behave as identity mappings for specific parameter sets . In our approach , layers truly become identity mappings , instead of just having the ability to behave as such mapping for specific data points . - Did the authors try their original design of u = g ( k ) f ( x ) + ( 1 - g ( k ) ) x where f ( x ) is a plain layer instead of a residual layer ? Based on the arguments made in the paper , this should work fine . Why was n't it tested ? If it does n't work , are the arguments incorrect or incomplete ? Such layers have already been partially explored in `` Identity Mappings in Deep Residual Networks '' , and perform similarly to Residual Networks . Unfortunately following the ~8 page limit was not possible with all discussions and experiments we had in mind . EDIT : We have realized that adding such results without having over 10 pages was doable . Therefore we will add them to the conference version in case of acceptance . - For the MNIST experiments , since the hyperparameters are fixed , the plots are misleading if any dependence on hyperparameters exists for the different models . This experiment appears to be based on Srivastava et al ( 2015 ) . If it is indeed designed to test optimization at aggressive depths , then apart from doing a hyperparameter search , the authors should not use regularization such as dropout or batch norm , which do not appear in the theoretical arguments for the architecture . We have not used Dropout for such experiments . We believe that the internal covariate shift is an orthogonal problem to the one of learning identity mappings -- even though they both negatively impact the optimization of deep networks -- , therefore we decided on using BN for all our tests . - Some questions regarding g ( ) : Was g ( ) always ReLU ? Does n't this have potential problems with g ( k ) becoming 0 and never recovering ? Does this also mean that for the wide resnet in Fig 7 , most residual blocks are zeroed out since k < 0 ? During prototyping we have tested sigmoid , linear and ReLU . Sigmoid had three major disadvantages : first , if just performed worse , second , we lost the linear interpretation of the extra dimension ( where the slices of the new dimension are linear combinations of the original model and the identity mapping , roughly speaking ) , lastly , g ( k ) could not be > 1 , which means the gate could not properly suppress the shortcut connection ( as the network does in the last layer for both experiments we ran ) . Linear and ReLU perform the same , except for when layers deactivate . We chose to use ReLU exactly for its deactivation : we propose our method as one whose layers can be fully pruned during training , hence making future epochs faster . Using the ReLU as g ( k ) is equivalent to using a linear gate where layers are removed from the model once k < 0 . Of course , nothing stops a linear function to be used as g. As for Fig 7 , thanks a lot for pointing the mistake , the values in the graph are actually the real values minus 1 . We are not currently sure what caused this , but we will be updating the paper with the correct graph soon ."}, "2": {"review_id": "Sywh5KYex-2", "review_text": "The paper presents a layer architecture where a single parameter is used to gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments ! We have added a proper evaluation on the learned k values , and achieved interesting conclusions on the pattern of the final k parameters . For example , in Residual Networks the lowest k values are of blocks that increase the dimension of the feature maps , suggesting that these units might be less relevant to a network , and conversely just maintaining the shortcut connection ( in this case , with a 1x1 convolution ) might be a good alternative . We focused our experimental validation on understanding the effects of the k parameter . Achieving such satisfying results ( 3.65 % /18.27 % ) on CIFAR was not expected at all , and we believe that achieving these results without requiring to tune hyperparameters or to compare several potential architectures was a positive factor of the proposed technique ."}}