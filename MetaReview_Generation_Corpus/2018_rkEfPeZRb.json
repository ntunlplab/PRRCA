{"year": "2018", "forum": "rkEfPeZRb", "title": "Variance-based Gradient Compression for Efficient Distributed Deep Learning", "decision": "Invite to Workshop Track", "meta_review": "The reviewers find the gradient compression approach novel and interesting, but they find the empirical evaluation not fully satisfactory. Some aspects of the paper have improved with the feedback from the reviewers, but because of the domain of the paper, experimental evaluation is very important. I recommend improving the experiments by incorporating the reviewers' comments.", "reviews": [{"review_id": "rkEfPeZRb-0", "review_text": "This paper proposes a variance-based gradient compression method to reduce the communication overhead of distributed deep learning. Experiments on real datasets are used for evaluation. The idea to adopt approximated variances of gradients to reduce communication cost seems to be interesting. However, there also exist several major issues in the paper. Firstly, the authors propose to combine two components to reduce communication cost, one being variance-based gradient compression and the other being quantization and parameter encoding. But the contributions of these two components are not separately analyzed or empirically verified. Secondly, the experimental results are unconvincing. The accuracy of Momentum SGD for \u2018Strom, \\tau=0.01\u2019 on CIFAR-10 is only 10.6%. Obviously, the learning procedure is not convergent. It is highly possible that the authors do not choose a good hyper-parameter. Furthermore, the proposed method (not the hybrid) is not necessarily better than Strom except for the case of Momentum SGD on CIFAR-10. Please note that the case of Momentum SGD on CIFAR-10 may have a problematic experimental setting for Strom. In addition, it is weird that the experiment on ImageNet does not adopt the same setting as that on CIFAR-10 to evaluate both Adam and Momentum SGD. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We are glad to hear that you found our algorithm interesting . > But the contributions of these two components are not separately analyzed or empirically verified . Thank you for your comment . The main contribution is intended to be the variance-based gradient compression , with the quantization provided as a way to fit both values of gradient elements and its index in 32-bit while not rounding many elements to zero . We amended our paper with the following : Sec 4.2 \u2019 \u2019 \u2019 To allow for comparison with other compression methods , we propose a basic quantization process . \u2026 \u2019 \u2019 \u2019 > The accuracy of Momentum SGD for \u2018 Strom , \\tau=0.01 \u2019 on CIFAR-10 is only 10.6 % . Obviously , the learning procedure is not convergent . It is highly possible that the authors do not choose a good hyper-parameter . Thank you for your comment . We amended our paper with the following : Sec.6.1 \u2019 \u2019 \u2019 We note that we observed unstable behaviors with other thresholds around 0.01. \u2019 \u2019 \u2019 Appendix D \u2019 \u2019 \u2019 The code is available in examples of Chainer on GitHub. \u2019 \u2019 \u2019 > Furthermore , the proposed method ( not the hybrid ) is not necessarily better than Strom except for the case of Momentum SGD on CIFAR-10 . Please note that the case of Momentum SGD on CIFAR-10 may have a problematic experimental setting for Strom . Thank you for your comment . We amended our paper with the following : Sec.6.1 \u2019 \u2019 \u2019 We also would like to mention the difficulty of hyperparameter tuning in Strom 's method . \u2026 On the other hand , our algorithm is free from such problem . Moreover , when we know good threshold for Strom 's algorithm , we can just combine ours to get further compression. \u2019 \u2019 \u2019 > In addition , it is weird that the experiment on ImageNet does not adopt the same setting as that on CIFAR-10 to evaluate both Adam and Momentum SGD . Thank you for your comment . We amended our paper with the following : Sec 6.2 \u2018 \u2019 \u2019 We also evaluated algorithms with replacing MomentumSGD and its learning rate scheduling to Adam with its default hyperparameter . \u2019 \u2019 \u2019"}, {"review_id": "rkEfPeZRb-1", "review_text": "The paper proposes a novel way of compressing gradient updates for distributed SGD, in order to speed up overall execution. While the technique is novel as far as I know (eq. (1) in particular), many details in the paper are poorly explained (I am unable to understand) and experimental results do not demonstrate that the problem targeted is actually alleviated. More detailed remarks: 1: Motivating with ImageNet taking over a week to train seems misplaced when we have papers claiming to train ImageNet in 1 hour, 24 mins, 15 mins... 4.1: Lemma 4.1 seems like you want B > 1, or clarify definition of V_B. 4.2: This section is not fully comprehensible to me. - It seems you are confusingly overloading the term gradient and words derived (also in other parts or the paper). What is \"maximum value of gradients in a matrix\"? Make sure to use something else, when talking about individual elements of a vector (which is constructed as an average of gradients), etc. - Rounding: do you use deterministic or random rounding? Do you then again store the inaccuracy? - I don't understand definition of d. It seems you subtract logarithm of a gradient from a scalar. - In total, I really don't know what is the object that actually gets communicated, and consequently when you remark that this can be combined with QSGD and the more below it, I don't understand it. This section has to be thoroughly explained, perhaps with some illustrative examples. 4.3: allgatherv remark: does that mean that this approach would not scale well to higher number of workers? 4.4: Remarks about quantization and mantissa manipulation are not clear to me again, or what is the point in doing so. Possible because the problems above. 5: I think this section is not too useful unless you can accompany it with actual efficient implementation and contrast the practical performance. 6: Given that I don't understand how you compress the information being communicated, it is hard to believe the utility of the method. The objective was to speed up training time because communication is bottleneck. If you provide 12,000x compression, is it any more practically useful than providing 120x compression? What would be the difference in runtime? Such questions are never discussed. Further, if in the implementation you discuss masking mantissa, I have serious concern about whether the compression protocol is feasible to implement efficiently, without writing some extremely low-level code. I think the soundness of work addressing this particular problem is damaged if not implemented properly (compared to other kinds of works in current ML related research). Therefore I highly recommend including proper time comparison with a baseline in the future. Further, I don't understand 2 things about the Tables. a) how do you combine the proposed method with Momentum in SGD? This is not discussed as far as I can see. b) What is \"QSGD, 2bit\" If I remember QSGD protocol correctly, there's no natural mapping of 2bit to its parameters.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review . We 're glad to hear that you found our technique to be novel . We 've amended our paper in light of your review . We hope this helps explain the details and demonstrate how our technique alleviates the problem of transmitting gradients between nodes . Section 1 > Motivating with ImageNet taking over a week to train seems misplaced when we have papers claiming to train ImageNet in 1 hour , 24 mins , 15 mins ... Thanks for the comment . We have amended our paper with the following : \u2018 \u2019 \u2019 For example , it takes over a week to train ResNet-50 on the ImageNet dataset if using a single GPU . \u2026 For example , when using 1000BASE-T Ethernet , communication takes at least ten times longer than forward and backward computation for ResNet-50 , making multiple nodes impractical . High performance interconnections such as InfiniBand and Omni-Path are an order of magnitude more expensive than commodity interconnections , which limits research and development of deep learning using large-scale datasets to a small number of researchers. \u2019 \u2019 \u2019 > 4.1 : Lemma 4.1 seems like you want B > 1 , or clarify definition of V_B . Correct.We have amended our paper with the following : \u2018 \u2019 \u2019 Lemma 4.1 A sufficient condition that a vector -g is a descent direction is \\|g - \\nabla f ( x ) \\|_2^2 < \\|g\\|_2^2 . We are interested in the case of g = \\nabla f_B ( x ) , the gradient vector of the loss function over B . By the weak law of large numbers , when B > 1 , the left-hand side with g = \\nabla f_B ( x ) can be estimated as follows. \u2019 \u2019 \u2019 Note , \\nabla_B f ( x ) in lemma 4.1 of our first paper was replaced with a symbol g. > 4.2 : This section is not fully comprehensible to me . > - It seems you are confusingly overloading the term gradient and words derived ( also in other parts or the paper ) . What is `` maximum value of gradients in a matrix '' ? Make sure to use something else , when talking about individual elements of a vector ( which is constructed as an average of gradients ) , etc . You \u2019 re right . We amended the paper to replace gradient with \u2018 gradient element \u2019 when we refer elements of gradient vectors . \u2018 \u2019 \u2019 Our quantization except for the sign bit is as follows . For a weight matrix W_k ( or a weight tensor in CNN ) , there is a group of gradient elements corresponding to the matrix . Let M_k be the maximum absolute value in the group. \u2019 \u2019 \u2019 > - Rounding : do you use deterministic or random rounding ? Do you then again store the inaccuracy ? Good questions . We amended our paper as follows : Sec 4.2 \u2019 \u2019 \u2019 ... We do not adopt stochastic rounding like QSGD nor accumulate rounding error g_i - g'_i for the next batch because this simple rounding does not harm accuracy empirically. \u2019 \u2019 \u2019 > - I do n't understand definition of d. It seems you subtract logarithm of a gradient from a scalar . d is a difference of two scalars . > In total , I really do n't know what is the object that actually gets communicated , and consequently when you remark that this can be combined with QSGD and the more below it , I do n't understand it . This section has to be thoroughly explained , perhaps with some illustrative examples . We hope our clarification between \u2018 gradient \u2019 and \u2018 gradient element \u2019 made the definition of d clearer . We amended our paper as follows : Sec 4.2 \u2018 \u2019 \u2019 ... After deciding which gradient elements to send , each worker sends pairs of a value of a gradient element and its parameter index \u2026 \u2019 \u2019 \u2019 Sec 4.2 \u2019 \u2019 \u2019 ... Because the variance-based sparsification method described in subsection 4.1 is orthogonal to the quantization shown above , we can reduce communication cost further using sparsity promoting quantization methods such as QSGD instead. \u2019 \u2019 \u2019 > 4.3 : allgatherv remark : does that mean that this approach would not scale well to higher number of workers ? It does scale well . We amended our paper with the following : Sec 4.3 \u2019 \u2019 \u2019 Thanks to the high compression ratio possible with this algorithm in combination with other compression methods , even large numbers of workers can be supported. \u2019 \u2019 \u2019 > 4.4 : Remarks about quantization and mantissa manipulation are not clear to me again , or what is the point in doing so . Possible because the problems above . To make a point of the mantissa operations clear , we amended our paper with the following : \u2018 \u2019 \u2019 The quantization of parameters described in subsection 4.2 can also be efficiently implemented with the standard binary floating point representation using only binary operations and integer arithmetic as follows . \u2019 \u2019 \u2019"}, {"review_id": "rkEfPeZRb-2", "review_text": "The authors propose a new gradient compression method for efficient distributed training of neural networks. The authors propose a novel way of measuring ambiguity based on the variance of the gradients. In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost. The proposed method can easily be combined with other existing method, i.e., Storm (2015), based on the absolute value of the gradient and shows further efficiency. The paper is well written: clear and easy to understand. The proposed method is simple yet powerful. Particularly, I found it interesting to re-evaluate the variance with (virtually) increasing larger batch size. The performance shown in the experiments is also impressive. I found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression. With this metric, the comparison would be easier and more intuitive. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and helpful suggestion . We tried to make a new single metric , however , we are not sure how to combine accuracy and compression ratio as they are not directly comparable . To make a comparison between methods more intuitive , we added scatter plots of accuracy and compression ratio in Appendix C ."}], "0": {"review_id": "rkEfPeZRb-0", "review_text": "This paper proposes a variance-based gradient compression method to reduce the communication overhead of distributed deep learning. Experiments on real datasets are used for evaluation. The idea to adopt approximated variances of gradients to reduce communication cost seems to be interesting. However, there also exist several major issues in the paper. Firstly, the authors propose to combine two components to reduce communication cost, one being variance-based gradient compression and the other being quantization and parameter encoding. But the contributions of these two components are not separately analyzed or empirically verified. Secondly, the experimental results are unconvincing. The accuracy of Momentum SGD for \u2018Strom, \\tau=0.01\u2019 on CIFAR-10 is only 10.6%. Obviously, the learning procedure is not convergent. It is highly possible that the authors do not choose a good hyper-parameter. Furthermore, the proposed method (not the hybrid) is not necessarily better than Strom except for the case of Momentum SGD on CIFAR-10. Please note that the case of Momentum SGD on CIFAR-10 may have a problematic experimental setting for Strom. In addition, it is weird that the experiment on ImageNet does not adopt the same setting as that on CIFAR-10 to evaluate both Adam and Momentum SGD. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We are glad to hear that you found our algorithm interesting . > But the contributions of these two components are not separately analyzed or empirically verified . Thank you for your comment . The main contribution is intended to be the variance-based gradient compression , with the quantization provided as a way to fit both values of gradient elements and its index in 32-bit while not rounding many elements to zero . We amended our paper with the following : Sec 4.2 \u2019 \u2019 \u2019 To allow for comparison with other compression methods , we propose a basic quantization process . \u2026 \u2019 \u2019 \u2019 > The accuracy of Momentum SGD for \u2018 Strom , \\tau=0.01 \u2019 on CIFAR-10 is only 10.6 % . Obviously , the learning procedure is not convergent . It is highly possible that the authors do not choose a good hyper-parameter . Thank you for your comment . We amended our paper with the following : Sec.6.1 \u2019 \u2019 \u2019 We note that we observed unstable behaviors with other thresholds around 0.01. \u2019 \u2019 \u2019 Appendix D \u2019 \u2019 \u2019 The code is available in examples of Chainer on GitHub. \u2019 \u2019 \u2019 > Furthermore , the proposed method ( not the hybrid ) is not necessarily better than Strom except for the case of Momentum SGD on CIFAR-10 . Please note that the case of Momentum SGD on CIFAR-10 may have a problematic experimental setting for Strom . Thank you for your comment . We amended our paper with the following : Sec.6.1 \u2019 \u2019 \u2019 We also would like to mention the difficulty of hyperparameter tuning in Strom 's method . \u2026 On the other hand , our algorithm is free from such problem . Moreover , when we know good threshold for Strom 's algorithm , we can just combine ours to get further compression. \u2019 \u2019 \u2019 > In addition , it is weird that the experiment on ImageNet does not adopt the same setting as that on CIFAR-10 to evaluate both Adam and Momentum SGD . Thank you for your comment . We amended our paper with the following : Sec 6.2 \u2018 \u2019 \u2019 We also evaluated algorithms with replacing MomentumSGD and its learning rate scheduling to Adam with its default hyperparameter . \u2019 \u2019 \u2019"}, "1": {"review_id": "rkEfPeZRb-1", "review_text": "The paper proposes a novel way of compressing gradient updates for distributed SGD, in order to speed up overall execution. While the technique is novel as far as I know (eq. (1) in particular), many details in the paper are poorly explained (I am unable to understand) and experimental results do not demonstrate that the problem targeted is actually alleviated. More detailed remarks: 1: Motivating with ImageNet taking over a week to train seems misplaced when we have papers claiming to train ImageNet in 1 hour, 24 mins, 15 mins... 4.1: Lemma 4.1 seems like you want B > 1, or clarify definition of V_B. 4.2: This section is not fully comprehensible to me. - It seems you are confusingly overloading the term gradient and words derived (also in other parts or the paper). What is \"maximum value of gradients in a matrix\"? Make sure to use something else, when talking about individual elements of a vector (which is constructed as an average of gradients), etc. - Rounding: do you use deterministic or random rounding? Do you then again store the inaccuracy? - I don't understand definition of d. It seems you subtract logarithm of a gradient from a scalar. - In total, I really don't know what is the object that actually gets communicated, and consequently when you remark that this can be combined with QSGD and the more below it, I don't understand it. This section has to be thoroughly explained, perhaps with some illustrative examples. 4.3: allgatherv remark: does that mean that this approach would not scale well to higher number of workers? 4.4: Remarks about quantization and mantissa manipulation are not clear to me again, or what is the point in doing so. Possible because the problems above. 5: I think this section is not too useful unless you can accompany it with actual efficient implementation and contrast the practical performance. 6: Given that I don't understand how you compress the information being communicated, it is hard to believe the utility of the method. The objective was to speed up training time because communication is bottleneck. If you provide 12,000x compression, is it any more practically useful than providing 120x compression? What would be the difference in runtime? Such questions are never discussed. Further, if in the implementation you discuss masking mantissa, I have serious concern about whether the compression protocol is feasible to implement efficiently, without writing some extremely low-level code. I think the soundness of work addressing this particular problem is damaged if not implemented properly (compared to other kinds of works in current ML related research). Therefore I highly recommend including proper time comparison with a baseline in the future. Further, I don't understand 2 things about the Tables. a) how do you combine the proposed method with Momentum in SGD? This is not discussed as far as I can see. b) What is \"QSGD, 2bit\" If I remember QSGD protocol correctly, there's no natural mapping of 2bit to its parameters.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review . We 're glad to hear that you found our technique to be novel . We 've amended our paper in light of your review . We hope this helps explain the details and demonstrate how our technique alleviates the problem of transmitting gradients between nodes . Section 1 > Motivating with ImageNet taking over a week to train seems misplaced when we have papers claiming to train ImageNet in 1 hour , 24 mins , 15 mins ... Thanks for the comment . We have amended our paper with the following : \u2018 \u2019 \u2019 For example , it takes over a week to train ResNet-50 on the ImageNet dataset if using a single GPU . \u2026 For example , when using 1000BASE-T Ethernet , communication takes at least ten times longer than forward and backward computation for ResNet-50 , making multiple nodes impractical . High performance interconnections such as InfiniBand and Omni-Path are an order of magnitude more expensive than commodity interconnections , which limits research and development of deep learning using large-scale datasets to a small number of researchers. \u2019 \u2019 \u2019 > 4.1 : Lemma 4.1 seems like you want B > 1 , or clarify definition of V_B . Correct.We have amended our paper with the following : \u2018 \u2019 \u2019 Lemma 4.1 A sufficient condition that a vector -g is a descent direction is \\|g - \\nabla f ( x ) \\|_2^2 < \\|g\\|_2^2 . We are interested in the case of g = \\nabla f_B ( x ) , the gradient vector of the loss function over B . By the weak law of large numbers , when B > 1 , the left-hand side with g = \\nabla f_B ( x ) can be estimated as follows. \u2019 \u2019 \u2019 Note , \\nabla_B f ( x ) in lemma 4.1 of our first paper was replaced with a symbol g. > 4.2 : This section is not fully comprehensible to me . > - It seems you are confusingly overloading the term gradient and words derived ( also in other parts or the paper ) . What is `` maximum value of gradients in a matrix '' ? Make sure to use something else , when talking about individual elements of a vector ( which is constructed as an average of gradients ) , etc . You \u2019 re right . We amended the paper to replace gradient with \u2018 gradient element \u2019 when we refer elements of gradient vectors . \u2018 \u2019 \u2019 Our quantization except for the sign bit is as follows . For a weight matrix W_k ( or a weight tensor in CNN ) , there is a group of gradient elements corresponding to the matrix . Let M_k be the maximum absolute value in the group. \u2019 \u2019 \u2019 > - Rounding : do you use deterministic or random rounding ? Do you then again store the inaccuracy ? Good questions . We amended our paper as follows : Sec 4.2 \u2019 \u2019 \u2019 ... We do not adopt stochastic rounding like QSGD nor accumulate rounding error g_i - g'_i for the next batch because this simple rounding does not harm accuracy empirically. \u2019 \u2019 \u2019 > - I do n't understand definition of d. It seems you subtract logarithm of a gradient from a scalar . d is a difference of two scalars . > In total , I really do n't know what is the object that actually gets communicated , and consequently when you remark that this can be combined with QSGD and the more below it , I do n't understand it . This section has to be thoroughly explained , perhaps with some illustrative examples . We hope our clarification between \u2018 gradient \u2019 and \u2018 gradient element \u2019 made the definition of d clearer . We amended our paper as follows : Sec 4.2 \u2018 \u2019 \u2019 ... After deciding which gradient elements to send , each worker sends pairs of a value of a gradient element and its parameter index \u2026 \u2019 \u2019 \u2019 Sec 4.2 \u2019 \u2019 \u2019 ... Because the variance-based sparsification method described in subsection 4.1 is orthogonal to the quantization shown above , we can reduce communication cost further using sparsity promoting quantization methods such as QSGD instead. \u2019 \u2019 \u2019 > 4.3 : allgatherv remark : does that mean that this approach would not scale well to higher number of workers ? It does scale well . We amended our paper with the following : Sec 4.3 \u2019 \u2019 \u2019 Thanks to the high compression ratio possible with this algorithm in combination with other compression methods , even large numbers of workers can be supported. \u2019 \u2019 \u2019 > 4.4 : Remarks about quantization and mantissa manipulation are not clear to me again , or what is the point in doing so . Possible because the problems above . To make a point of the mantissa operations clear , we amended our paper with the following : \u2018 \u2019 \u2019 The quantization of parameters described in subsection 4.2 can also be efficiently implemented with the standard binary floating point representation using only binary operations and integer arithmetic as follows . \u2019 \u2019 \u2019"}, "2": {"review_id": "rkEfPeZRb-2", "review_text": "The authors propose a new gradient compression method for efficient distributed training of neural networks. The authors propose a novel way of measuring ambiguity based on the variance of the gradients. In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost. The proposed method can easily be combined with other existing method, i.e., Storm (2015), based on the absolute value of the gradient and shows further efficiency. The paper is well written: clear and easy to understand. The proposed method is simple yet powerful. Particularly, I found it interesting to re-evaluate the variance with (virtually) increasing larger batch size. The performance shown in the experiments is also impressive. I found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression. With this metric, the comparison would be easier and more intuitive. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and helpful suggestion . We tried to make a new single metric , however , we are not sure how to combine accuracy and compression ratio as they are not directly comparable . To make a comparison between methods more intuitive , we added scatter plots of accuracy and compression ratio in Appendix C ."}}