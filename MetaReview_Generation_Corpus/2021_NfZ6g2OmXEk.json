{"year": "2021", "forum": "NfZ6g2OmXEk", "title": "Prioritized Level Replay", "decision": "Reject", "meta_review": "The paper presents a method for automatically generating levels of varying complexity for training the agent.  The results are well summarized in the paper abstract, \"significantly improved sample-efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments.\" The work is clearly presented, and the experiments are thorough. \n\nR1, R2, and R3 voted to accept the paper with 7, 6, and 7 scores. R4 voted to reject the paper with a score of 5. The reviewers mostly agree (except for R4) that significant performance gains have been achieved. R4 is unsatisfied as he/she believes that performance gains are small and exploit the simulator (e.g., using resets). \n\nThe paper's main pro is well summarized by R4's comment, \"The method of the paper is simple and can be incorporated into many existing RL algorithms.\"\n\nThe main drawback of the paper is that many curriculum learning techniques have been proposed in the past. E.g., Matiisen et al. (https://arxiv.org/pdf/1707.00183.pdf). In fact the authors discuss this work in the related work section, but dub it multi-agent RL work. This is not true. The method of Matiisen et al. is very similar to the proposed approach but uses a different criterion for learning progress. Comparison to this work is warranted, without which the paper should not be accepted. In the post-rebuttal discussion, R2 and R3 agree that this comparison is necessary. Therefore, I recommend that this paper be rejected for now and resubmitted to a future venue after incorporating a comparison with Matiisen et al. \n\n\n", "reviews": [{"review_id": "NfZ6g2OmXEk-0", "review_text": "This paper concerns about the use of experience replay in a way that past experience is sampled based on ( implicit ) levels so as for the agent to better adapt to the current task at hand . The authors defined a replay distribution ( where experience is sampled ) based on two scores relevant to learning potential and staleness . Due to its formulation , the change of replay distribution can be used as an outer-layer of a learning algorithm without any modification of the underlying learning mode . The authors conducted experiments over a set of benchmark data sets relevant to level-ness and found statistically significant improvements over more than half of the tasks . The overall impression of the paper is that it presents a simple yet effective solution to prioritizing experience in the presence of level-ness in a given task . The basic idea is finding out past experience with high `` learning potential '' by examining a past trajectory 's 'wrongness ' and how long the policy was not updated ( = likely still wrong ) . Point : The notion of level and its relevance to learning potential . First , the paper does not contain any mathematical ( or clear ) definition of level , which should be crucial to understand the paper . At the beginning it is only explained as different configurations ( i.e. , any non-singleton environment ) . Further , it is hard to understand why the notion of levels is even needed to be employed in the paper . An RL agent has a specific way to learn experience ( updating parameters ) and its artifacts makes `` experience replay '' useful in most RL agents . Then , there would be an optimal way of replaying experience at any given time a certain order of a subset of past trajectories to be replayed for a current policy . The current form of P_replay ( Eq.1 ) does not need any specific notion of level-ness but only 'learning potential ' . I suspect that Eq.1 also works for a singleton environment , which the authors excluded from consideration . Point : The conjecture about curriculum learning . It is reasonable to assume that the notion of hardness of a task for an RL agent is the difficulty of optimizing its policy ( =resulting in higher TD-errors ) . When the human understanding of easiness of a task ( i.e. , level ) matches the agent 's ability to optimize , we would safely say that PLR induces curriculum implicitly . It is nice to see such plots ( Figure 4 ) that empirically validate the conjecture . However , is n't it a much anticipated result ? Questions Q1 . Would different algorithms other than ( PPO + GAE ) make the results different from the current form ? Q2 : If we interpret P_S and P_C as two probability distributions , multiplying them seems more natural to me . What is the rationale behind for adding them not multiplying them ( or use ( 1-rho ) log P_S + ( rho ) log P_C ) ? Further , any reason for P_C being proportional to c-C_i ? Q2.How about learning hyper-parameters on the fly ? Both \\beta and \\rho might be adjusted throughout learning . Further , it is conceivable that the optimal \\beta and \\rho are not fixed quantities but can be dependent to a given pair of policy and trajectory . Minor Figure 1 , there are two taus . The top would be \\pi ? Background `` We to refer to '' = I read through all the reviews and rebuttals and I could better understand and evaluate the paper . I updated my score to 7 . Given that this replay scheme works fairly well ( intuitively , empirically ) , easy to understand and implement , fairly sufficient amount of empirical experimentation , I would like to see the paper accepted ( and adopted and improved by others ) . One more comment about staleness . I think staleness is a proxy measure for the ( unmeasured ) score of the 'current ' policy on that level . So I would like to see ( in future or revised version ) some experiments that measure how well staleness measure correlate with such score . Further , the way staleness is designed properly reflects how the score degrades as the level is n't played . Some idea . It would be nice to make a connection to multi-task learning where tasks share some similarities . Currently , level is somewhat 'linearly ' defined . If an agent plays level x , then staleness for level x ' ( something similar to x ' ) does n't have to be updated a lot compared to another task which might be dissimilar to level x . Hence , some similarity measure can be further employed ( or learn a metric ) .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their feedback to improve our paper . From reading your review , we are not very clear as to why the score is low given the fairly positive tenor of the review . We are confident that the points you make are addressed both in the revisions to the paper made based on your feedback , and by the responses below . We hope this alleviates any concerns you might have , and that you will be prepared to support the paper or further explain what stands between the paper and a supportive assessment on your part . # # # On the notion of replay We believe the reviewer has misunderstood fundamental aspects of our method . As defined in the paper , we use \u201c replay \u201d to refer to sampling a * new * trajectory from a level , * not * training on past trajectories collected from that level , e.g.from a replay buffer\u2014which is not performed by standard policy-gradient methods such as PPO used in this paper . We appreciate this distinction is only clearly drawn in page 2 of the paper , and we have tweaked the abstract and introduction to improve clarity on this point . # # # On combining the staleness and score distributions The reviewer asks * * \u201c if we interpret P_S and P_C as two probability distributions , multiplying them seems more natural to me . What is the rationale behind for adding them not multiplying them \u201d * * . Could the reviewer please clarify why taking the product seems more natural ? That would be a mechanism for taking the joint probability of two random variables . Here we have two distributions over one random variable ( which level to sample ) , and thus we induce a mixture over them by taking the convex combination ( which yields , of course , a valid and normalized distribution ) , as is done in mixture models such as GMMs . There is no canonical or \u201c one true way \u201d of doing such combinations of distributions , but we believe this is as close to standard as it comes in statistics . # # # On the definition of level We intentionally make a weak assumption of what a level is as this method is generally applicable to any environment in which variations of the environment instances can be determined by some index value , e.g. , a seed , a named environment configuration , etc . We are unsure what would be gained from an attempt to formally pin this down , but are happy to hear from you regarding this . Alternatively , would you be satisfied with further examples of what might constitute a \u201c level \u201d , at the point in the paper where the term is introduced ? # # # On surprisingness The reviewer states * * \u201c the improvement in empirical results is not particularly surprising \u201d * * . We respectfully strongly disagree : it is not obvious at all , as only a single score function worked well , and there are intuitive reasons to believe each of the tested score functions should work . For example , we imagine the reviewer would agree with us that using policy entropy is a perfectly valid hypothesis for inducing a curriculum that leads to improved generalization\u2014however our experiments show that the opposite is the case . Moreover , as discussed in Section 5.1 and further shown in Appendix C , the method only provides gains when the score-based distribution is mixed with the staleness-based distribution , while sampling from either of these two distributions separately does not work . There is a fairly intuitive explanation for this , post-hoc , which is that scores drift increasingly \u201c off-policy \u201d as the staleness increases , and the mixture cancels this out . We will include a brief mention of this in our results section , but we maintain that this is a novel and un-intuitive finding . Fortunately , most findings become intuitive and unsurprising once explained ."}, {"review_id": "NfZ6g2OmXEk-1", "review_text": "* * SUMMARY * * The present work considers the problem of learning in procedurally generated environments . This is a class of simulation environments in which each individual environment is created algorithmically where certain environmental factors are varied in each instance ( referred to as levels in this work ) . Learning algorithms in this setting typically use a fixed set of training and evaluation environments . The present work proposes to sample the training environments such that the learning progress of the agent is optimized . This is achieved by proposing an algorithm for level prioritization during training . The performance of the approach is demonstrated on the Procgen Benchmark and two MiniGrid benchmarks and the authors argue that their approach induces an implicit curriculum in sparse reward settings . * * STRENGTHS * * - The general idea of prioritization for level sampling makes a lot of sense and is demonstrated to improve sample-efficiency for skill learning in procedurally generated environments . - I also liked that the authors compared with a big variety of different scoring metrics . * * WEAKNESSES * * - The intuition of `` greater discrepancy between expected and actual returns , making \u000e $ \\delta_t $ a useful measure of the learning potential '' makes sense . The heuristic score also works well in practice . One limitation I see is that there is no theoretical justification for why the TD-error is a good predictor for learnability . - This is maybe more an avenue for future work than an actual weakness but it seems to me that the algorithm is not making use of all potentially useful information . In each timestep , it only considers the last score achieved in a level . Maybe it would also be interesting to consider the full history of scores . My intuition is that levels in which agents were historically very slow to learn are maybe not as useful ( or at least not useful at the moment ) . I.e. , maybe in order to learn competing at such levels it is better to compete on other levels first ? - Is there , at least from a qualitative perspective , an explanation for why certain environments do not benefit as much from the proposed level sampling approach ? * * REPRODUCIBILITY * * The work seems reproducible . Most of the information relevant for reproducibility is given in Appendices A & B . It would be great if the authors would also make the source code available . * * CLARITY * * Overall , I found the work to be very clearly written and have only minor questions/remarks : - To what extent does the use of TD-errors potentially limit the type of learning algorithms that can be used in the context of the proposed framework . Computing the TD-error requires a value function . As I understand it , some RL algorithms never compute a value function . - If I have n't overlooked it , there is no explanation of $ c $ after eq . ( 4 ) while $ C_i $ is explained earlier . Is $ c $ simply the current episode ? * * EVALUATIONS * * The work is compared with several scoring function baselines using PPO . While the authors claim that the method is applicable to other RL agents , the evaluations do not show any results with other agent types . The authors mention several different benchmarks in that space . It would be interesting to know why particularly Procgen Benchmark and MiniGrid environments were chosen . It is also not clearm to me why PPO is used as the base agent . Was this for ease of implementation / its popularity ? Would n't it make sense to use more recent agents to see the added benefit of the proposed approach . E.g. , would V-MPO be applicable here ? * * NOVELTY / RELEVANCE * * The work is very interesting and the authors make a compelling case that procedurally generated environments can benefit from a conscious sampling of the levels with regard to usefulness for learnability . I am not sure whether the claim `` Prioritized Level Replay induces an implicit curriculum , taking the agent gradually from easier to harder levels . '' is fully valid . As I understand it , the hardest levels are also the most likely to be sampled . The force counteracting this to some extent is the staleness-based sampling term $ P_C $ . For a gradual curriculum , I would expect $ P_S $ to be designed such that it does not choose the hardest level but the one promising the best learning outcome . Particularly in the early stages of the training , the hard levels might be less useful than levels of medium difficulty . * * SUMMARY * * I found that paper very interesting . While I am not working in the particular subfield of the work and can not sufficiently judge relation with prior works , I can confidently say that the idea and implementation details were conveyed very well . My main concerns are regarding the understanding of the `` failure cases '' and to what extent the graduality claim applies . That being said , I believe this line of work to be really interesting and to have a lot of potential for improved sample-efficiency when training RL agents in algorithmically generated simulation environments . * * POST-DISCUSSION UPDATE * * I want to thank the authors for correcting my misunderstandings , answering my questions , and providing additional material . As a consequence of this , I have raised my score to `` Accept '' . To answer your question about what would be needed for a higher score : For a strong accept recommendation , I would have expected a mix of several additional things such as a clear impact outside of own subfield , code availability at time of submission ( to evaluate how easy it is to reproduce the results and re-use the code ) , or more additional theoretical justification ( in the sense of new formal guarantees for at least certain aspects of the proposed method ) . While not directly working in this subfield , I still think this work is solid and worthy of publication .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful and detailed comments that will improve our paper . It is great to hear the reviewer found our approach sensible , our writing clear , and that they praised the large variety of scoring functions that are explored . We address , here and in our [ post summarizing the changes to the paper ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , your main concerns , and hope that this will lead to you considering strengthening your recommendation or explaining what still stands in the way , so that we may further improve the paper . # # # Clarification about curriculum \u201c As I understand it , the hardest levels are also the most likely to be sampled \u201d We respectfully believe this is a misconception . At the end of Section 5.1 on page 7 , we state \u201c easier levels result in non-zero , non-stationary returns earlier in training , while harder levels give rise to near stationary returns until the agent learns an improved policy that allows making further progress on the level [ ... ] sampling levels according to the L1 value-loss then leads to an implicit curriculum from easier to harder levels. \u201d What matters for a score of a level to be high based on L1 value-loss is whether or not the agent , given the current policy , over- or under-estimates value , which does not consistently correlate with the relative difficulty of the environment . For instance , in early stages of training the agent might correctly estimate low value for harder levels , thus sampling more frequently easier levels to improve it \u2019 s policy before gradually sampling harder levels more often . This is a hypothesis that we verify qualitatively in Figure 4 . # # # On the limits imposed by the use of TD error In our work , we investigated policy gradient methods , which near-uniformly make use of a value estimate . This does not cover all approaches to RL , but a large class of state-of-the-art actor-critic policy-gradient methods such as : PPO , IMPALA , A2C/A3C , A2C-AKTR , APPO , and Phasic Policy Gradients . We agree it would be interesting to investigate whether the core mechanisms of Prioritized Level Replay can also be combined with value-based RL methods ( e.g.DQN ) for future research . However , this is outside of the scope of the present paper , and is something we are investigating in follow-up work . # # # Theoretical justification of TD error for scoring learning potential : As motivated in the paper , the TD error is the difference between the empirical return and the predicted return . When this discrepancy is high , there is a greater opportunity for the agent to learn . In fact , the same reasoning is used in Schaul _et al._ 2016 ( Prioritized Experience Replay ) to motivate the use of TD errors as a learning signal for ranking the utility of sampling * past * transitions in the experience replay buffer . Further , the advantage-based gradient estimator used in nearly all actor-critic methods , including PPO which is used in our paper , entails computing nearly the same TD-error terms , so our use of TD-error-based scores may be seen as roughly correlating with the value of the gradient estimate resulting from the last trajectory taken over each level ."}, {"review_id": "NfZ6g2OmXEk-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Summary * * : This paper proposes a prioritized sampling strategy for task sampling in procedurally generated environments . While training an RL agent across many tasks ( levels ) , we can either sample a new task uniformly from the training task distribution or sample a new task with different weights . The paper claims that sampling based on the average magnitude of generalized advantage estimate ( GAE ) yields faster learning in most Procgen environments and a few MiniGrid environments . Overall , I found the idea to be simple and intuitive . But the benefit of using prioritized level replay is also not very consistent across different environments used in the paper . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Strengths * * : The method of the paper is simple and can be incorporated into many existing RL algorithms . The paper shows that L1 value loss is a good scoring metric for the prioritization by comparing several different choices . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Weaknesses * * : The advantage of using prioritized level replay against uniform sampling is rather small in many tasks ( 11 out of 19 tasks ) shown in the paper ( Climber , Coinrun , Dodgeball , Fruitbot , Heist , Jumper , Maze , Miner , Ninja , Starpilot , ObstructedMazeGamut-Medium ) . The paper only presents results in the easy mode of procgen . While I understand the reason due to the limit on the computational resources , it would be more convincing to show the results on at least 1 or 2 procgen tasks in the difficult mode . If the overall task difficulty is increased , then the advantage of learning in a curriculum ( starting from the easy tasks and then to the difficult tasks ) are expected to be more salient . While the scoring metrics used in the paper are all related to the policy function or value function that is being learned , how about a scoring metric that is only based on the number of steps that the agent experiences in a task and whether the agent fails or succeeds ? Intuitively , if the lifetime of an agent is short and the agent solves the task , it is an easy task . If the agent does not solve the task or it takes the agent many more steps to solve the task , it is a difficult task . Another metric to compare to is prioritize based on the return value of the trajectories . If the return value is high , then the task is probably already solved by the current policy , so we can sample such tasks less frequently . In Figure 4 , it seems the advantage of using L1 value loss for the prioritization in sampling is more obvious in easy environments ( Multiroom-N4-Random and ObstructedMazeGamut-Easy ) . But its performance becomes very close to the uniform sampling strategy in harder environments ( ObstructedMazeGamut-Medium ) . Why would the advantage of using prioritization ( hence implicit curriculum ) fade as the task difficulty increases ? In Figure 4 , it is hard to connect the top row to the bottom row as the top row uses the environment steps for the x-axis , the bottom row uses the number of PPO updates for the y-axis . I would suggest plot the bottom row figures in terms of the environment steps as well and use the same x-range . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Minor points * * : Some details about the experiment setup , especially the MiniGrid environments , are missing . For example , how do the MiniGrid environments look like , what does the difficulty mean in these environments , which parts of the environments are randomized across levels , reward structure , etc .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their feedback that will improve the paper . # # # On the benefits of Level Replay The reviewer states \u201c benefit of using prioritized level replay is also not very consistent across different environments \u201d / \u201c advantage of using prioritized level replay against uniform sampling is rather small in many tasks \u201d . As stated in our [ joint response to all reviewers ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , we want to strongly emphasize that in the initial submission of the paper we reported * statistically significantly * better generalization ( as determined by Welch \u2019 s t-test ) on the majority ( 11 out of the 16 ) of Procgen envs and are on par with the others . On MiniGrid , we report statistically significant improvements in sample efficiency on all 3 environments , and statistically significant gains in final test performance on ObstructedMazeGamut-Easy and ObstructedMazeGamut-Medium . In the updated version of the paper , we now set a new SOTA on Procgen Benchmark when our method is combined with the previous SOTA-method UCB-DrAC , and by a fair margin . In particular , these additional results show its applicability to other methods than standard PPO , setting a new SOTA for the OpenAI Procgen Benchmark via augmenting UCB-DrAC [ Raileanu _et al._ 2020 ] ( https : //arxiv.org/pdf/2006.12862.pdf ) with Prioritized Level Replay . This result also concretely demonstrates that our method can enable complementary improvements in combination with other powerful methods for improving generalization , which is a strong advantage of our method\u2014it can be implemented in conjunction with most any other method , and as we see , can thereby provide additive gains in sample-efficiency and generalization performance . # # # Evaluating against OpenAI Procgen Hard Using OpenAI Procgen Benchmark \u201c easy \u201d for our experiments is not only based on the high computational demand of training on \u201c hard \u201d , but also due to the precedent in the literature to use \u201c easy \u201d instead of \u201c hard \u201d . For example , see [ Laskin _et al._ ( NeurIPS 2020 ) \u201c Reinforcement Learning with Augmented Data \u201d ] ( https : //arxiv.org/abs/2004.14990 ) and [ Raileanu _et al._ ( 2020 ) \u201c Automatic data augmentation for generalization in deep reinforcement learning. \u201d ] ( https : //arxiv.org/abs/2006.12862 ) Nonetheless , we recognise the value of at least running some experiments on a selection of hard tasks . We have run experiments on Procgen benchmark \u2019 s hard difficulty setting and added them to the paper in Appendix C. We find that Prioritized Level Replay works on these environments as well , specifically resulting in an average gain across games of 39 % over PPO with uniform level sampling . This further strengthens our claims about the robustness and applicability of our method . Thank you for suggesting this , and we hope that these additional results give you the confidence you need to fully support the publication of this paper . # # # Regarding your step-count-based scoring function suggestion Thank you for suggesting this additional baseline . We have discussed this a lot over the last few days . Our concerns are : 1 . That trajectory length is not a signal that meaningfully correlates to difficulty . In some games , e.g.MiniGrid , shorter trajectories mean a better policy ( higher return ) , while on others , e.g.most Procgen Benchmark games , longer trajectories correlate with better policy ( surviving longer and therefore obtaining a higher return ) . This makes step-count-based scoring functions not generally transferable across environments , unlike the TD-error-based scoring functions , which we empirically show work across over a dozen environments . 2.Sampling based on return also does not make sense . If you bias toward sampling for high return , you will likely oversample the easy levels , which the agent will master quickly and reinforce this sampling bias , thereby only rarely having a chance at learning on harder levels . If you bias towards sampling low return levels , you will tend to sample hard levels that the agent can not solve , and the agent will make slower learning progress . As a result , we are unsure exactly what scoring function would make sense for conditioning on trajectory length and episode success or return . Do you have a specific one in mind ? If so , we are happy to attempt to try and run it on some environments and share any results before the end of the discussion period . That said , we will be open-sourcing the code and hope people will feel empowered to try their own scoring metrics and variants on our ideas . As described in the paper , Prioritized Level Replay describes a general framework for a class of selective-sampling algorithms for sampling training levels in an RL setting . The aim of our paper is to present this framework in addition to empirical studies demonstrating the effectiveness of a specific instance of this class of algorithms , value-based level replay ( where the scoring function is the L1 value-loss ) , across a wide variety of environments ."}, {"review_id": "NfZ6g2OmXEk-3", "review_text": "# # # Paper Summary This paper allows agents to set the initial conditions ( level ) for procedurally generated episodes during exploration to past observed values , and proposes to have agents form an intrinsic curriculum by resampling past levels based on a heuristic measure of expected learning progress . The authors test several heuristic measures and find that the average absolute magnitude of the generalized advantage estimate works well . The authors hypothesize that this intrinsic curriculum will improve optimization/learning relative to an agent that always samples initial conditions from the environment distribution . The authors verify that their prioritization strategy usually improves performance in several Progen Benchmark and MiniGrid environments , usually by a small but statistically significant amount , but sometimes by a large amount . # # # Summary Review ( highlights re : quality , clarity , originality and significance ) The paper is well written and clear after one understands the basic idea . The idea is simple , and the algorithm/experiments seem straightforward to reimplement . The experiments are about what one would expect and seem to be well executed . The idea is original but not particularly innovative ( this seems like the first heuristic prioritization approach that would come to mind given that the agent is able to choose the level ) . The improvement in empirical results is not particularly surprising ( if anything , I would have expected more large improvements like the ones on bigfish/leaper environments ) . As this method is constrained to procedurally generated environments ( or at least , evaluation of the method is constrained to procedurally generated environments ) , the significance seems rather limited . The required assumption seems rather strong , as it requires a simulator / control over the environment , which limits applicability . # # # Pros - This a simple idea that can improve performance in Procedurally Generated Environments given that the agent is allowed to set the initial conditions / pick the level . - The performance improvement in 4 of the 19 environments tested is large & seems absolute ( i.e. , it 's seems like a final performance improvement , not just a sample efficiency improvement ) . - The paper is well written/presented , easy to understand , and the empirical evaluation seems well done . The results do not seem difficult to replicate . # # # Cons - Despite being less intrusive than direct access to the level generation mechanism , the assumption that the agent can replay levels seems rather strong to me , and simplifies the task of learning procedurally generated environments very substantially . ( $ \\dagger $ ) I would argue that we don \u2019 t use procedurally generated environments as benchmarks in order to solve procedurally generated environments , but rather a tool for measuring generalization , so it 's unclear to me that a technique that improves sample efficiency only in a procedurally generated environment is useful . Unlike environment-agnostic techniques like prioritized replay , HER , intrinsic reward , intrinsic goal selection , etc. , this requires you to have control over the environment , which seems to limit the applicability . If this is only useful with a simulator , then the small gains in sample efficiency aren \u2019 t actually that relevant , though this approach does seem to improve final performance in 4 of the 19 environments tested . - It \u2019 s not clear until the second page whether your method is a prioritized replay buffer scheme , or a task selection scheme . Actually , I was certain it was a prioritized replay buffer scheme until the second page , because that is the more natural/general setting ( as noted above , I find the assumption that the agent can replay levels to be rather strong ) . - Several new hyperparameters are introduced ; this said , guidance/ablations are performed , and it seems like the choices will generalize decently well ( albeit there were different choices for ProcGen/Minigrid ) . # # # Questions / Etc . - My main question for the authors is to ask for a counterargument to ( $ \\dagger $ ) above . - It would be good if this can be shown to work in multi-goal setting , as it is quite similar to ProcGen setting ... you draw some distinctions , but I do think your approach would be applicable there .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their feedback that we will use to improve our paper . # # # On the applicability of this method The reviewer states : * * \u201c evaluation of the method is constrained to procedurally generated environments \u201d / \u201c unclear to me that a technique that improves sample efficiency only in a procedurally generated environment is useful \u201d * * . As we explain in our [ joint response to all reviewers ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , using procedurally generated environments for evaluation is not a weakness but instead a main strength of our approach compared to work that relies on specific properties of an underlying environment instance , e.g.Atari games in the Arcade Learning Environment . Procedurally generated environments are much harder to master due their constant stream of novel observations that the agent has to generalize towards . Many previously successful methods ( e.g.Go-Explore , count-based exploration , etc . to name just a few ) would fail as they assume the environment stays fixed and the agent can memorize trajectories to high value regions in the state space . The reviewer further states : * * \u201c it requires a simulator / control over the environment , which limits applicability \u201d : * * Note that assuming one can replay a level ( or , more concretely , any configuration of the environment ) is a much weaker assumption than assuming there is only a * single * configuration the agent needs to do well in ( as it is the case in Atari ) . In contrast to Atari , we test for systematic generalization by sampling completely unseen seeds ( and thus environment instances ) at test time . Additionally , note that any environment that displays random initialized states can be viewed as procedurally-generated . Take for example , a robot reaching task that starts with a random arrangement of objects . In this case , \u201c resetting the seed \u201d during training would equate to simply returning the objects to the corresponding initial arrangement , and a completely manageable task for real-world training , which requires resetting the initial arrangement of objects at the start of each episode anyway . If we are instead training in simulation with access to the simulator ( which is more often than not in RL ) , then why not take advantage of it ? As the goal of training on procedurally-generated environments is to test for and improve generalization , if a training strategy uses a privileged action\u2014such as resetting the env seed at the start of training episodes\u2014does not subvert the integrity of the test time evaluation protocol of the agent , then we should by all means take advantage of such a strategy . # # # Regarding sample efficiency and performance The reviewer also states * * \u201c small gains in sample efficiency aren \u2019 t actually that relevant , though this approach does seem to improve final performance in 4 of the 19 environments tested \u201d * * : As stated in our [ joint response to all reviewers ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , we strongly emphasize that in the initial submission of the paper we reported * statistically significantly * better generalization ( as determined by Welch \u2019 s t-test over 10 runs ) on not 4 but * * 11 * * out of the 16 Procgen Benchmark envs , 3 out of 3 MiniGrid environments tested ( in terms of sample efficiency and/or final test returns ) , and are on par for the others . In the updated version of the paper , we now report a new SOTA on Procgen benchmark when our method is used in combination with a UCB-DrAC agent , reporting statistically significant gains on 14 of 16 games , with much higher gains on average per game . # # # On defining the notion of replay Thank you for bringing to our attention that the notion of replay ( as in gathering * new * experience from a level ) used in our paper , in contrast to the notion of replay used in experience replay , is not clear until the second page . We agree , and we have changed the writing of the abstract and introduction to reflect this . We trust you will find the paper is improved as a result , but please let us know if it is somehow still in need of additional clarity . # # # Summary Thank you for your pertinent questions and comments . We hope the responses , and the improvements we have made to the paper in response to your feedback , have convinced you that the paper is worthy of your support . We strongly believe it proves the concept and is rigorously evaluated and compared , including ( as of this revision ) against the state of the art on the OpenAI Procgen Benchmark ( which it improves upon ) . Naturally , there are further experiments to be done , including investigating application of this method to looser notions of \u201c level \u201d outside of procgen ( e.g.starting states or , as you suggest , multi-goal settings ) , but these constitute ambitious and exciting matter for future work we hope to investigate . In the meantime , we would be grateful for your support for this paper , and are happy to further discuss outstanding concerns you may have , if any ."}], "0": {"review_id": "NfZ6g2OmXEk-0", "review_text": "This paper concerns about the use of experience replay in a way that past experience is sampled based on ( implicit ) levels so as for the agent to better adapt to the current task at hand . The authors defined a replay distribution ( where experience is sampled ) based on two scores relevant to learning potential and staleness . Due to its formulation , the change of replay distribution can be used as an outer-layer of a learning algorithm without any modification of the underlying learning mode . The authors conducted experiments over a set of benchmark data sets relevant to level-ness and found statistically significant improvements over more than half of the tasks . The overall impression of the paper is that it presents a simple yet effective solution to prioritizing experience in the presence of level-ness in a given task . The basic idea is finding out past experience with high `` learning potential '' by examining a past trajectory 's 'wrongness ' and how long the policy was not updated ( = likely still wrong ) . Point : The notion of level and its relevance to learning potential . First , the paper does not contain any mathematical ( or clear ) definition of level , which should be crucial to understand the paper . At the beginning it is only explained as different configurations ( i.e. , any non-singleton environment ) . Further , it is hard to understand why the notion of levels is even needed to be employed in the paper . An RL agent has a specific way to learn experience ( updating parameters ) and its artifacts makes `` experience replay '' useful in most RL agents . Then , there would be an optimal way of replaying experience at any given time a certain order of a subset of past trajectories to be replayed for a current policy . The current form of P_replay ( Eq.1 ) does not need any specific notion of level-ness but only 'learning potential ' . I suspect that Eq.1 also works for a singleton environment , which the authors excluded from consideration . Point : The conjecture about curriculum learning . It is reasonable to assume that the notion of hardness of a task for an RL agent is the difficulty of optimizing its policy ( =resulting in higher TD-errors ) . When the human understanding of easiness of a task ( i.e. , level ) matches the agent 's ability to optimize , we would safely say that PLR induces curriculum implicitly . It is nice to see such plots ( Figure 4 ) that empirically validate the conjecture . However , is n't it a much anticipated result ? Questions Q1 . Would different algorithms other than ( PPO + GAE ) make the results different from the current form ? Q2 : If we interpret P_S and P_C as two probability distributions , multiplying them seems more natural to me . What is the rationale behind for adding them not multiplying them ( or use ( 1-rho ) log P_S + ( rho ) log P_C ) ? Further , any reason for P_C being proportional to c-C_i ? Q2.How about learning hyper-parameters on the fly ? Both \\beta and \\rho might be adjusted throughout learning . Further , it is conceivable that the optimal \\beta and \\rho are not fixed quantities but can be dependent to a given pair of policy and trajectory . Minor Figure 1 , there are two taus . The top would be \\pi ? Background `` We to refer to '' = I read through all the reviews and rebuttals and I could better understand and evaluate the paper . I updated my score to 7 . Given that this replay scheme works fairly well ( intuitively , empirically ) , easy to understand and implement , fairly sufficient amount of empirical experimentation , I would like to see the paper accepted ( and adopted and improved by others ) . One more comment about staleness . I think staleness is a proxy measure for the ( unmeasured ) score of the 'current ' policy on that level . So I would like to see ( in future or revised version ) some experiments that measure how well staleness measure correlate with such score . Further , the way staleness is designed properly reflects how the score degrades as the level is n't played . Some idea . It would be nice to make a connection to multi-task learning where tasks share some similarities . Currently , level is somewhat 'linearly ' defined . If an agent plays level x , then staleness for level x ' ( something similar to x ' ) does n't have to be updated a lot compared to another task which might be dissimilar to level x . Hence , some similarity measure can be further employed ( or learn a metric ) .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their feedback to improve our paper . From reading your review , we are not very clear as to why the score is low given the fairly positive tenor of the review . We are confident that the points you make are addressed both in the revisions to the paper made based on your feedback , and by the responses below . We hope this alleviates any concerns you might have , and that you will be prepared to support the paper or further explain what stands between the paper and a supportive assessment on your part . # # # On the notion of replay We believe the reviewer has misunderstood fundamental aspects of our method . As defined in the paper , we use \u201c replay \u201d to refer to sampling a * new * trajectory from a level , * not * training on past trajectories collected from that level , e.g.from a replay buffer\u2014which is not performed by standard policy-gradient methods such as PPO used in this paper . We appreciate this distinction is only clearly drawn in page 2 of the paper , and we have tweaked the abstract and introduction to improve clarity on this point . # # # On combining the staleness and score distributions The reviewer asks * * \u201c if we interpret P_S and P_C as two probability distributions , multiplying them seems more natural to me . What is the rationale behind for adding them not multiplying them \u201d * * . Could the reviewer please clarify why taking the product seems more natural ? That would be a mechanism for taking the joint probability of two random variables . Here we have two distributions over one random variable ( which level to sample ) , and thus we induce a mixture over them by taking the convex combination ( which yields , of course , a valid and normalized distribution ) , as is done in mixture models such as GMMs . There is no canonical or \u201c one true way \u201d of doing such combinations of distributions , but we believe this is as close to standard as it comes in statistics . # # # On the definition of level We intentionally make a weak assumption of what a level is as this method is generally applicable to any environment in which variations of the environment instances can be determined by some index value , e.g. , a seed , a named environment configuration , etc . We are unsure what would be gained from an attempt to formally pin this down , but are happy to hear from you regarding this . Alternatively , would you be satisfied with further examples of what might constitute a \u201c level \u201d , at the point in the paper where the term is introduced ? # # # On surprisingness The reviewer states * * \u201c the improvement in empirical results is not particularly surprising \u201d * * . We respectfully strongly disagree : it is not obvious at all , as only a single score function worked well , and there are intuitive reasons to believe each of the tested score functions should work . For example , we imagine the reviewer would agree with us that using policy entropy is a perfectly valid hypothesis for inducing a curriculum that leads to improved generalization\u2014however our experiments show that the opposite is the case . Moreover , as discussed in Section 5.1 and further shown in Appendix C , the method only provides gains when the score-based distribution is mixed with the staleness-based distribution , while sampling from either of these two distributions separately does not work . There is a fairly intuitive explanation for this , post-hoc , which is that scores drift increasingly \u201c off-policy \u201d as the staleness increases , and the mixture cancels this out . We will include a brief mention of this in our results section , but we maintain that this is a novel and un-intuitive finding . Fortunately , most findings become intuitive and unsurprising once explained ."}, "1": {"review_id": "NfZ6g2OmXEk-1", "review_text": "* * SUMMARY * * The present work considers the problem of learning in procedurally generated environments . This is a class of simulation environments in which each individual environment is created algorithmically where certain environmental factors are varied in each instance ( referred to as levels in this work ) . Learning algorithms in this setting typically use a fixed set of training and evaluation environments . The present work proposes to sample the training environments such that the learning progress of the agent is optimized . This is achieved by proposing an algorithm for level prioritization during training . The performance of the approach is demonstrated on the Procgen Benchmark and two MiniGrid benchmarks and the authors argue that their approach induces an implicit curriculum in sparse reward settings . * * STRENGTHS * * - The general idea of prioritization for level sampling makes a lot of sense and is demonstrated to improve sample-efficiency for skill learning in procedurally generated environments . - I also liked that the authors compared with a big variety of different scoring metrics . * * WEAKNESSES * * - The intuition of `` greater discrepancy between expected and actual returns , making \u000e $ \\delta_t $ a useful measure of the learning potential '' makes sense . The heuristic score also works well in practice . One limitation I see is that there is no theoretical justification for why the TD-error is a good predictor for learnability . - This is maybe more an avenue for future work than an actual weakness but it seems to me that the algorithm is not making use of all potentially useful information . In each timestep , it only considers the last score achieved in a level . Maybe it would also be interesting to consider the full history of scores . My intuition is that levels in which agents were historically very slow to learn are maybe not as useful ( or at least not useful at the moment ) . I.e. , maybe in order to learn competing at such levels it is better to compete on other levels first ? - Is there , at least from a qualitative perspective , an explanation for why certain environments do not benefit as much from the proposed level sampling approach ? * * REPRODUCIBILITY * * The work seems reproducible . Most of the information relevant for reproducibility is given in Appendices A & B . It would be great if the authors would also make the source code available . * * CLARITY * * Overall , I found the work to be very clearly written and have only minor questions/remarks : - To what extent does the use of TD-errors potentially limit the type of learning algorithms that can be used in the context of the proposed framework . Computing the TD-error requires a value function . As I understand it , some RL algorithms never compute a value function . - If I have n't overlooked it , there is no explanation of $ c $ after eq . ( 4 ) while $ C_i $ is explained earlier . Is $ c $ simply the current episode ? * * EVALUATIONS * * The work is compared with several scoring function baselines using PPO . While the authors claim that the method is applicable to other RL agents , the evaluations do not show any results with other agent types . The authors mention several different benchmarks in that space . It would be interesting to know why particularly Procgen Benchmark and MiniGrid environments were chosen . It is also not clearm to me why PPO is used as the base agent . Was this for ease of implementation / its popularity ? Would n't it make sense to use more recent agents to see the added benefit of the proposed approach . E.g. , would V-MPO be applicable here ? * * NOVELTY / RELEVANCE * * The work is very interesting and the authors make a compelling case that procedurally generated environments can benefit from a conscious sampling of the levels with regard to usefulness for learnability . I am not sure whether the claim `` Prioritized Level Replay induces an implicit curriculum , taking the agent gradually from easier to harder levels . '' is fully valid . As I understand it , the hardest levels are also the most likely to be sampled . The force counteracting this to some extent is the staleness-based sampling term $ P_C $ . For a gradual curriculum , I would expect $ P_S $ to be designed such that it does not choose the hardest level but the one promising the best learning outcome . Particularly in the early stages of the training , the hard levels might be less useful than levels of medium difficulty . * * SUMMARY * * I found that paper very interesting . While I am not working in the particular subfield of the work and can not sufficiently judge relation with prior works , I can confidently say that the idea and implementation details were conveyed very well . My main concerns are regarding the understanding of the `` failure cases '' and to what extent the graduality claim applies . That being said , I believe this line of work to be really interesting and to have a lot of potential for improved sample-efficiency when training RL agents in algorithmically generated simulation environments . * * POST-DISCUSSION UPDATE * * I want to thank the authors for correcting my misunderstandings , answering my questions , and providing additional material . As a consequence of this , I have raised my score to `` Accept '' . To answer your question about what would be needed for a higher score : For a strong accept recommendation , I would have expected a mix of several additional things such as a clear impact outside of own subfield , code availability at time of submission ( to evaluate how easy it is to reproduce the results and re-use the code ) , or more additional theoretical justification ( in the sense of new formal guarantees for at least certain aspects of the proposed method ) . While not directly working in this subfield , I still think this work is solid and worthy of publication .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their insightful and detailed comments that will improve our paper . It is great to hear the reviewer found our approach sensible , our writing clear , and that they praised the large variety of scoring functions that are explored . We address , here and in our [ post summarizing the changes to the paper ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , your main concerns , and hope that this will lead to you considering strengthening your recommendation or explaining what still stands in the way , so that we may further improve the paper . # # # Clarification about curriculum \u201c As I understand it , the hardest levels are also the most likely to be sampled \u201d We respectfully believe this is a misconception . At the end of Section 5.1 on page 7 , we state \u201c easier levels result in non-zero , non-stationary returns earlier in training , while harder levels give rise to near stationary returns until the agent learns an improved policy that allows making further progress on the level [ ... ] sampling levels according to the L1 value-loss then leads to an implicit curriculum from easier to harder levels. \u201d What matters for a score of a level to be high based on L1 value-loss is whether or not the agent , given the current policy , over- or under-estimates value , which does not consistently correlate with the relative difficulty of the environment . For instance , in early stages of training the agent might correctly estimate low value for harder levels , thus sampling more frequently easier levels to improve it \u2019 s policy before gradually sampling harder levels more often . This is a hypothesis that we verify qualitatively in Figure 4 . # # # On the limits imposed by the use of TD error In our work , we investigated policy gradient methods , which near-uniformly make use of a value estimate . This does not cover all approaches to RL , but a large class of state-of-the-art actor-critic policy-gradient methods such as : PPO , IMPALA , A2C/A3C , A2C-AKTR , APPO , and Phasic Policy Gradients . We agree it would be interesting to investigate whether the core mechanisms of Prioritized Level Replay can also be combined with value-based RL methods ( e.g.DQN ) for future research . However , this is outside of the scope of the present paper , and is something we are investigating in follow-up work . # # # Theoretical justification of TD error for scoring learning potential : As motivated in the paper , the TD error is the difference between the empirical return and the predicted return . When this discrepancy is high , there is a greater opportunity for the agent to learn . In fact , the same reasoning is used in Schaul _et al._ 2016 ( Prioritized Experience Replay ) to motivate the use of TD errors as a learning signal for ranking the utility of sampling * past * transitions in the experience replay buffer . Further , the advantage-based gradient estimator used in nearly all actor-critic methods , including PPO which is used in our paper , entails computing nearly the same TD-error terms , so our use of TD-error-based scores may be seen as roughly correlating with the value of the gradient estimate resulting from the last trajectory taken over each level ."}, "2": {"review_id": "NfZ6g2OmXEk-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Summary * * : This paper proposes a prioritized sampling strategy for task sampling in procedurally generated environments . While training an RL agent across many tasks ( levels ) , we can either sample a new task uniformly from the training task distribution or sample a new task with different weights . The paper claims that sampling based on the average magnitude of generalized advantage estimate ( GAE ) yields faster learning in most Procgen environments and a few MiniGrid environments . Overall , I found the idea to be simple and intuitive . But the benefit of using prioritized level replay is also not very consistent across different environments used in the paper . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Strengths * * : The method of the paper is simple and can be incorporated into many existing RL algorithms . The paper shows that L1 value loss is a good scoring metric for the prioritization by comparing several different choices . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Weaknesses * * : The advantage of using prioritized level replay against uniform sampling is rather small in many tasks ( 11 out of 19 tasks ) shown in the paper ( Climber , Coinrun , Dodgeball , Fruitbot , Heist , Jumper , Maze , Miner , Ninja , Starpilot , ObstructedMazeGamut-Medium ) . The paper only presents results in the easy mode of procgen . While I understand the reason due to the limit on the computational resources , it would be more convincing to show the results on at least 1 or 2 procgen tasks in the difficult mode . If the overall task difficulty is increased , then the advantage of learning in a curriculum ( starting from the easy tasks and then to the difficult tasks ) are expected to be more salient . While the scoring metrics used in the paper are all related to the policy function or value function that is being learned , how about a scoring metric that is only based on the number of steps that the agent experiences in a task and whether the agent fails or succeeds ? Intuitively , if the lifetime of an agent is short and the agent solves the task , it is an easy task . If the agent does not solve the task or it takes the agent many more steps to solve the task , it is a difficult task . Another metric to compare to is prioritize based on the return value of the trajectories . If the return value is high , then the task is probably already solved by the current policy , so we can sample such tasks less frequently . In Figure 4 , it seems the advantage of using L1 value loss for the prioritization in sampling is more obvious in easy environments ( Multiroom-N4-Random and ObstructedMazeGamut-Easy ) . But its performance becomes very close to the uniform sampling strategy in harder environments ( ObstructedMazeGamut-Medium ) . Why would the advantage of using prioritization ( hence implicit curriculum ) fade as the task difficulty increases ? In Figure 4 , it is hard to connect the top row to the bottom row as the top row uses the environment steps for the x-axis , the bottom row uses the number of PPO updates for the y-axis . I would suggest plot the bottom row figures in terms of the environment steps as well and use the same x-range . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # * * Minor points * * : Some details about the experiment setup , especially the MiniGrid environments , are missing . For example , how do the MiniGrid environments look like , what does the difficulty mean in these environments , which parts of the environments are randomized across levels , reward structure , etc .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their feedback that will improve the paper . # # # On the benefits of Level Replay The reviewer states \u201c benefit of using prioritized level replay is also not very consistent across different environments \u201d / \u201c advantage of using prioritized level replay against uniform sampling is rather small in many tasks \u201d . As stated in our [ joint response to all reviewers ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , we want to strongly emphasize that in the initial submission of the paper we reported * statistically significantly * better generalization ( as determined by Welch \u2019 s t-test ) on the majority ( 11 out of the 16 ) of Procgen envs and are on par with the others . On MiniGrid , we report statistically significant improvements in sample efficiency on all 3 environments , and statistically significant gains in final test performance on ObstructedMazeGamut-Easy and ObstructedMazeGamut-Medium . In the updated version of the paper , we now set a new SOTA on Procgen Benchmark when our method is combined with the previous SOTA-method UCB-DrAC , and by a fair margin . In particular , these additional results show its applicability to other methods than standard PPO , setting a new SOTA for the OpenAI Procgen Benchmark via augmenting UCB-DrAC [ Raileanu _et al._ 2020 ] ( https : //arxiv.org/pdf/2006.12862.pdf ) with Prioritized Level Replay . This result also concretely demonstrates that our method can enable complementary improvements in combination with other powerful methods for improving generalization , which is a strong advantage of our method\u2014it can be implemented in conjunction with most any other method , and as we see , can thereby provide additive gains in sample-efficiency and generalization performance . # # # Evaluating against OpenAI Procgen Hard Using OpenAI Procgen Benchmark \u201c easy \u201d for our experiments is not only based on the high computational demand of training on \u201c hard \u201d , but also due to the precedent in the literature to use \u201c easy \u201d instead of \u201c hard \u201d . For example , see [ Laskin _et al._ ( NeurIPS 2020 ) \u201c Reinforcement Learning with Augmented Data \u201d ] ( https : //arxiv.org/abs/2004.14990 ) and [ Raileanu _et al._ ( 2020 ) \u201c Automatic data augmentation for generalization in deep reinforcement learning. \u201d ] ( https : //arxiv.org/abs/2006.12862 ) Nonetheless , we recognise the value of at least running some experiments on a selection of hard tasks . We have run experiments on Procgen benchmark \u2019 s hard difficulty setting and added them to the paper in Appendix C. We find that Prioritized Level Replay works on these environments as well , specifically resulting in an average gain across games of 39 % over PPO with uniform level sampling . This further strengthens our claims about the robustness and applicability of our method . Thank you for suggesting this , and we hope that these additional results give you the confidence you need to fully support the publication of this paper . # # # Regarding your step-count-based scoring function suggestion Thank you for suggesting this additional baseline . We have discussed this a lot over the last few days . Our concerns are : 1 . That trajectory length is not a signal that meaningfully correlates to difficulty . In some games , e.g.MiniGrid , shorter trajectories mean a better policy ( higher return ) , while on others , e.g.most Procgen Benchmark games , longer trajectories correlate with better policy ( surviving longer and therefore obtaining a higher return ) . This makes step-count-based scoring functions not generally transferable across environments , unlike the TD-error-based scoring functions , which we empirically show work across over a dozen environments . 2.Sampling based on return also does not make sense . If you bias toward sampling for high return , you will likely oversample the easy levels , which the agent will master quickly and reinforce this sampling bias , thereby only rarely having a chance at learning on harder levels . If you bias towards sampling low return levels , you will tend to sample hard levels that the agent can not solve , and the agent will make slower learning progress . As a result , we are unsure exactly what scoring function would make sense for conditioning on trajectory length and episode success or return . Do you have a specific one in mind ? If so , we are happy to attempt to try and run it on some environments and share any results before the end of the discussion period . That said , we will be open-sourcing the code and hope people will feel empowered to try their own scoring metrics and variants on our ideas . As described in the paper , Prioritized Level Replay describes a general framework for a class of selective-sampling algorithms for sampling training levels in an RL setting . The aim of our paper is to present this framework in addition to empirical studies demonstrating the effectiveness of a specific instance of this class of algorithms , value-based level replay ( where the scoring function is the L1 value-loss ) , across a wide variety of environments ."}, "3": {"review_id": "NfZ6g2OmXEk-3", "review_text": "# # # Paper Summary This paper allows agents to set the initial conditions ( level ) for procedurally generated episodes during exploration to past observed values , and proposes to have agents form an intrinsic curriculum by resampling past levels based on a heuristic measure of expected learning progress . The authors test several heuristic measures and find that the average absolute magnitude of the generalized advantage estimate works well . The authors hypothesize that this intrinsic curriculum will improve optimization/learning relative to an agent that always samples initial conditions from the environment distribution . The authors verify that their prioritization strategy usually improves performance in several Progen Benchmark and MiniGrid environments , usually by a small but statistically significant amount , but sometimes by a large amount . # # # Summary Review ( highlights re : quality , clarity , originality and significance ) The paper is well written and clear after one understands the basic idea . The idea is simple , and the algorithm/experiments seem straightforward to reimplement . The experiments are about what one would expect and seem to be well executed . The idea is original but not particularly innovative ( this seems like the first heuristic prioritization approach that would come to mind given that the agent is able to choose the level ) . The improvement in empirical results is not particularly surprising ( if anything , I would have expected more large improvements like the ones on bigfish/leaper environments ) . As this method is constrained to procedurally generated environments ( or at least , evaluation of the method is constrained to procedurally generated environments ) , the significance seems rather limited . The required assumption seems rather strong , as it requires a simulator / control over the environment , which limits applicability . # # # Pros - This a simple idea that can improve performance in Procedurally Generated Environments given that the agent is allowed to set the initial conditions / pick the level . - The performance improvement in 4 of the 19 environments tested is large & seems absolute ( i.e. , it 's seems like a final performance improvement , not just a sample efficiency improvement ) . - The paper is well written/presented , easy to understand , and the empirical evaluation seems well done . The results do not seem difficult to replicate . # # # Cons - Despite being less intrusive than direct access to the level generation mechanism , the assumption that the agent can replay levels seems rather strong to me , and simplifies the task of learning procedurally generated environments very substantially . ( $ \\dagger $ ) I would argue that we don \u2019 t use procedurally generated environments as benchmarks in order to solve procedurally generated environments , but rather a tool for measuring generalization , so it 's unclear to me that a technique that improves sample efficiency only in a procedurally generated environment is useful . Unlike environment-agnostic techniques like prioritized replay , HER , intrinsic reward , intrinsic goal selection , etc. , this requires you to have control over the environment , which seems to limit the applicability . If this is only useful with a simulator , then the small gains in sample efficiency aren \u2019 t actually that relevant , though this approach does seem to improve final performance in 4 of the 19 environments tested . - It \u2019 s not clear until the second page whether your method is a prioritized replay buffer scheme , or a task selection scheme . Actually , I was certain it was a prioritized replay buffer scheme until the second page , because that is the more natural/general setting ( as noted above , I find the assumption that the agent can replay levels to be rather strong ) . - Several new hyperparameters are introduced ; this said , guidance/ablations are performed , and it seems like the choices will generalize decently well ( albeit there were different choices for ProcGen/Minigrid ) . # # # Questions / Etc . - My main question for the authors is to ask for a counterargument to ( $ \\dagger $ ) above . - It would be good if this can be shown to work in multi-goal setting , as it is quite similar to ProcGen setting ... you draw some distinctions , but I do think your approach would be applicable there .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their feedback that we will use to improve our paper . # # # On the applicability of this method The reviewer states : * * \u201c evaluation of the method is constrained to procedurally generated environments \u201d / \u201c unclear to me that a technique that improves sample efficiency only in a procedurally generated environment is useful \u201d * * . As we explain in our [ joint response to all reviewers ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , using procedurally generated environments for evaluation is not a weakness but instead a main strength of our approach compared to work that relies on specific properties of an underlying environment instance , e.g.Atari games in the Arcade Learning Environment . Procedurally generated environments are much harder to master due their constant stream of novel observations that the agent has to generalize towards . Many previously successful methods ( e.g.Go-Explore , count-based exploration , etc . to name just a few ) would fail as they assume the environment stays fixed and the agent can memorize trajectories to high value regions in the state space . The reviewer further states : * * \u201c it requires a simulator / control over the environment , which limits applicability \u201d : * * Note that assuming one can replay a level ( or , more concretely , any configuration of the environment ) is a much weaker assumption than assuming there is only a * single * configuration the agent needs to do well in ( as it is the case in Atari ) . In contrast to Atari , we test for systematic generalization by sampling completely unseen seeds ( and thus environment instances ) at test time . Additionally , note that any environment that displays random initialized states can be viewed as procedurally-generated . Take for example , a robot reaching task that starts with a random arrangement of objects . In this case , \u201c resetting the seed \u201d during training would equate to simply returning the objects to the corresponding initial arrangement , and a completely manageable task for real-world training , which requires resetting the initial arrangement of objects at the start of each episode anyway . If we are instead training in simulation with access to the simulator ( which is more often than not in RL ) , then why not take advantage of it ? As the goal of training on procedurally-generated environments is to test for and improve generalization , if a training strategy uses a privileged action\u2014such as resetting the env seed at the start of training episodes\u2014does not subvert the integrity of the test time evaluation protocol of the agent , then we should by all means take advantage of such a strategy . # # # Regarding sample efficiency and performance The reviewer also states * * \u201c small gains in sample efficiency aren \u2019 t actually that relevant , though this approach does seem to improve final performance in 4 of the 19 environments tested \u201d * * : As stated in our [ joint response to all reviewers ] ( https : //openreview.net/forum ? id=NfZ6g2OmXEk & noteId=f5jXzTsAGK8 ) , we strongly emphasize that in the initial submission of the paper we reported * statistically significantly * better generalization ( as determined by Welch \u2019 s t-test over 10 runs ) on not 4 but * * 11 * * out of the 16 Procgen Benchmark envs , 3 out of 3 MiniGrid environments tested ( in terms of sample efficiency and/or final test returns ) , and are on par for the others . In the updated version of the paper , we now report a new SOTA on Procgen benchmark when our method is used in combination with a UCB-DrAC agent , reporting statistically significant gains on 14 of 16 games , with much higher gains on average per game . # # # On defining the notion of replay Thank you for bringing to our attention that the notion of replay ( as in gathering * new * experience from a level ) used in our paper , in contrast to the notion of replay used in experience replay , is not clear until the second page . We agree , and we have changed the writing of the abstract and introduction to reflect this . We trust you will find the paper is improved as a result , but please let us know if it is somehow still in need of additional clarity . # # # Summary Thank you for your pertinent questions and comments . We hope the responses , and the improvements we have made to the paper in response to your feedback , have convinced you that the paper is worthy of your support . We strongly believe it proves the concept and is rigorously evaluated and compared , including ( as of this revision ) against the state of the art on the OpenAI Procgen Benchmark ( which it improves upon ) . Naturally , there are further experiments to be done , including investigating application of this method to looser notions of \u201c level \u201d outside of procgen ( e.g.starting states or , as you suggest , multi-goal settings ) , but these constitute ambitious and exciting matter for future work we hope to investigate . In the meantime , we would be grateful for your support for this paper , and are happy to further discuss outstanding concerns you may have , if any ."}}