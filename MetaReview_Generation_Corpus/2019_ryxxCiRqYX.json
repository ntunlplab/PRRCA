{"year": "2019", "forum": "ryxxCiRqYX", "title": "Deep Layers as Stochastic Solvers", "decision": "Accept (Poster)", "meta_review": "This paper relates deep learning to convex optimization by showing that the forward pass though a dropout layer, linear layer (either convolutional or fully connected), and a nonlinear activation function is equivalent to taking one \u03c4-nice proximal gradient descent step on a a convex optimization objective. The paper shows (1) how different activation functions correspond to different proximal operators, (2) that replacing Bernoulli dropout with additive dropout corresponds to replacing the \u03c4-nice proximal gradient descent method with a variance-reduced proximal method, and (3) how to compute the Lipschitz constant required to set the optimal step size in the proximal step. The practical value of this perspective is illustrated in experiments that replace various layers in ConvNet architectures with proximal solvers, leading to performance improvements on CIFAR-10 and CIFAR-100. The reviewers felt that most of their concerns were adequately addressed in the discussion and revision, and that the paper should be accepted.", "reviews": [{"review_id": "ryxxCiRqYX-0", "review_text": "This paper theoretically verifies an equivalence between stochastic solvers on a particular class of convex optimization problems and a forward pass through a dropout layer followed by a linear layer and a non-linear activation. Experiments show that replacing a block of layers with multiple iterations of the corresponding solver improves classification accuracy. My detailed comments are as follows. *Positive points: 1. The perspective is novel and interesting, i.e., training a forward pass through a dropout layer followed by a linear layer and a non-linear activation is equivalent to optimizing a convex problem by a Proximal Stochastic Gradient method. More importantly, this perspective has been theoretically verified. 2. In the experiments, training networks with solvers replacing deep layers is able to improve accuracy significantly. *Negative points: 1. Some technical details are not clear and many notations are used without clear explanations. Specifically, many notations based on (Bibi & Ghanem, 2017) make the paper hard to follow. Moreover, there are many mistakes in proofs. Please revise the paper according to the following comments. 2. There are many limitations for the proposed method. Specifically, the theoretical results are hard to be extended to more general neural networks (e.g., ResNet) with Batch Normalization which are widely used. 3. The experiment section should be significantly improved. There are only two datasets (i.e., CIFAR-10, CIFAR-100). It would be convincing that more baselines are compared on other datasets, such as ImageNet. *Detailed comments: **Comments on technical issues. 1. In Problem (1), the definition of $g(x)$ and $f\u00ac_i()$ should be provided for clarity. 2. The motivation and some details of Function (2) should be provided since $F(x^l)$ is important for proving the equivalence between stochastic solvers and a forward network. In addition, $x$ should be corrected as $x^l$. 3. Is Equation (3) wrong? Based on the definition of Prox-GD in (Xiao & Zhang, 2014), it should be $x^l=Prox(x^{l-1} \u2013 1/L \\nabla F(x^l)) = Prox((I-1/L A)x^{l-1} + 1/L (AA^T x^l + b))$ which is different from Equation (3). Moreover, the Lipschitz constant w.r.t. maximal eigenvalue should be proved. 4. In Definitions D.1 and D.2, what is the definition of $fold_{H0}$? Is the dimensionality of $bdiag(D)$ wrong? Why is $bdiag(D)$ an identity mapping when $n_3=n_4$? 5. There are some issues on Equation (7) and its proofs. Is $A(:, i, :, :)$ and $\\vec{X}(i, :, :, :) $ wrong? It affects the results of Equation (8). Does Equation (25) miss the operator $fold_{HO}$ in Appendix G? Please check the proofs of Proposition 1. 6. There are some issues on proofs of Lemma 2. Why are $F_H \\otimes F_W \\otimes I_{n_1}$ and $F_H \\otimes F_W \\otimes I_{n_2}$ orthogonal? Is the third and fourth equality in (24) wrong? For the fourth equality in (24), Eigen decomposition seems to be for a matrix, not a tensor. **Comments on Experiments 1. Training Networks is equivalent to optimizing proximal solvers. Why can training networks with solvers replacing blocks of layers improve accuracy? Reasonable explanations should be provided. 2. Optimizing a convex optimization problem can easily obtain the optimal solution. What happens if solvers are used to replace more blocks of layers? Complexity analysis for these should be provided. 3. The experiments are only conducted on two datasets (i.e., CIFAR-10, CIFAR-100). It would be better to compared more baselines on other datasets, such as ImageNet. ", "rating": "7: Good paper, accept", "reply_text": "* We recommend reading this response in the revised PDF uploaded to OpenReview . This response is in Appendix L. The mathematical notation is easier to read in the PDF . We thank R2 for the comments and the positive feedback on the novelty of our approach . R2 raised several issues regarding the proofs . We have proofread all the proofs and there are no factual errors . In fact , some of the main results , e.g.Lemma 2 , have also been verified numerically . However , there were some minor typos and non-standard notation that may have been confusing . We have corrected these typos in the revised version uploaded to OpenReview . Below is a detailed answer to all of R2 's concerns . General Comments . ( 1 ) Unclear technical details and mistakes in the proofs . There are no mistakes in the proofs . We have thoroughly checked them and they are all correct . There were some typos that may have been behind R2 's confusion . We have corrected these typos and improved the notation . ( 2 ) Limitations of the current method . What about ResNets and BatchNorm ? Our framework can be directly applied to ResNets . A ResNet block can be viewed as two consecutive stochastic solvers . We have added Appendix J discussing this in the supplementary material . This approach is somewhat simplistic ; there is room for exciting future work . Normalization layers are easy to handle . Note that during test time normalization layers are linear ; thus they can be combined with the fully-connected or convolutional layer as a single linear layer ."}, {"review_id": "ryxxCiRqYX-1", "review_text": "Overview: This paper shows that the forward pass of a fully-connected layer (generalized to convolutions) followed by a nonlinearity in a neural network is equivalent to an iteration of a prox algorithm, where different regularizers in the objective of the related prox problem correspond to different nonlinearities such as ReLu. This connection is quite interesting. They further relate different stochastic prox algorithms to different dropout layers and show results of improved performance on CIFAR-10 and CIFAR-100 on several architectures. The paper is well-written. Major Concerns: 1. While the equivalence of one iteration of a prox algorithm and a single forward pass of the block is understandable, it is not clear what happens from making several iterations (10 in the case of fully-connected layers in the experiments) of the prox algorithm. It seems that this would be equivalent to making a forward pass through 10 equivalent blocks (i.e., 10 layers with the same weights and biases). But then the backward pass is still through the original network, so the problem being solved is not clear. Clarity on this would help. 2. Since the equivalence of 10 forward passes of a block are done at each iteration, using solvers does more computations (can be thought of as extra forward passes through extra layers as noted above), which makes the comparison not completely fair. Either adding more batches or more passes over the same batch multiple times (or at least for a few batches just to use the some computational power) would be more fair and likely improve the performance of the baseline networks. Minor Issues: 1. missing definitions such as g(x) at beginning of Section 3 and p in Proposition 1. 2. Give examples of where the prox problems in Table 1 show up in practice (outside of activation functions in neural networks) 3. It says \"for different choices of dropout rate the baseline can always be improved by...\" in the Experiments. This is not provable. 4. Include results for Dropout rate p=0 in Table 5.", "rating": "7: Good paper, accept", "reply_text": "* We recommend reading this response in the revised PDF uploaded to OpenReview . This response is in Appendix M. The mathematical notation is easier to read in the PDF . We thank R3 for the positive review and feedback . Below are our responses to all concerns . On the major concerns . ( 1 ) Some clarity on the forward/backward pass of networks with Prox solvers . R3 's description of the forward pass through the network with a Prox solver is correct . In general , the best way to understand how a network with a Prox solver operates in both forward and backward passes is to think of that layer with a Prox solver as a recurrent neural network . Thus , asking how one performs a backward pass through such a layer is equivalent to asking how one would perform a backward pass through a recurrent neural network . The backward pass through the Prox solver is still performed : not through the original network as R3 thought , but through the same network with the Prox solver , akin to backpropagation-through-time ( BPTT ) . This resembles the parameter update procedure in recurrent neural networks . ( 2 ) Adjusting baselines to perform the same amount of computation for a fair comparison . R3 is correct about the fact that networks with Prox solvers do in fact perform more computation . However , the capacity of both networks ( baseline and the Prox solver network ) is identical . This means that both networks have the same exact number of parameters and there is no advantage of the Prox solvers in terms of capacity over the baseline . This is the essential factor for a fair comparison , since accuracy is often considered as a function of network capacity rather than the amount of computation . Moreover , note that to the best of our knowledge we have reported the best results for the baselines in comparison to any publicly available online repository that performs training without using any test data statistics ( i.e.proper training ) . For instance , the results of VGG16 on CIFAR-10 are comparable to or better than some ResNet architectures on the same dataset . We are not aware of better numbers for the corresponding networks on the corresponding datasets . On the minor issues . ( 1 ) Missing definitions . We have addressed this in the revised version . We provided several examples of $ g ( \\mathbf { x } ) $ when it was first introduced in the first page . We have explicitly , as suggested , defined $ p $ in Proposition 1 . ( 2 ) Give examples of where the prox problems in Table 1 show up in practice ( outside of activation functions in neural networks ) . We are only aware of applications in activation functions in neural networks . But this is sufficient motivation for us . ( 3 ) On the statement `` for different choices of dropout rate the baseline can always be improved by ... '' in the Experiments . We have softened the statement in the revision . The statement is now `` we observe that for different choices of dropout rate the baseline performance improves upon replacing ... '' . ( 4 ) Include results for Dropout rate p=0 in Table 5 . We have added this experiment to Table 5 ."}, {"review_id": "ryxxCiRqYX-2", "review_text": "This paper presents a very interesting interpretation of the neural network architecture. I think what is remarkable is that the author presents the general results (beyond the dense layer) including a convolutional layer by using the higher-order tensor operation. Also, this research gives us new insight into the network architecture, and have the potential which leads to many interesting future directions. So I think this work has significant value for the community. The paper is clearly written and easy to follow in the meaning that the statement is clear and enough validation is shown. (I found some part of the proof are hard to follow.) \\questions In the experiment when you mention about \"embed solvers as a replacement to their corresponding blocks of layers\", I wonder how they are implemented. About the feedforward propagation, I guess that for example, the prox operator is applied multiple times to the input, but I cannot consider what happens about the backpropagation of the loss. In the experiment, the author mentioned that \"what happens if the algorithm is applied for multiple iterations?\". From this, I guess the author iterate the corresponding algorithms several times, but actually how many times were the iterations or are there any criterion to stop the algorithm? \\minor comments The definition of \\lambda_max below Eq(3) are not shown, thus should be added.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank R1 for the positive comments and review . ( 1 ) Definition of $ \\lambda_ { \\max } $ in Eq ( 3 ) . We have adjusted the text below Eq ( 3 ) to clearly state that $ \\lambda_ { \\text { max } } ( . ) $ is the maximum eigenvalue function . ( 2 ) On the implementation of solvers . The block of layers ( dropout followed by linear and nonlinear layers ) is replaced with an iterative solver , i.e.recurrent layer , that performs the Prox operator several times before generating an output . This is for the forward pass through the network . As for backpropagation , it is performed by simply unrolling the layers . This is commonly referred to as backpropagation through time in recurrent neural networks . While this is not particularly efficient in general , there are several potential ways to improve this by taking gradients implicitly through the argmin operator [ 1,2 ] . We leave this to future work . ( 3 ) On the number of iterations . The number of iterations was always kept constant . It was set to 10 as stated at the end of page 6 for all small networks . As for larger networks such as VGG16 , this number is fixed to 30 iterations as discussed in page 7 ( just below Table 3 ) . At present we do not have a universal criterion for choosing the number of iterations ; this is treated as a hyperparameter . [ 1 ] `` Techniques for Gradient-Based Bilevel Optimization with Non-smooth Lower Level Problems '' , Peter Ochs , Ren\u00e9 Ranftl , Thomas Brox , Thomas Pock . [ 2 ] `` On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-level Optimization '' , Stephen Gould , Basura Fernando , Anoop Cherian , Peter Anderson , Rodrigo Santa Cruz , Edison Guo ."}], "0": {"review_id": "ryxxCiRqYX-0", "review_text": "This paper theoretically verifies an equivalence between stochastic solvers on a particular class of convex optimization problems and a forward pass through a dropout layer followed by a linear layer and a non-linear activation. Experiments show that replacing a block of layers with multiple iterations of the corresponding solver improves classification accuracy. My detailed comments are as follows. *Positive points: 1. The perspective is novel and interesting, i.e., training a forward pass through a dropout layer followed by a linear layer and a non-linear activation is equivalent to optimizing a convex problem by a Proximal Stochastic Gradient method. More importantly, this perspective has been theoretically verified. 2. In the experiments, training networks with solvers replacing deep layers is able to improve accuracy significantly. *Negative points: 1. Some technical details are not clear and many notations are used without clear explanations. Specifically, many notations based on (Bibi & Ghanem, 2017) make the paper hard to follow. Moreover, there are many mistakes in proofs. Please revise the paper according to the following comments. 2. There are many limitations for the proposed method. Specifically, the theoretical results are hard to be extended to more general neural networks (e.g., ResNet) with Batch Normalization which are widely used. 3. The experiment section should be significantly improved. There are only two datasets (i.e., CIFAR-10, CIFAR-100). It would be convincing that more baselines are compared on other datasets, such as ImageNet. *Detailed comments: **Comments on technical issues. 1. In Problem (1), the definition of $g(x)$ and $f\u00ac_i()$ should be provided for clarity. 2. The motivation and some details of Function (2) should be provided since $F(x^l)$ is important for proving the equivalence between stochastic solvers and a forward network. In addition, $x$ should be corrected as $x^l$. 3. Is Equation (3) wrong? Based on the definition of Prox-GD in (Xiao & Zhang, 2014), it should be $x^l=Prox(x^{l-1} \u2013 1/L \\nabla F(x^l)) = Prox((I-1/L A)x^{l-1} + 1/L (AA^T x^l + b))$ which is different from Equation (3). Moreover, the Lipschitz constant w.r.t. maximal eigenvalue should be proved. 4. In Definitions D.1 and D.2, what is the definition of $fold_{H0}$? Is the dimensionality of $bdiag(D)$ wrong? Why is $bdiag(D)$ an identity mapping when $n_3=n_4$? 5. There are some issues on Equation (7) and its proofs. Is $A(:, i, :, :)$ and $\\vec{X}(i, :, :, :) $ wrong? It affects the results of Equation (8). Does Equation (25) miss the operator $fold_{HO}$ in Appendix G? Please check the proofs of Proposition 1. 6. There are some issues on proofs of Lemma 2. Why are $F_H \\otimes F_W \\otimes I_{n_1}$ and $F_H \\otimes F_W \\otimes I_{n_2}$ orthogonal? Is the third and fourth equality in (24) wrong? For the fourth equality in (24), Eigen decomposition seems to be for a matrix, not a tensor. **Comments on Experiments 1. Training Networks is equivalent to optimizing proximal solvers. Why can training networks with solvers replacing blocks of layers improve accuracy? Reasonable explanations should be provided. 2. Optimizing a convex optimization problem can easily obtain the optimal solution. What happens if solvers are used to replace more blocks of layers? Complexity analysis for these should be provided. 3. The experiments are only conducted on two datasets (i.e., CIFAR-10, CIFAR-100). It would be better to compared more baselines on other datasets, such as ImageNet. ", "rating": "7: Good paper, accept", "reply_text": "* We recommend reading this response in the revised PDF uploaded to OpenReview . This response is in Appendix L. The mathematical notation is easier to read in the PDF . We thank R2 for the comments and the positive feedback on the novelty of our approach . R2 raised several issues regarding the proofs . We have proofread all the proofs and there are no factual errors . In fact , some of the main results , e.g.Lemma 2 , have also been verified numerically . However , there were some minor typos and non-standard notation that may have been confusing . We have corrected these typos in the revised version uploaded to OpenReview . Below is a detailed answer to all of R2 's concerns . General Comments . ( 1 ) Unclear technical details and mistakes in the proofs . There are no mistakes in the proofs . We have thoroughly checked them and they are all correct . There were some typos that may have been behind R2 's confusion . We have corrected these typos and improved the notation . ( 2 ) Limitations of the current method . What about ResNets and BatchNorm ? Our framework can be directly applied to ResNets . A ResNet block can be viewed as two consecutive stochastic solvers . We have added Appendix J discussing this in the supplementary material . This approach is somewhat simplistic ; there is room for exciting future work . Normalization layers are easy to handle . Note that during test time normalization layers are linear ; thus they can be combined with the fully-connected or convolutional layer as a single linear layer ."}, "1": {"review_id": "ryxxCiRqYX-1", "review_text": "Overview: This paper shows that the forward pass of a fully-connected layer (generalized to convolutions) followed by a nonlinearity in a neural network is equivalent to an iteration of a prox algorithm, where different regularizers in the objective of the related prox problem correspond to different nonlinearities such as ReLu. This connection is quite interesting. They further relate different stochastic prox algorithms to different dropout layers and show results of improved performance on CIFAR-10 and CIFAR-100 on several architectures. The paper is well-written. Major Concerns: 1. While the equivalence of one iteration of a prox algorithm and a single forward pass of the block is understandable, it is not clear what happens from making several iterations (10 in the case of fully-connected layers in the experiments) of the prox algorithm. It seems that this would be equivalent to making a forward pass through 10 equivalent blocks (i.e., 10 layers with the same weights and biases). But then the backward pass is still through the original network, so the problem being solved is not clear. Clarity on this would help. 2. Since the equivalence of 10 forward passes of a block are done at each iteration, using solvers does more computations (can be thought of as extra forward passes through extra layers as noted above), which makes the comparison not completely fair. Either adding more batches or more passes over the same batch multiple times (or at least for a few batches just to use the some computational power) would be more fair and likely improve the performance of the baseline networks. Minor Issues: 1. missing definitions such as g(x) at beginning of Section 3 and p in Proposition 1. 2. Give examples of where the prox problems in Table 1 show up in practice (outside of activation functions in neural networks) 3. It says \"for different choices of dropout rate the baseline can always be improved by...\" in the Experiments. This is not provable. 4. Include results for Dropout rate p=0 in Table 5.", "rating": "7: Good paper, accept", "reply_text": "* We recommend reading this response in the revised PDF uploaded to OpenReview . This response is in Appendix M. The mathematical notation is easier to read in the PDF . We thank R3 for the positive review and feedback . Below are our responses to all concerns . On the major concerns . ( 1 ) Some clarity on the forward/backward pass of networks with Prox solvers . R3 's description of the forward pass through the network with a Prox solver is correct . In general , the best way to understand how a network with a Prox solver operates in both forward and backward passes is to think of that layer with a Prox solver as a recurrent neural network . Thus , asking how one performs a backward pass through such a layer is equivalent to asking how one would perform a backward pass through a recurrent neural network . The backward pass through the Prox solver is still performed : not through the original network as R3 thought , but through the same network with the Prox solver , akin to backpropagation-through-time ( BPTT ) . This resembles the parameter update procedure in recurrent neural networks . ( 2 ) Adjusting baselines to perform the same amount of computation for a fair comparison . R3 is correct about the fact that networks with Prox solvers do in fact perform more computation . However , the capacity of both networks ( baseline and the Prox solver network ) is identical . This means that both networks have the same exact number of parameters and there is no advantage of the Prox solvers in terms of capacity over the baseline . This is the essential factor for a fair comparison , since accuracy is often considered as a function of network capacity rather than the amount of computation . Moreover , note that to the best of our knowledge we have reported the best results for the baselines in comparison to any publicly available online repository that performs training without using any test data statistics ( i.e.proper training ) . For instance , the results of VGG16 on CIFAR-10 are comparable to or better than some ResNet architectures on the same dataset . We are not aware of better numbers for the corresponding networks on the corresponding datasets . On the minor issues . ( 1 ) Missing definitions . We have addressed this in the revised version . We provided several examples of $ g ( \\mathbf { x } ) $ when it was first introduced in the first page . We have explicitly , as suggested , defined $ p $ in Proposition 1 . ( 2 ) Give examples of where the prox problems in Table 1 show up in practice ( outside of activation functions in neural networks ) . We are only aware of applications in activation functions in neural networks . But this is sufficient motivation for us . ( 3 ) On the statement `` for different choices of dropout rate the baseline can always be improved by ... '' in the Experiments . We have softened the statement in the revision . The statement is now `` we observe that for different choices of dropout rate the baseline performance improves upon replacing ... '' . ( 4 ) Include results for Dropout rate p=0 in Table 5 . We have added this experiment to Table 5 ."}, "2": {"review_id": "ryxxCiRqYX-2", "review_text": "This paper presents a very interesting interpretation of the neural network architecture. I think what is remarkable is that the author presents the general results (beyond the dense layer) including a convolutional layer by using the higher-order tensor operation. Also, this research gives us new insight into the network architecture, and have the potential which leads to many interesting future directions. So I think this work has significant value for the community. The paper is clearly written and easy to follow in the meaning that the statement is clear and enough validation is shown. (I found some part of the proof are hard to follow.) \\questions In the experiment when you mention about \"embed solvers as a replacement to their corresponding blocks of layers\", I wonder how they are implemented. About the feedforward propagation, I guess that for example, the prox operator is applied multiple times to the input, but I cannot consider what happens about the backpropagation of the loss. In the experiment, the author mentioned that \"what happens if the algorithm is applied for multiple iterations?\". From this, I guess the author iterate the corresponding algorithms several times, but actually how many times were the iterations or are there any criterion to stop the algorithm? \\minor comments The definition of \\lambda_max below Eq(3) are not shown, thus should be added.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank R1 for the positive comments and review . ( 1 ) Definition of $ \\lambda_ { \\max } $ in Eq ( 3 ) . We have adjusted the text below Eq ( 3 ) to clearly state that $ \\lambda_ { \\text { max } } ( . ) $ is the maximum eigenvalue function . ( 2 ) On the implementation of solvers . The block of layers ( dropout followed by linear and nonlinear layers ) is replaced with an iterative solver , i.e.recurrent layer , that performs the Prox operator several times before generating an output . This is for the forward pass through the network . As for backpropagation , it is performed by simply unrolling the layers . This is commonly referred to as backpropagation through time in recurrent neural networks . While this is not particularly efficient in general , there are several potential ways to improve this by taking gradients implicitly through the argmin operator [ 1,2 ] . We leave this to future work . ( 3 ) On the number of iterations . The number of iterations was always kept constant . It was set to 10 as stated at the end of page 6 for all small networks . As for larger networks such as VGG16 , this number is fixed to 30 iterations as discussed in page 7 ( just below Table 3 ) . At present we do not have a universal criterion for choosing the number of iterations ; this is treated as a hyperparameter . [ 1 ] `` Techniques for Gradient-Based Bilevel Optimization with Non-smooth Lower Level Problems '' , Peter Ochs , Ren\u00e9 Ranftl , Thomas Brox , Thomas Pock . [ 2 ] `` On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-level Optimization '' , Stephen Gould , Basura Fernando , Anoop Cherian , Peter Anderson , Rodrigo Santa Cruz , Edison Guo ."}}