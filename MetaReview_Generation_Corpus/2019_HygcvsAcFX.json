{"year": "2019", "forum": "HygcvsAcFX", "title": "Optimal margin Distribution Network", "decision": "Reject", "meta_review": "The paper proposed an optimal margin distribution loss and applied PAC-Bayesian bounds that are from Sanov large deviation inequalities to give generalization error bounds for such a loss. Some interesting empirical results are shown to support the proposed method. \n\nThe majority of reviewers think the paper\u2019s empirical results are encouraging, although still in premature stage. The theoretical analysis is a kind of being standard. After reading the authors\u2019 response and revision, the reviewers do not change much of their opinions and think the paper better undergoes systematic further study on their proposal for big improvement.  \n\nBased on current ratings, the paper is therefore proposed to borderline lean rejection. \n", "reviews": [{"review_id": "HygcvsAcFX-0", "review_text": "The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018]. More precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical bounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and [Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some conditions) on the change of the layers (layer and interlayer cushion) as well as the activation contraction. It is also worth noting that the paper is using a differrent loss function comparing to [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss. Although the results seem interesting, the analysis is not convincible for me. A plus point is that the paper presents interesting numerical experiments showing the promising of the approach. Major comments: 1) The statement of the Theorem 1 is not clear: is it just under the assumptions of the lemmas or is it under all definitions and lemmas? 2) The proof of Theorem 1 is not clear: how do you get the inequality (5)? how do you get an upper bound on the KL divergence? This is not trivial for me! 3) What is \\rho in Theorem 1 and in Definition 2? 4) Your remark after Theorem 1 is not clear for me. you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1? a simple counter example would fit better the explanation here, I guest. Minor comments: 1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018] without precisely citations. I wonder how do you obtain your Lemma 1? 2) page3, after formula (1), your loss will first DECREASING, not \"increasing\". Check the sentence \"Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....\" ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and we will make the response issues clearly in revised version . # # The contribution in our paper # # Our paper theoretically proves that margin distribution plays an important role in the generalization of deep learning . Specifically , we propose a well-designed loss function inspired by [ Gao , AIJ 2013 ; Zhang , ICML 2017 ] , and it can effectively alleviate the overfitting of deep models . To the best of our knowledge , this is the first work that introduces margin distribution into the analysis of deep learning . We notice that some recent works [ Barlett , NIPS 2017 ; Neyshabur , ICLR 2018 ] have considered this problem , but they only focus on minimum margin , which is significantly different from our paper . The PAC-Bayesian framework is a convenient technique for margin theory analysis . In fact , other techniques like Rademacher complexity [ Koltchinskii , TIT 2001 ; Koltchinskii , ANN STAT 2002 ; Bartlett , MACH LEARN 2002 ; Barlett , NIPS 2017 ] could also be used to derive similar results . In our paper the PAC-Bayesian technique is an analysis approach rather than our main contribution . As emphasized before , the theoretical contribution is relating the generalization gap to margin distribution . # # Major comments # # Q1 . \u201c The statement of the Theorem 1 is not clear : is it just \u2026 definitions and lemmas ? \u201d : A. Theorem 1 is under all the lemmas and definitions in our paper . Q2. \u201c The proof of Theorem 1 is not clear : how do you get the inequality ( 5 ) ? how do you get an upper bound on the KL divergence ? \u201d : A . The inequality ( 5 ) is an extension of matrix version Hoeffding \u2019 s inequalities [ Tropp , FOCS 2012 ; Mackey , ANN STAT 2014 ] . Note that the KL divergence is between two normal distributions with different mean but the same variance , so it can be bounded by |\\vw|^2 / 2 \\sigma^2 . Q3. \u201c What is \\rho in Theorem 1 and in Definition 2 ? \u201d : A . As presented at the beginning of the Section 3 : \u201c and $ \\rho $ be an upper bound on the number of output units in each layer \u201d . Q4. \u201c you claim that the product is ( 3 ) is large , what if we restrict all the spectral norms equal to 1 ? \u201d : A . The impact of norms in deep learning is not similar to the linear models , we can \u2019 t directly normalize it to the value $ 1 $ . And the most common method to control the norm of weights is called weight decay , which tries to control the weights by a preset decay with a parameter . Because of its data-independence , the performance is not satisfied , and many more efficient methods to preventing the overfitting problem have been proposed , such as dropout and batch normalization . If we try to optimize the product of spectral norms directly as a regularization , we can find that the weights update of each layer is related to the product of spectral norms of other layers , the calculation cost for spectral norm is too large , and the weight decay does not consider this correlation . # # Minor comments Q1 . \u201c The Lemma 1 and 2 are almost the same to Lemma 1 and 2 \u2026 you obtain your Lemma 1 ? \u201d : A. Lemma 1 is obtained in a similar way with PAC-Bayesian work [ Neyshabur , ICLR 2018 ] and we have cited this paper in Sec.3 : \u201c ... we have to relate this PAC-Bayesian bound to the expected perturbed loss just like [ Neyshabur , ICLR 2018 ] derive the Lemma 1 in their paper. \u201d But for Lemma 2 , we make a lot of nontrivial modification due to the introduction of margin variance . Q2. \u201c page3 , after formula ( 1 ) , your loss will first DECREASING , not `` increasing '' . \u201d : A . We have refined the misleading description . The whole paper has been carefully improved . Thank you very much for your help !"}, {"review_id": "HygcvsAcFX-1", "review_text": "I consider that improving the generalization capability of neural networks on small dataset is an important line of research, and the method proposed here empirically provides great results. The proposed margin loss (Equation 1) is said to be \"specially adapted for accelerating the convergence velocity of networks by [the authors]\". I would like this statement to be explained better, or at least backed by empirical evidence. In the current state, I consider that the paper lacks an in-depth study of the properties of this handcrafted loss. Few is said on the benefits of having both a linear behavior for points inside the margin and a quadratic loss for far points. The impact of loss hyperparameters (r, \\gamma,\\mu) should be discussed thoughtfully; at some points in the paper, r and \\gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained. Moreover, almost nothing is said about \\mu. By considering a simplified loss function, the provided PAC-Bayes generalization bound (Theorem 1) consider solely the flat loss region [r-\\gamma, r+\\gamma], but shed no light on the benefit of the hinge and quadratic parts. I conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these. The empirical experiments show great evidence that the proposed method successfully improve generalization capability of neural networks on small datasets compared to classical methods. I appreciate the Inter/intra class variance study of Tables 2 and 3. I would like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written (it is not clear in the text if the soft hinge uses an hyperparameter). In the same spirit of my above comments, I would like to see how each loss hyperparameters impacts the results, instead of having access solely to the parameter values selected by the validation process. Typos and minor comments: - Abstract: \"And our ODN model also outperforms the other three loss models...\" Which three loss models? - Section 3: \"Specially, define L_0 as r=\\theta...\" I think it should be r=0 - Section 4.1: model-s => models - Page 7 (and elsewhere): Table. 2 => Table 2 - Please specify that \"Xent\" stands for cross-entropy - Figure 3: Please use larger font sizes - Proof of Lemma 2: Equation. 4 => Equation 4 ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and we will make the response issues clearly in revised version . Q1. \u201c lacks an in-depth study of the properties of this handcrafted loss \u201d and \u201c The impact of loss hyper-parameters ( r , \\gamma , \\mu ) should be discussed thoughtfully \u201d : A . We have added an extra appendix A to clearly explain the intuition behind this loss function . Q2. \u201c the provided PAC-Bayes generalization \u2026 shed no light on the benefit of the hinge and quadratic parts \u201d : A . Our paper proves that the generalization of deep learning heavily depends on the margin distribution . To optimize the margin distribution , we designed this ODN loss function . As a result , this well-designed loss function alleviates the overfitting problem of deep models efficiently . And we introduce how the hinge and quadratic part is derived from the intuition of margin distribution in appendix A. Q3 . \u201c would like the mathematical expression of the `` hinge loss '' and the `` soft hinge loss '' models to be explicitly written \u201d : A . We have explicitly presented the `` ( soft ) hinge loss '' in section 4.1 . We have also fixed the typos and the whole paper has been carefully improved . Thank you very much for your help !"}, {"review_id": "HygcvsAcFX-2", "review_text": "This paper presents a PAC-Bayesian bound for a margin loss. Theorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1? There are some typos in this paper. \u201cTo derive a expected risk bound\u201d: a -> an \u201cused to formalize error-resilience in Arora et al. (2018) as following:\u201d: following: -> follows. \u201cthe deep network from layer i to layer j\u201d, \u201cinjected before level i\u201d: i,j should be in the math mode. \u201cdependent on the network structure .\u201d there is an additional blank space after \u2018structure\u2019.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and we will make the response issues clearly in revised version . Q1. \u201c whether this theorem holds for other activations \u2026 instead of only the ReLU activation for all the layers \u201d : A . Current analysis only holds for ReLU , however , it \u2019 s not difficult to generalize to other activations since only the Lipschitz property is required . Q2. \u201c Can an activation satisfying Definition 3 have a similar bound to Theorem 1 \u201d : A . Definition 3 is a necessary but not sufficient condition . If other conditions are also satisfied , a similar bound can be achieved . Q3. \u201c does the convolutional layer simplify the bound in Theorem 1 \u201d : A . Of course Theorem 1 may be simplified by considering some special cases , but it \u2019 s beyond the scope of the paper . We have also fixed the typos and the whole paper has been carefully improved . Thank you very much for your help !"}], "0": {"review_id": "HygcvsAcFX-0", "review_text": "The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018]. More precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical bounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and [Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some conditions) on the change of the layers (layer and interlayer cushion) as well as the activation contraction. It is also worth noting that the paper is using a differrent loss function comparing to [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss. Although the results seem interesting, the analysis is not convincible for me. A plus point is that the paper presents interesting numerical experiments showing the promising of the approach. Major comments: 1) The statement of the Theorem 1 is not clear: is it just under the assumptions of the lemmas or is it under all definitions and lemmas? 2) The proof of Theorem 1 is not clear: how do you get the inequality (5)? how do you get an upper bound on the KL divergence? This is not trivial for me! 3) What is \\rho in Theorem 1 and in Definition 2? 4) Your remark after Theorem 1 is not clear for me. you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1? a simple counter example would fit better the explanation here, I guest. Minor comments: 1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018] without precisely citations. I wonder how do you obtain your Lemma 1? 2) page3, after formula (1), your loss will first DECREASING, not \"increasing\". Check the sentence \"Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....\" ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and we will make the response issues clearly in revised version . # # The contribution in our paper # # Our paper theoretically proves that margin distribution plays an important role in the generalization of deep learning . Specifically , we propose a well-designed loss function inspired by [ Gao , AIJ 2013 ; Zhang , ICML 2017 ] , and it can effectively alleviate the overfitting of deep models . To the best of our knowledge , this is the first work that introduces margin distribution into the analysis of deep learning . We notice that some recent works [ Barlett , NIPS 2017 ; Neyshabur , ICLR 2018 ] have considered this problem , but they only focus on minimum margin , which is significantly different from our paper . The PAC-Bayesian framework is a convenient technique for margin theory analysis . In fact , other techniques like Rademacher complexity [ Koltchinskii , TIT 2001 ; Koltchinskii , ANN STAT 2002 ; Bartlett , MACH LEARN 2002 ; Barlett , NIPS 2017 ] could also be used to derive similar results . In our paper the PAC-Bayesian technique is an analysis approach rather than our main contribution . As emphasized before , the theoretical contribution is relating the generalization gap to margin distribution . # # Major comments # # Q1 . \u201c The statement of the Theorem 1 is not clear : is it just \u2026 definitions and lemmas ? \u201d : A. Theorem 1 is under all the lemmas and definitions in our paper . Q2. \u201c The proof of Theorem 1 is not clear : how do you get the inequality ( 5 ) ? how do you get an upper bound on the KL divergence ? \u201d : A . The inequality ( 5 ) is an extension of matrix version Hoeffding \u2019 s inequalities [ Tropp , FOCS 2012 ; Mackey , ANN STAT 2014 ] . Note that the KL divergence is between two normal distributions with different mean but the same variance , so it can be bounded by |\\vw|^2 / 2 \\sigma^2 . Q3. \u201c What is \\rho in Theorem 1 and in Definition 2 ? \u201d : A . As presented at the beginning of the Section 3 : \u201c and $ \\rho $ be an upper bound on the number of output units in each layer \u201d . Q4. \u201c you claim that the product is ( 3 ) is large , what if we restrict all the spectral norms equal to 1 ? \u201d : A . The impact of norms in deep learning is not similar to the linear models , we can \u2019 t directly normalize it to the value $ 1 $ . And the most common method to control the norm of weights is called weight decay , which tries to control the weights by a preset decay with a parameter . Because of its data-independence , the performance is not satisfied , and many more efficient methods to preventing the overfitting problem have been proposed , such as dropout and batch normalization . If we try to optimize the product of spectral norms directly as a regularization , we can find that the weights update of each layer is related to the product of spectral norms of other layers , the calculation cost for spectral norm is too large , and the weight decay does not consider this correlation . # # Minor comments Q1 . \u201c The Lemma 1 and 2 are almost the same to Lemma 1 and 2 \u2026 you obtain your Lemma 1 ? \u201d : A. Lemma 1 is obtained in a similar way with PAC-Bayesian work [ Neyshabur , ICLR 2018 ] and we have cited this paper in Sec.3 : \u201c ... we have to relate this PAC-Bayesian bound to the expected perturbed loss just like [ Neyshabur , ICLR 2018 ] derive the Lemma 1 in their paper. \u201d But for Lemma 2 , we make a lot of nontrivial modification due to the introduction of margin variance . Q2. \u201c page3 , after formula ( 1 ) , your loss will first DECREASING , not `` increasing '' . \u201d : A . We have refined the misleading description . The whole paper has been carefully improved . Thank you very much for your help !"}, "1": {"review_id": "HygcvsAcFX-1", "review_text": "I consider that improving the generalization capability of neural networks on small dataset is an important line of research, and the method proposed here empirically provides great results. The proposed margin loss (Equation 1) is said to be \"specially adapted for accelerating the convergence velocity of networks by [the authors]\". I would like this statement to be explained better, or at least backed by empirical evidence. In the current state, I consider that the paper lacks an in-depth study of the properties of this handcrafted loss. Few is said on the benefits of having both a linear behavior for points inside the margin and a quadratic loss for far points. The impact of loss hyperparameters (r, \\gamma,\\mu) should be discussed thoughtfully; at some points in the paper, r and \\gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained. Moreover, almost nothing is said about \\mu. By considering a simplified loss function, the provided PAC-Bayes generalization bound (Theorem 1) consider solely the flat loss region [r-\\gamma, r+\\gamma], but shed no light on the benefit of the hinge and quadratic parts. I conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these. The empirical experiments show great evidence that the proposed method successfully improve generalization capability of neural networks on small datasets compared to classical methods. I appreciate the Inter/intra class variance study of Tables 2 and 3. I would like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written (it is not clear in the text if the soft hinge uses an hyperparameter). In the same spirit of my above comments, I would like to see how each loss hyperparameters impacts the results, instead of having access solely to the parameter values selected by the validation process. Typos and minor comments: - Abstract: \"And our ODN model also outperforms the other three loss models...\" Which three loss models? - Section 3: \"Specially, define L_0 as r=\\theta...\" I think it should be r=0 - Section 4.1: model-s => models - Page 7 (and elsewhere): Table. 2 => Table 2 - Please specify that \"Xent\" stands for cross-entropy - Figure 3: Please use larger font sizes - Proof of Lemma 2: Equation. 4 => Equation 4 ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and we will make the response issues clearly in revised version . Q1. \u201c lacks an in-depth study of the properties of this handcrafted loss \u201d and \u201c The impact of loss hyper-parameters ( r , \\gamma , \\mu ) should be discussed thoughtfully \u201d : A . We have added an extra appendix A to clearly explain the intuition behind this loss function . Q2. \u201c the provided PAC-Bayes generalization \u2026 shed no light on the benefit of the hinge and quadratic parts \u201d : A . Our paper proves that the generalization of deep learning heavily depends on the margin distribution . To optimize the margin distribution , we designed this ODN loss function . As a result , this well-designed loss function alleviates the overfitting problem of deep models efficiently . And we introduce how the hinge and quadratic part is derived from the intuition of margin distribution in appendix A. Q3 . \u201c would like the mathematical expression of the `` hinge loss '' and the `` soft hinge loss '' models to be explicitly written \u201d : A . We have explicitly presented the `` ( soft ) hinge loss '' in section 4.1 . We have also fixed the typos and the whole paper has been carefully improved . Thank you very much for your help !"}, "2": {"review_id": "HygcvsAcFX-2", "review_text": "This paper presents a PAC-Bayesian bound for a margin loss. Theorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1? There are some typos in this paper. \u201cTo derive a expected risk bound\u201d: a -> an \u201cused to formalize error-resilience in Arora et al. (2018) as following:\u201d: following: -> follows. \u201cthe deep network from layer i to layer j\u201d, \u201cinjected before level i\u201d: i,j should be in the math mode. \u201cdependent on the network structure .\u201d there is an additional blank space after \u2018structure\u2019.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and we will make the response issues clearly in revised version . Q1. \u201c whether this theorem holds for other activations \u2026 instead of only the ReLU activation for all the layers \u201d : A . Current analysis only holds for ReLU , however , it \u2019 s not difficult to generalize to other activations since only the Lipschitz property is required . Q2. \u201c Can an activation satisfying Definition 3 have a similar bound to Theorem 1 \u201d : A . Definition 3 is a necessary but not sufficient condition . If other conditions are also satisfied , a similar bound can be achieved . Q3. \u201c does the convolutional layer simplify the bound in Theorem 1 \u201d : A . Of course Theorem 1 may be simplified by considering some special cases , but it \u2019 s beyond the scope of the paper . We have also fixed the typos and the whole paper has been carefully improved . Thank you very much for your help !"}}