{"year": "2018", "forum": "rk6H0ZbRb", "title": "Intriguing Properties of Adversarial Examples", "decision": "Invite to Workshop Track", "meta_review": "I am somewhat of two minds from the paper. The authors show empirically that adversarial perturbation error follows power law and looks for a possible explanation. The tie in with generalization is not clear to me and makes me wonder how to evaluate the significance of the finding of the power law distribution..  On the other hand, the authors present an interesting analysis, show that the finding holds in all the cases they explored and also found that architecture search can be used to find neural networks that are more resilient to adversarial search (the last shouldn't be surprising if that was indeed the training criterion).\n\nAll in all, I think that while the paper needs a further iteration prior to publication, it already contains interesting bits that could spur very interesting discussion at the Workshop.\n\n(Side note: There's a reference missing on page 4, first paragraph)", "reviews": [{"review_id": "rk6H0ZbRb-0", "review_text": "This paper insists that adversarial error for small adversarial perturbation follows power low as a function of the perturbation size, and explains the cause by the logit-difference distributions using mean-field theory. Then, the authors propose two methods for improving adversarial robustness (entropy regularization and NAS with reinforcement learning). [strong points] * Based on experimental results over a broad range of datasets, deep network models and their attacks. * Discovery of the fact that adversarial error follows a power low as a function of the perturbation size epsilon for small epsilon. * They found entropy regularization improves adversarial robustness. * Their neural architecture search (NAS) with reinforcement learning found robust deep networks. [weak points] * Unclear derivation of Eq. (9). (What expansion is used in Eq. (21)?) * Non-strict argument using mean-field theory. * Unclear connection between their discovered universality and their proposals (entropy regularization and NAS with reinforcement learning).", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for a careful reading of the manuscript and the helpful feedback . We are glad that the reviewer found several strong points about our paper . Below we respond point by point to the criticism : 1 ) In Eq . ( 21 ) we expand both F ( r+\\Delta_ { 1j } ) and P ( r+\\Delta_ { 1j } ) to lowest order in \\Delta_ { 1j } , using regular Taylor expansion . We have added another step to the derivation to clarify . 2 ) While mean field theory is an approximate framework , it has a long history of effective use across a wide range of fields studying complex behavior including machine learning . For example , there are papers that approach neural networks from a mean field perspective dating back to at least 1989 [ 1 ] . Here , at each step of our calculation we evaluate the validity of the mean field approximation in Fig.3 and Fig.4.If there is a specific point in the approximation that the reviewer objects to , we would be happy to address it further . 3 ) Our proposed entropy regularization is directly related to the finding that logit difference distribution is universal . Since the adversarial error has a universal form due to the universal behavior of logit difference distribution , we tried to increase the logit differences to make our models more robust . As we show in Fig.6 , the entropy regularizer does increase the logit differences , as expected . Due to the increased logit differences , models that were trained with and without adversarial training are more robust to adversarial examples , as shown in Fig.5 ( MNIST ) and Fig.11 ( CIFAR10 ) . As mentioned in the paper , although the functional form of the adversarial error is universal , better models are quantitatively more robust to adversarial examples ( e.g.Figs 1a and 1b ) . Given this , we wanted to study whether architecture can be engineered to improve adversarial accuracy . As mentioned in our submission , recent papers have found that larger models are more robust , but left unanswered whether models that generalize better are less susceptible to adversarial examples [ 2,3 ] . Using NAS , we show that models that generalize better are more robust , however model size does not seem to correlate strongly with adversarial sensitivity . Our findings together present a unified analysis of a model \u2019 s sensitivity to adversarial examples : commonalities among datasets , cause of the commonalities , and dependence on architecture . [ 1 ] Peterson , Carsten . `` A mean field theory learning algorithm for neural networks . '' Complex systems 1 ( 1987 ) : 995-1019 . [ 2 ] Kurakin , Alexey , Ian Goodfellow , and Samy Bengio . `` Adversarial machine learning at scale . '' arXiv preprint arXiv:1611.01236 ( 2016 ) . [ 3 ] Madry , Aleksander , et al . `` Towards deep learning models resistant to adversarial attacks . '' arXiv preprint arXiv:1706.06083 ( 2017 ) ."}, {"review_id": "rk6H0ZbRb-1", "review_text": "Very intriguing paper and results to say the least. I like the way it is written, and the neat interpretations that the authors give of what is going on (instead of assuming that readers will see the same). There is a well presented story of experiments to follow which gives us insight into the problem. Interesting insight into defensive distillation and the effects of uncertainty in neural networks. Quality/Clarity: well written and was easy for me to read Originality: Brings both new ideas and unexpected experimental results. Significance: Creates more questions than it answers, which imo is a positive as this topic definitely deserves more research. Remarks: - Maybe re-render Figure 3 at a higher resolution? - The caption of Figure 5 doesn't match the labels in the figure's legend, and also has a weird wording, making it unclear what (a) and (b) refer to. - In section 4 you say you test your models with FGSM accuracy, but in Figure 7 you report stepll and PGD accuracy, could you also plot the same curves for FGSM? - In Figure 4, I'm not sure I understand the right-tail of the distributions. Does it mean that when Delta_ij is very large, epsilon can be very small and still cause an adversarial pertubation? If so does it mean that overconfidence in the extreme is also bad? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for a careful reading of the manuscript and the helpful suggestions . We are delighted that the reviewer thinks this topic deserves more research ; we certainly agree ! We implemented the suggestions by the reviewer , as detailed below : - Re-rendered Fig.3 at a higher resolution . We noticed that Fig.3 may look pixelated on certain web browsers , but rendered correctly on all pdf viewers we have tried . - Corrected the typos and clarified the caption of Fig.5.We appreciate the reviewer noticing this . - Experiment 1 networks were trained with stepll and Experiment 2 networks were trained with PGD . However , we did use FGSM accuracy on the validation set to choose the architectures . For this reason , we followed the reviewer 's suggestion and plotted the same curves for FGSM attack in Figure 15 . - Fig.4 presents histograms where both axes are shown in log scale . The right-tail of the distributions signify that there are not many samples with as large \\Delta_ { ij } values ."}, {"review_id": "rk6H0ZbRb-2", "review_text": "This work presents an empirical study aiming at improving the understanding of the vulnerability of neural networks to adversarial examples. Paraphrasing the authors, the main observation of the study is that the vulnerability is due to an inherent uncertainty that neural networks have about their predictions ( the difference between the logits). This is consistent across architectures, datasets. Further, the authors note that \"the universality is not a result of the specific content of these datasets nor the ability of the model to generalize.\" While this empirical study contains valuable information, its above conclusions are factually wrong. It can be theoretically proven at least using two routes. They are also in contradiction with other empirical observations consistent across several previous studies. 1-Constructive counter-argument: Consider a neural network that always outputs a constant prediction. It (1) is by definition independent of any dataset (2) generalizes perfectly (3) has zero adversarial error, hence contradicting the central statement of the paper. 2- Analysis-based counter-argument: Consider a neural network with one hidden layer and two classes. It is easy to show that the difference between the scores (logits) of the two classes is linear in the operator norm of the hidden weight matrix and linear in the L2-norm of the last weight vector. Therefore, the robustness of the model indeed depends on its capability to generalize because the latter is essentially governed by the geometric margin of the linear separator and the spectral norm of the weight matrix (see [1,2,3]). QED. 3- Further, the lack of calibration of neural networks and its causes are well known. Among other things, it is due to the use of building blocks (such as batch-norm [4]), regularization (e.g., weight decay) or the use of softmax+cross-entropy during training. While this is convenient for optimization reasons, it indeed hurts the calibration. The authors should try to train a neural network with a large margin criteria and see if the same phenomenon still holds when they measure the geometric margin. Another alternative is to use a temperature with the softmax[4]. Therefore, the observations of the empirical study cannot be generalized to neural networks and should be explicitly restricted to neural networks using softmax with cross-entropy as criteria. I believe the conclusions of this study are misleading, hence I recommend to reject the paper. [1] Spectrally Normalized Margin-bounds Margin bounds for neural networks (Bartlett et al., 2017) [2] Parseval Networks: Improving Robustness to Adversarial Examples (Cisse et al., 2017) [3] Formal Guarantees on the Robustness of a classifier against adversarial examples (Hein et al., 2017) [4] On the Calibration of Modern Neural Networks (Guo et al., 2017)", "rating": "3: Clear rejection", "reply_text": "We would first like to thank the reviewer for their careful reading of our manuscript and thoughtful comments . We are glad that the reviewer believes our study contains valuable information and interesting experiments . Meanwhile , we would like to address the concerns raised . Summary : We are confident that our results and conclusions are not at odds with the perspective of the referee . We believe the main issue stems from some ambiguous language in the original text that we have now corrected . We have also implemented the additional experiments proposed by the referee and have found them to corroborate our original conclusions . Details : We believe that there is some confusion regarding what was meant by our statement \u201c the universality is not a result of the specific content of these datasets nor the ability of the model to generalize . '' We are not proposing that the susceptibility of a neural network to adversarial examples is independent of its ability to generalize . Instead , we are saying that the functional form of the adversarial error as a function of epsilon does not depend on generalization ( i.e.that it should scale like A * \\epsilon regardless of the network \u2019 s ability to generalize , as shown by our experiments on randomly sampled logits and MNIST with randomly-shuffled labels ) . In fact , we agree with the referee that the constant , A , will depend on the spectral norm of the Jacobian ( and hence the readout weight matrix ) and on the network \u2019 s ability to generalize . We copy some excerpts from the original submission to corroborate this below . However , the reviewer \u2019 s concerns allowed us to realize that our original phrasing was ambiguous . We have therefore reworded our conclusions to be clearer by replacing the problematic statement with : \u201c Here we show that distributions of logit differences have a universal functional form . This functional form is independent of architecture , dataset , and training protocol ; nor does it change during training. \u201d We have also removed the sentence \u201c Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. \u201d Excerpts from the original text showing agreement with the referee : \u201c We observe that although the qualitative form of logit differences and adversarial error is universal , it can be quantitatively improved with entropy regularization and better network architectures. \u201d \u201c ... vanilla NASNet-A ( best clean accuracy in our study ) has a lower adversarial error than adversarially trained [ models ] \u2026 \u201d In eq.8 we find that the threshold for an adversarial error is proportional to J^TJ . This is clearly proportional to the spectral norm of the Jacobian ."}], "0": {"review_id": "rk6H0ZbRb-0", "review_text": "This paper insists that adversarial error for small adversarial perturbation follows power low as a function of the perturbation size, and explains the cause by the logit-difference distributions using mean-field theory. Then, the authors propose two methods for improving adversarial robustness (entropy regularization and NAS with reinforcement learning). [strong points] * Based on experimental results over a broad range of datasets, deep network models and their attacks. * Discovery of the fact that adversarial error follows a power low as a function of the perturbation size epsilon for small epsilon. * They found entropy regularization improves adversarial robustness. * Their neural architecture search (NAS) with reinforcement learning found robust deep networks. [weak points] * Unclear derivation of Eq. (9). (What expansion is used in Eq. (21)?) * Non-strict argument using mean-field theory. * Unclear connection between their discovered universality and their proposals (entropy regularization and NAS with reinforcement learning).", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for a careful reading of the manuscript and the helpful feedback . We are glad that the reviewer found several strong points about our paper . Below we respond point by point to the criticism : 1 ) In Eq . ( 21 ) we expand both F ( r+\\Delta_ { 1j } ) and P ( r+\\Delta_ { 1j } ) to lowest order in \\Delta_ { 1j } , using regular Taylor expansion . We have added another step to the derivation to clarify . 2 ) While mean field theory is an approximate framework , it has a long history of effective use across a wide range of fields studying complex behavior including machine learning . For example , there are papers that approach neural networks from a mean field perspective dating back to at least 1989 [ 1 ] . Here , at each step of our calculation we evaluate the validity of the mean field approximation in Fig.3 and Fig.4.If there is a specific point in the approximation that the reviewer objects to , we would be happy to address it further . 3 ) Our proposed entropy regularization is directly related to the finding that logit difference distribution is universal . Since the adversarial error has a universal form due to the universal behavior of logit difference distribution , we tried to increase the logit differences to make our models more robust . As we show in Fig.6 , the entropy regularizer does increase the logit differences , as expected . Due to the increased logit differences , models that were trained with and without adversarial training are more robust to adversarial examples , as shown in Fig.5 ( MNIST ) and Fig.11 ( CIFAR10 ) . As mentioned in the paper , although the functional form of the adversarial error is universal , better models are quantitatively more robust to adversarial examples ( e.g.Figs 1a and 1b ) . Given this , we wanted to study whether architecture can be engineered to improve adversarial accuracy . As mentioned in our submission , recent papers have found that larger models are more robust , but left unanswered whether models that generalize better are less susceptible to adversarial examples [ 2,3 ] . Using NAS , we show that models that generalize better are more robust , however model size does not seem to correlate strongly with adversarial sensitivity . Our findings together present a unified analysis of a model \u2019 s sensitivity to adversarial examples : commonalities among datasets , cause of the commonalities , and dependence on architecture . [ 1 ] Peterson , Carsten . `` A mean field theory learning algorithm for neural networks . '' Complex systems 1 ( 1987 ) : 995-1019 . [ 2 ] Kurakin , Alexey , Ian Goodfellow , and Samy Bengio . `` Adversarial machine learning at scale . '' arXiv preprint arXiv:1611.01236 ( 2016 ) . [ 3 ] Madry , Aleksander , et al . `` Towards deep learning models resistant to adversarial attacks . '' arXiv preprint arXiv:1706.06083 ( 2017 ) ."}, "1": {"review_id": "rk6H0ZbRb-1", "review_text": "Very intriguing paper and results to say the least. I like the way it is written, and the neat interpretations that the authors give of what is going on (instead of assuming that readers will see the same). There is a well presented story of experiments to follow which gives us insight into the problem. Interesting insight into defensive distillation and the effects of uncertainty in neural networks. Quality/Clarity: well written and was easy for me to read Originality: Brings both new ideas and unexpected experimental results. Significance: Creates more questions than it answers, which imo is a positive as this topic definitely deserves more research. Remarks: - Maybe re-render Figure 3 at a higher resolution? - The caption of Figure 5 doesn't match the labels in the figure's legend, and also has a weird wording, making it unclear what (a) and (b) refer to. - In section 4 you say you test your models with FGSM accuracy, but in Figure 7 you report stepll and PGD accuracy, could you also plot the same curves for FGSM? - In Figure 4, I'm not sure I understand the right-tail of the distributions. Does it mean that when Delta_ij is very large, epsilon can be very small and still cause an adversarial pertubation? If so does it mean that overconfidence in the extreme is also bad? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for a careful reading of the manuscript and the helpful suggestions . We are delighted that the reviewer thinks this topic deserves more research ; we certainly agree ! We implemented the suggestions by the reviewer , as detailed below : - Re-rendered Fig.3 at a higher resolution . We noticed that Fig.3 may look pixelated on certain web browsers , but rendered correctly on all pdf viewers we have tried . - Corrected the typos and clarified the caption of Fig.5.We appreciate the reviewer noticing this . - Experiment 1 networks were trained with stepll and Experiment 2 networks were trained with PGD . However , we did use FGSM accuracy on the validation set to choose the architectures . For this reason , we followed the reviewer 's suggestion and plotted the same curves for FGSM attack in Figure 15 . - Fig.4 presents histograms where both axes are shown in log scale . The right-tail of the distributions signify that there are not many samples with as large \\Delta_ { ij } values ."}, "2": {"review_id": "rk6H0ZbRb-2", "review_text": "This work presents an empirical study aiming at improving the understanding of the vulnerability of neural networks to adversarial examples. Paraphrasing the authors, the main observation of the study is that the vulnerability is due to an inherent uncertainty that neural networks have about their predictions ( the difference between the logits). This is consistent across architectures, datasets. Further, the authors note that \"the universality is not a result of the specific content of these datasets nor the ability of the model to generalize.\" While this empirical study contains valuable information, its above conclusions are factually wrong. It can be theoretically proven at least using two routes. They are also in contradiction with other empirical observations consistent across several previous studies. 1-Constructive counter-argument: Consider a neural network that always outputs a constant prediction. It (1) is by definition independent of any dataset (2) generalizes perfectly (3) has zero adversarial error, hence contradicting the central statement of the paper. 2- Analysis-based counter-argument: Consider a neural network with one hidden layer and two classes. It is easy to show that the difference between the scores (logits) of the two classes is linear in the operator norm of the hidden weight matrix and linear in the L2-norm of the last weight vector. Therefore, the robustness of the model indeed depends on its capability to generalize because the latter is essentially governed by the geometric margin of the linear separator and the spectral norm of the weight matrix (see [1,2,3]). QED. 3- Further, the lack of calibration of neural networks and its causes are well known. Among other things, it is due to the use of building blocks (such as batch-norm [4]), regularization (e.g., weight decay) or the use of softmax+cross-entropy during training. While this is convenient for optimization reasons, it indeed hurts the calibration. The authors should try to train a neural network with a large margin criteria and see if the same phenomenon still holds when they measure the geometric margin. Another alternative is to use a temperature with the softmax[4]. Therefore, the observations of the empirical study cannot be generalized to neural networks and should be explicitly restricted to neural networks using softmax with cross-entropy as criteria. I believe the conclusions of this study are misleading, hence I recommend to reject the paper. [1] Spectrally Normalized Margin-bounds Margin bounds for neural networks (Bartlett et al., 2017) [2] Parseval Networks: Improving Robustness to Adversarial Examples (Cisse et al., 2017) [3] Formal Guarantees on the Robustness of a classifier against adversarial examples (Hein et al., 2017) [4] On the Calibration of Modern Neural Networks (Guo et al., 2017)", "rating": "3: Clear rejection", "reply_text": "We would first like to thank the reviewer for their careful reading of our manuscript and thoughtful comments . We are glad that the reviewer believes our study contains valuable information and interesting experiments . Meanwhile , we would like to address the concerns raised . Summary : We are confident that our results and conclusions are not at odds with the perspective of the referee . We believe the main issue stems from some ambiguous language in the original text that we have now corrected . We have also implemented the additional experiments proposed by the referee and have found them to corroborate our original conclusions . Details : We believe that there is some confusion regarding what was meant by our statement \u201c the universality is not a result of the specific content of these datasets nor the ability of the model to generalize . '' We are not proposing that the susceptibility of a neural network to adversarial examples is independent of its ability to generalize . Instead , we are saying that the functional form of the adversarial error as a function of epsilon does not depend on generalization ( i.e.that it should scale like A * \\epsilon regardless of the network \u2019 s ability to generalize , as shown by our experiments on randomly sampled logits and MNIST with randomly-shuffled labels ) . In fact , we agree with the referee that the constant , A , will depend on the spectral norm of the Jacobian ( and hence the readout weight matrix ) and on the network \u2019 s ability to generalize . We copy some excerpts from the original submission to corroborate this below . However , the reviewer \u2019 s concerns allowed us to realize that our original phrasing was ambiguous . We have therefore reworded our conclusions to be clearer by replacing the problematic statement with : \u201c Here we show that distributions of logit differences have a universal functional form . This functional form is independent of architecture , dataset , and training protocol ; nor does it change during training. \u201d We have also removed the sentence \u201c Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. \u201d Excerpts from the original text showing agreement with the referee : \u201c We observe that although the qualitative form of logit differences and adversarial error is universal , it can be quantitatively improved with entropy regularization and better network architectures. \u201d \u201c ... vanilla NASNet-A ( best clean accuracy in our study ) has a lower adversarial error than adversarially trained [ models ] \u2026 \u201d In eq.8 we find that the threshold for an adversarial error is proportional to J^TJ . This is clearly proportional to the spectral norm of the Jacobian ."}}