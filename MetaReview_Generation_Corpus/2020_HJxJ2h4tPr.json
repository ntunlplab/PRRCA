{"year": "2020", "forum": "HJxJ2h4tPr", "title": "HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion", "decision": "Reject", "meta_review": "This paper proposes a multi-frame super-resolution method including recursive fusion for co-registration and registration loss to solve the problem where the super-resolution results and the high-resolution labels are not pixel-wise aligned. While reviewer #1 is positive about this paper, reviewer #2 and #3 rated weak reject and reject respectively. Both reviewer #2 and #3 have extensive experience in the topic of image super-resolution. The major concerns raised by the reviewers include the lack of many references, the comparison of recursive fusion with related work, limited test databases, using a single translational motion for the SR images, and limited novelty on the network modules.  The authors provided detailed response to the concerns, however they did not change the overall rating of the reviewers. While the ACs agree that this work has merits, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.", "reviews": [{"review_id": "HJxJ2h4tPr-0", "review_text": "This paper proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. Because the ground truth SR image is typically misaligned with the estimation SR image, the authors proposed to learn the shift with a neural network 'ShiftNet' in a cooperative setting with HighRes-net. The experiments were performed on the ESA challenge on satellite images, showing good results. Overall, I found this paper interesting, and the method described is both clever and efficient. While some points need to be clarified, I am in favor of accepting this paper to ICLR. Positive aspects: - the paper is very clear and easy to read, with nice figures. - a sensitivity analysis on many different parameters or types of inputs are made, which makes this paper an interesting research paper. For example, the tests on the type of reference image to stack at the beginning are very interesting. - While I am not an expert on super-resolution, I do see a clever algorithm, that can be for example used with different number of input views. - The end-to-end framework is also quite interesting as it allows to be spread easily across the very large satellite images users, with the code aldready publicly available. - Lastly, the results are good wrt to the state-of-the-art, as the algorithm was proposed during a 2019 challenge and was in the top ones on the private leaderboard. Remarks and clarifications: - After looking at the challenge website, it is quite strange to find that the results values in Table 1, in terms of public and private scores, differ from the actual leaderboards, even if the metric is still the same. Thus, I was not able to understand (i) if the authors really participate to the challenge (ii) what method they presented during the challenge as 3 or 4 are shown here (iii) what was their ranking in the real leaderboard. It is important to be precise. From what I see, they might be second on the private score (the one that matters), if so it can be clarified also in the abstract where the word 'topped' was used. - cPSNR metric : (i) can you please explain the acronym; (ii) it was the competition metric, so it means it is not 'we use' but 'the challenge used' - which will also show that it was not a choice to satisfy your results, but a standard metric. - Lanczos interpolation: can you please explain in a small sentence in what this interpolation differs from the others? - ShiftNet: what is this network? We only know tha it is adapted from HomographyNet, but we don't have any information about how it is composed. We just know from App. 1 that it has a very large number of parameters (34M). Why is that? Why a rigid registration needs such a large number of parameters? - median anchor image: this is one of the interesting points of the paper. Can you please just clarify that you take a naive median image? Such as: for pixel (i,j), med(i,j)= med( LR1(i,j), LR2(i,j) ,...) . - You are saying 'each imageset is padded to 32 views'. What do you mean? I thought your network was able to be used with different numbers of views. How did you pad your imageset? - you list 'several baselines': for me, a baseline is something to compare to - usually, it is even something easy, such as the ESA baseline. But in your list, you are mixing baselines, other state-of-the-art methods (some of which you don't beat, so it's not a 'baseline'...), and your methods. It is very difficult to know which ones are yours. Please make different lists and/or identify yours in the table 1. - What is the Ensemble method? It appears that in the paper, you explain the 'HighRes-net + shiftNet' method, but actually, the 'Ensemble' is the better one, while not clearly described. I don't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously. - How did you select the hyperparameters of your model? Scientific questions: - Is it possible to have views of different sizes as inputs? Or views with missing parts? - Usually, we have a high resolution image that is different in terms of type of acquisition (like a different type of satellite, but also true in medical images for ex.). Is your network ready for that? I mean: can we constrain the method even if it is not the same type of acquisition as ground truth? In that case, is that possible to have a super-resolution of the type of the input LR images? Typos: - is comprised of -> is composed of - in Table 7, the bold number should be the beta=infinity as it is the best one. It will be clearer, even if of course, a good train score does not mean a good method because of overfitting. ", "rating": "8: Accept", "reply_text": "Thank you for your detailed assessment of our work 1 . > \u201c it is quite strange to find that the results values in Table 1 , in terms of public and private scores , differ from the actual leaderboards , even if the metric is still the same . Thus , I was not able to understand ( i ) if the authors really participate to the challenge ( ii ) what method they presented during the challenge as 3 or 4 are shown here ( iii ) what was their ranking in the real leaderboard . It is important to be precise . From what I see , they might be second on the private score ( the one that matters ) , if so it can be clarified in the abstract where the word 'topped ' was used. \u201d To clear any confusion , we did participate in the ESA challenge as team Rifat . By \u201c topped \u201d we mean \u201c achieved competitive results \u201d . Our ensemble model , an ensemble of two HighRes-net models trained separately ( discussed in a comment below ) achieved - 0.947388637793901 ( 1st position ) on the public leaderboard : https : //kelvins.esa.int/proba-v-super-resolution/leaderboard/ - 0.9477450367529225 ( 2nd position ) on the final leaderboard ( public + private ) : https : //kelvins.esa.int/proba-v-super-resolution/results/ The source of the confusion was the rounding up of the scores to 4 decimals - reporting 0.9474 on the Public leaderboard and 0.9477 on the Public+Private . DeepSUM achieved the best score ( 0.9474466476281652 ) in the final leaderboard . To get a sense of the scale , note that the 3rd place scored 0.9576339586408439 . Beyond the cPSNR evaluation metric , our model requires an order of magnitude less training time - 10 hours for HighRes-net VS 1 week for DeepSUM . This is because the DeepSUM architecture first learns to upscale each low-res view with a shared SISR-type ( Single-Image SR ) net . Then all the downstream tasks must to be learned on the size of a high-res image - x3 upscaling factor for PROBA-V , which means a x9 increase in memory demand . To summarize , HighRes-net is competitive in terms of cPSNR , and also an order of magnitude faster to train . Thank you for pointing out these sources of confusion . We \u2019 ll write the scores in their full precision in the revised manuscript . -- -- -- -- -- -- -- -- - 7 . > \u201c you list 'several baselines ' : for me , a baseline is something to compare to - usually , it is even something easy , such as the ESA baseline . But in your list , you are mixing baselines , other state-of-the-art methods ( some of which you do n't beat , so it 's not a 'baseline ' ... ) , and your methods . It is very difficult to know which ones are yours . Please make different lists and/or identify yours in the table 1. \u201d Thank you for this suggestion . We have identified which lines in Table 1 correspond to our methods . -- -- -- -- -- -- -- -- -- 8 . > \u201c What is the Ensemble method ? It appears that in the paper , you explain the 'HighRes-net + shiftNet ' method , but actually , the 'Ensemble ' is the better one , while not clearly described . I do n't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously . We can see how this is confusing . Our paper is about the HighRes-net + ShiftNet method indeed . For our Ensemble method , we trained two HighRes-net+shiftNet models , one with K=16 and one with K=32 frames . We averaged HighRes-Net_16 + HighRes-Net_32 outputs for our predictions on the final leaderboard . We have included these details in our revised manuscript ."}, {"review_id": "HJxJ2h4tPr-1", "review_text": "The paper proposes a framework including recursive fusion to co-registration and registration loss to solve the problem that the super-resolution results and the high-resolution labels are not pixel aligned. Besides, the method is able to achieve good performance in the Proba-V Kelvin dataset. However, I have some concerns about this paper: 1) This paper lacks many references. Recently, many works focus on multi-frame super-resolution containing video super-resolution and stereo image super-resolution via deep learning. They are using multiple low-resolution image to construct high-resolution image. For example: Stereo super-resolution: Jeon, Daniel S., et al. \"Enhancing the spatial resolution of stereo images using a parallax prior.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018. Wang, Longguang, et al. \"Learning parallax attention for stereo image super-resolution.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2019. Video super-resolution: Tao, Xin, et al. \"Detail-revealing deep video super-resolution.\" *Proceedings of the IEEE International Conference on Computer Vision*. 2017. FRVSR: Sajjadi, Mehdi SM, Raviteja Vemulapalli, and Matthew Brown. \"Frame-recurrent video super-resolution.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018. FFCVSR: Yan, Bo, Chuming Lin, and Weimin Tan. \"Frame and Feature-Context Video Super-Resolution.\" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 33. 2019. EDVR: Wang, Xintao, et al. \"Edvr: Video restoration with enhanced deformable convolutional networks.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*. 2019. 2) Recursive fusion is aimed to fuse multiple low-resolution image information. Recently, more and more work utilize different methods to fuse multiple low-resolution image. For example, Tao et al proposes SPMC (Sub-pixel Motion Compensation) to align image, FRVSR uses unsupervised flow network that predicts optical flow to warp image, FFCVSR directly concatenate low-resolution image as the input of 2D convolutional network to fuse the information, and EDVR fuses multiple image features via utilizing deformable convolution. Thus, what is the advantage of recursive fusion compared to the above methods? This paper should discuss the difference between recursive fusion and the above methods. 3) Registration loss is important in this paper and it can solve the problem the output SR is not pixel-wise aligned to the HR ground truth. Registration loss utilizes ShiftNet that is adapted from HomographyNet. Thus, what is the difference between ShiftNet and HomographyNet? This paper should add some details about ShiftNet and Lanczos interpolation. 4) It is better to test more datasets and compare with more state-of-the-art methods. This paper only tests in a satellite image dataset. Some datasets can be considered such as VID4 dataset in video super-resolution.", "rating": "3: Weak Reject", "reply_text": "1. > \u201c This paper lacks many references . Recently , many works focus on MFSR containing video SR and stereo image SR via deep learning. \u201d Thank you for pointing out these references . We 've included them in our revision . We want to stress that our setting is different from video SR in several ways : - We learn to super-resolve sets and not sequences of low-res views . Video SR relies on motion estimation from a sequence of observations . Also , prediction at time t=T relies on predictions at t < T ( autoregressive approach ) . Whereas in our case , we predict a single image from an unordered set of low-res inputs . - In our setting , the low-res views are multi-temporal ( taken at different times ) from different revisits . Please see Paragraph 1 in our comment to Reviewer 3 : https : //openreview.net/forum ? id=HJxJ2h4tPr & noteId=B1l6sKmUoB Our work different from Stereo SR : - Multi-temporality ( see above ) - These images were taken at a nadir direction ( top-down view ) above different subpoints ( coordinates below satellite ) . Whereas Stereo SR assumes that both views focus on the same point , but from different angles . -- - 2. > \u201c Thus , what is the advantage of recursive fusion compared to the above methods [ Video SR papers : SPMC , FRVSR , FFCVSR , EDVR ] ? This paper should discuss the difference between recursive fusion and the above methods. \u201d SPMC , FRVSR , FFCVSR and EDVR are all video SR algorithms . They all assume the input to be a temporal sequence of frames . Motion or optical flow can be estimated to super-resolve the sequences of frames . ( see Part 1 of this comment above ) . In this work , we do not assume low-res inputs to be ordered in time . Our training input is a set of low-res views with unknown timestamps and our target output is a single image - not another sequence . More importantly , those Video SR methods were trained and tested on synthetically down-scaled data - we elaborate on this on Part 4 below . DeepSUM ( Molini et al . ) is another method that scored similarly to ours in the same ESA competition leaderboard with the same dataset . Their paper already showed competitive results compared to an architecture inspired by DUF ( `` Deep video SR network using dynamic upsampling filters without explicit motion compensation . `` CVPR , 2018 ) Thank for raising this important question . We 've included this in the Related Work discussion . -- - 3. > \u201c It is better to test more datasets and compare with more state-of-the-art methods . This paper only tests in a satellite image dataset . Some datasets can be considered such as VID4 dataset in video super-resolution. \u201d Many benchmarks exist for Super Resolution : VIDEO SR : - Vid4 : C. Liu and D. Sun . A bayesian approach to adaptive video super resolution . In CVPR , pages 209\u2013216 . IEEE , 2011 . - Vimeo 90K : Xue , Tianfan , et al . `` Video enhancement with task-oriented flow . '' International Journal of Computer Vision 127.8 ( 2019 ) : 1106-1125 . - Y10 : Sajjadi , Mehdi SM , Raviteja Vemulapalli , and Matthew Brown . `` Frame-recurrent video super-resolution . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2018.- REDS4 ( NTIRE 2019 ) : Nah , Seungjun , et al . `` Ntire 2019 challenge on video deblurring : Methods and results . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops . 2019.SISR : - Set 5 : M. Bevilacqua , A. Roumy , C. Guillemot , and M. L. AlberiMorel . Low-complexity single-image super-resolution based on nonnegative neighbor embedding . 2012.- Set 14 : R. Zeyde , M. Elad , and M. Protter . On single image scale-up using sparse-representations . In International Conference on Curves and Surfaces , pages 711\u2013730 . Springer , 2010 . STEREO : - FLICKR 1024 : Wang , Yingqian , et al . `` Flickr1024 : A Large-Scale Dataset for Stereo Image Super-Resolution . '' CVPR Workshops 2019 However , in all of these datasets , the low-res views are artificially down-scaled , and prior work has shown such methods do not generalize to real world low-res imagery . See also paragraph 4 of our comment to Reviewer3 : https : //openreview.net/forum ? id=HJxJ2h4tPr & noteId=ryxbd5DuiH \u201c The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling ( or in a few cases by blurring followed by down-sampling ) . We show that such methods fail to produce good results when applied to real-world low-resolution , low quality images \u201d - https : //arxiv.org/abs/1807.11458 . See also , Zero shot paper . Shocher , Assaf , Nadav Cohen , and Michal Irani . `` \u201c zero-shot \u201d super-resolution using deep internal learning . '' CVPR 2018 . We agree that benchmarking on more datasets and with more models would strengthen our paper . We are not aware of other publicly available datasets for MFSR with real low-res ( not artificially down-sampled ) images . By working on this dataset , we want to encourage the community to consider on real-world ( not synthetic ) images for super-resolution ."}, {"review_id": "HJxJ2h4tPr-2", "review_text": "This paper presents a multi-frame super-resolution method applied to satellite imagery. It first estimates a reference image for the multiple input LR images by median filtering. Then it pairwise encodes the reference image and each of the multiple images in a recursive fashion then fuses the corresponding feature maps with residual blocks and bottleneck layers until only one feature maps for the entire multiple images obtained. In other words, LR images are fused into a single global encoding. Then, it applies a standard upsampling network to obtain the super-resolved image this image is fed into a network that estimates only the translational shift, and the shifted image with the estimated translation parameters finally resampled. A major concern is the estimation of a single translational motion for the SR image at the end of the network after all multiple images are already fused. The fusion strategy disregards the underlying spatially varying motion. This explicitly assumes the images are on a flat surface, which perhaps an acceptable assumption for high-orbit satellite imagery where the ground surface depth variances might be negligible. Still, this is a very critical limitation of the method. Besides, I am not convinced that pair-wise fusion can handle significant translational fusion as the filters have shared parameters. How a single convolutional layer accomplishes a global encoding and compensates for any translation between any LR image pair is neither articulated nor convincing discussion and evaluations are provided. Of course, such a problematic approach needs at least some kind of motion compensation, which may explain the need for the ShiftNet layer at the end. Nevertheless, this seems quite problematic. Even assuming the method only applies to satellite imagery, it lacks mechanisms to compensate/distinguish cloud coverage and atmospheric distortions. Characterization of satellite imagery noise models (Weibull, etc.) common in such imagery as a prior also completely disregarded. For these reasons, the proposed method fails to be considered as a comprehensive approach for multi-image super-resolution of satellite imagery. Novelty-wise, there is very little as all modules have been commonly used for SR tasks. ", "rating": "1: Reject", "reply_text": "Thank you for reviewing our work . Can you please clarify what you mean by the following sentence ? > `` The fusion strategy disregards the underlying spatially varying motion '' Are you suggesting that the fusion strategy disregards the relative translation between the low-res views ?"}], "0": {"review_id": "HJxJ2h4tPr-0", "review_text": "This paper proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. Because the ground truth SR image is typically misaligned with the estimation SR image, the authors proposed to learn the shift with a neural network 'ShiftNet' in a cooperative setting with HighRes-net. The experiments were performed on the ESA challenge on satellite images, showing good results. Overall, I found this paper interesting, and the method described is both clever and efficient. While some points need to be clarified, I am in favor of accepting this paper to ICLR. Positive aspects: - the paper is very clear and easy to read, with nice figures. - a sensitivity analysis on many different parameters or types of inputs are made, which makes this paper an interesting research paper. For example, the tests on the type of reference image to stack at the beginning are very interesting. - While I am not an expert on super-resolution, I do see a clever algorithm, that can be for example used with different number of input views. - The end-to-end framework is also quite interesting as it allows to be spread easily across the very large satellite images users, with the code aldready publicly available. - Lastly, the results are good wrt to the state-of-the-art, as the algorithm was proposed during a 2019 challenge and was in the top ones on the private leaderboard. Remarks and clarifications: - After looking at the challenge website, it is quite strange to find that the results values in Table 1, in terms of public and private scores, differ from the actual leaderboards, even if the metric is still the same. Thus, I was not able to understand (i) if the authors really participate to the challenge (ii) what method they presented during the challenge as 3 or 4 are shown here (iii) what was their ranking in the real leaderboard. It is important to be precise. From what I see, they might be second on the private score (the one that matters), if so it can be clarified also in the abstract where the word 'topped' was used. - cPSNR metric : (i) can you please explain the acronym; (ii) it was the competition metric, so it means it is not 'we use' but 'the challenge used' - which will also show that it was not a choice to satisfy your results, but a standard metric. - Lanczos interpolation: can you please explain in a small sentence in what this interpolation differs from the others? - ShiftNet: what is this network? We only know tha it is adapted from HomographyNet, but we don't have any information about how it is composed. We just know from App. 1 that it has a very large number of parameters (34M). Why is that? Why a rigid registration needs such a large number of parameters? - median anchor image: this is one of the interesting points of the paper. Can you please just clarify that you take a naive median image? Such as: for pixel (i,j), med(i,j)= med( LR1(i,j), LR2(i,j) ,...) . - You are saying 'each imageset is padded to 32 views'. What do you mean? I thought your network was able to be used with different numbers of views. How did you pad your imageset? - you list 'several baselines': for me, a baseline is something to compare to - usually, it is even something easy, such as the ESA baseline. But in your list, you are mixing baselines, other state-of-the-art methods (some of which you don't beat, so it's not a 'baseline'...), and your methods. It is very difficult to know which ones are yours. Please make different lists and/or identify yours in the table 1. - What is the Ensemble method? It appears that in the paper, you explain the 'HighRes-net + shiftNet' method, but actually, the 'Ensemble' is the better one, while not clearly described. I don't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously. - How did you select the hyperparameters of your model? Scientific questions: - Is it possible to have views of different sizes as inputs? Or views with missing parts? - Usually, we have a high resolution image that is different in terms of type of acquisition (like a different type of satellite, but also true in medical images for ex.). Is your network ready for that? I mean: can we constrain the method even if it is not the same type of acquisition as ground truth? In that case, is that possible to have a super-resolution of the type of the input LR images? Typos: - is comprised of -> is composed of - in Table 7, the bold number should be the beta=infinity as it is the best one. It will be clearer, even if of course, a good train score does not mean a good method because of overfitting. ", "rating": "8: Accept", "reply_text": "Thank you for your detailed assessment of our work 1 . > \u201c it is quite strange to find that the results values in Table 1 , in terms of public and private scores , differ from the actual leaderboards , even if the metric is still the same . Thus , I was not able to understand ( i ) if the authors really participate to the challenge ( ii ) what method they presented during the challenge as 3 or 4 are shown here ( iii ) what was their ranking in the real leaderboard . It is important to be precise . From what I see , they might be second on the private score ( the one that matters ) , if so it can be clarified in the abstract where the word 'topped ' was used. \u201d To clear any confusion , we did participate in the ESA challenge as team Rifat . By \u201c topped \u201d we mean \u201c achieved competitive results \u201d . Our ensemble model , an ensemble of two HighRes-net models trained separately ( discussed in a comment below ) achieved - 0.947388637793901 ( 1st position ) on the public leaderboard : https : //kelvins.esa.int/proba-v-super-resolution/leaderboard/ - 0.9477450367529225 ( 2nd position ) on the final leaderboard ( public + private ) : https : //kelvins.esa.int/proba-v-super-resolution/results/ The source of the confusion was the rounding up of the scores to 4 decimals - reporting 0.9474 on the Public leaderboard and 0.9477 on the Public+Private . DeepSUM achieved the best score ( 0.9474466476281652 ) in the final leaderboard . To get a sense of the scale , note that the 3rd place scored 0.9576339586408439 . Beyond the cPSNR evaluation metric , our model requires an order of magnitude less training time - 10 hours for HighRes-net VS 1 week for DeepSUM . This is because the DeepSUM architecture first learns to upscale each low-res view with a shared SISR-type ( Single-Image SR ) net . Then all the downstream tasks must to be learned on the size of a high-res image - x3 upscaling factor for PROBA-V , which means a x9 increase in memory demand . To summarize , HighRes-net is competitive in terms of cPSNR , and also an order of magnitude faster to train . Thank you for pointing out these sources of confusion . We \u2019 ll write the scores in their full precision in the revised manuscript . -- -- -- -- -- -- -- -- - 7 . > \u201c you list 'several baselines ' : for me , a baseline is something to compare to - usually , it is even something easy , such as the ESA baseline . But in your list , you are mixing baselines , other state-of-the-art methods ( some of which you do n't beat , so it 's not a 'baseline ' ... ) , and your methods . It is very difficult to know which ones are yours . Please make different lists and/or identify yours in the table 1. \u201d Thank you for this suggestion . We have identified which lines in Table 1 correspond to our methods . -- -- -- -- -- -- -- -- -- 8 . > \u201c What is the Ensemble method ? It appears that in the paper , you explain the 'HighRes-net + shiftNet ' method , but actually , the 'Ensemble ' is the better one , while not clearly described . I do n't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously . We can see how this is confusing . Our paper is about the HighRes-net + ShiftNet method indeed . For our Ensemble method , we trained two HighRes-net+shiftNet models , one with K=16 and one with K=32 frames . We averaged HighRes-Net_16 + HighRes-Net_32 outputs for our predictions on the final leaderboard . We have included these details in our revised manuscript ."}, "1": {"review_id": "HJxJ2h4tPr-1", "review_text": "The paper proposes a framework including recursive fusion to co-registration and registration loss to solve the problem that the super-resolution results and the high-resolution labels are not pixel aligned. Besides, the method is able to achieve good performance in the Proba-V Kelvin dataset. However, I have some concerns about this paper: 1) This paper lacks many references. Recently, many works focus on multi-frame super-resolution containing video super-resolution and stereo image super-resolution via deep learning. They are using multiple low-resolution image to construct high-resolution image. For example: Stereo super-resolution: Jeon, Daniel S., et al. \"Enhancing the spatial resolution of stereo images using a parallax prior.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018. Wang, Longguang, et al. \"Learning parallax attention for stereo image super-resolution.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2019. Video super-resolution: Tao, Xin, et al. \"Detail-revealing deep video super-resolution.\" *Proceedings of the IEEE International Conference on Computer Vision*. 2017. FRVSR: Sajjadi, Mehdi SM, Raviteja Vemulapalli, and Matthew Brown. \"Frame-recurrent video super-resolution.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018. FFCVSR: Yan, Bo, Chuming Lin, and Weimin Tan. \"Frame and Feature-Context Video Super-Resolution.\" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 33. 2019. EDVR: Wang, Xintao, et al. \"Edvr: Video restoration with enhanced deformable convolutional networks.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*. 2019. 2) Recursive fusion is aimed to fuse multiple low-resolution image information. Recently, more and more work utilize different methods to fuse multiple low-resolution image. For example, Tao et al proposes SPMC (Sub-pixel Motion Compensation) to align image, FRVSR uses unsupervised flow network that predicts optical flow to warp image, FFCVSR directly concatenate low-resolution image as the input of 2D convolutional network to fuse the information, and EDVR fuses multiple image features via utilizing deformable convolution. Thus, what is the advantage of recursive fusion compared to the above methods? This paper should discuss the difference between recursive fusion and the above methods. 3) Registration loss is important in this paper and it can solve the problem the output SR is not pixel-wise aligned to the HR ground truth. Registration loss utilizes ShiftNet that is adapted from HomographyNet. Thus, what is the difference between ShiftNet and HomographyNet? This paper should add some details about ShiftNet and Lanczos interpolation. 4) It is better to test more datasets and compare with more state-of-the-art methods. This paper only tests in a satellite image dataset. Some datasets can be considered such as VID4 dataset in video super-resolution.", "rating": "3: Weak Reject", "reply_text": "1. > \u201c This paper lacks many references . Recently , many works focus on MFSR containing video SR and stereo image SR via deep learning. \u201d Thank you for pointing out these references . We 've included them in our revision . We want to stress that our setting is different from video SR in several ways : - We learn to super-resolve sets and not sequences of low-res views . Video SR relies on motion estimation from a sequence of observations . Also , prediction at time t=T relies on predictions at t < T ( autoregressive approach ) . Whereas in our case , we predict a single image from an unordered set of low-res inputs . - In our setting , the low-res views are multi-temporal ( taken at different times ) from different revisits . Please see Paragraph 1 in our comment to Reviewer 3 : https : //openreview.net/forum ? id=HJxJ2h4tPr & noteId=B1l6sKmUoB Our work different from Stereo SR : - Multi-temporality ( see above ) - These images were taken at a nadir direction ( top-down view ) above different subpoints ( coordinates below satellite ) . Whereas Stereo SR assumes that both views focus on the same point , but from different angles . -- - 2. > \u201c Thus , what is the advantage of recursive fusion compared to the above methods [ Video SR papers : SPMC , FRVSR , FFCVSR , EDVR ] ? This paper should discuss the difference between recursive fusion and the above methods. \u201d SPMC , FRVSR , FFCVSR and EDVR are all video SR algorithms . They all assume the input to be a temporal sequence of frames . Motion or optical flow can be estimated to super-resolve the sequences of frames . ( see Part 1 of this comment above ) . In this work , we do not assume low-res inputs to be ordered in time . Our training input is a set of low-res views with unknown timestamps and our target output is a single image - not another sequence . More importantly , those Video SR methods were trained and tested on synthetically down-scaled data - we elaborate on this on Part 4 below . DeepSUM ( Molini et al . ) is another method that scored similarly to ours in the same ESA competition leaderboard with the same dataset . Their paper already showed competitive results compared to an architecture inspired by DUF ( `` Deep video SR network using dynamic upsampling filters without explicit motion compensation . `` CVPR , 2018 ) Thank for raising this important question . We 've included this in the Related Work discussion . -- - 3. > \u201c It is better to test more datasets and compare with more state-of-the-art methods . This paper only tests in a satellite image dataset . Some datasets can be considered such as VID4 dataset in video super-resolution. \u201d Many benchmarks exist for Super Resolution : VIDEO SR : - Vid4 : C. Liu and D. Sun . A bayesian approach to adaptive video super resolution . In CVPR , pages 209\u2013216 . IEEE , 2011 . - Vimeo 90K : Xue , Tianfan , et al . `` Video enhancement with task-oriented flow . '' International Journal of Computer Vision 127.8 ( 2019 ) : 1106-1125 . - Y10 : Sajjadi , Mehdi SM , Raviteja Vemulapalli , and Matthew Brown . `` Frame-recurrent video super-resolution . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2018.- REDS4 ( NTIRE 2019 ) : Nah , Seungjun , et al . `` Ntire 2019 challenge on video deblurring : Methods and results . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops . 2019.SISR : - Set 5 : M. Bevilacqua , A. Roumy , C. Guillemot , and M. L. AlberiMorel . Low-complexity single-image super-resolution based on nonnegative neighbor embedding . 2012.- Set 14 : R. Zeyde , M. Elad , and M. Protter . On single image scale-up using sparse-representations . In International Conference on Curves and Surfaces , pages 711\u2013730 . Springer , 2010 . STEREO : - FLICKR 1024 : Wang , Yingqian , et al . `` Flickr1024 : A Large-Scale Dataset for Stereo Image Super-Resolution . '' CVPR Workshops 2019 However , in all of these datasets , the low-res views are artificially down-scaled , and prior work has shown such methods do not generalize to real world low-res imagery . See also paragraph 4 of our comment to Reviewer3 : https : //openreview.net/forum ? id=HJxJ2h4tPr & noteId=ryxbd5DuiH \u201c The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling ( or in a few cases by blurring followed by down-sampling ) . We show that such methods fail to produce good results when applied to real-world low-resolution , low quality images \u201d - https : //arxiv.org/abs/1807.11458 . See also , Zero shot paper . Shocher , Assaf , Nadav Cohen , and Michal Irani . `` \u201c zero-shot \u201d super-resolution using deep internal learning . '' CVPR 2018 . We agree that benchmarking on more datasets and with more models would strengthen our paper . We are not aware of other publicly available datasets for MFSR with real low-res ( not artificially down-sampled ) images . By working on this dataset , we want to encourage the community to consider on real-world ( not synthetic ) images for super-resolution ."}, "2": {"review_id": "HJxJ2h4tPr-2", "review_text": "This paper presents a multi-frame super-resolution method applied to satellite imagery. It first estimates a reference image for the multiple input LR images by median filtering. Then it pairwise encodes the reference image and each of the multiple images in a recursive fashion then fuses the corresponding feature maps with residual blocks and bottleneck layers until only one feature maps for the entire multiple images obtained. In other words, LR images are fused into a single global encoding. Then, it applies a standard upsampling network to obtain the super-resolved image this image is fed into a network that estimates only the translational shift, and the shifted image with the estimated translation parameters finally resampled. A major concern is the estimation of a single translational motion for the SR image at the end of the network after all multiple images are already fused. The fusion strategy disregards the underlying spatially varying motion. This explicitly assumes the images are on a flat surface, which perhaps an acceptable assumption for high-orbit satellite imagery where the ground surface depth variances might be negligible. Still, this is a very critical limitation of the method. Besides, I am not convinced that pair-wise fusion can handle significant translational fusion as the filters have shared parameters. How a single convolutional layer accomplishes a global encoding and compensates for any translation between any LR image pair is neither articulated nor convincing discussion and evaluations are provided. Of course, such a problematic approach needs at least some kind of motion compensation, which may explain the need for the ShiftNet layer at the end. Nevertheless, this seems quite problematic. Even assuming the method only applies to satellite imagery, it lacks mechanisms to compensate/distinguish cloud coverage and atmospheric distortions. Characterization of satellite imagery noise models (Weibull, etc.) common in such imagery as a prior also completely disregarded. For these reasons, the proposed method fails to be considered as a comprehensive approach for multi-image super-resolution of satellite imagery. Novelty-wise, there is very little as all modules have been commonly used for SR tasks. ", "rating": "1: Reject", "reply_text": "Thank you for reviewing our work . Can you please clarify what you mean by the following sentence ? > `` The fusion strategy disregards the underlying spatially varying motion '' Are you suggesting that the fusion strategy disregards the relative translation between the low-res views ?"}}