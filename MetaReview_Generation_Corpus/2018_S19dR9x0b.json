{"year": "2018", "forum": "S19dR9x0b", "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks", "decision": "Accept (Poster)", "meta_review": "The reviewers unanimously agree that this paper is worth publication at ICLR. Please address the feedback of the reviewers and discuss exactly how the potential speed up rates are computed in the appendix. I speed up rates to be different for different devices.", "reviews": [{"review_id": "S19dR9x0b-0", "review_text": "I have read the comments and clarifications from the authors. They have added extra experiments, and clarified the speed-ups concern raised by others. I keep my original rating of the paper. --------------- ORIGINAL REVIEW: This paper introduces a multi-bit quantization method for recurrent neural networks, which is built on alternating the minimization formulated by Guo et al. 2017 by first fixing the \\alpha values and then finding the optimal binary codes b_i with a BST, to then estimate \\alpha with the refined approximation by Guo et al. 2017, iteratively. The observation that the optimal binary code can be computed with a BST is simple and elegant. The paper is easy to follow and the topic of reducing memory and speeding up computations for RNN and DNN is interesting and relevant to the community. The overall contribution on model quantization is based on existing methods, which makes the novelty of the paper suffer a bit. Said that, applying it to RNN is a convincing and a strong motivation. Also, in the paper it is shown how the matrix multiplications of the quantized model can be speeded up using 64 bits operation in CPU. This is, not only saves memory storage and usage, but also on runtime calculation using CPU, which is an important characteristic when there are limited computational resources. Results on language models show that the models with quantized weights with 3 bits obtain the same or even slightly better performance on the tested datasets with impressive speed-ups and memory savings. For completeness, it would be interesting, and I would strongly encourage to add a discussion or even an experiment using feedforward DNN with a simple dataset as MNIST, as most of previous work discussed in the paper report experiments on DNN that are feedforward. Would the speed-ups and memory savings obtained for RNN hold also for feedforward networks? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "As we are quantizing the weight and activation to reduce the most costly matrix multiplication to binary operation , it is of no difference for RNNs and feedforward networks when concerning the speed-ups and memory savings . Please refer to the replies to common issues for the experiments on MNIST ."}, {"review_id": "S19dR9x0b-1", "review_text": "Revision: The authors have addressed my concerns around the achievable speedup. I am increasing my score to 7. Original Review: The paper proposes a technique for quantizing neural network weight matrices by representing columns of weight matrices as linear combinations of binary (+1/-1) vectors. Given a weight vector, the paper proposes an alternating optimization procedure to estimate the set of k binary vectors and coefficients that best represent (in terms of MSE) the original vector. This yields a k-bit quantization. First, the coefficients/binary weights are initialized using a greedy procedure proposed in prior work. Then, the binary weights are updated using a clever binary search procedure, followed by updates to the coefficients. Experiments are conducted in an RNN context for some language modeling tasks. The paper is relatively easy to read, and the technique is clearly explained. The technique is as far as I can tell novel, and does seem to represent an improvement over existing approaches for similar multi-bit quantization strategies. I have a few questions/concerns. First, I am quite skeptical of many of the speedup calculations: These are rather delicate to do properly, and depend on the specific instructions available, SIMD widths, the number of ALUs present in a core, etc. All of these can easily shift numbers around by a factor of 2-8x. Without an implementation in hand, comparing against a well-optimized reference GEMM for full floating point, it's not clear how much faster this approach really would be in practice. Also, the online quantization of activations doesn't seem to be factored into the speedup calculations, and no benchmarks are provided demonstrating how fast the quantization is (unless I'm missing something). This is concerning since the claimed speedups aren't possible without the online quantization of actiations. It would have been nice to have more discussion of/comparison with other approaches capable of 2-4 bit quantization, such as some of the recent work on ternary quantization, product quantization approaches, or at least scalar (per-dimension) k-means (non-uniform quantization). Finally, the experiments are reasonable, but the choice of RNN setting isn't clear to me. It would have been easier to compare to prior work if the experiments also included some standard image classification tasks (e.g., CIFAR10). Overall though, I think the paper does just enough to warrant acceptance.", "rating": "7: Good paper, accept", "reply_text": "Please refer to the replies to common issues on speedup . Ternary quantization [ 1 ] is an extension to the binary quantization with one more feasible state , 0 . It does quantization by tackling $ \\min_ { \\alpha , t } \\|w \u2013 \\alpha * t \\|_2^2 $ with $ t $ restricted to { -1,0 , +1 } . However , currently there is no efficient algorithm to solve this problem . Instead , Li et al . [ 1 ] suggested to empirically set the entries $ w_i $ with absolute scales less than $ 0.7/n \\|w\\|_1 $ to 0 ( n is the number of entries ) and binarize the left entries with a closed-form solution as discussed in our paper . In fact , ternary quantization is a special case of the 2-bit quantization in our paper , i.e. , $ \\min_ { \\alpha_1 , \\alpha_2 , b_1 , b_2 } \\|w \u2013 \\alpha_1 * b_1 - \\alpha_2 * b_2 \\|_2^2 $ with an additional constraint that $ \\alpha_1 = \\alpha_2 $ . Thus our alternating multi-bit quantization method can easily extend to solve it . In parallel to our binarized quantization , vector quantization is applied to compress the weights for feedforward neural networks [ 2 ] [ 3 ] . Different from ours where all weights are directly constraint to { -1 , +1 } , vector quantization learns a small codebook by applying k-means clustering to the weights or conducting product quantization . The weights are then reconstructed by indexing the codebook . It has been shown that by such a technique , the number of parameters can be reduced by an order of magnitude with limited accuracy loss [ 2 ] . It is possible that our mutli-bit quantized binary weight can be further compressed by using the product quantization . However , this is out-of-the scope of this paper and we leave it for future work . We will incorporate the above discussions in the revised version . As for the experiment on image classification tasks , we have done on MNIST ( see the replies to common issues ) . We will also report the results on CIFAR10 if time permits . [ 1 ] Li , Fengfu et al.Ternary weight networks , arXiv:1605.04711 . [ 2 ] Gong , Yunchao et al.Compressing Deep Convolutional Networks using Vector Quantization , arXiv:1412.6115 [ 3 ] Han , Song et al.Deep compression : Compressing deep neural networks with pruning , trained quantization and huffman coding , ICLR 15 ."}, {"review_id": "S19dR9x0b-2", "review_text": " Summary of the paper ------------------------------- The authors propose a new way to perform multi-bit quantization based on greedy approximation and binary search tree for RNNs. They first show how this method, applied to the parameters only, performs on pre-trained networks and show great performances compared to other existing techniques on PTB. Then they present results with the method applied to both parameters and activations during training on 3 NLP datasets, showing again great performances compared to existing technique. Clarity, Significance and Correctness -------------------------------------------------- Clarity: The paper is clearly written. Significance: I'm not familiar with the quantization literature, so I'll let more knowledgeable reviewers evaluate this point. Correctness: The paper is technically correct. Questions -------------- 1. It would be nice to have those memory and speed gains for training as well. Is it possible to use those quantization methods to train networks from scratch, i.e. without using a pre-train model? Pros ------ 1. The paper defines clear goals and contributions. 2. Existing methods (and their differences) are clearly and concisely presented. 3. The proposed method is well explained. 4. The experimental setup shows clear results compared to the non-quantized baselines and other quantization techniques. Cons ------- 1. It would be nice to have another experiment not based on text (speech recognition / synthesis, audio, biological signals, ...) to see how it generalizes to other kind of data (although I can't see why it wouldn't). Typos -------- 1. abstract: \"gate recurrent unit\" -> \"gated recurrent unit\" 2. equation (6): remove parenthesis in c_(t-1) 3. section 4, paragraph 1: \"For the weight matrices, instead of on the whole, we quantize them row by row.\" -> \"We don't apply quantization on the full matrices but rather row by row.\" 4. section 4, paragraph 2: Which W matrix is it? W_h? (2x) Note ------- Since I'm not familiar with the quantization literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.", "rating": "7: Good paper, accept", "reply_text": "The memory costs during training can mainly be divided into two parts : the weights and the activations for backpropagation . For the weights , as a full precision should be maintained ( See Eq . ( 7 ) ) , they can not be reduced . For the activations , as it is enough to maintain a quantized version for backpropagation , we can have memory gains in this part . The time costs during training can also be divided into two parts : the forward and backward pass . During the forward pass , as the most costly full precision multiplications are transformed into the much faster binarized multiplications , we can have speed-ups in this part . During the backward , as we need to compute a full precision gradient , no speed-ups can be achieved . We conduct experiments of training from scratch in the PTB dataset and observe that it would result in 1~2 PPW worse than using a pre-trained model . But when combining with the continuation technique , that is , setting the initial number of bit to be large , then gradually decreasing it during training , it will result in almost no loss or even slightly better on accuracy . In fact , using a pre-trained model can also be regarded as such continuation technique , but coarser and simpler . In section 4 , paragraph 2 , W do means W_h . We will address other small typos in the revised version . We are also conducting experiments on non-text data and will report the results if time permits ."}], "0": {"review_id": "S19dR9x0b-0", "review_text": "I have read the comments and clarifications from the authors. They have added extra experiments, and clarified the speed-ups concern raised by others. I keep my original rating of the paper. --------------- ORIGINAL REVIEW: This paper introduces a multi-bit quantization method for recurrent neural networks, which is built on alternating the minimization formulated by Guo et al. 2017 by first fixing the \\alpha values and then finding the optimal binary codes b_i with a BST, to then estimate \\alpha with the refined approximation by Guo et al. 2017, iteratively. The observation that the optimal binary code can be computed with a BST is simple and elegant. The paper is easy to follow and the topic of reducing memory and speeding up computations for RNN and DNN is interesting and relevant to the community. The overall contribution on model quantization is based on existing methods, which makes the novelty of the paper suffer a bit. Said that, applying it to RNN is a convincing and a strong motivation. Also, in the paper it is shown how the matrix multiplications of the quantized model can be speeded up using 64 bits operation in CPU. This is, not only saves memory storage and usage, but also on runtime calculation using CPU, which is an important characteristic when there are limited computational resources. Results on language models show that the models with quantized weights with 3 bits obtain the same or even slightly better performance on the tested datasets with impressive speed-ups and memory savings. For completeness, it would be interesting, and I would strongly encourage to add a discussion or even an experiment using feedforward DNN with a simple dataset as MNIST, as most of previous work discussed in the paper report experiments on DNN that are feedforward. Would the speed-ups and memory savings obtained for RNN hold also for feedforward networks? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "As we are quantizing the weight and activation to reduce the most costly matrix multiplication to binary operation , it is of no difference for RNNs and feedforward networks when concerning the speed-ups and memory savings . Please refer to the replies to common issues for the experiments on MNIST ."}, "1": {"review_id": "S19dR9x0b-1", "review_text": "Revision: The authors have addressed my concerns around the achievable speedup. I am increasing my score to 7. Original Review: The paper proposes a technique for quantizing neural network weight matrices by representing columns of weight matrices as linear combinations of binary (+1/-1) vectors. Given a weight vector, the paper proposes an alternating optimization procedure to estimate the set of k binary vectors and coefficients that best represent (in terms of MSE) the original vector. This yields a k-bit quantization. First, the coefficients/binary weights are initialized using a greedy procedure proposed in prior work. Then, the binary weights are updated using a clever binary search procedure, followed by updates to the coefficients. Experiments are conducted in an RNN context for some language modeling tasks. The paper is relatively easy to read, and the technique is clearly explained. The technique is as far as I can tell novel, and does seem to represent an improvement over existing approaches for similar multi-bit quantization strategies. I have a few questions/concerns. First, I am quite skeptical of many of the speedup calculations: These are rather delicate to do properly, and depend on the specific instructions available, SIMD widths, the number of ALUs present in a core, etc. All of these can easily shift numbers around by a factor of 2-8x. Without an implementation in hand, comparing against a well-optimized reference GEMM for full floating point, it's not clear how much faster this approach really would be in practice. Also, the online quantization of activations doesn't seem to be factored into the speedup calculations, and no benchmarks are provided demonstrating how fast the quantization is (unless I'm missing something). This is concerning since the claimed speedups aren't possible without the online quantization of actiations. It would have been nice to have more discussion of/comparison with other approaches capable of 2-4 bit quantization, such as some of the recent work on ternary quantization, product quantization approaches, or at least scalar (per-dimension) k-means (non-uniform quantization). Finally, the experiments are reasonable, but the choice of RNN setting isn't clear to me. It would have been easier to compare to prior work if the experiments also included some standard image classification tasks (e.g., CIFAR10). Overall though, I think the paper does just enough to warrant acceptance.", "rating": "7: Good paper, accept", "reply_text": "Please refer to the replies to common issues on speedup . Ternary quantization [ 1 ] is an extension to the binary quantization with one more feasible state , 0 . It does quantization by tackling $ \\min_ { \\alpha , t } \\|w \u2013 \\alpha * t \\|_2^2 $ with $ t $ restricted to { -1,0 , +1 } . However , currently there is no efficient algorithm to solve this problem . Instead , Li et al . [ 1 ] suggested to empirically set the entries $ w_i $ with absolute scales less than $ 0.7/n \\|w\\|_1 $ to 0 ( n is the number of entries ) and binarize the left entries with a closed-form solution as discussed in our paper . In fact , ternary quantization is a special case of the 2-bit quantization in our paper , i.e. , $ \\min_ { \\alpha_1 , \\alpha_2 , b_1 , b_2 } \\|w \u2013 \\alpha_1 * b_1 - \\alpha_2 * b_2 \\|_2^2 $ with an additional constraint that $ \\alpha_1 = \\alpha_2 $ . Thus our alternating multi-bit quantization method can easily extend to solve it . In parallel to our binarized quantization , vector quantization is applied to compress the weights for feedforward neural networks [ 2 ] [ 3 ] . Different from ours where all weights are directly constraint to { -1 , +1 } , vector quantization learns a small codebook by applying k-means clustering to the weights or conducting product quantization . The weights are then reconstructed by indexing the codebook . It has been shown that by such a technique , the number of parameters can be reduced by an order of magnitude with limited accuracy loss [ 2 ] . It is possible that our mutli-bit quantized binary weight can be further compressed by using the product quantization . However , this is out-of-the scope of this paper and we leave it for future work . We will incorporate the above discussions in the revised version . As for the experiment on image classification tasks , we have done on MNIST ( see the replies to common issues ) . We will also report the results on CIFAR10 if time permits . [ 1 ] Li , Fengfu et al.Ternary weight networks , arXiv:1605.04711 . [ 2 ] Gong , Yunchao et al.Compressing Deep Convolutional Networks using Vector Quantization , arXiv:1412.6115 [ 3 ] Han , Song et al.Deep compression : Compressing deep neural networks with pruning , trained quantization and huffman coding , ICLR 15 ."}, "2": {"review_id": "S19dR9x0b-2", "review_text": " Summary of the paper ------------------------------- The authors propose a new way to perform multi-bit quantization based on greedy approximation and binary search tree for RNNs. They first show how this method, applied to the parameters only, performs on pre-trained networks and show great performances compared to other existing techniques on PTB. Then they present results with the method applied to both parameters and activations during training on 3 NLP datasets, showing again great performances compared to existing technique. Clarity, Significance and Correctness -------------------------------------------------- Clarity: The paper is clearly written. Significance: I'm not familiar with the quantization literature, so I'll let more knowledgeable reviewers evaluate this point. Correctness: The paper is technically correct. Questions -------------- 1. It would be nice to have those memory and speed gains for training as well. Is it possible to use those quantization methods to train networks from scratch, i.e. without using a pre-train model? Pros ------ 1. The paper defines clear goals and contributions. 2. Existing methods (and their differences) are clearly and concisely presented. 3. The proposed method is well explained. 4. The experimental setup shows clear results compared to the non-quantized baselines and other quantization techniques. Cons ------- 1. It would be nice to have another experiment not based on text (speech recognition / synthesis, audio, biological signals, ...) to see how it generalizes to other kind of data (although I can't see why it wouldn't). Typos -------- 1. abstract: \"gate recurrent unit\" -> \"gated recurrent unit\" 2. equation (6): remove parenthesis in c_(t-1) 3. section 4, paragraph 1: \"For the weight matrices, instead of on the whole, we quantize them row by row.\" -> \"We don't apply quantization on the full matrices but rather row by row.\" 4. section 4, paragraph 2: Which W matrix is it? W_h? (2x) Note ------- Since I'm not familiar with the quantization literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.", "rating": "7: Good paper, accept", "reply_text": "The memory costs during training can mainly be divided into two parts : the weights and the activations for backpropagation . For the weights , as a full precision should be maintained ( See Eq . ( 7 ) ) , they can not be reduced . For the activations , as it is enough to maintain a quantized version for backpropagation , we can have memory gains in this part . The time costs during training can also be divided into two parts : the forward and backward pass . During the forward pass , as the most costly full precision multiplications are transformed into the much faster binarized multiplications , we can have speed-ups in this part . During the backward , as we need to compute a full precision gradient , no speed-ups can be achieved . We conduct experiments of training from scratch in the PTB dataset and observe that it would result in 1~2 PPW worse than using a pre-trained model . But when combining with the continuation technique , that is , setting the initial number of bit to be large , then gradually decreasing it during training , it will result in almost no loss or even slightly better on accuracy . In fact , using a pre-trained model can also be regarded as such continuation technique , but coarser and simpler . In section 4 , paragraph 2 , W do means W_h . We will address other small typos in the revised version . We are also conducting experiments on non-text data and will report the results if time permits ."}}