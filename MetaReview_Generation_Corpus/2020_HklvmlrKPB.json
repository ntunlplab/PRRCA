{"year": "2020", "forum": "HklvmlrKPB", "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "decision": "Reject", "meta_review": "The paper scores low on novelty. The experiments and model analysis are not very strong.", "reviews": [{"review_id": "HklvmlrKPB-0", "review_text": " Summary The paper proposes to combine the video modeling approaches based on autoregressive flows (e.g. Kumar\u201919) with amortized variational inference (e.g. Denton\u201918), wherein an autoregressive latent variable model optimized with variational inference is extended with an autoregressive flow that further transforms the output of the latent variable model while allowing to compute exact conditional probability. This is motivated with a physical intuition, where a dynamics model can benefit from decorrelating the inputs, and it is demonstrated that layers of autoregressive flows can represent derivatives of the original signal. In a proof-of-concept experiment, it is shown that using a layer of autoregressive flow improves NLL of a latent variable model. Decision The paper presents an interesting method and tackles an important problem. At the same time, the properties of the proposed method are not well exposed and the experimental evaluation is incomplete. Moreover, the motivation of the paper is confusingly disconnected from the proposed model. I rate this paper as borderline, but am hopeful that some of the issues will be clarified during the discussion period. Pros - The paper is well-motivated and tackles a significant problem. - The proposed method is novel. - The paper is well-written. Cons - The experimental evaluation is incomplete and does not expose the properties of the method fully. Comparisons to prior art are missing. (see below) - The motivation is disconnected from the proposed model. The introduction of the paper motivates a model that hierarchically decorrelates a sequence of frames to arrive at a fully factorized model, which is later motivated with a physical example. However, the method proposed in the paper is instead a single layer of autoregressive flow on top of a powerful latent variable model! This is expressed in the title, but only glossed over in the abstract and introduction. The writing has to be updated to coherently focus on the contribution of the paper. Questions (ordered by decreasing importance) 1. In table 1, quantitative results are reported for the introduced methods. It is shown that introducing autoregressive flows achieves better likelihood and better generalization. However, quantitative comparisons with published methods that were evaluated on these datasets are missing, such as Denton\u201918 and Kumar\u201919. A quick calculation shows that Kumar et al. achieves a log-likelihood of -0.43 in Table 1 when converted to this paper\u2019s metric, although it is possible my conversion is incorrect. Is the presented model competitive with previously published results? 2. No qualitative generation results are presented. Since the model achieves a high likelihood it is likely to do well on one-frame prediction, and possibly would even work on autoregressive multi-step prediction. Is the model capable of generation of diverse and plausible video? 3. The paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling. However the proposed approach in fact produces latents that are correlated in time! Since the prior over latent variables is conditioned on past frames, the model can in fact learn a correlated representation and still achieve optimal likelihood. Moreover, the position of both the digit and the robot arm could be seen in what should be the decorrelated image in Fig 4. Is there solid quantitative (or even qualitative) evidence that the model learns a \u2018more decorrelated\u2019 representation beyond the fact that it copies the background and that the likelihood improves? The evaluation in this paper does not convince me that the model learns a temporally decorrelated representation. 4. Were modern techniques beyond affine flows considered, such as from Kingma\u201918, Kumar\u201919? Two layers of affine flows are likely insufficient to model the complexity of these data, which makes the comparison to the purely flow-based models somewhat unfair. 5. It is stated that the paper is \u201cthe first to demonstrate flows across time steps for video data\u201d, however, the related work by Kumar et al. proposes a somewhat similar model in which conditional flows are used to model video data. Do Kumar et al. not \u201cdemonstrate flows across time steps\u201d? Minor comments 1. Eq (10) and (12) seem to be inconsistent. Perhaps x_t = x_t-1 + u_t-1 was meant in eq (10)? 2. Line before eq(14): it not true that u_t-1 = x_t-1 - x_t-2. It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N(x_t; x_t-1 + u_t-1, Sigma). It is possible that eq(14) is still correct as the variance of Gaussians is additive. 3. The following work uses autoregressive flows for modeling temporal dynamics and should be cited: Rhinehart\u201918,19 Rhinehart et al, Deep Imitative Models for Flexible Inference, Planning, and Control Rhinehart et al, PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings --------------------- Update 11.19 ----------------------- The newly provided experiments support some of the claims of the paper. In particular, I appreciate the plot showing that the proposed method successfully learns a more decorrelated representation over time, and the provided qualitative samples from the model. The authors also clarified my questions about motivation. At the same time, the proposed method is not shown to compare well to state-of-the-art approaches. I am leaning towards accepting the paper, but I believe the method would have a much larger impact if its properties were more fully exposed. == comparison with Denton&Fergus'18 (SVG) == When trained with beta=1, as the authors suggest for comparison, this method is known to perform poorly. There are two possible ways of alleviating this: 1) to train with the modified objective as in the paper but evaluate the true lower bound on the likelihood, or 2) interpret the beta as the fixed variance of the decoder distribution. Given the results the authors have provided, I believe the latter option will lead to SVG outperforming the proposed approach. == Correlation plot == Thanks for performing this experiment! While measuring correlation only captures linear dependencies, which is likely mostly the background image, this plot shows that the model indeed learns to (linearly) decorrelate the frames in the sequence. == Samples == Thanks for providing samples from the model! While the performance on BAIR is not quite convincing, the MNIST samples look very good. = Kumar et al. comparison == The author's response convinces me that the proposed model is significantly different from Kumar et al. in scope, as Kumar et al simply use a per-frame normalizing flow encoder coupled with a sequential prior. == eqs. 10, 12 == The authors' response cleared my confusion, the equations are correct.", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments ! Here , we will attempt to address additional specific points : \u201c The paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling . However the proposed approach in fact produces latents that are correlated in time ! \u2026 Is there solid quantitative ( or even qualitative ) evidence that the model learns a \u2018 more decorrelated \u2019 representation \u201d It should be noted that while these flows have the capability of removing temporal correlations , they may not be able to remove all temporal dependencies . Thus , it can still be beneficial to model these dependencies in the base distribution of the flow . The motivation is not that we want to remove all temporal dependencies , but rather that we would like to remove as much as possible to simplify modeling for the sequential latent variable model . Based on your suggestion , we have provided a quantitative confirmation that the result of the flow is less temporally correlated than the input . \u201c Were modern techniques beyond affine flows considered , such as from Kingma \u2019 18 , Kumar \u2019 19 ? Two layers of affine flows are likely insufficient to model the complexity of these data , which makes the comparison to the purely flow-based models somewhat unfair. \u201d It is important to note that we are applying flows across time steps , rather than within a time step . If we were to apply a method like GLOW ( which is affine ) in an analogous way , this would involve applying the flow on half the time steps as a function of the other half of the steps . Methods like VideoFlow apply flows within time steps . While this may further improve performance by removing spatial correlations , this is not the motivation of our work . Many flows are part of the general family of affine flows ( e.g.NICE , RealNVP , IAF , MAF , GLOW ) , so we felt this was an important place to start in developing this technique . We included comparisons with standalone flow-based models to demonstrate that these models work well as generative models on their own . Note that a single affine autoregressive flow is exactly equivalent to an autoregressive model , which can perform quite well in practice . \u201c Do Kumar et al.not \u201c demonstrate flows across time steps \u201d ? \u201d While Kumar et al.do apply flows within a sequential context , we mean to distinguish between applying flows within a time step vs. across time steps , as in our work . Kumar et al.use non-flow-based models to model temporal dependencies . Other works , such as van den Oord et al.with audio data , do use flows across time steps , as we do here . \u201c Eq ( 10 ) and ( 12 ) seem to be inconsistent . Perhaps x_t = x_t-1 + u_t-1 was meant in eq ( 10 ) ? \u201d We understand the point of confusion , however , Eqs . 10 and 12 are consistent . In Eq.10 , x_t = x_t-1 + u_t gives the exact value of x_t , but in Eq.12 , x_t-1 + u_t-1 gives the mean of the Gaussian distribution over x_t . This can be seen by plugging the random variable for u_t , i.e.Eq.13 , into Eq.10. \u201c Line before eq ( 14 ) : it not true that u_t-1 = x_t-1 - x_t-2 . It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N ( x_t ; x_t-1 + u_t-1 , Sigma ) . It is possible that eq ( 14 ) is still correct as the variance of Gaussians is additive. \u201d This follows directly from the definition of u . To be clear , x and u are simply different ways of expressing the same randomness , subject to different offset values . This is because the transform between u_t and x_t is deterministic . All of the stochasticity originates from w_t . \u201c The following work uses autoregressive flows for modeling temporal dynamics and should be cited : Rhinehart \u2019 18,19 \u201d Thank you for these references . We have included them in the updated draft ."}, {"review_id": "HklvmlrKPB-1", "review_text": "Summary: The paper discusses ways to use autoregressive flows in sequence modelling. Two main variants are considered: (a) An affine autoregressive flow directly modelling the data. (b) An affine autoregressive flow whose base distribution is a sequential VAE; equivalently, a sequential VAE whose decoder is an affine autoregressive flow. Pros: The paper is very well written and crystal clear. I particularly appreciated the motivating example that shows how each layer of an affine autoregressive flow reduces the order of a linear dynamical system by 1, and the connections with modelling temporal changes and moving reference frames. The methods are technically correct and well-motivated. The experiments are done well. Overall, the paper scores high on writing and technical quality. Cons: In my opinion, the paper scores low on novelty and original contribution. In general, it's not clear to me what the claimed contribution is. More specifically: Is the claimed contribution new methodology for modelling sequences? In my opinion, using flows as VAE decoders, or adding latent variables to a flow model and training it variationally, are standard applications of existing techniques and I wouldn't consider them particularly novel. Is the claimed contribution improved modelling performance? The main results are that (a) replacing Gaussian decoders with autoregressive flows improves performance, and (b) adding latent variables to the base distribution of an affine autoregressive flow also improves performance. Both of these results are exactly what one would expect from our experience with these methods. Other than that, the paper doesn't present any results that indicate the particular models used enable us to do things we couldn't do before, or improve against the state of the art in sequence modelling. Is the claimed contribution useful representations? The motivation for using the flow in this particular way as a VAE decoder is that the flow will model low-level correlations whereas the latent variables will capture high-level dynamics. However, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way, Decision: Even though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution. Suggestions for improvement: My main suggestion to the authors is to keep up the good work, but also reflect on what the specific contribution of the paper is, and try to make a stronger case for it. Some minor suggestions/corrections follow: Eq. (8): As written, the expression makes little sense as \\sigma is a vector. I understand that there is supposed to be a sum over the elements of log\\sigma, so I'd suggest expressing that more clearly. Eq. (9): It seems to me that the last Jacobian is upside down. In general, it would be good to be more thorough on how this paper is similar to related work and how it differs. There is also this related work which may be good to discuss: Latent Normalizing Flows for Discrete Sequences, https://arxiv.org/abs/1901.10548 In the particle analogy of the motivating example of section 3.1, it would be good to say explicitly that x is the position, u is the velocity and w is the force, to make the example even more intuitive. The paper only considers affine autoregressive flows, but there has been a lot of recent work on non-affine autoregressive flows that are more expressive, for example: Neural Autoregressive Flows, https://arxiv.org/abs/1804.00779 Sum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325 Neural Spline Flows, https://arxiv.org/abs/1906.04032 Such flows could improve the experimental results of the paper. At the very least, it would be good to discuss them as more flexible alternatives. In section 3.2, a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions (e.g. pixels) of y_t. In the experimental section, it would be good to describe on a high level what the architecture of the VAE is, especially the architecture of the prior and the encoder, and the types of distributions used there (e.g. diagonal Gaussians or otherwise). It would be good to show samples from the models in the experimental results.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments ! Here , we will attempt to address additional specific points : \u201c Is the claimed contribution new methodology for modeling sequences ? In my opinion , using flows as VAE decoders , or adding latent variables to a flow model and training it variationally , are standard applications of existing techniques and I would n't consider them particularly novel. \u201d As mentioned , flows and VAE models have been combined in various ways ( though , in our opinion , this is still under-explored ) , and we do not claim to introduce this combination . While affine autoregressive flows are a popular class of flow-based models , to the best of our knowledge , the application of these flows across time steps for the purposes of simplifying video modeling is novel . Specifically , our main contribution is identifying flows as a useful technique for pre-processing sequences to simplify downstream modeling . \u201c Is the claimed contribution improved modeling performance ? The main results are that ( a ) replacing Gaussian decoders with autoregressive flows improves performance , and ( b ) adding latent variables to the base distribution of an affine autoregressive flow also improves performance . Both of these results are exactly what one would expect from our experience with these methods. \u201d Improved modeling performance is one of the results of our method . It does seem that including more flexible model components should obviously improve performance . However , from the perspective from a sequential latent variable model , it is unclear where to incorporate dynamics . For example , one could include more latent variables or recurrent networks at various stages . We specifically propose using autoregressive flows as a type of \u2018 pre-processing \u2019 stage , resulting in a new sequence with dynamics that are hopefully easier to model . In our experiments , we attempt to control for the complexity of each model . \u201c Is the claimed contribution useful representations ? \u201d This is not something that we investigated in this paper , although we intend to investigate this more thoroughly . \u201c Eq . ( 8 ) : As written , the expression makes little sense as \\sigma is a vector . I understand that there is supposed to be a sum over the elements of log\\sigma , so I 'd suggest expressing that more clearly. \u201d You are correct . This has been updated in the submission . Thank you ! \u201c Eq . ( 9 ) : It seems to me that the last Jacobian is upside down. \u201d Indeed . Thanks again ! \u201c In the particle analogy of the motivating example of section 3.1 , it would be good to say explicitly that x is the position , u is the velocity and w is the force , to make the example even more intuitive. \u201d We have stated this in the updated submission . \u201c The paper only considers affine autoregressive flows , but there has been a lot of recent work on non-affine autoregressive flows that are more expressive\u2026At the very least , it would be good to discuss them as more flexible alternatives. \u201d We have included a discussion of these non-affine flows in the updated submission . We chose affine flows for their relative simplicity while still yielding reasonable performance . \u201c In section 3.2 , a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions ( e.g.pixels ) of y_t. \u201d We have stated this limitation more specifically . However , for the purposes of removing correlations across time , rather than space , they are useful . \u201c In the experimental section , it would be good to describe on a high level what the architecture of the VAE is , especially the architecture of the prior and the encoder , and the types of distributions used there ( e.g.diagonal Gaussians or otherwise ) . \u201d We have included a more thorough discussion of the model architectures , as well as diagrams in Appendix B ."}, {"review_id": "HklvmlrKPB-2", "review_text": "This paper proposes to model temporal sequences using autoregressive flows across time steps, that allow to model more explicitly temporal changes of the input, i.e. how the input x_t has changed w.r.t x_{<t}. As also stated by the authors, this is a generalization of other work that instead of modelling the input at each time step, models temporal differences between consecutive time steps. To the best of my knowledge, this is the first work that models normalizing flows in the sequential setting in this way (to be fair however, the idea is fairly obvious). Overall I found the paper interesting, and I think it is well written, so I am leaning towards acceptance. My biggest concern in the paper is the experimental section that could be improved in several ways: - the paper misses broader perfoemance comparisons against other state of the art models, in particular videoflow which is quite related to the models introduced in this paper. - how does the model perform on longer sequences, e.g. for long term generation? I would expect that such a direct dependence of the temporal dynamics on the frames of the video may make it hard for the model to coherently predict future latent states for many time steps. - What would happen if we used the same trick of modelling the conditional likelihood in this way in other SOTA models? - what are the computational requirements of the models presented in this paper? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments ! Here , we will attempt to address additional specific points : \u201c the paper misses broader performance comparisons against other state of the art models , in particular videoflow which is quite related to the models introduced in this paper. \u201d As discussed in our common response , our goal was not to propose a specific video-modeling architecture , but rather to propose a technique for improving sequence modeling . VideoFlow applies flows within each time step , unlike our proposed technique , which operates across time steps . VideoFlow is also significantly larger than the models that we investigated , consisting of 3 levels of latent variables , each with 24 steps of flow , and each flow containing 5 residual blocks . In contrast , the models in our experiments consist of just one or two flows , with each component of our models parameterized with relatively simple convolutional or recurrent networks . As stated in our common response , our quantitative results are on-par with log-likelihood estimates for previous works , like SVG ( Denton & Fergus , 2018 ) . \u201c What would happen if we used the same trick of modeling the conditional likelihood in this way in other SOTA models ? \u201d We chose a representative sequential latent variable model for our experiments . However , we suspect this technique will apply broadly to many sequence modeling settings . Indeed , as we noted in our submission , VideoFlow models differences in variables , which we discuss as a special case of our technique . Before the camera-ready deadline , we intend to conduct additional experiments applying our technique to some of these previously proposed models . \u201c what are the computational requirements of the models presented in this paper ? \u201d Autoregressive flows , in the sequential context , add only a constant computational cost to each time step , requiring only a single forward pass for evaluation and generation ."}], "0": {"review_id": "HklvmlrKPB-0", "review_text": " Summary The paper proposes to combine the video modeling approaches based on autoregressive flows (e.g. Kumar\u201919) with amortized variational inference (e.g. Denton\u201918), wherein an autoregressive latent variable model optimized with variational inference is extended with an autoregressive flow that further transforms the output of the latent variable model while allowing to compute exact conditional probability. This is motivated with a physical intuition, where a dynamics model can benefit from decorrelating the inputs, and it is demonstrated that layers of autoregressive flows can represent derivatives of the original signal. In a proof-of-concept experiment, it is shown that using a layer of autoregressive flow improves NLL of a latent variable model. Decision The paper presents an interesting method and tackles an important problem. At the same time, the properties of the proposed method are not well exposed and the experimental evaluation is incomplete. Moreover, the motivation of the paper is confusingly disconnected from the proposed model. I rate this paper as borderline, but am hopeful that some of the issues will be clarified during the discussion period. Pros - The paper is well-motivated and tackles a significant problem. - The proposed method is novel. - The paper is well-written. Cons - The experimental evaluation is incomplete and does not expose the properties of the method fully. Comparisons to prior art are missing. (see below) - The motivation is disconnected from the proposed model. The introduction of the paper motivates a model that hierarchically decorrelates a sequence of frames to arrive at a fully factorized model, which is later motivated with a physical example. However, the method proposed in the paper is instead a single layer of autoregressive flow on top of a powerful latent variable model! This is expressed in the title, but only glossed over in the abstract and introduction. The writing has to be updated to coherently focus on the contribution of the paper. Questions (ordered by decreasing importance) 1. In table 1, quantitative results are reported for the introduced methods. It is shown that introducing autoregressive flows achieves better likelihood and better generalization. However, quantitative comparisons with published methods that were evaluated on these datasets are missing, such as Denton\u201918 and Kumar\u201919. A quick calculation shows that Kumar et al. achieves a log-likelihood of -0.43 in Table 1 when converted to this paper\u2019s metric, although it is possible my conversion is incorrect. Is the presented model competitive with previously published results? 2. No qualitative generation results are presented. Since the model achieves a high likelihood it is likely to do well on one-frame prediction, and possibly would even work on autoregressive multi-step prediction. Is the model capable of generation of diverse and plausible video? 3. The paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling. However the proposed approach in fact produces latents that are correlated in time! Since the prior over latent variables is conditioned on past frames, the model can in fact learn a correlated representation and still achieve optimal likelihood. Moreover, the position of both the digit and the robot arm could be seen in what should be the decorrelated image in Fig 4. Is there solid quantitative (or even qualitative) evidence that the model learns a \u2018more decorrelated\u2019 representation beyond the fact that it copies the background and that the likelihood improves? The evaluation in this paper does not convince me that the model learns a temporally decorrelated representation. 4. Were modern techniques beyond affine flows considered, such as from Kingma\u201918, Kumar\u201919? Two layers of affine flows are likely insufficient to model the complexity of these data, which makes the comparison to the purely flow-based models somewhat unfair. 5. It is stated that the paper is \u201cthe first to demonstrate flows across time steps for video data\u201d, however, the related work by Kumar et al. proposes a somewhat similar model in which conditional flows are used to model video data. Do Kumar et al. not \u201cdemonstrate flows across time steps\u201d? Minor comments 1. Eq (10) and (12) seem to be inconsistent. Perhaps x_t = x_t-1 + u_t-1 was meant in eq (10)? 2. Line before eq(14): it not true that u_t-1 = x_t-1 - x_t-2. It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N(x_t; x_t-1 + u_t-1, Sigma). It is possible that eq(14) is still correct as the variance of Gaussians is additive. 3. The following work uses autoregressive flows for modeling temporal dynamics and should be cited: Rhinehart\u201918,19 Rhinehart et al, Deep Imitative Models for Flexible Inference, Planning, and Control Rhinehart et al, PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings --------------------- Update 11.19 ----------------------- The newly provided experiments support some of the claims of the paper. In particular, I appreciate the plot showing that the proposed method successfully learns a more decorrelated representation over time, and the provided qualitative samples from the model. The authors also clarified my questions about motivation. At the same time, the proposed method is not shown to compare well to state-of-the-art approaches. I am leaning towards accepting the paper, but I believe the method would have a much larger impact if its properties were more fully exposed. == comparison with Denton&Fergus'18 (SVG) == When trained with beta=1, as the authors suggest for comparison, this method is known to perform poorly. There are two possible ways of alleviating this: 1) to train with the modified objective as in the paper but evaluate the true lower bound on the likelihood, or 2) interpret the beta as the fixed variance of the decoder distribution. Given the results the authors have provided, I believe the latter option will lead to SVG outperforming the proposed approach. == Correlation plot == Thanks for performing this experiment! While measuring correlation only captures linear dependencies, which is likely mostly the background image, this plot shows that the model indeed learns to (linearly) decorrelate the frames in the sequence. == Samples == Thanks for providing samples from the model! While the performance on BAIR is not quite convincing, the MNIST samples look very good. = Kumar et al. comparison == The author's response convinces me that the proposed model is significantly different from Kumar et al. in scope, as Kumar et al simply use a per-frame normalizing flow encoder coupled with a sequential prior. == eqs. 10, 12 == The authors' response cleared my confusion, the equations are correct.", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments ! Here , we will attempt to address additional specific points : \u201c The paper has a lengthy section 3.1 that convincingly explains that decorrelating latent variables in time is important for sequence modeling . However the proposed approach in fact produces latents that are correlated in time ! \u2026 Is there solid quantitative ( or even qualitative ) evidence that the model learns a \u2018 more decorrelated \u2019 representation \u201d It should be noted that while these flows have the capability of removing temporal correlations , they may not be able to remove all temporal dependencies . Thus , it can still be beneficial to model these dependencies in the base distribution of the flow . The motivation is not that we want to remove all temporal dependencies , but rather that we would like to remove as much as possible to simplify modeling for the sequential latent variable model . Based on your suggestion , we have provided a quantitative confirmation that the result of the flow is less temporally correlated than the input . \u201c Were modern techniques beyond affine flows considered , such as from Kingma \u2019 18 , Kumar \u2019 19 ? Two layers of affine flows are likely insufficient to model the complexity of these data , which makes the comparison to the purely flow-based models somewhat unfair. \u201d It is important to note that we are applying flows across time steps , rather than within a time step . If we were to apply a method like GLOW ( which is affine ) in an analogous way , this would involve applying the flow on half the time steps as a function of the other half of the steps . Methods like VideoFlow apply flows within time steps . While this may further improve performance by removing spatial correlations , this is not the motivation of our work . Many flows are part of the general family of affine flows ( e.g.NICE , RealNVP , IAF , MAF , GLOW ) , so we felt this was an important place to start in developing this technique . We included comparisons with standalone flow-based models to demonstrate that these models work well as generative models on their own . Note that a single affine autoregressive flow is exactly equivalent to an autoregressive model , which can perform quite well in practice . \u201c Do Kumar et al.not \u201c demonstrate flows across time steps \u201d ? \u201d While Kumar et al.do apply flows within a sequential context , we mean to distinguish between applying flows within a time step vs. across time steps , as in our work . Kumar et al.use non-flow-based models to model temporal dependencies . Other works , such as van den Oord et al.with audio data , do use flows across time steps , as we do here . \u201c Eq ( 10 ) and ( 12 ) seem to be inconsistent . Perhaps x_t = x_t-1 + u_t-1 was meant in eq ( 10 ) ? \u201d We understand the point of confusion , however , Eqs . 10 and 12 are consistent . In Eq.10 , x_t = x_t-1 + u_t gives the exact value of x_t , but in Eq.12 , x_t-1 + u_t-1 gives the mean of the Gaussian distribution over x_t . This can be seen by plugging the random variable for u_t , i.e.Eq.13 , into Eq.10. \u201c Line before eq ( 14 ) : it not true that u_t-1 = x_t-1 - x_t-2 . It would be true if the deterministic x_t = x_t-1 + u_t-1 model was assumed instead of the gaussian N ( x_t ; x_t-1 + u_t-1 , Sigma ) . It is possible that eq ( 14 ) is still correct as the variance of Gaussians is additive. \u201d This follows directly from the definition of u . To be clear , x and u are simply different ways of expressing the same randomness , subject to different offset values . This is because the transform between u_t and x_t is deterministic . All of the stochasticity originates from w_t . \u201c The following work uses autoregressive flows for modeling temporal dynamics and should be cited : Rhinehart \u2019 18,19 \u201d Thank you for these references . We have included them in the updated draft ."}, "1": {"review_id": "HklvmlrKPB-1", "review_text": "Summary: The paper discusses ways to use autoregressive flows in sequence modelling. Two main variants are considered: (a) An affine autoregressive flow directly modelling the data. (b) An affine autoregressive flow whose base distribution is a sequential VAE; equivalently, a sequential VAE whose decoder is an affine autoregressive flow. Pros: The paper is very well written and crystal clear. I particularly appreciated the motivating example that shows how each layer of an affine autoregressive flow reduces the order of a linear dynamical system by 1, and the connections with modelling temporal changes and moving reference frames. The methods are technically correct and well-motivated. The experiments are done well. Overall, the paper scores high on writing and technical quality. Cons: In my opinion, the paper scores low on novelty and original contribution. In general, it's not clear to me what the claimed contribution is. More specifically: Is the claimed contribution new methodology for modelling sequences? In my opinion, using flows as VAE decoders, or adding latent variables to a flow model and training it variationally, are standard applications of existing techniques and I wouldn't consider them particularly novel. Is the claimed contribution improved modelling performance? The main results are that (a) replacing Gaussian decoders with autoregressive flows improves performance, and (b) adding latent variables to the base distribution of an affine autoregressive flow also improves performance. Both of these results are exactly what one would expect from our experience with these methods. Other than that, the paper doesn't present any results that indicate the particular models used enable us to do things we couldn't do before, or improve against the state of the art in sequence modelling. Is the claimed contribution useful representations? The motivation for using the flow in this particular way as a VAE decoder is that the flow will model low-level correlations whereas the latent variables will capture high-level dynamics. However, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way, Decision: Even though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution. Suggestions for improvement: My main suggestion to the authors is to keep up the good work, but also reflect on what the specific contribution of the paper is, and try to make a stronger case for it. Some minor suggestions/corrections follow: Eq. (8): As written, the expression makes little sense as \\sigma is a vector. I understand that there is supposed to be a sum over the elements of log\\sigma, so I'd suggest expressing that more clearly. Eq. (9): It seems to me that the last Jacobian is upside down. In general, it would be good to be more thorough on how this paper is similar to related work and how it differs. There is also this related work which may be good to discuss: Latent Normalizing Flows for Discrete Sequences, https://arxiv.org/abs/1901.10548 In the particle analogy of the motivating example of section 3.1, it would be good to say explicitly that x is the position, u is the velocity and w is the force, to make the example even more intuitive. The paper only considers affine autoregressive flows, but there has been a lot of recent work on non-affine autoregressive flows that are more expressive, for example: Neural Autoregressive Flows, https://arxiv.org/abs/1804.00779 Sum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325 Neural Spline Flows, https://arxiv.org/abs/1906.04032 Such flows could improve the experimental results of the paper. At the very least, it would be good to discuss them as more flexible alternatives. In section 3.2, a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions (e.g. pixels) of y_t. In the experimental section, it would be good to describe on a high level what the architecture of the VAE is, especially the architecture of the prior and the encoder, and the types of distributions used there (e.g. diagonal Gaussians or otherwise). It would be good to show samples from the models in the experimental results.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments ! Here , we will attempt to address additional specific points : \u201c Is the claimed contribution new methodology for modeling sequences ? In my opinion , using flows as VAE decoders , or adding latent variables to a flow model and training it variationally , are standard applications of existing techniques and I would n't consider them particularly novel. \u201d As mentioned , flows and VAE models have been combined in various ways ( though , in our opinion , this is still under-explored ) , and we do not claim to introduce this combination . While affine autoregressive flows are a popular class of flow-based models , to the best of our knowledge , the application of these flows across time steps for the purposes of simplifying video modeling is novel . Specifically , our main contribution is identifying flows as a useful technique for pre-processing sequences to simplify downstream modeling . \u201c Is the claimed contribution improved modeling performance ? The main results are that ( a ) replacing Gaussian decoders with autoregressive flows improves performance , and ( b ) adding latent variables to the base distribution of an affine autoregressive flow also improves performance . Both of these results are exactly what one would expect from our experience with these methods. \u201d Improved modeling performance is one of the results of our method . It does seem that including more flexible model components should obviously improve performance . However , from the perspective from a sequential latent variable model , it is unclear where to incorporate dynamics . For example , one could include more latent variables or recurrent networks at various stages . We specifically propose using autoregressive flows as a type of \u2018 pre-processing \u2019 stage , resulting in a new sequence with dynamics that are hopefully easier to model . In our experiments , we attempt to control for the complexity of each model . \u201c Is the claimed contribution useful representations ? \u201d This is not something that we investigated in this paper , although we intend to investigate this more thoroughly . \u201c Eq . ( 8 ) : As written , the expression makes little sense as \\sigma is a vector . I understand that there is supposed to be a sum over the elements of log\\sigma , so I 'd suggest expressing that more clearly. \u201d You are correct . This has been updated in the submission . Thank you ! \u201c Eq . ( 9 ) : It seems to me that the last Jacobian is upside down. \u201d Indeed . Thanks again ! \u201c In the particle analogy of the motivating example of section 3.1 , it would be good to say explicitly that x is the position , u is the velocity and w is the force , to make the example even more intuitive. \u201d We have stated this in the updated submission . \u201c The paper only considers affine autoregressive flows , but there has been a lot of recent work on non-affine autoregressive flows that are more expressive\u2026At the very least , it would be good to discuss them as more flexible alternatives. \u201d We have included a discussion of these non-affine flows in the updated submission . We chose affine flows for their relative simplicity while still yielding reasonable performance . \u201c In section 3.2 , a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions ( e.g.pixels ) of y_t. \u201d We have stated this limitation more specifically . However , for the purposes of removing correlations across time , rather than space , they are useful . \u201c In the experimental section , it would be good to describe on a high level what the architecture of the VAE is , especially the architecture of the prior and the encoder , and the types of distributions used there ( e.g.diagonal Gaussians or otherwise ) . \u201d We have included a more thorough discussion of the model architectures , as well as diagrams in Appendix B ."}, "2": {"review_id": "HklvmlrKPB-2", "review_text": "This paper proposes to model temporal sequences using autoregressive flows across time steps, that allow to model more explicitly temporal changes of the input, i.e. how the input x_t has changed w.r.t x_{<t}. As also stated by the authors, this is a generalization of other work that instead of modelling the input at each time step, models temporal differences between consecutive time steps. To the best of my knowledge, this is the first work that models normalizing flows in the sequential setting in this way (to be fair however, the idea is fairly obvious). Overall I found the paper interesting, and I think it is well written, so I am leaning towards acceptance. My biggest concern in the paper is the experimental section that could be improved in several ways: - the paper misses broader perfoemance comparisons against other state of the art models, in particular videoflow which is quite related to the models introduced in this paper. - how does the model perform on longer sequences, e.g. for long term generation? I would expect that such a direct dependence of the temporal dynamics on the frames of the video may make it hard for the model to coherently predict future latent states for many time steps. - What would happen if we used the same trick of modelling the conditional likelihood in this way in other SOTA models? - what are the computational requirements of the models presented in this paper? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments ! Here , we will attempt to address additional specific points : \u201c the paper misses broader performance comparisons against other state of the art models , in particular videoflow which is quite related to the models introduced in this paper. \u201d As discussed in our common response , our goal was not to propose a specific video-modeling architecture , but rather to propose a technique for improving sequence modeling . VideoFlow applies flows within each time step , unlike our proposed technique , which operates across time steps . VideoFlow is also significantly larger than the models that we investigated , consisting of 3 levels of latent variables , each with 24 steps of flow , and each flow containing 5 residual blocks . In contrast , the models in our experiments consist of just one or two flows , with each component of our models parameterized with relatively simple convolutional or recurrent networks . As stated in our common response , our quantitative results are on-par with log-likelihood estimates for previous works , like SVG ( Denton & Fergus , 2018 ) . \u201c What would happen if we used the same trick of modeling the conditional likelihood in this way in other SOTA models ? \u201d We chose a representative sequential latent variable model for our experiments . However , we suspect this technique will apply broadly to many sequence modeling settings . Indeed , as we noted in our submission , VideoFlow models differences in variables , which we discuss as a special case of our technique . Before the camera-ready deadline , we intend to conduct additional experiments applying our technique to some of these previously proposed models . \u201c what are the computational requirements of the models presented in this paper ? \u201d Autoregressive flows , in the sequential context , add only a constant computational cost to each time step , requiring only a single forward pass for evaluation and generation ."}}