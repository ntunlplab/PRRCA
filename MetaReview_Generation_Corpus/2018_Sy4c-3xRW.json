{"year": "2018", "forum": "Sy4c-3xRW", "title": "DropMax: Adaptive Stochastic Softmax", "decision": "Invite to Workshop Track", "meta_review": "This paper proposes a general regularization algorithm which builds on the dropout idea. This is a very significant topic. The overall motivation is good, but the specific design choices are less well motivated over, for example, ad-hoc choices. Some concerns remain after the post-rebuttal discussion with the reviewers: the improvement is incremental in terms of concepts and methodology, the clarity needs to be improved and the experiments are somehow weak.\nIn summary, the main idea and research direction is interesting, but the attempted generality of the algorithm and the significance of the area call for a more clear and convincing presentation.\n", "reviews": [{"review_id": "Sy4c-3xRW-0", "review_text": "This paper propose an adaptive dropout strategy for class logits. They learn a distribution q(z | x, y) that randomly throw class logits. By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks. They learn the dropout distribution by variational inference with concrete relaxation. Overall I think this is a good paper. The technique sounds, the presentation is clear and I have not seen similar paper elsewhere (not 100% sure about the originality of the work though). Pro: * General algorithm Con: * The experiment is a little weak. Only on CIFAR100 the proposed approach is much better than other approaches. I would like to see the results on more datasets. Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate your comments - The experiment is a little weak . Only on CIFAR100 the proposed approach is much better than other approaches . I would like to see the results on more datasets . Maybe should also compare with more dropout algorithms , such as DropConnect and MaxOut . : We are experimenting on ImageNet 1K dataset , and will include the results if we obtain the results by the rebuttal deadline . DropConnect and MaxOut are not much relevant to our motivation of learning an ensemble of multiple classifiers in a single training stage , as they do not drop out classes ."}, {"review_id": "Sy4c-3xRW-1", "review_text": "Pros - The proposed model is a nice way of multiplicatively combining two features : one which determines which classes to pay attention to, and other that provides useful features for discrimination. - The adaptive component seems to provide improvements for small dataset sizes and large number of classes. Cons - \"One can easily see that if o_t(x; w) = 0, then class t becomes neutral in the classification and the gradients are not back-propagated from it.\" : This does not seem to be true. Even if the logits are zero, the class would have a non-zero probability and would receive gradients. Do the authors mean exp(o_t(x;w)) = 0 ? - Related to the above, it should be clarified what is meant by dropping a class. Is its logit set to zero or -\\infty ? Excluding a class from the softmax is equivalent to having a logit of -\\infty, not zero. However, from the equations in the paper it seems that the logit is set to zero. This would not result in excluding the unit. The overall effect would just be to raise the magnitude of logits across the entire softmax. - It seems that the model benefits from at least two separate effects - one is the attention mechanism provided by the sigmoids, and the other is the stochasticity during training. Presently, it is not clear if only one of the components is providing most of the benefits, or if both things are useful. It would be great to compare this model to a non-stochastic one which just has the multiplicative effects applied in a deterministic way (during both training and testing). - The objective of the attention mechanism that sets the dropout mask seems to be the same as the primary objective of classifying the input, and the attention mechanism is prevented from solving the task by adding an extra entropy regularization. It would be useful to explain more why this is needed. Would it not be fine if the attention mechanism did a perfect job of selecting the class ? Quality The paper makes relevant comparisons and is overall well-motivated. However, some aspects of the paper can be improved by adding more explanations. Clarity Some crucial aspects of the paper are unclear as mentioned above. Originality The main contribution of the paper is similar to multiplicative gating. The added stochasticity and the model ensembling interpretation is probably novel. However, experiments are insufficient to determine whether it is this novelty that contributes to improved performance or just the gating. Significance This paper makes incremental improvements and would be of moderate interest to the machine learning community. Typos : - In Eq 3, the numerator has z_t. Should that be z_y ? - In Eq 5, the denominator has z_y. Should that be z_t ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate your comments . - It seems that the model benefits from at least two separate effects - one is the attention mechanism provided by the sigmoids , and the other is the stochasticity during training . Presently , it is not clear if only one of the components is providing most of the benefits , or if both things are useful . It would be great to compare this model to a non-stochastic one which just has the multiplicative effects applied in a deterministic way ( during both training and testing ) . : As said , our model benefits from two separate effects - 1 ) adaptive input-dependant attention generation and 2 ) stochasticity during training . The effect of 1 ) is clear since our adaptive dropmax significantly outperforms random dropmax . To show the effect of 2 ) we added in the results from the deterministic model in the revision , which we name as Deterministic-Attention , in Table 1 . This model is almost identical to \u201c Adaptive-Dropout \u201d , except that the stochastic \u2018 z_t \u2019 is replaced with deterministic \u2018 \\rho_t \u2019 . We observe that stochasticity does indeed help improve the model performance , as Adaptive-Dropmax outperformed Deterministic-Attention by 0.59 % in MNIST-1K , 0.22 % in MNIST-5K and similarly on the other datasets ( except on MNIST-55K ) . Further , our deterministic attention model has both KL term and entropy regularizer as in \u201c Adaptive-Dropout \u201d , with \\lambda found via separate holdout set , such that the target class is strongly attended for each input while non-target classes are not . This design , which is also used in our Adaptive-Dropmax , is also a novelty of our model since a naive implementation of deterministic attention produces much worse results than the base model , - The objective of the attention mechanism that sets the dropout mask seems to be the same as the primary objective of classifying the input , and the attention mechanism is prevented from solving the task by adding an extra entropy regularization . It would be useful to explain more why this is needed . Would it not be fine if the attention mechanism did a perfect job of selecting the class ? : The objective of the dropout mask generator is to stochastically rule out non-target classes such that the model can learn features for both coarse-grained and fine-grained classification . If we allow the dropout mask generator to become another classifier , then the original classifier has no problem to solve and will not learn anything useful , and thus we should differentiate the role of the classifier and the dropout mask generator . We found that even in the case where it is easy enough for the mask generator to do a perfect job of selecting the target ( See Figure 4 ( a ) in the revision - Figure 3 ( a ) in the original paper ) the performance was the best when the non-target classes are not completely ruled out as \\lambda was found to be nonzero ( 0.1 ~ 0.0001 ) . To verify it , we experimented with Deterministic-Attention model , with the Sigm ( ) in Eq . ( 4 ) replaced with Softmax ( ) . It makes the mask generator to be another classifier , because generated masks become mutually exclusive , with only one of them close to 1 per each instance . The entropy regularizer ( 14 ) is removed for our purpose . We tested it on MNIST and Cifar-100 , and the results are as follows : MNIST-1K : 7.13 MNIST-5K : 2.57 MNIST-55K : 1.09 Cifar-100 : 30.38 The results are similar to or worse than the baseline , meaning that the role of the mask generator should be controlled in a principled way . - The main contribution of the paper is similar to multiplicative gating . Experiments are insufficient to determine whether it is just the gating that contributes to improved performance . : As mentioned above , the newly added in experimental results for the deterministic attention model shows that the stochasticity is still important for obtaining meaningful performance improvement , as it enables to obtain an ensemble of exponentially many classifiers in a single model training ."}, {"review_id": "Sy4c-3xRW-2", "review_text": "The paper discusses dropping out the pre-softmax logits in an adaptive manner. This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout. In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference. The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour. A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1. I would have liked to have seen results on ImageNet. I don't find (the too small) Figure 2 to be compelling evidence that \"our dropmax effectively prevents overfiting by converging to much lower test loss\". The test loss in question looks like a noisy version of the base test loss with a slightly lower mean. There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage. Figure 3 illustrates the idea nicely. Which of the MNIST models from Table 1 was used? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We really appreciate your comments . - The paper discusses dropping out the pre-softmax logits in an adaptive manner . This is n't a huge conceptual leap given previous work , for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interpretations of dropout . : The main focus of this paper is not interpreting dropout ( or adaptive dropout ) wrt variational inference . Those are simply our choice of tools for solving the proposed problem , and the main novelty comes from stochastically ruling out classes from consideration at each iteration . None of the previous work exploits such idea . - The variational approximation is a bit odd in that it does n't have any variational parameters . : Our decision of setting the q ( or recognition ) network the same as the p ( or prior ) network is motivated from ( Sohn et al. , 2015 ) ( Section 4.2 ) . Since we are training with q network while predicting with p network , the consistency between the two network is crucial in obtaining the desired performance . It is indicated by KL [ q||p ] term in the Eq . ( 7 ) .Suppose use a different set of variational parameters \\phi for q ( z|x , y ) . The problem in this case is that reconstructing y with q ( z|x , y ; \\phi ) and reconstructing y with p ( z|x ; \\theta ) are significantly different in their difficulties . The former is much easier because it learns trivial mapping y - > z - > y , where the dimension of z is the same as that of y . Thus , we decided to replace q ( z|x , y ; \\phi ) with q ( z|x , y ; \\theta ) that shares the same structure and the set of parameters with p ( z|x ; \\theta ) . In our preliminary experiment , we also experimented with the model that uses a separate parameter for q , but it did not work well . - a further regulariser in equation ( 14 ) is needed to give the desired behaviour . : Since regularized variational inference is a general framework and allows us to avoid the weird solution all z=0 or z=1 , we argue that ( 14 ) is reasonable . - I would have liked to have seen results on ImageNet . : We will run the experiments on the ImageNet dataset and will include the results in the revision if we obtain the results by the rebuttal deadline . - I do n't find ( the too small ) Figure 2 to be compelling evidence that `` our dropmax effectively prevents overfitting by converging to much lower test loss '' . The test loss in question looks like a noisy version of the base test loss with a slightly lower mean . : The plot was not the most representative and we included in a more stable version in the revision . Also the main point we want to make with Figure 2 is that our model is still able to achieve lower test loss , while retaining the same convergence speed as the baseline . - There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage . : We have corrected the grammatical errors in the revision . - Which of the MNIST models from Table 1 was used ? : We used the MNIST-1K model ."}], "0": {"review_id": "Sy4c-3xRW-0", "review_text": "This paper propose an adaptive dropout strategy for class logits. They learn a distribution q(z | x, y) that randomly throw class logits. By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks. They learn the dropout distribution by variational inference with concrete relaxation. Overall I think this is a good paper. The technique sounds, the presentation is clear and I have not seen similar paper elsewhere (not 100% sure about the originality of the work though). Pro: * General algorithm Con: * The experiment is a little weak. Only on CIFAR100 the proposed approach is much better than other approaches. I would like to see the results on more datasets. Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate your comments - The experiment is a little weak . Only on CIFAR100 the proposed approach is much better than other approaches . I would like to see the results on more datasets . Maybe should also compare with more dropout algorithms , such as DropConnect and MaxOut . : We are experimenting on ImageNet 1K dataset , and will include the results if we obtain the results by the rebuttal deadline . DropConnect and MaxOut are not much relevant to our motivation of learning an ensemble of multiple classifiers in a single training stage , as they do not drop out classes ."}, "1": {"review_id": "Sy4c-3xRW-1", "review_text": "Pros - The proposed model is a nice way of multiplicatively combining two features : one which determines which classes to pay attention to, and other that provides useful features for discrimination. - The adaptive component seems to provide improvements for small dataset sizes and large number of classes. Cons - \"One can easily see that if o_t(x; w) = 0, then class t becomes neutral in the classification and the gradients are not back-propagated from it.\" : This does not seem to be true. Even if the logits are zero, the class would have a non-zero probability and would receive gradients. Do the authors mean exp(o_t(x;w)) = 0 ? - Related to the above, it should be clarified what is meant by dropping a class. Is its logit set to zero or -\\infty ? Excluding a class from the softmax is equivalent to having a logit of -\\infty, not zero. However, from the equations in the paper it seems that the logit is set to zero. This would not result in excluding the unit. The overall effect would just be to raise the magnitude of logits across the entire softmax. - It seems that the model benefits from at least two separate effects - one is the attention mechanism provided by the sigmoids, and the other is the stochasticity during training. Presently, it is not clear if only one of the components is providing most of the benefits, or if both things are useful. It would be great to compare this model to a non-stochastic one which just has the multiplicative effects applied in a deterministic way (during both training and testing). - The objective of the attention mechanism that sets the dropout mask seems to be the same as the primary objective of classifying the input, and the attention mechanism is prevented from solving the task by adding an extra entropy regularization. It would be useful to explain more why this is needed. Would it not be fine if the attention mechanism did a perfect job of selecting the class ? Quality The paper makes relevant comparisons and is overall well-motivated. However, some aspects of the paper can be improved by adding more explanations. Clarity Some crucial aspects of the paper are unclear as mentioned above. Originality The main contribution of the paper is similar to multiplicative gating. The added stochasticity and the model ensembling interpretation is probably novel. However, experiments are insufficient to determine whether it is this novelty that contributes to improved performance or just the gating. Significance This paper makes incremental improvements and would be of moderate interest to the machine learning community. Typos : - In Eq 3, the numerator has z_t. Should that be z_y ? - In Eq 5, the denominator has z_y. Should that be z_t ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We really appreciate your comments . - It seems that the model benefits from at least two separate effects - one is the attention mechanism provided by the sigmoids , and the other is the stochasticity during training . Presently , it is not clear if only one of the components is providing most of the benefits , or if both things are useful . It would be great to compare this model to a non-stochastic one which just has the multiplicative effects applied in a deterministic way ( during both training and testing ) . : As said , our model benefits from two separate effects - 1 ) adaptive input-dependant attention generation and 2 ) stochasticity during training . The effect of 1 ) is clear since our adaptive dropmax significantly outperforms random dropmax . To show the effect of 2 ) we added in the results from the deterministic model in the revision , which we name as Deterministic-Attention , in Table 1 . This model is almost identical to \u201c Adaptive-Dropout \u201d , except that the stochastic \u2018 z_t \u2019 is replaced with deterministic \u2018 \\rho_t \u2019 . We observe that stochasticity does indeed help improve the model performance , as Adaptive-Dropmax outperformed Deterministic-Attention by 0.59 % in MNIST-1K , 0.22 % in MNIST-5K and similarly on the other datasets ( except on MNIST-55K ) . Further , our deterministic attention model has both KL term and entropy regularizer as in \u201c Adaptive-Dropout \u201d , with \\lambda found via separate holdout set , such that the target class is strongly attended for each input while non-target classes are not . This design , which is also used in our Adaptive-Dropmax , is also a novelty of our model since a naive implementation of deterministic attention produces much worse results than the base model , - The objective of the attention mechanism that sets the dropout mask seems to be the same as the primary objective of classifying the input , and the attention mechanism is prevented from solving the task by adding an extra entropy regularization . It would be useful to explain more why this is needed . Would it not be fine if the attention mechanism did a perfect job of selecting the class ? : The objective of the dropout mask generator is to stochastically rule out non-target classes such that the model can learn features for both coarse-grained and fine-grained classification . If we allow the dropout mask generator to become another classifier , then the original classifier has no problem to solve and will not learn anything useful , and thus we should differentiate the role of the classifier and the dropout mask generator . We found that even in the case where it is easy enough for the mask generator to do a perfect job of selecting the target ( See Figure 4 ( a ) in the revision - Figure 3 ( a ) in the original paper ) the performance was the best when the non-target classes are not completely ruled out as \\lambda was found to be nonzero ( 0.1 ~ 0.0001 ) . To verify it , we experimented with Deterministic-Attention model , with the Sigm ( ) in Eq . ( 4 ) replaced with Softmax ( ) . It makes the mask generator to be another classifier , because generated masks become mutually exclusive , with only one of them close to 1 per each instance . The entropy regularizer ( 14 ) is removed for our purpose . We tested it on MNIST and Cifar-100 , and the results are as follows : MNIST-1K : 7.13 MNIST-5K : 2.57 MNIST-55K : 1.09 Cifar-100 : 30.38 The results are similar to or worse than the baseline , meaning that the role of the mask generator should be controlled in a principled way . - The main contribution of the paper is similar to multiplicative gating . Experiments are insufficient to determine whether it is just the gating that contributes to improved performance . : As mentioned above , the newly added in experimental results for the deterministic attention model shows that the stochasticity is still important for obtaining meaningful performance improvement , as it enables to obtain an ensemble of exponentially many classifiers in a single model training ."}, "2": {"review_id": "Sy4c-3xRW-2", "review_text": "The paper discusses dropping out the pre-softmax logits in an adaptive manner. This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout. In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference. The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour. A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1. I would have liked to have seen results on ImageNet. I don't find (the too small) Figure 2 to be compelling evidence that \"our dropmax effectively prevents overfiting by converging to much lower test loss\". The test loss in question looks like a noisy version of the base test loss with a slightly lower mean. There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage. Figure 3 illustrates the idea nicely. Which of the MNIST models from Table 1 was used? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We really appreciate your comments . - The paper discusses dropping out the pre-softmax logits in an adaptive manner . This is n't a huge conceptual leap given previous work , for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interpretations of dropout . : The main focus of this paper is not interpreting dropout ( or adaptive dropout ) wrt variational inference . Those are simply our choice of tools for solving the proposed problem , and the main novelty comes from stochastically ruling out classes from consideration at each iteration . None of the previous work exploits such idea . - The variational approximation is a bit odd in that it does n't have any variational parameters . : Our decision of setting the q ( or recognition ) network the same as the p ( or prior ) network is motivated from ( Sohn et al. , 2015 ) ( Section 4.2 ) . Since we are training with q network while predicting with p network , the consistency between the two network is crucial in obtaining the desired performance . It is indicated by KL [ q||p ] term in the Eq . ( 7 ) .Suppose use a different set of variational parameters \\phi for q ( z|x , y ) . The problem in this case is that reconstructing y with q ( z|x , y ; \\phi ) and reconstructing y with p ( z|x ; \\theta ) are significantly different in their difficulties . The former is much easier because it learns trivial mapping y - > z - > y , where the dimension of z is the same as that of y . Thus , we decided to replace q ( z|x , y ; \\phi ) with q ( z|x , y ; \\theta ) that shares the same structure and the set of parameters with p ( z|x ; \\theta ) . In our preliminary experiment , we also experimented with the model that uses a separate parameter for q , but it did not work well . - a further regulariser in equation ( 14 ) is needed to give the desired behaviour . : Since regularized variational inference is a general framework and allows us to avoid the weird solution all z=0 or z=1 , we argue that ( 14 ) is reasonable . - I would have liked to have seen results on ImageNet . : We will run the experiments on the ImageNet dataset and will include the results in the revision if we obtain the results by the rebuttal deadline . - I do n't find ( the too small ) Figure 2 to be compelling evidence that `` our dropmax effectively prevents overfitting by converging to much lower test loss '' . The test loss in question looks like a noisy version of the base test loss with a slightly lower mean . : The plot was not the most representative and we included in a more stable version in the revision . Also the main point we want to make with Figure 2 is that our model is still able to achieve lower test loss , while retaining the same convergence speed as the baseline . - There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage . : We have corrected the grammatical errors in the revision . - Which of the MNIST models from Table 1 was used ? : We used the MNIST-1K model ."}}