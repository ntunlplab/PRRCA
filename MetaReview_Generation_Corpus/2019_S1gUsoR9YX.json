{"year": "2019", "forum": "S1gUsoR9YX", "title": "Multilingual Neural Machine Translation with Knowledge Distillation", "decision": "Accept (Poster)", "meta_review": "This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model). There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.", "reviews": [{"review_id": "S1gUsoR9YX-0", "review_text": "... I would have liked to see some more insights. The authors present a method for distilling knowledge from individual models to train a multilingual model. The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models. The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models. Please find below my comments and questions. 1) The authors have done a commendable job of validating their hypothesis on multiple datasets. Solid experimentation is definitely the main strength of this paper. 2) However, this strength also makes way for a weakness. The entire experimental section is just filled with tables and numbers. The same message is repeated across these multiple tables (multi+distill > single > multi). Beyond this message there are no other insights. For example, - How does the performance depend on the divergence between source and target language? - Why is there more important on some languages and less on others ? - Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets. - What happens when the target language is something other than English? All the experiments report results from X-->English, why not in the other direction? The model then is not really \"completely\" multilingual. It is multi-source-->single target. - Can you comment on the total training time ? - What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ? What do you mean by accuracy here? Only later when you mention that \\threshold = 1 BLEU it became clear that accuracy means BLEU in this context ? 3) Is it all worth it? One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset). I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself. 4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation +++++++++++++++++++ I have updated my rating after reading author's responses ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for the reviews and comments ! Here are our responses to the comments . Question1 : How does the divergence between the source and target language influence the performance , and why is there more important on some languages and less on others ? Answer1 : We found that the performance gains of our method correlate with the training data size of each language . If a language has small training data , then it is likely to get more improvement due to the benefit of multilingual training . Question2 : Why are the improvements on the TED dataset so much higher ? Answer2 : We guess you meant the improvements of our method over individual models on the TED dataset . Some languages in the TED dataset are of small data size , and thus they get higher improvement from multilingual training , by leveraging the training data from other ( similar ) languages . Question3 : What happens when the target language is something other than English ? Answer3 : We have provided the English-to-many translation results on the IWSLT dataset in the table below . The BLEU scores in ( ) represent the difference between the multilingual model and individual models . Delta represents the improvements of our multi-distillation method over the multi-baseline . We can see our method consistently outperforms the multilingual baseline on all languages , and can nearly match or even surpass the accuracy of the individual models , even if one-to-many translation is considered harder than many-to-one translation . We have also updated the results in the paper . We will provide more results in the following days . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Language | Individual | Multilingual-baseline | Multilingual-distill | Delta en-ar | 13.67 | 12.73 ( -0.94 ) | 13.80 ( 0.13 ) | 1.07 en-cs | 17.81 | 17.33 ( -0.48 ) | 18.69 ( 0.88 ) | 1.37 en-de | 26.13 | 25.16 ( -0.97 ) | 26.76 ( 0.63 ) | 1.60 en-he | 24.15 | 22.73 ( -1.42 ) | 24.42 ( 0.27 ) | 1.69 en-nl | 30.88 | 29.51 ( -1.37 ) | 30.52 ( -0.36 ) | 1.01 en-pt | 37.63 | 35.93 ( -1.70 ) | 37.23 ( -0.40 ) | 1.30 en-ro | 27.23 | 25.68 ( -1.55 ) | 27.11 ( -0.12 ) | 1.42 en-ru | 17.40 | 16.26 ( -1.14 ) | 17.42 ( 0.02 ) | 1.16 en-th | 26.45 | 27.18 ( 0.73 ) | 27.62 ( 1.17 ) | 0.45 en-tr | 12.47 | 11.63 ( -0.84 ) | 12.84 ( 0.37 ) | 1.21 en-vi | 27.88 | 28.04 ( 0.16 ) | 28.69 ( 0.81 ) | 0.65 en-zh | 10.95 | 10.12 ( -0.83 ) | 10.41 ( -0.54 ) | 0.29 -- -- -- -- -- --"}, {"review_id": "S1gUsoR9YX-1", "review_text": "The authors apply knowledge distillation for many-to-one multilingual neural machine translation, first training separate models for each language pair. For most language pairs, performance matches or improves upon single-task baselines. Strengths: Improvements upon the baselines are fairly impressive, especially for the 44-language model. The approach is quite simple and could be easily implemented by other groups. The paper is well-written and easy to understand. At inference, only a single model needs to be retained, which is memory-efficient. Weaknesses: The authors only test distillation in a many-to-one scenario. I believe that providing results for many-to-many multilingual NMT would be valuable. Overall, this approach increases training time as all single-task models must have converged before beginning distillation. The authors provide no direct comparison to other work, which makes it hard to know how strong the baselines are. At least for WMT, I would suggest reporting results with mteval-v13a (or SACREBLEU), so that results can be compared against official results. Questions: For the top-K approach, do you normalize the top K probabilities so that they sum to 1 or not? Did you consider applying sequence knowledge distillation (Kim and Rush, 2016) (using the baseline beam search output as references) instead of word knowledge distillation? *** EDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper. I am increasing my rating from 6 to 7.", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 3 for the reviews and comments ! Here are our responses to the comments . 1.Regarding the results of many-to-many translations We have provided the English-to-many translation results on the IWSLT dataset in the table below . The BLEU scores in ( ) represent the difference between the multilingual model and individual models . Delta represents the improvements of our multi-distillation method over the multi-baseline . We can see our method consistently outperforms the multilingual baseline on all languages , and can nearly match or even surpass the accuracy of the individual models , even if one-to-many translation is considered harder than many-to-one translation . We have also updated the results in the paper . We will provide more results on the WMT dataset in the following days and make comparison with previous works . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Language | Individual | Multilingual-baseline | Multilingual-distill | Delta en-ar | 13.67 | 12.73 ( -0.94 ) | 13.80 ( 0.13 ) | 1.07 en-cs | 17.81 | 17.33 ( -0.48 ) | 18.69 ( 0.88 ) | 1.37 en-de | 26.13 | 25.16 ( -0.97 ) | 26.76 ( 0.63 ) | 1.60 en-he | 24.15 | 22.73 ( -1.42 ) | 24.42 ( 0.27 ) | 1.69 en-nl | 30.88 | 29.51 ( -1.37 ) | 30.52 ( -0.36 ) | 1.01 en-pt | 37.63 | 35.93 ( -1.70 ) | 37.23 ( -0.40 ) | 1.30 en-ro | 27.23 | 25.68 ( -1.55 ) | 27.11 ( -0.12 ) | 1.42 en-ru | 17.40 | 16.26 ( -1.14 ) | 17.42 ( 0.02 ) | 1.16 en-th | 26.45 | 27.18 ( 0.73 ) | 27.62 ( 1.17 ) | 0.45 en-tr | 12.47 | 11.63 ( -0.84 ) | 12.84 ( 0.37 ) | 1.21 en-vi | 27.88 | 28.04 ( 0.16 ) | 28.69 ( 0.81 ) | 0.65 en-zh | 10.95 | 10.12 ( -0.83 ) | 10.41 ( -0.54 ) | 0.29 -- -- -- -- -- -- Regarding training time The individual models need to be pre-trained , which will incur additional time . According to the training time statistics on IWSLT dataset with NVIDIA V100 GPU , it takes nearly 4 hours to train the individual model with 1 GPU . The total GPU time is 4hours * 12 GPUs for 12 languages . The training time for multilingual baseline is nearly 11hours * 4GPUs , while our method is nearly 13 hours * 4GPUs . Our method only takes extra 2hours * 4GPUs for the multilingual training and 4 hours * 12GPUs for the individual model pre-training . Furthermore , we can assume the individual models are pre-given , which is reasonable because the production system usually wants to adapt the individual translation into multilingual setting , at the benefit of saving maintenance cost while with no accuracy degradation or even with accuracy improvement , which is exactly the goal of this work . 3.Regarding the top-K distillation Yes , we normalize the top K probabilities so that they sum to 1 . We have added the description in the new version . 4.Regarding sequence-level knowledge distillation We have tried sequence-level knowledge distillation . It results in consistently inferior accuracy on all languages compared with word-level knowledge distillation used in our work . The results are listed as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Language | Sequence-level distillation | Word-level distillation en-ar | 12.79 | 13.80 en-cs | 17.01 | 18.69 en-de | 25.89 | 26.76 en-he | 22.92 | 24.42 en-nl | 29.99 | 30.52 en-pt | 36.12 | 37.23 en-ro | 25.75 | 27.11 en-ru | 16.38 | 17.42 en-th | 27.52 | 27.62 en-tr | 11.11 | 12.84 en-vi | 28.08 | 28.69 en-zh | 10.25 | 10.41 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}, {"review_id": "S1gUsoR9YX-2", "review_text": "Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models. Periodically compare the validation BLEU score of the multilingual model with that of each individual model, and turn off distillation for language pairs where the multilingual model is better. On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs. Supplementary experiments justify the strategy of selectively turning off distillation, and quantify the effect using only the top 8 vocabulary items in distillation. The main idea makes sense, and the results are very convincing, especially since it appears that hyper-parameters were not tuned extensively (eg, weight of 0.5 on the distillation loss, for all language pairs). Implementation should be very straightforward, especially with the trick of pre-computing top-k probabilities from the teacher model at each corpus position. One small barrier to practical application that the authors fail to acknowledge is the requirement to train individual models, which will at least double training time compared to a single multilingual model. The main missing experiment is higher-capacity multilingual models, which Johnson et al show to be beneficial in settings with a large number of language pairs. Using a multilingual model of the same (relatively small) size as the individual models as is done here is likely to be suboptimal, especially for the 44-language pair TED setting. A related point is that the corpora used seem to be quite small (eg 4.5M and 1M sentences for WMT Czech and German, respectively, while the available training corpora are closer to 15M and 4.5M). Although performance relative to individual models is still impressive - and seems to be better than than in previous work - this makes the experiments comparing to the multilingual baseline less meaningful. Also missing are experiments on out-of-English translation, which would establish the viability of the proposed technique for many-to-many translation via bridging. Out-of-English is a more difficult problem than into-English. I can\u2019t see any reason the proposed technique wouldn\u2019t also work in this setting, but this remains to be shown. Although it\u2019s great that the technique is shown to work without embellishments, there are a few obvious strategies it would have been interesting to explore, such as making the weight on the distillation loss dependent on the difference in performance between the multilingual and individual models; and allowing for the distillation loss to be turned back on if the performance of the multilingual model starts to drift back down for a particular language pair. I also wondered about the effect of the gradient accumulation strategy in algorithm 1, where individual batches from each language pair are effectively grouped into one giant batch for the purpose of parameter updates. I can see that this could stabilize training, but it would be good to know whether it\u2019s crucial for success, especially when the number of language pairs is large. Further details: As aforementioned -> As mentioned (1) 2nd line: Doesn't make sense as written. You need to distinguish the gold y_t from hypothesized ones in the 1() function. Above (2): is served as -> serves as 3.2 First paragraph. Since D presumably consists of D^l for all languages l, L_ALL(D,...) should be a function of teacher parameters theta^l for all languages l rather than just one as written. In top-K distillation, is the teacher distribution renormalized or simply truncated? Generalization analysis, pg 8: presumably you are sampling from N(0, sigma^2) - this should be described as such. Reference: Johnson et al, \u201cGoogle\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation\u201d TACL, 2017.", "rating": "7: Good paper, accept", "reply_text": "4.Regarding out-of-English translation We have provided the English-to-many translation results on the IWSLT dataset in the table below . The BLEU scores in ( ) represent the difference between the multilingual model and individual models . Delta represents the improvements of our multi-distillation method over the multi-baseline . We can see our method consistently outperforms the multilingual baseline on all languages , and can nearly match or even surpass the accuracy of the individual models , even if one-to-many translation is considered harder than many-to-one translation . We have also updated the results in the paper . We will provide more results in the following days . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Language | Individual | Multilingual-baseline | Multilingual-distill | Delta en-ar | 13.67 | 12.73 ( -0.94 ) | 13.80 ( 0.13 ) | 1.07 en-cs | 17.81 | 17.33 ( -0.48 ) | 18.69 ( 0.88 ) | 1.37 en-de | 26.13 | 25.16 ( -0.97 ) | 26.76 ( 0.63 ) | 1.60 en-he | 24.15 | 22.73 ( -1.42 ) | 24.42 ( 0.27 ) | 1.69 en-nl | 30.88 | 29.51 ( -1.37 ) | 30.52 ( -0.36 ) | 1.01 en-pt | 37.63 | 35.93 ( -1.70 ) | 37.23 ( -0.40 ) | 1.30 en-ro | 27.23 | 25.68 ( -1.55 ) | 27.11 ( -0.12 ) | 1.42 en-ru | 17.40 | 16.26 ( -1.14 ) | 17.42 ( 0.02 ) | 1.16 en-th | 26.45 | 27.18 ( 0.73 ) | 27.62 ( 1.17 ) | 0.45 en-tr | 12.47 | 11.63 ( -0.84 ) | 12.84 ( 0.37 ) | 1.21 en-vi | 27.88 | 28.04 ( 0.16 ) | 28.69 ( 0.81 ) | 0.65 en-zh | 10.95 | 10.12 ( -0.83 ) | 10.41 ( -0.54 ) | 0.29 -- -- -- -- -- -- Regarding the weight on the distillation loss and turning back the distillation loss Thanks for the suggestion . We have provided the results for turning back the distillation loss with a hard threshold in the submitted version . According to the number ( Table 7 in the updated version ) , back distillation improves the accuracy of individual models . We quickly try a simple adaptive weight that changes according to BLEU gap between the teacher and student model : \\lambda = 0.9 * ( 1/2 ) ^ { max ( BLEU_student + 2 - BLEU_teacher , 0 ) } , which means if a student is lower than the teacher by more than 2 BLEU score , the weight is 0.9 . After that , the weight is decayed exponentially . The initial results on IWSLT dataset demonstrate that there is no much difference ( with an average of 0.3 BLEU score ) compared with the constant weight we used in this paper . We will conduct a comprehensive study on this kind of back distillation you mentioned in the future work . 6.Regarding the gradient accumulation strategy We have conducted experiments to analyze if it is critical for the importance of gradient accumulation . We found it is not critical for the model training . We run an experiment on the IWSLT dataset without gradient accumulation , i.e. , updating the model parameters immediately with the training data from a single language . But in order to make sure the update in the two settings has the same batch size , which is a critical hyperparameter for model training , we increase the batch size for the single language by 12 times , to be the same with the batch size in the setting of gradient accumulation . We found the accuracies of the two settings are nearly the same , with 0.3 BLEU higher or lower at most . 7.Regarding the writing and typos We have fixed the typos in the new version . In top-K distillation , the teacher distribution is renormalized ."}], "0": {"review_id": "S1gUsoR9YX-0", "review_text": "... I would have liked to see some more insights. The authors present a method for distilling knowledge from individual models to train a multilingual model. The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models. The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models. Please find below my comments and questions. 1) The authors have done a commendable job of validating their hypothesis on multiple datasets. Solid experimentation is definitely the main strength of this paper. 2) However, this strength also makes way for a weakness. The entire experimental section is just filled with tables and numbers. The same message is repeated across these multiple tables (multi+distill > single > multi). Beyond this message there are no other insights. For example, - How does the performance depend on the divergence between source and target language? - Why is there more important on some languages and less on others ? - Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets. - What happens when the target language is something other than English? All the experiments report results from X-->English, why not in the other direction? The model then is not really \"completely\" multilingual. It is multi-source-->single target. - Can you comment on the total training time ? - What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ? What do you mean by accuracy here? Only later when you mention that \\threshold = 1 BLEU it became clear that accuracy means BLEU in this context ? 3) Is it all worth it? One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset). I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself. 4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation +++++++++++++++++++ I have updated my rating after reading author's responses ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for the reviews and comments ! Here are our responses to the comments . Question1 : How does the divergence between the source and target language influence the performance , and why is there more important on some languages and less on others ? Answer1 : We found that the performance gains of our method correlate with the training data size of each language . If a language has small training data , then it is likely to get more improvement due to the benefit of multilingual training . Question2 : Why are the improvements on the TED dataset so much higher ? Answer2 : We guess you meant the improvements of our method over individual models on the TED dataset . Some languages in the TED dataset are of small data size , and thus they get higher improvement from multilingual training , by leveraging the training data from other ( similar ) languages . Question3 : What happens when the target language is something other than English ? Answer3 : We have provided the English-to-many translation results on the IWSLT dataset in the table below . The BLEU scores in ( ) represent the difference between the multilingual model and individual models . Delta represents the improvements of our multi-distillation method over the multi-baseline . We can see our method consistently outperforms the multilingual baseline on all languages , and can nearly match or even surpass the accuracy of the individual models , even if one-to-many translation is considered harder than many-to-one translation . We have also updated the results in the paper . We will provide more results in the following days . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Language | Individual | Multilingual-baseline | Multilingual-distill | Delta en-ar | 13.67 | 12.73 ( -0.94 ) | 13.80 ( 0.13 ) | 1.07 en-cs | 17.81 | 17.33 ( -0.48 ) | 18.69 ( 0.88 ) | 1.37 en-de | 26.13 | 25.16 ( -0.97 ) | 26.76 ( 0.63 ) | 1.60 en-he | 24.15 | 22.73 ( -1.42 ) | 24.42 ( 0.27 ) | 1.69 en-nl | 30.88 | 29.51 ( -1.37 ) | 30.52 ( -0.36 ) | 1.01 en-pt | 37.63 | 35.93 ( -1.70 ) | 37.23 ( -0.40 ) | 1.30 en-ro | 27.23 | 25.68 ( -1.55 ) | 27.11 ( -0.12 ) | 1.42 en-ru | 17.40 | 16.26 ( -1.14 ) | 17.42 ( 0.02 ) | 1.16 en-th | 26.45 | 27.18 ( 0.73 ) | 27.62 ( 1.17 ) | 0.45 en-tr | 12.47 | 11.63 ( -0.84 ) | 12.84 ( 0.37 ) | 1.21 en-vi | 27.88 | 28.04 ( 0.16 ) | 28.69 ( 0.81 ) | 0.65 en-zh | 10.95 | 10.12 ( -0.83 ) | 10.41 ( -0.54 ) | 0.29 -- -- -- -- -- --"}, "1": {"review_id": "S1gUsoR9YX-1", "review_text": "The authors apply knowledge distillation for many-to-one multilingual neural machine translation, first training separate models for each language pair. For most language pairs, performance matches or improves upon single-task baselines. Strengths: Improvements upon the baselines are fairly impressive, especially for the 44-language model. The approach is quite simple and could be easily implemented by other groups. The paper is well-written and easy to understand. At inference, only a single model needs to be retained, which is memory-efficient. Weaknesses: The authors only test distillation in a many-to-one scenario. I believe that providing results for many-to-many multilingual NMT would be valuable. Overall, this approach increases training time as all single-task models must have converged before beginning distillation. The authors provide no direct comparison to other work, which makes it hard to know how strong the baselines are. At least for WMT, I would suggest reporting results with mteval-v13a (or SACREBLEU), so that results can be compared against official results. Questions: For the top-K approach, do you normalize the top K probabilities so that they sum to 1 or not? Did you consider applying sequence knowledge distillation (Kim and Rush, 2016) (using the baseline beam search output as references) instead of word knowledge distillation? *** EDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper. I am increasing my rating from 6 to 7.", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 3 for the reviews and comments ! Here are our responses to the comments . 1.Regarding the results of many-to-many translations We have provided the English-to-many translation results on the IWSLT dataset in the table below . The BLEU scores in ( ) represent the difference between the multilingual model and individual models . Delta represents the improvements of our multi-distillation method over the multi-baseline . We can see our method consistently outperforms the multilingual baseline on all languages , and can nearly match or even surpass the accuracy of the individual models , even if one-to-many translation is considered harder than many-to-one translation . We have also updated the results in the paper . We will provide more results on the WMT dataset in the following days and make comparison with previous works . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Language | Individual | Multilingual-baseline | Multilingual-distill | Delta en-ar | 13.67 | 12.73 ( -0.94 ) | 13.80 ( 0.13 ) | 1.07 en-cs | 17.81 | 17.33 ( -0.48 ) | 18.69 ( 0.88 ) | 1.37 en-de | 26.13 | 25.16 ( -0.97 ) | 26.76 ( 0.63 ) | 1.60 en-he | 24.15 | 22.73 ( -1.42 ) | 24.42 ( 0.27 ) | 1.69 en-nl | 30.88 | 29.51 ( -1.37 ) | 30.52 ( -0.36 ) | 1.01 en-pt | 37.63 | 35.93 ( -1.70 ) | 37.23 ( -0.40 ) | 1.30 en-ro | 27.23 | 25.68 ( -1.55 ) | 27.11 ( -0.12 ) | 1.42 en-ru | 17.40 | 16.26 ( -1.14 ) | 17.42 ( 0.02 ) | 1.16 en-th | 26.45 | 27.18 ( 0.73 ) | 27.62 ( 1.17 ) | 0.45 en-tr | 12.47 | 11.63 ( -0.84 ) | 12.84 ( 0.37 ) | 1.21 en-vi | 27.88 | 28.04 ( 0.16 ) | 28.69 ( 0.81 ) | 0.65 en-zh | 10.95 | 10.12 ( -0.83 ) | 10.41 ( -0.54 ) | 0.29 -- -- -- -- -- -- Regarding training time The individual models need to be pre-trained , which will incur additional time . According to the training time statistics on IWSLT dataset with NVIDIA V100 GPU , it takes nearly 4 hours to train the individual model with 1 GPU . The total GPU time is 4hours * 12 GPUs for 12 languages . The training time for multilingual baseline is nearly 11hours * 4GPUs , while our method is nearly 13 hours * 4GPUs . Our method only takes extra 2hours * 4GPUs for the multilingual training and 4 hours * 12GPUs for the individual model pre-training . Furthermore , we can assume the individual models are pre-given , which is reasonable because the production system usually wants to adapt the individual translation into multilingual setting , at the benefit of saving maintenance cost while with no accuracy degradation or even with accuracy improvement , which is exactly the goal of this work . 3.Regarding the top-K distillation Yes , we normalize the top K probabilities so that they sum to 1 . We have added the description in the new version . 4.Regarding sequence-level knowledge distillation We have tried sequence-level knowledge distillation . It results in consistently inferior accuracy on all languages compared with word-level knowledge distillation used in our work . The results are listed as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Language | Sequence-level distillation | Word-level distillation en-ar | 12.79 | 13.80 en-cs | 17.01 | 18.69 en-de | 25.89 | 26.76 en-he | 22.92 | 24.42 en-nl | 29.99 | 30.52 en-pt | 36.12 | 37.23 en-ro | 25.75 | 27.11 en-ru | 16.38 | 17.42 en-th | 27.52 | 27.62 en-tr | 11.11 | 12.84 en-vi | 28.08 | 28.69 en-zh | 10.25 | 10.41 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}, "2": {"review_id": "S1gUsoR9YX-2", "review_text": "Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models. Periodically compare the validation BLEU score of the multilingual model with that of each individual model, and turn off distillation for language pairs where the multilingual model is better. On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs. Supplementary experiments justify the strategy of selectively turning off distillation, and quantify the effect using only the top 8 vocabulary items in distillation. The main idea makes sense, and the results are very convincing, especially since it appears that hyper-parameters were not tuned extensively (eg, weight of 0.5 on the distillation loss, for all language pairs). Implementation should be very straightforward, especially with the trick of pre-computing top-k probabilities from the teacher model at each corpus position. One small barrier to practical application that the authors fail to acknowledge is the requirement to train individual models, which will at least double training time compared to a single multilingual model. The main missing experiment is higher-capacity multilingual models, which Johnson et al show to be beneficial in settings with a large number of language pairs. Using a multilingual model of the same (relatively small) size as the individual models as is done here is likely to be suboptimal, especially for the 44-language pair TED setting. A related point is that the corpora used seem to be quite small (eg 4.5M and 1M sentences for WMT Czech and German, respectively, while the available training corpora are closer to 15M and 4.5M). Although performance relative to individual models is still impressive - and seems to be better than than in previous work - this makes the experiments comparing to the multilingual baseline less meaningful. Also missing are experiments on out-of-English translation, which would establish the viability of the proposed technique for many-to-many translation via bridging. Out-of-English is a more difficult problem than into-English. I can\u2019t see any reason the proposed technique wouldn\u2019t also work in this setting, but this remains to be shown. Although it\u2019s great that the technique is shown to work without embellishments, there are a few obvious strategies it would have been interesting to explore, such as making the weight on the distillation loss dependent on the difference in performance between the multilingual and individual models; and allowing for the distillation loss to be turned back on if the performance of the multilingual model starts to drift back down for a particular language pair. I also wondered about the effect of the gradient accumulation strategy in algorithm 1, where individual batches from each language pair are effectively grouped into one giant batch for the purpose of parameter updates. I can see that this could stabilize training, but it would be good to know whether it\u2019s crucial for success, especially when the number of language pairs is large. Further details: As aforementioned -> As mentioned (1) 2nd line: Doesn't make sense as written. You need to distinguish the gold y_t from hypothesized ones in the 1() function. Above (2): is served as -> serves as 3.2 First paragraph. Since D presumably consists of D^l for all languages l, L_ALL(D,...) should be a function of teacher parameters theta^l for all languages l rather than just one as written. In top-K distillation, is the teacher distribution renormalized or simply truncated? Generalization analysis, pg 8: presumably you are sampling from N(0, sigma^2) - this should be described as such. Reference: Johnson et al, \u201cGoogle\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation\u201d TACL, 2017.", "rating": "7: Good paper, accept", "reply_text": "4.Regarding out-of-English translation We have provided the English-to-many translation results on the IWSLT dataset in the table below . The BLEU scores in ( ) represent the difference between the multilingual model and individual models . Delta represents the improvements of our multi-distillation method over the multi-baseline . We can see our method consistently outperforms the multilingual baseline on all languages , and can nearly match or even surpass the accuracy of the individual models , even if one-to-many translation is considered harder than many-to-one translation . We have also updated the results in the paper . We will provide more results in the following days . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Language | Individual | Multilingual-baseline | Multilingual-distill | Delta en-ar | 13.67 | 12.73 ( -0.94 ) | 13.80 ( 0.13 ) | 1.07 en-cs | 17.81 | 17.33 ( -0.48 ) | 18.69 ( 0.88 ) | 1.37 en-de | 26.13 | 25.16 ( -0.97 ) | 26.76 ( 0.63 ) | 1.60 en-he | 24.15 | 22.73 ( -1.42 ) | 24.42 ( 0.27 ) | 1.69 en-nl | 30.88 | 29.51 ( -1.37 ) | 30.52 ( -0.36 ) | 1.01 en-pt | 37.63 | 35.93 ( -1.70 ) | 37.23 ( -0.40 ) | 1.30 en-ro | 27.23 | 25.68 ( -1.55 ) | 27.11 ( -0.12 ) | 1.42 en-ru | 17.40 | 16.26 ( -1.14 ) | 17.42 ( 0.02 ) | 1.16 en-th | 26.45 | 27.18 ( 0.73 ) | 27.62 ( 1.17 ) | 0.45 en-tr | 12.47 | 11.63 ( -0.84 ) | 12.84 ( 0.37 ) | 1.21 en-vi | 27.88 | 28.04 ( 0.16 ) | 28.69 ( 0.81 ) | 0.65 en-zh | 10.95 | 10.12 ( -0.83 ) | 10.41 ( -0.54 ) | 0.29 -- -- -- -- -- -- Regarding the weight on the distillation loss and turning back the distillation loss Thanks for the suggestion . We have provided the results for turning back the distillation loss with a hard threshold in the submitted version . According to the number ( Table 7 in the updated version ) , back distillation improves the accuracy of individual models . We quickly try a simple adaptive weight that changes according to BLEU gap between the teacher and student model : \\lambda = 0.9 * ( 1/2 ) ^ { max ( BLEU_student + 2 - BLEU_teacher , 0 ) } , which means if a student is lower than the teacher by more than 2 BLEU score , the weight is 0.9 . After that , the weight is decayed exponentially . The initial results on IWSLT dataset demonstrate that there is no much difference ( with an average of 0.3 BLEU score ) compared with the constant weight we used in this paper . We will conduct a comprehensive study on this kind of back distillation you mentioned in the future work . 6.Regarding the gradient accumulation strategy We have conducted experiments to analyze if it is critical for the importance of gradient accumulation . We found it is not critical for the model training . We run an experiment on the IWSLT dataset without gradient accumulation , i.e. , updating the model parameters immediately with the training data from a single language . But in order to make sure the update in the two settings has the same batch size , which is a critical hyperparameter for model training , we increase the batch size for the single language by 12 times , to be the same with the batch size in the setting of gradient accumulation . We found the accuracies of the two settings are nearly the same , with 0.3 BLEU higher or lower at most . 7.Regarding the writing and typos We have fixed the typos in the new version . In top-K distillation , the teacher distribution is renormalized ."}}