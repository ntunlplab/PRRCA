{"year": "2017", "forum": "rJq_YBqxx", "title": "Deep Character-Level Neural Machine Translation By Learning Morphology", "decision": "Reject", "meta_review": "This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work. \n \n Pros:\n - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.\n \n Mixed: \n - Some found the paper clear, praising it as a \"well-written paper\", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved\n - Reviewers were also split on results. Some found the results quite \"compelling\" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work\n - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR\n \n Cons:\n - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering. \n - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison.", "reviews": [{"review_id": "rJq_YBqxx-0", "review_text": "Update after reading the authors' responses & the paper revision dated Dec 21: I have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5. The main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch. ----- This is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). Moreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder. One minor comment: annotate h_t in Figure 1. [1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your comments ! Below are the one-to-one responses to the comments . Comment 1 : Comparison with Luong & Manning , 2016 [ 1 ] . Response : We think it is unfair to compare with such a deep model , neither Lee et al , 2016 [ 2 ] nor Chung et al 2016 [ 3 ] compared with Luong & Manning , 2016 [ 1 ] . Besides , our model is much efficient than [ 1 ] , because their model is trained for 3 months ( you could check the training time in their paper ) . However , our En-Cs model is trained for only 15 days on a single NVIDIA TITAN X GPU . We have added the detailed training time in the revision . Thanks for your suggestion ! Comment 2 : The HGRU thing is over-complicated in terms of presentation . Response : It is correct that HGRU is either continue the character decoder or reset using word-level states at boundaries . However , it is tricky when implementing . We have presented the trick ( by using an auxiliary sequence and the matrix R ) of our implementation in the submission . Besides , it is not only more elegant but also more efficient ( trained for only 15 days ) to decode all target words at the character level with HGRU . Please refer to the revision for the training time in Table 1 . Comment 3 : Novelty and specific constraints . Response : We think another major novelty of our work is the word encoder . As you can see , the attention mechanism also does not specifically enforce the model to align on one word , the alignment is learnt from data . Similarly , the morphemes or subword units are learnt from data . Actually , it does provide the nice embedding ( Figure 3 ) and segmentation ( Figure 5 ) . We have tried several constraints like sparsity , but less useful . It is better to model the words as an energy model ( like the attention mechanism ) which is used in the submission . Comment 4 : Annotate h_t in Figure 1 . Response : The h_t is the hidden state of RNN which is omitted in Figure 1 . To clarify we have added it to Figure 1 in the revision . Thanks ! [ 1 ] Minh-Thang Luong and Christopher D. Manning . 2016.Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models . ACL.https : //arxiv.org/pdf/1604.00788v2.pdf [ 2 ] Lee , Jason , Kyunghyun Cho , and Thomas Hofmann . `` Fully Character-Level Neural Machine Translation without Explicit Segmentation . '' arXiv preprint arXiv:1610.03017 ( 2016 ) . [ 3 ] Chung , Junyoung , Kyunghyun Cho , and Yoshua Bengio . `` A character-level decoder without explicit segmentation for neural machine translation . '' arXiv preprint arXiv:1603.06147 ( 2016 ) ."}, {"review_id": "rJq_YBqxx-1", "review_text": " * Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature. * Review: - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers? - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited. * Some Requests: -Can you add the size of the models to the Table 1? - Can you add some of the failure cases of your model, where the model failed to translate correctly? * An Overview of the Review: Pros: - The paper is well written - Extensive analysis of the model on various language pairs - Convincing experimental results. Cons: - The model is complicated. - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty. - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs. [1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your insightful comments . We have added the size of models in Table 1 . We could find that our model is the smallest one to achieve the comparable performance . We also added more translation samples in the appendix . Thanks for your suggestion . Below are the one-to-one responses to the comments . Comment 1 : Mainly an architecture engineering/application paper , not much novelty . Response : We combine 6 RNNs to build this network . We think it is important to build from the basic components for new functionality in deep learning . The famous networks such as ResNet and GoogLeNet are also built from several existing components . Comment 2 : The idea of using hierarchical decoders . Response : It 's true that the idea of using hierarchical decoders have been explored before and we have cited the paper [ 4 ] in the revision . Various hierarchical decoders operate on different level . For example , the hierarchical decoder in [ 4 ] decodes from sentence level to word level . However , none of the prior hierarchical decoders are applicable to NMT because of the training inefficiency . For example , Ling et al , [ 2 ] and Luong et al , [ 3 ] have tried the hierarchical decoder , but both are not efficient . The model of Luong et al , [ 3 ] needs to take 3 months to train the purely character models ( see , Table 1 ) . To our knowledge , our method is the first to make the hierarchical decoder work on NMT . Comment 3 : Learning words representation . Response : We would like to highlight that another major novelty of our work is learning words representation that an ordinary RNN failed to learn ( as shown in Figure 3 and Figure 5 ) . It is not only useful in NMT , but also potentially useful in other NLP areas like language models . Comment 4 : The model is complicated . Response : We admit that it is a little complicated to implement this network using existent deep learning frameworks , thus we make our codes available online . However , in terms of the model size , our model is the simplest and smallest ( we have added the size of models in the revision , thanks for your suggestion . ) . More importantly , we think our system is less complicated compared to [ 1 ] which uses thousands of filters ( their model is much larger than ours ) . It 's more natural and straightforward to using RNNs in this context ( we describe the detailed architecture in Appendix ) . Because of using RNNs , our model is less redundant . Comment 5 : The proposed model is potentially slower . Response : Our model is slower than the word-base models . However , the training efficiency of our model is comparable to Lee et al , [ 1 ] which is also a character-based model ( see , Table 1 ) . Moreover , the character-based models avoid the large vocabulary and the large softmax function at the cost of the training speed . If you have any further comments , please let us know . Thanks . [ 1 ] Lee , Jason , Kyunghyun Cho , and Thomas Hofmann . `` Fully Character-Level Neural Machine Translation without Explicit Segmentation . '' [ 2 ] Ling , Wang , et al . `` Character-based Neural Machine Translation . '' [ 3 ] Minh-Thang Luong and Christopher D. Manning . 2016 . `` Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models . '' [ 4 ] Serban IV , Sordoni A , Bengio Y , Courville A , Pineau J. Hierarchical neural network generative models for movie dialogues . arXiv preprint arXiv:1507.04808 . 2015 Jul 17 ."}, {"review_id": "rJq_YBqxx-2", "review_text": "The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks. The quantitative results are quite good and the qualitative results are quite encouraging. First, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it\u2019s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess. Second, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model, because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model. On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5. To conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your nice comments ! We think our system is less complicated compared to [ 1 ] which uses thousands of filters , and it 's more natural to encode words with RNN . Below are the one-to-one responses to the comments . Comment 1 : Presentation . Response : As Figure 2 suggests , the RNN sentence encoder receives one vector per word . Notation h_t indicates the hidden state of RNN unit , thus we used it in both Subsection 3.1 and 3.2 . We have clarified it in the revision and we will add an Appendix to explain unambiguously how the model works . Thanks for your suggestion . Comment 2 : Training time . Response : En-Cs model is trained for 15 days and Cs-En model is trained for about 22 days , so the training speed is similar to Lee et al , 2016 [ 1 ] . We have added the comparison in Table 1 in the revision . As mentioned in the paper , all the models are trained on a single NVIDIA TITAN X GPU . You could test the code which released on github ( https : //github.com/swordyork/dcnmt ) . Actually , it takes less memory and much easier for back-propagation because of the hierarchical architecture . Comment 3 : Trivial baseline . Response : As mentioned in the introduction , the encoder in Ling et al , 2015 [ 2 ] has encountered some problems ; they just take the last hidden state for each RNN ( C2W model ) and their results are not competitive ( the result in their paper and the result in Table 1 ) . Besides , our previous version ( https : //arxiv.org/pdf/1608.04738.pdf and the corresponding code https : //github.com/swordyork/dcnmt/tree/old-version ) also could be considered as a trivial baseline which takes the last hidden state of each RNN . However , the results are not such competitive ( En-Fr BLEU on newstest2014 is 31.76 , and this version is 32.85 . ) . Thus we have devised such an architecture which encodes the source word and source sentence more detailed . Thanks ! [ 1 ] Lee , Jason , Kyunghyun Cho , and Thomas Hofmann . `` Fully Character-Level Neural Machine Translation without Explicit Segmentation . '' [ 2 ] Ling , Wang , et al . `` Character-based Neural Machine Translation . ''"}], "0": {"review_id": "rJq_YBqxx-0", "review_text": "Update after reading the authors' responses & the paper revision dated Dec 21: I have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5. The main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch. ----- This is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). Moreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder. One minor comment: annotate h_t in Figure 1. [1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your comments ! Below are the one-to-one responses to the comments . Comment 1 : Comparison with Luong & Manning , 2016 [ 1 ] . Response : We think it is unfair to compare with such a deep model , neither Lee et al , 2016 [ 2 ] nor Chung et al 2016 [ 3 ] compared with Luong & Manning , 2016 [ 1 ] . Besides , our model is much efficient than [ 1 ] , because their model is trained for 3 months ( you could check the training time in their paper ) . However , our En-Cs model is trained for only 15 days on a single NVIDIA TITAN X GPU . We have added the detailed training time in the revision . Thanks for your suggestion ! Comment 2 : The HGRU thing is over-complicated in terms of presentation . Response : It is correct that HGRU is either continue the character decoder or reset using word-level states at boundaries . However , it is tricky when implementing . We have presented the trick ( by using an auxiliary sequence and the matrix R ) of our implementation in the submission . Besides , it is not only more elegant but also more efficient ( trained for only 15 days ) to decode all target words at the character level with HGRU . Please refer to the revision for the training time in Table 1 . Comment 3 : Novelty and specific constraints . Response : We think another major novelty of our work is the word encoder . As you can see , the attention mechanism also does not specifically enforce the model to align on one word , the alignment is learnt from data . Similarly , the morphemes or subword units are learnt from data . Actually , it does provide the nice embedding ( Figure 3 ) and segmentation ( Figure 5 ) . We have tried several constraints like sparsity , but less useful . It is better to model the words as an energy model ( like the attention mechanism ) which is used in the submission . Comment 4 : Annotate h_t in Figure 1 . Response : The h_t is the hidden state of RNN which is omitted in Figure 1 . To clarify we have added it to Figure 1 in the revision . Thanks ! [ 1 ] Minh-Thang Luong and Christopher D. Manning . 2016.Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models . ACL.https : //arxiv.org/pdf/1604.00788v2.pdf [ 2 ] Lee , Jason , Kyunghyun Cho , and Thomas Hofmann . `` Fully Character-Level Neural Machine Translation without Explicit Segmentation . '' arXiv preprint arXiv:1610.03017 ( 2016 ) . [ 3 ] Chung , Junyoung , Kyunghyun Cho , and Yoshua Bengio . `` A character-level decoder without explicit segmentation for neural machine translation . '' arXiv preprint arXiv:1603.06147 ( 2016 ) ."}, "1": {"review_id": "rJq_YBqxx-1", "review_text": " * Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature. * Review: - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers? - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited. * Some Requests: -Can you add the size of the models to the Table 1? - Can you add some of the failure cases of your model, where the model failed to translate correctly? * An Overview of the Review: Pros: - The paper is well written - Extensive analysis of the model on various language pairs - Convincing experimental results. Cons: - The model is complicated. - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty. - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs. [1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your insightful comments . We have added the size of models in Table 1 . We could find that our model is the smallest one to achieve the comparable performance . We also added more translation samples in the appendix . Thanks for your suggestion . Below are the one-to-one responses to the comments . Comment 1 : Mainly an architecture engineering/application paper , not much novelty . Response : We combine 6 RNNs to build this network . We think it is important to build from the basic components for new functionality in deep learning . The famous networks such as ResNet and GoogLeNet are also built from several existing components . Comment 2 : The idea of using hierarchical decoders . Response : It 's true that the idea of using hierarchical decoders have been explored before and we have cited the paper [ 4 ] in the revision . Various hierarchical decoders operate on different level . For example , the hierarchical decoder in [ 4 ] decodes from sentence level to word level . However , none of the prior hierarchical decoders are applicable to NMT because of the training inefficiency . For example , Ling et al , [ 2 ] and Luong et al , [ 3 ] have tried the hierarchical decoder , but both are not efficient . The model of Luong et al , [ 3 ] needs to take 3 months to train the purely character models ( see , Table 1 ) . To our knowledge , our method is the first to make the hierarchical decoder work on NMT . Comment 3 : Learning words representation . Response : We would like to highlight that another major novelty of our work is learning words representation that an ordinary RNN failed to learn ( as shown in Figure 3 and Figure 5 ) . It is not only useful in NMT , but also potentially useful in other NLP areas like language models . Comment 4 : The model is complicated . Response : We admit that it is a little complicated to implement this network using existent deep learning frameworks , thus we make our codes available online . However , in terms of the model size , our model is the simplest and smallest ( we have added the size of models in the revision , thanks for your suggestion . ) . More importantly , we think our system is less complicated compared to [ 1 ] which uses thousands of filters ( their model is much larger than ours ) . It 's more natural and straightforward to using RNNs in this context ( we describe the detailed architecture in Appendix ) . Because of using RNNs , our model is less redundant . Comment 5 : The proposed model is potentially slower . Response : Our model is slower than the word-base models . However , the training efficiency of our model is comparable to Lee et al , [ 1 ] which is also a character-based model ( see , Table 1 ) . Moreover , the character-based models avoid the large vocabulary and the large softmax function at the cost of the training speed . If you have any further comments , please let us know . Thanks . [ 1 ] Lee , Jason , Kyunghyun Cho , and Thomas Hofmann . `` Fully Character-Level Neural Machine Translation without Explicit Segmentation . '' [ 2 ] Ling , Wang , et al . `` Character-based Neural Machine Translation . '' [ 3 ] Minh-Thang Luong and Christopher D. Manning . 2016 . `` Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models . '' [ 4 ] Serban IV , Sordoni A , Bengio Y , Courville A , Pineau J. Hierarchical neural network generative models for movie dialogues . arXiv preprint arXiv:1507.04808 . 2015 Jul 17 ."}, "2": {"review_id": "rJq_YBqxx-2", "review_text": "The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks. The quantitative results are quite good and the qualitative results are quite encouraging. First, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it\u2019s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess. Second, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model, because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model. On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5. To conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your nice comments ! We think our system is less complicated compared to [ 1 ] which uses thousands of filters , and it 's more natural to encode words with RNN . Below are the one-to-one responses to the comments . Comment 1 : Presentation . Response : As Figure 2 suggests , the RNN sentence encoder receives one vector per word . Notation h_t indicates the hidden state of RNN unit , thus we used it in both Subsection 3.1 and 3.2 . We have clarified it in the revision and we will add an Appendix to explain unambiguously how the model works . Thanks for your suggestion . Comment 2 : Training time . Response : En-Cs model is trained for 15 days and Cs-En model is trained for about 22 days , so the training speed is similar to Lee et al , 2016 [ 1 ] . We have added the comparison in Table 1 in the revision . As mentioned in the paper , all the models are trained on a single NVIDIA TITAN X GPU . You could test the code which released on github ( https : //github.com/swordyork/dcnmt ) . Actually , it takes less memory and much easier for back-propagation because of the hierarchical architecture . Comment 3 : Trivial baseline . Response : As mentioned in the introduction , the encoder in Ling et al , 2015 [ 2 ] has encountered some problems ; they just take the last hidden state for each RNN ( C2W model ) and their results are not competitive ( the result in their paper and the result in Table 1 ) . Besides , our previous version ( https : //arxiv.org/pdf/1608.04738.pdf and the corresponding code https : //github.com/swordyork/dcnmt/tree/old-version ) also could be considered as a trivial baseline which takes the last hidden state of each RNN . However , the results are not such competitive ( En-Fr BLEU on newstest2014 is 31.76 , and this version is 32.85 . ) . Thus we have devised such an architecture which encodes the source word and source sentence more detailed . Thanks ! [ 1 ] Lee , Jason , Kyunghyun Cho , and Thomas Hofmann . `` Fully Character-Level Neural Machine Translation without Explicit Segmentation . '' [ 2 ] Ling , Wang , et al . `` Character-based Neural Machine Translation . ''"}}