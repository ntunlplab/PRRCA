{"year": "2021", "forum": "yoVo1fThmS1", "title": "Novelty Detection via Robust Variational Autoencoding", "decision": "Reject", "meta_review": "The paper proposes a novelty detection method when training data is itself noisy. A VAE-based approach is developed that promotes robustness of the VAE. The paper assumes that the encoder a two-component Gaussian mixture distribution, individual components denoting inliers and outliers.\n\nThe paper hopes that the posterior of the inliers (normal data points) can be represented by a low-rank covariance matrix, while the outliers need a full covariance. Another notable modification is that the Wasserstein-1 regularization is used to replace the KL-regularization in the ELBO, which is claimed to be more suitable to the low-rank modeling. \n\nWhile this is a relevant problem, and the idea is perhaps interesting, some concerns have been raised.\n* The details how to fit the model with the desired mixture posterior in practice is unclear.\n* The arguments of section 3 to illustrate the superiority of Wasserstein were found unconvincing, with limiting/unclear assumptions\n* The ultra-low latent space dimension (2) is not sufficiently justified \n* The experimental section and the selected datasets are small scale, it would be good to include a free larger scale datasets (at least cifar10).\n* Comparisons to the open set recognition, or out-of-distribution (OOD) detection would have been a plus.\n\nOverall, this is an OK paper but not yet of sufficient quality.", "reviews": [{"review_id": "yoVo1fThmS1-0", "review_text": "This paper proposes a robust novelty detection method ( `` MAW '' ) to model the distribution of the training data in the presence of high fraction ( corruption ratios up to 30\\ % ) of outliers . The method add new features to the variational autoencoder ( VAE ) , to detect and isolate the outlier so that the learned distribution only represent the inlier distribution : 1 . Uses a carefully designed dimension reduction component to extracts latent lower-dimensional features of the latent distribution . 2.Model the distribution of latent representation as a mixture of Gaussian low-rank inliers and full-rank outliers , both using full covariances instead of diagonal covariances as commonly used in previous VAE-based methods for novelty detection . 3.Penalizes the Wasserstein-1 distance between the data distribution and the latent distribution from the prior distribution . Under a special setting , it theoretically proves that using the Wasserstein-1 metric for regularization yields outliers-robust estimation and is suitable to the low-rank modeling of inliers , while the commonly used Kullback-Leibler ( KL ) divergence does not . 4.Using the least absolute deviation error for reconstruction . Experiments on popular anomaly detection datasets demonstrate state-of-the-art results of MAW on standard benchmarks for novelty detection . After rebuttal Several typos : In Figure 1 , to be consistent the X and Z may be written in lower case . In Eq . ( 6 ) , the transpose should be applied to the right $ U_1^ { ( i ) } $ instead of the left one . In Appendix C.1 , `` It seems that MAW seems to learn '' should be `` It seems that MAW learns '' or `` MAW seems to learn '' . In Appendix D.3 : In line 2 above Proposition D.2 , `` the ill-posedness of ( 11 ) with $ \\mathcal { R } = W_2 $ '' : The $ \\mathcal { R } = W_2 $ should be $ \\mathcal { R } = KL $ . Please rephrase `` the KL divergence fails is unsuitable for low-rank covariance modeling '' , by e.g. , removing `` fails '' or inserting `` and '' between `` fails is '' .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank and appreciate your encouraging review ."}, {"review_id": "yoVo1fThmS1-1", "review_text": "The paper seeks to address the problem of novelty detection under the circumstance of having high corruptions in the training data . This is different from most previous work , which often assumes that training dataset is pure . To address this issue , a VAE-based approach is adopted in this paper , with several modifications made to the vanilla VAE to promote the robustness of VAE in detecting outliers in the corruption circumstance . Among the modifications , the paper assumes the posterior is approximated by a two-component Gaussian mixture distribution , with each having a low-rank and full-rank covariance matrix , respectively . The paper hopes that the posterior of inliers ( normal data points ) can be represented by the low-rank covariance matrix , while that of outliers can not . Another notable modification is that the Wasserstein-1 regularization is used to replace the KL-regularization in the ELBO , which is claimed to be more suitable to the low-rank modeling . Some experiments are conducted to evaluate the outlier detection performance of the proposed method under corrupted circumstance . Strength : 1 . The problem of detecting outliers under highly corrupted environment is of practical importance and is not investigated under the extremely corrupted circumstance . 2.The idea of proposing to use low-rank and full-rank geometry characteristics to separate inliers and outliers is interesting . Weakness : 1 . Although the idea of using low/full-rank geometric characteristic to detect outliers is interesting , the paper barely states how to implement this idea , that is , how to enforce the inliers resides in the low-rank covariance matrix , while the outlier will not . Without any specific design , we can not believe this will be realized automatically . 2.The theoretical result ( Section 3 ) established to argue the superiority of Wasserstein regularization in robustness is better than the original KL regularization in vanilla VAE is not convincing , or is not practically meaningful . That is because the result only reveals that under a very special case , the Wasserstein may learn the true distribution under corrupted environment . We can not see this result has any implication under a broader or more general circumstance . 3.I guess the true reason to choose W_1 over KL distance is that because a Gaussian mixture posterior is employed , the KL can not be evaluated in close-form . By resorting to W_1 distance , the distance can be estimated with the samples , but the KL distance can not be estimated in this way . Overall , I can not buy the argument that the authors proposed to use the W_1 distance/regularization here . 4.I also have some concerns over the choosing of hyper-parameters . The paper sets the dimension of latent representation ( d ) to be 2 , which , I think , is too small . But if it is set to be an appropriate value ( e.g.100 ) , the posterior with a full-rank covariance matrix will be computationally expensive . Moreover , I also think that it is not a good idea to set the mixture coefficient $ \\eta $ to be a fixed value . Generally , this should be learned from data , because you can not know the ratio of different components in advance . 5.I also have some doubts over the experiment settings . At the training stage , the training data is corrupted by a fraction of outliers . But if the data is corrupted by \u2018 outliers \u2019 , that means the model has already made use of the information of outliers . So , if the ratio of outlier in the training dataset is higher , it may be more favorable to the proposed method . For example , if the ratio is large , e.g.c=0.3 , this problem under this setting is more like a clustering problem . So , this kind of settings may be not fair to the comparing methods . Maybe , the better setting should be let the training data corrupted by some data that is not in both the training and testing datasets .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q1.Although the idea of using low/full-rank geometric characteristic to detect outliers is interesting , the paper barely states how to implement this idea , that is , how to enforce the inliers resides in the low-rank covariance matrix , while the outlier will not . Without any specific design , we can not believe this will be realized automatically . Response : Since we do not have labels for the training set , we can not supervisedly learn the two components of the mixture model . However , the use of two robust losses ( least absolute deviation and the $ W_1 $ distance ) helps obtain a careful model for the inliers , which is robust to outliers . Note that in our testing we only use the model for the inliers . So the special \u201c design \u201d is the use of robust losses and we can not foresee any other way of \u201c enforcement \u201d without having labels for the training set . In order to further explain some intuition of the mechanism of MAW , let us assume that the inliers are sampled from a distribution on a low-dimensional manifold that can be encoded by a Gaussian on a low-dimensional latent space by using a certain mapping . We assume further that the outliers are arbitrary , but the percentage of any subgroup of them , that can be sampled from a similar kind of a distribution , is smaller than that of the inliers . Given this assumption and considering the latent space , MAW aims to model the mixture component of the inliers as a Gaussian with low-rank covariance ( and that of the outliers as a Gaussian with full-rank covariance ) . In order to provide some technical intuition why this happens in practice and how the robust functions help with this , we consider three complementary cases , where either the inlier model has a full rank covariance or the outlier component has a low-rank covariance and we explain why they are unlikely to occur . In the first case , the inliers and outliers are both modeled ( in the latent space ) by Gaussian distributions with low-rank covariances . In this case , the W1 distance is minimized over a smaller set and thus the loss is increased . We remark that one may not use the KL divergence instead of W1 as we show in our theory that the KL divergence can not be applied to our low-rank modeling paradigm ( see ill-posedness in Proposition D.2 and its proof ) . In the second case , both the inliers and outliers are modeled ( in the latent space ) with full-rank Gaussians . In this case it is most likely that the minimizer for the inliers will be full-rank , and thus due to the assumed low-dimensional structure of the inliers , it will result in an increase of the reconstruction error . In the third case , the inliers are modeled ( in the latent space ) by a Gaussian with full-rank covariance and the outliers are modeled ( in the latent space ) by a Gaussian with a low-rank covariance . In this case , the L1-distance in the reconstruction loss will increase the reconstruction loss . The increase due to the L1-distance is more significant than due to the L2-distance . We hope that these more technical ideas help explain why there is a hidden mechanism that makes things work . Q2.The theoretical result ( Section 3 ) established to argue the superiority of Wasserstein regularization in robustness is better than the original KL regularization in vanilla VAE is not convincing , or is not practically meaningful . That is because the result only reveals that under a very special case , the Wasserstein may learn the true distribution under corrupted environment . We can not see this result has any implication under a broader or more general circumstance . Response : We don \u2019 t agree that our theory is not practically meaningful . It is a common practice by researchers who care about theory to formulate a special setting under which some guarantees can be provided for a special setting . Such guarantees may highlight some interesting features of the complicated process . In our case , the interesting observations gained by the theory is that minimizing the Wasserstein distance is more robust than minimizing the KL-divergence . Furthermore , the theory shows that the KL divergence is not suitable for a low-rank covariance of any of the mixture components . We are not sure what is the main objection to our model and theory . Nevertheless , we comment that our epsilon-separation assumption just assures that the two means of the inlier Gaussian and the outlier Gaussian can not be too close together . Since it may seem restrictive , we replaced it with the equivalent constraint that the distance of the two means is greater than or equal to epsilon ( instead of exactly equal to epsilon ) . Anyway , it is truly challenging to further improve our theory . The current proofs are nontrivial and required some effort and we are sorry that such an effort was not appreciated by the reviewer . We are not aware of a stronger theory in this specific area ."}, {"review_id": "yoVo1fThmS1-2", "review_text": "Compared with conventional VAE , this paper incorporates the following strategies to improve VAE for novelty detection from corrupted training data . 1 ) By considering that outliers tend to have more complex structures , this paper assumes inliers and ourliers lie on Gaussian distributions with different ranks , and proposes a Gaussian mixture model for posterior q ( z|x ) including two component : a low-rank multivariate Gaussian distribution component for modeling inliers and a full-rank multivariate Gaussian distribution component for outliers . 2 ) Applying the Wasserstein distance ( W distance ) between q ( z|x ) and p ( z ) instead of the KL divergence in the original VAE . The paper also proves the superiority of using W distance over KL divergence , by showing that the minimized value exists in W distance between a Gaussian mixture distribution q ( z|x ) and Gaussian prior p ( z ) . 3 ) 3 ) For Rrconstruction loss , this model adopts ||x-D ( z ) ||_2 as described in ( 3 ) instead of ||x-D ( z ) ||^2_2 , which is used in conventional VAE . This practice helps to alleviate the problem that the loss item in conventional VAE being too small when data point deviates from the center of the Gaussian distribution . The experiments show that proposed method achieves good result among 4 datasets , which domonstrate the effectiveness of the three strategies . Advantages of this paper : although Gaussian mixture model has been used in previous work of VAE , this model innovatively propose Gaussian components with different ranks for modeling inlier and outliers to improve the performance for training data with corrupted samples . This work also gives a theoretical guarantee for the advantage of W distance minimization in given setting . Potential drawbacks : Problem setting is confusing . Outliers/novelties are supposed to be rare in the dataset , while the paper assumes that a nontrivial fraction of outliers existing in the data . In addition , more recent work is not compared , such as Ruff , L. , Vandermeulen , R. A. , G\u00f6rnitz , N. , Binder , A. , M\u00fcller , E. , M\u00fcller , K. R. , Kloft , M. ( 2019 ) . Deep semi-supervised anomaly detection . arXiv preprint arXiv:1906.02694", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q1.Problem setting is confusing . Outliers/novelties are supposed to be rare in the dataset , while the paper assumes that a nontrivial fraction of outliers existing in the data . Response : When having sufficient experience and expertise on a carefully studied area and sufficiently precise tools to collect data , then one can produce training sets with few outliers . However , there are at least two important scenarios , where our method is needed . The first scenario includes new areas of studies , where it is unclear how to distinguish between normal and abnormal points . For example , in the beginning of the COVID-19 pandemic it was hard to diagnose COVID-19 patients and distinguish them from the rest of patients , for example , patients with flu . Therefore , if you focus on the set of COVID-19 positive patients and designate them as inliers , then you can expect a nontrivial portion of outliers , among the admitted patients . Another scenario occurs when it is very hard to make precise measurements . This scenario occurs in some image processing/computer vision tasks of cryogenic electron microscopy ( cryo-EM ) , which aims at reconstructing 3D structures of biological molecules at near-atomic resolution from their 2D projection images . Such reconstruction needs to start with the removal of outliers . However , in practice , there might be a huge portion of 2D images that are outliers ( outlier ratios may up to 40 % or more ) . These anomalous images may come from the damage of the biomolecules themselves or from the unrelated information ( e.g.micrographic noise produced by cameras or micro water drop or ice on the particles ) . Due to the limited methods for outlier detection with large corruption , researchers usually need to manually pick out anomalous images . However , this is laborious and error-prone , since the scale is too micro to be visible . We thus believe that our setting with non-trivial corruption ( e.g.training outlier ratio up to 0.3 ) is valuable to the practitioner in certain situations . Also note that even in the case where the outlier ratio is as small ( e.g. , 10 % ) , our method still generally outperforms other benchmarks ( see Figs.2 and 3 for example ) ."}, {"review_id": "yoVo1fThmS1-3", "review_text": "This study proposes a novel method that can work well even the training data is corrupted by partial data from the unknown domain . Though it deals with the well-known problem called 'Noisy data/label ' , its approach is not the same thing as the previous works as it focuses on variational autoencoder on the task of novelty detection . And its arguments and statistical assumptions are followed by mathematical proofs . Overall , it is an interesting approach and I believe it would give a good way to ML practitioners who are struggling with noisy datasets in real-world applications . However , there some questions/comments about the article which may make the study more consolidate : Questions - In the description of the proposed method , MAW , Discriminator generates Loss ( Lw1 ) by comparing between Zgen and Zhyp . And Zhyp is unimodal distribution while Zgen follows MoG . I wonder whether there is a risk that inlier and outlier distributions are mixed ( combined ) as the loss makes the generator generates just the same mu/sigma regardless of the domains . If so , is there any equilibrium trick required so that the generator would not be strong too much ? - Though it is hard to pre-estimate how the outlier distribution looks like , it is more common to assume the outlier distribution has multi-modal than uni-modal . However , the proposed method approximates the outlier distribution as unimodal Gaussian distribution . Is it possible to model the outliers as multi-modal distribution such as MoG ? - In the experiment with the multiclass dataset , the number of possible inlier domains is the same as the number of classes in the dataset . And the characteristic of 'training data ' may be different by each combination . I wonder the experiment of this study covered all possible sets . And the corrupted data is sampled randomly from the other classes . Is there any deviation in the performance by each sampling ? - This study aims to generate the model to be robust to corrupted training data . However , in the result , it is not clear that the proposed method is more robust than others as the AUC/AP from MAW falls ( maybe greater than others ) as the outlier ratio increases . The authors may give explanations about the result in detail . Additional Comments - The readability of Figure 2 , 3 is not good . How about showing them on the tables ? - This study shows the superiority from four datasets ( image , non-image ) . However , there is more dataset widely used for novelty detection such as ( Fashion ) MNIST or MVTech . The authors may consider doing the same experiments on the other dataset . -The authors may compare the method not only to the novelty detection methods , but many previous works which also aims to be robust to noisy data ( or label ) in the training process .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1.In the description of the proposed method , MAW , Discriminator generates Loss ( Lw1 ) by comparing between Zgen and Zhyp . And Zhyp is unimodal distribution while Zgen follows MoG . I wonder whether there is a risk that inlier and outlier distributions are mixed ( combined ) as the loss makes the generator generates just the same mu/sigma regardless of the domains . If so , is there any equilibrium trick required so that the generator would not be strong too much ? Response : We don \u2019 t think our model prefers a case where both Z \u2019 s have the same means . The inliers and outliers should have different structures . If their latent distributions are close , then the set $ \\cal { Q } $ of distributions q over which we minimize is significantly smaller and this should lead to a larger W1 error and thus larger reconstruction error . We thus believe that the minimization of the reconstruction error avoids this case where the latent distributions of the inliers and outliers are too close . Furthermore , in Figure 4 we tested the case where we have a single Gaussian ( it is reported in this figure as MAW-single Gaussian and compared among other variants with MAW ) . Note that the reported performance of the use of a single Gaussian is worse than that of MAW . Q2.Though it is hard to pre-estimate how the outlier distribution looks like , it is more common to assume the outlier distribution has multi-modal than uni-modal . However , the proposed method approximates the outlier distribution as unimodal Gaussian distribution . Is it possible to model the outliers as multi-modal distribution such as MoG ? Response : While it is possible to model the outliers as a mixture of Gaussians , however , we point out 3 disadvantages of this approach . First of all , the outlier distribution is not important in our work , unlike the inlier distribution . Second of all , the model for MoG of outliers is more complex and the number of mixture components is unclear . Third of all , there is no clear loss for discriminating between the classes of outliers ."}], "0": {"review_id": "yoVo1fThmS1-0", "review_text": "This paper proposes a robust novelty detection method ( `` MAW '' ) to model the distribution of the training data in the presence of high fraction ( corruption ratios up to 30\\ % ) of outliers . The method add new features to the variational autoencoder ( VAE ) , to detect and isolate the outlier so that the learned distribution only represent the inlier distribution : 1 . Uses a carefully designed dimension reduction component to extracts latent lower-dimensional features of the latent distribution . 2.Model the distribution of latent representation as a mixture of Gaussian low-rank inliers and full-rank outliers , both using full covariances instead of diagonal covariances as commonly used in previous VAE-based methods for novelty detection . 3.Penalizes the Wasserstein-1 distance between the data distribution and the latent distribution from the prior distribution . Under a special setting , it theoretically proves that using the Wasserstein-1 metric for regularization yields outliers-robust estimation and is suitable to the low-rank modeling of inliers , while the commonly used Kullback-Leibler ( KL ) divergence does not . 4.Using the least absolute deviation error for reconstruction . Experiments on popular anomaly detection datasets demonstrate state-of-the-art results of MAW on standard benchmarks for novelty detection . After rebuttal Several typos : In Figure 1 , to be consistent the X and Z may be written in lower case . In Eq . ( 6 ) , the transpose should be applied to the right $ U_1^ { ( i ) } $ instead of the left one . In Appendix C.1 , `` It seems that MAW seems to learn '' should be `` It seems that MAW learns '' or `` MAW seems to learn '' . In Appendix D.3 : In line 2 above Proposition D.2 , `` the ill-posedness of ( 11 ) with $ \\mathcal { R } = W_2 $ '' : The $ \\mathcal { R } = W_2 $ should be $ \\mathcal { R } = KL $ . Please rephrase `` the KL divergence fails is unsuitable for low-rank covariance modeling '' , by e.g. , removing `` fails '' or inserting `` and '' between `` fails is '' .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank and appreciate your encouraging review ."}, "1": {"review_id": "yoVo1fThmS1-1", "review_text": "The paper seeks to address the problem of novelty detection under the circumstance of having high corruptions in the training data . This is different from most previous work , which often assumes that training dataset is pure . To address this issue , a VAE-based approach is adopted in this paper , with several modifications made to the vanilla VAE to promote the robustness of VAE in detecting outliers in the corruption circumstance . Among the modifications , the paper assumes the posterior is approximated by a two-component Gaussian mixture distribution , with each having a low-rank and full-rank covariance matrix , respectively . The paper hopes that the posterior of inliers ( normal data points ) can be represented by the low-rank covariance matrix , while that of outliers can not . Another notable modification is that the Wasserstein-1 regularization is used to replace the KL-regularization in the ELBO , which is claimed to be more suitable to the low-rank modeling . Some experiments are conducted to evaluate the outlier detection performance of the proposed method under corrupted circumstance . Strength : 1 . The problem of detecting outliers under highly corrupted environment is of practical importance and is not investigated under the extremely corrupted circumstance . 2.The idea of proposing to use low-rank and full-rank geometry characteristics to separate inliers and outliers is interesting . Weakness : 1 . Although the idea of using low/full-rank geometric characteristic to detect outliers is interesting , the paper barely states how to implement this idea , that is , how to enforce the inliers resides in the low-rank covariance matrix , while the outlier will not . Without any specific design , we can not believe this will be realized automatically . 2.The theoretical result ( Section 3 ) established to argue the superiority of Wasserstein regularization in robustness is better than the original KL regularization in vanilla VAE is not convincing , or is not practically meaningful . That is because the result only reveals that under a very special case , the Wasserstein may learn the true distribution under corrupted environment . We can not see this result has any implication under a broader or more general circumstance . 3.I guess the true reason to choose W_1 over KL distance is that because a Gaussian mixture posterior is employed , the KL can not be evaluated in close-form . By resorting to W_1 distance , the distance can be estimated with the samples , but the KL distance can not be estimated in this way . Overall , I can not buy the argument that the authors proposed to use the W_1 distance/regularization here . 4.I also have some concerns over the choosing of hyper-parameters . The paper sets the dimension of latent representation ( d ) to be 2 , which , I think , is too small . But if it is set to be an appropriate value ( e.g.100 ) , the posterior with a full-rank covariance matrix will be computationally expensive . Moreover , I also think that it is not a good idea to set the mixture coefficient $ \\eta $ to be a fixed value . Generally , this should be learned from data , because you can not know the ratio of different components in advance . 5.I also have some doubts over the experiment settings . At the training stage , the training data is corrupted by a fraction of outliers . But if the data is corrupted by \u2018 outliers \u2019 , that means the model has already made use of the information of outliers . So , if the ratio of outlier in the training dataset is higher , it may be more favorable to the proposed method . For example , if the ratio is large , e.g.c=0.3 , this problem under this setting is more like a clustering problem . So , this kind of settings may be not fair to the comparing methods . Maybe , the better setting should be let the training data corrupted by some data that is not in both the training and testing datasets .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q1.Although the idea of using low/full-rank geometric characteristic to detect outliers is interesting , the paper barely states how to implement this idea , that is , how to enforce the inliers resides in the low-rank covariance matrix , while the outlier will not . Without any specific design , we can not believe this will be realized automatically . Response : Since we do not have labels for the training set , we can not supervisedly learn the two components of the mixture model . However , the use of two robust losses ( least absolute deviation and the $ W_1 $ distance ) helps obtain a careful model for the inliers , which is robust to outliers . Note that in our testing we only use the model for the inliers . So the special \u201c design \u201d is the use of robust losses and we can not foresee any other way of \u201c enforcement \u201d without having labels for the training set . In order to further explain some intuition of the mechanism of MAW , let us assume that the inliers are sampled from a distribution on a low-dimensional manifold that can be encoded by a Gaussian on a low-dimensional latent space by using a certain mapping . We assume further that the outliers are arbitrary , but the percentage of any subgroup of them , that can be sampled from a similar kind of a distribution , is smaller than that of the inliers . Given this assumption and considering the latent space , MAW aims to model the mixture component of the inliers as a Gaussian with low-rank covariance ( and that of the outliers as a Gaussian with full-rank covariance ) . In order to provide some technical intuition why this happens in practice and how the robust functions help with this , we consider three complementary cases , where either the inlier model has a full rank covariance or the outlier component has a low-rank covariance and we explain why they are unlikely to occur . In the first case , the inliers and outliers are both modeled ( in the latent space ) by Gaussian distributions with low-rank covariances . In this case , the W1 distance is minimized over a smaller set and thus the loss is increased . We remark that one may not use the KL divergence instead of W1 as we show in our theory that the KL divergence can not be applied to our low-rank modeling paradigm ( see ill-posedness in Proposition D.2 and its proof ) . In the second case , both the inliers and outliers are modeled ( in the latent space ) with full-rank Gaussians . In this case it is most likely that the minimizer for the inliers will be full-rank , and thus due to the assumed low-dimensional structure of the inliers , it will result in an increase of the reconstruction error . In the third case , the inliers are modeled ( in the latent space ) by a Gaussian with full-rank covariance and the outliers are modeled ( in the latent space ) by a Gaussian with a low-rank covariance . In this case , the L1-distance in the reconstruction loss will increase the reconstruction loss . The increase due to the L1-distance is more significant than due to the L2-distance . We hope that these more technical ideas help explain why there is a hidden mechanism that makes things work . Q2.The theoretical result ( Section 3 ) established to argue the superiority of Wasserstein regularization in robustness is better than the original KL regularization in vanilla VAE is not convincing , or is not practically meaningful . That is because the result only reveals that under a very special case , the Wasserstein may learn the true distribution under corrupted environment . We can not see this result has any implication under a broader or more general circumstance . Response : We don \u2019 t agree that our theory is not practically meaningful . It is a common practice by researchers who care about theory to formulate a special setting under which some guarantees can be provided for a special setting . Such guarantees may highlight some interesting features of the complicated process . In our case , the interesting observations gained by the theory is that minimizing the Wasserstein distance is more robust than minimizing the KL-divergence . Furthermore , the theory shows that the KL divergence is not suitable for a low-rank covariance of any of the mixture components . We are not sure what is the main objection to our model and theory . Nevertheless , we comment that our epsilon-separation assumption just assures that the two means of the inlier Gaussian and the outlier Gaussian can not be too close together . Since it may seem restrictive , we replaced it with the equivalent constraint that the distance of the two means is greater than or equal to epsilon ( instead of exactly equal to epsilon ) . Anyway , it is truly challenging to further improve our theory . The current proofs are nontrivial and required some effort and we are sorry that such an effort was not appreciated by the reviewer . We are not aware of a stronger theory in this specific area ."}, "2": {"review_id": "yoVo1fThmS1-2", "review_text": "Compared with conventional VAE , this paper incorporates the following strategies to improve VAE for novelty detection from corrupted training data . 1 ) By considering that outliers tend to have more complex structures , this paper assumes inliers and ourliers lie on Gaussian distributions with different ranks , and proposes a Gaussian mixture model for posterior q ( z|x ) including two component : a low-rank multivariate Gaussian distribution component for modeling inliers and a full-rank multivariate Gaussian distribution component for outliers . 2 ) Applying the Wasserstein distance ( W distance ) between q ( z|x ) and p ( z ) instead of the KL divergence in the original VAE . The paper also proves the superiority of using W distance over KL divergence , by showing that the minimized value exists in W distance between a Gaussian mixture distribution q ( z|x ) and Gaussian prior p ( z ) . 3 ) 3 ) For Rrconstruction loss , this model adopts ||x-D ( z ) ||_2 as described in ( 3 ) instead of ||x-D ( z ) ||^2_2 , which is used in conventional VAE . This practice helps to alleviate the problem that the loss item in conventional VAE being too small when data point deviates from the center of the Gaussian distribution . The experiments show that proposed method achieves good result among 4 datasets , which domonstrate the effectiveness of the three strategies . Advantages of this paper : although Gaussian mixture model has been used in previous work of VAE , this model innovatively propose Gaussian components with different ranks for modeling inlier and outliers to improve the performance for training data with corrupted samples . This work also gives a theoretical guarantee for the advantage of W distance minimization in given setting . Potential drawbacks : Problem setting is confusing . Outliers/novelties are supposed to be rare in the dataset , while the paper assumes that a nontrivial fraction of outliers existing in the data . In addition , more recent work is not compared , such as Ruff , L. , Vandermeulen , R. A. , G\u00f6rnitz , N. , Binder , A. , M\u00fcller , E. , M\u00fcller , K. R. , Kloft , M. ( 2019 ) . Deep semi-supervised anomaly detection . arXiv preprint arXiv:1906.02694", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q1.Problem setting is confusing . Outliers/novelties are supposed to be rare in the dataset , while the paper assumes that a nontrivial fraction of outliers existing in the data . Response : When having sufficient experience and expertise on a carefully studied area and sufficiently precise tools to collect data , then one can produce training sets with few outliers . However , there are at least two important scenarios , where our method is needed . The first scenario includes new areas of studies , where it is unclear how to distinguish between normal and abnormal points . For example , in the beginning of the COVID-19 pandemic it was hard to diagnose COVID-19 patients and distinguish them from the rest of patients , for example , patients with flu . Therefore , if you focus on the set of COVID-19 positive patients and designate them as inliers , then you can expect a nontrivial portion of outliers , among the admitted patients . Another scenario occurs when it is very hard to make precise measurements . This scenario occurs in some image processing/computer vision tasks of cryogenic electron microscopy ( cryo-EM ) , which aims at reconstructing 3D structures of biological molecules at near-atomic resolution from their 2D projection images . Such reconstruction needs to start with the removal of outliers . However , in practice , there might be a huge portion of 2D images that are outliers ( outlier ratios may up to 40 % or more ) . These anomalous images may come from the damage of the biomolecules themselves or from the unrelated information ( e.g.micrographic noise produced by cameras or micro water drop or ice on the particles ) . Due to the limited methods for outlier detection with large corruption , researchers usually need to manually pick out anomalous images . However , this is laborious and error-prone , since the scale is too micro to be visible . We thus believe that our setting with non-trivial corruption ( e.g.training outlier ratio up to 0.3 ) is valuable to the practitioner in certain situations . Also note that even in the case where the outlier ratio is as small ( e.g. , 10 % ) , our method still generally outperforms other benchmarks ( see Figs.2 and 3 for example ) ."}, "3": {"review_id": "yoVo1fThmS1-3", "review_text": "This study proposes a novel method that can work well even the training data is corrupted by partial data from the unknown domain . Though it deals with the well-known problem called 'Noisy data/label ' , its approach is not the same thing as the previous works as it focuses on variational autoencoder on the task of novelty detection . And its arguments and statistical assumptions are followed by mathematical proofs . Overall , it is an interesting approach and I believe it would give a good way to ML practitioners who are struggling with noisy datasets in real-world applications . However , there some questions/comments about the article which may make the study more consolidate : Questions - In the description of the proposed method , MAW , Discriminator generates Loss ( Lw1 ) by comparing between Zgen and Zhyp . And Zhyp is unimodal distribution while Zgen follows MoG . I wonder whether there is a risk that inlier and outlier distributions are mixed ( combined ) as the loss makes the generator generates just the same mu/sigma regardless of the domains . If so , is there any equilibrium trick required so that the generator would not be strong too much ? - Though it is hard to pre-estimate how the outlier distribution looks like , it is more common to assume the outlier distribution has multi-modal than uni-modal . However , the proposed method approximates the outlier distribution as unimodal Gaussian distribution . Is it possible to model the outliers as multi-modal distribution such as MoG ? - In the experiment with the multiclass dataset , the number of possible inlier domains is the same as the number of classes in the dataset . And the characteristic of 'training data ' may be different by each combination . I wonder the experiment of this study covered all possible sets . And the corrupted data is sampled randomly from the other classes . Is there any deviation in the performance by each sampling ? - This study aims to generate the model to be robust to corrupted training data . However , in the result , it is not clear that the proposed method is more robust than others as the AUC/AP from MAW falls ( maybe greater than others ) as the outlier ratio increases . The authors may give explanations about the result in detail . Additional Comments - The readability of Figure 2 , 3 is not good . How about showing them on the tables ? - This study shows the superiority from four datasets ( image , non-image ) . However , there is more dataset widely used for novelty detection such as ( Fashion ) MNIST or MVTech . The authors may consider doing the same experiments on the other dataset . -The authors may compare the method not only to the novelty detection methods , but many previous works which also aims to be robust to noisy data ( or label ) in the training process .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1.In the description of the proposed method , MAW , Discriminator generates Loss ( Lw1 ) by comparing between Zgen and Zhyp . And Zhyp is unimodal distribution while Zgen follows MoG . I wonder whether there is a risk that inlier and outlier distributions are mixed ( combined ) as the loss makes the generator generates just the same mu/sigma regardless of the domains . If so , is there any equilibrium trick required so that the generator would not be strong too much ? Response : We don \u2019 t think our model prefers a case where both Z \u2019 s have the same means . The inliers and outliers should have different structures . If their latent distributions are close , then the set $ \\cal { Q } $ of distributions q over which we minimize is significantly smaller and this should lead to a larger W1 error and thus larger reconstruction error . We thus believe that the minimization of the reconstruction error avoids this case where the latent distributions of the inliers and outliers are too close . Furthermore , in Figure 4 we tested the case where we have a single Gaussian ( it is reported in this figure as MAW-single Gaussian and compared among other variants with MAW ) . Note that the reported performance of the use of a single Gaussian is worse than that of MAW . Q2.Though it is hard to pre-estimate how the outlier distribution looks like , it is more common to assume the outlier distribution has multi-modal than uni-modal . However , the proposed method approximates the outlier distribution as unimodal Gaussian distribution . Is it possible to model the outliers as multi-modal distribution such as MoG ? Response : While it is possible to model the outliers as a mixture of Gaussians , however , we point out 3 disadvantages of this approach . First of all , the outlier distribution is not important in our work , unlike the inlier distribution . Second of all , the model for MoG of outliers is more complex and the number of mixture components is unclear . Third of all , there is no clear loss for discriminating between the classes of outliers ."}}