{"year": "2021", "forum": "USCNapootw", "title": "Certify or Predict: Boosting Certified Robustness with Compositional Architectures", "decision": "Accept (Poster)", "meta_review": "This paper proposes a selection mechanism to choose between a certified model with low clean accuracy and a naturally trained model with high accuracy, to improve the standard clean accuracy for certifiably robust models.  At a high-level, the idea behind this combined system is that when the certified model cannot certify, one should avoid using it for classification, but rather should use a naturally trained model.  A state-of-the-art naturally trained networks is used as the \"core network\", and a small certification network with high certifiable robustness is used as the \"certification network\". The major contribution is a selection network that adaptively chooses between these two networks. \n\nPro\n+ The idea of using two networks adaptively is novel. The proposed selection mechanism has been shown to be able to combine the merits of both networks to obtain better natural accuracy with good certified robustness. \n\n\nCon\n- The experiment section still has room for improvement. Specifically, the presentation of the results were not convincingly conveying the tradeoff between the clean accuracy and the certified accuracy.  After the rebuttal, the authors made some improvements that addressed many of the concerns about the clarity and reproducibility issues. However, reviewers suggest further polishing the experiment section. \n\nOverall, I think the novelty of the paper combined with the promising results achieved outweigh the presentation issues. I would recommend accepting this paper. ", "reviews": [{"review_id": "USCNapootw-0", "review_text": "# Summary of Contributions The paper presents an approach to trade off natural accuracy and certified robustness by combining a network with high natural accuracy ( the \u201c core network \u201d ) with a second network with high certifiable robustness ( the \u201c certification network \u201d ) . A selection mechanism is used to decide which network an input sample should be processed by . The selection mechanism allows the combined system to perform significantly better than a weighted average of the core and certification networks ( e.g.randomly assigning input to the core network with some probability $ p $ ) would . # Score Recommendation Despite the weaknesses in experimental evidence , clarity and reproducibility identified below , I recommend an acceptance because the authors have demonstrated that the selection mechanism presented works for non-trivial problems , providing a simple way to trade off natural accuracy and certified robustness . ACE can consistently benefit from advances that improve the natural accuracy of the core network . In addition , as long as a selection mechanism can be found that is compatible with the certification network , it would be possible for ACE to leverage improvements in certified defenses . While results are only presented for $ l_\\infty $ perturbations , I expect that the same approach can be applied to different perturbations , as long as it is possible for the selection mechanism to have a tunable selection rate while having non-trivial robustness to the perturbation of choice . # Weaknesses # # Missing Experimental Evidence - The paper claims in the abstract that it is the first to obtain a high natural accuracy with non-trivial certified robustness ; the results ( 91.6 % natural accuracy , 22.8 % certified robustness ) are compared to the prior state-of-the art ( 77.4 % natural accuracy , 16.5 % certified robustness ) . However , I am concerned that the comparison may not be completely fair ( if the network the comparison is made to is one of the CROWN-IBP trained DM-Large networks ; see the section on \u201c Clarity \u201d below ) . The paper itself acknowledges ( in the first paragraph of page 7 ) that they did not conduct the extensive training required to obtain the performance reported in the CROWN-IBP paper . - The authors should either train the CROWN-IBP DM-Large network with at least as much resources as in the original paper , or clearly identify in both the abstract and introduction that the amount of compute used was constrained . - The paper claims that \u201c ACE produces much more favorable robustness-accuracy trade-offs than varying hyperparameters of the existing certified defenses \u201d on the basis of comparing DM-Large CROWN-IBP to the Conv2 COLT-based ACE SelectionNet . I believe that more evidence is required to substantiate this claim , since the choice of a base network is rather arbitrary ( why choose one comparable to the second DM-Large network , not the first or the third ? ) . - One possible set of experiments is to use each of the 5 DM-Large networks with non-trivial certifiable accuracy as the certification network , and then showing that the resulting families of ACE SelectionNets have a better robustness-accuracy tradeoff . # # Possible Experimental Errors The Conv3-COLT network in Figure 2 has a performance ( ~75 % natural accuracy , ~50 % certifiable accuracy ) that is significantly worse than that reported in the original COLT paper ( 78.4 % natural accuracy , 60.5 % certified robustness ) . What is the cause of this significant gap ? # # Clarity - The term \u201c ConvMedBig \u201d is used in the caption for Figure 2 and elsewhere in the paper , but is not defined in the original paper . ( It appears that the authors may be referencing [ the name in code ] ( https : //github.com/eth-sri/colt/blob/20f30b073558ae80e5e726515998c1f31d48b6c6/code/networks.py # L79 ) ) . The authors should provide more detail about specifically which network this is . In fact , it appears from the third paragraph of Section 7 ( and the code linked above ) that \u201c ConvMedBig \u201d matches the network Conv3 exactly . If this is the case , the same name should be used . - The authors compare to a prior approach with 77.4 % accuracy and 16.5 % certified robustness but do not specify what this approach is . ( It appears from Figure 2 that this may be one of the DM-Large CROWN-IBP networks ) - At the bottom of page 6 , the paper states that \u201c the smaller networks to which COLT scales lack capacity to obtain the kind of robustness-accuracy trade-off that we target \u201d . What does this mean ? A significant proportion of the results in Table 1 are presented for COLT , so I \u2019 m confused by this statement . # # Reproducibility - Hyperparameters for the PGD attacks used are not provided , making it difficult to understand the strength of the adversarial attack being used . ( If the adversarial attack is weak , the adversarial accuracy presented in Table 1 may be significantly higher than the actual robust model accuracy ) . - More details should be provided about the algorithm used for certifying the networks in Table 1 ( other than the Entropy-COLT-Conv2 network ) . The third paragraph of Section 5 states that \u201c we only use \u2026 convex relaxation-based certification methods based on intervals and zonotopes \u201d , but I couldn \u2019 t find any further details ( for example , what zonotopes were used to verify the certification network ? ) # Questions for Authors - I \u2019 d like to better understand how the adversarial accuracy of the network was evaluated ; the paper only mentions that it is \u201c usually computed using an adversarial attack such as PGD \u201d ( see the first paragraph of Page 3 ) . One of my concerns is that the selection mechanism ( particularly where a selection network is used ) may reduce the success of PGD adversarial attacks without increasing the robustness of the network , possibly via gradient masking [ 1 ] . - In paragraph 2 of Section 5 , the paper specifies that an adversarially trained network was used as the core network . Given that the last paragraph of Section 4 states that \u201c we assume that certification of the core-network always fails \u201d , why did you choose an adversarially trained network ( which presumably has slightly worse natural accuracy ? ) [ 1 ] : Papernot , Nicolas , Patrick McDaniel , Ian Goodfellow , Somesh Jha , Z. Berkay Celik , and Ananthram Swami . `` Practical black-box attacks against machine learning . '' In Proceedings of the 2017 ACM on Asia conference on computer and communications security , pp . 506-519.2017. # Additional Feedback - hyperparamters \u2192 hyperparameters ( 4th line , third paragraph of section 5 ) - Figure 2 presents many different networks but it is not clear what the point of the figure is . Compounding the issue is the fact that the corresponding discussion begins almost an entire page later . For improved clarity , the authors should consider adding more to the caption for Figure 2 or moving it closer to the discussion . - The term \u201c natural accuracy \u201d and \u201c standard accuracy \u201d is used interchangeably ; the paper should settle on one . - Figure 7 labels the y-axis \u201c std of input zono errors \u201d but this term is never introduced anywhere else in the text . ( Is this the standard deviation , perhaps ? ) # Post-Rebuttal Comments I 've maintained my score at 6 . During the comment period , the authors made progress in improving the clarity of their presentation . As with reviewer 3 , I feel that there is still room for improvement ; in particular , moving some experiments in Section 5 to the appendix could make for a more focused paper with a clearer message for the reader . ( Unfortunately , we did not have enough time during the comment period to get there ) . I 'd also note that the paper is now at nine pages ; this means that I am holding it to a higher bar . Overall , however , I continue to recommend an acceptance as the method to trade off natural accuracy and certified robustness is simple and significantly improves on the state of the art ; for me , these strengths outweight the remaining issues .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful and detailed questions and comments . We did in fact identify some of the same questions but could only conduct the corresponding experiments after the initial submission deadline . We rephrased your comments and hope to answer all points below : Q : Why do the CROWN-IBP results you report for DM-Large not match the original results from [ 4 ] ? - > In short , we originally used a shorter training schedule and have now rerun the experiments using the full training schedule from [ 4 ] . As we believe this question to be important for all reviewers , please refer to the general response where we have answered it in more detail . Q : Can you provide additional evidence to substantiate the claim that ACE produces a more favourable accuracy-robustness tradeoff than current state-of-the-art methods ? - > We added experiments and visualisations along the lines of Figure 2 and answered this question in more detail in the general response , as we believe it to be important for all reviewers . We did not use all CROWN IBP models to train corresponding ACE models , because we believe it to not be attractive to use networks of the same architecture that are already strictly worse than some ACE models . Instead , we show that even when using significantly weaker certification networks ( of a different architecture ) there is still a range of natural accuracies , where a corresponding ACE model obtains higher certified accuracies , then the CROWN IBP models ( Figure 3 formerly 7 ) . Q : Why do the accuracies you report for Conv3 don \u2019 t match those reported by [ 1 ] ? - > Note that [ 1 ] optimized for a certified accuracy using MILP certification [ 5 ] while we instead focus on the much cheaper DeepZ verification [ 3 ] . Therefore we trained a network optimized for zonotope certification , which gains around 7.4 % of certifiable accuracy at the cost of 4.7 % natural accuracy ( 78.4/40.8 ) vs ( 73.7/48.2 ) . We have now added an experiment where we use the network from [ 1 ] ( and MILP certification for the certification network ) as described in the general response ( see Figure 2 , formerly Appendix C ) . Q : The Conv3 and ConvMedBig architecture seem to be identical . Why do you use different names ? - > The Conv3 architecture is called ConvMedBig in the repository associated with [ 1 ] . We used that name when referring to it as a reference for comparison and used Conv3 when talking about our networks . We agree that this might be confusing , especially since the name ConvMedBig is not used in their paper . We have now homogenized our notation . Note that Conv5 is also the same architecture as DM-Large , where we applied the same rule to decide when to use which name and have now also homogenized our notation to Conv5 . Q : Where do you specify how the performance of the state-of-the-art-methods , mentioned in abstract , is obtained ? Are they what you measured for the CROWN-IBP trained Conv5 networks ? - > Indeed these numbers were taken from the Conv5 networks we trained with CROWN-IBP . We updated the numbers to correspond to the more expensive training schedule discussed above and updated our evaluation section to note where we take these numbers from . We chose the Conv5 setpoint , by first selecting a setpoint for ACE in its intended working range and then selecting the Conv5 setpoint with the largest smaller certified accuracy , to give it the best chance of beating our natural accuracy . We agree that individual setpoints are not ideal for illustrating the trade-off character , therefore we illustrate the trade-off between certified and standard accuracy in Figure 2 ( and now also Figure 3 , 4 , and 7 ) ( formerly 5 , 7 , and 8 ) ) . EDIT : We rearranged some of our figures for a second revised version and have changed the figure numbers in this response accordingly . [ 1 ] Balunovic , Mislav , and Martin Vechev . `` Adversarial training and provable defenses : Bridging the gap . '' International Conference on Learning Representations . 2019 . [ 3 ] Singh , Gagandeep , et al . `` Fast and effective robustness certification . '' Advances in Neural Information Processing Systems 31 ( 2018 ) : 10802-10813 . [ 4 ] Zhang , Huan , et al . `` Towards stable and efficient training of verifiably robust neural networks . '' arXiv preprint arXiv:1906.06316 ( 2019 ) . [ 5 ] Tjeng , Vincent , Kai Xiao , and Russ Tedrake . `` Evaluating robustness of neural networks with mixed integer programming . '' arXiv preprint arXiv:1711.07356 ( 2017 ) ."}, {"review_id": "USCNapootw-1", "review_text": "This paper focuses on improving the standard ( clean ) accuracy for certifiably robust models . To achieve good certified accuracy , previous works typically make the standard accuracy much worse than naturally trained models . The authors propose a selection mechanism to choose between a certified model with low clean accuracy and a naturally trained model with high clean accuracy . At a high level , when the certified model can not certify , there is no point to use it for classification . A naturally trained model ( which can not be certified as well ) is selected to improve standard accuracy . Strengths : Most previous works on certified defense focus on improving certified accuracy , and standard accuracy is usually sacrificed . This paper focuses on a different and important setting where high standard accuracy is desired , which is neglected by many previous works . I think this is a good step . The proposed selection scheme can balance a certifiably robust model with a naturally trained but highly accurate model . Such a combination can be helpful in the settings where high prediction accuracy is required . The proposed method is technically sound . Using a certified selector makes the whole network certified when it chooses the certified network . To improve clean accuracy , The core network is used when the certified selector chooses the core network ( i.e. , the selector believes the certified network can not make a good prediction on this example ) or can not certify . The paper overall is well motivated and organized . Issues and questions : At a high level , this certification scheme does not improve certified accuracy ( it only makes it worse ) ; it only helps with the verified accuracy vs. clean accuracy trade-off . Thus , a crucial part of evaluation is to show the verified accuracy vs. clean accuracy tradeoff . However , it is not well demonstrated in the experiments . Especially , I think results Table 1 are not so useful because we ca n't see how the baseline certified defense models perform and can not see this tradeoff . Also , the certified accuracy numbers are really low compared to other works , and sometimes close to 0 ( e.g. , on ImageNet-200 only 3 % accuracy ) . Thus , it is important to show a tradeoff figure here . I recommend using figures similar to Figure 2 to present the results for all settings ( CIFAR 2/255 and 8/255 ; downscaled ImageNet-200 at 1/255 ) ( but be aware Figure 2 has its own issues , see comments below ) . Importantly , we should fix a well known certified model ( e.g. , COLT or CROWN-IBP ) and then , apply ACE with different thresholds to see how the clean accuracy improves with dropped verified accuracy . For CIFAR , COLT or CROWN-IBP pretrained models can be used as the base certified model . For Imagenet-200 , I found a recent work [ 1 ] presented certified defense models on 64 * 64 TinyImageNet and ImageNet datasets which can be helpful . They reported around 15 % certified accuracy and also uses much larger model structures which should improve the results in this paper by using their pretrained models as the base certified model for selection ( I doubt the simple CNN models in this paper are sufficient for ImageNet ) . Again , the trade-off part is the most important results to see in this paper , which is not well demonstrated . Figure 2 made a misleading comparison because the ACE based methods are using COLT as the base certified classifier and it is inappropriate to compare it to CROWN-IBP with different kappas . We should either also use COLT trained with different weights on natural loss ( similar to the kappa in CROWN-IBP ) to see this tradeoff , or use CROWN-IBP as the base certified classifier in this figure . Especially , in the CIFAR 2/255 setting , COLT achieves better clean accuracy than CROWN-IBP , so this gives ACE an advantage in this comparison , and the claim that ACE achieves a better trade-off than using the tuned kappa parameters in ( CROWN- ) IBP training can not be justified . It also seems to me that in Figure 2 the CIFAR 2/255 CROWN-IBP numbers are much worse than the ones reported in CROWN-IBP paper ( they reported 28.48 % standard error and 46.03 % verified error ) , but in Figure 2 it is much worse ( ~35 % standard error and ~50 % verified error ) . If we use the correct CROWN-IBP model , it should start at a similar place at the ACE based methods in Figure 2 , rather than on the far left . Can you explain ? Conclusion : I like the aim on standard accuracy and the network selection idea proposed in this paper , but its current evaluation is partially missing or misleading and can not justify all claims . So I can not recommend accepting its current version . However I will be glad to discuss with the authors and re-evaluate the paper based on new evaluation results from the authors . I will be happy to accept this paper if the authors can address my issues mentioned above . # # # After rebuttal See my reply below for my comments after rebuttal . Overall I feel the paper still has room for improvement and there are several open issues , but it has been improved so it is marginally above acceptance threshold now . Reference : [ 1 ] Xu , Kaidi , et al . `` Provable , Scalable and Automatic Perturbation Analysis on General Computational Graphs '' https : //arxiv.org/pdf/2002.12920", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful and detailed questions and comments . We did in fact identify some of the same questions but could only conduct the corresponding experiments after the initial submission deadline . We rephrased your comments and hope to answer all points below : Q : Can you provide Figures along the lines of Figure 2 for all experiments demonstrating the improvement in tradeoff between natural and certified accuracy and add experiments with established certification networks ? - > Yes , we have added Figures 3 , 4 , and 7 ( formerly 5 , 7 and 8 ) in the style of Figure 2 for CIFAR-10 at 2/255 , 8/255 and TinyImageNet at 1/255 , respectively . As we believe this question to be important for all reviewers , please see a more detailed answer in the general response . Q : Can you use the strong provable networks trained by [ 2 ] to demonstrate good results are attainable using ACE at ImageNet sized tasks ? - > Yes , we added results on TinyImageNet at 1/255 using the WideResNet from [ 2 ] both as a certification network and as a feature extractor for the selection network . Using these networks , we achieve significantly better results than on ImageNet200 , despite not being able to leverage full sized images on the core-network . This demonstrates that ACE scales to more challenging tasks if sufficiently strong , underlying provable networks are available . This experiment is illustrated in Figure 4 ( formerly 8 ) . Q : Can you perform an experiment either comparing COLT based ACE models with COLT trained networks using different natural loss components or comparing CROWN-IBP based ACE models to CROWN-IBP networks trained with different $ \\kappa $ ? - > Yes , we have done both . We added an experiment comparing CROWN-IBP trained Conv5 models with different $ \\kappa $ with ACE models based on some of these Conv5 networks for CIFAR10 at 8/255 ( Figure 3 formerly 7 ) and an experiment comparing COLT trained Conv3 networks with various natural loss components ( Note that COLT usually does not use a natural loss component ) with COLT based ACE models for CIFAR-10 at 2/255 ( Figure 2 formerly 5 ) . However , we don \u2019 t believe the latter to be completely fair , as COLT only scales to small models which lack the capacity to achieve a comparable natural accuracy , even when trained without any robustness considerations . Q : Is it misleading to compare an ACE model based on COLT with individual models using a cheaper certified training method such as CROWN-IBP ? - > No , we believe it to be a feature of the ACE architecture that small models with high certified accuracies , trained and certified with expensive provable training and certification methods , can be combined with larger core-networks to obtain ACE models that achieve high natural accuracies , typically not accessible to networks of a size to which these expensive methods scale . We believe a comparison with more scalable provable training methods to be appropriate if these are required to scale to the larger networks required to achieve comparable natural accuracies . However , we want to point out that the certification networks we used for the comparison in Figure 2 are actually worse than the CROWN-IBP networks and that it requires less than half the GPU time to train and certify these models . Using more expensive certification methods such as MILP , which don \u2019 t scale to Conv5 , we can improve the performance of the ACE model further . EDIT : We rearranged some of our figures for a second revised version and have changed the figure numbers in this response accordingly . [ 1 ] Zhang , Huan , et al . `` Towards stable and efficient training of verifiably robust neural networks . '' arXiv preprint arXiv:1906.06316 ( 2019 ) . [ 2 ] Xu , Kaidi , et al . `` Automatic Perturbation Analysis on General Computational Graphs . '' arXiv preprint arXiv:2002.12920 ( 2020 ) . [ 3 ] Balunovic , Mislav , and Martin Vechev . `` Adversarial training and provable defenses : Bridging the gap . '' International Conference on Learning Representations . 2019 ."}, {"review_id": "USCNapootw-2", "review_text": "This paper proposes a new network architecture that combines 1 ) a state-of-the-art deep neural network with high accuracy ( but potentially no robustness certificate ) , and 2 ) a small certification network with high certifiable robustness ( but not necessarily very high accuracy ) , using a selection network that adaptively chooses between these two networks . They show that by doing so , the new architecture is able to take advantage of both networks and thus obtain good natural accuracy with better certified robustness that significantly improves upon prior benchmarks . The main advantage of this framework is its flexibility in allowing arbitrary combinations of STOA deep networks with any networks with certified robustness and their selection mechanism is able to make good use of both . 1.I like this simple idea and I am glad to see its good performance , although I wish the author can develop more theoretical results to quantify the value of a hybrid model . 2.According to ( 2 ) , the objective may not be differentiable because of the binary function $ g $ . Can you elaborate on how gradient-based algorithms are applied to this formulation ? 3.Can you provide some interpretation of the learned selection mechanism $ g_ { \\theta_s } $ ? In particular , what features of the samples make them be passed through the core or the certification network ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their interesting questions and comments . We rephrased your comments and hope to answer all points below : Q : Can you develop more theoretical results to quantify the value of a hybrid model ? - > It is difficult to provide general theoretical guarantees on the certified accuracy of any network , as this depends highly on the underlying distribution . It has for example been shown that distributions can be constructed such that a Bayes optimal classifier with perfect accuracy in the natural case performs arbitrarily badly under arbitrarily small perturbations [ 1 ] . Could the reviewer specify what kind of theoretical guarantees they had in mind ? Q : According to ( 2 ) , the objective may not be differentiable because of the binary function $ g $ . Can you elaborate on how gradient-based algorithms are applied to this formulation ? - > Yes , indeed using the binary function g directly in a loss function constructed analogously to the compositional network would lead to problems with differentiability . We avoid these issues by leveraging the compositional architecture to decompose the training problem . This allows us to train the certification- and core-networks in isolation , before computing labels for the selection-network and then training it separately as well . This turns the training of every individual component-network into a standard ( provable ) training task and allows us to freely choose training methods independently for every component-network . Q : Can you provide some interpretation of the learned selection mechanism , in particular , what features of the samples are key for selection ? - > Yes , we can provide some interpretation of how the selection mechanism decides . We observe that if a certification network has difficulty differentiating a group of classes in an adversarial setting ( blocks of high off-diagonal terms in the confusion matrix ) , while they are easy to differentiate from other classes , then classes from this group are selected at a much lower rate for certification . An example for such a group , are the animal classes in CIFAR-10 . Whether this effect is due to the selection network learning underlying features that make these samples more difficult to classify provably correct , learning that all of these classes are difficult to certify , or most likely a combination of the two , we can not say with certainty . Because the selection-network was not set up specifically to be an interpretable model ( which generally incurs accuracy penalties ) , it is difficult to pinpoint individual features learned by the selection mechanism . We added an experiment comparing three selection networks on an otherwise identical ACE model . We transferred one selection network from a different Conv3 ACE model , trained on using labels based on the adversarial correctness of the sample and trained one in the standard way using provable correctness . We observe that the transferred selection network performs very well , while the one trained using adversarial accuracy performs notably worse . This suggests to us that a ) the certification difficulty of a sample is stable over different certification networks at least to some degree and b ) that the selection network does learn features distinguishing the difficulty of finding an adversarial example from provable robustness . We present these results in more detail in Appendix D and hope this provides some intuition on how the selection decision is made . [ 1 ] Zhang , Hongyang , et al . `` Theoretically principled trade-off between robustness and accuracy . '' arXiv preprint arXiv:1901.08573 ( 2019 ) ."}], "0": {"review_id": "USCNapootw-0", "review_text": "# Summary of Contributions The paper presents an approach to trade off natural accuracy and certified robustness by combining a network with high natural accuracy ( the \u201c core network \u201d ) with a second network with high certifiable robustness ( the \u201c certification network \u201d ) . A selection mechanism is used to decide which network an input sample should be processed by . The selection mechanism allows the combined system to perform significantly better than a weighted average of the core and certification networks ( e.g.randomly assigning input to the core network with some probability $ p $ ) would . # Score Recommendation Despite the weaknesses in experimental evidence , clarity and reproducibility identified below , I recommend an acceptance because the authors have demonstrated that the selection mechanism presented works for non-trivial problems , providing a simple way to trade off natural accuracy and certified robustness . ACE can consistently benefit from advances that improve the natural accuracy of the core network . In addition , as long as a selection mechanism can be found that is compatible with the certification network , it would be possible for ACE to leverage improvements in certified defenses . While results are only presented for $ l_\\infty $ perturbations , I expect that the same approach can be applied to different perturbations , as long as it is possible for the selection mechanism to have a tunable selection rate while having non-trivial robustness to the perturbation of choice . # Weaknesses # # Missing Experimental Evidence - The paper claims in the abstract that it is the first to obtain a high natural accuracy with non-trivial certified robustness ; the results ( 91.6 % natural accuracy , 22.8 % certified robustness ) are compared to the prior state-of-the art ( 77.4 % natural accuracy , 16.5 % certified robustness ) . However , I am concerned that the comparison may not be completely fair ( if the network the comparison is made to is one of the CROWN-IBP trained DM-Large networks ; see the section on \u201c Clarity \u201d below ) . The paper itself acknowledges ( in the first paragraph of page 7 ) that they did not conduct the extensive training required to obtain the performance reported in the CROWN-IBP paper . - The authors should either train the CROWN-IBP DM-Large network with at least as much resources as in the original paper , or clearly identify in both the abstract and introduction that the amount of compute used was constrained . - The paper claims that \u201c ACE produces much more favorable robustness-accuracy trade-offs than varying hyperparameters of the existing certified defenses \u201d on the basis of comparing DM-Large CROWN-IBP to the Conv2 COLT-based ACE SelectionNet . I believe that more evidence is required to substantiate this claim , since the choice of a base network is rather arbitrary ( why choose one comparable to the second DM-Large network , not the first or the third ? ) . - One possible set of experiments is to use each of the 5 DM-Large networks with non-trivial certifiable accuracy as the certification network , and then showing that the resulting families of ACE SelectionNets have a better robustness-accuracy tradeoff . # # Possible Experimental Errors The Conv3-COLT network in Figure 2 has a performance ( ~75 % natural accuracy , ~50 % certifiable accuracy ) that is significantly worse than that reported in the original COLT paper ( 78.4 % natural accuracy , 60.5 % certified robustness ) . What is the cause of this significant gap ? # # Clarity - The term \u201c ConvMedBig \u201d is used in the caption for Figure 2 and elsewhere in the paper , but is not defined in the original paper . ( It appears that the authors may be referencing [ the name in code ] ( https : //github.com/eth-sri/colt/blob/20f30b073558ae80e5e726515998c1f31d48b6c6/code/networks.py # L79 ) ) . The authors should provide more detail about specifically which network this is . In fact , it appears from the third paragraph of Section 7 ( and the code linked above ) that \u201c ConvMedBig \u201d matches the network Conv3 exactly . If this is the case , the same name should be used . - The authors compare to a prior approach with 77.4 % accuracy and 16.5 % certified robustness but do not specify what this approach is . ( It appears from Figure 2 that this may be one of the DM-Large CROWN-IBP networks ) - At the bottom of page 6 , the paper states that \u201c the smaller networks to which COLT scales lack capacity to obtain the kind of robustness-accuracy trade-off that we target \u201d . What does this mean ? A significant proportion of the results in Table 1 are presented for COLT , so I \u2019 m confused by this statement . # # Reproducibility - Hyperparameters for the PGD attacks used are not provided , making it difficult to understand the strength of the adversarial attack being used . ( If the adversarial attack is weak , the adversarial accuracy presented in Table 1 may be significantly higher than the actual robust model accuracy ) . - More details should be provided about the algorithm used for certifying the networks in Table 1 ( other than the Entropy-COLT-Conv2 network ) . The third paragraph of Section 5 states that \u201c we only use \u2026 convex relaxation-based certification methods based on intervals and zonotopes \u201d , but I couldn \u2019 t find any further details ( for example , what zonotopes were used to verify the certification network ? ) # Questions for Authors - I \u2019 d like to better understand how the adversarial accuracy of the network was evaluated ; the paper only mentions that it is \u201c usually computed using an adversarial attack such as PGD \u201d ( see the first paragraph of Page 3 ) . One of my concerns is that the selection mechanism ( particularly where a selection network is used ) may reduce the success of PGD adversarial attacks without increasing the robustness of the network , possibly via gradient masking [ 1 ] . - In paragraph 2 of Section 5 , the paper specifies that an adversarially trained network was used as the core network . Given that the last paragraph of Section 4 states that \u201c we assume that certification of the core-network always fails \u201d , why did you choose an adversarially trained network ( which presumably has slightly worse natural accuracy ? ) [ 1 ] : Papernot , Nicolas , Patrick McDaniel , Ian Goodfellow , Somesh Jha , Z. Berkay Celik , and Ananthram Swami . `` Practical black-box attacks against machine learning . '' In Proceedings of the 2017 ACM on Asia conference on computer and communications security , pp . 506-519.2017. # Additional Feedback - hyperparamters \u2192 hyperparameters ( 4th line , third paragraph of section 5 ) - Figure 2 presents many different networks but it is not clear what the point of the figure is . Compounding the issue is the fact that the corresponding discussion begins almost an entire page later . For improved clarity , the authors should consider adding more to the caption for Figure 2 or moving it closer to the discussion . - The term \u201c natural accuracy \u201d and \u201c standard accuracy \u201d is used interchangeably ; the paper should settle on one . - Figure 7 labels the y-axis \u201c std of input zono errors \u201d but this term is never introduced anywhere else in the text . ( Is this the standard deviation , perhaps ? ) # Post-Rebuttal Comments I 've maintained my score at 6 . During the comment period , the authors made progress in improving the clarity of their presentation . As with reviewer 3 , I feel that there is still room for improvement ; in particular , moving some experiments in Section 5 to the appendix could make for a more focused paper with a clearer message for the reader . ( Unfortunately , we did not have enough time during the comment period to get there ) . I 'd also note that the paper is now at nine pages ; this means that I am holding it to a higher bar . Overall , however , I continue to recommend an acceptance as the method to trade off natural accuracy and certified robustness is simple and significantly improves on the state of the art ; for me , these strengths outweight the remaining issues .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful and detailed questions and comments . We did in fact identify some of the same questions but could only conduct the corresponding experiments after the initial submission deadline . We rephrased your comments and hope to answer all points below : Q : Why do the CROWN-IBP results you report for DM-Large not match the original results from [ 4 ] ? - > In short , we originally used a shorter training schedule and have now rerun the experiments using the full training schedule from [ 4 ] . As we believe this question to be important for all reviewers , please refer to the general response where we have answered it in more detail . Q : Can you provide additional evidence to substantiate the claim that ACE produces a more favourable accuracy-robustness tradeoff than current state-of-the-art methods ? - > We added experiments and visualisations along the lines of Figure 2 and answered this question in more detail in the general response , as we believe it to be important for all reviewers . We did not use all CROWN IBP models to train corresponding ACE models , because we believe it to not be attractive to use networks of the same architecture that are already strictly worse than some ACE models . Instead , we show that even when using significantly weaker certification networks ( of a different architecture ) there is still a range of natural accuracies , where a corresponding ACE model obtains higher certified accuracies , then the CROWN IBP models ( Figure 3 formerly 7 ) . Q : Why do the accuracies you report for Conv3 don \u2019 t match those reported by [ 1 ] ? - > Note that [ 1 ] optimized for a certified accuracy using MILP certification [ 5 ] while we instead focus on the much cheaper DeepZ verification [ 3 ] . Therefore we trained a network optimized for zonotope certification , which gains around 7.4 % of certifiable accuracy at the cost of 4.7 % natural accuracy ( 78.4/40.8 ) vs ( 73.7/48.2 ) . We have now added an experiment where we use the network from [ 1 ] ( and MILP certification for the certification network ) as described in the general response ( see Figure 2 , formerly Appendix C ) . Q : The Conv3 and ConvMedBig architecture seem to be identical . Why do you use different names ? - > The Conv3 architecture is called ConvMedBig in the repository associated with [ 1 ] . We used that name when referring to it as a reference for comparison and used Conv3 when talking about our networks . We agree that this might be confusing , especially since the name ConvMedBig is not used in their paper . We have now homogenized our notation . Note that Conv5 is also the same architecture as DM-Large , where we applied the same rule to decide when to use which name and have now also homogenized our notation to Conv5 . Q : Where do you specify how the performance of the state-of-the-art-methods , mentioned in abstract , is obtained ? Are they what you measured for the CROWN-IBP trained Conv5 networks ? - > Indeed these numbers were taken from the Conv5 networks we trained with CROWN-IBP . We updated the numbers to correspond to the more expensive training schedule discussed above and updated our evaluation section to note where we take these numbers from . We chose the Conv5 setpoint , by first selecting a setpoint for ACE in its intended working range and then selecting the Conv5 setpoint with the largest smaller certified accuracy , to give it the best chance of beating our natural accuracy . We agree that individual setpoints are not ideal for illustrating the trade-off character , therefore we illustrate the trade-off between certified and standard accuracy in Figure 2 ( and now also Figure 3 , 4 , and 7 ) ( formerly 5 , 7 , and 8 ) ) . EDIT : We rearranged some of our figures for a second revised version and have changed the figure numbers in this response accordingly . [ 1 ] Balunovic , Mislav , and Martin Vechev . `` Adversarial training and provable defenses : Bridging the gap . '' International Conference on Learning Representations . 2019 . [ 3 ] Singh , Gagandeep , et al . `` Fast and effective robustness certification . '' Advances in Neural Information Processing Systems 31 ( 2018 ) : 10802-10813 . [ 4 ] Zhang , Huan , et al . `` Towards stable and efficient training of verifiably robust neural networks . '' arXiv preprint arXiv:1906.06316 ( 2019 ) . [ 5 ] Tjeng , Vincent , Kai Xiao , and Russ Tedrake . `` Evaluating robustness of neural networks with mixed integer programming . '' arXiv preprint arXiv:1711.07356 ( 2017 ) ."}, "1": {"review_id": "USCNapootw-1", "review_text": "This paper focuses on improving the standard ( clean ) accuracy for certifiably robust models . To achieve good certified accuracy , previous works typically make the standard accuracy much worse than naturally trained models . The authors propose a selection mechanism to choose between a certified model with low clean accuracy and a naturally trained model with high clean accuracy . At a high level , when the certified model can not certify , there is no point to use it for classification . A naturally trained model ( which can not be certified as well ) is selected to improve standard accuracy . Strengths : Most previous works on certified defense focus on improving certified accuracy , and standard accuracy is usually sacrificed . This paper focuses on a different and important setting where high standard accuracy is desired , which is neglected by many previous works . I think this is a good step . The proposed selection scheme can balance a certifiably robust model with a naturally trained but highly accurate model . Such a combination can be helpful in the settings where high prediction accuracy is required . The proposed method is technically sound . Using a certified selector makes the whole network certified when it chooses the certified network . To improve clean accuracy , The core network is used when the certified selector chooses the core network ( i.e. , the selector believes the certified network can not make a good prediction on this example ) or can not certify . The paper overall is well motivated and organized . Issues and questions : At a high level , this certification scheme does not improve certified accuracy ( it only makes it worse ) ; it only helps with the verified accuracy vs. clean accuracy trade-off . Thus , a crucial part of evaluation is to show the verified accuracy vs. clean accuracy tradeoff . However , it is not well demonstrated in the experiments . Especially , I think results Table 1 are not so useful because we ca n't see how the baseline certified defense models perform and can not see this tradeoff . Also , the certified accuracy numbers are really low compared to other works , and sometimes close to 0 ( e.g. , on ImageNet-200 only 3 % accuracy ) . Thus , it is important to show a tradeoff figure here . I recommend using figures similar to Figure 2 to present the results for all settings ( CIFAR 2/255 and 8/255 ; downscaled ImageNet-200 at 1/255 ) ( but be aware Figure 2 has its own issues , see comments below ) . Importantly , we should fix a well known certified model ( e.g. , COLT or CROWN-IBP ) and then , apply ACE with different thresholds to see how the clean accuracy improves with dropped verified accuracy . For CIFAR , COLT or CROWN-IBP pretrained models can be used as the base certified model . For Imagenet-200 , I found a recent work [ 1 ] presented certified defense models on 64 * 64 TinyImageNet and ImageNet datasets which can be helpful . They reported around 15 % certified accuracy and also uses much larger model structures which should improve the results in this paper by using their pretrained models as the base certified model for selection ( I doubt the simple CNN models in this paper are sufficient for ImageNet ) . Again , the trade-off part is the most important results to see in this paper , which is not well demonstrated . Figure 2 made a misleading comparison because the ACE based methods are using COLT as the base certified classifier and it is inappropriate to compare it to CROWN-IBP with different kappas . We should either also use COLT trained with different weights on natural loss ( similar to the kappa in CROWN-IBP ) to see this tradeoff , or use CROWN-IBP as the base certified classifier in this figure . Especially , in the CIFAR 2/255 setting , COLT achieves better clean accuracy than CROWN-IBP , so this gives ACE an advantage in this comparison , and the claim that ACE achieves a better trade-off than using the tuned kappa parameters in ( CROWN- ) IBP training can not be justified . It also seems to me that in Figure 2 the CIFAR 2/255 CROWN-IBP numbers are much worse than the ones reported in CROWN-IBP paper ( they reported 28.48 % standard error and 46.03 % verified error ) , but in Figure 2 it is much worse ( ~35 % standard error and ~50 % verified error ) . If we use the correct CROWN-IBP model , it should start at a similar place at the ACE based methods in Figure 2 , rather than on the far left . Can you explain ? Conclusion : I like the aim on standard accuracy and the network selection idea proposed in this paper , but its current evaluation is partially missing or misleading and can not justify all claims . So I can not recommend accepting its current version . However I will be glad to discuss with the authors and re-evaluate the paper based on new evaluation results from the authors . I will be happy to accept this paper if the authors can address my issues mentioned above . # # # After rebuttal See my reply below for my comments after rebuttal . Overall I feel the paper still has room for improvement and there are several open issues , but it has been improved so it is marginally above acceptance threshold now . Reference : [ 1 ] Xu , Kaidi , et al . `` Provable , Scalable and Automatic Perturbation Analysis on General Computational Graphs '' https : //arxiv.org/pdf/2002.12920", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful and detailed questions and comments . We did in fact identify some of the same questions but could only conduct the corresponding experiments after the initial submission deadline . We rephrased your comments and hope to answer all points below : Q : Can you provide Figures along the lines of Figure 2 for all experiments demonstrating the improvement in tradeoff between natural and certified accuracy and add experiments with established certification networks ? - > Yes , we have added Figures 3 , 4 , and 7 ( formerly 5 , 7 and 8 ) in the style of Figure 2 for CIFAR-10 at 2/255 , 8/255 and TinyImageNet at 1/255 , respectively . As we believe this question to be important for all reviewers , please see a more detailed answer in the general response . Q : Can you use the strong provable networks trained by [ 2 ] to demonstrate good results are attainable using ACE at ImageNet sized tasks ? - > Yes , we added results on TinyImageNet at 1/255 using the WideResNet from [ 2 ] both as a certification network and as a feature extractor for the selection network . Using these networks , we achieve significantly better results than on ImageNet200 , despite not being able to leverage full sized images on the core-network . This demonstrates that ACE scales to more challenging tasks if sufficiently strong , underlying provable networks are available . This experiment is illustrated in Figure 4 ( formerly 8 ) . Q : Can you perform an experiment either comparing COLT based ACE models with COLT trained networks using different natural loss components or comparing CROWN-IBP based ACE models to CROWN-IBP networks trained with different $ \\kappa $ ? - > Yes , we have done both . We added an experiment comparing CROWN-IBP trained Conv5 models with different $ \\kappa $ with ACE models based on some of these Conv5 networks for CIFAR10 at 8/255 ( Figure 3 formerly 7 ) and an experiment comparing COLT trained Conv3 networks with various natural loss components ( Note that COLT usually does not use a natural loss component ) with COLT based ACE models for CIFAR-10 at 2/255 ( Figure 2 formerly 5 ) . However , we don \u2019 t believe the latter to be completely fair , as COLT only scales to small models which lack the capacity to achieve a comparable natural accuracy , even when trained without any robustness considerations . Q : Is it misleading to compare an ACE model based on COLT with individual models using a cheaper certified training method such as CROWN-IBP ? - > No , we believe it to be a feature of the ACE architecture that small models with high certified accuracies , trained and certified with expensive provable training and certification methods , can be combined with larger core-networks to obtain ACE models that achieve high natural accuracies , typically not accessible to networks of a size to which these expensive methods scale . We believe a comparison with more scalable provable training methods to be appropriate if these are required to scale to the larger networks required to achieve comparable natural accuracies . However , we want to point out that the certification networks we used for the comparison in Figure 2 are actually worse than the CROWN-IBP networks and that it requires less than half the GPU time to train and certify these models . Using more expensive certification methods such as MILP , which don \u2019 t scale to Conv5 , we can improve the performance of the ACE model further . EDIT : We rearranged some of our figures for a second revised version and have changed the figure numbers in this response accordingly . [ 1 ] Zhang , Huan , et al . `` Towards stable and efficient training of verifiably robust neural networks . '' arXiv preprint arXiv:1906.06316 ( 2019 ) . [ 2 ] Xu , Kaidi , et al . `` Automatic Perturbation Analysis on General Computational Graphs . '' arXiv preprint arXiv:2002.12920 ( 2020 ) . [ 3 ] Balunovic , Mislav , and Martin Vechev . `` Adversarial training and provable defenses : Bridging the gap . '' International Conference on Learning Representations . 2019 ."}, "2": {"review_id": "USCNapootw-2", "review_text": "This paper proposes a new network architecture that combines 1 ) a state-of-the-art deep neural network with high accuracy ( but potentially no robustness certificate ) , and 2 ) a small certification network with high certifiable robustness ( but not necessarily very high accuracy ) , using a selection network that adaptively chooses between these two networks . They show that by doing so , the new architecture is able to take advantage of both networks and thus obtain good natural accuracy with better certified robustness that significantly improves upon prior benchmarks . The main advantage of this framework is its flexibility in allowing arbitrary combinations of STOA deep networks with any networks with certified robustness and their selection mechanism is able to make good use of both . 1.I like this simple idea and I am glad to see its good performance , although I wish the author can develop more theoretical results to quantify the value of a hybrid model . 2.According to ( 2 ) , the objective may not be differentiable because of the binary function $ g $ . Can you elaborate on how gradient-based algorithms are applied to this formulation ? 3.Can you provide some interpretation of the learned selection mechanism $ g_ { \\theta_s } $ ? In particular , what features of the samples make them be passed through the core or the certification network ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their interesting questions and comments . We rephrased your comments and hope to answer all points below : Q : Can you develop more theoretical results to quantify the value of a hybrid model ? - > It is difficult to provide general theoretical guarantees on the certified accuracy of any network , as this depends highly on the underlying distribution . It has for example been shown that distributions can be constructed such that a Bayes optimal classifier with perfect accuracy in the natural case performs arbitrarily badly under arbitrarily small perturbations [ 1 ] . Could the reviewer specify what kind of theoretical guarantees they had in mind ? Q : According to ( 2 ) , the objective may not be differentiable because of the binary function $ g $ . Can you elaborate on how gradient-based algorithms are applied to this formulation ? - > Yes , indeed using the binary function g directly in a loss function constructed analogously to the compositional network would lead to problems with differentiability . We avoid these issues by leveraging the compositional architecture to decompose the training problem . This allows us to train the certification- and core-networks in isolation , before computing labels for the selection-network and then training it separately as well . This turns the training of every individual component-network into a standard ( provable ) training task and allows us to freely choose training methods independently for every component-network . Q : Can you provide some interpretation of the learned selection mechanism , in particular , what features of the samples are key for selection ? - > Yes , we can provide some interpretation of how the selection mechanism decides . We observe that if a certification network has difficulty differentiating a group of classes in an adversarial setting ( blocks of high off-diagonal terms in the confusion matrix ) , while they are easy to differentiate from other classes , then classes from this group are selected at a much lower rate for certification . An example for such a group , are the animal classes in CIFAR-10 . Whether this effect is due to the selection network learning underlying features that make these samples more difficult to classify provably correct , learning that all of these classes are difficult to certify , or most likely a combination of the two , we can not say with certainty . Because the selection-network was not set up specifically to be an interpretable model ( which generally incurs accuracy penalties ) , it is difficult to pinpoint individual features learned by the selection mechanism . We added an experiment comparing three selection networks on an otherwise identical ACE model . We transferred one selection network from a different Conv3 ACE model , trained on using labels based on the adversarial correctness of the sample and trained one in the standard way using provable correctness . We observe that the transferred selection network performs very well , while the one trained using adversarial accuracy performs notably worse . This suggests to us that a ) the certification difficulty of a sample is stable over different certification networks at least to some degree and b ) that the selection network does learn features distinguishing the difficulty of finding an adversarial example from provable robustness . We present these results in more detail in Appendix D and hope this provides some intuition on how the selection decision is made . [ 1 ] Zhang , Hongyang , et al . `` Theoretically principled trade-off between robustness and accuracy . '' arXiv preprint arXiv:1901.08573 ( 2019 ) ."}}