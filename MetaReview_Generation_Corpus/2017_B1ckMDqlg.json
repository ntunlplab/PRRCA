{"year": "2017", "forum": "B1ckMDqlg", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "decision": "Accept (Poster)", "meta_review": "The paper uses mixtures of experts to increase the capacity of deep networks, and describes the implementation of such a model on a cluster of GPUs. The proposed mixture model achieves strong performances in language modeling and machine translation.", "reviews": [{"review_id": "B1ckMDqlg-0", "review_text": "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs. Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation. I have the several comments on the paper: - I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. - The paper is missing some important references in conditional computation (e.g. https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning. - One very important lesson from the conditional computation literature is that while we can in theory incur much less computation, in practice (especially with the current GPU architectures) the actual time does not match the theory. This can be due to inefficient branching in GPUs. It would be nice if the paper includes a discussion of how their model (and perhaps implementation) deal with this problem, and why it scales well in practice. - Table 1 and Table 3 contain repetitive information, and I think they should be combined in one (maybe moving Table 3 to appendix). One thing I do not understand is how does the number of ops/timestep relate to the training time. This also related to the pervious comment. ", "rating": "7: Good paper, accept", "reply_text": "Yes.Our paper does need a better discussion of different approaches to conditional computation , and their practical performance implications . The strength of our method is that our computation time _does_ match the theory - i.e.that we are getting a significant fraction of the advertised TFLOPs of our gpus . We will rewrite so as to include a better discussion of the issues and to include clearer performance measures ."}, {"review_id": "B1ckMDqlg-1", "review_text": "Paper Strengths: -- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner -- The effective batch size for training the MoE drastically increased also -- Interesting experimental results on the effects of increasing the number of MoEs, which is expected. Paper Weaknesses: --- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss the use of MoE and other alternatives in terms of computational efficiency and other factors. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . We 'd love to add a comparison of MoE to alternative ways of increasing model capacity . We are aware of the following alternatives : 1 . Dense layers ( conventional layers requiring O ( 1 ) computations per parameter per training example ) . Expanding the number of parameters in the dense layers proportionally increases training time . 2.Embedding layers ( such as the word-embedding layers in our language models ) . These can be hugely sparse . We can expand the capacity of an embedding layer by either increasing its dimensionality or its feature space . Our belief is that MoE layers are more powerful than embedding layers , as evidenced by the experiments in our paper , where adding an MoE layer improved perplexity even in the presence of a large embedding layer . We attribute this to the fact that MoE layers , similar to dense layers , can handle orders of magnitude more parameter-example-interactions than embedding layers . The embedding layers tend to be limited by network bandwidth , as the parameters need to be sent over the network , whereas the MoE layers are limited only by GPU compute power , which tends to be much greater . Other than these , which alternatives do you think are most important to discuss ?"}, {"review_id": "B1ckMDqlg-2", "review_text": "This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps. By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient. Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes. Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems. An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior. While two loss terms were employed to balance the use of experts, these are not explored in the experiments section. It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes. Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation. Small comment: I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots. The H-H line has 3 points on left but 5 on the right? Also would be nice if the colors matched between corresponding lines.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your helpful suggestions . We updated our paper to address them as follows : Real computational load and system behavior : We added concrete computational efficiency metrics ( TFLOPS/GPU ) to our language modeling experiments . ( see Tables 1 , 7 and 8 ) . Effect of loss terms : We added more detailed discussions related to our balancing losses as well as experimental tests to show their effectiveness ( see Section 4 and Appendix A ) . Figure clarity : We modified Figure 3 ( which is now Figure 2 ) . Section 5.1 provides full explanations ."}], "0": {"review_id": "B1ckMDqlg-0", "review_text": "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs. Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation. I have the several comments on the paper: - I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. - The paper is missing some important references in conditional computation (e.g. https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning. - One very important lesson from the conditional computation literature is that while we can in theory incur much less computation, in practice (especially with the current GPU architectures) the actual time does not match the theory. This can be due to inefficient branching in GPUs. It would be nice if the paper includes a discussion of how their model (and perhaps implementation) deal with this problem, and why it scales well in practice. - Table 1 and Table 3 contain repetitive information, and I think they should be combined in one (maybe moving Table 3 to appendix). One thing I do not understand is how does the number of ops/timestep relate to the training time. This also related to the pervious comment. ", "rating": "7: Good paper, accept", "reply_text": "Yes.Our paper does need a better discussion of different approaches to conditional computation , and their practical performance implications . The strength of our method is that our computation time _does_ match the theory - i.e.that we are getting a significant fraction of the advertised TFLOPs of our gpus . We will rewrite so as to include a better discussion of the issues and to include clearer performance measures ."}, "1": {"review_id": "B1ckMDqlg-1", "review_text": "Paper Strengths: -- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner -- The effective batch size for training the MoE drastically increased also -- Interesting experimental results on the effects of increasing the number of MoEs, which is expected. Paper Weaknesses: --- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss the use of MoE and other alternatives in terms of computational efficiency and other factors. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review . We 'd love to add a comparison of MoE to alternative ways of increasing model capacity . We are aware of the following alternatives : 1 . Dense layers ( conventional layers requiring O ( 1 ) computations per parameter per training example ) . Expanding the number of parameters in the dense layers proportionally increases training time . 2.Embedding layers ( such as the word-embedding layers in our language models ) . These can be hugely sparse . We can expand the capacity of an embedding layer by either increasing its dimensionality or its feature space . Our belief is that MoE layers are more powerful than embedding layers , as evidenced by the experiments in our paper , where adding an MoE layer improved perplexity even in the presence of a large embedding layer . We attribute this to the fact that MoE layers , similar to dense layers , can handle orders of magnitude more parameter-example-interactions than embedding layers . The embedding layers tend to be limited by network bandwidth , as the parameters need to be sent over the network , whereas the MoE layers are limited only by GPU compute power , which tends to be much greater . Other than these , which alternatives do you think are most important to discuss ?"}, "2": {"review_id": "B1ckMDqlg-2", "review_text": "This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps. By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient. Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes. Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems. An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior. While two loss terms were employed to balance the use of experts, these are not explored in the experiments section. It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes. Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation. Small comment: I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots. The H-H line has 3 points on left but 5 on the right? Also would be nice if the colors matched between corresponding lines.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your helpful suggestions . We updated our paper to address them as follows : Real computational load and system behavior : We added concrete computational efficiency metrics ( TFLOPS/GPU ) to our language modeling experiments . ( see Tables 1 , 7 and 8 ) . Effect of loss terms : We added more detailed discussions related to our balancing losses as well as experimental tests to show their effectiveness ( see Section 4 and Appendix A ) . Figure clarity : We modified Figure 3 ( which is now Figure 2 ) . Section 5.1 provides full explanations ."}}