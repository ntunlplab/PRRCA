{"year": "2018", "forum": "rJGY8GbR-", "title": "Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion", "decision": "Invite to Workshop Track", "meta_review": "All the reviewers agree that this is an interesting paper but have concerns about readability and presentation. There is also concern that many results are speculative and not concretely tested. I recommend the authors to carefully investigate their claims with stronger experiments and submit it to another venue. I recommend presenting at ICLR workshop to obtain further feedback.", "reviews": [{"review_id": "rJGY8GbR--0", "review_text": "This paper further develops the research program using mean field theory to predict generalization performance of deep neural networks. As with all recent mean-field papers, the main query here is to what extent the assumptions (Axioms 1+2, which basically define the asymptotic parameters of interest to be the quantities defined in Sec. 2.; and also the fully connected residual structure of the network) apply in practice. This is answered using the same empirical standard as in [Yang and Schoenholz, Schoenholz et al.], i.e. showing that the dynamics of initialization predict generalization behavior on MNIST according to theory. As with the earlier papers in this recent program, the paper is notation-heavy but generally written well, though there is some overreliance on the readers' knowledge of previous work, for instance in presenting the evidence as above. Try as I might, I cannot find a detailed explanation of the color scale for the important Fig. 4. A small notation issue: the current Hebrew letter for the gradient quantity does not go with the other Greek letters and is typographically poor choice because of underlining, etc.). Also, several of the citations should be fixed to reflect peer-reviewed publication of Arxiv papers. I was not able to review all the proofs, but what I checked was sound. Finally, the techniques of WV and VV would be more applicable if it were not for the very tenuous relationship between gradient explosion and performance, which should be mentioned more than the one time it appears in the paper.", "rating": "7: Good paper, accept", "reply_text": "We respond to your comments as follows . > As with the earlier papers in this recent program , the paper is notation-heavy but generally written well , though there is some overreliance on the readers ' knowledge of previous work , for instance in presenting the evidence as above . Thank you for your kind review . We agree that this overreliance has lead to poor presentation of our results . We have significantly rewritten our main text , devoting much space to summarizing the previous work and context , while toning down the heaviness of notation and technicality in favor of more intuitive discussion . See the changelog for a full list of changesl > Try as I might , I can not find a detailed explanation of the color scale for the important Fig.4.Thank you for pointing this out . We have added color bars to our heatmaps . > A small notation issue : the current Hebrew letter for the gradient quantity does not go with the other Greek letters and is typographically poor choice because of underlining , etc . ) . We have changed the Hebrew daleth to the Greek letter Chi , and bolded all mean field quantities to make them more readable . We have also compiled a symbol glossary to ameliorate the notation heaviness of our paper . > Also , several of the citations should be fixed to reflect peer-reviewed publication of Arxiv papers . Thank you for pointing out the error . We have updated the citations accordingly . > I was not able to review all the proofs , but what I checked was sound . > Finally , the techniques of WV and VV would be more applicable if it were not for the very tenuous relationship between gradient explosion and performance , which should be mentioned more than the one time it appears in the paper . It is true that , as Yang and Schoenholz observed in their NIPS 2017 paper , ReLU resnets are not bottlenecked by trainability but rather by ( metric ) expressivity . This is what we find in the zig phase of ReLU resnet VV , where metric expressivity predicts performance . However , VV does indeed decrease the activation explosion of ReLU resnets to prevent forward computation from overflowing . In the updated version of our paper , we have included our experiments on applying VV to tanh resnets , and there variance decay does improve performance by reducing gradient explosion . This is apparent in our figure 3 ( in the new version ) , which shows that the optimal variance decay is larger for larger depth L. Again , this is expected based on Yang and Schoenholz 's observation that tanh resnets are bottlenecked by trainability when variances are too large . Let us know if you are satisfied with our responses ."}, {"review_id": "rJGY8GbR--1", "review_text": "The authors study mean field theory for deep neural nets. To the best of my knowledge we do not have a good understanding of mean field theory for neural networks and this paper and some references therein are starting to address some of it. However, my concern about the paper is in readability. I am very familiar with the literature on mean field theory but less so on deep nets. I found it difficult to follow many parts because the authors assume that the reader will have the knowledge of all the terminology in the paper, which there is a lot of. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We have revamped the presentation of the paper , improving its presentation and addressing your concerns in readability . We hope you can give it another read ."}, {"review_id": "rJGY8GbR--2", "review_text": "Mean field theory is an approach to analysing complex systems where correlations between highly dependent random variables are ignored, thus making the problem analytically tractable. It is hoped that analytical insights gained in this idealised setting might translate back to the original (and far messier) problem. The authors use a mean field theory approach to study how varying certain network hyperparameters with depth can effect gradient and activation statistics. A correlation between the behaviour of these statistics and training performance on MNIST is noted. As someone asked to conduct an 'emergency' review of this paper, I would have greatly appreciated the authors making more of an effort to present their results clearly. Some general comments in this regard: Clarity issues: - the authors appear to have ignored the ICLR style guidelines - the references are all written in green, making them difficult to read - figures are either missing color maps or make poor choice of colors - the figure captions are difficult to understand in isolation from the main text - the authors themselves appear to muddle their 'zigs' and 'zags' (first line of discussion) Now to get to the actual content of the paper. The authors do not properly place their work in context. Mean field theory has been studied in the context of neural networks at least since the 80's. Entire books have been written on the statistical mechanics of neural networks. It seems wrong that the authors only cite papers on this matter going back to 2016. With that said, the main thrust of the paper is very interesting. The authors derive recurrence relations for mean activations and gradients. They show how scaling layer width and initialisation variance with depth can better control the propagation of these means. The results of their calculations appear to match their random network simulations, and this part of the work seems strong. What is not clear is what effect we should expect these quantities to have on learning? The authors claim there is a tradeoff between expressivity and exploding gradients. This seems quite speculative since it is not clear to me what effect either of these things will have on training. For one, how expressive does a model need to be to correctly classify MNIST? And are exploding gradients necessarily a bad thing? Provided they do not reach infinity, can we not just choose a smaller learning rate? I'm open to reevaluating the review if the issues of clarity and missing literature review are fixed.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate you answering the emergency call to review our paper . Our responses are as follows . > Clarity issues : > - the authors appear to have ignored the ICLR style guidelines In the new version , we have done the following : Abstract merged into 1 paragraph . Changed table title to be lower case except first word and pronoun . We have put parentheses around tail citations . Please let us know if you found more violations of the style guideline . > - the references are all written in green , making them difficult to read We thought that they actually improve readability , but based on your suggestion we have turned off colored links . > - figures are either missing color maps or make poor choice of colors Thank you for pointing this out . We have added color bars and improved color choices , especially in the heatmaps and their contour overlays . > - the figure captions are difficult to understand in isolation from the main text In response to your feedback , we have made figure captions much more self-contained . > - the authors themselves appear to muddle their 'zigs ' and 'zags ' ( first line of discussion ) Thanks for pointing out this error . It has been fixed . > Now to get to the actual content of the paper . The authors do not properly place their work in context . Mean field theory has been studied in the context of neural networks at least since the 80 's . Entire books have been written on the statistical mechanics of neural networks . It seems wrong that the authors only cite papers on this matter going back to 2016 . We apologize for this omission . In the new version , a significant chunk of the introduction is used for surveying previous works on mean field theory of neural networks ."}], "0": {"review_id": "rJGY8GbR--0", "review_text": "This paper further develops the research program using mean field theory to predict generalization performance of deep neural networks. As with all recent mean-field papers, the main query here is to what extent the assumptions (Axioms 1+2, which basically define the asymptotic parameters of interest to be the quantities defined in Sec. 2.; and also the fully connected residual structure of the network) apply in practice. This is answered using the same empirical standard as in [Yang and Schoenholz, Schoenholz et al.], i.e. showing that the dynamics of initialization predict generalization behavior on MNIST according to theory. As with the earlier papers in this recent program, the paper is notation-heavy but generally written well, though there is some overreliance on the readers' knowledge of previous work, for instance in presenting the evidence as above. Try as I might, I cannot find a detailed explanation of the color scale for the important Fig. 4. A small notation issue: the current Hebrew letter for the gradient quantity does not go with the other Greek letters and is typographically poor choice because of underlining, etc.). Also, several of the citations should be fixed to reflect peer-reviewed publication of Arxiv papers. I was not able to review all the proofs, but what I checked was sound. Finally, the techniques of WV and VV would be more applicable if it were not for the very tenuous relationship between gradient explosion and performance, which should be mentioned more than the one time it appears in the paper.", "rating": "7: Good paper, accept", "reply_text": "We respond to your comments as follows . > As with the earlier papers in this recent program , the paper is notation-heavy but generally written well , though there is some overreliance on the readers ' knowledge of previous work , for instance in presenting the evidence as above . Thank you for your kind review . We agree that this overreliance has lead to poor presentation of our results . We have significantly rewritten our main text , devoting much space to summarizing the previous work and context , while toning down the heaviness of notation and technicality in favor of more intuitive discussion . See the changelog for a full list of changesl > Try as I might , I can not find a detailed explanation of the color scale for the important Fig.4.Thank you for pointing this out . We have added color bars to our heatmaps . > A small notation issue : the current Hebrew letter for the gradient quantity does not go with the other Greek letters and is typographically poor choice because of underlining , etc . ) . We have changed the Hebrew daleth to the Greek letter Chi , and bolded all mean field quantities to make them more readable . We have also compiled a symbol glossary to ameliorate the notation heaviness of our paper . > Also , several of the citations should be fixed to reflect peer-reviewed publication of Arxiv papers . Thank you for pointing out the error . We have updated the citations accordingly . > I was not able to review all the proofs , but what I checked was sound . > Finally , the techniques of WV and VV would be more applicable if it were not for the very tenuous relationship between gradient explosion and performance , which should be mentioned more than the one time it appears in the paper . It is true that , as Yang and Schoenholz observed in their NIPS 2017 paper , ReLU resnets are not bottlenecked by trainability but rather by ( metric ) expressivity . This is what we find in the zig phase of ReLU resnet VV , where metric expressivity predicts performance . However , VV does indeed decrease the activation explosion of ReLU resnets to prevent forward computation from overflowing . In the updated version of our paper , we have included our experiments on applying VV to tanh resnets , and there variance decay does improve performance by reducing gradient explosion . This is apparent in our figure 3 ( in the new version ) , which shows that the optimal variance decay is larger for larger depth L. Again , this is expected based on Yang and Schoenholz 's observation that tanh resnets are bottlenecked by trainability when variances are too large . Let us know if you are satisfied with our responses ."}, "1": {"review_id": "rJGY8GbR--1", "review_text": "The authors study mean field theory for deep neural nets. To the best of my knowledge we do not have a good understanding of mean field theory for neural networks and this paper and some references therein are starting to address some of it. However, my concern about the paper is in readability. I am very familiar with the literature on mean field theory but less so on deep nets. I found it difficult to follow many parts because the authors assume that the reader will have the knowledge of all the terminology in the paper, which there is a lot of. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We have revamped the presentation of the paper , improving its presentation and addressing your concerns in readability . We hope you can give it another read ."}, "2": {"review_id": "rJGY8GbR--2", "review_text": "Mean field theory is an approach to analysing complex systems where correlations between highly dependent random variables are ignored, thus making the problem analytically tractable. It is hoped that analytical insights gained in this idealised setting might translate back to the original (and far messier) problem. The authors use a mean field theory approach to study how varying certain network hyperparameters with depth can effect gradient and activation statistics. A correlation between the behaviour of these statistics and training performance on MNIST is noted. As someone asked to conduct an 'emergency' review of this paper, I would have greatly appreciated the authors making more of an effort to present their results clearly. Some general comments in this regard: Clarity issues: - the authors appear to have ignored the ICLR style guidelines - the references are all written in green, making them difficult to read - figures are either missing color maps or make poor choice of colors - the figure captions are difficult to understand in isolation from the main text - the authors themselves appear to muddle their 'zigs' and 'zags' (first line of discussion) Now to get to the actual content of the paper. The authors do not properly place their work in context. Mean field theory has been studied in the context of neural networks at least since the 80's. Entire books have been written on the statistical mechanics of neural networks. It seems wrong that the authors only cite papers on this matter going back to 2016. With that said, the main thrust of the paper is very interesting. The authors derive recurrence relations for mean activations and gradients. They show how scaling layer width and initialisation variance with depth can better control the propagation of these means. The results of their calculations appear to match their random network simulations, and this part of the work seems strong. What is not clear is what effect we should expect these quantities to have on learning? The authors claim there is a tradeoff between expressivity and exploding gradients. This seems quite speculative since it is not clear to me what effect either of these things will have on training. For one, how expressive does a model need to be to correctly classify MNIST? And are exploding gradients necessarily a bad thing? Provided they do not reach infinity, can we not just choose a smaller learning rate? I'm open to reevaluating the review if the issues of clarity and missing literature review are fixed.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate you answering the emergency call to review our paper . Our responses are as follows . > Clarity issues : > - the authors appear to have ignored the ICLR style guidelines In the new version , we have done the following : Abstract merged into 1 paragraph . Changed table title to be lower case except first word and pronoun . We have put parentheses around tail citations . Please let us know if you found more violations of the style guideline . > - the references are all written in green , making them difficult to read We thought that they actually improve readability , but based on your suggestion we have turned off colored links . > - figures are either missing color maps or make poor choice of colors Thank you for pointing this out . We have added color bars and improved color choices , especially in the heatmaps and their contour overlays . > - the figure captions are difficult to understand in isolation from the main text In response to your feedback , we have made figure captions much more self-contained . > - the authors themselves appear to muddle their 'zigs ' and 'zags ' ( first line of discussion ) Thanks for pointing out this error . It has been fixed . > Now to get to the actual content of the paper . The authors do not properly place their work in context . Mean field theory has been studied in the context of neural networks at least since the 80 's . Entire books have been written on the statistical mechanics of neural networks . It seems wrong that the authors only cite papers on this matter going back to 2016 . We apologize for this omission . In the new version , a significant chunk of the introduction is used for surveying previous works on mean field theory of neural networks ."}}