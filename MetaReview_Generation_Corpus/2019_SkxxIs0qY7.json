{"year": "2019", "forum": "SkxxIs0qY7", "title": "CoT: Cooperative Training for Generative Modeling of Discrete Data", "decision": "Reject", "meta_review": "The paper proposes an original and interesting alternative to GANs for optimizing a (proxy to) Jensen-Shannon divergence for discrete sequence data. Experimental results seem promising. Official reviewers were largely positive based on originality and results. However, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a \"low-variance, bias-free algorithm\" to optimize JS.  Given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue (review reposted on the current OpenReview thread on Nov. 6), the AC feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. The current version of the paper does neither. The algorithm is not unbiased for at least two reasons pointed out in discussions: a) in practice a parameterized mediator will be unable to match the true P+G, at best yielding a useful biased estimate (not unlike how GAN's parameterized discriminator induces bias). b) One would need to use REINFORCE (or similar) to get an unbiased estimate of the gradient in Eq. 13, a key detail omitted from the paper. From the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with Eq. 13 (they commented \"most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this.\"). But rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the REINFORCE term thus effectively trading variance for bias. \nOn December 4th authors posted a justification confirming heuristically dropping the REINFORCE terms when taking the gradient of Eq. 13, and said they could attach detailed analysis and experiment results in the camera-ready version.  However if one of the \"most important idea\" of the paper is how to avoid REINFORCE (as still implied and highlighted in the abstract), the AC finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by (unreportedly) dropping the term. \n\nThe approach remains original, interesting, and potentially promising, but as it currently stands, AC and SAC agreed that inexact theoretical over-claiming and insufficient justification and in-depth analysis of key heuristic shortcuts/tradeoffs (however useful) are too important for their fixing to be entrusted to a final camera-ready revision step. A major revision that clearly adresses these issues in depth (both in how the approach is presented and in supporting experiments) will constitute a much more convincing, sound, and impactful research contribution.\n\n", "reviews": [{"review_id": "SkxxIs0qY7-0", "review_text": "Pros: This paper is easy to follow. The idea is nice in three folds. 1. By changing the auxiliary model's role from a discriminator to a mediator, it directly optimizes the JSD measure, which is a symmetrized and smoothed version of KL divergence. 2. Moreover, the mediator and the generator follow similar predictive goals, rather than the opposite goals of G and D in GANs. 3. For discrete sequential data, it avoids approximating expected rewards using Markov rollouts. Cons: Some details are missing in the experiments. 1. In Table 2 of [A], LeakGAN, SeqGAN and RankGAN all show significantly better performances in terms of BLEU on EMNLP2017 WMT, compared to results reported in Table 3 of the submission. Any difference? 2. The Word Mover Distance is computed by training a discriminator, which could be unstable. Could you provide other metrics to evaluate diveristy like self-bleu? [A] Guo, Jiaxian, et al. \"Long text generation via adversarial training with leaked information.\" arXiv preprint arXiv:1709.08624 (2017). Misc: 1. How will the number of samples (i.e. batch size) affect CoT ? 2. How is the applicability of CoT for continuous data? It seems to me there is no theoretical difficulties to apply CoT on continuous data.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for reviewing our paper ! Response to your concerns : 1 . We have contacted one of the authors of the LeakGAN , finding that the data pre-processing and post-processing of ours and theirs are different . This makes the results quite different . 2.We will present with an error bar in the coming revised version . Response to Misc : 1 . This is an interesting topic , we would have some discussion about it if we have found interesting conclusions . 2.We 've actually implemented a continous version of CoT , of which the prior distribution is replaced by Beta Distribution instead of Multinomial Distribution in the current discrete version of CoT . However such a model does not perform well . This is an interesting direction for further research and survey ."}, {"review_id": "SkxxIs0qY7-1", "review_text": "*Summary* A clear an interresting presentation on learning sequences distributions. It achieve this objective by replacing the discriminator with a \"mediator\", a mixture between the training distribution and the target distribution which is estimated via maximum likelihood. *Pros* - Original idea for modelling distribution of sequence data - Theoretical convergence in the Jensen Shanon divergence sense - Promising experiments *Cons* - No major cons to the best of my knowledge *Typos* - It would be very nice to have black and white / color blind friendly graphs - Eq 10 too long - Introduce J_m & J_g in sentence - Coma at the end of Eq 5, and maybe align Generator and Discriminator in some position (e.g. at the semi colon). - missing dot at Eq 8. *Question* - How would you ensure reproducibility (e.g. link to some code?) - Is there any hope to obtain consistency (convergence) wrt other metrics?", "rating": "7: Good paper, accept", "reply_text": "Thanks for reviewing our paper . 1.For reproducibility , we are preparing for a open-source code base . After the paper is de-anonymized , we will attach a link to it . 2.Before the paper of CoT is completed , we have had attempts at several different divergences , including JSD ( CoT ) , Reverse KL ( as is described in the appendix ) , Wasserstein-1 distance , etc . However , only CoT and Reverse KL succeed in getting rid of pre-training via MLE . Reverse KL appears to have mode collapsing problem , therefore CoT is finally the chosen model . However , this is a good direction for further research . We are also interested ."}, {"review_id": "SkxxIs0qY7-2", "review_text": "The paper proposes an interesting method, where the discriminator is replaced by a component that estimates the density that is the mixture of the data and the generator's distributions. In a sense, that component is only a device that allows estimating a Jensen-Shannon divergence for the generator to then be optimized against. Other GAN papers have replaced their discriminator by a similar device (e.g., WGANs, ..), but the present formulation seems novel. The numerical experiments presented on a synthetic Turing test and text generation from EMNLP's 2017 news dataset appear promising. Overall, the mediator seems to allow to achieve lower Jensen-Shannon (JS) divergence values in the experiments (and is kind of designed for that). Although this may be an improvement with respect to existing methods for discrete sequential data, it may also be limited in that it may not easily extend to other types of divergences that have proved superior to JS in some continuous settings. The paper is rather clear, although there are lots of small grammatical errors as well as odd formulations which end up being distracting or confusing. The language should be proof-read carefully. Pros: - Generative modeling of sequence data still in its infancy - Potentially lower variance than policy gradient approaches - Experiments are promising Cons: - Lots of grammatical errors and odd formulations Questions: - Equation 14: what does it mean to find the \"maximum entropy solution\" for the given optimization problem? - Figure 2: how do (b) and (c) relate to each other? Remarks, small typos and odd formulations: - \"for measuring M_\\/phi\": what does measuring mean in this context? - What does small m refer to? Algorithm 1 says the total number of steps but it is also used in the main text as an index for J and \\pi (for mediator?) - Equation block 8: J_m has not been defined yet - \"the supports of distributions G and P\"... -> G without subscript has now been defined in this context - \"if the training being perfect\" - \"tend to get stuck in some sub-optimals\" - the learned distribution \"collapseS\" - \"since the data distribution is, thus ...\" - \"that measures a\" -> \"that estimates a ...\"? - \"a predictive module\": a bit unclear - generative v. discriminative is more usual terminology - \"is well ensured\" - \"with the cost of diversity\" -> \"at the cost of diversity\"? - \"has theoretical guarantee\" - in the references: \"ALIAS PARTH GOYAL\" (all caps) - \"let p denote the intermediate states\": I don't understand what this is. Where is \"p\" used? (proof of Theorem 3) - \"CoT theoretically guarantees the training effectiveness\": what does that mean? - Figure 3: \"epochs\" -> \"Epochs\" - Algorithm 1: what does \"mixed balanced samples\" mean? Make this more precise - \"wide-ranged\" - Equation 10 is too long and equation number is not properly formatted - Figures hard to read in black & white - Figure 2 doesn't use the same limits for the Y axis of the two NLL plots, making comparisons difficult. The two NLL plots are also not side-by-side", "rating": "7: Good paper, accept", "reply_text": "Thanks for reviewing our paper ! Response to your concerns : We will provide with a carefully-revised version of the paper according to your generous suggestions . Answer to the questions : 1 . If there is no constraint on the entropy of the solution of the objective function , the objective would not be equivalent to minimization of JSD . Instead , it would simply be calculating the entropy of M , which is useless . 2.Figure 2 ( a ) ( b ) shows CoT is more robust under its main evaluation to g-m balance compared to the g-d balance of SeqGAN . Ideally , Figure 2 ( a ) should also be showing SeqGAN 's performance under evaluation of JSD , however , in our attempts , SeqGAN always diverges under such evaluation . Figure 2 ( c ) show that the convergence of CoT is steady and quite fast under evaluation of NLL_ { oracle } , which is biased on quality ."}], "0": {"review_id": "SkxxIs0qY7-0", "review_text": "Pros: This paper is easy to follow. The idea is nice in three folds. 1. By changing the auxiliary model's role from a discriminator to a mediator, it directly optimizes the JSD measure, which is a symmetrized and smoothed version of KL divergence. 2. Moreover, the mediator and the generator follow similar predictive goals, rather than the opposite goals of G and D in GANs. 3. For discrete sequential data, it avoids approximating expected rewards using Markov rollouts. Cons: Some details are missing in the experiments. 1. In Table 2 of [A], LeakGAN, SeqGAN and RankGAN all show significantly better performances in terms of BLEU on EMNLP2017 WMT, compared to results reported in Table 3 of the submission. Any difference? 2. The Word Mover Distance is computed by training a discriminator, which could be unstable. Could you provide other metrics to evaluate diveristy like self-bleu? [A] Guo, Jiaxian, et al. \"Long text generation via adversarial training with leaked information.\" arXiv preprint arXiv:1709.08624 (2017). Misc: 1. How will the number of samples (i.e. batch size) affect CoT ? 2. How is the applicability of CoT for continuous data? It seems to me there is no theoretical difficulties to apply CoT on continuous data.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for reviewing our paper ! Response to your concerns : 1 . We have contacted one of the authors of the LeakGAN , finding that the data pre-processing and post-processing of ours and theirs are different . This makes the results quite different . 2.We will present with an error bar in the coming revised version . Response to Misc : 1 . This is an interesting topic , we would have some discussion about it if we have found interesting conclusions . 2.We 've actually implemented a continous version of CoT , of which the prior distribution is replaced by Beta Distribution instead of Multinomial Distribution in the current discrete version of CoT . However such a model does not perform well . This is an interesting direction for further research and survey ."}, "1": {"review_id": "SkxxIs0qY7-1", "review_text": "*Summary* A clear an interresting presentation on learning sequences distributions. It achieve this objective by replacing the discriminator with a \"mediator\", a mixture between the training distribution and the target distribution which is estimated via maximum likelihood. *Pros* - Original idea for modelling distribution of sequence data - Theoretical convergence in the Jensen Shanon divergence sense - Promising experiments *Cons* - No major cons to the best of my knowledge *Typos* - It would be very nice to have black and white / color blind friendly graphs - Eq 10 too long - Introduce J_m & J_g in sentence - Coma at the end of Eq 5, and maybe align Generator and Discriminator in some position (e.g. at the semi colon). - missing dot at Eq 8. *Question* - How would you ensure reproducibility (e.g. link to some code?) - Is there any hope to obtain consistency (convergence) wrt other metrics?", "rating": "7: Good paper, accept", "reply_text": "Thanks for reviewing our paper . 1.For reproducibility , we are preparing for a open-source code base . After the paper is de-anonymized , we will attach a link to it . 2.Before the paper of CoT is completed , we have had attempts at several different divergences , including JSD ( CoT ) , Reverse KL ( as is described in the appendix ) , Wasserstein-1 distance , etc . However , only CoT and Reverse KL succeed in getting rid of pre-training via MLE . Reverse KL appears to have mode collapsing problem , therefore CoT is finally the chosen model . However , this is a good direction for further research . We are also interested ."}, "2": {"review_id": "SkxxIs0qY7-2", "review_text": "The paper proposes an interesting method, where the discriminator is replaced by a component that estimates the density that is the mixture of the data and the generator's distributions. In a sense, that component is only a device that allows estimating a Jensen-Shannon divergence for the generator to then be optimized against. Other GAN papers have replaced their discriminator by a similar device (e.g., WGANs, ..), but the present formulation seems novel. The numerical experiments presented on a synthetic Turing test and text generation from EMNLP's 2017 news dataset appear promising. Overall, the mediator seems to allow to achieve lower Jensen-Shannon (JS) divergence values in the experiments (and is kind of designed for that). Although this may be an improvement with respect to existing methods for discrete sequential data, it may also be limited in that it may not easily extend to other types of divergences that have proved superior to JS in some continuous settings. The paper is rather clear, although there are lots of small grammatical errors as well as odd formulations which end up being distracting or confusing. The language should be proof-read carefully. Pros: - Generative modeling of sequence data still in its infancy - Potentially lower variance than policy gradient approaches - Experiments are promising Cons: - Lots of grammatical errors and odd formulations Questions: - Equation 14: what does it mean to find the \"maximum entropy solution\" for the given optimization problem? - Figure 2: how do (b) and (c) relate to each other? Remarks, small typos and odd formulations: - \"for measuring M_\\/phi\": what does measuring mean in this context? - What does small m refer to? Algorithm 1 says the total number of steps but it is also used in the main text as an index for J and \\pi (for mediator?) - Equation block 8: J_m has not been defined yet - \"the supports of distributions G and P\"... -> G without subscript has now been defined in this context - \"if the training being perfect\" - \"tend to get stuck in some sub-optimals\" - the learned distribution \"collapseS\" - \"since the data distribution is, thus ...\" - \"that measures a\" -> \"that estimates a ...\"? - \"a predictive module\": a bit unclear - generative v. discriminative is more usual terminology - \"is well ensured\" - \"with the cost of diversity\" -> \"at the cost of diversity\"? - \"has theoretical guarantee\" - in the references: \"ALIAS PARTH GOYAL\" (all caps) - \"let p denote the intermediate states\": I don't understand what this is. Where is \"p\" used? (proof of Theorem 3) - \"CoT theoretically guarantees the training effectiveness\": what does that mean? - Figure 3: \"epochs\" -> \"Epochs\" - Algorithm 1: what does \"mixed balanced samples\" mean? Make this more precise - \"wide-ranged\" - Equation 10 is too long and equation number is not properly formatted - Figures hard to read in black & white - Figure 2 doesn't use the same limits for the Y axis of the two NLL plots, making comparisons difficult. The two NLL plots are also not side-by-side", "rating": "7: Good paper, accept", "reply_text": "Thanks for reviewing our paper ! Response to your concerns : We will provide with a carefully-revised version of the paper according to your generous suggestions . Answer to the questions : 1 . If there is no constraint on the entropy of the solution of the objective function , the objective would not be equivalent to minimization of JSD . Instead , it would simply be calculating the entropy of M , which is useless . 2.Figure 2 ( a ) ( b ) shows CoT is more robust under its main evaluation to g-m balance compared to the g-d balance of SeqGAN . Ideally , Figure 2 ( a ) should also be showing SeqGAN 's performance under evaluation of JSD , however , in our attempts , SeqGAN always diverges under such evaluation . Figure 2 ( c ) show that the convergence of CoT is steady and quite fast under evaluation of NLL_ { oracle } , which is biased on quality ."}}