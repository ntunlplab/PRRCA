{"year": "2018", "forum": "rkLyJl-0-", "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks", "decision": "Accept (Poster)", "meta_review": "Pros:\n+ Clearly written paper.\n+ Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases.\n+ Thorough evaluation against the state of the art.\n\nCons:\n- No theoretical guarantees for the algorithm.\n\nThis paper belongs in ICLR if there is enough space.\n", "reviews": [{"review_id": "rkLyJl-0--0", "review_text": "The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. In general, I like the paper. To me, Algorithm 1 looks like some kind of proximal-point type algorithm. Algorithm 2 is more heuristic approach, with a couple of parameters to tune it. Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. E.g. there is no results of Algorithm 1. I know it serves as a motivation, but it would be nice to see how it works. Otherwise, the paper is clearly written. The topic is important, but I am a bit afraid of significance. One thing what I do not understand is, that why they did not compare with Adam? (they mention Adam algorithm soo many times, that it should be compared to). I am also not sure, how sensitive the results are for different datasets? Algorithm 2 really needs so many parameters (not just learning rate). How \\alpha, \\beta, \\gamma, \\mu, \\eta, K influence the speed? how sensitive is the algorithm for different choices of those parameters? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you AnonReviewer2 for your comments . Here are our responses : We have added a number of new experiments , including ( 1 ) Solving a stochastic convex optimization problem ( where the Neumann optimizer is far better than SGD or Adam ) , ( 2 ) Comparisons with Adam on Inception-V3 ( see below ) and ( 3 ) Multiple runs of the Neumann algorithm on Inception-V3 showing that the previous experiments are reproducible . To the comment about running Algorithm 1 : we \u2019 ve run it on stochastic convex problems before , where it performs much better than either SGD or Adam . On deep neural nets , our earlier experience with similar \u201c two-loop \u201d algorithms ( i.e. , freeze the mini-batch , and perform substantial inner-loop computation ) lead us to the conclusion that Algorithm 1 would most likely not perform very well at training deep neural nets . The main difficulty is that the inner loop iterations \u201c overfit \u201d to the mini-batch . As you mentioned , this is meant to purely motivational for Algorithm 2 . Adam achieves similar ( or worse ) results to the RMSprop baselines ( Figure 1 ) : in comparison to our Neumann optimizer , the training is slower , the output model is lower quality , and the optimizer scales poorly . When training with Adam , we observed instability with default parameters ( especially , epsilon ) . We changed it to 0.01 and 1.0 and have two runs which show dramatically different results . Our initial reason for not including comparisons to Adam was that we wanted to use standard models and training parameters ( i.e. , the Inception and Resnet papers use RMSprop ) . We think that the significance in our paper lies in the strong experimental results : 1 . Significantly improved accuracy in output models ( using a small number of workers ) over published baselines -- i.e. , just switching over to our optimizer will increase accuracy by 0.8-0.9 % . 2.Excellent scaling behaviour ( even using a very large number of workers ) . For example , our experimental results for ( 2 ) are strictly stronger than those in the literature for large batch training . The results that we have hold for ImageNet , but also for CIFAR-10 and CIFAR-100 with no change in hyperparameters , so we think that the results are likely to carry over to most modern CNN architectures on image datasets -- the hyperparameter choice will likely work out of the box ( much like the beta_1 , beta_2 and epsilon parameters in Adam ) . We agree that there appears to be quite a few hyperparameters , but \\alpha and \\beta are regularization coefficients , so they have to be roughly scaled to the loss ; \\gamma is a moving average coefficient and never needs to be changed ; \\mu is dependent only on time , not the model ; finally , training is quite insensitive to K ( as mentioned in Section 3.2 ) . Thus , the only hyperparameter that needs to be specified is the learning rate \\eta , and that does determine the speed of optimization ."}, {"review_id": "rkLyJl-0--1", "review_text": " This paper presents a new 2nd-order algorithm that implicitly uses curvature information, and it shows the intuition behind the approximation schemes in the algorithms and also validates the heuristics in various experiments. The method involves using Neumann Series and Richardson iteration to avoid Hessian-vector product in second order method for NN. In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization. The numerical examples are relatively clear and easy to figure out details. 1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer. For example, one simple experiment would be showing how it works for convex problems, e.g., logistic regression. Realistic DNN systems are very complex, and evaluating the method in a simple setting would help a lot in determining what if anything is novel about the method. 2. Also, for deep learning problems, it would be more convincing to see how different initialization can affect the performances. 3. Although the authors present their algorithm as a second order method at beginning, the final algorithm is kind of like a complex momentum SGD with limited memory. Rather than simply throwing out a new method with a new name, it would be helpful to understand what the steps of this method are implicitly doing. Please explain more about this. 4. It said that the algorithm is hyperparameter free except for learning rate. However, it is hard to see why there is no need to tune other hyperparameters, e.g., Cubic Regularizer, Repulsive Regularizer. The effect/sensitivity of hyperparameters for second order methods are quite different than hyperparameters for first order methods, and it is of interest to know how hyperparameters for implicit second order methods perform. 5. For Section 4.2, the well know benefit by using large batch size to train models is that it could reduce training time and epochs. However, from Table 3, there is no such phenomenon. Please explain. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you AnonReviewer3 for your thoughts and comments : we address your comments below and hope to clear up one misconception ( caused by poor labelling of Table 3 ) : 1 . We have added an experiment in Appendix B to show the results on a synthetic logistic regression problem . We compared the Neumann optimizer with SGD , Adam and a Newton algorithm for varying batch sizes . Our method outperforms SGD and Adam consistently , and while Newton \u2019 s method descends to a better loss , it comes at a steep per-step cost . We believe there are other large batch methods like Nesterov and SVRG that might get to lower losses than our method . However , none of these algorithms perform well on training a deep neural net . 2.We 've included an Appendix D with a new experiment illustrating that different initializations and trajectories of optimization all give the same quality model output ( for the Inception V3 model ) . 3.We 're not quite sure what the reviewer is looking for here : it seems that Section 2 gives a derivation of the method : the method is implicitly inverting the Hessian ( which is convexified after regularization ) of a mini-batch . Our algorithm crucially differs from standard momentum in that gradient evaluation occurs at a different point from the current iterate ( in Algorithm 1 ) , and we are not applying an exponential decay ( a standard momentum update would blow up if you did this ) . 4.We agree that it is of interest to further study the sensitivity to hyperparameters . The results that we have hold for ImageNet , but also for CIFAR-10 and CIFAR-100 with no change in hyperparameters , so we think that the results are likely to carry over to most modern CNN architectures on image datasets -- the hyperparameter choice will likely work out of the box ( much like the beta_1 , beta_2 and epsilon parameters in Adam ) . We agree that there appears to be quite a few hyperparameters , but \\alpha and \\beta are regularization coefficients , so they have to be roughly scaled to the loss ; \\gamma is a moving average coefficient and never needs to be changed ; \\mu is dependent only on time , not the model ; finally , training is quite insensitive to K ( as mentioned in Section 3.2 ) . Thus , the only hyperparameter that needs to be specified is the learning rate \\eta , and that does determine the speed of optimization . 5.The epochs listed in Table 3 are total epochs ( i.e. , total sum of all samples seen by all workers ) , so using twice as many workers is in fact twice as fast ( we 've updated the table to clarify this ) . We 're a little concerned that we were not clear on the significance of the experimental results : our algorithm scales up to a batch size of 32000 ( beating state-of-the-art for large batch training ) , and we obtain linear speedups across this regime i.e. , we can run 500 workers , in 1/10th the time that it takes the usual 50 worker baseline . We think of this as the major contribution of our work ."}, {"review_id": "rkLyJl-0--2", "review_text": "Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training. The paper also provides the effectiveness of the algorithm by training ImageNet models (Inception-V3, Resnet-50, Resnet-101, and Inception-Resnet-V2). Comments: I really appreciate the author(s) by providing experiments using real models on the ImageNet dataset. The algorithm seems to be easily used in practice. I do not have many comments for this paper since it focuses only in practical view without theory guarantee rigorously. As you mention in the paper that the algorithm uses the same amount of computation and memory as Adam optimizer, but could you please provide the reason why you only compare Neumann Optimizer with Baseline RMSProp but not with Adam? As we know, Adam is currently very well-known algorithm to train DNN. Do you think it would be interesting if you could compare the efficiency of Neumann optimizer with Adam? I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm. The question is that, with the given architectures and dataset, what algorithm should people consider to use between Neumann optimizer and Adam? Why should people use Neumann optimizer but not Adam, which is already very well-known? If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published. Minor comments: Page 3, in eq. (3): missing \u201c-\u201c sign Page 3, in eq. (6): missing \u201ctranspose\u201d on \\nabla \\hat{f} Page 4, first equation: O(|| \\eta*mu_t ||^2) Page 5, in eq. (9): m_{k-1} ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you AnonReviewer1 for your feedback and comments . We ran a new set of experiments comparing Adam , RMSprop and Neumann ( Figure 1 ) . Adam achieves similar ( or worse ) results to the RMSprop baselines : in comparison to our Neumann optimizer , the training is slower , the output model is lower quality , and the optimizer scales poorly . When training with Adam , we observed instability with default parameters ( especially , epsilon ) . We changed it to 0.01 and 1.0 and have two runs which show dramatically different results . Our initial reason for not including comparisons to Adam was that we wanted to use standard models and training parameters ( i.e. , the Inception and Resnet papers use RMSprop ) . We hope that practitioners will consider Neumann over Adam for the following reasons : - Significantly higher quality output models when training using few GPUs . - Ability to scale up to vastly more GPUs/TPUs , and overall decreased training time . We \u2019 ve incorporated your minor comments -- thanks again !"}], "0": {"review_id": "rkLyJl-0--0", "review_text": "The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. In general, I like the paper. To me, Algorithm 1 looks like some kind of proximal-point type algorithm. Algorithm 2 is more heuristic approach, with a couple of parameters to tune it. Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. E.g. there is no results of Algorithm 1. I know it serves as a motivation, but it would be nice to see how it works. Otherwise, the paper is clearly written. The topic is important, but I am a bit afraid of significance. One thing what I do not understand is, that why they did not compare with Adam? (they mention Adam algorithm soo many times, that it should be compared to). I am also not sure, how sensitive the results are for different datasets? Algorithm 2 really needs so many parameters (not just learning rate). How \\alpha, \\beta, \\gamma, \\mu, \\eta, K influence the speed? how sensitive is the algorithm for different choices of those parameters? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you AnonReviewer2 for your comments . Here are our responses : We have added a number of new experiments , including ( 1 ) Solving a stochastic convex optimization problem ( where the Neumann optimizer is far better than SGD or Adam ) , ( 2 ) Comparisons with Adam on Inception-V3 ( see below ) and ( 3 ) Multiple runs of the Neumann algorithm on Inception-V3 showing that the previous experiments are reproducible . To the comment about running Algorithm 1 : we \u2019 ve run it on stochastic convex problems before , where it performs much better than either SGD or Adam . On deep neural nets , our earlier experience with similar \u201c two-loop \u201d algorithms ( i.e. , freeze the mini-batch , and perform substantial inner-loop computation ) lead us to the conclusion that Algorithm 1 would most likely not perform very well at training deep neural nets . The main difficulty is that the inner loop iterations \u201c overfit \u201d to the mini-batch . As you mentioned , this is meant to purely motivational for Algorithm 2 . Adam achieves similar ( or worse ) results to the RMSprop baselines ( Figure 1 ) : in comparison to our Neumann optimizer , the training is slower , the output model is lower quality , and the optimizer scales poorly . When training with Adam , we observed instability with default parameters ( especially , epsilon ) . We changed it to 0.01 and 1.0 and have two runs which show dramatically different results . Our initial reason for not including comparisons to Adam was that we wanted to use standard models and training parameters ( i.e. , the Inception and Resnet papers use RMSprop ) . We think that the significance in our paper lies in the strong experimental results : 1 . Significantly improved accuracy in output models ( using a small number of workers ) over published baselines -- i.e. , just switching over to our optimizer will increase accuracy by 0.8-0.9 % . 2.Excellent scaling behaviour ( even using a very large number of workers ) . For example , our experimental results for ( 2 ) are strictly stronger than those in the literature for large batch training . The results that we have hold for ImageNet , but also for CIFAR-10 and CIFAR-100 with no change in hyperparameters , so we think that the results are likely to carry over to most modern CNN architectures on image datasets -- the hyperparameter choice will likely work out of the box ( much like the beta_1 , beta_2 and epsilon parameters in Adam ) . We agree that there appears to be quite a few hyperparameters , but \\alpha and \\beta are regularization coefficients , so they have to be roughly scaled to the loss ; \\gamma is a moving average coefficient and never needs to be changed ; \\mu is dependent only on time , not the model ; finally , training is quite insensitive to K ( as mentioned in Section 3.2 ) . Thus , the only hyperparameter that needs to be specified is the learning rate \\eta , and that does determine the speed of optimization ."}, "1": {"review_id": "rkLyJl-0--1", "review_text": " This paper presents a new 2nd-order algorithm that implicitly uses curvature information, and it shows the intuition behind the approximation schemes in the algorithms and also validates the heuristics in various experiments. The method involves using Neumann Series and Richardson iteration to avoid Hessian-vector product in second order method for NN. In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization. The numerical examples are relatively clear and easy to figure out details. 1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer. For example, one simple experiment would be showing how it works for convex problems, e.g., logistic regression. Realistic DNN systems are very complex, and evaluating the method in a simple setting would help a lot in determining what if anything is novel about the method. 2. Also, for deep learning problems, it would be more convincing to see how different initialization can affect the performances. 3. Although the authors present their algorithm as a second order method at beginning, the final algorithm is kind of like a complex momentum SGD with limited memory. Rather than simply throwing out a new method with a new name, it would be helpful to understand what the steps of this method are implicitly doing. Please explain more about this. 4. It said that the algorithm is hyperparameter free except for learning rate. However, it is hard to see why there is no need to tune other hyperparameters, e.g., Cubic Regularizer, Repulsive Regularizer. The effect/sensitivity of hyperparameters for second order methods are quite different than hyperparameters for first order methods, and it is of interest to know how hyperparameters for implicit second order methods perform. 5. For Section 4.2, the well know benefit by using large batch size to train models is that it could reduce training time and epochs. However, from Table 3, there is no such phenomenon. Please explain. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you AnonReviewer3 for your thoughts and comments : we address your comments below and hope to clear up one misconception ( caused by poor labelling of Table 3 ) : 1 . We have added an experiment in Appendix B to show the results on a synthetic logistic regression problem . We compared the Neumann optimizer with SGD , Adam and a Newton algorithm for varying batch sizes . Our method outperforms SGD and Adam consistently , and while Newton \u2019 s method descends to a better loss , it comes at a steep per-step cost . We believe there are other large batch methods like Nesterov and SVRG that might get to lower losses than our method . However , none of these algorithms perform well on training a deep neural net . 2.We 've included an Appendix D with a new experiment illustrating that different initializations and trajectories of optimization all give the same quality model output ( for the Inception V3 model ) . 3.We 're not quite sure what the reviewer is looking for here : it seems that Section 2 gives a derivation of the method : the method is implicitly inverting the Hessian ( which is convexified after regularization ) of a mini-batch . Our algorithm crucially differs from standard momentum in that gradient evaluation occurs at a different point from the current iterate ( in Algorithm 1 ) , and we are not applying an exponential decay ( a standard momentum update would blow up if you did this ) . 4.We agree that it is of interest to further study the sensitivity to hyperparameters . The results that we have hold for ImageNet , but also for CIFAR-10 and CIFAR-100 with no change in hyperparameters , so we think that the results are likely to carry over to most modern CNN architectures on image datasets -- the hyperparameter choice will likely work out of the box ( much like the beta_1 , beta_2 and epsilon parameters in Adam ) . We agree that there appears to be quite a few hyperparameters , but \\alpha and \\beta are regularization coefficients , so they have to be roughly scaled to the loss ; \\gamma is a moving average coefficient and never needs to be changed ; \\mu is dependent only on time , not the model ; finally , training is quite insensitive to K ( as mentioned in Section 3.2 ) . Thus , the only hyperparameter that needs to be specified is the learning rate \\eta , and that does determine the speed of optimization . 5.The epochs listed in Table 3 are total epochs ( i.e. , total sum of all samples seen by all workers ) , so using twice as many workers is in fact twice as fast ( we 've updated the table to clarify this ) . We 're a little concerned that we were not clear on the significance of the experimental results : our algorithm scales up to a batch size of 32000 ( beating state-of-the-art for large batch training ) , and we obtain linear speedups across this regime i.e. , we can run 500 workers , in 1/10th the time that it takes the usual 50 worker baseline . We think of this as the major contribution of our work ."}, "2": {"review_id": "rkLyJl-0--2", "review_text": "Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training. The paper also provides the effectiveness of the algorithm by training ImageNet models (Inception-V3, Resnet-50, Resnet-101, and Inception-Resnet-V2). Comments: I really appreciate the author(s) by providing experiments using real models on the ImageNet dataset. The algorithm seems to be easily used in practice. I do not have many comments for this paper since it focuses only in practical view without theory guarantee rigorously. As you mention in the paper that the algorithm uses the same amount of computation and memory as Adam optimizer, but could you please provide the reason why you only compare Neumann Optimizer with Baseline RMSProp but not with Adam? As we know, Adam is currently very well-known algorithm to train DNN. Do you think it would be interesting if you could compare the efficiency of Neumann optimizer with Adam? I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm. The question is that, with the given architectures and dataset, what algorithm should people consider to use between Neumann optimizer and Adam? Why should people use Neumann optimizer but not Adam, which is already very well-known? If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published. Minor comments: Page 3, in eq. (3): missing \u201c-\u201c sign Page 3, in eq. (6): missing \u201ctranspose\u201d on \\nabla \\hat{f} Page 4, first equation: O(|| \\eta*mu_t ||^2) Page 5, in eq. (9): m_{k-1} ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you AnonReviewer1 for your feedback and comments . We ran a new set of experiments comparing Adam , RMSprop and Neumann ( Figure 1 ) . Adam achieves similar ( or worse ) results to the RMSprop baselines : in comparison to our Neumann optimizer , the training is slower , the output model is lower quality , and the optimizer scales poorly . When training with Adam , we observed instability with default parameters ( especially , epsilon ) . We changed it to 0.01 and 1.0 and have two runs which show dramatically different results . Our initial reason for not including comparisons to Adam was that we wanted to use standard models and training parameters ( i.e. , the Inception and Resnet papers use RMSprop ) . We hope that practitioners will consider Neumann over Adam for the following reasons : - Significantly higher quality output models when training using few GPUs . - Ability to scale up to vastly more GPUs/TPUs , and overall decreased training time . We \u2019 ve incorporated your minor comments -- thanks again !"}}