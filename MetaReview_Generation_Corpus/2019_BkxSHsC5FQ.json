{"year": "2019", "forum": "BkxSHsC5FQ", "title": "SupportNet: solving catastrophic forgetting in class incremental learning with support data", "decision": "Reject", "meta_review": "The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks. The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM. The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper. However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach. Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it. Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation).", "reviews": [{"review_id": "BkxSHsC5FQ-0", "review_text": "This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset. Merging the support vector data and new data, the network can keep the knowledge on the previous task. The use of support vector concept is interesting, but this paper has some issues to be improved. Pros and Cons (+) Interesting idea (+) Diverse experimental results on six datasets including benchmark and real-world datasets (-) Lack of related work on recent catastrophic forgetting (-) Limited comparing results (-) Limited analysis of feature regularizers Detailed comments - I am curious how we can assure that SVM's decision boundary is similar or same to NN's boundary - SupportNet is a method to use some of the previous data. For fair comparisons, SupportNet needs to be compared with other models using previous samples such as GEM [Lopez-Paz and Ranzato, 2017]. - Following papers are omitted in related work: 1. Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017. 2. Shin et al. Continual Learning with Deep Generative Replay, NIPS 2017. Also, the model needs to be compared with two models. - There is no result and analysis for feature regularizers. As the authors referred, the features of support vector data continuously change as the learning goes on. So, I am curious how the feature regularizer has effects on the performance. This can be performed by visualizing the change of support vector features via t-SNE as the incremental learning proceeds - The authors used 2000 support vectors for MNIST, Cifar-10, and Cifar-100. However, this size might be quite large considering their difficulty. - How is the pattern of EwC using some samples in the old dataset? - iCaRL was evaluated on ImageNet. Is there any reason not to be evaluated on ImageNet? - What kind of NNs is used for each dataset? And what kind of kernel is used for SVM? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "For reviewer 1 : We would like to thank the reviewer for the detailed comments , which helped us improve the manuscript . Please find below the detailed response to each comment . If possible , could you please explain more on comment ( 6 ) ? We did not fully understand the question . Detailed comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 1 ) `` I am curious how we can assure that SVM 's decision boundary is similar or same to NN 's boundary '' Reply : Thank you for asking this , which is the basis of SupporNet . In fact , the decision boundary of the whole neural network can be very different from that of SVM trained from the original data . However , what we discussed in the manuscript is the decision boundaries of the neural network 's last layer and that of SVM trained from the input of the network 's last layer . For more details , as we referred in the manuscript , please refer to [ Soudry et al.18 ] for the theoretical analysis . In terms of thorough experiments , you can refer to [ Li et al.18 ] .Although the assumption needs to be strong ( separable data ) to prove the decision boundary of SVM is the same as that of the deep learning 's last layer , in practice , the two decision boundaries are very similar as shown in [ Li et al.18 ] . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 2 ) `` SupportNet is a method to use some of the previous data . For fair comparisons , SupportNet needs to be compared with other models using previous samples such as GEM [ Lopez-Paz and Ranzato , 2017 ] . '' Reply : Thank you for pointing out this related work . In our experiments , we did compare our method with another method using the sampled old data idea , iCaRL , which is suggested by [ Kemker et al.18 ] to be the previous state-of-the-art method for class incremental learning . But following your suggestion , we added the comparison of SupportNet with GEM [ Lopez-Paz and Ranzato , 2017 ] and the other two methods you mentioned in the revision , using MNIST , which is the shared dataset of all the methods . We used the code released by the authors , except for modifying the code to make the experiment setting being consistent with the other methods . As suggested by Figure 3 ( A ) , all the three methods suggested by you indeed performed well , however , our method can still outperform the three methods with a large margin . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 3 ) `` Following papers are omitted in related work : ... '' Reply : Thank you very much for pointing out these two interesting works . We added the comparison with the two methods , whose results are shown in Figure 3 ( A ) . As suggested by the figure , these two methods indeed perform relatively well , however , SupportNet can still clearly outperform the two methods . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 4 ) `` There is no result and analysis for feature regularizers . As the authors referred , the features of support vector data continuously change as the learning goes on . So , I am curious how the feature regularizer has effects on the performance . This can be performed by visualizing the change of support vector features via t-SNE as the incremental learning proceeds '' Reply : Thank you very much for raising this up . Feature regularizer analysis is indeed important , which is also suggested by reviewer 3 . Following the reviewer 3 's suggestion , we have added the performance comparison of SupportNet only using feature regularizer and only using support data , whose results are shown in Fig.3 ( C ) and which shows that the feature regularizer indeed contributes to the performance gain of SupportNet . But your suggestion is also very constructive . Using t-SNE can help us understand what the feature representation has been learned by the model . However , only checking the feature representation of the support data is not enough . In fact , we can constrain the feature representation of the support data very strictly by using a large feature regularizer coefficient . This will have two disadvantages . First , the model 's flexibility is reduced and the model has less capacity to learn new classes . Second , the feature representation of non-support data may change which can have a negative impact on the model 's performance . In fact , what we want is the stability of the feature representation of all the old data instead of just the support data . To show that , we added the t-SNE plotting of feature representation of 2000 random data points along the training process in Section D in the Appendix . As shown in the figures , the feature representation still varies , but compared with those which we do not apply regularizers , the shape variance is much smaller . This suggests that the EWC regularizer and feature regularizer can indeed stabilize the hidden representation of old data , which is important for the effectiveness of SupportNet . -- -Thank you again and we will continue in the next post -- -"}, {"review_id": "BkxSHsC5FQ-1", "review_text": "Summary: The authors offer a novel incremental learning method called SupportNet to combat catastrophic forgetting that can be seen in standard deep learning models. Catastrophic forgetting is the phenomenon where the networks don\u2019t retain old knowledge when they learn new knowledge. SupportNet uses resnet network with 32 layers, trains an SVM on the last layer and the support vector points from this SVM are given to the network along with the new data. Furthermore, two regularizers, feature and EWC regularizer, are added to the network. The feature regularizer forces the network to produce fixed representation for the old data, since if the feature representation for the old data changes when the network is fine-tuned on the new data then the support vectors generated from the old feature representation of the old data would become invalid. The EWC regularizer works by constraining parameters crucial for the classification of the old data, making it harder for the network to change them. SupportNet is compared to five methods (all data: network is re-trained with new and old data, upper bound for performance, iCaRL: state-of-the-art method for incremental learning, EWC: Only EWC regularizer added, Fine tune: Only new data, Random guessing: Random guess to assign labels) on six datasets (MNIST, CIFAR-10, CIFAR-100, Enzyme Function Prediction, HeLa Subcellular Structure Classification, Breast Tumor Classification). It shows some improvement in overall accuracy with each newly added class when compared to iCaRL, EWC, Fine Tune and Random guessing. Additionally, they show that overfitting for the real training data (a chosen subset of old data and the new data) is a problem for the competition iCaRL and affects SupportNet to a much lesser degree. Pros: (1) The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. Additionally, they offer a feature regularizer that penalizes the network for changing the feature representation of the support data when training the network on new data and an EWC regularizer that constrains the parameters that are crucial for the classification of the old data and makes it harder to change them. (2) The authors use six different datasets and several other approaches (subsets of their method\u2019s components, other competing methods) to show these three components alleviate catastrophic forgetting and show improvement in overall accuracy. (3) The paper is well written and easy to follow. Cons: Major Points: (1) To show that the method proposed in the paper addresses catastrophic forgetting, in addition to the overall accuracy shown in Figure 3, it is also necessary to show the accuracy of different models on old classes when new classes are added to the network. This will strengthen the argument that the improvement in accuracy is indeed due to correct classification on old data. (2) The authors claim that iCaRL suffers from overfitting on real training data (section 4.1) however Table 2 shows iCaRL only on the enzyme function prediction which is also the dataset where the difference in performance between iCaRL and SupportNet is the largest. To support the general overfitting claim made in section 4.1, the authors should repeat this analysis on any of the other five datasets where the performance difference between the two methods is much smaller. SupportNet also suffers from overfitting (Table 3, Accuracy: test data: 83.9%, real training data: 98.7%) although to a lesser extent than iCaRL. (3) The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed. To prove their usefulness, add two methods to Figure 3: (a)A method that uses support points without any regularizer. (b) A method that uses support points with just the feature regularizer. Other points: (1) In section 2.3.2, EWC regularizer, Eq. 9: We think F(theta_new) should be F(theta_old) since we want to constrain parameters crucial for classification of old data and should be computing Fisher Information for the old parameters. (2) In section 2.1 Deep Learning and SVM: additional steps are needed to show how Eq. 3 is derived from Eq. 2. (3) In section 2.1 Deep Learning and SVM: In the line before Eq. 4. \u201ct represtent\u201d instead of \u201ct represents\u201d. (4) Figures are small and hard to read. Please increase the size and resolution of the figures. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to sincerely thank you for your insightful and detailed comments , which summarized the pros and cons of our manuscript comprehensively and pointed out the direction for us to improve the manuscript . Please find below our point-by-point reply to your comments . Major Points : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 1 ) . `` To show that the method proposed in the paper addresses catastrophic forgetting , in addition to the overall accuracy shown in Figure 3 , it is also necessary to show the accuracy of different models on old classes when new classes are added to the network . This will strengthen the argument that the improvement in accuracy is indeed due to correct classification on old data . '' Reply : This is a very good suggestions . We initially included a series of confusion matrices in the manuscript to show different methods ' performance on the old classes . But since we want to control the number of pages to be around 8 , we removed those confusion matrices in the submitted version . We now put the confusion matrices back in Section B in the Appendix . In addition , we added a series of accuracy matrices , showing the performance of different methods on old classes on MNIST along the incremental training process in Section C in the Appendix . Those confusion matrices and accuracy matrices show that SupportNet 's performance over the old classes is very close to that of the newest classes and SupportNet 's improvement over accuracy is indeed owing to the correct prediction on the old class data . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 2 ) . `` To support the general overfitting claim made in section 4.1 , the authors should repeat this analysis on any of the other five datasets where the performance difference between the two methods is much smaller . SupportNet also suffers from overfitting ( Table 3 , Accuracy : test data : 83.9 % , real training data : 98.7 % ) although to a lesser extent than iCaRL . '' Reply : Thank you for this nice suggestion . We repeated the same analysis on MNIST and added the result to Table 2 . The result of MNIST shares the same pattern of the result on EC data . In terms of your comments on overfitting , we would like to explain a little bit more . As we discussed in Section 4.1 , what we want to show in Table 2 is that the support data selected by our method are critical for the deep learning training . To show that , we reported the model 's performance on 'All training data ' ( all the training data of all the time points that had once been fed to the model , including both all the old training data and the new training data ) and 'Real training data ' ( the training data of the last time point , including the new training data and the support data ) in Table 2 . The results in Table 2 indeed support our claim , since the performance difference of SupportNet between 'Real training data ' ( 98.7 % ) and 'All training data ' ( 92.0 % ) is much small than that of iCaRL ( 99.1 % and 62.6 % ) . But you are right , SupportNet is not free of overfitting , which is the common problem of all deep learning based methods and should be handled with more efforts in the future . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 3 ) . `` The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed . To prove their usefulness , add two methods to Figure 3 : ( a ) A method that uses support points without any regularizer . ( b ) A method that uses support points with just the feature regularizer. `` Reply : Thank you for the very nice suggestion . We take the EC data as an example and added the comparison , which is shown in Fig.3 ( C ) .As shown in the figure , even with the support data along and without any regularizers , SupportNet can already outperform iCaRL on this dataset , which shows the effectiveness of combining SVM with deep learning to select the critical data points for training deep learning model in the continual learning scenario . At the same time , the two regularizers , feature regularizer and EWC regularizer , can improve the performance of SupportNet to different extents . Basically , the EWC regularizer has a larger impact on the SupportNet 's performance than the feature regularizer . It is understandable since the EWC regularizer will influence each parameter individually according to their contribution while the feature regularizer tries to preserve the feature representations of the support data . Using feature regularizer along , the feature representations of the non-support data might be changed because of further training , which can have a negative impact on the model 's overall performance . Since those two regularizers are responsible for different aspects in our framework , combining those two consolidation regularizers together with the support data , SupportNet can achieve the optimal performance . -- -Thank you again and we will continue in the next post -- -"}, {"review_id": "BkxSHsC5FQ-2", "review_text": "This paper presents a continual learning method that aims to overcome the catastrophic forgetting problem by holding out small number of samples for each task to be used in training for new tasks. Specifially, these representative samples for each task are selected as support vectors of a SVM trained on it. The proposed method, SupportNet, is validated on a continual learning task of a classifier against two existing continual learning approaches, which it outperforms. Pros - Idea of using SVM to identify the most important samples for classification makes sense. Cons - The idea of storing a small subset of a original dataset for each task has been already explored in [Nguyen et al. 18], and thus is not novel. - Thus the contribution of this work reduces to the use of SVM to identify the most important samples, but the effectiveness of this approach is not validated since it does not compare against [Nguyen et al. 18]. - Also it leaves out many of the recent work on continual learning. - The idea of using SVM for identifying important samples is not very attractive since an SVM will have a very different decision boundary from the one from the trained DNN. - Also this method is only applicable to a classification task and not to other tasks such as regression or RL. Thus considering the lack of novelty and experimental validation, I recommend rejecting this paper. [Nguyen et al. 18] Variational Continual Learning, ICLR 2018", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for the comments . Please find below for the detailed responses to each comment . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 1,2 ) `` - The idea of storing a small subset of a original dataset for each task has been already explored in [ Nguyen et al.18 ] , and thus is not novel . - Thus the contribution of this work reduces to the use of SVM to identify the most important samples , but the effectiveness of this approach is not validated since it does not compare against [ Nguyen et al.18 ] . `` Reply : Thank you very much for pointing out this very recent paper [ Nguyen et al.18 ] , which was published several months ago . The method is indeed a very good method and we are aware of it months ago . Unfortunately , at that time , we have already finished the manuscript and did not compare with it in the original submission . In the revision , we added the comparison of SupportNet with the three versions of this method and all the other methods pointed out by the other reviewer . We performed the experiments on the dataset that was used by all the methods , MNIST . We used the code published by the authors and modified it to make the experiment environment being consistent . As suggested by Fig.3 ( A ) , this very recent method , VCL , indeed performed very impressively , outperforming all the other method , except for SupportNet . In fact , the coreset version of VCL 's performance is very close to that of SupporNet . But nevertheless , SupportNet can still outperform all the three versions of VCL along the incremental learning process . However , we would like to further clarify on your comment , which , we believe , is somehow unfair . First , [ Nguyen et al.18 ] was published after we almost finished our initial paper . Second , using the subset of old data for further training is indeed not a novel idea , but it is a category of methods , called rehearsal methods [ Parisi et al.18 ] .It was proposed long before [ Nguyen et al.18 ] .iCaRL [ Rebuff et al.17 ] and GEM [ Lopez-Paz and Ranzato , 2017 ] also belong to this category . Third , not comparing against [ Nguyen et al.18 ] does not mean we were unable to or did not validate the effectiveness of the proposed method in the original submission . In fact , we compared our method with the iCaRL [ Rebuff et al.17 ] in the original submission , which is a representative and previous state-of-the-art method within this category , as suggested by [ Kemker et al.18 ] .Furthermore , we compared our method with the performance empirical upper bound method : training new models from scratch using all the available dataset at each time point . All the results in the original submission clearly suggested the effectiveness of using SVM to identify the most important samples since it can outperform iCaRL with a large margin on all the dataset and even achieve near-optimal performance on EC dataset and MNIST . Fourth , proposing a simple and effective method of selecting the important data point is not a marginal contribution . As suggested by [ Parisi et al.18 ] , for this kind of methods , the most important things are to find the most useful and important points to preserve and to build up the framework to take advantage of the old data and the new data . Furthermore , instead of being just the support data selector , SupportNet is a framework , which contains both the data selector and the consolidation regularizers . The effectiveness of its components is well-tested by the comprehensive experiments in the manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 3 ) `` Also it leaves out many of the recent work on continual learning . '' Reply : Thank you very much for raising this concern ! In the original version , we compared our method with the previous state-of-the-art method for class incremental learning , iCaRL , as suggested by [ Kemker et al.18 ] .However , your concern is reasonable . We have added the comparison of SupportNet with all the other recent methods that have been pointed out by the reviewers , which is shown in Fig.3 ( A ) .The comprehensive experiments demonstrate the effectiveness of SupportNet . -- -Thank you again and we will continue in the next post -- -"}], "0": {"review_id": "BkxSHsC5FQ-0", "review_text": "This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset. Merging the support vector data and new data, the network can keep the knowledge on the previous task. The use of support vector concept is interesting, but this paper has some issues to be improved. Pros and Cons (+) Interesting idea (+) Diverse experimental results on six datasets including benchmark and real-world datasets (-) Lack of related work on recent catastrophic forgetting (-) Limited comparing results (-) Limited analysis of feature regularizers Detailed comments - I am curious how we can assure that SVM's decision boundary is similar or same to NN's boundary - SupportNet is a method to use some of the previous data. For fair comparisons, SupportNet needs to be compared with other models using previous samples such as GEM [Lopez-Paz and Ranzato, 2017]. - Following papers are omitted in related work: 1. Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017. 2. Shin et al. Continual Learning with Deep Generative Replay, NIPS 2017. Also, the model needs to be compared with two models. - There is no result and analysis for feature regularizers. As the authors referred, the features of support vector data continuously change as the learning goes on. So, I am curious how the feature regularizer has effects on the performance. This can be performed by visualizing the change of support vector features via t-SNE as the incremental learning proceeds - The authors used 2000 support vectors for MNIST, Cifar-10, and Cifar-100. However, this size might be quite large considering their difficulty. - How is the pattern of EwC using some samples in the old dataset? - iCaRL was evaluated on ImageNet. Is there any reason not to be evaluated on ImageNet? - What kind of NNs is used for each dataset? And what kind of kernel is used for SVM? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "For reviewer 1 : We would like to thank the reviewer for the detailed comments , which helped us improve the manuscript . Please find below the detailed response to each comment . If possible , could you please explain more on comment ( 6 ) ? We did not fully understand the question . Detailed comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 1 ) `` I am curious how we can assure that SVM 's decision boundary is similar or same to NN 's boundary '' Reply : Thank you for asking this , which is the basis of SupporNet . In fact , the decision boundary of the whole neural network can be very different from that of SVM trained from the original data . However , what we discussed in the manuscript is the decision boundaries of the neural network 's last layer and that of SVM trained from the input of the network 's last layer . For more details , as we referred in the manuscript , please refer to [ Soudry et al.18 ] for the theoretical analysis . In terms of thorough experiments , you can refer to [ Li et al.18 ] .Although the assumption needs to be strong ( separable data ) to prove the decision boundary of SVM is the same as that of the deep learning 's last layer , in practice , the two decision boundaries are very similar as shown in [ Li et al.18 ] . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 2 ) `` SupportNet is a method to use some of the previous data . For fair comparisons , SupportNet needs to be compared with other models using previous samples such as GEM [ Lopez-Paz and Ranzato , 2017 ] . '' Reply : Thank you for pointing out this related work . In our experiments , we did compare our method with another method using the sampled old data idea , iCaRL , which is suggested by [ Kemker et al.18 ] to be the previous state-of-the-art method for class incremental learning . But following your suggestion , we added the comparison of SupportNet with GEM [ Lopez-Paz and Ranzato , 2017 ] and the other two methods you mentioned in the revision , using MNIST , which is the shared dataset of all the methods . We used the code released by the authors , except for modifying the code to make the experiment setting being consistent with the other methods . As suggested by Figure 3 ( A ) , all the three methods suggested by you indeed performed well , however , our method can still outperform the three methods with a large margin . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 3 ) `` Following papers are omitted in related work : ... '' Reply : Thank you very much for pointing out these two interesting works . We added the comparison with the two methods , whose results are shown in Figure 3 ( A ) . As suggested by the figure , these two methods indeed perform relatively well , however , SupportNet can still clearly outperform the two methods . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 4 ) `` There is no result and analysis for feature regularizers . As the authors referred , the features of support vector data continuously change as the learning goes on . So , I am curious how the feature regularizer has effects on the performance . This can be performed by visualizing the change of support vector features via t-SNE as the incremental learning proceeds '' Reply : Thank you very much for raising this up . Feature regularizer analysis is indeed important , which is also suggested by reviewer 3 . Following the reviewer 3 's suggestion , we have added the performance comparison of SupportNet only using feature regularizer and only using support data , whose results are shown in Fig.3 ( C ) and which shows that the feature regularizer indeed contributes to the performance gain of SupportNet . But your suggestion is also very constructive . Using t-SNE can help us understand what the feature representation has been learned by the model . However , only checking the feature representation of the support data is not enough . In fact , we can constrain the feature representation of the support data very strictly by using a large feature regularizer coefficient . This will have two disadvantages . First , the model 's flexibility is reduced and the model has less capacity to learn new classes . Second , the feature representation of non-support data may change which can have a negative impact on the model 's performance . In fact , what we want is the stability of the feature representation of all the old data instead of just the support data . To show that , we added the t-SNE plotting of feature representation of 2000 random data points along the training process in Section D in the Appendix . As shown in the figures , the feature representation still varies , but compared with those which we do not apply regularizers , the shape variance is much smaller . This suggests that the EWC regularizer and feature regularizer can indeed stabilize the hidden representation of old data , which is important for the effectiveness of SupportNet . -- -Thank you again and we will continue in the next post -- -"}, "1": {"review_id": "BkxSHsC5FQ-1", "review_text": "Summary: The authors offer a novel incremental learning method called SupportNet to combat catastrophic forgetting that can be seen in standard deep learning models. Catastrophic forgetting is the phenomenon where the networks don\u2019t retain old knowledge when they learn new knowledge. SupportNet uses resnet network with 32 layers, trains an SVM on the last layer and the support vector points from this SVM are given to the network along with the new data. Furthermore, two regularizers, feature and EWC regularizer, are added to the network. The feature regularizer forces the network to produce fixed representation for the old data, since if the feature representation for the old data changes when the network is fine-tuned on the new data then the support vectors generated from the old feature representation of the old data would become invalid. The EWC regularizer works by constraining parameters crucial for the classification of the old data, making it harder for the network to change them. SupportNet is compared to five methods (all data: network is re-trained with new and old data, upper bound for performance, iCaRL: state-of-the-art method for incremental learning, EWC: Only EWC regularizer added, Fine tune: Only new data, Random guessing: Random guess to assign labels) on six datasets (MNIST, CIFAR-10, CIFAR-100, Enzyme Function Prediction, HeLa Subcellular Structure Classification, Breast Tumor Classification). It shows some improvement in overall accuracy with each newly added class when compared to iCaRL, EWC, Fine Tune and Random guessing. Additionally, they show that overfitting for the real training data (a chosen subset of old data and the new data) is a problem for the competition iCaRL and affects SupportNet to a much lesser degree. Pros: (1) The authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. Additionally, they offer a feature regularizer that penalizes the network for changing the feature representation of the support data when training the network on new data and an EWC regularizer that constrains the parameters that are crucial for the classification of the old data and makes it harder to change them. (2) The authors use six different datasets and several other approaches (subsets of their method\u2019s components, other competing methods) to show these three components alleviate catastrophic forgetting and show improvement in overall accuracy. (3) The paper is well written and easy to follow. Cons: Major Points: (1) To show that the method proposed in the paper addresses catastrophic forgetting, in addition to the overall accuracy shown in Figure 3, it is also necessary to show the accuracy of different models on old classes when new classes are added to the network. This will strengthen the argument that the improvement in accuracy is indeed due to correct classification on old data. (2) The authors claim that iCaRL suffers from overfitting on real training data (section 4.1) however Table 2 shows iCaRL only on the enzyme function prediction which is also the dataset where the difference in performance between iCaRL and SupportNet is the largest. To support the general overfitting claim made in section 4.1, the authors should repeat this analysis on any of the other five datasets where the performance difference between the two methods is much smaller. SupportNet also suffers from overfitting (Table 3, Accuracy: test data: 83.9%, real training data: 98.7%) although to a lesser extent than iCaRL. (3) The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed. To prove their usefulness, add two methods to Figure 3: (a)A method that uses support points without any regularizer. (b) A method that uses support points with just the feature regularizer. Other points: (1) In section 2.3.2, EWC regularizer, Eq. 9: We think F(theta_new) should be F(theta_old) since we want to constrain parameters crucial for classification of old data and should be computing Fisher Information for the old parameters. (2) In section 2.1 Deep Learning and SVM: additional steps are needed to show how Eq. 3 is derived from Eq. 2. (3) In section 2.1 Deep Learning and SVM: In the line before Eq. 4. \u201ct represtent\u201d instead of \u201ct represents\u201d. (4) Figures are small and hard to read. Please increase the size and resolution of the figures. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to sincerely thank you for your insightful and detailed comments , which summarized the pros and cons of our manuscript comprehensively and pointed out the direction for us to improve the manuscript . Please find below our point-by-point reply to your comments . Major Points : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 1 ) . `` To show that the method proposed in the paper addresses catastrophic forgetting , in addition to the overall accuracy shown in Figure 3 , it is also necessary to show the accuracy of different models on old classes when new classes are added to the network . This will strengthen the argument that the improvement in accuracy is indeed due to correct classification on old data . '' Reply : This is a very good suggestions . We initially included a series of confusion matrices in the manuscript to show different methods ' performance on the old classes . But since we want to control the number of pages to be around 8 , we removed those confusion matrices in the submitted version . We now put the confusion matrices back in Section B in the Appendix . In addition , we added a series of accuracy matrices , showing the performance of different methods on old classes on MNIST along the incremental training process in Section C in the Appendix . Those confusion matrices and accuracy matrices show that SupportNet 's performance over the old classes is very close to that of the newest classes and SupportNet 's improvement over accuracy is indeed owing to the correct prediction on the old class data . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 2 ) . `` To support the general overfitting claim made in section 4.1 , the authors should repeat this analysis on any of the other five datasets where the performance difference between the two methods is much smaller . SupportNet also suffers from overfitting ( Table 3 , Accuracy : test data : 83.9 % , real training data : 98.7 % ) although to a lesser extent than iCaRL . '' Reply : Thank you for this nice suggestion . We repeated the same analysis on MNIST and added the result to Table 2 . The result of MNIST shares the same pattern of the result on EC data . In terms of your comments on overfitting , we would like to explain a little bit more . As we discussed in Section 4.1 , what we want to show in Table 2 is that the support data selected by our method are critical for the deep learning training . To show that , we reported the model 's performance on 'All training data ' ( all the training data of all the time points that had once been fed to the model , including both all the old training data and the new training data ) and 'Real training data ' ( the training data of the last time point , including the new training data and the support data ) in Table 2 . The results in Table 2 indeed support our claim , since the performance difference of SupportNet between 'Real training data ' ( 98.7 % ) and 'All training data ' ( 92.0 % ) is much small than that of iCaRL ( 99.1 % and 62.6 % ) . But you are right , SupportNet is not free of overfitting , which is the common problem of all deep learning based methods and should be handled with more efforts in the future . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 3 ) . `` The individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed . To prove their usefulness , add two methods to Figure 3 : ( a ) A method that uses support points without any regularizer . ( b ) A method that uses support points with just the feature regularizer. `` Reply : Thank you for the very nice suggestion . We take the EC data as an example and added the comparison , which is shown in Fig.3 ( C ) .As shown in the figure , even with the support data along and without any regularizers , SupportNet can already outperform iCaRL on this dataset , which shows the effectiveness of combining SVM with deep learning to select the critical data points for training deep learning model in the continual learning scenario . At the same time , the two regularizers , feature regularizer and EWC regularizer , can improve the performance of SupportNet to different extents . Basically , the EWC regularizer has a larger impact on the SupportNet 's performance than the feature regularizer . It is understandable since the EWC regularizer will influence each parameter individually according to their contribution while the feature regularizer tries to preserve the feature representations of the support data . Using feature regularizer along , the feature representations of the non-support data might be changed because of further training , which can have a negative impact on the model 's overall performance . Since those two regularizers are responsible for different aspects in our framework , combining those two consolidation regularizers together with the support data , SupportNet can achieve the optimal performance . -- -Thank you again and we will continue in the next post -- -"}, "2": {"review_id": "BkxSHsC5FQ-2", "review_text": "This paper presents a continual learning method that aims to overcome the catastrophic forgetting problem by holding out small number of samples for each task to be used in training for new tasks. Specifially, these representative samples for each task are selected as support vectors of a SVM trained on it. The proposed method, SupportNet, is validated on a continual learning task of a classifier against two existing continual learning approaches, which it outperforms. Pros - Idea of using SVM to identify the most important samples for classification makes sense. Cons - The idea of storing a small subset of a original dataset for each task has been already explored in [Nguyen et al. 18], and thus is not novel. - Thus the contribution of this work reduces to the use of SVM to identify the most important samples, but the effectiveness of this approach is not validated since it does not compare against [Nguyen et al. 18]. - Also it leaves out many of the recent work on continual learning. - The idea of using SVM for identifying important samples is not very attractive since an SVM will have a very different decision boundary from the one from the trained DNN. - Also this method is only applicable to a classification task and not to other tasks such as regression or RL. Thus considering the lack of novelty and experimental validation, I recommend rejecting this paper. [Nguyen et al. 18] Variational Continual Learning, ICLR 2018", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for the comments . Please find below for the detailed responses to each comment . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 1,2 ) `` - The idea of storing a small subset of a original dataset for each task has been already explored in [ Nguyen et al.18 ] , and thus is not novel . - Thus the contribution of this work reduces to the use of SVM to identify the most important samples , but the effectiveness of this approach is not validated since it does not compare against [ Nguyen et al.18 ] . `` Reply : Thank you very much for pointing out this very recent paper [ Nguyen et al.18 ] , which was published several months ago . The method is indeed a very good method and we are aware of it months ago . Unfortunately , at that time , we have already finished the manuscript and did not compare with it in the original submission . In the revision , we added the comparison of SupportNet with the three versions of this method and all the other methods pointed out by the other reviewer . We performed the experiments on the dataset that was used by all the methods , MNIST . We used the code published by the authors and modified it to make the experiment environment being consistent . As suggested by Fig.3 ( A ) , this very recent method , VCL , indeed performed very impressively , outperforming all the other method , except for SupportNet . In fact , the coreset version of VCL 's performance is very close to that of SupporNet . But nevertheless , SupportNet can still outperform all the three versions of VCL along the incremental learning process . However , we would like to further clarify on your comment , which , we believe , is somehow unfair . First , [ Nguyen et al.18 ] was published after we almost finished our initial paper . Second , using the subset of old data for further training is indeed not a novel idea , but it is a category of methods , called rehearsal methods [ Parisi et al.18 ] .It was proposed long before [ Nguyen et al.18 ] .iCaRL [ Rebuff et al.17 ] and GEM [ Lopez-Paz and Ranzato , 2017 ] also belong to this category . Third , not comparing against [ Nguyen et al.18 ] does not mean we were unable to or did not validate the effectiveness of the proposed method in the original submission . In fact , we compared our method with the iCaRL [ Rebuff et al.17 ] in the original submission , which is a representative and previous state-of-the-art method within this category , as suggested by [ Kemker et al.18 ] .Furthermore , we compared our method with the performance empirical upper bound method : training new models from scratch using all the available dataset at each time point . All the results in the original submission clearly suggested the effectiveness of using SVM to identify the most important samples since it can outperform iCaRL with a large margin on all the dataset and even achieve near-optimal performance on EC dataset and MNIST . Fourth , proposing a simple and effective method of selecting the important data point is not a marginal contribution . As suggested by [ Parisi et al.18 ] , for this kind of methods , the most important things are to find the most useful and important points to preserve and to build up the framework to take advantage of the old data and the new data . Furthermore , instead of being just the support data selector , SupportNet is a framework , which contains both the data selector and the consolidation regularizers . The effectiveness of its components is well-tested by the comprehensive experiments in the manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ( 3 ) `` Also it leaves out many of the recent work on continual learning . '' Reply : Thank you very much for raising this concern ! In the original version , we compared our method with the previous state-of-the-art method for class incremental learning , iCaRL , as suggested by [ Kemker et al.18 ] .However , your concern is reasonable . We have added the comparison of SupportNet with all the other recent methods that have been pointed out by the reviewers , which is shown in Fig.3 ( A ) .The comprehensive experiments demonstrate the effectiveness of SupportNet . -- -Thank you again and we will continue in the next post -- -"}}