{"year": "2021", "forum": "Oe2XI-Aft-k", "title": "Perturbation Type Categorization for Multiple $\\ell_p$ Bounded Adversarial Robustness", "decision": "Reject", "meta_review": "The paper proposes a model to defend against multiple lp norm attacks by classifying those attacks. The reviewers raised several concerns about the methodologies. Furthermore, it's not clear how the proposed algorithm can deal with an unseen attack (e.g., only trained on l1, l_infty attacks but encounter l2 attack in the testing phase). The assumption that the attack types are known beforehand is restricted.  ", "reviews": [{"review_id": "Oe2XI-Aft-k-0", "review_text": "This paper proposes an ensemble approach to deal with multiple perturbation types . The underlying idea is to train a robust classifier for each perturbation type ( i.e. , l1 , l2 , and l-inf ) and choose a model to predict based on the decision of a perturbation classifier which is trained to distinguish perturbation types . The idea is interesting , and the experiments seem solid ; however , I am hesitating to give this paper an acceptance decision due to some unclear points as follows : 1 . I have not checked the proof of Theorem 1 , but it seems a bit counter-intuitive to me . The reason is as follows . Let \u2019 s assume that we are attacking a clean image x . We start from x0 inside both l1 and l-inf balls for example . We further assume that during the process of updating x { t } , we never do any projection onto any ball ( i.e. , x { t } always lie inside the balls ) . If so , there is not any difference between x_adv w.r.t l1 and l_inf , meaning that there are possibly a certain number of adversarial examples shared across l1 and l-inf attacks . 2.In your Theorem 1 ( also your experiments ) , the radius of l1 is eps and that of l-inf is eps/sqrt ( d ) , meaning that the ball of l-inf is a subset of the ball of l1 . However , in Section 5.3 , you claimed that \u201c The dotted line shows the decision boundary for the perturbation classifier C_adv , which correctly classifies inputs subjected to large \u2018 1 perturbations \u03b400 as \u2018 1 attacks ( green ) , but can misclassify samples with smaller perturbations \u201d . It seems that you reckon l-inf adversarial examples are having more perturbation than l1 , do not you ? 3.Theorem 2 is not understandable to me . What is worst-case adversary ( this needs to be explained and defined because we can understand this notion in several different ways ) ? What is delta ? 4.I recommend testing Projector against the attack over a uniform average of Mp in addition to what is doing . 5.It is encouraging to compare your proposed method against ( Francesco Croce and Matthias Hein , 2020 a ) . You did mention this work , but not compare it in experiments .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailing your concerns . We are happy to note that you found the idea to be interesting , while being backed up with solid experiments . We attempt to clarify all your concerns in this response , and we will also revise our paper to make these points clearer . \u200c \u200c We discuss the * * separability of perturbation types * * in the common response : [ Non-Overlapping Nature of different perturbation regions ] ( https : //openreview.net/forum ? id=Oe2XI-Aft-k & noteId=V1q5prt8RqE ) . The response addresses Cons ( 1,2 ) in your review . Following up on this discussion : * The radius of l1 is eps and that of l-inf is eps/sqrt ( d ) , meaning that the ball of linf is a subset of the ball of l1 * While the * magnitude * of the radius of the $ \\ell_1 $ region is larger than the $ \\ell_\\infty $ region , none of the regions subsume the other . It is possible for a point in the $ \\ell_\\infty $ space to exist outside the $ \\ell_1 $ region . Example : Consider a 100-dimensional input $ \\delta = [ 0.3,0.3 , ... ,0.3 ] $ . It exists in the $ \\ell_\\infty $ region of $ \\epsilon_\\infty = 0.3 $ . However , the sum of absolute value of each component of $ \\delta $ , or its $ \\ell_1 $ norm is equal to 30 . Hence , the perturbation does not belong to a region of size $ \\epsilon_1 = 10 $ . \u200c \u200c * In Section 5.3 , you claimed ... It seems that you reckon l-inf adversarial examples are having more perturbation than l1 * Our discussion of the perturbation classifier does not utilize this assumption . Instead , we mean that for an $ \\ell_\\infty $ adversary , if the generated adversarial perturbation is large based on the $ \\ell_\\infty $ norm , then the perturbation classifier is likely to correctly classify the adversarial example as $ \\ell_\\infty $ . On the other hand , when the generated adversarial perturbation is small based on the $ \\ell_\\infty $ norm , the perturbation classifier may not correctly categorize the perturbation type , because the perturbation may not be recognizable enough . We will revise the description of Section 5.3 to make these points clearer , and we will clarify the meaning of small and large perturbations . \u200c \u200c # # # Theorem 2 # # # # * Terminologies * 1 . Worst-case adversary refers to an adaptive adversary that has the full knowledge of the defense strategy , and makes the strongest adversarial decision given the perturbation constraints . We will add the explanation in our revision . 2.As described in Equation 3 , $ \\delta $ is the perturbation induced over the original input $ x $ , such that $ x_ { adv } = x + \\delta $ . 3. $ \\alpha , \\sigma , d $ are defined in the problem description in Section 4.1 . \u200c \u200c # # # # * High-Level Summary * In Theorem 1 , we showed that it is possible to distinguish between the distributions of inputs that were subjected to $ \\ell_1 $ and $ \\ell_\\infty $ perturbations . In Theorem 2 , we prove lower-bounds on the robustness of our two-stage pipeline under an adaptive attacker , and show that the adversary faces a trade-off between fooling the perturbation classifier and the second-level models . \u200c * * What does the theorem claim ? * * Error rate of the * PROTECTOR * model , $ P_e $ is less than 0.01 under the considered threat model . * * What is the problem setting ? * * 1.Data points are sampled from $ \\mathcal { D } $ defined in Section 4.1 . 2.Adversarial perturbation regions considered are the $ \\ell_1 $ and $ \\ell_\\infty $ regions . 3.Protector pipeline consists of adversarially trained models $ M_ { 1 , \\epsilon_1 } , M_ { \\infty , \\epsilon_\\infty } $ , and an attack classifier $ C_ { adv } $ . 3.The perturbation size $ \\boldsymbol { \\epsilon_1 } = \\alpha + 2\\sigma $ and $ \\boldsymbol { \\epsilon_\\infty } = \\frac { \\alpha + 2\\sigma } { \\sqrt { d } } $ . For a more detailed description , please refer to the proof sketch of the theorem provided at the beginning of Appendix C. In a subsequent revision , we will utilize the additional page for the main paper to highlight the proof sketch , and provide a more intuitive explanation of the theorem . \u200c \u200c # # # Additional Comparisons As noted in the paper , we build on adversarial training baselines ( Tramer & Boneh 2019 , Maini et.al.2020 ) .However , Croce & Hein 2020 , study provable robustness . As a result , the perturbation sizes considered in their work are significantly smaller than those in empirical works such as ours . For example , in case of the MNIST dataset , while we study empirical robustness at $ \\epsilon_1 $ = 10.0 , Croce & Hein study the upper and lower bounds of the robust test error on $ \\epsilon_1 $ = 1.0 ( see Table 1 in [ Croce & Hein 2020 ] ( https : //arxiv.org/abs/1905.11213 ) ) . They present a range of certified robust error rates , without empirical results as evaluated in our work & the baselines considered . Thus , the two works have different objectives and are not empirically comparable . \u200cWe will make this point clearer in our revision . \u200c Thank you for your time . We will shortly provide evaluation results on the uniform average of $ M_p $ in a succeeding response . In the meantime , please let us know if you have any more questions ."}, {"review_id": "Oe2XI-Aft-k-1", "review_text": "In theoretical analysis , the authors conclude with an Gaussian assumption on the data distribution . This assumption is very restrictive , and the corresponding conclusion does not provide a general view of what is happening for real datasets . Gaussian condition on dataset is very unrealistic , and its theoretical analysis should only be considered as dealing with a toy model . Authors should remove this Gaussian assumption , or at least give strong reasons in plain natural language why Gaussianity can represent natural datasets in the adversarial study .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank\u200c \u200cyou\u200c \u200cfor\u200c \u200cyour\u200c \u200ccomment.\u200c \u200cWe\u200c \u200cwould\u200c \u200clike\u200c \u200cto\u200c \u200cpoint\u200c \u200cout\u200c \u200cthat\u200c \u200cin\u200c \u200cthe\u200c \u200cadversarial\u200c \u200cmachine\u200c \u200clearning\u200c \u200c literature , \u200c \u200cpeople\u200c \u200chave\u200c \u200crelied\u200c \u200con\u200c \u200ccertain\u200c \u200cassumptions\u200c \u200cto\u200c \u200cbuild\u200c \u200ctheoretical\u200c \u200cframeworks.\u200c \u200cThough\u200c \u200c these\u200c \u200ctheoretical\u200c \u200cframeworks\u200c \u200care\u200c \u200cbased\u200c \u200con\u200c \u200csimplified\u200c \u200cassumptions , \u200c \u200cthey\u200c \u200cmotivate\u200c \u200cempirical\u200c \u200c approaches\u200c \u200cthat\u200c \u200cwork\u200c \u200cin\u200c \u200cpractice.\u200c \u200cSpecifically , \u200c \u200cthe\u200c \u200cGaussian\u200c \u200cassumption\u200c \u200chas\u200c \u200cbeen\u200c \u200cmade\u200c \u200cin\u200c \u200ca\u200c \u200c number\u200c \u200cof\u200c \u200cprior\u200c \u200cwork\u200c \u200con\u200c \u200cadversarial\u200c \u200cmachine\u200c \u200clearning.\u200c \u200cIn\u200c \u200cthe\u200c \u200cfollowing , \u200c \u200cwe\u200c \u200clist\u200c \u200ca\u200c \u200cfew\u200c \u200cpapers\u200c \u200c published\u200c \u200cin\u200c \u200crecent\u200c \u200cmachine\u200c \u200clearning\u200c \u200cconferences : \u200c \u200c [ [ NeurIPS \u2019 19 ] \u200c \u200cIlyas\u200c \u200cet\u200c \u200cal. , \u200c \u200cAdversarial\u200c \u200cExamples\u200c \u200cAre\u200c \u200cNot\u200c \u200cBugs , \u200c \u200cThey\u200c \u200cAre\u200c \u200cFeatures ] ( https : //arxiv.org/abs/1905.02175 ) \u200c\u200c \u200c ( See\u200c \u200cSection\u200c \u200c 4 ) \u200c \u200c [ [ ICLR \u2019 19 ] \u200c \u200cTsipras\u200c \u200cet\u200c \u200cal. , \u200c \u200cRobustness\u200c \u200cMay\u200c \u200cBe\u200c \u200cat\u200c \u200cOdds\u200c \u200cwith\u200c \u200cAccuracy\u200c\u200c ] ( https : //arxiv.org/abs/1805.12152 ) \u200c ( See\u200c \u200cSection\u200c \u200c2.1 ) \u200c \u200c [ [ NeurIPS \u2019 19 ] \u200c \u200cTramer\u200c \u200cet\u200c \u200cal. , \u200c \u200cAdversarial\u200c \u200cTraining\u200c \u200cand\u200c \u200cRobustness\u200c \u200cfor\u200c \u200cMultiple\u200c \u200cPerturbations ] ( https : //arxiv.org/abs/1904.13000 ) \u200c\u200c \u200c ( See\u200c \u200cSection\u200c \u200c2.2 ) \u200c \u200c \u200c In\u200c \u200cour\u200c \u200cpaper , \u200c \u200cwe\u200c \u200cmention\u200c \u200cthat\u200c \u200cthe\u200c \u200cassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cour\u200c \u200cwork\u200c \u200care\u200c \u200cadapted\u200c \u200cfrom\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.\u200c \u200c ( Section\u200c \u200c4.1\u200c \u200cand\u200c \u200cAppendix\u200c \u200cA\u200c \u200cin\u200c \u200cour\u200c \u200cpaper ) .\u200c \u200cIn\u200c \u200cAppendix\u200c \u200cA\u200c \u200cof\u200c \u200cour\u200c \u200cpaper , \u200c \u200cwe\u200c \u200chave\u200c \u200ccompared\u200c \u200cour\u200c \u200c assumptions\u200c \u200cwith\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.\u200c \u200cWe\u200c \u200cdiscussed\u200c \u200cthat : \u200c \u200c ( 1 ) \u200c \u200cthe\u200c \u200cassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cour\u200c \u200cwork\u200c \u200cbetter\u200c \u200c represent\u200c \u200cthe\u200c \u200ccharacteristics\u200c \u200cof\u200c \u200creal-world\u200c \u200cimage\u200c \u200cdatasets ; \u200c \u200cand\u200c \u200c ( 2 ) \u200c \u200cour\u200c \u200cresults\u200c \u200cnaturally\u200c \u200chold\u200c \u200cwith\u200c \u200c assumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal. , \u200c \u200cwhich\u200c \u200cassumes\u200c \u200cthe\u200c \u200cpresence\u200c \u200cof\u200c \u200ca\u200c \u200cstochastic\u200c \u200cinput\u200c \u200cfeature\u200c \u200c ( explained\u200c \u200cin\u200c \u200cAppendix\u200c \u200cA ) .\u200c \u200c \u200c On\u200c \u200cthe\u200c \u200cother\u200c \u200chand , \u200c \u200cwe\u200c \u200cwould\u200c \u200clike\u200c \u200cto\u200c \u200chighlight\u200c \u200cthat\u200c \u200cwhile\u200c \u200cthe\u200c \u200ctheoretical\u200c \u200cproofs\u200c \u200care\u200c \u200cimportant\u200c \u200cparts\u200c \u200c of\u200c \u200cthe\u200c \u200cpaper\u200c \u200cto\u200c \u200cmotivate\u200c \u200cthe\u200c \u200c\u200c * PROTECTOR * \u200cframework , \u200c \u200cthe\u200c \u200cstrong\u200c \u200cempirical\u200c \u200cresults\u200c \u200care\u200c \u200calso\u200c \u200ckey\u200c \u200c contributions\u200c \u200cof\u200c \u200cour\u200c \u200cwork.\u200c \u200cIn\u200c \u200cSection\u200c \u200c6 , \u200c \u200cwe\u200c \u200cshow\u200c \u200cthat\u200c \u200con\u200c \u200cadversarial\u200c \u200cexamples\u200c \u200cgenerated\u200c \u200cusing\u200c \u200c various\u200c \u200cattack\u200c \u200calgorithms , \u200c \u200cthe\u200c \u200cProtector\u200c \u200cframework\u200c \u200coutperforms\u200c \u200cthe\u200c \u200cstate-of-the-art\u200c \u200cdefenses\u200c \u200cby\u200c \u200c large\u200c \u200cmargins.\u200c \u200cWe\u200c \u200curge\u200c \u200cyou\u200c \u200cto\u200c \u200ckindly\u200c \u200calso\u200c \u200cconsider\u200c \u200cthe\u200c \u200cempirical\u200c \u200cand\u200c \u200cmethodological\u200c \u200c components\u200c \u200cof\u200c \u200cour\u200c \u200cwork\u200c \u200cin\u200c \u200cyour\u200c \u200creview\u200c \u200cand\u200c \u200csubsequent\u200c \u200cdecision.\u200c \u200c \u200c"}, {"review_id": "Oe2XI-Aft-k-2", "review_text": "Summary : This paper proposed a pipeline method which first classify the attack type and then choose the proper predictor for that type . The authors provide both theoretical and experimental proof to support their pipeline method . Pros : ( 1 ) The idea is good and reasonable . Also , the paper is well written and easy to read . The proof part is clear and natural . ( 2 ) The result of theorem 1 is interesting and it proved the different perturbation types ( under this problem setting ) is separable with a high probability . Although the setting is a little restrictive , it is still very impressive . Cons : ( 1 ) The method and analysis only apply to the Lp attack type . It will be good if it can be extended . ( 2 ) Assume that we treat the two pipelines as the whole process ( end-end deep network ) , e.g. , the first layers determines the type and then that type will determine which part to activate for training the data points , that is , you have a network with special topology and structure . If the attacker only cares about this input and output of this network ( no pipeline here ) , we should have an attack strategy . How to explain this view using your analysis ? Could we say this is equivalent to changing the structure of the network ? From this perspective , could you tell the difference between these two views ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your insightful feedback on the work . We are glad you liked the clarity of the paper and our theoretical arguments that motivate the empirical results . We address your concerns and questions below , and we will revise our paper to make these points clearer . \u200c \u200c # # # $ \\ell_p $ Norm Setting We agree that the problem of multiple $ \\ell_p $ robustness may be a little restrictive , but it is also one of the only settings in which we have a standardized notion of attack and defense benchmarks . We believe that multiple perturbation robustness to other threat models will naturally follow in our * PROTECTOR * framework , as more attack types and individual models robust to them become standardized . However , in the absence of such attacks and defenses , it is difficult to demonstrate the efficacy of the same . We also note that the problem of multiple $ \\ell_p $ norm robustness has attracted a lot of attention from the community . Apart from the citations in the main paper , there are at least the following submissions at ICLR this year that also study the same problem : \u200c [ Towards Defending Multiple Adversarial Perturbations via Gated Batch Normalization ] ( https : //openreview.net/forum ? id=Utc4Yd1RD_s ) [ Composite Adversarial Training for Multiple Adversarial Perturbations and Beyond ] ( https : //openreview.net/forum ? id=H92-E4kFwbR ) [ Learning to Generate Noise for Multi-Attack Robustness ] ( https : //openreview.net/forum ? id=tv8n52XbO4p ) \u200c \u200c # # # Topological Model * If the attacker only cares about this input and output of this network ( no pipeline here ) , we should have an attack strategy . How to explain this view using your analysis ? Could we say this is equivalent to changing the structure of the network ? From this perspective , could you tell the difference between these two views ? * \u200c \u200c # # # # * * Attack Strategy * * For both the standard attack ( Eq ( 1 ) ) and the softmax adaptive attack ( Eq ( 4 ) ) , our entire pipeline ( with two stages ) is considered as a single model at test time . The adversary utilizes the knowledge of the input and output pairs from the entire pipeline to frame the attack strategy , as also suggested by you . In fact , the adaptive attack presented in ( Eq ( 4 ) ) further exploits the knowledge of the internal model structure to propagate full gradients through the two stages , and hence produces stronger adversarial examples than standard attacks that only care about model input and output . We agree that our entire pipeline is essentially equivalent to a topological model , for which the first few layers perform perturbation type categorization , and then specialized layers for different perturbation types follow . To explain why our pipeline is supposed to be more robust to multiple perturbation types , we would like to draw your attention towards the adversarial trade-off discussed in Theorem 2 and represented in Figure 1.b . Specifically , we design this topological structure to establish a trade-off for the adversary between fooling the two stages of the topology , even when it taking a unified view of the model and * not * attacking the two stages separately . \u200c \u200c # # # # * * Training the Topological Model * * The key advantage that * PROTECTOR * brings is in the ability to train such a topological model in the first place . Recent work ( [ [ ICML2020 ] Adversarial Robustness Against the Union of Multiple Perturbation Models ] ( https : //arxiv.org/abs/1909.04068 ) ) has demonstrated how naive augmentation of attacks from different perturbation types leads to unstable and inconsistent training when used to train a single robust model , and often suffers from various kinds of gradient masking . On the other hand , as mentioned in Section 5.1 , we observe that the training of the perturbation categorizer is stable and consistently achieves good performance . Further , as also detailed in Section 5.1 , these previous models take an extremely large amount of computational resources to train . For instance , the state-of-the-art network for $ \\ell_\\infty $ robustness ( with data augmentation ) takes nearly 2 GPU days ( https : //github.com/yaircarmon/semisup-adv ) . Doing the same with augmentation of data points for different types of $ \\ell_p $ attacks like in [ [ NeurIPS \u2019 19 ] Tramer et al. , Adversarial Training and Robustness for Multiple Perturbations ] ( https : //arxiv.org/abs/1904.13000 ) will require 3x the time . On the other hand , the * PROTECTOR * framework is able to leverage the strong predictions of pre-existing individually robust models to train a perturbation categorizer in only a few hours . We are happy to provide any further clarification about the work and thank you for your time ."}, {"review_id": "Oe2XI-Aft-k-3", "review_text": "The paper proposes a two-stage defense method to improve the adversarial robustness over different perturbation types . Specifically , it first builds a hierarchical binary classifier to differentiable the perturbation types and then uses the result to guide to its corresponding defense models . It first proves the different types of perturbations could be separable and the adversary could be weakened to fool the binary classifier . It shows their methods achieve a clear improvement in the experiments . Pros : 1.The paper is good-written and easy to follow . 2.The proposed idea is interesting . 3.The experiment is detailed and comprehensive . Cons : 1.There is a major problem in their method . In the last sentence of section 5.2 , it says uses the soft relaxation only in generating the adversarial example , but not for inference . It clearly caused the gradient making problem in the adversarial attack later on to test the robustness . The gradient is blocked before reaching into the binary classifier so that the adversarial attack fails , which I think it is not truly improving the model 's robustness . 2.In my opinion , this method is just a dynamic voting based model ensemble . Just take the binary classifier as a voting procedure . Therefore , the traditional adversarial attacks wo n't work in general . I would suggest using the soft relaxation in the inference as well for the adversarial attack . 3.Also , the assumption that different norm adversarial examples could be clearly separable might be wrong . You could find the adversarial examples that satisfy both l1 , l2 and l_inf constraint by just choosing the \\epsilon for every norm differently .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback and insightful comment on the possibility of gradient masking . We would like to clarify an important misunderstanding about the evaluation adequacy in this response , and will provide a detailed discussion on the adversarial example separability in a followup post . * '' In the last sentence of section 5.2 , it says uses the soft relaxation only in generating the adversarial example , but not for inference . It clearly caused the gradient making problem in the adversarial attack later on to test the robustness . `` * We would like to clarify that in our experiments , the adversary always constructs adversarial examples using the softmax mode in the pipeline , as described in Equation 4 . Indeed , the possibility of gradient masking was our primary motivation to include adaptive attacks ( using the softmax relaxation ) , as discussed in Section 5.2 . With our adaptive attacks , the gradient is never blocked while generating the adversarial example . The model applies the \u201c max \u201d mode ( described in Equation 1 ) only to provide the final prediction for generated adversarial examples . In our evaluation , we found a negligible impact of changing to the \u201c softmax \u201d mode for producing the predictions . For example , in case of the APGD ( $ \\ell_\\infty $ , $ \\ell_2 $ ) attacks on CIFAR-10 , our results are as follows : Robust Accuracies | Attack ||| | Max Approach ( Eq ( 1 ) ) | Softmax Approach ( Eq ( 4 ) ) | | | -- |-|-|-| -- | -- | | | APGD-CE $ \\ell_2 $ ( $ \\epsilon_2 $ = 0.5 ) ||| | 75.7 % | 75.6 % | | | APGD-DLR $ \\ell_2 $ ( $ \\epsilon_2 $ = 0.5 ) ||| | 76.5 % | 76.7 % | | | APGD-CE $ \\ell_\\infty $ ( $ \\epsilon_\\infty $ = 0.03 ) ||| | 86.9 % | 86.9 % | | | APGD-DLR $ \\ell_\\infty $ ( $ \\epsilon_\\infty $ = 0.03 ) |||| 91.8 % | 91.2 % | | In the spirit of consistency of the defense irrespective of whether the attack was adaptive or not , we decided to follow Eq ( 1 ) for the final accuracy calculation everywhere . Further , as noted in the paper , we evaluate our models with the most comprehensive set of attacks in the adversarial ML literature , including both gradient-based and gradient-free attacks . We hope that you reconsider your review in light of this clarification . We will address other concerns in a succeeding response , and will revise our submission to make these points clearer ."}], "0": {"review_id": "Oe2XI-Aft-k-0", "review_text": "This paper proposes an ensemble approach to deal with multiple perturbation types . The underlying idea is to train a robust classifier for each perturbation type ( i.e. , l1 , l2 , and l-inf ) and choose a model to predict based on the decision of a perturbation classifier which is trained to distinguish perturbation types . The idea is interesting , and the experiments seem solid ; however , I am hesitating to give this paper an acceptance decision due to some unclear points as follows : 1 . I have not checked the proof of Theorem 1 , but it seems a bit counter-intuitive to me . The reason is as follows . Let \u2019 s assume that we are attacking a clean image x . We start from x0 inside both l1 and l-inf balls for example . We further assume that during the process of updating x { t } , we never do any projection onto any ball ( i.e. , x { t } always lie inside the balls ) . If so , there is not any difference between x_adv w.r.t l1 and l_inf , meaning that there are possibly a certain number of adversarial examples shared across l1 and l-inf attacks . 2.In your Theorem 1 ( also your experiments ) , the radius of l1 is eps and that of l-inf is eps/sqrt ( d ) , meaning that the ball of l-inf is a subset of the ball of l1 . However , in Section 5.3 , you claimed that \u201c The dotted line shows the decision boundary for the perturbation classifier C_adv , which correctly classifies inputs subjected to large \u2018 1 perturbations \u03b400 as \u2018 1 attacks ( green ) , but can misclassify samples with smaller perturbations \u201d . It seems that you reckon l-inf adversarial examples are having more perturbation than l1 , do not you ? 3.Theorem 2 is not understandable to me . What is worst-case adversary ( this needs to be explained and defined because we can understand this notion in several different ways ) ? What is delta ? 4.I recommend testing Projector against the attack over a uniform average of Mp in addition to what is doing . 5.It is encouraging to compare your proposed method against ( Francesco Croce and Matthias Hein , 2020 a ) . You did mention this work , but not compare it in experiments .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailing your concerns . We are happy to note that you found the idea to be interesting , while being backed up with solid experiments . We attempt to clarify all your concerns in this response , and we will also revise our paper to make these points clearer . \u200c \u200c We discuss the * * separability of perturbation types * * in the common response : [ Non-Overlapping Nature of different perturbation regions ] ( https : //openreview.net/forum ? id=Oe2XI-Aft-k & noteId=V1q5prt8RqE ) . The response addresses Cons ( 1,2 ) in your review . Following up on this discussion : * The radius of l1 is eps and that of l-inf is eps/sqrt ( d ) , meaning that the ball of linf is a subset of the ball of l1 * While the * magnitude * of the radius of the $ \\ell_1 $ region is larger than the $ \\ell_\\infty $ region , none of the regions subsume the other . It is possible for a point in the $ \\ell_\\infty $ space to exist outside the $ \\ell_1 $ region . Example : Consider a 100-dimensional input $ \\delta = [ 0.3,0.3 , ... ,0.3 ] $ . It exists in the $ \\ell_\\infty $ region of $ \\epsilon_\\infty = 0.3 $ . However , the sum of absolute value of each component of $ \\delta $ , or its $ \\ell_1 $ norm is equal to 30 . Hence , the perturbation does not belong to a region of size $ \\epsilon_1 = 10 $ . \u200c \u200c * In Section 5.3 , you claimed ... It seems that you reckon l-inf adversarial examples are having more perturbation than l1 * Our discussion of the perturbation classifier does not utilize this assumption . Instead , we mean that for an $ \\ell_\\infty $ adversary , if the generated adversarial perturbation is large based on the $ \\ell_\\infty $ norm , then the perturbation classifier is likely to correctly classify the adversarial example as $ \\ell_\\infty $ . On the other hand , when the generated adversarial perturbation is small based on the $ \\ell_\\infty $ norm , the perturbation classifier may not correctly categorize the perturbation type , because the perturbation may not be recognizable enough . We will revise the description of Section 5.3 to make these points clearer , and we will clarify the meaning of small and large perturbations . \u200c \u200c # # # Theorem 2 # # # # * Terminologies * 1 . Worst-case adversary refers to an adaptive adversary that has the full knowledge of the defense strategy , and makes the strongest adversarial decision given the perturbation constraints . We will add the explanation in our revision . 2.As described in Equation 3 , $ \\delta $ is the perturbation induced over the original input $ x $ , such that $ x_ { adv } = x + \\delta $ . 3. $ \\alpha , \\sigma , d $ are defined in the problem description in Section 4.1 . \u200c \u200c # # # # * High-Level Summary * In Theorem 1 , we showed that it is possible to distinguish between the distributions of inputs that were subjected to $ \\ell_1 $ and $ \\ell_\\infty $ perturbations . In Theorem 2 , we prove lower-bounds on the robustness of our two-stage pipeline under an adaptive attacker , and show that the adversary faces a trade-off between fooling the perturbation classifier and the second-level models . \u200c * * What does the theorem claim ? * * Error rate of the * PROTECTOR * model , $ P_e $ is less than 0.01 under the considered threat model . * * What is the problem setting ? * * 1.Data points are sampled from $ \\mathcal { D } $ defined in Section 4.1 . 2.Adversarial perturbation regions considered are the $ \\ell_1 $ and $ \\ell_\\infty $ regions . 3.Protector pipeline consists of adversarially trained models $ M_ { 1 , \\epsilon_1 } , M_ { \\infty , \\epsilon_\\infty } $ , and an attack classifier $ C_ { adv } $ . 3.The perturbation size $ \\boldsymbol { \\epsilon_1 } = \\alpha + 2\\sigma $ and $ \\boldsymbol { \\epsilon_\\infty } = \\frac { \\alpha + 2\\sigma } { \\sqrt { d } } $ . For a more detailed description , please refer to the proof sketch of the theorem provided at the beginning of Appendix C. In a subsequent revision , we will utilize the additional page for the main paper to highlight the proof sketch , and provide a more intuitive explanation of the theorem . \u200c \u200c # # # Additional Comparisons As noted in the paper , we build on adversarial training baselines ( Tramer & Boneh 2019 , Maini et.al.2020 ) .However , Croce & Hein 2020 , study provable robustness . As a result , the perturbation sizes considered in their work are significantly smaller than those in empirical works such as ours . For example , in case of the MNIST dataset , while we study empirical robustness at $ \\epsilon_1 $ = 10.0 , Croce & Hein study the upper and lower bounds of the robust test error on $ \\epsilon_1 $ = 1.0 ( see Table 1 in [ Croce & Hein 2020 ] ( https : //arxiv.org/abs/1905.11213 ) ) . They present a range of certified robust error rates , without empirical results as evaluated in our work & the baselines considered . Thus , the two works have different objectives and are not empirically comparable . \u200cWe will make this point clearer in our revision . \u200c Thank you for your time . We will shortly provide evaluation results on the uniform average of $ M_p $ in a succeeding response . In the meantime , please let us know if you have any more questions ."}, "1": {"review_id": "Oe2XI-Aft-k-1", "review_text": "In theoretical analysis , the authors conclude with an Gaussian assumption on the data distribution . This assumption is very restrictive , and the corresponding conclusion does not provide a general view of what is happening for real datasets . Gaussian condition on dataset is very unrealistic , and its theoretical analysis should only be considered as dealing with a toy model . Authors should remove this Gaussian assumption , or at least give strong reasons in plain natural language why Gaussianity can represent natural datasets in the adversarial study .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank\u200c \u200cyou\u200c \u200cfor\u200c \u200cyour\u200c \u200ccomment.\u200c \u200cWe\u200c \u200cwould\u200c \u200clike\u200c \u200cto\u200c \u200cpoint\u200c \u200cout\u200c \u200cthat\u200c \u200cin\u200c \u200cthe\u200c \u200cadversarial\u200c \u200cmachine\u200c \u200clearning\u200c \u200c literature , \u200c \u200cpeople\u200c \u200chave\u200c \u200crelied\u200c \u200con\u200c \u200ccertain\u200c \u200cassumptions\u200c \u200cto\u200c \u200cbuild\u200c \u200ctheoretical\u200c \u200cframeworks.\u200c \u200cThough\u200c \u200c these\u200c \u200ctheoretical\u200c \u200cframeworks\u200c \u200care\u200c \u200cbased\u200c \u200con\u200c \u200csimplified\u200c \u200cassumptions , \u200c \u200cthey\u200c \u200cmotivate\u200c \u200cempirical\u200c \u200c approaches\u200c \u200cthat\u200c \u200cwork\u200c \u200cin\u200c \u200cpractice.\u200c \u200cSpecifically , \u200c \u200cthe\u200c \u200cGaussian\u200c \u200cassumption\u200c \u200chas\u200c \u200cbeen\u200c \u200cmade\u200c \u200cin\u200c \u200ca\u200c \u200c number\u200c \u200cof\u200c \u200cprior\u200c \u200cwork\u200c \u200con\u200c \u200cadversarial\u200c \u200cmachine\u200c \u200clearning.\u200c \u200cIn\u200c \u200cthe\u200c \u200cfollowing , \u200c \u200cwe\u200c \u200clist\u200c \u200ca\u200c \u200cfew\u200c \u200cpapers\u200c \u200c published\u200c \u200cin\u200c \u200crecent\u200c \u200cmachine\u200c \u200clearning\u200c \u200cconferences : \u200c \u200c [ [ NeurIPS \u2019 19 ] \u200c \u200cIlyas\u200c \u200cet\u200c \u200cal. , \u200c \u200cAdversarial\u200c \u200cExamples\u200c \u200cAre\u200c \u200cNot\u200c \u200cBugs , \u200c \u200cThey\u200c \u200cAre\u200c \u200cFeatures ] ( https : //arxiv.org/abs/1905.02175 ) \u200c\u200c \u200c ( See\u200c \u200cSection\u200c \u200c 4 ) \u200c \u200c [ [ ICLR \u2019 19 ] \u200c \u200cTsipras\u200c \u200cet\u200c \u200cal. , \u200c \u200cRobustness\u200c \u200cMay\u200c \u200cBe\u200c \u200cat\u200c \u200cOdds\u200c \u200cwith\u200c \u200cAccuracy\u200c\u200c ] ( https : //arxiv.org/abs/1805.12152 ) \u200c ( See\u200c \u200cSection\u200c \u200c2.1 ) \u200c \u200c [ [ NeurIPS \u2019 19 ] \u200c \u200cTramer\u200c \u200cet\u200c \u200cal. , \u200c \u200cAdversarial\u200c \u200cTraining\u200c \u200cand\u200c \u200cRobustness\u200c \u200cfor\u200c \u200cMultiple\u200c \u200cPerturbations ] ( https : //arxiv.org/abs/1904.13000 ) \u200c\u200c \u200c ( See\u200c \u200cSection\u200c \u200c2.2 ) \u200c \u200c \u200c In\u200c \u200cour\u200c \u200cpaper , \u200c \u200cwe\u200c \u200cmention\u200c \u200cthat\u200c \u200cthe\u200c \u200cassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cour\u200c \u200cwork\u200c \u200care\u200c \u200cadapted\u200c \u200cfrom\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.\u200c \u200c ( Section\u200c \u200c4.1\u200c \u200cand\u200c \u200cAppendix\u200c \u200cA\u200c \u200cin\u200c \u200cour\u200c \u200cpaper ) .\u200c \u200cIn\u200c \u200cAppendix\u200c \u200cA\u200c \u200cof\u200c \u200cour\u200c \u200cpaper , \u200c \u200cwe\u200c \u200chave\u200c \u200ccompared\u200c \u200cour\u200c \u200c assumptions\u200c \u200cwith\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal.\u200c \u200cWe\u200c \u200cdiscussed\u200c \u200cthat : \u200c \u200c ( 1 ) \u200c \u200cthe\u200c \u200cassumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cour\u200c \u200cwork\u200c \u200cbetter\u200c \u200c represent\u200c \u200cthe\u200c \u200ccharacteristics\u200c \u200cof\u200c \u200creal-world\u200c \u200cimage\u200c \u200cdatasets ; \u200c \u200cand\u200c \u200c ( 2 ) \u200c \u200cour\u200c \u200cresults\u200c \u200cnaturally\u200c \u200chold\u200c \u200cwith\u200c \u200c assumptions\u200c \u200cmade\u200c \u200cin\u200c \u200cIlyas\u200c \u200cet\u200c \u200cal. , \u200c \u200cwhich\u200c \u200cassumes\u200c \u200cthe\u200c \u200cpresence\u200c \u200cof\u200c \u200ca\u200c \u200cstochastic\u200c \u200cinput\u200c \u200cfeature\u200c \u200c ( explained\u200c \u200cin\u200c \u200cAppendix\u200c \u200cA ) .\u200c \u200c \u200c On\u200c \u200cthe\u200c \u200cother\u200c \u200chand , \u200c \u200cwe\u200c \u200cwould\u200c \u200clike\u200c \u200cto\u200c \u200chighlight\u200c \u200cthat\u200c \u200cwhile\u200c \u200cthe\u200c \u200ctheoretical\u200c \u200cproofs\u200c \u200care\u200c \u200cimportant\u200c \u200cparts\u200c \u200c of\u200c \u200cthe\u200c \u200cpaper\u200c \u200cto\u200c \u200cmotivate\u200c \u200cthe\u200c \u200c\u200c * PROTECTOR * \u200cframework , \u200c \u200cthe\u200c \u200cstrong\u200c \u200cempirical\u200c \u200cresults\u200c \u200care\u200c \u200calso\u200c \u200ckey\u200c \u200c contributions\u200c \u200cof\u200c \u200cour\u200c \u200cwork.\u200c \u200cIn\u200c \u200cSection\u200c \u200c6 , \u200c \u200cwe\u200c \u200cshow\u200c \u200cthat\u200c \u200con\u200c \u200cadversarial\u200c \u200cexamples\u200c \u200cgenerated\u200c \u200cusing\u200c \u200c various\u200c \u200cattack\u200c \u200calgorithms , \u200c \u200cthe\u200c \u200cProtector\u200c \u200cframework\u200c \u200coutperforms\u200c \u200cthe\u200c \u200cstate-of-the-art\u200c \u200cdefenses\u200c \u200cby\u200c \u200c large\u200c \u200cmargins.\u200c \u200cWe\u200c \u200curge\u200c \u200cyou\u200c \u200cto\u200c \u200ckindly\u200c \u200calso\u200c \u200cconsider\u200c \u200cthe\u200c \u200cempirical\u200c \u200cand\u200c \u200cmethodological\u200c \u200c components\u200c \u200cof\u200c \u200cour\u200c \u200cwork\u200c \u200cin\u200c \u200cyour\u200c \u200creview\u200c \u200cand\u200c \u200csubsequent\u200c \u200cdecision.\u200c \u200c \u200c"}, "2": {"review_id": "Oe2XI-Aft-k-2", "review_text": "Summary : This paper proposed a pipeline method which first classify the attack type and then choose the proper predictor for that type . The authors provide both theoretical and experimental proof to support their pipeline method . Pros : ( 1 ) The idea is good and reasonable . Also , the paper is well written and easy to read . The proof part is clear and natural . ( 2 ) The result of theorem 1 is interesting and it proved the different perturbation types ( under this problem setting ) is separable with a high probability . Although the setting is a little restrictive , it is still very impressive . Cons : ( 1 ) The method and analysis only apply to the Lp attack type . It will be good if it can be extended . ( 2 ) Assume that we treat the two pipelines as the whole process ( end-end deep network ) , e.g. , the first layers determines the type and then that type will determine which part to activate for training the data points , that is , you have a network with special topology and structure . If the attacker only cares about this input and output of this network ( no pipeline here ) , we should have an attack strategy . How to explain this view using your analysis ? Could we say this is equivalent to changing the structure of the network ? From this perspective , could you tell the difference between these two views ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your insightful feedback on the work . We are glad you liked the clarity of the paper and our theoretical arguments that motivate the empirical results . We address your concerns and questions below , and we will revise our paper to make these points clearer . \u200c \u200c # # # $ \\ell_p $ Norm Setting We agree that the problem of multiple $ \\ell_p $ robustness may be a little restrictive , but it is also one of the only settings in which we have a standardized notion of attack and defense benchmarks . We believe that multiple perturbation robustness to other threat models will naturally follow in our * PROTECTOR * framework , as more attack types and individual models robust to them become standardized . However , in the absence of such attacks and defenses , it is difficult to demonstrate the efficacy of the same . We also note that the problem of multiple $ \\ell_p $ norm robustness has attracted a lot of attention from the community . Apart from the citations in the main paper , there are at least the following submissions at ICLR this year that also study the same problem : \u200c [ Towards Defending Multiple Adversarial Perturbations via Gated Batch Normalization ] ( https : //openreview.net/forum ? id=Utc4Yd1RD_s ) [ Composite Adversarial Training for Multiple Adversarial Perturbations and Beyond ] ( https : //openreview.net/forum ? id=H92-E4kFwbR ) [ Learning to Generate Noise for Multi-Attack Robustness ] ( https : //openreview.net/forum ? id=tv8n52XbO4p ) \u200c \u200c # # # Topological Model * If the attacker only cares about this input and output of this network ( no pipeline here ) , we should have an attack strategy . How to explain this view using your analysis ? Could we say this is equivalent to changing the structure of the network ? From this perspective , could you tell the difference between these two views ? * \u200c \u200c # # # # * * Attack Strategy * * For both the standard attack ( Eq ( 1 ) ) and the softmax adaptive attack ( Eq ( 4 ) ) , our entire pipeline ( with two stages ) is considered as a single model at test time . The adversary utilizes the knowledge of the input and output pairs from the entire pipeline to frame the attack strategy , as also suggested by you . In fact , the adaptive attack presented in ( Eq ( 4 ) ) further exploits the knowledge of the internal model structure to propagate full gradients through the two stages , and hence produces stronger adversarial examples than standard attacks that only care about model input and output . We agree that our entire pipeline is essentially equivalent to a topological model , for which the first few layers perform perturbation type categorization , and then specialized layers for different perturbation types follow . To explain why our pipeline is supposed to be more robust to multiple perturbation types , we would like to draw your attention towards the adversarial trade-off discussed in Theorem 2 and represented in Figure 1.b . Specifically , we design this topological structure to establish a trade-off for the adversary between fooling the two stages of the topology , even when it taking a unified view of the model and * not * attacking the two stages separately . \u200c \u200c # # # # * * Training the Topological Model * * The key advantage that * PROTECTOR * brings is in the ability to train such a topological model in the first place . Recent work ( [ [ ICML2020 ] Adversarial Robustness Against the Union of Multiple Perturbation Models ] ( https : //arxiv.org/abs/1909.04068 ) ) has demonstrated how naive augmentation of attacks from different perturbation types leads to unstable and inconsistent training when used to train a single robust model , and often suffers from various kinds of gradient masking . On the other hand , as mentioned in Section 5.1 , we observe that the training of the perturbation categorizer is stable and consistently achieves good performance . Further , as also detailed in Section 5.1 , these previous models take an extremely large amount of computational resources to train . For instance , the state-of-the-art network for $ \\ell_\\infty $ robustness ( with data augmentation ) takes nearly 2 GPU days ( https : //github.com/yaircarmon/semisup-adv ) . Doing the same with augmentation of data points for different types of $ \\ell_p $ attacks like in [ [ NeurIPS \u2019 19 ] Tramer et al. , Adversarial Training and Robustness for Multiple Perturbations ] ( https : //arxiv.org/abs/1904.13000 ) will require 3x the time . On the other hand , the * PROTECTOR * framework is able to leverage the strong predictions of pre-existing individually robust models to train a perturbation categorizer in only a few hours . We are happy to provide any further clarification about the work and thank you for your time ."}, "3": {"review_id": "Oe2XI-Aft-k-3", "review_text": "The paper proposes a two-stage defense method to improve the adversarial robustness over different perturbation types . Specifically , it first builds a hierarchical binary classifier to differentiable the perturbation types and then uses the result to guide to its corresponding defense models . It first proves the different types of perturbations could be separable and the adversary could be weakened to fool the binary classifier . It shows their methods achieve a clear improvement in the experiments . Pros : 1.The paper is good-written and easy to follow . 2.The proposed idea is interesting . 3.The experiment is detailed and comprehensive . Cons : 1.There is a major problem in their method . In the last sentence of section 5.2 , it says uses the soft relaxation only in generating the adversarial example , but not for inference . It clearly caused the gradient making problem in the adversarial attack later on to test the robustness . The gradient is blocked before reaching into the binary classifier so that the adversarial attack fails , which I think it is not truly improving the model 's robustness . 2.In my opinion , this method is just a dynamic voting based model ensemble . Just take the binary classifier as a voting procedure . Therefore , the traditional adversarial attacks wo n't work in general . I would suggest using the soft relaxation in the inference as well for the adversarial attack . 3.Also , the assumption that different norm adversarial examples could be clearly separable might be wrong . You could find the adversarial examples that satisfy both l1 , l2 and l_inf constraint by just choosing the \\epsilon for every norm differently .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback and insightful comment on the possibility of gradient masking . We would like to clarify an important misunderstanding about the evaluation adequacy in this response , and will provide a detailed discussion on the adversarial example separability in a followup post . * '' In the last sentence of section 5.2 , it says uses the soft relaxation only in generating the adversarial example , but not for inference . It clearly caused the gradient making problem in the adversarial attack later on to test the robustness . `` * We would like to clarify that in our experiments , the adversary always constructs adversarial examples using the softmax mode in the pipeline , as described in Equation 4 . Indeed , the possibility of gradient masking was our primary motivation to include adaptive attacks ( using the softmax relaxation ) , as discussed in Section 5.2 . With our adaptive attacks , the gradient is never blocked while generating the adversarial example . The model applies the \u201c max \u201d mode ( described in Equation 1 ) only to provide the final prediction for generated adversarial examples . In our evaluation , we found a negligible impact of changing to the \u201c softmax \u201d mode for producing the predictions . For example , in case of the APGD ( $ \\ell_\\infty $ , $ \\ell_2 $ ) attacks on CIFAR-10 , our results are as follows : Robust Accuracies | Attack ||| | Max Approach ( Eq ( 1 ) ) | Softmax Approach ( Eq ( 4 ) ) | | | -- |-|-|-| -- | -- | | | APGD-CE $ \\ell_2 $ ( $ \\epsilon_2 $ = 0.5 ) ||| | 75.7 % | 75.6 % | | | APGD-DLR $ \\ell_2 $ ( $ \\epsilon_2 $ = 0.5 ) ||| | 76.5 % | 76.7 % | | | APGD-CE $ \\ell_\\infty $ ( $ \\epsilon_\\infty $ = 0.03 ) ||| | 86.9 % | 86.9 % | | | APGD-DLR $ \\ell_\\infty $ ( $ \\epsilon_\\infty $ = 0.03 ) |||| 91.8 % | 91.2 % | | In the spirit of consistency of the defense irrespective of whether the attack was adaptive or not , we decided to follow Eq ( 1 ) for the final accuracy calculation everywhere . Further , as noted in the paper , we evaluate our models with the most comprehensive set of attacks in the adversarial ML literature , including both gradient-based and gradient-free attacks . We hope that you reconsider your review in light of this clarification . We will address other concerns in a succeeding response , and will revise our submission to make these points clearer ."}}