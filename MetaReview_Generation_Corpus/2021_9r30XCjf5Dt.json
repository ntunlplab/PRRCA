{"year": "2021", "forum": "9r30XCjf5Dt", "title": "Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics", "decision": "Accept (Poster)", "meta_review": "The paper focuses on adversarial attacks for RL, which is an exciting understudied research direction, and can be of interest to the community. All the reviewers are (mildly) positive about the paper and the author competently replied to the concerns expressed by the reviewers. ", "reviews": [{"review_id": "9r30XCjf5Dt-0", "review_text": "# # # # Summary : The paper studies poisoning attacks on RL agents , in which the attacker influences the agent 's learning process by changing the feedback obtained from the environment . The focus is put on attacking policy-based deep RL agents , without necessarily having access to the underlying MDP model of the environment . The paper proposes a new poisoning algorithm , called Vulnerability-Aware Adversarial Critic Poison , and experimentally demonstrates its effectiveness on 5 different RL environments . # # # # High level comments ( pros & cons ) : -The paper studies an important and interesting topic , poisoning attacks on RL agents , and develops a novel deep learning methods for designing more efficient and scalable attack strategies . -In contrast to prior work , the proposed algorithm utilizes deep RL techniques , making it applicable to more complex environments . The experimental results indicate the usefulness of the proposed method . -The clarity of the paper could be improved , including the statement and description of the optimization problem and the algorithm . Furthermore , it is not clear how the problem formulation compares to prior work . Some claims in the paper should be stated more precisely . -The proposed algorithm is a greedy approach and does not have provable guarantees on the optimality of the derived attack strategies . This is in contrast to prior work cited in this paper , which appears to have some guarantees on the performance of the attack . Overall , I enjoyed reading the paper , and its contributions seem novel and important for the line of work on poisoning attacks in RL . However , I also think that the paper could be improved in terms of clarity , and additional justifications and explanations could be added throughout the paper . # # # # More specific comments and suggestions for improvement : -The exposition of the results could be improved , and some parts clarified and made more precise . For example , footnote 1 is confusing , since it states that this paper assumes that the attacker poisons observations . On the other hand , the paper also mentions results on 'Hybrid aim poisoning ' . It is also confusing that optimization problem ( Q ) is defined as weighted loss , and then in the first paragraph of Section 4 we have the claim : 'Without loss of generality , we assume the loss weights $ j = 1 $ for all $ j = 1 , ... , K $ . ' . The first sentence in Challenge II is also not clear : why is Markovian property important in 'are no longer i.i.d.due to the Markovian property ' ? There are also sentences that do not seem to be precise . E.g. , the sentence 'However , in complex environments such as Atari games , knowing the dynamics of the MDP is difficult . ' does n't seem to be precise ( since dynamics can be obtained from Atari simulator ... ) . Given that the paper motivates its setting with Atari games , it is also not clear why Atari games were not used as a test-bed . -Parts of the optimization problem ( Q ) are somewhat confusing . In particular , the paragraph that explains constraint ( b ) ( imitate the learner ) , does not seem to precisely specify what this constraint looks like . In the white-box attack , it is written that the attacker knows the learner 's policy and can directly copy it . It is also stated for black-box attacker that it 'may know the learner \u2019 s algorithm ' , but does not know the learner 's policy . On the other hand , in Section 2 , it is written that 'white-box attackers , who know the learner \u2019 s model , and black-box attackers , who do not know the learner \u2019 s model . '.These parts could be explained in more detail , or more precisely stated . -Constraints ( c ) and ( d ) control the attacker 's influence , but the paper does not seem to indicate the practical importance of having both constraints . It might be useful add some discussion on this , as having both constraints seems to affects the algorithmic design proposed in the paper , and the complexity of the optimization problem . -The discussion in the related work section seems to put emphasis on practical importance of the proposed approach compared to some of the recent papers on poisoning attacks . However , it does not seem to elaborate on the differences in problem formulations , i.e. , optimization problems and objectives . -The focus seems to be on an episodic setting in which after each episode ( iteration ) , an RL agents updates its policy after the data is possibly poisoned . I 'm wondering to what extent would these result generalize to fully online setting in which an agent can change its policy after each action taken . Moreover , it is not clear how one can poison e.g.observations only after an episode ends ( since the same observations are needed to derive actions from the agent 's policy ) . Additional discussion on this would be valuable . -The model of the black box-attack is somewhat ambiguous . It is first stated that a black-box attacker may know the learner 's algorithm , but it is not specified to what extent the attacker relies on the knowledge about the learner 's algorithm . Furthermore , the notion of pseudo-learner does not seem to be defined . -Minor : There are two 'Step 5 ' in the description of the algorithm . Furthermore , the algorithm uses variables $ \\psi $ and $ \\Psi $ , which do not seem to be defined before section 4.3 . The paper also contains minor typos , e.g . : - the test-time evasion attacks Chen et al . ( 2019 ) where the attacker crafts - > citation style -alter the environment ( e.g.change the transition probabilities ) , - > remove space before , -On the contrary , We consider non-omniscient attackers - 'We ' should be 'we ' - ' a learner gains from the environment , i.e. , $ O = ( O^s , O^a , O^r ) $ ' - > Is $ O^a $ observed ? -Compared with Problem ( Q ) , - > ( Q ) should be ( Q ) -The solution to Problem ( P ) is always feasible to Problem ( Q , although ... - > e.g . ( Q should be ( Q ) etc . # # # # Questions : a. I did n't understand footnote 1 . It states that this paper assumes that the attacker poisons observations , but on the other hand , the results seem to suggest that other attacks are also considered . Could you clarify what types of attacks are considered in the paper ? b.The optimization problem ( Q ) seems to be different from the ones studied in ( Ma et al.2019 ) and ( Rakhsha et al.2020 ) .Could you elaborate on the differences between these attack formulations ? How does your setting relate to the setting of ( Zhang and Parkes , 2008 ) in terms of computational complexity ? c. The budget constraint ( c ) in optimization problem ( Q ) assumes that the 'cost ' of an attack is either 0 or 1 , whereas the constraint ( d ) already limits the 'power ' of the attack . Could you explain the practical importance of imposing these two constraints together ? d. Which practical application would support the episodic poisoning setting described in this paper ? e. How exactly is the pseudo-learner defined in this paper ( e.g. , in Section 5 ) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank R # 4 for the valuable and detailed feedback . Most of R # 4 's concerns are on the clarity of demonstrations and detailed explanations of the method . Due to the complexity of poisoning RL , lots of new and notations/concepts are required in our paper . For clarity , we organized the most important messages/concepts and provided as many intuitive explanations as possible through the main paper ; therefore the reader might have to find clarifications from the appendix for some technical details . For example , the * * detailed poisoning formulation and comparison with related works * * are in Appendix B ; more rigorous definitions of the stability radius are in Appendix D ; * * detailed algorithm illustration * * is in Appendix E ; we also provide some * * theoretical analysis * * on the relaxation problem in Appendix F. We understand that reviewers are not obligated to find all details in the appendix , therefore we greatly appreciate the reviewer 's comments , and have modified the main body of the paper according to R # 4 's valuable suggestions . We summarize all questions raised by R # 4 into 4 categories . * * Part 1.Questions and confusion due to some misinterpretation of our paper . * * * * Part 2 . Questions about problem formulation and theory . * * * * Part 3 . Questions about algorithm details and extensions . * * * * Part 4 . Questions due to imprecise/ambiguous wording in our previous manuscript . * * We will address all questions in each category respectively in 4 replies . All Refs : [ 1 ] Amin Rakhsha , et al.Policy teaching via environment poisoning : Training-time adversarial attacks against reinforcement learning . [ 2 ] Vahid Behzadan , et al.Vulnerability of deep reinforcement learning to policy induction attacks . [ 3 ] Yunhan Huang , et al.Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals . [ 4 ] Yizhen Wang , et at . Data Poisoning Attacks against Online Learning . [ 5 ] Alexander Turner et al.Clean-Label Backdoor Attacks [ 6 ] Yuzhe Ma , et al.Policy poisoning in batch reinforcement learning and control . [ 7 ] Haoqi Zhang , et al.Value-based policy teaching with active indirect elicitation"}, {"review_id": "9r30XCjf5Dt-1", "review_text": "This paper studies a very important problem of poisoning attack against RL when the attacker is not omniscient . This is an important next step , as most prior work assumes omniscient for the sake of a more rigorous theoretical understanding ( e.g.Rakhsha et al. , 2020 , Zhang et al.2020 ) .However , the approach taken by this paper is too heuristic and only applies to a very limited setting where the learner needs to perform * * on-policy * * policy gradient methods , which no STOA algorithm does due to its poor sample efficiency , so there is n't much empirical value . At a high level , the approach this paper takes can be summarized as follows : It defines the optimal poisoning attack problem in an unknown environment as a * * sequential decision making problem * * , which is well-motivated and clear . It then proposed to simply use a * * greedy algorithm * * that only optimizes the current step 's cost without caring about what happens in the future . Then , of course , the greedy attacker does n't require the knowledge of the environment 's transition . Even if the attacker does know it , it would n't be using it anyway , because it only cares about the current step . And prior work has already shown that in the online data poisoning context , the greedy strategy can be * * exponentially worse * * than the optimal attack strategy . See [ 1 ] .Some technical questions : 1 . Correctness of Proposition 2 : In the Remarks of section 4.1 . It is mentioned that the value difference of two policies differed by at most delta in total variance distance will also be bounded by $ poly ( \\delta , 1/ ( 1-\\gamma ) ) $ . I did n't check the proof thoroughly but I feel that there are counter-examples ? Consider the classic `` combination lock '' MDP , where states form a chain of length H , and there are two actions in each state : the `` right '' action moves you right to the adjacent state , and the `` left '' action teleports you back to the starting state ( the left-most state ) . All rewards are zero except for when the agent successfully arrives at the right-most state , which takes H right actions consecutively . Now , the optimal policy ( always go right ) will have value $ 1 $ . But a $ \\delta $ -perturbed policy which now has $ \\delta $ probability of going left will only have value $ ( 1-\\delta ) ^H $ ( assuming a fixed episode length H and no discounting ) . So the gap seems to be exponential rather than polynomial . How does this example fit into the conclusion of proposition 2 ? 2.About adversarial critic : What exactly is this $ \\tilde V_\\omega $ estimating ? Is it estimating the value of the optimal policy , the learner 's current behavior policy , or the learner 's current $ \\hat\\pi_k $ which according to the paper is different from the behavior policy ? If it was the latter two , how does the attacker keep track of the value of an ever-changing policy , and how does he use this value estimate to evaluate the value of some other potential poisoned policy ? It 's all very confusing . 3.The estimated rank in step 6 of section 4.3 is only unbiased if the policy discrepancies $ \\hat \\psi_k $ are * i.i.d . * , which they clearly are not . [ 1 ] Xuezhou Zhang , Xiaojin Zhu , Laurent Lessard . Online Data Poisoning Attack . L4DC 2020 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> * * Q3 : Does the combination lock MDP violate Proposition 2 ? * * A3 : This example * * does not violate our Prop . 2 * * , because Prop . 2 upper bounds the * * reward/value drop * * caused by poisoning , i.e. , $ V_ { \\pi } ( s_0 ) -V_ { \\pi^\\prime } ( s_0 ) $ . In the combination lock MDP , if we assume non-discounting ( $ \\gamma=1 $ ) as R # 2 assumes , then our upper bound becomes $ \\infty $ since $ ( 1-\\gamma ) $ is in the denominator , thus the upper bound is valid ; in the other case , if $ 0\\leq\\gamma < 1 $ , the inequality given by Prop . 2 is $ $ V_ { \\pi } ( s_0 ) -V_ { \\pi^\\prime } ( s_0 ) = \\gamma^H - ( 1-\\delta ) ^H \\gamma^H \\leq a\\frac { \\delta^2\\gamma } { ( 1-\\gamma ) ^2 } + b\\delta $ $ where $ a $ and $ b $ are independent of $ \\delta , \\gamma $ and $ \\pi^\\prime $ , as detailed by our Prop . 2 in Appendix D. Although it is hard to derive an analytical proof of the inequality due to the complex form , we implemented a numerical grid test for \\delta=0:0.001:1 , \\gamma=0:0.001:1 , and for different H \u2019 s , none of them violates the above inequality . > * * Q4 : What exactly is the adversarial critic $ \\tilde { V } _\\omega $ estimating ? how does the attacker keep track of the value of an ever-changing policy , and how does he use this value estimate to evaluate the value of some other potential poisoned policy ? * * A4 : ( 1 ) * * What is the adversarial critic estimating ? * * $ \\tilde { V } _\\omega $ is the value function of the learner 's current behavior policy ( as we stated in Section 4.2 , it is trained with the unpoisoned trajectories generated by the learner 's policy ) . In other words , $ \\tilde { V } _\\omega $ is the critic for the learner 's current policy . Note that the critic/value function learned by the learner is not the correct value for its policy , because the learner uses poisoned trajectories to update its critic . And the attacker , on the contrary , observes the clean trajectories generated by the learner 's policy , thus can learn a correct critic for the policy . ( 2 ) * * How does the attacker keep track of the value of an ever-changing policy , and how does he use this value estimate to evaluate the value of some other potential poisoned policy ? * * $ \\tilde { V } _\\omega $ is the value of the policy in the current iteration , and the value will be updated in the next iteration when the policy changes . It is exactly the Actor-Critic framework , but the critic is owned by the adversary . For estimating the value of a potential poisoned policy , we use importance sampling , as we described in the last two paragraphs in Section 4.2 . We admit that in the original submission , some messages about the adversarial critic are embedded in the descriptions and might be hard to find for readers , although we did mention them . We have modified the paper to make these details more explicit . > * * Q5 : The estimated rank in step 6 of section 4.3 is not unbiased . * * A5 : R # 2 is right that the estimated rank is not unbiased . We apologize for this imprecise description . But the algorithm itself does not need the rank to be unbiased , we choose this rank estimation because it works much better than simply ranking the historical policy discrepancies . We have removed the word `` unbiased '' in our modified version ."}, {"review_id": "9r30XCjf5Dt-2", "review_text": "The paper studies poisoning attacks against online reinforcement learning agents . The attacker has the power of manipulating the training data , i.e. , state-action-reward trajectories , in order to achieve some attack goal . The attack can be completely black-box , meaning that the proposed method allows an attack setting where the attacker has no knowledge of the RL algorithm used by the victim agent or the environment . In this scenario , the authors proposed that the attacker can imitate the learning procedure of the victim , and then based on the imitated policy ; the attacker designs how to poison the training data . The attack is formulated as a bi-level optimization , where the lower level involves the imitated learning procedure . Due to the intractability of sequential optimization , the original formulation is simplified so that only the attack only solves the attack on the current training data . This procedure is repeated in every episode to achieve sequential attacks . Experiments on a variety of tasks demonstrate the superiority of the proposed attack . Compared to prior works , the main advantage of this paper lies in that the attack can be applied in more complicated tasks where state or action space is continuous . Furthermore , the attack takes into account the adversarial effect of the current attack on future behavior of the victim agent . Therefore , the attack achieves better overall performance ( e.g.more times of target-action selection ) than traditional gradient-based attack such as FGSM . Another strength of the paper is that it provides some theoretical analysis in terms of how the relaxed optimization approximates the original complex attack optimization . This is in general a hard question to answer . Although the analysis is only about attack feasibility and how to test sub-optimality in hindsight , there is value in deriving those theoretical results . The experimental part of the paper is also strong . The authors have shown convincing results that demonstrate the proposed VA2C-P attack outperforms existing gradient-based FGSM attacks . Moreover , there are systematic empirical investigations on how the attack constraint parameter epsilon affects the attack performance . Finally the paper is well-written and provides a nice summary of prior works , as well as why each prior work fails to achieve some desired property of attack in an ideal sequential attack scenario . Therefore , overall I think the paper is nice and makes significant contribution . One disadvantage of the paper is that while the paper claims able to handle sequential attacks , the relaxed attack optimization seems solving only the desired manipulation on training data in the current episode . As a result , the solution is definitely sub-optimal . The authors provided a method of evaluating whether the implemented attack is sub-optimal in hindsight , specifically the proposition 7 . This result , while being interesting , is not useful in that it can not help practitioners gauge if the computed attack is optimal or not , since it \u2019 s a necessary condition . I am wondering if the authors could provide some insight on how sub-optimal is the attack in this paper , and potential ways to further improve it . I also want to point out that the attacker knowledge assumed in this paper is not strictly less than prior works such as in Zhang 2020 . Both need to have access to a simulator of the MDP and victim environment , and both need to exhaust large amount of training data before obtaining a good attack policy . In this paper , the attacker needs to imitate the victim learner , and the accuracy of imitation result will depend on how many training data are available .", "rating": "7: Good paper, accept", "reply_text": "We appreciate the positive feedback and constructive suggestions provided by R # 1 . Especially , we appreciate a lot that R # 1 noticed our theoretical analysis about the relaxation and thought them valuable . We address some questions raised by R # 1 as follows : > * * Q1 : How to determine the sub-optimality of the attack ? * * A1 : ( Insights on measuring sub-optimality ) R # 1 is right that Proposition 7 provides a necessary condition for the attack being optimal . And we admit that due to the high non-convexity , it is difficult to judge whether an attack is globally optimal for the whole attacking process . However , Prop . 7 also provides an idea of judging how likely a past attack decision is locally optimal . Equation ( 15 ) essentially measures whether the actual search ( attack ) direction matches the optimal direction . If they match , then the attack is at least locally optimal ; if they do not match , then the attack is nonoptimal even locally . In practice , we want the LHS of Equation ( 15 ) to be as * * aligned to the RHS as possible * * for local optimality , because it suggests that the attacker is leading the agent to the desired direction . Otherwise , if the search direction is opposite to the optimal direction , one may consider changing the attack strategy ( but note that being nonoptimal does not necessarily mean that the current poisoning is bad ; it may still deprave the victim significantly ) . > * * Q2 : Are there ways to improve the optimality of attacking ? * * A2 : There are some potential ways to further improve our proposed attack , although they may require more computations and knowledge . ( 1 ) The attacker can fit a prediction model for the unknown environment ( predict future states and rewards given the current state and action ) using the trajectories generated by the victim policy . Then the attacker can predict future trajectories . In this case , we can use Proposition 7 as `` foresight '' rather than `` hindsight '' . More specifically , we may look $ N $ -iterations ahead , and figure out a poison direction that is optimal for the next $ N $ iterations instead of the next one iteration . However , it may require more computations than our current method . ( 2 ) If the attacker is allowed to directly interact with the environment ( which is not allowed in our paper ) , or even the attacker knows the dynamics of the environment , then it may be possible to pre-compute the `` ideal '' attacks . For example , assume one is doing targeted poisoning , then using Inverse RL , one can design an `` ideal '' reward function such that any agent will learn the target policy if the rewards are given by the `` ideal '' reward function . ( Although Inverse RL focuses on teaching the agent a good policy , the adversary can do the opposite and set a malicious policy as the target . ) Then during the victim 's online learning process , the goal of the attacker is to perturb the actual observations toward the `` ideal '' attack directions as much as possible under their budget and power constraints . This simplified problem is still hard to solve due to the limited budget and power , but it would be more effective since it has extra knowledge of the environment . > * * Q3 : Is the required knowledge less than prior work ? * * A3 : As mentioned in A2 ( 2 ) , we would like to clarify that our proposed attacker * * needs not to interact with the MDP or a simulator * * ; we only use the learner 's observations generated via the learner 's interaction with the environment . The word `` imitate '' might be misleading to readers , and we will make it more clear in the modified version . In our Problem ( Q ) line ( b ) , the notation $ \\mathcal { O } _j $ is the observation that is already gained by the learner/victim in the $ j $ -th iteration , and $ \\check { \\mathcal { O } } _j $ denotes the perturbed/poisoned observation with a specific poison aim $ \\check { \\mathcal { D } } _j $ ( $ \\check { \\mathcal { D } } _j $ is to be determined by the attacker himself ) . And imitating just means that the attacker wants to follow what the victim would do once it receives the poisoned observation $ \\check { \\mathcal { O } } _j $ . In other words , the attacker maintains its own `` copy of victim '' by feeding it with whatever training data the actual victim uses . Therefore , the attacker only needs to `` eavesdrop '' on the interactions between the learner and the environment , and requires * * no extra training data from the environment * * . In contrast , paper [ 1 ] mentioned by R # 1 proposes a white-box reward-poisoning method , as well as sound theoretical results for the feasibility and optimality of targeted attacks , although [ 1 ] focuses on finite MDPs and the attacker requires the knowledge of the MDP parameters . In contrast , our attacking strategy does not require knowledge of the MDP , and works for large , continuous MDPs , although optimality is not guaranteed . Therefore , our poisoning method and the method in [ 1 ] work in different scenarios . Refs : [ 1 ] Zhang , et al.Adaptive Reward-Poisoning Attacks against Reinforcement Learning . 2020 ."}, {"review_id": "9r30XCjf5Dt-3", "review_text": "Summary : This paper proposes a poisoning algorithm named Vulnerability-Aware Adversarial Critic Poison ( VA2C-P ) to attack policy-based deep reinforcement learning agents . The poisoning attack is formulated as a sequential bilevel optimisation problem ( Problem Q ) , where the attacker either minimises the expected total rewards of the learner ( non-targeted poisoning ) , or forces the learner to learn a target policy ( targeted poisoning ) . To solve Problem Q , VA2C-P mainly makes two decision : ( 1 ) when to attack : a new metric named stability radius is proposed to decide the attack timing , ( 2 ) how to attack : a mechanism of adversarial critic is designed to solve a relaxed version of Problem Q by only considering the loss of the immediate next iteration . Pros : 1.It is an important question to investigate how policy-based RL algorithms can be poisoned by adversarial attacks . 2.It is novel to propose a poisoning method against policy-based RL agents , which has not been studied before . 3.The proposed poisoning framework ( Problem Q ) is a general formulation that covers a variety of models . 4.VA2C-P has been demonstrated to be effective in targeted and untargeted attacks , under both white-box and black-box settings . Cons : 1.This paper considers a scenario where ( 1 ) \u201c the attacker does not know the underlying dynamics of MDP , and can not directly interact with the environment , either \u201d ; ( 2 ) the attacker is able to obtain the states observed by the agent , their actions and rewards . Is ( 2 ) a realistic setting ? Especially , how is the reward accessible to the attacker ? 2.The black-box attack studied in the paper is closer to white-box attack than to black-box attack : the attacker can still access the past and current observations ( state + action + reward ) , and the only limit is that the policy $ \\pi $ of the target model is unknown . This type of black-box attack is unrealistic in many situations .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank R # 3 for the detailed summarization and valuable comments . And we address the concerns R # 3 mentioned as follows , most of which are related to the knowledge of a poisoning attacker . > * * Q1 : This paper considers a scenario where ( 1 ) \u201c the attacker does not know the underlying dynamics of MDP , and can not directly interact with the environment , either \u201d ; ( 2 ) the attacker is able to obtain the states observed by the agent , their actions and rewards . Is ( 2 ) a realistic setting ? Especially , how is the reward accessible to the attacker ? * * A1 : In many RL applications , an agent learns by interacting with the outside environment . For example , learning how to drive on a road , learning how to recommend on a website , learning how to communicate by talking with people online , etc . In these cases , an attacker may eavesdrop on the interactions and alter them . Especially , the reward is usually a signal sent from the environment to the agent . For instance , consider a chat robot that learns by talking with people via the internet . The reward can be simply defined as the rating score people give to it after the chat . The rating scores are submitted by people from their devices , and transmitted through the internet into the robot 's server . In this process , an attacker can perform man-in-the-middle attacks , blocking and changing the scores . Our Figure 5 in Appendix B visualizes this process and compares it with the poisoning process in supervised learning . Essentially , poisoning is to alter the training data , and in RL , the `` training data '' is just the interaction trajectories ( states , actions and rewards ) . Thus the access to states , actions and rewards is analogous to the access to the training data , which is a common assumption in papers about poisoning . In addition , we would like to point out that almost all existing RL poisoning works [ 1-4 ] assume access to the interaction data ( states , actions and rewards ) , and most of them [ 1,3,4 ] also assume knowledge of to the MDP dynamics , or assume the attacker can directly interact with the environment [ 2 ] . > * * Q2 : The black-box attack studied in the paper is closer to white-box attack than to black-box attack : the attacker can still access the past and current observations ( state + action + reward ) , and the only limit is that the policy $ \\pi $ of the target model is unknown . This type of black-box attack is unrealistic in many situations . * * A2 : To the best of our knowledge , black-box attacking mainly refers to the case where the attacker does not know the learner 's model . Many popular works on black-box attacking [ 5 ] assume that the attacker is able to `` observe labels assigned by the DNN for chosen inputs '' , which is similar to our setting in the RL regime , since states are the inputs to the DNN , actions are the outputs . However , we agree that investigating the attacks under the scenarios that the attacker knows even less , especially with no knowledge of the reward , is also an important research topic . We would like to explore how a black-box attacker with knowledge of neither the learner 's model nor the learner 's reward history could poison the agent in the future . Refs : [ 1 ] Amin Rakhsha , et al.Policy teaching via environment poisoning : Training-time adversarial attacks against reinforcement learning . [ 2 ] Vahid Behzadan , et al.Vulnerability of deep reinforcement learning to policy induction attacks . [ 3 ] Yunhan Huang , et al.Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals . [ 4 ] Yuzhe Ma , et al.Policy poisoning in batch reinforcement learning and control . [ 5 ] Nicolas Papernot , et al.Practical Black-Box Attacks against Machine Learning"}], "0": {"review_id": "9r30XCjf5Dt-0", "review_text": "# # # # Summary : The paper studies poisoning attacks on RL agents , in which the attacker influences the agent 's learning process by changing the feedback obtained from the environment . The focus is put on attacking policy-based deep RL agents , without necessarily having access to the underlying MDP model of the environment . The paper proposes a new poisoning algorithm , called Vulnerability-Aware Adversarial Critic Poison , and experimentally demonstrates its effectiveness on 5 different RL environments . # # # # High level comments ( pros & cons ) : -The paper studies an important and interesting topic , poisoning attacks on RL agents , and develops a novel deep learning methods for designing more efficient and scalable attack strategies . -In contrast to prior work , the proposed algorithm utilizes deep RL techniques , making it applicable to more complex environments . The experimental results indicate the usefulness of the proposed method . -The clarity of the paper could be improved , including the statement and description of the optimization problem and the algorithm . Furthermore , it is not clear how the problem formulation compares to prior work . Some claims in the paper should be stated more precisely . -The proposed algorithm is a greedy approach and does not have provable guarantees on the optimality of the derived attack strategies . This is in contrast to prior work cited in this paper , which appears to have some guarantees on the performance of the attack . Overall , I enjoyed reading the paper , and its contributions seem novel and important for the line of work on poisoning attacks in RL . However , I also think that the paper could be improved in terms of clarity , and additional justifications and explanations could be added throughout the paper . # # # # More specific comments and suggestions for improvement : -The exposition of the results could be improved , and some parts clarified and made more precise . For example , footnote 1 is confusing , since it states that this paper assumes that the attacker poisons observations . On the other hand , the paper also mentions results on 'Hybrid aim poisoning ' . It is also confusing that optimization problem ( Q ) is defined as weighted loss , and then in the first paragraph of Section 4 we have the claim : 'Without loss of generality , we assume the loss weights $ j = 1 $ for all $ j = 1 , ... , K $ . ' . The first sentence in Challenge II is also not clear : why is Markovian property important in 'are no longer i.i.d.due to the Markovian property ' ? There are also sentences that do not seem to be precise . E.g. , the sentence 'However , in complex environments such as Atari games , knowing the dynamics of the MDP is difficult . ' does n't seem to be precise ( since dynamics can be obtained from Atari simulator ... ) . Given that the paper motivates its setting with Atari games , it is also not clear why Atari games were not used as a test-bed . -Parts of the optimization problem ( Q ) are somewhat confusing . In particular , the paragraph that explains constraint ( b ) ( imitate the learner ) , does not seem to precisely specify what this constraint looks like . In the white-box attack , it is written that the attacker knows the learner 's policy and can directly copy it . It is also stated for black-box attacker that it 'may know the learner \u2019 s algorithm ' , but does not know the learner 's policy . On the other hand , in Section 2 , it is written that 'white-box attackers , who know the learner \u2019 s model , and black-box attackers , who do not know the learner \u2019 s model . '.These parts could be explained in more detail , or more precisely stated . -Constraints ( c ) and ( d ) control the attacker 's influence , but the paper does not seem to indicate the practical importance of having both constraints . It might be useful add some discussion on this , as having both constraints seems to affects the algorithmic design proposed in the paper , and the complexity of the optimization problem . -The discussion in the related work section seems to put emphasis on practical importance of the proposed approach compared to some of the recent papers on poisoning attacks . However , it does not seem to elaborate on the differences in problem formulations , i.e. , optimization problems and objectives . -The focus seems to be on an episodic setting in which after each episode ( iteration ) , an RL agents updates its policy after the data is possibly poisoned . I 'm wondering to what extent would these result generalize to fully online setting in which an agent can change its policy after each action taken . Moreover , it is not clear how one can poison e.g.observations only after an episode ends ( since the same observations are needed to derive actions from the agent 's policy ) . Additional discussion on this would be valuable . -The model of the black box-attack is somewhat ambiguous . It is first stated that a black-box attacker may know the learner 's algorithm , but it is not specified to what extent the attacker relies on the knowledge about the learner 's algorithm . Furthermore , the notion of pseudo-learner does not seem to be defined . -Minor : There are two 'Step 5 ' in the description of the algorithm . Furthermore , the algorithm uses variables $ \\psi $ and $ \\Psi $ , which do not seem to be defined before section 4.3 . The paper also contains minor typos , e.g . : - the test-time evasion attacks Chen et al . ( 2019 ) where the attacker crafts - > citation style -alter the environment ( e.g.change the transition probabilities ) , - > remove space before , -On the contrary , We consider non-omniscient attackers - 'We ' should be 'we ' - ' a learner gains from the environment , i.e. , $ O = ( O^s , O^a , O^r ) $ ' - > Is $ O^a $ observed ? -Compared with Problem ( Q ) , - > ( Q ) should be ( Q ) -The solution to Problem ( P ) is always feasible to Problem ( Q , although ... - > e.g . ( Q should be ( Q ) etc . # # # # Questions : a. I did n't understand footnote 1 . It states that this paper assumes that the attacker poisons observations , but on the other hand , the results seem to suggest that other attacks are also considered . Could you clarify what types of attacks are considered in the paper ? b.The optimization problem ( Q ) seems to be different from the ones studied in ( Ma et al.2019 ) and ( Rakhsha et al.2020 ) .Could you elaborate on the differences between these attack formulations ? How does your setting relate to the setting of ( Zhang and Parkes , 2008 ) in terms of computational complexity ? c. The budget constraint ( c ) in optimization problem ( Q ) assumes that the 'cost ' of an attack is either 0 or 1 , whereas the constraint ( d ) already limits the 'power ' of the attack . Could you explain the practical importance of imposing these two constraints together ? d. Which practical application would support the episodic poisoning setting described in this paper ? e. How exactly is the pseudo-learner defined in this paper ( e.g. , in Section 5 ) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank R # 4 for the valuable and detailed feedback . Most of R # 4 's concerns are on the clarity of demonstrations and detailed explanations of the method . Due to the complexity of poisoning RL , lots of new and notations/concepts are required in our paper . For clarity , we organized the most important messages/concepts and provided as many intuitive explanations as possible through the main paper ; therefore the reader might have to find clarifications from the appendix for some technical details . For example , the * * detailed poisoning formulation and comparison with related works * * are in Appendix B ; more rigorous definitions of the stability radius are in Appendix D ; * * detailed algorithm illustration * * is in Appendix E ; we also provide some * * theoretical analysis * * on the relaxation problem in Appendix F. We understand that reviewers are not obligated to find all details in the appendix , therefore we greatly appreciate the reviewer 's comments , and have modified the main body of the paper according to R # 4 's valuable suggestions . We summarize all questions raised by R # 4 into 4 categories . * * Part 1.Questions and confusion due to some misinterpretation of our paper . * * * * Part 2 . Questions about problem formulation and theory . * * * * Part 3 . Questions about algorithm details and extensions . * * * * Part 4 . Questions due to imprecise/ambiguous wording in our previous manuscript . * * We will address all questions in each category respectively in 4 replies . All Refs : [ 1 ] Amin Rakhsha , et al.Policy teaching via environment poisoning : Training-time adversarial attacks against reinforcement learning . [ 2 ] Vahid Behzadan , et al.Vulnerability of deep reinforcement learning to policy induction attacks . [ 3 ] Yunhan Huang , et al.Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals . [ 4 ] Yizhen Wang , et at . Data Poisoning Attacks against Online Learning . [ 5 ] Alexander Turner et al.Clean-Label Backdoor Attacks [ 6 ] Yuzhe Ma , et al.Policy poisoning in batch reinforcement learning and control . [ 7 ] Haoqi Zhang , et al.Value-based policy teaching with active indirect elicitation"}, "1": {"review_id": "9r30XCjf5Dt-1", "review_text": "This paper studies a very important problem of poisoning attack against RL when the attacker is not omniscient . This is an important next step , as most prior work assumes omniscient for the sake of a more rigorous theoretical understanding ( e.g.Rakhsha et al. , 2020 , Zhang et al.2020 ) .However , the approach taken by this paper is too heuristic and only applies to a very limited setting where the learner needs to perform * * on-policy * * policy gradient methods , which no STOA algorithm does due to its poor sample efficiency , so there is n't much empirical value . At a high level , the approach this paper takes can be summarized as follows : It defines the optimal poisoning attack problem in an unknown environment as a * * sequential decision making problem * * , which is well-motivated and clear . It then proposed to simply use a * * greedy algorithm * * that only optimizes the current step 's cost without caring about what happens in the future . Then , of course , the greedy attacker does n't require the knowledge of the environment 's transition . Even if the attacker does know it , it would n't be using it anyway , because it only cares about the current step . And prior work has already shown that in the online data poisoning context , the greedy strategy can be * * exponentially worse * * than the optimal attack strategy . See [ 1 ] .Some technical questions : 1 . Correctness of Proposition 2 : In the Remarks of section 4.1 . It is mentioned that the value difference of two policies differed by at most delta in total variance distance will also be bounded by $ poly ( \\delta , 1/ ( 1-\\gamma ) ) $ . I did n't check the proof thoroughly but I feel that there are counter-examples ? Consider the classic `` combination lock '' MDP , where states form a chain of length H , and there are two actions in each state : the `` right '' action moves you right to the adjacent state , and the `` left '' action teleports you back to the starting state ( the left-most state ) . All rewards are zero except for when the agent successfully arrives at the right-most state , which takes H right actions consecutively . Now , the optimal policy ( always go right ) will have value $ 1 $ . But a $ \\delta $ -perturbed policy which now has $ \\delta $ probability of going left will only have value $ ( 1-\\delta ) ^H $ ( assuming a fixed episode length H and no discounting ) . So the gap seems to be exponential rather than polynomial . How does this example fit into the conclusion of proposition 2 ? 2.About adversarial critic : What exactly is this $ \\tilde V_\\omega $ estimating ? Is it estimating the value of the optimal policy , the learner 's current behavior policy , or the learner 's current $ \\hat\\pi_k $ which according to the paper is different from the behavior policy ? If it was the latter two , how does the attacker keep track of the value of an ever-changing policy , and how does he use this value estimate to evaluate the value of some other potential poisoned policy ? It 's all very confusing . 3.The estimated rank in step 6 of section 4.3 is only unbiased if the policy discrepancies $ \\hat \\psi_k $ are * i.i.d . * , which they clearly are not . [ 1 ] Xuezhou Zhang , Xiaojin Zhu , Laurent Lessard . Online Data Poisoning Attack . L4DC 2020 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> * * Q3 : Does the combination lock MDP violate Proposition 2 ? * * A3 : This example * * does not violate our Prop . 2 * * , because Prop . 2 upper bounds the * * reward/value drop * * caused by poisoning , i.e. , $ V_ { \\pi } ( s_0 ) -V_ { \\pi^\\prime } ( s_0 ) $ . In the combination lock MDP , if we assume non-discounting ( $ \\gamma=1 $ ) as R # 2 assumes , then our upper bound becomes $ \\infty $ since $ ( 1-\\gamma ) $ is in the denominator , thus the upper bound is valid ; in the other case , if $ 0\\leq\\gamma < 1 $ , the inequality given by Prop . 2 is $ $ V_ { \\pi } ( s_0 ) -V_ { \\pi^\\prime } ( s_0 ) = \\gamma^H - ( 1-\\delta ) ^H \\gamma^H \\leq a\\frac { \\delta^2\\gamma } { ( 1-\\gamma ) ^2 } + b\\delta $ $ where $ a $ and $ b $ are independent of $ \\delta , \\gamma $ and $ \\pi^\\prime $ , as detailed by our Prop . 2 in Appendix D. Although it is hard to derive an analytical proof of the inequality due to the complex form , we implemented a numerical grid test for \\delta=0:0.001:1 , \\gamma=0:0.001:1 , and for different H \u2019 s , none of them violates the above inequality . > * * Q4 : What exactly is the adversarial critic $ \\tilde { V } _\\omega $ estimating ? how does the attacker keep track of the value of an ever-changing policy , and how does he use this value estimate to evaluate the value of some other potential poisoned policy ? * * A4 : ( 1 ) * * What is the adversarial critic estimating ? * * $ \\tilde { V } _\\omega $ is the value function of the learner 's current behavior policy ( as we stated in Section 4.2 , it is trained with the unpoisoned trajectories generated by the learner 's policy ) . In other words , $ \\tilde { V } _\\omega $ is the critic for the learner 's current policy . Note that the critic/value function learned by the learner is not the correct value for its policy , because the learner uses poisoned trajectories to update its critic . And the attacker , on the contrary , observes the clean trajectories generated by the learner 's policy , thus can learn a correct critic for the policy . ( 2 ) * * How does the attacker keep track of the value of an ever-changing policy , and how does he use this value estimate to evaluate the value of some other potential poisoned policy ? * * $ \\tilde { V } _\\omega $ is the value of the policy in the current iteration , and the value will be updated in the next iteration when the policy changes . It is exactly the Actor-Critic framework , but the critic is owned by the adversary . For estimating the value of a potential poisoned policy , we use importance sampling , as we described in the last two paragraphs in Section 4.2 . We admit that in the original submission , some messages about the adversarial critic are embedded in the descriptions and might be hard to find for readers , although we did mention them . We have modified the paper to make these details more explicit . > * * Q5 : The estimated rank in step 6 of section 4.3 is not unbiased . * * A5 : R # 2 is right that the estimated rank is not unbiased . We apologize for this imprecise description . But the algorithm itself does not need the rank to be unbiased , we choose this rank estimation because it works much better than simply ranking the historical policy discrepancies . We have removed the word `` unbiased '' in our modified version ."}, "2": {"review_id": "9r30XCjf5Dt-2", "review_text": "The paper studies poisoning attacks against online reinforcement learning agents . The attacker has the power of manipulating the training data , i.e. , state-action-reward trajectories , in order to achieve some attack goal . The attack can be completely black-box , meaning that the proposed method allows an attack setting where the attacker has no knowledge of the RL algorithm used by the victim agent or the environment . In this scenario , the authors proposed that the attacker can imitate the learning procedure of the victim , and then based on the imitated policy ; the attacker designs how to poison the training data . The attack is formulated as a bi-level optimization , where the lower level involves the imitated learning procedure . Due to the intractability of sequential optimization , the original formulation is simplified so that only the attack only solves the attack on the current training data . This procedure is repeated in every episode to achieve sequential attacks . Experiments on a variety of tasks demonstrate the superiority of the proposed attack . Compared to prior works , the main advantage of this paper lies in that the attack can be applied in more complicated tasks where state or action space is continuous . Furthermore , the attack takes into account the adversarial effect of the current attack on future behavior of the victim agent . Therefore , the attack achieves better overall performance ( e.g.more times of target-action selection ) than traditional gradient-based attack such as FGSM . Another strength of the paper is that it provides some theoretical analysis in terms of how the relaxed optimization approximates the original complex attack optimization . This is in general a hard question to answer . Although the analysis is only about attack feasibility and how to test sub-optimality in hindsight , there is value in deriving those theoretical results . The experimental part of the paper is also strong . The authors have shown convincing results that demonstrate the proposed VA2C-P attack outperforms existing gradient-based FGSM attacks . Moreover , there are systematic empirical investigations on how the attack constraint parameter epsilon affects the attack performance . Finally the paper is well-written and provides a nice summary of prior works , as well as why each prior work fails to achieve some desired property of attack in an ideal sequential attack scenario . Therefore , overall I think the paper is nice and makes significant contribution . One disadvantage of the paper is that while the paper claims able to handle sequential attacks , the relaxed attack optimization seems solving only the desired manipulation on training data in the current episode . As a result , the solution is definitely sub-optimal . The authors provided a method of evaluating whether the implemented attack is sub-optimal in hindsight , specifically the proposition 7 . This result , while being interesting , is not useful in that it can not help practitioners gauge if the computed attack is optimal or not , since it \u2019 s a necessary condition . I am wondering if the authors could provide some insight on how sub-optimal is the attack in this paper , and potential ways to further improve it . I also want to point out that the attacker knowledge assumed in this paper is not strictly less than prior works such as in Zhang 2020 . Both need to have access to a simulator of the MDP and victim environment , and both need to exhaust large amount of training data before obtaining a good attack policy . In this paper , the attacker needs to imitate the victim learner , and the accuracy of imitation result will depend on how many training data are available .", "rating": "7: Good paper, accept", "reply_text": "We appreciate the positive feedback and constructive suggestions provided by R # 1 . Especially , we appreciate a lot that R # 1 noticed our theoretical analysis about the relaxation and thought them valuable . We address some questions raised by R # 1 as follows : > * * Q1 : How to determine the sub-optimality of the attack ? * * A1 : ( Insights on measuring sub-optimality ) R # 1 is right that Proposition 7 provides a necessary condition for the attack being optimal . And we admit that due to the high non-convexity , it is difficult to judge whether an attack is globally optimal for the whole attacking process . However , Prop . 7 also provides an idea of judging how likely a past attack decision is locally optimal . Equation ( 15 ) essentially measures whether the actual search ( attack ) direction matches the optimal direction . If they match , then the attack is at least locally optimal ; if they do not match , then the attack is nonoptimal even locally . In practice , we want the LHS of Equation ( 15 ) to be as * * aligned to the RHS as possible * * for local optimality , because it suggests that the attacker is leading the agent to the desired direction . Otherwise , if the search direction is opposite to the optimal direction , one may consider changing the attack strategy ( but note that being nonoptimal does not necessarily mean that the current poisoning is bad ; it may still deprave the victim significantly ) . > * * Q2 : Are there ways to improve the optimality of attacking ? * * A2 : There are some potential ways to further improve our proposed attack , although they may require more computations and knowledge . ( 1 ) The attacker can fit a prediction model for the unknown environment ( predict future states and rewards given the current state and action ) using the trajectories generated by the victim policy . Then the attacker can predict future trajectories . In this case , we can use Proposition 7 as `` foresight '' rather than `` hindsight '' . More specifically , we may look $ N $ -iterations ahead , and figure out a poison direction that is optimal for the next $ N $ iterations instead of the next one iteration . However , it may require more computations than our current method . ( 2 ) If the attacker is allowed to directly interact with the environment ( which is not allowed in our paper ) , or even the attacker knows the dynamics of the environment , then it may be possible to pre-compute the `` ideal '' attacks . For example , assume one is doing targeted poisoning , then using Inverse RL , one can design an `` ideal '' reward function such that any agent will learn the target policy if the rewards are given by the `` ideal '' reward function . ( Although Inverse RL focuses on teaching the agent a good policy , the adversary can do the opposite and set a malicious policy as the target . ) Then during the victim 's online learning process , the goal of the attacker is to perturb the actual observations toward the `` ideal '' attack directions as much as possible under their budget and power constraints . This simplified problem is still hard to solve due to the limited budget and power , but it would be more effective since it has extra knowledge of the environment . > * * Q3 : Is the required knowledge less than prior work ? * * A3 : As mentioned in A2 ( 2 ) , we would like to clarify that our proposed attacker * * needs not to interact with the MDP or a simulator * * ; we only use the learner 's observations generated via the learner 's interaction with the environment . The word `` imitate '' might be misleading to readers , and we will make it more clear in the modified version . In our Problem ( Q ) line ( b ) , the notation $ \\mathcal { O } _j $ is the observation that is already gained by the learner/victim in the $ j $ -th iteration , and $ \\check { \\mathcal { O } } _j $ denotes the perturbed/poisoned observation with a specific poison aim $ \\check { \\mathcal { D } } _j $ ( $ \\check { \\mathcal { D } } _j $ is to be determined by the attacker himself ) . And imitating just means that the attacker wants to follow what the victim would do once it receives the poisoned observation $ \\check { \\mathcal { O } } _j $ . In other words , the attacker maintains its own `` copy of victim '' by feeding it with whatever training data the actual victim uses . Therefore , the attacker only needs to `` eavesdrop '' on the interactions between the learner and the environment , and requires * * no extra training data from the environment * * . In contrast , paper [ 1 ] mentioned by R # 1 proposes a white-box reward-poisoning method , as well as sound theoretical results for the feasibility and optimality of targeted attacks , although [ 1 ] focuses on finite MDPs and the attacker requires the knowledge of the MDP parameters . In contrast , our attacking strategy does not require knowledge of the MDP , and works for large , continuous MDPs , although optimality is not guaranteed . Therefore , our poisoning method and the method in [ 1 ] work in different scenarios . Refs : [ 1 ] Zhang , et al.Adaptive Reward-Poisoning Attacks against Reinforcement Learning . 2020 ."}, "3": {"review_id": "9r30XCjf5Dt-3", "review_text": "Summary : This paper proposes a poisoning algorithm named Vulnerability-Aware Adversarial Critic Poison ( VA2C-P ) to attack policy-based deep reinforcement learning agents . The poisoning attack is formulated as a sequential bilevel optimisation problem ( Problem Q ) , where the attacker either minimises the expected total rewards of the learner ( non-targeted poisoning ) , or forces the learner to learn a target policy ( targeted poisoning ) . To solve Problem Q , VA2C-P mainly makes two decision : ( 1 ) when to attack : a new metric named stability radius is proposed to decide the attack timing , ( 2 ) how to attack : a mechanism of adversarial critic is designed to solve a relaxed version of Problem Q by only considering the loss of the immediate next iteration . Pros : 1.It is an important question to investigate how policy-based RL algorithms can be poisoned by adversarial attacks . 2.It is novel to propose a poisoning method against policy-based RL agents , which has not been studied before . 3.The proposed poisoning framework ( Problem Q ) is a general formulation that covers a variety of models . 4.VA2C-P has been demonstrated to be effective in targeted and untargeted attacks , under both white-box and black-box settings . Cons : 1.This paper considers a scenario where ( 1 ) \u201c the attacker does not know the underlying dynamics of MDP , and can not directly interact with the environment , either \u201d ; ( 2 ) the attacker is able to obtain the states observed by the agent , their actions and rewards . Is ( 2 ) a realistic setting ? Especially , how is the reward accessible to the attacker ? 2.The black-box attack studied in the paper is closer to white-box attack than to black-box attack : the attacker can still access the past and current observations ( state + action + reward ) , and the only limit is that the policy $ \\pi $ of the target model is unknown . This type of black-box attack is unrealistic in many situations .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank R # 3 for the detailed summarization and valuable comments . And we address the concerns R # 3 mentioned as follows , most of which are related to the knowledge of a poisoning attacker . > * * Q1 : This paper considers a scenario where ( 1 ) \u201c the attacker does not know the underlying dynamics of MDP , and can not directly interact with the environment , either \u201d ; ( 2 ) the attacker is able to obtain the states observed by the agent , their actions and rewards . Is ( 2 ) a realistic setting ? Especially , how is the reward accessible to the attacker ? * * A1 : In many RL applications , an agent learns by interacting with the outside environment . For example , learning how to drive on a road , learning how to recommend on a website , learning how to communicate by talking with people online , etc . In these cases , an attacker may eavesdrop on the interactions and alter them . Especially , the reward is usually a signal sent from the environment to the agent . For instance , consider a chat robot that learns by talking with people via the internet . The reward can be simply defined as the rating score people give to it after the chat . The rating scores are submitted by people from their devices , and transmitted through the internet into the robot 's server . In this process , an attacker can perform man-in-the-middle attacks , blocking and changing the scores . Our Figure 5 in Appendix B visualizes this process and compares it with the poisoning process in supervised learning . Essentially , poisoning is to alter the training data , and in RL , the `` training data '' is just the interaction trajectories ( states , actions and rewards ) . Thus the access to states , actions and rewards is analogous to the access to the training data , which is a common assumption in papers about poisoning . In addition , we would like to point out that almost all existing RL poisoning works [ 1-4 ] assume access to the interaction data ( states , actions and rewards ) , and most of them [ 1,3,4 ] also assume knowledge of to the MDP dynamics , or assume the attacker can directly interact with the environment [ 2 ] . > * * Q2 : The black-box attack studied in the paper is closer to white-box attack than to black-box attack : the attacker can still access the past and current observations ( state + action + reward ) , and the only limit is that the policy $ \\pi $ of the target model is unknown . This type of black-box attack is unrealistic in many situations . * * A2 : To the best of our knowledge , black-box attacking mainly refers to the case where the attacker does not know the learner 's model . Many popular works on black-box attacking [ 5 ] assume that the attacker is able to `` observe labels assigned by the DNN for chosen inputs '' , which is similar to our setting in the RL regime , since states are the inputs to the DNN , actions are the outputs . However , we agree that investigating the attacks under the scenarios that the attacker knows even less , especially with no knowledge of the reward , is also an important research topic . We would like to explore how a black-box attacker with knowledge of neither the learner 's model nor the learner 's reward history could poison the agent in the future . Refs : [ 1 ] Amin Rakhsha , et al.Policy teaching via environment poisoning : Training-time adversarial attacks against reinforcement learning . [ 2 ] Vahid Behzadan , et al.Vulnerability of deep reinforcement learning to policy induction attacks . [ 3 ] Yunhan Huang , et al.Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals . [ 4 ] Yuzhe Ma , et al.Policy poisoning in batch reinforcement learning and control . [ 5 ] Nicolas Papernot , et al.Practical Black-Box Attacks against Machine Learning"}}