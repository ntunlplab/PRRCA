{"year": "2019", "forum": "Hk4fpoA5Km", "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning", "decision": "Accept (Poster)", "meta_review": "This work highlights the problem of biased rewards present in common adversarial imitation learning implementations, and proposes adding absorbing states to to fix the issue. This is combined with an off-policy training algorithm, yielding significantly improved sample efficiency, whose benefits are convincingly shown empirically. The paper is well written and clearly presents the contributions. Questions were satisfactorily answered during discussion, and resulted in an improved submission, a paper that all reviewers now agree is worth presenting at ICLR.\n", "reviews": [{"review_id": "Hk4fpoA5Km-0", "review_text": "The paper suggests to use TD3 to compute an off-policy update instead of the TRPO/PPO updates in GAIL/AIRL in order to increase sample efficiency. The paper further discusses the problem of implicit step penalties and survival bias caused by absorbing states, when using the upper-bounded/lower-bounded reward functions log(D) and -(1-log(D)) respectively. To tackle these problem, the paper proposes to explicit add a unique absorbing state at the end of each trajectory, such that its rewards can be learned as well. Pro: The paper is well written and clearly presented. Using a more sample efficient RL method for the policy update is sensible and turned out effective in the experiments. Properly handling simulator resets in MDPs is a well known problem in reinforcement learning that I think is insufficiently discussed in the context of IRL. Cons: The contributions seem rather small. a) Replacing the policy update is trivial, since the rl methods are used as black-box modules for the discussed AIL methods. b) Using importance weighting to reuse old trajectories for the discriminator update hardly counts as a contribution either--especially when the importance weights are simply omitted in practice. I also think that the reported problems due to the high variance have not been sufficiently investigated. There should be a better solution than just pretending that the replay buffer corresponds to roll-outs of the current policy. Would it maybe help to use self-normalized importance weights? The paper does also not analyze how such assumption/approximation affects the theoretical guarantees. c) The problem with absorbing states is in my opinion the most interesting contribution of the paper. However, the discussion is rather shallow and I do not think that the illustrative example is very convincing. Section 4.1.1. argues that for the given policy roll-out, the discriminator reward puts more reward on the policy trajectory than the expert trajectory. However, it is neither surprising nor problematic that the discriminator reward does not produce the desired behavior during learning. By assigning more cumulative reward for s2_a1->s1 than for s2_a2->g, the policy would (after a few more updates) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q(s2,a2) > Q(s2,a1)--when the policy would match the expert exactly. The illustrative example also uses more policy-labeled transitions than agent-labeled ones for learning the classifier, which may also be problematic. The paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states, which I think is not true in general. A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state. Hence, the immediate reward for choosing such action can be made larger than the discounted future reward when not ending the episode (for any gamma < 1). Even for state-only reward functions the problem does not persist when reseting the environment after reaching the absorbing state such that the training trajectories contain states that are only reached if the simulator gets reset. Hence, I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented. This may be different for resets due to time limits that can not be predicted by the last state-action tuple. However, issues relating to time limits are not addressed in the paper. I also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation. --------------- Update 21.11.2018 I think my initial assessment was too positive. During the rebuttal, I noticed that the discussion of reward bias was not only shallow but also wrong in some aspects and very misleading, because problems arising from hacky implementations of some RL toolboxes were discussed as theoretical shortcoming of AIL algorithms. Hence, I think the initial submission should be clearly rejected. However, the authors submitted a revised version that presents the root of the observed problem much more accurately. I think that the revised version is substantially better than the original submission. However, I think that my initial rating is still valid (better: became valid), because the main issues that I raised for the initial submission still apply to the current revision, namely: - The technical contributions are minor. - The theoretical discussion (in particular regarding absorbing states) is quite shallow. The merits of the paper are: - Good results due to off-policy learning - Raising awareness and providing a fix for a common pitfall I think that the problems arising from incorrectly treated absorbing states needs to be discussed more profoundly. Some suggestions: Section 3.1 \"As we discuss in detail in Section 4.2 [...]\" I think this should refer to section 4.1. Also the discussion should in section 4.1 should be a bit more detailed. How do common implementations implicitly assign zero rewards? Which implementations are affected? Which papers published inferior results due to this bug? I think it is also important to note, that absorbing states are hidden from the algorithm and that the reward function is thus only applied to non-absorbing states. \"We will demonstrate empirically in Section 4.1 [...]\" The demonstration is currently missing. I think it would be nice to illustrate the problem on a simple example. The original example might actually work, as shown by the code example of the rebuttal, however the explanation was not convincing. Maybe it would be easier to argue with a simpler algorithm (e.g MaxEnt-IRL, potentially projecting the rewards to positive values)? Section 3.1 seems to focus too much on resets that are caused by time limits. Such resets are inherently different from terminal states such as falling down in locomotion tasks, because they can not be modelled with the given MDP formulation unless time is considered part of the state. Indeed, I think that for infinite horizon MDPs without time-awareness, time limits can not be modelled using absorbing states (I think the RL book misses to mention that time needs to be part of the state such that the policy remains Markovian, which is a bit misleading). Instead those resets are often handled by returning an estimate of the future return (bootstrapping). This treatment of time limits is already part of the TD3 implementation and as far as I understood not the focus of the paper. Instead section 3.1. should focus on resets caused by task failure/completion, which can actually be modelled with absorbing states, because the agent will always transition to the absorbing state when a terminal state is reached which is in line with Markovian dynamics. Section 4.2 should also add a few more details. Did I understand correctly, that when computing the return R_T the sum is indeed finite and stopped after a fixed horizon? If yes, this should be reflected in the equation, and the horizon should be mentioned in the paper. The paper should also better explain how the proposed fix enables the algorithm to learn the reward of the absorbing state. For example, section 4.2. does not even mention that the state s_a was added as part of the solution. ------------- Update 22.11.2018 By highlighting the difference between termination due to time-limits and termination due to task completion, and by better describing how the proposed fix addresses the problem of reward bias that is present in common AIL implementations, the newest revision further improves the submission. I think that the submission can get accepted and I adapted my rating accordingly. Minor: Conclusion should also squeeze in somehow that the reward biases are caused by the implementations. Typo in 4.2: \"Thus, when sample[sic] from the replay buffer AIL algorithms will be able to see absorbing states there[sic] were previous hidden, [...]\" ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the detailed and constructive feedback . We address the above mentioned points and add some additional experiments , as detailed below . c ) \u201c By assigning more cumulative reward for s2_a1- > s1 than for s2_a2- > g , the policy would ( after a few more updates ) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q ( s2 , a2 ) > Q ( s2 , a1 ) -- when the policy would match the expert exactly. \u201d \u201c The paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states , which I think is not true in general . A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state. \u201d > This is a good point , and we will discuss this situation in more detail in the final paper . However , we do not believe that this directly applies to adversarial learning algorithms , such as the ones studied in our paper . We provide discussion as well as a numerical example below , which will be included in the paper . The aforementioned situation can only happen in the limit , but the next discriminator update will return the policy to the previous state , in which it is more advantageous to take a loop , according to the GAIL reward definition . Therefore , the original formulation of the algorithm does not converge in this case . In contrast , learning rewards for the absorbing states will resolve this issue . Moreover , the example provided by the reviewer assumes that we can fix the reward function at some point of training and then train the policy to optimality according to this reward function ; while devising a scheme to early terminate learning of the reward function is possible , it is not specified by the dynamic reward learning mechanisms of the GAIL algorithm , which alternates updates between the policy and the discriminator . Please see a simple script that illustrates the example ( anonymous link ) : https : //colab.research.google.com/drive/1gV56NLik367nslwK7iJzs8WTe5tD-BO5 This specific toy example will be included into our open source release . \u201c Hence , I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented. \u201d > Could you please clarify what do you mean by a correct implementation of simulation resets ? \u201c I also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation. \u201d > We think that it is less stable to analytically compute the returns for absorbing states as it introduces a high variance for TD updates of the value network due to the fact that we bootstrapped for all states . The issue is well known and usually solved by using target networks ( see https : //www.nature.com/articles/nature14236 ) . \u201c This may be different for resets due to time limits that can not be predicted by the last state-action tuple . However , issues relating to time limits are not addressed in the paper \u201d > Although some of the benchmark tasks do have an episodic time limit , an off-policy RL algorithm can still calculate a ( discounted ) target value at the last time step in such environments , which is what our implementation of TD3 actually does . Please see the original implementation of TD3 for more details : https : //github.com/sfujim/TD3/blob/master/main.py # L123 a ) We note that this does make a substantial difference in terms of sample efficiency over prior work on adversarial IL , as shown in Figure 4 -- we believe that such substantial improvements in efficiency are of interest to the ICLR community , though it is not the sole contribution of our paper . b ) We did use normalized importance weights , but unfortunately did not find that the resulting method performed well , while simply omitted importance weights achieved good performance . We think that the naive way of estimating importance weights increases variance of updates . We will analyze this further in the final version , but for now we would emphasize that this is not the primary contribution of the work , but only a technical detail that we discussed for completeness ."}, {"review_id": "Hk4fpoA5Km-1", "review_text": "The authors find 2 issues with Adversarial Imitation Learning-style algorithms: I) implicit bias in the reward functions and II) despite abilities of coping with little data, high interaction with the environment is required. The authors suggest \"Discriminator-Actor-Critic\" - an off-policy Reinforcement Learning reducing complexity up to 10 and being unbiased, hence very flexible. Several standard tasks, a robotic, and a VR task are used to show-case the effectiveness by a working implementation in TensorFlow Eager. The paper is well written, and there is practically no criticism. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the feedback and appreciate the strong recommendation ."}, {"review_id": "Hk4fpoA5Km-2", "review_text": "This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed \"Discriminator-Actor-Critic\". They key point here being that they propose a replay buffer to sample transitions from. It is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. Pros: * Well written * Motivation is clear * Example on biased reward functions * Experiments are carefully designed and thorough Cons: * The analysis of the results in section 5.1 is a bit short Questions: * You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning? * What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend * In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments? * Is it possible to show results of the effect of absorbing states on the Mujoco environments? Minor suggestions: In Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . We have extended the section 5.1 of the manuscript as suggested by the reviewer . Below are detailed answers for the reviewer \u2019 s concerns : 1 ) To simplify the exposition we omitted the entropy penalty as it does not contribute meaningfully to the algorithm performance in our experimentation . Similar findings were observed in the GAIL paper , where the authors disregarded the entropy coefficient for every tested environment , except for the Reacher environment . 2 ) We added the performance of a random policy to the graph to be consistent with the original GAIL paper . We believe that it improves readability of the plot by providing necessary scaling . 3 ) We already started working on additional experimentation as requested . We will update the manuscript as soon as we gather these results . 4 ) We observed the same effect of having absorbing states in the Kuka arm tasks ( Fig.6 ) , as in the MuJoCo environments . Also , we evaluated absorbing states within the AIRL framework for Walker-2D and Hopper environments ( Fig.7 ) .We demonstrate that proper handling of absorbing states is critical for effectively imitating the expert policy . In addition , we updated the paper to accommodate the minor suggestions proposed by the reviewer ."}], "0": {"review_id": "Hk4fpoA5Km-0", "review_text": "The paper suggests to use TD3 to compute an off-policy update instead of the TRPO/PPO updates in GAIL/AIRL in order to increase sample efficiency. The paper further discusses the problem of implicit step penalties and survival bias caused by absorbing states, when using the upper-bounded/lower-bounded reward functions log(D) and -(1-log(D)) respectively. To tackle these problem, the paper proposes to explicit add a unique absorbing state at the end of each trajectory, such that its rewards can be learned as well. Pro: The paper is well written and clearly presented. Using a more sample efficient RL method for the policy update is sensible and turned out effective in the experiments. Properly handling simulator resets in MDPs is a well known problem in reinforcement learning that I think is insufficiently discussed in the context of IRL. Cons: The contributions seem rather small. a) Replacing the policy update is trivial, since the rl methods are used as black-box modules for the discussed AIL methods. b) Using importance weighting to reuse old trajectories for the discriminator update hardly counts as a contribution either--especially when the importance weights are simply omitted in practice. I also think that the reported problems due to the high variance have not been sufficiently investigated. There should be a better solution than just pretending that the replay buffer corresponds to roll-outs of the current policy. Would it maybe help to use self-normalized importance weights? The paper does also not analyze how such assumption/approximation affects the theoretical guarantees. c) The problem with absorbing states is in my opinion the most interesting contribution of the paper. However, the discussion is rather shallow and I do not think that the illustrative example is very convincing. Section 4.1.1. argues that for the given policy roll-out, the discriminator reward puts more reward on the policy trajectory than the expert trajectory. However, it is neither surprising nor problematic that the discriminator reward does not produce the desired behavior during learning. By assigning more cumulative reward for s2_a1->s1 than for s2_a2->g, the policy would (after a few more updates) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q(s2,a2) > Q(s2,a1)--when the policy would match the expert exactly. The illustrative example also uses more policy-labeled transitions than agent-labeled ones for learning the classifier, which may also be problematic. The paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states, which I think is not true in general. A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state. Hence, the immediate reward for choosing such action can be made larger than the discounted future reward when not ending the episode (for any gamma < 1). Even for state-only reward functions the problem does not persist when reseting the environment after reaching the absorbing state such that the training trajectories contain states that are only reached if the simulator gets reset. Hence, I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented. This may be different for resets due to time limits that can not be predicted by the last state-action tuple. However, issues relating to time limits are not addressed in the paper. I also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation. --------------- Update 21.11.2018 I think my initial assessment was too positive. During the rebuttal, I noticed that the discussion of reward bias was not only shallow but also wrong in some aspects and very misleading, because problems arising from hacky implementations of some RL toolboxes were discussed as theoretical shortcoming of AIL algorithms. Hence, I think the initial submission should be clearly rejected. However, the authors submitted a revised version that presents the root of the observed problem much more accurately. I think that the revised version is substantially better than the original submission. However, I think that my initial rating is still valid (better: became valid), because the main issues that I raised for the initial submission still apply to the current revision, namely: - The technical contributions are minor. - The theoretical discussion (in particular regarding absorbing states) is quite shallow. The merits of the paper are: - Good results due to off-policy learning - Raising awareness and providing a fix for a common pitfall I think that the problems arising from incorrectly treated absorbing states needs to be discussed more profoundly. Some suggestions: Section 3.1 \"As we discuss in detail in Section 4.2 [...]\" I think this should refer to section 4.1. Also the discussion should in section 4.1 should be a bit more detailed. How do common implementations implicitly assign zero rewards? Which implementations are affected? Which papers published inferior results due to this bug? I think it is also important to note, that absorbing states are hidden from the algorithm and that the reward function is thus only applied to non-absorbing states. \"We will demonstrate empirically in Section 4.1 [...]\" The demonstration is currently missing. I think it would be nice to illustrate the problem on a simple example. The original example might actually work, as shown by the code example of the rebuttal, however the explanation was not convincing. Maybe it would be easier to argue with a simpler algorithm (e.g MaxEnt-IRL, potentially projecting the rewards to positive values)? Section 3.1 seems to focus too much on resets that are caused by time limits. Such resets are inherently different from terminal states such as falling down in locomotion tasks, because they can not be modelled with the given MDP formulation unless time is considered part of the state. Indeed, I think that for infinite horizon MDPs without time-awareness, time limits can not be modelled using absorbing states (I think the RL book misses to mention that time needs to be part of the state such that the policy remains Markovian, which is a bit misleading). Instead those resets are often handled by returning an estimate of the future return (bootstrapping). This treatment of time limits is already part of the TD3 implementation and as far as I understood not the focus of the paper. Instead section 3.1. should focus on resets caused by task failure/completion, which can actually be modelled with absorbing states, because the agent will always transition to the absorbing state when a terminal state is reached which is in line with Markovian dynamics. Section 4.2 should also add a few more details. Did I understand correctly, that when computing the return R_T the sum is indeed finite and stopped after a fixed horizon? If yes, this should be reflected in the equation, and the horizon should be mentioned in the paper. The paper should also better explain how the proposed fix enables the algorithm to learn the reward of the absorbing state. For example, section 4.2. does not even mention that the state s_a was added as part of the solution. ------------- Update 22.11.2018 By highlighting the difference between termination due to time-limits and termination due to task completion, and by better describing how the proposed fix addresses the problem of reward bias that is present in common AIL implementations, the newest revision further improves the submission. I think that the submission can get accepted and I adapted my rating accordingly. Minor: Conclusion should also squeeze in somehow that the reward biases are caused by the implementations. Typo in 4.2: \"Thus, when sample[sic] from the replay buffer AIL algorithms will be able to see absorbing states there[sic] were previous hidden, [...]\" ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the detailed and constructive feedback . We address the above mentioned points and add some additional experiments , as detailed below . c ) \u201c By assigning more cumulative reward for s2_a1- > s1 than for s2_a2- > g , the policy would ( after a few more updates ) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q ( s2 , a2 ) > Q ( s2 , a1 ) -- when the policy would match the expert exactly. \u201d \u201c The paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states , which I think is not true in general . A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state. \u201d > This is a good point , and we will discuss this situation in more detail in the final paper . However , we do not believe that this directly applies to adversarial learning algorithms , such as the ones studied in our paper . We provide discussion as well as a numerical example below , which will be included in the paper . The aforementioned situation can only happen in the limit , but the next discriminator update will return the policy to the previous state , in which it is more advantageous to take a loop , according to the GAIL reward definition . Therefore , the original formulation of the algorithm does not converge in this case . In contrast , learning rewards for the absorbing states will resolve this issue . Moreover , the example provided by the reviewer assumes that we can fix the reward function at some point of training and then train the policy to optimality according to this reward function ; while devising a scheme to early terminate learning of the reward function is possible , it is not specified by the dynamic reward learning mechanisms of the GAIL algorithm , which alternates updates between the policy and the discriminator . Please see a simple script that illustrates the example ( anonymous link ) : https : //colab.research.google.com/drive/1gV56NLik367nslwK7iJzs8WTe5tD-BO5 This specific toy example will be included into our open source release . \u201c Hence , I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented. \u201d > Could you please clarify what do you mean by a correct implementation of simulation resets ? \u201c I also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation. \u201d > We think that it is less stable to analytically compute the returns for absorbing states as it introduces a high variance for TD updates of the value network due to the fact that we bootstrapped for all states . The issue is well known and usually solved by using target networks ( see https : //www.nature.com/articles/nature14236 ) . \u201c This may be different for resets due to time limits that can not be predicted by the last state-action tuple . However , issues relating to time limits are not addressed in the paper \u201d > Although some of the benchmark tasks do have an episodic time limit , an off-policy RL algorithm can still calculate a ( discounted ) target value at the last time step in such environments , which is what our implementation of TD3 actually does . Please see the original implementation of TD3 for more details : https : //github.com/sfujim/TD3/blob/master/main.py # L123 a ) We note that this does make a substantial difference in terms of sample efficiency over prior work on adversarial IL , as shown in Figure 4 -- we believe that such substantial improvements in efficiency are of interest to the ICLR community , though it is not the sole contribution of our paper . b ) We did use normalized importance weights , but unfortunately did not find that the resulting method performed well , while simply omitted importance weights achieved good performance . We think that the naive way of estimating importance weights increases variance of updates . We will analyze this further in the final version , but for now we would emphasize that this is not the primary contribution of the work , but only a technical detail that we discussed for completeness ."}, "1": {"review_id": "Hk4fpoA5Km-1", "review_text": "The authors find 2 issues with Adversarial Imitation Learning-style algorithms: I) implicit bias in the reward functions and II) despite abilities of coping with little data, high interaction with the environment is required. The authors suggest \"Discriminator-Actor-Critic\" - an off-policy Reinforcement Learning reducing complexity up to 10 and being unbiased, hence very flexible. Several standard tasks, a robotic, and a VR task are used to show-case the effectiveness by a working implementation in TensorFlow Eager. The paper is well written, and there is practically no criticism. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the feedback and appreciate the strong recommendation ."}, "2": {"review_id": "Hk4fpoA5Km-2", "review_text": "This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed \"Discriminator-Actor-Critic\". They key point here being that they propose a replay buffer to sample transitions from. It is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. Pros: * Well written * Motivation is clear * Example on biased reward functions * Experiments are carefully designed and thorough Cons: * The analysis of the results in section 5.1 is a bit short Questions: * You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning? * What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend * In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments? * Is it possible to show results of the effect of absorbing states on the Mujoco environments? Minor suggestions: In Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . We have extended the section 5.1 of the manuscript as suggested by the reviewer . Below are detailed answers for the reviewer \u2019 s concerns : 1 ) To simplify the exposition we omitted the entropy penalty as it does not contribute meaningfully to the algorithm performance in our experimentation . Similar findings were observed in the GAIL paper , where the authors disregarded the entropy coefficient for every tested environment , except for the Reacher environment . 2 ) We added the performance of a random policy to the graph to be consistent with the original GAIL paper . We believe that it improves readability of the plot by providing necessary scaling . 3 ) We already started working on additional experimentation as requested . We will update the manuscript as soon as we gather these results . 4 ) We observed the same effect of having absorbing states in the Kuka arm tasks ( Fig.6 ) , as in the MuJoCo environments . Also , we evaluated absorbing states within the AIRL framework for Walker-2D and Hopper environments ( Fig.7 ) .We demonstrate that proper handling of absorbing states is critical for effectively imitating the expert policy . In addition , we updated the paper to accommodate the minor suggestions proposed by the reviewer ."}}