{"year": "2017", "forum": "HJgXCV9xx", "title": "Dialogue Learning With Human-in-the-Loop", "decision": "Accept (Poster)", "meta_review": "pros:\n - demonstration that using teacher's feedback to improve performance in a dialogue system can be made to work \n  in a real-world setting\n - comprehensive experiments\n \n cons:\n - lack of technical novelty due to prior work\n - not all agree with the RL vs not-RL (pre-built datasets) distinction suggested in the paper with respect to the previous work\n \n Overall, the paper makes a number of practical contributions and evaluation, rather than theoretical novelty.", "reviews": [{"review_id": "HJgXCV9xx-0", "review_text": " SUMMARY: This paper describes a set of experiments evaluating techniques for training a dialogue agent via reinforcement learning. A standard memory network architecture is trained on both bAbI and a version of the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous experiments are performed comparing the behavior of different training algorithms under various experimental conditions. STRENGTHS: The experimentation is comprehensive. I agree with the authors that these results provide additional useful insight into the performance of the model in the 2016 paper (henceforth W16). WEAKNESSES: This is essentially an appendix to the earlier paper. There is no new machine learning content. Secondarily, the paper seems to confuse the distinction between \"training with an adaptive sampling procedure\" and \"training in interactive environments\" more generally. In particular, no comparisons are presented to the to the experiments with a static exploration policy presented in W16, when the two training can & should be evaluated side-by-side. The only meaningful changes between this work and W16 involve simple (and already well-studied) changes to the form of this exploration policy. My primary concern remains about novelty: the extra data introduced here is welcome enough, but probably belongs in a *ACL short paper or a technical report. This work does not stand on its own, and an ICLR submission is not an appropriate vehicle for presenting it. \"REINFORCEMENT LEARNING\" [Update: concerns in this section have been addressed by the authors.] This paper attempts to make a hard distinction between the reinforcement learning condition considered here and the (\"non-RL\") condition considered in W16. I don't think this distinction is nearly as sharp as it's made out to be. As already noted in Weston 2016, the RBI objective is a special case of vanilla policy gradient with a zero baseline and off-policy samples. In this sense the version of RBI considered in this paper is the same as in W16, but with a different exploration policy; REINFORCE is the same objective with a nontrivial baseline. Similarly, the change in FP is only a change to the sampling policy. The fixed dataset / online learning distinction is not especially meaningful when the fixed dataset consists of endless synthetic data. It should be noted that some variants of the exploration policy in W16 provide a stronger training signal than is available in the RL \"from scratch\" setting here: in particular, when $\\pi_acc = 0.5$ the training samples will feature much denser reward. However, if I correctly understand Figures 3 and 4 in this paper, the completely random initial policy achieves an average reward of ~0.3 on bAbI and ~0.1 on movies---as good or better than the other exploration policies in W16! I think this paper would be a lot clearer if the delta from W16 were expressed directly in terms of their different exploration policies, rather than trying to cast all of the previous work as \"not RL\" when it can be straightforwardly accommodated in the RL framework. I was quite confused by the fact that no direct comparisons are made to the training conditions in the earlier work. I think this is a symptom of the problem discussed above: once this paper adopts the position that this work is about RL and the previous work is not, it becomes possible to declare that the two training scenarios are incomparable. I really think this is a mistake---to the extent that the off-policy sample generators used in the previous paper are worse than chance, it is always possible to compare to them fairly here. Evaluating everything in the \"online\" setting and presenting side-by-side experiments would provide a much more informative picture of the comparative behavior of the various training objectives. ON-POLICY VS OFF-POLICY Vanilla policy gradient methods like the ones here typically can't use off-policy samples without a little extra hand-holding (importance sampling, trust region methods, etc.). They seem to work out of the box for a few of the experiments in this paper, which is an interesting result on its own. It would be nice to have some discussion of why that might be the case. OTHER NOTES - The claim that \"batch size is related to off-policy learning\" is a little odd. There are lots of on-policy algorithms that require the agent to collect a large batch of transitions from the current policy before performing an (on-policy) update. - I think the experiments on fine-tuning to human workers are the most exciting part of this work, and I would have preferred to see these discussed (and explored with) in much more detail rather than being relegated to the penultimate paragraphs.", "rating": "5: Marginally below acceptance threshold", "reply_text": "AnonReviewer5 writes : > \u201c This paper attempts to make a hard distinction between the `` reinforcement learning '' objectives considered here and the ( non-RL ) objectives considered in the previous work . I do n't think this distinction is nearly as sharp as it 's made out to be. \u201d We think it is sharp , but perhaps we didn \u2019 t explain it clearly enough . In the previous paper , the policy was chosen using an omniscient labeler , such that acc % of the answers are _always_ correct . This was not learned , and was fixed to generate the datasets . This is a fake setting and in a real setting , you simply can not do that ( you do n't have access to an omniscient labeler ) . In a real setting you have to learn a policy completely from scratch , online , starting with a random policy . This is what we refer to as the online / RL setting , and the previous paper didn \u2019 t do it , it instead built a fixed dataset using the omniscient labeler . So , before this paper it just wasn \u2019 t clear if any of the introduced methods actually worked : ( 1 ) when you need to learn the policy ( which you always do ) ; and ( 2 ) using real language ( which crucially was n't tried before , only simulated text was used , here we used MTurk to fix that ) . These two points are crucial to the success of these methods in a realistic reinforcement learning setting . We thus believe this is an important contribution ."}, {"review_id": "HJgXCV9xx-1", "review_text": "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher\u2019s feedback to significantly improve performance. Overall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult. My main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston: \u201c(i) That earlier work did not use the natural reinforcement learning/online setting, but \u201ccheated\u201d with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.\u201d Point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that \u201cthe model also works if we collect the data online (i.e. the agent\u2019s policy is used to collect data rather than a fixed policy beforehand)\u201d. While this is a step in the right direction, I\u2019m not sure if it\u2019s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. EDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Good papers are n't always about the novelty of the algorithm . There is also the learning task , the investigation and the results . For example , we would n't have hundreds of papers about convolutional neural nets and recurrent neural nets if people did n't think there were things worth exploring beyond the original papers . This is only the first paper after Weston'16 . This paper tries to investigate whether you can learn in an online fashion with humans in the loop using their responses , not just rewards , which has never been done before . In retrospect , several findings may seem evident ; however , when we started this investigation we did not know the answer to the following questions : can one learn from textual feedback alone when training from scratch ( without an oracle policy ) ? What does it take to make methods working in the off-line setting perform well in the online one ? Does it matter to make training more off-policy ( increase the amount of data collection using the current policy ) ? Are these algorithm stable in practice ? Can our training algorithms adapt to the richness of natural language spoken by humans ? What 's a setting that can work with humans in the loop ? Our work provides a variety of simple and practical extensions for making the algorithms proposed by Weston NIPSP'16 work in a setting that matters for real applications . Of course , we hope that somebody will improve upon these results with entirely new algorithms in the future , but the analysis of the strong baseline methods we analyzed here is a required stepping stone . However , the results show this is a valuable and practical direction ."}, {"review_id": "HJgXCV9xx-2", "review_text": "As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting. several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success? The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision. 2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges. 3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching \u2026 or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal. 4. relation to prior work Weston\u201916 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston\u201916 - and not replacing it. In this case Weston\u201916 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn\u2019t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing. There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston'16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.", "rating": "7: Good paper, accept", "reply_text": "In regards to point 4 and your final point `` Comparison to prior work ( in particular Weston'16 ) , should be made more explicit '' ( and to the discussion with AnonReviewer5 , see that for details ) we have now updated the paper to include both detailed discussion in the related work section and comparison in the experiments section with regards to Weston '16 . We hope this issue and the contribution in general is now much more clear ."}], "0": {"review_id": "HJgXCV9xx-0", "review_text": " SUMMARY: This paper describes a set of experiments evaluating techniques for training a dialogue agent via reinforcement learning. A standard memory network architecture is trained on both bAbI and a version of the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous experiments are performed comparing the behavior of different training algorithms under various experimental conditions. STRENGTHS: The experimentation is comprehensive. I agree with the authors that these results provide additional useful insight into the performance of the model in the 2016 paper (henceforth W16). WEAKNESSES: This is essentially an appendix to the earlier paper. There is no new machine learning content. Secondarily, the paper seems to confuse the distinction between \"training with an adaptive sampling procedure\" and \"training in interactive environments\" more generally. In particular, no comparisons are presented to the to the experiments with a static exploration policy presented in W16, when the two training can & should be evaluated side-by-side. The only meaningful changes between this work and W16 involve simple (and already well-studied) changes to the form of this exploration policy. My primary concern remains about novelty: the extra data introduced here is welcome enough, but probably belongs in a *ACL short paper or a technical report. This work does not stand on its own, and an ICLR submission is not an appropriate vehicle for presenting it. \"REINFORCEMENT LEARNING\" [Update: concerns in this section have been addressed by the authors.] This paper attempts to make a hard distinction between the reinforcement learning condition considered here and the (\"non-RL\") condition considered in W16. I don't think this distinction is nearly as sharp as it's made out to be. As already noted in Weston 2016, the RBI objective is a special case of vanilla policy gradient with a zero baseline and off-policy samples. In this sense the version of RBI considered in this paper is the same as in W16, but with a different exploration policy; REINFORCE is the same objective with a nontrivial baseline. Similarly, the change in FP is only a change to the sampling policy. The fixed dataset / online learning distinction is not especially meaningful when the fixed dataset consists of endless synthetic data. It should be noted that some variants of the exploration policy in W16 provide a stronger training signal than is available in the RL \"from scratch\" setting here: in particular, when $\\pi_acc = 0.5$ the training samples will feature much denser reward. However, if I correctly understand Figures 3 and 4 in this paper, the completely random initial policy achieves an average reward of ~0.3 on bAbI and ~0.1 on movies---as good or better than the other exploration policies in W16! I think this paper would be a lot clearer if the delta from W16 were expressed directly in terms of their different exploration policies, rather than trying to cast all of the previous work as \"not RL\" when it can be straightforwardly accommodated in the RL framework. I was quite confused by the fact that no direct comparisons are made to the training conditions in the earlier work. I think this is a symptom of the problem discussed above: once this paper adopts the position that this work is about RL and the previous work is not, it becomes possible to declare that the two training scenarios are incomparable. I really think this is a mistake---to the extent that the off-policy sample generators used in the previous paper are worse than chance, it is always possible to compare to them fairly here. Evaluating everything in the \"online\" setting and presenting side-by-side experiments would provide a much more informative picture of the comparative behavior of the various training objectives. ON-POLICY VS OFF-POLICY Vanilla policy gradient methods like the ones here typically can't use off-policy samples without a little extra hand-holding (importance sampling, trust region methods, etc.). They seem to work out of the box for a few of the experiments in this paper, which is an interesting result on its own. It would be nice to have some discussion of why that might be the case. OTHER NOTES - The claim that \"batch size is related to off-policy learning\" is a little odd. There are lots of on-policy algorithms that require the agent to collect a large batch of transitions from the current policy before performing an (on-policy) update. - I think the experiments on fine-tuning to human workers are the most exciting part of this work, and I would have preferred to see these discussed (and explored with) in much more detail rather than being relegated to the penultimate paragraphs.", "rating": "5: Marginally below acceptance threshold", "reply_text": "AnonReviewer5 writes : > \u201c This paper attempts to make a hard distinction between the `` reinforcement learning '' objectives considered here and the ( non-RL ) objectives considered in the previous work . I do n't think this distinction is nearly as sharp as it 's made out to be. \u201d We think it is sharp , but perhaps we didn \u2019 t explain it clearly enough . In the previous paper , the policy was chosen using an omniscient labeler , such that acc % of the answers are _always_ correct . This was not learned , and was fixed to generate the datasets . This is a fake setting and in a real setting , you simply can not do that ( you do n't have access to an omniscient labeler ) . In a real setting you have to learn a policy completely from scratch , online , starting with a random policy . This is what we refer to as the online / RL setting , and the previous paper didn \u2019 t do it , it instead built a fixed dataset using the omniscient labeler . So , before this paper it just wasn \u2019 t clear if any of the introduced methods actually worked : ( 1 ) when you need to learn the policy ( which you always do ) ; and ( 2 ) using real language ( which crucially was n't tried before , only simulated text was used , here we used MTurk to fix that ) . These two points are crucial to the success of these methods in a realistic reinforcement learning setting . We thus believe this is an important contribution ."}, "1": {"review_id": "HJgXCV9xx-1", "review_text": "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher\u2019s feedback to significantly improve performance. Overall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult. My main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston: \u201c(i) That earlier work did not use the natural reinforcement learning/online setting, but \u201ccheated\u201d with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.\u201d Point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that \u201cthe model also works if we collect the data online (i.e. the agent\u2019s policy is used to collect data rather than a fixed policy beforehand)\u201d. While this is a step in the right direction, I\u2019m not sure if it\u2019s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. EDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Good papers are n't always about the novelty of the algorithm . There is also the learning task , the investigation and the results . For example , we would n't have hundreds of papers about convolutional neural nets and recurrent neural nets if people did n't think there were things worth exploring beyond the original papers . This is only the first paper after Weston'16 . This paper tries to investigate whether you can learn in an online fashion with humans in the loop using their responses , not just rewards , which has never been done before . In retrospect , several findings may seem evident ; however , when we started this investigation we did not know the answer to the following questions : can one learn from textual feedback alone when training from scratch ( without an oracle policy ) ? What does it take to make methods working in the off-line setting perform well in the online one ? Does it matter to make training more off-policy ( increase the amount of data collection using the current policy ) ? Are these algorithm stable in practice ? Can our training algorithms adapt to the richness of natural language spoken by humans ? What 's a setting that can work with humans in the loop ? Our work provides a variety of simple and practical extensions for making the algorithms proposed by Weston NIPSP'16 work in a setting that matters for real applications . Of course , we hope that somebody will improve upon these results with entirely new algorithms in the future , but the analysis of the strong baseline methods we analyzed here is a required stepping stone . However , the results show this is a valuable and practical direction ."}, "2": {"review_id": "HJgXCV9xx-2", "review_text": "As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting. several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success? The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision. 2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges. 3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching \u2026 or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal. 4. relation to prior work Weston\u201916 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston\u201916 - and not replacing it. In this case Weston\u201916 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn\u2019t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing. There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston'16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.", "rating": "7: Good paper, accept", "reply_text": "In regards to point 4 and your final point `` Comparison to prior work ( in particular Weston'16 ) , should be made more explicit '' ( and to the discussion with AnonReviewer5 , see that for details ) we have now updated the paper to include both detailed discussion in the related work section and comparison in the experiments section with regards to Weston '16 . We hope this issue and the contribution in general is now much more clear ."}}