{"year": "2020", "forum": "S1gyl6Vtvr", "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "decision": "Reject", "meta_review": "This paper presents a method to learn a pruned convolutional network during conventional training.  Pruning the network has advantages (in deployment) of reducing the final model size and reducing the required FLOPS for compute.  The method adds a pruning mask on each layer with an additional sparsity loss on the mask variables. The method avoids the cost of a train-prune-retrain optimization process that has been used in several earlier papers.  The method is evaluated on CIFAR-10 and ImageNet with three standard convolutional network architectures.  The results show comparable performance to the original networks with the learned sparse networks. \n\nThe reviewers made many substantial comments on the paper and most of these were addressed in the author response and subsequent discussion.  For example, Reviewer1 mentioned two other papers that promote sparsity implicitly during training (Q3), and the authors acknowledged the omission and described how those methods had less flexibility on a target metric (FLOPS) that is not parameter size.  Many of the author responses described changes to an updated paper that would clarify the claims and results (R1: Q2-7, R2:Q3).\n\nHowever, the reviewers raised many concerns for the original paper and they did not see an updated paper that contains the proposed revisions.  Given the numerous concerns with the original submission, the reviewers wanted to see the revised paper to assess whether their concerns had been addressed adequately. Additionally, the paper does not have a comparison experiment with state-of the art results, and the current results were not sufficiently convincing for the reviewers.  Reviewer1 and author response to questions 13--15 suggest that the experimental results with ResNet-34 are inadequate to show the benefits of the approach, but results for the proposed method with the larger ResNet-50 (which could show benefits) are not yet ready. \n\nThe current paper is not ready for publication.\n", "reviews": [{"review_id": "S1gyl6Vtvr-0", "review_text": "This paper proposes a framework for training time filter pruning for convolutional neural networks. The main idea is to use a trainable soft binary mask to zero out convolutional filters and corresponding batch norm parameters. Pros: + The proposed method seems relatively easy to implement. + The quantitative results in the paper indicate that MaskConvNet achieves performance competitive with previously proposed pruning methods. Cons: - Writing of the paper could be significantly improved. See some examples below. - The main thing that bothered me about the method was the usage of hard sigmoid. If a mask component ever gets into one of the flat regions it won\u2019t be able to escape. The authors propose a workaround which they call \u201cmask decay update\u201d. This approach looks quite hacky and I\u2019m not sure how easy it is to make it work in practice. Notes/questions: * Abstract: \u201celegant support\u201d -> \u201csupport\u201d * Everywhere in the text: Back-to-back citations should have the form (citation1; citation2; \u2026) * Section 1, paragraph 3: \u201csuffer one\u201d -> \u201csuffer from one\u201d * Section 1, paragraph 4: \u201cabove mentioned\u201d -> \u201cabove-mentioned\u201d * Figure 1: The figure would greatly benefit from a detailed description. What\u2019s IF, OF and OF? The reader shouldn\u2019t be guessing. * Section 2, paragraph 3: \u201ccorresponded\u201d -> \u201ccorresponding\u201d * Section 3.1, paragraph 2: \u201cW \\in R\u201d \u2013 W is probably not a scalar value therefore it\u2019s in R^n. The same goes for the mask. * Section 3.1, paragraph 2: \u201cIt\u2019s easy to know ...\u201d \u2013 this sentence needs to be rewritten, e.g., \u201cOne can see that \u2026\u201d * Section 3.1, paragraph 2: \u201csparser\u201d -> \u201cmore sparse\u201d * Section 3.2, \u201cExtension to Multi-metrics\u201d: \u201cFLOPs\u201d are never defined in the paper. How is this quantity computed exactly? I\u2019m also not entirely sure how useful it is to introduce multiple lambdas \u2013 it seems that this case corresponds to a single lambda which is a sum \\lambda_i. * Section 3.3, paragraph 1: \u201cundersparsed\u201d, \u201coversparsed\u201d \u2013 not sure if these words exist. Maybe rephrase instead of introducing new terms? * Section 3.3, paragraph 1: \u201cvery laborious\u201d -> \u201claborious\u201d * Figure 3: Why not show points all the way to 0 sparsity? * Section 4.2, CIFAR-10: The authors mention that (Lemaire et al., 19) achieve better FLOP sparsity due to usage of Knowledge Distillation. From this sentence alone it\u2019s not clear how exactly KD helps. Why can\u2019t KD be applied in the proposed framework? I\u2019d appreciate if the authors could elaborate on this. I must admit that I\u2019m not an expert in the field of NN pruning but I\u2019m surprised that training-time masking of filters has not been tried before. Even if it\u2019s really the case I\u2019m not entirely confident that the paper should be accepted: the \u201cunsparsification\u201d looks more like a hack than a principled approach and the overall quality of writing needs to be improved. I\u2019m giving a borderline score for now but I\u2019m willing to increase it provided that the rebuttal addresses my concerns.", "rating": "3: Weak Reject", "reply_text": "Thank you for reviewing our work . We try to correct all the grammatical mistakes and typos . Below we address your main concern . Q1 : The main thing that bothered me about the method was the usage of Hard Sigmoid . If a mask component ever gets into one of the flat regions it won \u2019 t be able to escape . The authors propose a workaround which they call \u201c mask decay update \u201d . This approach looks quite hacky and I \u2019 m not sure how easy it is to make it work in practice . Unlike heuristic pruning approaches such as magnitude-based pruning via thresholding , trainable pruning involves differentiating through 0 values during back-propagation , which is a long-standing difficult problem in the literature of network pruning . If learning mask as discrete binary values { 0,1 } , one can not get true gradients during back-propagation , but only estimates under Expectation assumptions . For example , the principled approaches like Variational Dropout [ 1 ] and L0-regularization [ 2 ] proposed continuous relaxation/re-parameterization to learn the activation probability of the mask . However , this method has `` sparse '' gradients issue as noted by L0-regularization . BAR [ 3 ] used the same re-parametrization trick to learn mask and reported difficulty in getting good accuracy with this technique unless using some tuned linearly decreasing regularization strength and knowledge distillation . For SSL [ 4 ] , MorphNet [ 5 ] and Network Slimming [ 6 ] , once a weight parameter is set to 0 during training , it is always 0 and can never be re-activated again . Other options include ( 1 ) using REINFORCE to learn the mask values , but this has high variance issue , or ( 2 ) using the Straight-Through Estimator to backprop through 0 ( i.e. , defining the backward pass \u201c gradient \u201d as other functions such as identity function ) , but this has biased gradients estimate issue , as referred to discussions in [ 2 ] . Instead of learning mask with discrete values , we proposed the soft-binarization approach \u2013 MaskConvNet , for differentiating through mostly continuous values on the Hard Sigmoid function , where the mask values are in the interval [ 0,1 ] instead of discrete { 0,1 } . Our method does not rely on costly and noisy sampling procedures , and not requiring regularization strength scheduling as in [ 3 ] . With soft-binarization , the model learns mask outputs with mostly continuous values . When the model is reaching the budget sparsity , the final pruned structure is also found . More importantly , the reverse mask weight-decay is a method to recover dead neurons if the pruning rate exceeds the pre-defined budget . Reverse weight-decay requires non-zero parameters to work , and this is the reason why we use hard-sigmoid : the actual mask parameters stay non-zero while the mask outputs are zero ( when mask parameters are smaller than -0.5 ) . In the appendix section , we show that this method is robust to hyper-parameter selections . Mask decay value is set to 1e-5 or less and EMA alpha=0.99 regardless of the network architectures and the datasets used . We agree that other methods have their appeal due to their theoretical grounding ( less `` hacky '' ) . However , we can see lots of `` hacks '' in regularization literature such as Dropout , Label Smoothing , Confidence Penalty , Cutout , Mixup , Knowledge Distillation , BatchNorm , etc . Some took years to be improved and be theoretically sound , e.g.Dropout - > Variational Dropout , MC Dropout , etc . And some still have no 100 % agreed consensus or analytically proved on why it works , e.g.Knowledge Distillation and BatchNorm . Likewise , we leave the investigation for the theoretical analysis of reverse weight-decay for future work . [ 1 ] Variational Dropout and the Local Reparameterization Trick https : //arxiv.org/pdf/1506.02557.pdf [ 2 ] Learning Sparse Neural Networks through L0 Regularization https : //arxiv.org/abs/1712.01312 [ 3 ] Structured Pruning of Neural Networks With Budget-Aware Regularization http : //openaccess.thecvf.com/content_CVPR_2019/html/Lemaire_Structured_Pruning_of_Neural_Networks_With_Budget-Aware_Regularization_CVPR_2019_paper.html [ 4 ] Learning Structured Sparsity in Deep Neural Networks https : //arxiv.org/abs/1608.03665 [ 5 ] Learning Efficient Convolutional Networks through Network Slimming https : //arxiv.org/abs/1708.06519 [ 6 ] MorphNet : Fast & Simple Resource-Constrained Structure Learning of Deep Networks https : //arxiv.org/abs/1711.06798"}, {"review_id": "S1gyl6Vtvr-1", "review_text": "In this work, the authors propose a network pruning method to learn a pruned network during training. Specifically, they add a pruning mask for each layer and induce a sparisity loss on the mask variables during training. The pruned network is obtained by applying the learned mask to the networks. The paper seems to be well contained. However, my assessment of this paper is weak reject. I am mainly concerned with the novelty of this method. Also i think some more evaluation is needed to fully understand the effectiveness of this method. My questions are summarized as follows: Q1: In the methods part, the authors said that \u201cPrevious pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged.\u201d Can the authors give some reference here as to which pruning approaches? Q2: Did the authors compare the proposed approach to training the pruned networks from scratch as done in [1]? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [1]? Q3: What is the difference of your approach to [2]? They seem to be very similar. I think it is necessary to add some discussion in the related work. Is there any experimental results for comparison with [2]? [1] Rethinking the Value of Network Pruning. Liu et al. ICLR 2019 [2] AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference. Luo et al. Arxiv, 2018.", "rating": "3: Weak Reject", "reply_text": "Thank you for reviewing our work . Q1 : In the methods part , the authors said that \u201c Previous pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged. \u201d Can the authors give some reference here as to which pruning approaches ? Most of the train-prune-retrain pruning approaches suffer this issue . For example , Network Slimming [ 3 ] and MorphNet [ 4 ] proposed to sparsify BN scale/shift parameters . However , even though the scale is exactly 0 , when they pruned the corresponding filter ( i.e.cut down output depth for current Conv layer and input depth for the next Conv layer ) , the accuracy of the pruned model drops and must be fine-tuned . According to the appendix section of MorphNet , the BN statistics are disrupted after pruning and must be corrected with several thousand iterations with a tiny learning rate . Our method masked the filters of Covn/FC layer and the corresponding BN layer with the same mask vector to condition BN to ignore the masked filters during training , thus avoid the requirement for finetuning . Q2 : Did the authors compare the proposed approach to training the pruned networks from scratch as done in [ 1 ] ? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [ 1 ] ? We added experiments to compare our method with the train-from-scratch method named Soft Filter Pruning ( SFP ) [ 5 ] with ResNet56 v1 on CIFAR10 , as also done in the paper \u201c Rethinking the value of network pruning \u201d [ 1 ] . SFP pruned all layers in ResNet56 with a pre-defined uniform pruning rate across layers . For a fair comparison , we trained the masked ResNet56 by masking all the layers in each basic block ( residual connection ) . Results show that our method and SFP ( without fine-tuning ) achieves 92.9 % and 93.1 % accuracy respectively , when both methods prune 30 % of the weight parameters . ( Note that our results on ResNet56 are a bit different from the numbers shown in Table 1 in our submission.For a fair comparison with L1-Pruning [ 6 ] , we trained ResNet56 without masking the last Conv layer in each basic block , as what L1-Pruning did in the experiments . ) We also show the sparsity patterns of the pruned networks for VGG19 ( pruned 90 % of the weight parameters , with accuracy 93.5 % on CIFAR10 ) and ResNet56 ( pruned 30 % of the parameters , with accuracy 92.9 % on CIFAR10 ) . One can see that the pattern emerged from our method is similar to the pattern observed in the paper \u201c Rethinking the value of network pruning \u201d , e.g.for VGG19 , late layers tend to have more redundancy than early layers , and for ResNet56 , the pruning ratio tends to be uniform across stages . ============================================================= ======== Layer-wise sparsity pattern for VGG19 with 90 % of weight parameters pruned . Layer # Filters ratio to be kept Conv 1 0.734 Conv 2 0.984 Conv 3 0.961 Conv 4 0.992 Conv 5 0.914 Conv 6 0.758 Conv 7 0.617 Conv 8 0.520 Conv 9 0.250 Conv 10 0.172 Conv 11 0.127 Conv 12 0.115 Conv 13 0.146 Conv 14 0.145 Conv 15 0.160 Conv 16 0.361 ============================================================= ======== ============================================================= ======== Stage-wise sparsity pattern for ResNet56 with 30 % of weight parameters pruned . Layer # Params ratio to be kept Stage 1 0.783 Stage 2 0.681 Stage 3 0.878 ============================================================= ======== Q3 : What is the difference of your approach to [ 2 ] ? They seem to be very similar . I think it is necessary to add some discussion in the related work . Is there any experimental results for comparison with [ 2 ] ? Thanks for pointing us the AutoPruner paper , we will add it to our reference . Basically , AutoPruner proposed to prune weight by directly masking activation responses , rather than weight filters . Since activation responses are not learnable parameters , they add a AutoPruner layer to the network architecture . The AutoPrunner layer takes activation responses as input and the output is an approximate binary code in which zero value indicates the corresponding filter will be pruned . The AutoPrunner layer composes of average pooling , max pooling , a fully-connected layer followed by a scaled sigmoid function , which introduces a large number of additional trainable parameters and computational complexity to the network . We would like to run AutoPruner experiments for comparisons , unfortunately , we didn \u2019 t find any source codes available on Github or the author \u2019 s website . We will probably implement the method by ourselves and add comparisons in the near future ."}, {"review_id": "S1gyl6Vtvr-2", "review_text": "This paper proposes a masking process to improve the pruning of DNN. In addition, the algorithm proposes to automatically allocate the pruning rates over layers. On the positive side: - I do believe the main contribution is automatically allocating the pruning rates over the network. Related works: - How does this relate to methods using gating to prune in the presence of residual layers or BN? - Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works. - I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper). Method: - The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron' within the layer. How is this really related? - Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right? - The claim that this method is 'much simpler' is a bit subjective. I do not see why. Please elaborate. - I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers). - I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture? - What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? - The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. - My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 before 3.4). - The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? Experiments: - There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. - Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. - In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model? - How are the actual groups made? Minor details: - Please improve figure 1. It is not easy to understand. Same with Figure 3. What is seen in Figure 4. - I do believe the WideResNet-28-10 number of parameters for BAR is not correct. - Section 4.2 is a bit overselling. I do not see 'much-higher' parameter sparsity. The claims are mostly valid for VGG type of networks in this particular setting. [1] Learning the Number of Neurons in Deep Networks, NeurIps 2016 [2] Compression-aware training of DNN, NeurIps 2017 ", "rating": "3: Weak Reject", "reply_text": "Thank you for reviewing our work . We answer your questions below : Q1 : How does this relate to methods using gating to prune in the presence of residual layers or BN ? Previous works prune weights/filters/layers via ( 1 ) L0/L1 regularization or variational dropout on weights/BN , or ( 2 ) simple binary thresholding defined over weight magnitudes / filters / BN norms . All these works can be treated as \u201c hard \u201d pruning through a gating function , where a hand-crafted threshold is required to make decision whether or not to prune . Thus , ( iterative ) fine-tuning is introduced to amortize the accuracy loss due to hard pruning . Our work differs from the prior art in that we perform \u201c soft \u201d pruning where a soft filter-wise mask with continuous values in [ 0,1 ] is automatically learned together with filter weight parameters , and filters are reparametrized by element-wise multiplications between filter mask and filter weights . Filters with mask value 0 are pruned away from the network architecture . Thus , fine-tuning after training is not required anymore , and also threshold is not required thanks to the use of Hard Sigmoid as the activation function for the mask layer . Q2 : Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works . Agree.We will update our draft to make the point more clearly . Q3 : I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training ( even not explicitly stated ) to maintain the zeros in the network . At least in [ 1,2 ] the training time is the same as the time used for training a network from scratch ( same claim as in this paper ) . Thanks for pointing us these 2 papers , we will add them to the reference . As mentioned in our draft , sparsity learning based pruning via L0/L1 regularization or variational dropout is similar to our work in the sense that they learn mask to prune the network . However , these works suffer from the problem that the sparsity is enforced for parameter size only , and not directly related to the target metric ( e.g.FLOPs , or energy ) . While the mask defined in our work supports metrics beyond parameter size seamlessly . Q4 : The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron ' within the layer . How is this really related ? In early training all mask parameters are initialized as 0 and fed into hard-sigmoid activation function , the output mask is 0.5 for all filters , meaning that all filters are with equal importance . Like filter weight parameters , these mask parameters will also be continuously updated with SGD optimizer during training , until the model converges and meets the sparsity budget . According to the hard-sigmoid function , the final output mask is in the range of [ 0,1 ] , important neurons will have mask output of 1 , and unimportant neurons will have mask output of 0 . Q5 : Why the initialization of the mask is to the mean ? I think I am confused there . For initialization , I would argue all the neurons/parameters are relevant , right ? To clarify , the mask after Hard Sigmoid is all initialized as 0.5 , indicating that all filters are with the equal importance ; we think this is reasonable as there is no prior knowledge on which filters are more important than the others at the beginning of training . We also investigated other initialization methods for the mask , such as all mask initialized as 1 or random Gaussian . Results are comparable to initialization with 0.5 . Another reason we initialize the mask with constant is we do n't want to disrupt He/Xavier initialization . Q6 : The claim that this method is 'much simpler ' is a bit subjective . I do not see why . Please elaborate . We admit it is a bit subjective and will remove it accordingly . The message we would like to deliver is that the proposed mask module is light-weight and the additional computational cost introduced during training can be ignored . Moreover , by using shared BN mask , we did not need to retrain or fine-tune the pruned model , unlike previous work such as MorphNet [ 1 ] and Network Slimming [ 2 ]"}], "0": {"review_id": "S1gyl6Vtvr-0", "review_text": "This paper proposes a framework for training time filter pruning for convolutional neural networks. The main idea is to use a trainable soft binary mask to zero out convolutional filters and corresponding batch norm parameters. Pros: + The proposed method seems relatively easy to implement. + The quantitative results in the paper indicate that MaskConvNet achieves performance competitive with previously proposed pruning methods. Cons: - Writing of the paper could be significantly improved. See some examples below. - The main thing that bothered me about the method was the usage of hard sigmoid. If a mask component ever gets into one of the flat regions it won\u2019t be able to escape. The authors propose a workaround which they call \u201cmask decay update\u201d. This approach looks quite hacky and I\u2019m not sure how easy it is to make it work in practice. Notes/questions: * Abstract: \u201celegant support\u201d -> \u201csupport\u201d * Everywhere in the text: Back-to-back citations should have the form (citation1; citation2; \u2026) * Section 1, paragraph 3: \u201csuffer one\u201d -> \u201csuffer from one\u201d * Section 1, paragraph 4: \u201cabove mentioned\u201d -> \u201cabove-mentioned\u201d * Figure 1: The figure would greatly benefit from a detailed description. What\u2019s IF, OF and OF? The reader shouldn\u2019t be guessing. * Section 2, paragraph 3: \u201ccorresponded\u201d -> \u201ccorresponding\u201d * Section 3.1, paragraph 2: \u201cW \\in R\u201d \u2013 W is probably not a scalar value therefore it\u2019s in R^n. The same goes for the mask. * Section 3.1, paragraph 2: \u201cIt\u2019s easy to know ...\u201d \u2013 this sentence needs to be rewritten, e.g., \u201cOne can see that \u2026\u201d * Section 3.1, paragraph 2: \u201csparser\u201d -> \u201cmore sparse\u201d * Section 3.2, \u201cExtension to Multi-metrics\u201d: \u201cFLOPs\u201d are never defined in the paper. How is this quantity computed exactly? I\u2019m also not entirely sure how useful it is to introduce multiple lambdas \u2013 it seems that this case corresponds to a single lambda which is a sum \\lambda_i. * Section 3.3, paragraph 1: \u201cundersparsed\u201d, \u201coversparsed\u201d \u2013 not sure if these words exist. Maybe rephrase instead of introducing new terms? * Section 3.3, paragraph 1: \u201cvery laborious\u201d -> \u201claborious\u201d * Figure 3: Why not show points all the way to 0 sparsity? * Section 4.2, CIFAR-10: The authors mention that (Lemaire et al., 19) achieve better FLOP sparsity due to usage of Knowledge Distillation. From this sentence alone it\u2019s not clear how exactly KD helps. Why can\u2019t KD be applied in the proposed framework? I\u2019d appreciate if the authors could elaborate on this. I must admit that I\u2019m not an expert in the field of NN pruning but I\u2019m surprised that training-time masking of filters has not been tried before. Even if it\u2019s really the case I\u2019m not entirely confident that the paper should be accepted: the \u201cunsparsification\u201d looks more like a hack than a principled approach and the overall quality of writing needs to be improved. I\u2019m giving a borderline score for now but I\u2019m willing to increase it provided that the rebuttal addresses my concerns.", "rating": "3: Weak Reject", "reply_text": "Thank you for reviewing our work . We try to correct all the grammatical mistakes and typos . Below we address your main concern . Q1 : The main thing that bothered me about the method was the usage of Hard Sigmoid . If a mask component ever gets into one of the flat regions it won \u2019 t be able to escape . The authors propose a workaround which they call \u201c mask decay update \u201d . This approach looks quite hacky and I \u2019 m not sure how easy it is to make it work in practice . Unlike heuristic pruning approaches such as magnitude-based pruning via thresholding , trainable pruning involves differentiating through 0 values during back-propagation , which is a long-standing difficult problem in the literature of network pruning . If learning mask as discrete binary values { 0,1 } , one can not get true gradients during back-propagation , but only estimates under Expectation assumptions . For example , the principled approaches like Variational Dropout [ 1 ] and L0-regularization [ 2 ] proposed continuous relaxation/re-parameterization to learn the activation probability of the mask . However , this method has `` sparse '' gradients issue as noted by L0-regularization . BAR [ 3 ] used the same re-parametrization trick to learn mask and reported difficulty in getting good accuracy with this technique unless using some tuned linearly decreasing regularization strength and knowledge distillation . For SSL [ 4 ] , MorphNet [ 5 ] and Network Slimming [ 6 ] , once a weight parameter is set to 0 during training , it is always 0 and can never be re-activated again . Other options include ( 1 ) using REINFORCE to learn the mask values , but this has high variance issue , or ( 2 ) using the Straight-Through Estimator to backprop through 0 ( i.e. , defining the backward pass \u201c gradient \u201d as other functions such as identity function ) , but this has biased gradients estimate issue , as referred to discussions in [ 2 ] . Instead of learning mask with discrete values , we proposed the soft-binarization approach \u2013 MaskConvNet , for differentiating through mostly continuous values on the Hard Sigmoid function , where the mask values are in the interval [ 0,1 ] instead of discrete { 0,1 } . Our method does not rely on costly and noisy sampling procedures , and not requiring regularization strength scheduling as in [ 3 ] . With soft-binarization , the model learns mask outputs with mostly continuous values . When the model is reaching the budget sparsity , the final pruned structure is also found . More importantly , the reverse mask weight-decay is a method to recover dead neurons if the pruning rate exceeds the pre-defined budget . Reverse weight-decay requires non-zero parameters to work , and this is the reason why we use hard-sigmoid : the actual mask parameters stay non-zero while the mask outputs are zero ( when mask parameters are smaller than -0.5 ) . In the appendix section , we show that this method is robust to hyper-parameter selections . Mask decay value is set to 1e-5 or less and EMA alpha=0.99 regardless of the network architectures and the datasets used . We agree that other methods have their appeal due to their theoretical grounding ( less `` hacky '' ) . However , we can see lots of `` hacks '' in regularization literature such as Dropout , Label Smoothing , Confidence Penalty , Cutout , Mixup , Knowledge Distillation , BatchNorm , etc . Some took years to be improved and be theoretically sound , e.g.Dropout - > Variational Dropout , MC Dropout , etc . And some still have no 100 % agreed consensus or analytically proved on why it works , e.g.Knowledge Distillation and BatchNorm . Likewise , we leave the investigation for the theoretical analysis of reverse weight-decay for future work . [ 1 ] Variational Dropout and the Local Reparameterization Trick https : //arxiv.org/pdf/1506.02557.pdf [ 2 ] Learning Sparse Neural Networks through L0 Regularization https : //arxiv.org/abs/1712.01312 [ 3 ] Structured Pruning of Neural Networks With Budget-Aware Regularization http : //openaccess.thecvf.com/content_CVPR_2019/html/Lemaire_Structured_Pruning_of_Neural_Networks_With_Budget-Aware_Regularization_CVPR_2019_paper.html [ 4 ] Learning Structured Sparsity in Deep Neural Networks https : //arxiv.org/abs/1608.03665 [ 5 ] Learning Efficient Convolutional Networks through Network Slimming https : //arxiv.org/abs/1708.06519 [ 6 ] MorphNet : Fast & Simple Resource-Constrained Structure Learning of Deep Networks https : //arxiv.org/abs/1711.06798"}, "1": {"review_id": "S1gyl6Vtvr-1", "review_text": "In this work, the authors propose a network pruning method to learn a pruned network during training. Specifically, they add a pruning mask for each layer and induce a sparisity loss on the mask variables during training. The pruned network is obtained by applying the learned mask to the networks. The paper seems to be well contained. However, my assessment of this paper is weak reject. I am mainly concerned with the novelty of this method. Also i think some more evaluation is needed to fully understand the effectiveness of this method. My questions are summarized as follows: Q1: In the methods part, the authors said that \u201cPrevious pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged.\u201d Can the authors give some reference here as to which pruning approaches? Q2: Did the authors compare the proposed approach to training the pruned networks from scratch as done in [1]? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [1]? Q3: What is the difference of your approach to [2]? They seem to be very similar. I think it is necessary to add some discussion in the related work. Is there any experimental results for comparison with [2]? [1] Rethinking the Value of Network Pruning. Liu et al. ICLR 2019 [2] AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference. Luo et al. Arxiv, 2018.", "rating": "3: Weak Reject", "reply_text": "Thank you for reviewing our work . Q1 : In the methods part , the authors said that \u201c Previous pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged. \u201d Can the authors give some reference here as to which pruning approaches ? Most of the train-prune-retrain pruning approaches suffer this issue . For example , Network Slimming [ 3 ] and MorphNet [ 4 ] proposed to sparsify BN scale/shift parameters . However , even though the scale is exactly 0 , when they pruned the corresponding filter ( i.e.cut down output depth for current Conv layer and input depth for the next Conv layer ) , the accuracy of the pruned model drops and must be fine-tuned . According to the appendix section of MorphNet , the BN statistics are disrupted after pruning and must be corrected with several thousand iterations with a tiny learning rate . Our method masked the filters of Covn/FC layer and the corresponding BN layer with the same mask vector to condition BN to ignore the masked filters during training , thus avoid the requirement for finetuning . Q2 : Did the authors compare the proposed approach to training the pruned networks from scratch as done in [ 1 ] ? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [ 1 ] ? We added experiments to compare our method with the train-from-scratch method named Soft Filter Pruning ( SFP ) [ 5 ] with ResNet56 v1 on CIFAR10 , as also done in the paper \u201c Rethinking the value of network pruning \u201d [ 1 ] . SFP pruned all layers in ResNet56 with a pre-defined uniform pruning rate across layers . For a fair comparison , we trained the masked ResNet56 by masking all the layers in each basic block ( residual connection ) . Results show that our method and SFP ( without fine-tuning ) achieves 92.9 % and 93.1 % accuracy respectively , when both methods prune 30 % of the weight parameters . ( Note that our results on ResNet56 are a bit different from the numbers shown in Table 1 in our submission.For a fair comparison with L1-Pruning [ 6 ] , we trained ResNet56 without masking the last Conv layer in each basic block , as what L1-Pruning did in the experiments . ) We also show the sparsity patterns of the pruned networks for VGG19 ( pruned 90 % of the weight parameters , with accuracy 93.5 % on CIFAR10 ) and ResNet56 ( pruned 30 % of the parameters , with accuracy 92.9 % on CIFAR10 ) . One can see that the pattern emerged from our method is similar to the pattern observed in the paper \u201c Rethinking the value of network pruning \u201d , e.g.for VGG19 , late layers tend to have more redundancy than early layers , and for ResNet56 , the pruning ratio tends to be uniform across stages . ============================================================= ======== Layer-wise sparsity pattern for VGG19 with 90 % of weight parameters pruned . Layer # Filters ratio to be kept Conv 1 0.734 Conv 2 0.984 Conv 3 0.961 Conv 4 0.992 Conv 5 0.914 Conv 6 0.758 Conv 7 0.617 Conv 8 0.520 Conv 9 0.250 Conv 10 0.172 Conv 11 0.127 Conv 12 0.115 Conv 13 0.146 Conv 14 0.145 Conv 15 0.160 Conv 16 0.361 ============================================================= ======== ============================================================= ======== Stage-wise sparsity pattern for ResNet56 with 30 % of weight parameters pruned . Layer # Params ratio to be kept Stage 1 0.783 Stage 2 0.681 Stage 3 0.878 ============================================================= ======== Q3 : What is the difference of your approach to [ 2 ] ? They seem to be very similar . I think it is necessary to add some discussion in the related work . Is there any experimental results for comparison with [ 2 ] ? Thanks for pointing us the AutoPruner paper , we will add it to our reference . Basically , AutoPruner proposed to prune weight by directly masking activation responses , rather than weight filters . Since activation responses are not learnable parameters , they add a AutoPruner layer to the network architecture . The AutoPrunner layer takes activation responses as input and the output is an approximate binary code in which zero value indicates the corresponding filter will be pruned . The AutoPrunner layer composes of average pooling , max pooling , a fully-connected layer followed by a scaled sigmoid function , which introduces a large number of additional trainable parameters and computational complexity to the network . We would like to run AutoPruner experiments for comparisons , unfortunately , we didn \u2019 t find any source codes available on Github or the author \u2019 s website . We will probably implement the method by ourselves and add comparisons in the near future ."}, "2": {"review_id": "S1gyl6Vtvr-2", "review_text": "This paper proposes a masking process to improve the pruning of DNN. In addition, the algorithm proposes to automatically allocate the pruning rates over layers. On the positive side: - I do believe the main contribution is automatically allocating the pruning rates over the network. Related works: - How does this relate to methods using gating to prune in the presence of residual layers or BN? - Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works. - I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper). Method: - The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron' within the layer. How is this really related? - Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right? - The claim that this method is 'much simpler' is a bit subjective. I do not see why. Please elaborate. - I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers). - I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture? - What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? - The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. - My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 before 3.4). - The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? Experiments: - There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. - Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. - In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model? - How are the actual groups made? Minor details: - Please improve figure 1. It is not easy to understand. Same with Figure 3. What is seen in Figure 4. - I do believe the WideResNet-28-10 number of parameters for BAR is not correct. - Section 4.2 is a bit overselling. I do not see 'much-higher' parameter sparsity. The claims are mostly valid for VGG type of networks in this particular setting. [1] Learning the Number of Neurons in Deep Networks, NeurIps 2016 [2] Compression-aware training of DNN, NeurIps 2017 ", "rating": "3: Weak Reject", "reply_text": "Thank you for reviewing our work . We answer your questions below : Q1 : How does this relate to methods using gating to prune in the presence of residual layers or BN ? Previous works prune weights/filters/layers via ( 1 ) L0/L1 regularization or variational dropout on weights/BN , or ( 2 ) simple binary thresholding defined over weight magnitudes / filters / BN norms . All these works can be treated as \u201c hard \u201d pruning through a gating function , where a hand-crafted threshold is required to make decision whether or not to prune . Thus , ( iterative ) fine-tuning is introduced to amortize the accuracy loss due to hard pruning . Our work differs from the prior art in that we perform \u201c soft \u201d pruning where a soft filter-wise mask with continuous values in [ 0,1 ] is automatically learned together with filter weight parameters , and filters are reparametrized by element-wise multiplications between filter mask and filter weights . Filters with mask value 0 are pruned away from the network architecture . Thus , fine-tuning after training is not required anymore , and also threshold is not required thanks to the use of Hard Sigmoid as the activation function for the mask layer . Q2 : Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works . Agree.We will update our draft to make the point more clearly . Q3 : I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training ( even not explicitly stated ) to maintain the zeros in the network . At least in [ 1,2 ] the training time is the same as the time used for training a network from scratch ( same claim as in this paper ) . Thanks for pointing us these 2 papers , we will add them to the reference . As mentioned in our draft , sparsity learning based pruning via L0/L1 regularization or variational dropout is similar to our work in the sense that they learn mask to prune the network . However , these works suffer from the problem that the sparsity is enforced for parameter size only , and not directly related to the target metric ( e.g.FLOPs , or energy ) . While the mask defined in our work supports metrics beyond parameter size seamlessly . Q4 : The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron ' within the layer . How is this really related ? In early training all mask parameters are initialized as 0 and fed into hard-sigmoid activation function , the output mask is 0.5 for all filters , meaning that all filters are with equal importance . Like filter weight parameters , these mask parameters will also be continuously updated with SGD optimizer during training , until the model converges and meets the sparsity budget . According to the hard-sigmoid function , the final output mask is in the range of [ 0,1 ] , important neurons will have mask output of 1 , and unimportant neurons will have mask output of 0 . Q5 : Why the initialization of the mask is to the mean ? I think I am confused there . For initialization , I would argue all the neurons/parameters are relevant , right ? To clarify , the mask after Hard Sigmoid is all initialized as 0.5 , indicating that all filters are with the equal importance ; we think this is reasonable as there is no prior knowledge on which filters are more important than the others at the beginning of training . We also investigated other initialization methods for the mask , such as all mask initialized as 1 or random Gaussian . Results are comparable to initialization with 0.5 . Another reason we initialize the mask with constant is we do n't want to disrupt He/Xavier initialization . Q6 : The claim that this method is 'much simpler ' is a bit subjective . I do not see why . Please elaborate . We admit it is a bit subjective and will remove it accordingly . The message we would like to deliver is that the proposed mask module is light-weight and the additional computational cost introduced during training can be ignored . Moreover , by using shared BN mask , we did not need to retrain or fine-tune the pruned model , unlike previous work such as MorphNet [ 1 ] and Network Slimming [ 2 ]"}}