{"year": "2018", "forum": "HJDV5YxCW", "title": "Heterogeneous Bitwidth Binarization in Convolutional Neural Networks", "decision": "Reject", "meta_review": "All of the reviewers find the approach interesting, but they have reservations regarding the practical impact and empirical evaluation. The paper needs improvement both on the motivation and on the experimental results by including more baseline methods and neural architectures.\n", "reviews": [{"review_id": "HJDV5YxCW-0", "review_text": "This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase. Though this is an important direction to investigate, there are several issues: 1. Comparison with previous results is misleading: a. 1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%. b. Hubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper. Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit. Therefore, it is not clear that the proposed methods improve over previous approaches. 2. It is not clear to me: in which dimension of the tensors are we saving the scale factor? If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass? 3. The review of the literature is inaccurate. For example, it is not true that Courbariaux et al. (2016) \u201cfurther improved accuracy on small datasets\u201d: the main novelty there was binarizing the activations (which typically decreased the accuracy). Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \"a significant improvement over previous work\" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors). Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases. For example, see Miyashita, Lee and Murmann 2016, and Hubara et al. %%% After Author's Clarification %%% This paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer 1 points out flaws in comparisons with related work . [ Rastegari shows 56.8 % accuracy , we only show 55.2 % accuracy ( row 4 of Table 1 ) ] We measure the result of binarizing * all * layers of Alexnet ( similar to Dong et al , from rows 1 and 2 ) . Rastegari et al * do not binarize the first or last layer * . We consider Dong 's result to be more challenging and chose to compare to it . However , we are happy to also compare to Rastegari 's configuration if reviewers think it is misleading not to . Or we can just make this difference ( currently noted in section 4.3 ) explicit in the table . [ Hubara got 51 % with 1-bit weights/2-bit activations when binarizing all layers , whereas we got 51.5 % without binarizing first and last layer ] We admit that this partly slipped by us . We chose our binarization configuration to compare to the many other pieces of work in Table 1 , none of which ( to our understanding ) binarize first and last layers . Please note however , that Hubara uses 8-bit binarization for the first layer , which is arguably closer to `` no binarization '' than to the conventional 1- and 2-bit binarization . Further , other work ( e.g.Tang et al AAAI 2017 ) has shown that binarizing the last layer , unlike the first , does not result in much accuracy loss . But we are happy to report Hubara 's configuration if reviewers deem this as misleading . [ Are scale factors stored per feature map or neuron ? ] No , they are stored per kernel exactly as in Rastegari et al.Scale factor multiplication can be done after the binary product by multiplying each output feature map by it 's corresponding scalar . The amount of added work is equivalent to replacing ReLU activations with PReLU activations , which does not have a significant effect on network inference time . We can make this point more explicit in the Implementation section . [ Misunderstandings of key innovations and contributions in related work ] We are embarrassed at our misunderstanding of the literature . We thank the reviewer and will correct these and improve our understanding ."}, {"review_id": "HJDV5YxCW-1", "review_text": "The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights. 1. The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them. 2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet. Indeed, the authors have commented: \"AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.\" So, please show that. 3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy? My concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016]. 4. Is trade-off between 1 to 2 bits really important? Compared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). Is such improvement really important? Reference: [a]. Trained Ternary Quantization. ICLR 2017 [b]. Extremely low bit neural network: Squeeze the last bit out with ADMM. arvix 2017", "rating": "5: Marginally below acceptance threshold", "reply_text": "The reviewer questions whether the performance improvements we claim are significant . In detail : [ Comparison to related work ] We thank the reviewer for these references . Reference A , on ternary networks , is quite similar to the work of Li et al that we reference in related work , but we will include further discussion . Reference B suggests training improvements ( not binarization techniques ) , which we will look to adopt . [ Evaluation in larger models : please show results on Resnet/GoogleNet ] We selected AlexNet to illustrate most of the work , since it has been used exclusively in the community to compare approaches . However , do note that * * unlike any paper so far * * , we have shown results also on Mobilenet , which is the state-of-the art object recognition model as of fall 2017 , from Google ( contribution 4 in the intro , and last paragraph of section 4.3 ) . Mobilenet yields comparable accuracy to Resnet and GoogleNet , but is also much faster than them , and is therefore a challenging benchmark . Perhaps we should highlight this result better ? [ Complexity of implementation ] Gaining performance from binarized models is indeed complex , especially on a CPU . In fact , no paper provides the many crucial details , such as machine specific vectorization/tiling/loop fusion algorithms essential to gaining real-world speedup . Even Courbariaux et al , section 4 , only gives a sketch in this direction . Admittedly , heterogeneous bitwidths will add to this complexity , so this is a fair concern on a CPU . However , implementation is fairly straightforward on an FPGA , because we simply lay out custom gate patterns for each bit pattern to be XNOR 'd against ( see e.g.https : //arxiv.org/pdf/1612.07119.pdf , esp . section 4.3.2 for a similar implementation in the homogeneous bitwidth case ) . The custom pattern for processing 2 bits is only slightly different from the 1-bit version . Perhaps we can focus the implementation section on sketch how to perturb this standard FPGA-based design ? [ No speed vs accuracy number ] As mentioned above , almost no paper in this area reports measured speedups , just improvements in coarsely estimated instruction counts . In the FPGA context , we could similarly report coarse estimates the number of cycles , chip real estate and power consumed . However , roughly speaking these ( especially the latter two that are our goal ) are simply proportional to average bitwidth of the operations programmable into hardware . We could make this explicit in the text when we discuss the implementation above . [ A 1.4/2x = 0.7x reduction is not significant ] Although this is a subjective call and hard to argue against , it is worth noting that our gains are * on top of * optimized binary implementations . Further , note that FPGA implementations of DNNs are now running at cloud scale e.g.in the Azure cloud ( https : //www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/ ) . A 30 % improvement in space/energy efficiency with no accuracy loss is considered quite significant at these scales ."}, {"review_id": "HJDV5YxCW-2", "review_text": "This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory). The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension. The description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages. Experiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain. Results are promising. Overall, I am leaning towards a rejection mostly due to limited novelty. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer describes our work as seeking to use different bit rates for different layers , and points out that the work is not novel enough overall . We would like to point out that in fact , we are not looking to simply binarize different layers at different bitwidths ( although as a baseline we do so in figure 3 ( a ) ) . In that baseline , assuming we select * up front * what the bitwidth k of each layer is , we use the not-so-novel approach of simply applying standard k-bit binarization algorithms with different k to each layer . This is indeed simple to do , but figure 3 ( a ) shows that such naive selection only provides `` linear '' increase in accuracy ( e.g. , using 1.5 bits on average gives only the average of 1-bit and 2-bit accuracies , which is interesting but perhaps not surprising ) . Instead , in our main contribution , we are asking the question `` if we * learned * what bitwidth to assign to * each * parameter ( jointly with its value ) , could we get better-than-linear speedup '' . This learning of bitwidths is what is novel about our goal , and techniques . Learning bitwidths requires changing the training algorithm in a non-obvious way , using the mask-generation scheme of algorithm 1 , and the middle-out scheme for thresholding . We should emphasize that the operations of equation 4 , Algorithm 1 and equation 6 are not performed in a one-time `` post-processing '' step , but on every forward propagation during training . We submit that this learning algorithm is quite novel . Given that the bitwidth is such a fundamental aspect of model parameters , we hope that learning them jointly with values should be of broad interest to the ICLR community ."}], "0": {"review_id": "HJDV5YxCW-0", "review_text": "This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase. Though this is an important direction to investigate, there are several issues: 1. Comparison with previous results is misleading: a. 1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%. b. Hubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper. Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit. Therefore, it is not clear that the proposed methods improve over previous approaches. 2. It is not clear to me: in which dimension of the tensors are we saving the scale factor? If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass? 3. The review of the literature is inaccurate. For example, it is not true that Courbariaux et al. (2016) \u201cfurther improved accuracy on small datasets\u201d: the main novelty there was binarizing the activations (which typically decreased the accuracy). Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \"a significant improvement over previous work\" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors). Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases. For example, see Miyashita, Lee and Murmann 2016, and Hubara et al. %%% After Author's Clarification %%% This paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer 1 points out flaws in comparisons with related work . [ Rastegari shows 56.8 % accuracy , we only show 55.2 % accuracy ( row 4 of Table 1 ) ] We measure the result of binarizing * all * layers of Alexnet ( similar to Dong et al , from rows 1 and 2 ) . Rastegari et al * do not binarize the first or last layer * . We consider Dong 's result to be more challenging and chose to compare to it . However , we are happy to also compare to Rastegari 's configuration if reviewers think it is misleading not to . Or we can just make this difference ( currently noted in section 4.3 ) explicit in the table . [ Hubara got 51 % with 1-bit weights/2-bit activations when binarizing all layers , whereas we got 51.5 % without binarizing first and last layer ] We admit that this partly slipped by us . We chose our binarization configuration to compare to the many other pieces of work in Table 1 , none of which ( to our understanding ) binarize first and last layers . Please note however , that Hubara uses 8-bit binarization for the first layer , which is arguably closer to `` no binarization '' than to the conventional 1- and 2-bit binarization . Further , other work ( e.g.Tang et al AAAI 2017 ) has shown that binarizing the last layer , unlike the first , does not result in much accuracy loss . But we are happy to report Hubara 's configuration if reviewers deem this as misleading . [ Are scale factors stored per feature map or neuron ? ] No , they are stored per kernel exactly as in Rastegari et al.Scale factor multiplication can be done after the binary product by multiplying each output feature map by it 's corresponding scalar . The amount of added work is equivalent to replacing ReLU activations with PReLU activations , which does not have a significant effect on network inference time . We can make this point more explicit in the Implementation section . [ Misunderstandings of key innovations and contributions in related work ] We are embarrassed at our misunderstanding of the literature . We thank the reviewer and will correct these and improve our understanding ."}, "1": {"review_id": "HJDV5YxCW-1", "review_text": "The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights. 1. The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them. 2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet. Indeed, the authors have commented: \"AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.\" So, please show that. 3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy? My concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016]. 4. Is trade-off between 1 to 2 bits really important? Compared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). Is such improvement really important? Reference: [a]. Trained Ternary Quantization. ICLR 2017 [b]. Extremely low bit neural network: Squeeze the last bit out with ADMM. arvix 2017", "rating": "5: Marginally below acceptance threshold", "reply_text": "The reviewer questions whether the performance improvements we claim are significant . In detail : [ Comparison to related work ] We thank the reviewer for these references . Reference A , on ternary networks , is quite similar to the work of Li et al that we reference in related work , but we will include further discussion . Reference B suggests training improvements ( not binarization techniques ) , which we will look to adopt . [ Evaluation in larger models : please show results on Resnet/GoogleNet ] We selected AlexNet to illustrate most of the work , since it has been used exclusively in the community to compare approaches . However , do note that * * unlike any paper so far * * , we have shown results also on Mobilenet , which is the state-of-the art object recognition model as of fall 2017 , from Google ( contribution 4 in the intro , and last paragraph of section 4.3 ) . Mobilenet yields comparable accuracy to Resnet and GoogleNet , but is also much faster than them , and is therefore a challenging benchmark . Perhaps we should highlight this result better ? [ Complexity of implementation ] Gaining performance from binarized models is indeed complex , especially on a CPU . In fact , no paper provides the many crucial details , such as machine specific vectorization/tiling/loop fusion algorithms essential to gaining real-world speedup . Even Courbariaux et al , section 4 , only gives a sketch in this direction . Admittedly , heterogeneous bitwidths will add to this complexity , so this is a fair concern on a CPU . However , implementation is fairly straightforward on an FPGA , because we simply lay out custom gate patterns for each bit pattern to be XNOR 'd against ( see e.g.https : //arxiv.org/pdf/1612.07119.pdf , esp . section 4.3.2 for a similar implementation in the homogeneous bitwidth case ) . The custom pattern for processing 2 bits is only slightly different from the 1-bit version . Perhaps we can focus the implementation section on sketch how to perturb this standard FPGA-based design ? [ No speed vs accuracy number ] As mentioned above , almost no paper in this area reports measured speedups , just improvements in coarsely estimated instruction counts . In the FPGA context , we could similarly report coarse estimates the number of cycles , chip real estate and power consumed . However , roughly speaking these ( especially the latter two that are our goal ) are simply proportional to average bitwidth of the operations programmable into hardware . We could make this explicit in the text when we discuss the implementation above . [ A 1.4/2x = 0.7x reduction is not significant ] Although this is a subjective call and hard to argue against , it is worth noting that our gains are * on top of * optimized binary implementations . Further , note that FPGA implementations of DNNs are now running at cloud scale e.g.in the Azure cloud ( https : //www.microsoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/ ) . A 30 % improvement in space/energy efficiency with no accuracy loss is considered quite significant at these scales ."}, "2": {"review_id": "HJDV5YxCW-2", "review_text": "This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory). The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension. The description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages. Experiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain. Results are promising. Overall, I am leaning towards a rejection mostly due to limited novelty. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer describes our work as seeking to use different bit rates for different layers , and points out that the work is not novel enough overall . We would like to point out that in fact , we are not looking to simply binarize different layers at different bitwidths ( although as a baseline we do so in figure 3 ( a ) ) . In that baseline , assuming we select * up front * what the bitwidth k of each layer is , we use the not-so-novel approach of simply applying standard k-bit binarization algorithms with different k to each layer . This is indeed simple to do , but figure 3 ( a ) shows that such naive selection only provides `` linear '' increase in accuracy ( e.g. , using 1.5 bits on average gives only the average of 1-bit and 2-bit accuracies , which is interesting but perhaps not surprising ) . Instead , in our main contribution , we are asking the question `` if we * learned * what bitwidth to assign to * each * parameter ( jointly with its value ) , could we get better-than-linear speedup '' . This learning of bitwidths is what is novel about our goal , and techniques . Learning bitwidths requires changing the training algorithm in a non-obvious way , using the mask-generation scheme of algorithm 1 , and the middle-out scheme for thresholding . We should emphasize that the operations of equation 4 , Algorithm 1 and equation 6 are not performed in a one-time `` post-processing '' step , but on every forward propagation during training . We submit that this learning algorithm is quite novel . Given that the bitwidth is such a fundamental aspect of model parameters , we hope that learning them jointly with values should be of broad interest to the ICLR community ."}}