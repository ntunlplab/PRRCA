{"year": "2019", "forum": "HygTE309t7", "title": "Outlier Detection from Image Data", "decision": "Reject", "meta_review": "The paper proposes a decision forest based method for outlier detection.\n\nThe reviewers and AC note the improvement over the existing method is incremental.\n\nAlthough  the problem is of significant practical importance, AC decided that the authors should do more works to attract the attention of a broader range of ICLR audience.", "reviews": [{"review_id": "HygTE309t7-0", "review_text": "Summary: This paper modifies an existing technique designed for image classification to make it applicable to outlier detection. Strengths: The outlined problem is of significant practical importance. Weaknesses: - The improvement over the existing method is incremental; - The regularization on routing decision may not really be necessary as, in DNDF, the soft splits start as uniform and gradually converge to something close to hard splits; this is discussed in the supplementary material of the DNDF paper; - the datasets tested are standard image datasets, not even captured from vehicles or video surveillance. The SVHN (street view numbers) dataset is the closest the experiments get to the motivating application. Overall assessment: reject Recommendations for the authors: Test on a surveillance or street view benchmark. Even then, it's questionable whether the paper is suitable for ICLR due to lack of methodological novelty. Note: I'd like to apologize to the authors for the delay in submitting this review. It was due to a technical error on my part (I thought the reviews had posted, but they had not). In the spirit of independent evaluation, this review was not influenced by the other comments on this paper. I will follow-up with a response which will take into account the existing dialogue.", "rating": "4: Ok but not good enough - rejection", "reply_text": "[ COMMMENT FROM REVIEWER ] : The improvement over the existing method ( DNDF ) is incremental RESPONSE : Our approach represents an effective solution to an extremely important problem . In fact , our approach significantly outperforms the state-of-the-art in the accuracy of outlier detection as shown in our experimental study . While our approach leverages some of the DNDF principles , we introduce several critical insights to render it effective at detecting outliers . First , given that the DNDF approach was designed for improving classification accuracy , it was not obvious it would be applicable to tackling the outlier detection problem . Indeed , we are the first to leverage DNDF for addressing the outlier detection problem . We do this by unifying the best practice of unsupervised outlier detection with our observation that the max route of each tree in DNDF effectively captures the outlierness of each image . Second , we refine the core method with several technical innovations to assure effective outlier detection , while concurrently also yielding high accuracy for image classification . By this , the existing CNN-based image classifiers are enhanced to have the ability to reject a testing image as being an outlier if it does not `` sufficiently '' belong to any of the existing classes known in the training data . In particular , we proposed two new techniques , namely , an information-theoretic regularization strategy based on routing decisions and a new network architecture that ensures that each tree in the forest is completely independent . Further , as additional innovation , we designed a new joint learning method that optimizes the parameters for the decision node and for the prediction nodes in one integrated step through back-propagation , abandoning the two-step optimization strategy used in DNDF . Based on the above described innovations , our approach is not only technically novel but also useful , as it is highly effective at detecting outliers . We have revised the Proposed Approach and Contributions of the Introduction section ( Section 1 ) to reflect the above discussion . [ COMMMENT FROM REVIEWER ] : The regularization on routing decision may not really be necessary as , in DNDF , the soft splits start as uniform and gradually converge to something close to hard splits . OUR RESPONSE : Indeed , we have had explored this question . Namely , although the original DNDF method does start from uniform soft splits and gradually converges to close to hard splits , directly using the max-route probability in DNDF to detect outliers does not achieve very high accuracy as we have demonstrated in our experimental study ( Table 1 ) . Our proposed regularization strategy based on routing decisions , namely penalizing large entropy probability distributions of the routing decision , significantly outperforms this original DNDF design for the problem of detecting outliers . As illustrated in Table 1 of our experimental section , using our regularization method , the accuracy of detecting CIFAR-100 outliers out of the CIFAR-10 dataset increases from 84.64 % to 94.69 % , that is , we gain an over 10 % improvement . Further , the accuracy of detecting MNIST outliers increases from 67.59 % to 94.94 % representing a close to 30 % improvement in accuracy . These experimental findings on benchmark data sets provide strong evidence that our proposed design , in particular with the addition of the proposed regularization strategy , is both necessary and highly effective . [ COMMMENT FROM REVIEWER ] : The datasets tested are standard image datasets , not even captured from vehicles or video surveillance . OUR RESPONSE : The datasets we used in our experiments are indeed commonly used in state-of-the-art image outlier detection papers such as [ 1 ] and [ 2 ] . In our experiments we focused on these datasets because this ensures a fair comparison of our proposed outlier detection approach against the state-of-the-art . In the future , we will be happy to extend our evaluation to surveillance or street view data sets -- especially if we can gain access to data sets labeled with outliers . We thus thank you for this suggestion . [ 1 ] Ruff , Lukas , et al . `` Deep one-class classification . '' International Conference on Machine Learning . 2018 [ 2 ] Zhou , Chong , and Randy C. Paffenroth . `` Anomaly detection with robust deep autoencoders . '' Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2017 ."}, {"review_id": "HygTE309t7-1", "review_text": "The paper proposes a decision forest based method for outlier detection and claims that it is better than current methods. A few questions: What is the precise definition of maximum weighted sum? Why not using maximum probability instead in Figure 1? Are they equivalent? What does this 8.1701 threshold refer to? What architectures you use for the experiment in Section 2? Comments: The observation that simple methods for outlier detection are not good enough is interesting, and deserves deeper understanding. However, directly calculating max. prob. may be a weak baseline. A stronger method to compare with would be using dropout during testing, see [1], which is easy to calculate and very practical (can easily be deployed to other tasks such as sequence tagging). The extensibility of the proposed method is not clear to me. Also, the reason that the observed failure of detection happens may due to the optimization procedure, i.e., how you train the model matters. The authors should provide the details of the training methods and architectures, along with the observation. The baseline compared in the experiments are methods that do not use the classification feature. It would be necessary to compare with stronger baselines, such as using dropout. Typo: 'a sample x $\\in$ based on its features' Reference: [1] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, by Yarin Gal, Zoubin Ghahramani ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ COMMMENT FROM REVIEWER ] : ( 1 ) the definition of maximum weighted sum ; ( 2 ) Why not using maximum probability in Figure 1 ? are they equivalent ? ( 3 ) What the 8.1701 threshold refer to ; ( 4 ) the architectures used for the experiment in Section 2 RESPONSE : Thank you for the detailed review . We have carefully revised Section 2 of our paper to include our responses to your questions , as explained below . The code and models used in this work will be made public after the double blind review process is completed . ( 1 ) The maximum weighted sum corresponds to the input of the softmax layer . For each class $ C_i $ , the final fully connected layer before the softmax layer produces a weighted sum score $ s_i $ . The score is computed by sum ( F_j w_ { ji } | 0 < j < n+1 ) where $ F_j $ is a feature and $ w_ { ji } $ is the learned weight of the FC layer that connects $ F_j $ and class $ C_i $ . The maximum weighted sum is the largest weighted sum score among all classes , defined as max ( s_1 , s_2 , ... , s_m ) . ( 2 ) The maximum weighted sum score is not equal to the maximum probability . Probabilities can be computed by applying a softmax layer to the weighted sum scores . That is softmax ( s_1 , s_2 , ... s_m ) = ( p_1 , p_2 , ... p_m ) . The maximum probability then corresponds to the largest probability defined as max ( p_1 , p_2 , ... , p_m ) . In other words , the maximum weighted sum score corresponds to the maximum score before the softmax layer , while the maximum probability is the maximum score after the softmax layer . We also worked with this maximum probability as the outlierness measure of each image . However , we discovered that using maximum probability performed worse than using maximum weighted sum . Therefore , we selected the better of the two , namely , the maximum weighted sum as our baseline method . ( 3 ) As explained in Sec.2 , the constant `` 8.1701 '' shown in Figure 1 is the cutoff threshold used in detecting outliers . It corresponds to the 5000-th smallest maximum weighted sum among the images in the training set CIFAR-10 . Then at the inference time we consider images with maximum weighted sum smaller than 8.1701 as outliers . This cutoff threshold is variable . ( 4 ) The architecture we used for the experiments in Section 2 is identical to the ones used in our experimental section ( Section 5.2 ) .The architecture is similar to the VGG-13 model with Batch Normalization . Specifically , the number of channels for the convolutional layers are [ 32 , 32 , \u2019 M \u2019 , 64 , 64 , \u2019 M \u2019 , 128 , 128 , \u2019 M \u2019 , 256 , 256 , \u2019 M \u2019 , 128 , 128 , \u2019 M \u2019 ] , where \u2018 M \u2019 is the max-pooling layer with kernel size=2 , stride size=2 . The kernel size for each convolutional layer is 3 . Batch normalization and Relu functions are applied after each convolutional layer . We previously had already described the training process we used in detail in our experimental section ."}, {"review_id": "HygTE309t7-2", "review_text": "Pros ---- [Originality/Clarity] The manuscript presents a novel technique for outlier detection in a supervised learning setting where something is considered an outlier if it is not a member of any of the \"known\" classes in the supervised learning problem at hand. The proposed solution builds upon an existing technique (deep neural forests). The authors clearly explain the enhancements proposed and the manuscript is quite easy to follow. [Clarity/Significance] The enhancements proposed are empirically evaluated in a manner that clearly shows the impact of the proposed schemes over the existing technique. For the data sets considered, the proposed schemes have demonstrated significant improvements for this scoped version of outlier detection. [Significance] The proposed scheme for improving the performance of the ensemble of the neural decision trees could be of independent interest in the supervised learning setting. Limitations ----------- [Significance] Based on my familiarity with the traditional literature on outlier detection in an unsupervised setting, it would be helpful for me to have some motivation for this problem of outlier detection in a supervised setting. For example, the authors mention that this outlier detection problem might allow us to identify images which are incorrectly labelled as one of the \"known\" classes even though the image is not a true member of any of the known classes, and might subsequently require (manual) inspection. However, if this technique would actually be used in such a scenario, the parameters of the empirical evaluation, such as a threshold for outliers that considers 5000 images as outliers, seem unreasonable. Usually number of outliers (intended for manual inspection) are fairly low. Empirical evaluations with a smaller number of outliers is more meaningful and representative of a real application in my opinion. [Significance] Another somewhat related question I have is the applicability of this proposed outlier detection scheme in the unsupervised scheme where there are no labels and no classification task in the first place. Is the proposed scheme narrowly scoped to the supervised setting? [Comments on empirical evaluations] - While the proposed schemes of novel inlier-ness score (weighted sum vs. max route), novel regularization scheme and ensemble of less correlated neural decision trees are extremely interesting and do show great improvements over the considered existing schemes, it is not clear to me why the use of something like Isolation Forest (or other more traditional unsupervised outlier detection schemes such as nearest/farthest neighbour based) on the learned representations just before the softmax is not sufficient. This way, the classification performance of the network remains the same and the outlier detection is performed on the learned features (since the learned features are assumed to be a better representation of the images than the raw image features). The current results do not completely convince me that the proposed involved scheme is absolutely necessary for the considered task of outlier detection in a supervised setting. - [minor] Along these lines, considering existing simple baselines such as auto-encoder based outlier detection should be considered to demonstrate the true utility of the proposed scheme. Reconstruction error is a fairly useful notion of outlier-ness. I acknowledge that I have considered the authors' argument that auto-encoders were formulated for dimensionality reduction. [Minor questions] - In Equation 10, it is not clear to me why (x,y) \\in \\mathcal{T}. I thought \\mathcal{T} is the set of trees and (x,y) was the sample-label pair. - It would be good understand if this proposed scheme is limited to the multiclass classification problem or is it also applicable to the multilabel classification problem (where each sample can have multiple labels). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ COMMNENT FROM REVIEWER ] : [ Significance ] A threshold that considers 5000 images as outliers seem unreasonable . Usually number of outliers intended for manual inspection is low . RESPONSE : With respect to the motivation , the key issue , as noted in our response to reviewer 3 above ( `` The extensibility of the proposed method '' ) , is that an image classifier trained on a particular class of images may be exposed to images at inference time that contain objects that are from none of the classes the classifier was trained on . In such cases , the classifier will happily produce an output , and using simple confidence-based methods for rejecting such objects will often result in labeling new previously unseen objects as an existing class , sometimes with surprisingly high probability . The main goal of our method is to reject such spurious detections . The threshold setting of 5000 as an input parameter is chosen for establishing an outlierness cutoff threshold for detecting outliers . It is used during the training phase . In other words , the users do not need to evaluate these 5000 images in the training set . We have updated Section 2 of our draft to avoid this confusion . As an input parameter , it can be set to either a larger or a smaller value per the need of the application . For example , if the user believes that the percentage of outliers in her application is large , then it should be set as a relatively large value . Otherwise it could be set to a smaller number . This is something an application domain expert would have to explore , given this will vary based on the targeted application and its scope . However , to respond to your thoughts concerning the choice of an outlier threshold , we add charts into Appendix E to illustrate how the outlier detection accuracy changes as this input parameter varies . As discussed in Appendix E , our proposed method significantly outperforms the state-of-art in almost all cases , and does well across a range of thresholds . [ COMMENT FROM REVIEWER ] : [ Significance ] Is the proposed scheme narrowly scoped to the supervised setting ? RESPONSE : Our approach does not rely on the labeled outliers to train an outlier classifier , although it needs labeled inliers to produce an outlierness score for each image and establish an outlierness cutoff threshold to detect outliers . As also noted in our response to reviewer 3 ( The extensibility of the proposed method ) , we believe that our approach is broadly applicable in a rich variety of real world applications for two reasons : ( 1 ) It resolves a significant limitation of traditional image classifiers . Given one testing image , an existing CNN image classifier will assign this image to one of the classes observed in the training set , even if it does not belong to any known class in the training data set . For example , given a cat image , if we test it on a CNN model trained using MNIST , this cat image will be erroneously assigned to one of the digit classes . In the real applications , it is common for images supplied at inference time to not belong to any class known in the training data -- for example , consider an autonomous vehicle trained mostly on urban imagery taken to the desert , where it sees sand , cacti , and tumbleweed for the first time . Our approach thus enhances any of the existing CNN-based classifiers with this powerful `` rejection '' ability . That is , it no longer blindly assigns a testing image to one of the known classes . Instead , an image will be rejected as being an outlier if it does not `` sufficiently '' belong to any of the existing classes . ( 2 ) Real applications tend to have a sufficient amount of normal data , and thus are able to more easily provide us with a large amount of labeled normal data for training the classification model , while they lack access to labeled outliers due to the rarity of outliers . Thus , an approach , such as ours , that uses only labeled inliers , and does NOT rely on the availability of outlier labels is a preferred situation in practice ."}], "0": {"review_id": "HygTE309t7-0", "review_text": "Summary: This paper modifies an existing technique designed for image classification to make it applicable to outlier detection. Strengths: The outlined problem is of significant practical importance. Weaknesses: - The improvement over the existing method is incremental; - The regularization on routing decision may not really be necessary as, in DNDF, the soft splits start as uniform and gradually converge to something close to hard splits; this is discussed in the supplementary material of the DNDF paper; - the datasets tested are standard image datasets, not even captured from vehicles or video surveillance. The SVHN (street view numbers) dataset is the closest the experiments get to the motivating application. Overall assessment: reject Recommendations for the authors: Test on a surveillance or street view benchmark. Even then, it's questionable whether the paper is suitable for ICLR due to lack of methodological novelty. Note: I'd like to apologize to the authors for the delay in submitting this review. It was due to a technical error on my part (I thought the reviews had posted, but they had not). In the spirit of independent evaluation, this review was not influenced by the other comments on this paper. I will follow-up with a response which will take into account the existing dialogue.", "rating": "4: Ok but not good enough - rejection", "reply_text": "[ COMMMENT FROM REVIEWER ] : The improvement over the existing method ( DNDF ) is incremental RESPONSE : Our approach represents an effective solution to an extremely important problem . In fact , our approach significantly outperforms the state-of-the-art in the accuracy of outlier detection as shown in our experimental study . While our approach leverages some of the DNDF principles , we introduce several critical insights to render it effective at detecting outliers . First , given that the DNDF approach was designed for improving classification accuracy , it was not obvious it would be applicable to tackling the outlier detection problem . Indeed , we are the first to leverage DNDF for addressing the outlier detection problem . We do this by unifying the best practice of unsupervised outlier detection with our observation that the max route of each tree in DNDF effectively captures the outlierness of each image . Second , we refine the core method with several technical innovations to assure effective outlier detection , while concurrently also yielding high accuracy for image classification . By this , the existing CNN-based image classifiers are enhanced to have the ability to reject a testing image as being an outlier if it does not `` sufficiently '' belong to any of the existing classes known in the training data . In particular , we proposed two new techniques , namely , an information-theoretic regularization strategy based on routing decisions and a new network architecture that ensures that each tree in the forest is completely independent . Further , as additional innovation , we designed a new joint learning method that optimizes the parameters for the decision node and for the prediction nodes in one integrated step through back-propagation , abandoning the two-step optimization strategy used in DNDF . Based on the above described innovations , our approach is not only technically novel but also useful , as it is highly effective at detecting outliers . We have revised the Proposed Approach and Contributions of the Introduction section ( Section 1 ) to reflect the above discussion . [ COMMMENT FROM REVIEWER ] : The regularization on routing decision may not really be necessary as , in DNDF , the soft splits start as uniform and gradually converge to something close to hard splits . OUR RESPONSE : Indeed , we have had explored this question . Namely , although the original DNDF method does start from uniform soft splits and gradually converges to close to hard splits , directly using the max-route probability in DNDF to detect outliers does not achieve very high accuracy as we have demonstrated in our experimental study ( Table 1 ) . Our proposed regularization strategy based on routing decisions , namely penalizing large entropy probability distributions of the routing decision , significantly outperforms this original DNDF design for the problem of detecting outliers . As illustrated in Table 1 of our experimental section , using our regularization method , the accuracy of detecting CIFAR-100 outliers out of the CIFAR-10 dataset increases from 84.64 % to 94.69 % , that is , we gain an over 10 % improvement . Further , the accuracy of detecting MNIST outliers increases from 67.59 % to 94.94 % representing a close to 30 % improvement in accuracy . These experimental findings on benchmark data sets provide strong evidence that our proposed design , in particular with the addition of the proposed regularization strategy , is both necessary and highly effective . [ COMMMENT FROM REVIEWER ] : The datasets tested are standard image datasets , not even captured from vehicles or video surveillance . OUR RESPONSE : The datasets we used in our experiments are indeed commonly used in state-of-the-art image outlier detection papers such as [ 1 ] and [ 2 ] . In our experiments we focused on these datasets because this ensures a fair comparison of our proposed outlier detection approach against the state-of-the-art . In the future , we will be happy to extend our evaluation to surveillance or street view data sets -- especially if we can gain access to data sets labeled with outliers . We thus thank you for this suggestion . [ 1 ] Ruff , Lukas , et al . `` Deep one-class classification . '' International Conference on Machine Learning . 2018 [ 2 ] Zhou , Chong , and Randy C. Paffenroth . `` Anomaly detection with robust deep autoencoders . '' Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2017 ."}, "1": {"review_id": "HygTE309t7-1", "review_text": "The paper proposes a decision forest based method for outlier detection and claims that it is better than current methods. A few questions: What is the precise definition of maximum weighted sum? Why not using maximum probability instead in Figure 1? Are they equivalent? What does this 8.1701 threshold refer to? What architectures you use for the experiment in Section 2? Comments: The observation that simple methods for outlier detection are not good enough is interesting, and deserves deeper understanding. However, directly calculating max. prob. may be a weak baseline. A stronger method to compare with would be using dropout during testing, see [1], which is easy to calculate and very practical (can easily be deployed to other tasks such as sequence tagging). The extensibility of the proposed method is not clear to me. Also, the reason that the observed failure of detection happens may due to the optimization procedure, i.e., how you train the model matters. The authors should provide the details of the training methods and architectures, along with the observation. The baseline compared in the experiments are methods that do not use the classification feature. It would be necessary to compare with stronger baselines, such as using dropout. Typo: 'a sample x $\\in$ based on its features' Reference: [1] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, by Yarin Gal, Zoubin Ghahramani ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ COMMMENT FROM REVIEWER ] : ( 1 ) the definition of maximum weighted sum ; ( 2 ) Why not using maximum probability in Figure 1 ? are they equivalent ? ( 3 ) What the 8.1701 threshold refer to ; ( 4 ) the architectures used for the experiment in Section 2 RESPONSE : Thank you for the detailed review . We have carefully revised Section 2 of our paper to include our responses to your questions , as explained below . The code and models used in this work will be made public after the double blind review process is completed . ( 1 ) The maximum weighted sum corresponds to the input of the softmax layer . For each class $ C_i $ , the final fully connected layer before the softmax layer produces a weighted sum score $ s_i $ . The score is computed by sum ( F_j w_ { ji } | 0 < j < n+1 ) where $ F_j $ is a feature and $ w_ { ji } $ is the learned weight of the FC layer that connects $ F_j $ and class $ C_i $ . The maximum weighted sum is the largest weighted sum score among all classes , defined as max ( s_1 , s_2 , ... , s_m ) . ( 2 ) The maximum weighted sum score is not equal to the maximum probability . Probabilities can be computed by applying a softmax layer to the weighted sum scores . That is softmax ( s_1 , s_2 , ... s_m ) = ( p_1 , p_2 , ... p_m ) . The maximum probability then corresponds to the largest probability defined as max ( p_1 , p_2 , ... , p_m ) . In other words , the maximum weighted sum score corresponds to the maximum score before the softmax layer , while the maximum probability is the maximum score after the softmax layer . We also worked with this maximum probability as the outlierness measure of each image . However , we discovered that using maximum probability performed worse than using maximum weighted sum . Therefore , we selected the better of the two , namely , the maximum weighted sum as our baseline method . ( 3 ) As explained in Sec.2 , the constant `` 8.1701 '' shown in Figure 1 is the cutoff threshold used in detecting outliers . It corresponds to the 5000-th smallest maximum weighted sum among the images in the training set CIFAR-10 . Then at the inference time we consider images with maximum weighted sum smaller than 8.1701 as outliers . This cutoff threshold is variable . ( 4 ) The architecture we used for the experiments in Section 2 is identical to the ones used in our experimental section ( Section 5.2 ) .The architecture is similar to the VGG-13 model with Batch Normalization . Specifically , the number of channels for the convolutional layers are [ 32 , 32 , \u2019 M \u2019 , 64 , 64 , \u2019 M \u2019 , 128 , 128 , \u2019 M \u2019 , 256 , 256 , \u2019 M \u2019 , 128 , 128 , \u2019 M \u2019 ] , where \u2018 M \u2019 is the max-pooling layer with kernel size=2 , stride size=2 . The kernel size for each convolutional layer is 3 . Batch normalization and Relu functions are applied after each convolutional layer . We previously had already described the training process we used in detail in our experimental section ."}, "2": {"review_id": "HygTE309t7-2", "review_text": "Pros ---- [Originality/Clarity] The manuscript presents a novel technique for outlier detection in a supervised learning setting where something is considered an outlier if it is not a member of any of the \"known\" classes in the supervised learning problem at hand. The proposed solution builds upon an existing technique (deep neural forests). The authors clearly explain the enhancements proposed and the manuscript is quite easy to follow. [Clarity/Significance] The enhancements proposed are empirically evaluated in a manner that clearly shows the impact of the proposed schemes over the existing technique. For the data sets considered, the proposed schemes have demonstrated significant improvements for this scoped version of outlier detection. [Significance] The proposed scheme for improving the performance of the ensemble of the neural decision trees could be of independent interest in the supervised learning setting. Limitations ----------- [Significance] Based on my familiarity with the traditional literature on outlier detection in an unsupervised setting, it would be helpful for me to have some motivation for this problem of outlier detection in a supervised setting. For example, the authors mention that this outlier detection problem might allow us to identify images which are incorrectly labelled as one of the \"known\" classes even though the image is not a true member of any of the known classes, and might subsequently require (manual) inspection. However, if this technique would actually be used in such a scenario, the parameters of the empirical evaluation, such as a threshold for outliers that considers 5000 images as outliers, seem unreasonable. Usually number of outliers (intended for manual inspection) are fairly low. Empirical evaluations with a smaller number of outliers is more meaningful and representative of a real application in my opinion. [Significance] Another somewhat related question I have is the applicability of this proposed outlier detection scheme in the unsupervised scheme where there are no labels and no classification task in the first place. Is the proposed scheme narrowly scoped to the supervised setting? [Comments on empirical evaluations] - While the proposed schemes of novel inlier-ness score (weighted sum vs. max route), novel regularization scheme and ensemble of less correlated neural decision trees are extremely interesting and do show great improvements over the considered existing schemes, it is not clear to me why the use of something like Isolation Forest (or other more traditional unsupervised outlier detection schemes such as nearest/farthest neighbour based) on the learned representations just before the softmax is not sufficient. This way, the classification performance of the network remains the same and the outlier detection is performed on the learned features (since the learned features are assumed to be a better representation of the images than the raw image features). The current results do not completely convince me that the proposed involved scheme is absolutely necessary for the considered task of outlier detection in a supervised setting. - [minor] Along these lines, considering existing simple baselines such as auto-encoder based outlier detection should be considered to demonstrate the true utility of the proposed scheme. Reconstruction error is a fairly useful notion of outlier-ness. I acknowledge that I have considered the authors' argument that auto-encoders were formulated for dimensionality reduction. [Minor questions] - In Equation 10, it is not clear to me why (x,y) \\in \\mathcal{T}. I thought \\mathcal{T} is the set of trees and (x,y) was the sample-label pair. - It would be good understand if this proposed scheme is limited to the multiclass classification problem or is it also applicable to the multilabel classification problem (where each sample can have multiple labels). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ COMMNENT FROM REVIEWER ] : [ Significance ] A threshold that considers 5000 images as outliers seem unreasonable . Usually number of outliers intended for manual inspection is low . RESPONSE : With respect to the motivation , the key issue , as noted in our response to reviewer 3 above ( `` The extensibility of the proposed method '' ) , is that an image classifier trained on a particular class of images may be exposed to images at inference time that contain objects that are from none of the classes the classifier was trained on . In such cases , the classifier will happily produce an output , and using simple confidence-based methods for rejecting such objects will often result in labeling new previously unseen objects as an existing class , sometimes with surprisingly high probability . The main goal of our method is to reject such spurious detections . The threshold setting of 5000 as an input parameter is chosen for establishing an outlierness cutoff threshold for detecting outliers . It is used during the training phase . In other words , the users do not need to evaluate these 5000 images in the training set . We have updated Section 2 of our draft to avoid this confusion . As an input parameter , it can be set to either a larger or a smaller value per the need of the application . For example , if the user believes that the percentage of outliers in her application is large , then it should be set as a relatively large value . Otherwise it could be set to a smaller number . This is something an application domain expert would have to explore , given this will vary based on the targeted application and its scope . However , to respond to your thoughts concerning the choice of an outlier threshold , we add charts into Appendix E to illustrate how the outlier detection accuracy changes as this input parameter varies . As discussed in Appendix E , our proposed method significantly outperforms the state-of-art in almost all cases , and does well across a range of thresholds . [ COMMENT FROM REVIEWER ] : [ Significance ] Is the proposed scheme narrowly scoped to the supervised setting ? RESPONSE : Our approach does not rely on the labeled outliers to train an outlier classifier , although it needs labeled inliers to produce an outlierness score for each image and establish an outlierness cutoff threshold to detect outliers . As also noted in our response to reviewer 3 ( The extensibility of the proposed method ) , we believe that our approach is broadly applicable in a rich variety of real world applications for two reasons : ( 1 ) It resolves a significant limitation of traditional image classifiers . Given one testing image , an existing CNN image classifier will assign this image to one of the classes observed in the training set , even if it does not belong to any known class in the training data set . For example , given a cat image , if we test it on a CNN model trained using MNIST , this cat image will be erroneously assigned to one of the digit classes . In the real applications , it is common for images supplied at inference time to not belong to any class known in the training data -- for example , consider an autonomous vehicle trained mostly on urban imagery taken to the desert , where it sees sand , cacti , and tumbleweed for the first time . Our approach thus enhances any of the existing CNN-based classifiers with this powerful `` rejection '' ability . That is , it no longer blindly assigns a testing image to one of the known classes . Instead , an image will be rejected as being an outlier if it does not `` sufficiently '' belong to any of the existing classes . ( 2 ) Real applications tend to have a sufficient amount of normal data , and thus are able to more easily provide us with a large amount of labeled normal data for training the classification model , while they lack access to labeled outliers due to the rarity of outliers . Thus , an approach , such as ours , that uses only labeled inliers , and does NOT rely on the availability of outlier labels is a preferred situation in practice ."}}