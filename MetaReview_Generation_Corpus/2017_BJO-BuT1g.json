{"year": "2017", "forum": "BJO-BuT1g", "title": "A Learned Representation For Artistic Style", "decision": "Accept (Poster)", "meta_review": "The reviewers (two of whom stated maximum confidence) are in consensus that this is a high-quality paper. It also attracted some public feedback which was also positive. The authors have already incorporated much of the feedback into their revised paper. This seems to be a clear accept in my opinion.", "reviews": [{"review_id": "BJO-BuT1g-0", "review_text": "This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set \u201cconditional instance normalization\u201d parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I\u2019m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I\u2019m less confident in my expertise in this area, I\u2019m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it\u2019s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it\u2019s clear. In fact, I\u2019m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I\u2019m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn\u2019t it be \u201cVGG-16\u201d network (not \"VGG-19\u201d)? Page 2: \u201cnewtork\u201d => \u201cnetwork\u201d. Paragraph after Eq. (5): \u201cmuch less\u201d => \u201cfewer\u201d.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . We found them very useful in improving the quality of the manuscript . Following your suggestions , we have made the following changes to the manuscript : + Subsection heading 2.1 was removed to group all prior art review in the same place . + Work related to feedforward networks is now cited in the prior art review section and in Fig 2 . + We appreciate the reviewer \u2019 s suggestion to combine Figures 2 and 3 . After some thought we decided to keep these figures separate because ( a ) the two figures convey distinct points and ( b ) Figure 2 recapitulates previous work while Figure 3 highlights our unique contribution . The latter point is quite important as we would not want to confuse a reader about what is unique to this paper . If the reviewer feels strongly otherwise , please do let us know . + For Equation 5 , we have added additional text to clarify x and z . + For Equation 4 , we felt that the equations were already quite heavy so rather then add a subscript s , we added explicit , additional text to highlight that the style transfer network is specific to a painting style . + In Fig 5 ( left ) , an explanation for the curve colors has been added to the caption . + The \u201c newtork \u201d typo has been corrected in page 2 . + \u201c much less \u201d has been replaced with \u201c fewer \u201d after Eq 5 . Regarding the mention of VGG-19 in page 1 : This is intentional . Gatys et al.use VGG-19 , whereas we use VGG-16 like Johnson et al ."}, {"review_id": "BJO-BuT1g-1", "review_text": "The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer. Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space. Here I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as \u201cThe parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles\u201d can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting. In conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your comments . We agree with you that the interpretability of the learned style representation is an important area of investigation . This is something we are currently working on . The results are very preliminary , and we do not feel like they should be included in the manuscript yet , but following another reviewer \u2019 s point we tried breaking the network into 3 sections ( encoder , residual and decoder ) and independently carrying out the style interpolation for the 3 sections . Broadly speaking we found the `` encoder '' did almost nothing ; the `` residual '' performed most of the interesting spatial decomposition of the image and the `` decoder '' performed the colorization of the image . Like was said above , more work is needed in this direction , but our goal would be to provide a more detailed analysis in the camera-ready version of the paper ."}, {"review_id": "BJO-BuT1g-2", "review_text": "CONTRIBUTIONS The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results. NOVELTY The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style. MISSING CITATION The following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al: Li and Wand, \"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\", ECCV 2016 CLARITY The paper is very well written and easy to follow. SIGNIFICANCE Though simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device. EVALUATION Like many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively. SUMMARY The problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference. Pros - Simple modification to feedforward neural style transfer with several improvements over prior work - Strong qualitative results - Well-written - Open-source code has already been released Cons - Slightly incremental - Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your feedback . We like the idea of adding new styles to a mobile application by only sending over a small number of style-specific parameters . We will be adding it to our discussion and credit you for the suggestion in our next revision . We will be adding a reference to Li and Wand alongside other feedforward style transfer citations . Thank you for pointing it out . We also acknowledged Justin Johnson \u2019 s GitHub repository as prior work on optimization-based style blending in the paper . One interesting difference that we would like to point out is that our approach to style blending produces homogeneous pastiches , whereas the optimization-based approach produces heterogeneous pastiches with some areas exhibiting one set of style features and other areas exhibiting a different set of style features . We agree that the style and content losses are not always well correlated with the perceptual quality of the results . In the paper , we use these metrics to back the statement that the optimization of the N-styles version of the feedforward style transfer network is not significantly impacted when compared with single-style networks . One potential pitfall is that the N-styles network could behave very differently than single-style networks despite having comparable losses , which we account for by qualitatively comparing pastiches . We welcome suggestions to improve the reliability of the quantitative evaluation ."}], "0": {"review_id": "BJO-BuT1g-0", "review_text": "This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set \u201cconditional instance normalization\u201d parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I\u2019m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I\u2019m less confident in my expertise in this area, I\u2019m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it\u2019s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it\u2019s clear. In fact, I\u2019m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I\u2019m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn\u2019t it be \u201cVGG-16\u201d network (not \"VGG-19\u201d)? Page 2: \u201cnewtork\u201d => \u201cnetwork\u201d. Paragraph after Eq. (5): \u201cmuch less\u201d => \u201cfewer\u201d.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . We found them very useful in improving the quality of the manuscript . Following your suggestions , we have made the following changes to the manuscript : + Subsection heading 2.1 was removed to group all prior art review in the same place . + Work related to feedforward networks is now cited in the prior art review section and in Fig 2 . + We appreciate the reviewer \u2019 s suggestion to combine Figures 2 and 3 . After some thought we decided to keep these figures separate because ( a ) the two figures convey distinct points and ( b ) Figure 2 recapitulates previous work while Figure 3 highlights our unique contribution . The latter point is quite important as we would not want to confuse a reader about what is unique to this paper . If the reviewer feels strongly otherwise , please do let us know . + For Equation 5 , we have added additional text to clarify x and z . + For Equation 4 , we felt that the equations were already quite heavy so rather then add a subscript s , we added explicit , additional text to highlight that the style transfer network is specific to a painting style . + In Fig 5 ( left ) , an explanation for the curve colors has been added to the caption . + The \u201c newtork \u201d typo has been corrected in page 2 . + \u201c much less \u201d has been replaced with \u201c fewer \u201d after Eq 5 . Regarding the mention of VGG-19 in page 1 : This is intentional . Gatys et al.use VGG-19 , whereas we use VGG-16 like Johnson et al ."}, "1": {"review_id": "BJO-BuT1g-1", "review_text": "The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer. Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space. Here I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as \u201cThe parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles\u201d can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting. In conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your comments . We agree with you that the interpretability of the learned style representation is an important area of investigation . This is something we are currently working on . The results are very preliminary , and we do not feel like they should be included in the manuscript yet , but following another reviewer \u2019 s point we tried breaking the network into 3 sections ( encoder , residual and decoder ) and independently carrying out the style interpolation for the 3 sections . Broadly speaking we found the `` encoder '' did almost nothing ; the `` residual '' performed most of the interesting spatial decomposition of the image and the `` decoder '' performed the colorization of the image . Like was said above , more work is needed in this direction , but our goal would be to provide a more detailed analysis in the camera-ready version of the paper ."}, "2": {"review_id": "BJO-BuT1g-2", "review_text": "CONTRIBUTIONS The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results. NOVELTY The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style. MISSING CITATION The following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al: Li and Wand, \"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\", ECCV 2016 CLARITY The paper is very well written and easy to follow. SIGNIFICANCE Though simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device. EVALUATION Like many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively. SUMMARY The problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference. Pros - Simple modification to feedforward neural style transfer with several improvements over prior work - Strong qualitative results - Well-written - Open-source code has already been released Cons - Slightly incremental - Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your feedback . We like the idea of adding new styles to a mobile application by only sending over a small number of style-specific parameters . We will be adding it to our discussion and credit you for the suggestion in our next revision . We will be adding a reference to Li and Wand alongside other feedforward style transfer citations . Thank you for pointing it out . We also acknowledged Justin Johnson \u2019 s GitHub repository as prior work on optimization-based style blending in the paper . One interesting difference that we would like to point out is that our approach to style blending produces homogeneous pastiches , whereas the optimization-based approach produces heterogeneous pastiches with some areas exhibiting one set of style features and other areas exhibiting a different set of style features . We agree that the style and content losses are not always well correlated with the perceptual quality of the results . In the paper , we use these metrics to back the statement that the optimization of the N-styles version of the feedforward style transfer network is not significantly impacted when compared with single-style networks . One potential pitfall is that the N-styles network could behave very differently than single-style networks despite having comparable losses , which we account for by qualitatively comparing pastiches . We welcome suggestions to improve the reliability of the quantitative evaluation ."}}