{"year": "2019", "forum": "r1xN5oA5tm", "title": "Phrase-Based Attentions", "decision": "Reject", "meta_review": "All reviewers agree in their assessment that this paper does not meet the bar for ICLR. The area chair commends the authors for their detailed responses.", "reviews": [{"review_id": "r1xN5oA5tm-0", "review_text": "This paper presents an attention mechanism that computes a weighted sum over not only single tokens but ngrams (phrases). Experiments on WMT14 show a slight advantage over token-based attention. The model is elegant and presented very clearly. I really liked the motivation too. Having said that, I am not sold on the claim that phrasal attention actually helps, for two reasons: 1) The comparison to previous results is weak. There are more datasets, models, and hyperparameter settings that need to be tested. 2) Phrasal attention essentially adds an additional convolution layer, i.e. it adds parameters and complexity to the proposed model over the baseline. This needs to be controlled by, for example, adding another transformer block to the baseline model. The question that such an experiment would answer is \"does phrasal attention help more than an extra transformer layer?\" In my view, it is a more interesting question than \"does phrasal attention help more than nothing?\" Also related to concern (2), I think that the authors should check whether the relative improvement from phrasal attention grows/shrinks as a function of the encoder's depth. It could be that deep enough encoders (e.g. 10 layers) already contain some latent representation of phrases, and that this approach mainly benefits shallower architectures (e.g. 2 layers). === MINOR POINTS === If I understand the math behind 3.1.2 correctly, you're first applying a 1x1 conv to K, and then an nx1 conv. Since there's no non-linearity in the middle, isn't this equivalent to the first method? The only difference seems to be that you're assuming the low-rank decomposition fo the bilinear term at a different point (and thus get a different number of parameters, unless d_k = d_q). Have you tried dividing by sqrt(d_k * n) in 3.1.1 too? While the overall model is well explained, I found 3.3 harder to parse.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful reviews . We address your comments as follows . 1/ The comparison to previous results is weak . There are more datasets , models , and hyperparameter settings that need to be tested : We agree that more experiments with other datasets , model variants and hyperparameter settings would make the paper stronger . Due to the lack of GPU machines ( which is common in an academic setting ) , we could perform only a handful of experiments before the submission deadline . However , we continued performing the intended experiments after the deadline . In particular , we have experimented with the English-to-Russian and Russian-to-English translation tasks ( i.e. , more datasets ) , and explored other model variants and hyperparameters ( e.g. , higher order ngrams ) . We revised our paper accordingly . Here is the summary of the new results for your consideration : For En-Ru : Base transformer : 35.64 Homogeneous QueryKernel : 36.31 Heterogeneous ConvKV with n-gram ( 1-2 ) : 36.81 Heterogenenous QueryKernel with n-gram ( 1-2-3-4 ) :37.39 Interleaved heterogeneous QueryKernel ( 1-2 ) : 37.24 For Ru-En Base transformer : 34.56 Homogeneous ConKV : 34.75 Heterogenenous ConvKV with n-gram ( 1-2 ) : 35.10 Heterogenenous ConvKV with n-gram ( 1-2-3-4 ) : 35.91 Interleaved heterogeneous ConvKV ( 1-2 ) : 34.70 We also conducted more experiments as per your request : transformer base ( 6 layers ) : 63M params , 26.08 BLEU transformer big ( 6 layers ) : 214M params , 26.63 BLEU transformer base ( 10 layers ) : 91M params , 26.60 BLEU heterogeneous 4 layers ( ConvKV ) : 60M params , 26.63 BLEU heterogeneous 6 layers ( ConvKV ) : 80M params , 27.04 BLEU As you may see , transformer big and transformer base ( 10-layers ) have more parameters ; they should perform better than transformer base . But the margins are not significant because of limited batch size ( explained later ) . On the other hand , our heterogeneous model with just 4 layers is already on par with transformer base 10 layers or transformer big , though it has less parameters . Please note that the original transformer paper ( Vaswani et al.2017 ) conducted their experiments at a more massive scale ( base and big models were trained with 8 GPUs ) than what we could afford in a common academic lab . There have been many evidences that practical training of the transformer networks ( theirs and ours ) is significantly susceptible to the batch size ( which increases with the number of GPUs used ) , and training a 1-GPU setup for sufficiently long does not produce similar results as an 8-GPU setup ; please see the discussion https : //github.com/tensorflow/tensor2tensor/issues/444 or this material https : //ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf for more details . We compare our model with a version of the transformer that is trained on a single GPU using the same setting and code provided by the authors . In order to make a fair comparison to the baseline , we kept all of our experimental settings identical across all experiments . Thus , the comparison of our model ( 27.40 BLEU ) should be made with this ( 1-GPU ) baseline ( 26.07 BLEU ) as opposed to the reported results ( 27.30 BLEU with 8 GPUs ) in the original paper , because we believe that is not a fair comparison . With due respect , to comment on your statement \u201c Experiments on WMT14 show a slight advantage over token-based attention \u201d , 1.3 BLEU improvements in English-to-German translation task is quite large ( as acknowledged by Reviewer 2 ) . Our new experiments on English-Russian translation tasks show even larger gains ( up to 1.75 BLEU ) compared to the baseline . We urge our reviewers not to penalize our work for not having industry-scale GPU facilities , rather evaluate it based on its scientific merits . We think , the two proposed phrase-based attention methods ( ConvKV and QueryK ) incorporated within the proposed architectures ( homogeneous , heterogeneous , and interleaved ) provide novel and complete solutions to phrasal attentions that as a whole is a significant contribution to the community . Response continue in the next comment ."}, {"review_id": "r1xN5oA5tm-1", "review_text": "Phrase-Based Attention Paper Summary: Neural translation attention computes latent alignments which pairs input/target positions. Phrase-based systems used to align pairs of spans (n-grams) rather than individual positions, this work explores neural architectures to align spans. It does so by compositing attention and convolution operations. It reports empirical results that compares n-gram to uni-gram attention. Review: This paper reads well. It provides appropriate context. The equations are correct. It lacks a few references I mentioned below. The main weaknesses of the work lies in its motivation and in the empirical results. This work motivation ignores an important aspect of neural MT: the vectors that attention compares (\u201cqueries\u201d and \u201ckeys\u201d) do not summarizes a single token/unigram. These vectors aggregate information across nearby positions (convolutional tokens, Ghering et al 2017), all previous tokens (recurrent models, Suskever et al 2014) or the whole source sentence (transformer, Vaswani et al 2017). Moreover multiple layers of attention are composed in modern decoders, comparing vectors which integrates information from both source and target. These vectors cannot be considered as the representation of a single unigram from the source or from the target. The key-value convolution method 3.1.1 is not different from (Ghering et al 2017) which alternates computing convolution and attention multiple times. The query as kernel is a contribution of this work, it is highly related to the concurrent submission to ICLR on dynamic convolution \u201cPay Less Attention with Lightweight and Dynamic Convolutions\u201d. This other work however reports better empirical results over the same benchmark. On empirical results, it seems that the table does not include recent results from work on weighted transformer (Ahmed et al 2017) or relative attention (Shaw et al 2018). Also a 0.1 BLEU improvement over Vaswani et al seems brittle, is your result averaged over multiple runs, could the base transformer be better with as many parameters/updates as your model? Review Summary: This work starts from the premise that current models attends to unigram representation, which is wrong (keys and values already depends on multiple source/target positions). The empirical results are missing recent improvements. The reported empirical advantage compared to baseline is thin. The most interesting contribution is the query as kernel approach: however the concurrent submission \u201cPay Less Attention with Lightweight and Dynamic Convolutions\u201d obtains better empirical results with a similar idea. Missing references: Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. 2017. Weighted transformer network for machine translation. arxiv, 1711.02132. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. of NAACL. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful reviews . We address your comments as follows . 1/ This work motivation ignores an important aspect of neural MT : the vectors that attention compares ( \u201c queries \u201d and \u201c keys \u201d ) do not summarizes a single token/unigram . These vectors aggregate information across nearby positions ( convolutional tokens , Ghering et al 2017 ) , all previous tokens ( recurrent models , Suskever et al 2014 ) or the whole source sentence ( transformer , Vaswani et al 2017 ) . Moreover multiple layers of attention are composed in modern decoders , comparing vectors which integrates information from both source and target . These vectors can not be considered as the representation of a single unigram from the source or from the target . We are sorry if our writing gave you the wrong impression that the vectors for the tokens are computed independently . However , we did NEVER state that in our paper , rather we wrote ( in the Introduction ) : \u201c Despite the advantages , the concept of phrasal attentions has largely been neglected in NMT , as most NMT models generate translations token-by-token autoregressively , and use the token-based attention method which is order invariant . Therefore , the intuition of phrase-based translation is vague in existing NMT systems that solely depend on the underlying neural architectures ( recurrent , convolutional , or self-attention ) to incorporate compositional information. \u201d To elaborate on this , what we mean by \u201c token-based attention \u201d is described by the formula : Softmax ( QK^T ) V , which is , by definition , order-invariant and token-based . This means that if we change the order of the vectors in Q , K and V , there is no difference in the resulting attention scores and the context vectors . However , we did not say that the inputs to this formula ( Q , K , V ) are order invariant ( see Sec.2 ) .Indeed , they can be order-variant , inter-dependent and causal ( in case of the decoder ) . Different architectures have their own ways to ensure that theses inputs have such characteristics before passing to the attention . For instance , recurrent cells encode all previous tokens to the current one , convolutional layers encodes nearby tokens to the current one , self-attention encodes all ( or previous in case of decoder ) vectors from Keys and Values embedded by positional encoding . We hope this clarifies the confusion , and now let us explain why phrasal attention is needed for neural machine translation . It is true that the underlying architectures encode vector representations for tokens by aggregating information across multiple locations . But these vectors represent the respective input \u201c tokens \u201d considering their context . This is similar to ELMo/BERT representation of the tokens where the nearby vectors provide context-relevant clues only to represent the current token , and one does not use this as a representation of the phrase , rather uses it as a representation of the corresponding token . Note that we do not use ELMo/BERT representation of the tokens directly for the prediction tasks , rather they are incorporated into a model to consider the task-specific dependencies often modeled as inductive biases . For example , for NER task , these representations are fed into a bi-LSTM-CRF to model dependencies not only the in the input representations but also in the output sequence ( between NER tags ) . For SQuAD QA task , the representations are used in a bidirectional attention flow network ( BiDAF , 2017 ) to model task-relevant structures . Similarly , for textual entailment , the vectors are used in the ESIM model ( Chen et al.2017 ) that uses a bi-LSTM encoder , followed by a matrix attention layer and inference layers . The main point here is that the token representations learned by ELMo/BERT are incorporated into a model for the target task . Our phrase-based attention methods provide a model for the NMT task to incorporate phrasal alignments ( as explicit inductive bias in the NMT model ) , and this is orthogonal to the underlying representation learning neural architectures . Response continues to next comment ."}, {"review_id": "r1xN5oA5tm-2", "review_text": "The authors propose to include phrases (contiguous n-grams of wordpieces) in both the self-attention and encoder-decoder attention modules of the Transformer model (Vaswani et al., 2017). In standard multi-head attention, the logits of the attention distribution of each head is computed as the dot-product between query and key representations, which are position-specific. In the phrase-based attention proposed here, a convolution is first computed over the query, key, value sequences before logits are computed as before (a few variants of this scheme are explored). Results show an improvement of up to 1.3 BLEU points compared to the baseline Transformer model. However, the lack of a controlled experiment sheds substantial doubt on the efficiacy of the model (see below). Contributions ------------------- Proposes a simple way to incorporate n-grams in the Transformer model. The implementation is straightforward and should be fully replicable in an afternoon. Having an inductive bias towards modeling of longer phrases seems intuitively useful, in particular when using subword representations, where subword units are often ambiguous. This is also motivated by the fact that prior work has shown that subword regularization, where sampling different subword segmentations during training can be useful. Improvements in BLEU scores are quite strong. Issues --------- The experiments do not control for parameter count. The phrasal attention model adds significant number of parameters (e.g., \"interleaved attention\" corresponds to 3x the number of 1D convolution parameters in the attention layer). It is well established that more parameters correspond to increased BLEU scores (e.g., the 2x parameter count in the \"big\" Transformer setting from Vaswani et al. (2017) results in over 1 BLEU point improvement). This needs to be fixed! The model is a very modest extension of the original Transformer model and so its value to the community beyond improved numbers is somewhat questionable. While an explicit inductive bias for phrases seem plausible, it may be that this can already be fully captured by multi-head attention. With positional encodings, two heads can easily attend to adjacent positions which gives the model the same capacity as the convolutional phrase model. The result in the paper that trigrams do not add anything on top of bigrams signals to me that the model is already implicitly capturing phrase-level aspects in the multi-head attention. I would urge the authors to verify this by looking at gradient information (https://arxiv.org/abs/1312.6034). There are several unsubstantiated claims: \"Without specific attention to phrases, a particular attention function has to depend entirely on the token-level softmax scores of a phrase for phrasal alignment, which is not robust and reliable, thus making it more difficult to learn the mappings.\" - The attention is positional, but not necessarily token-based. The model has capacity to represent phrases in subsequent layers. WIth h heads , a position in the k-th layer can in principle represent h^k grams (each slot in layer 2 can represent a h-gram and so on). The differences in training setup compared to Vaswani et al. (2017) needs to be explicit (\"most of the training settings\" is too handwavy). Please list any differences. The notation is somewhat cumbersome and could use some polishing. For example, the input and output symbols both range over indices in [1,n]. The multi-head attention formulas also do not match the ones from Vaswani et al. (2017) fully. Please ensure consistency and readability of symbols and formulas. The model inspection would be much improved by variance analysis. For example, the numbers in table 3 would be more useful if accompanied by variance across training runs. The particular allocation could well be an effect of random initialization. I could also see other reasons for this particular allocation than phrases being more useful in intermediate layers (e.g., positional encodings in the first layer is a strong bias towards token-to-token attention, it could be that the magnitude of convolved vectors is larger than the batch-normalized unigram encodings, so that logits are larger. Questions -------------- In \"query-as-kernel convolution\", it is unclear whether you map Q[t, :] into n x d_q x d_k convolution kernel parameters, or if each element of the window around Q[t] of width n is mapped to a convolution kernel parameter. Also what is the exact form of the transformation. Do you transform the d_q dimensional vectors in Q to a d_q x d_k matrix? Is this done by mapping to a d_q * d_k dimensional vector which is then rearranged into the convolution kernel matrix? Does the model tend to choose one particular n-gram type for a particular position, or will it select different n-gram types for the same position? \"The selection of which n-gram to assign to how many heads is arbitrary\" - How is this arbitrary? This seems a rather strong inductive bias? \"However, the homogeneity restriction may limit the model to learn interactions between different n-gram types\" - How is the case? It seems rather that the limitation is that the model cannot dynamically allocate heads to the most relevant n-gram type? I do not understand equation 14. Do you mean I_dec = I_cross = (...)? \"Phrase-to-phrase mapping helps model local agreement, e.g., between an adjective and a noun (in terms of gender, number and case) or between subject and verb (in terms of person and number).\" Is this actually verified with experiments / model inspection? \"This is especially necessary when the target language is morphologically rich, like German, whose words are usually compounded with sub-words expressing different meanings and grammatical structures\" This claim should be verified, e.g. by comparing to English-French as well as model inspection. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful reviews . We address your comments as follows . 1/ The experiments do not control for parameter count . The phrasal attention model adds significant number of parameters ( e.g. , `` interleaved attention '' corresponds to 3x the number of 1D convolution parameters in the attention layer ) . It is well established that more parameters correspond to increased BLEU scores ( e.g. , the 2x parameter count in the `` big '' Transformer setting from Vaswani et al . ( 2017 ) results in over 1 BLEU point improvement ) . This needs to be fixed ! We have now provided parameter counts of the models with the results in Table 1 in the revised version of the paper . Thanks for the suggestion . However , with due respect , the parameter counts do not match with your estimates ; the base transformer has 63M parameters , the interleaved attention ( our biggest model ) has 116M parameters , while the transformer big has 214M parameters . That is , our interleaved model has approximately 1.8x more parameters than the transformer base , while the transformer big has almost 3.4x more parameters than the base transformer . In addition , what really makes the difference in BLEU scores between Vaswani et al . ( 2017 ) and ours is the batch size ( or the number of GPUs ) used for training , which has been shown empirically to affect the results substantially . Please see the discussion https : //github.com/tensorflow/tensor2tensor/issues/444 or this material https : //ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf for more details .. We also observed the same issue for both the transformer and our phrasal attentions . The transformer used a massive setting of 8 GPUs , while we could only afford to run our experiments on a single GPU . Therefore , we compare our models with the base transformer that is trained with 1 GPU using the same settings and codes provided by the authors . This baseline gives 26.07 BLEU . We claim our gain ( 1.3 BLEU ) from this baseline instead of the reported 8-GPU result to ensure a fair comparison . Having said that , we conducted more experiments on English - Russian and Russian - English and conduct analysis and experiments on the number of parameters relative to base transformer and big transformer . For En-Ru : Base transformer : 35.64 Homogeneous QueryKernel : 36.31 Heterogeneous ConvKV with n-gram ( 1-2 ) : 36.81 Heterogenenous QueryKernel with n-gram ( 1-2-3-4 ) : 37.39 Interleaved heterogeneous QueryKernel ( 1-2 ) : 37.24 For Ru-En Base transformer : 34.56 Homogeneous ConKV : 34.75 Heterogenenous ConvKV with n-gram ( 1-2 ) : 35.10 Heterogenenous ConvKV with n-gram ( 1-2-3-4 ) : 35.91 Interleaved heterogeneous ConvKV ( 1-2 ) : 34.70 We also conducted more experiments as per your request : transformer base ( 6 layers ) : 63M params , 26.08 BLEU transformer big ( 6 layers ) : 214M params , 26.63 BLEU transformer base ( 10 layers ) : 91M params , 26.60 BLEU heterogeneous 4 layers ( ConvKV ) : 60M params , 26.63 BLEU heterogeneous 6 layers ( ConvKV ) : 80M params , 27.04 BLEU As you may see , transformer big and transformer base ( 10-layers ) have more parameters ; they should perform better than transformer base . But the margins are not significant because of limited batch size ( explained later ) . On the other hand , our heterogeneous model with just 4 layers is already on par with transformer base 10 layers or transformer big , though it has less parameters . Response continues in the next comment ."}], "0": {"review_id": "r1xN5oA5tm-0", "review_text": "This paper presents an attention mechanism that computes a weighted sum over not only single tokens but ngrams (phrases). Experiments on WMT14 show a slight advantage over token-based attention. The model is elegant and presented very clearly. I really liked the motivation too. Having said that, I am not sold on the claim that phrasal attention actually helps, for two reasons: 1) The comparison to previous results is weak. There are more datasets, models, and hyperparameter settings that need to be tested. 2) Phrasal attention essentially adds an additional convolution layer, i.e. it adds parameters and complexity to the proposed model over the baseline. This needs to be controlled by, for example, adding another transformer block to the baseline model. The question that such an experiment would answer is \"does phrasal attention help more than an extra transformer layer?\" In my view, it is a more interesting question than \"does phrasal attention help more than nothing?\" Also related to concern (2), I think that the authors should check whether the relative improvement from phrasal attention grows/shrinks as a function of the encoder's depth. It could be that deep enough encoders (e.g. 10 layers) already contain some latent representation of phrases, and that this approach mainly benefits shallower architectures (e.g. 2 layers). === MINOR POINTS === If I understand the math behind 3.1.2 correctly, you're first applying a 1x1 conv to K, and then an nx1 conv. Since there's no non-linearity in the middle, isn't this equivalent to the first method? The only difference seems to be that you're assuming the low-rank decomposition fo the bilinear term at a different point (and thus get a different number of parameters, unless d_k = d_q). Have you tried dividing by sqrt(d_k * n) in 3.1.1 too? While the overall model is well explained, I found 3.3 harder to parse.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful reviews . We address your comments as follows . 1/ The comparison to previous results is weak . There are more datasets , models , and hyperparameter settings that need to be tested : We agree that more experiments with other datasets , model variants and hyperparameter settings would make the paper stronger . Due to the lack of GPU machines ( which is common in an academic setting ) , we could perform only a handful of experiments before the submission deadline . However , we continued performing the intended experiments after the deadline . In particular , we have experimented with the English-to-Russian and Russian-to-English translation tasks ( i.e. , more datasets ) , and explored other model variants and hyperparameters ( e.g. , higher order ngrams ) . We revised our paper accordingly . Here is the summary of the new results for your consideration : For En-Ru : Base transformer : 35.64 Homogeneous QueryKernel : 36.31 Heterogeneous ConvKV with n-gram ( 1-2 ) : 36.81 Heterogenenous QueryKernel with n-gram ( 1-2-3-4 ) :37.39 Interleaved heterogeneous QueryKernel ( 1-2 ) : 37.24 For Ru-En Base transformer : 34.56 Homogeneous ConKV : 34.75 Heterogenenous ConvKV with n-gram ( 1-2 ) : 35.10 Heterogenenous ConvKV with n-gram ( 1-2-3-4 ) : 35.91 Interleaved heterogeneous ConvKV ( 1-2 ) : 34.70 We also conducted more experiments as per your request : transformer base ( 6 layers ) : 63M params , 26.08 BLEU transformer big ( 6 layers ) : 214M params , 26.63 BLEU transformer base ( 10 layers ) : 91M params , 26.60 BLEU heterogeneous 4 layers ( ConvKV ) : 60M params , 26.63 BLEU heterogeneous 6 layers ( ConvKV ) : 80M params , 27.04 BLEU As you may see , transformer big and transformer base ( 10-layers ) have more parameters ; they should perform better than transformer base . But the margins are not significant because of limited batch size ( explained later ) . On the other hand , our heterogeneous model with just 4 layers is already on par with transformer base 10 layers or transformer big , though it has less parameters . Please note that the original transformer paper ( Vaswani et al.2017 ) conducted their experiments at a more massive scale ( base and big models were trained with 8 GPUs ) than what we could afford in a common academic lab . There have been many evidences that practical training of the transformer networks ( theirs and ours ) is significantly susceptible to the batch size ( which increases with the number of GPUs used ) , and training a 1-GPU setup for sufficiently long does not produce similar results as an 8-GPU setup ; please see the discussion https : //github.com/tensorflow/tensor2tensor/issues/444 or this material https : //ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf for more details . We compare our model with a version of the transformer that is trained on a single GPU using the same setting and code provided by the authors . In order to make a fair comparison to the baseline , we kept all of our experimental settings identical across all experiments . Thus , the comparison of our model ( 27.40 BLEU ) should be made with this ( 1-GPU ) baseline ( 26.07 BLEU ) as opposed to the reported results ( 27.30 BLEU with 8 GPUs ) in the original paper , because we believe that is not a fair comparison . With due respect , to comment on your statement \u201c Experiments on WMT14 show a slight advantage over token-based attention \u201d , 1.3 BLEU improvements in English-to-German translation task is quite large ( as acknowledged by Reviewer 2 ) . Our new experiments on English-Russian translation tasks show even larger gains ( up to 1.75 BLEU ) compared to the baseline . We urge our reviewers not to penalize our work for not having industry-scale GPU facilities , rather evaluate it based on its scientific merits . We think , the two proposed phrase-based attention methods ( ConvKV and QueryK ) incorporated within the proposed architectures ( homogeneous , heterogeneous , and interleaved ) provide novel and complete solutions to phrasal attentions that as a whole is a significant contribution to the community . Response continue in the next comment ."}, "1": {"review_id": "r1xN5oA5tm-1", "review_text": "Phrase-Based Attention Paper Summary: Neural translation attention computes latent alignments which pairs input/target positions. Phrase-based systems used to align pairs of spans (n-grams) rather than individual positions, this work explores neural architectures to align spans. It does so by compositing attention and convolution operations. It reports empirical results that compares n-gram to uni-gram attention. Review: This paper reads well. It provides appropriate context. The equations are correct. It lacks a few references I mentioned below. The main weaknesses of the work lies in its motivation and in the empirical results. This work motivation ignores an important aspect of neural MT: the vectors that attention compares (\u201cqueries\u201d and \u201ckeys\u201d) do not summarizes a single token/unigram. These vectors aggregate information across nearby positions (convolutional tokens, Ghering et al 2017), all previous tokens (recurrent models, Suskever et al 2014) or the whole source sentence (transformer, Vaswani et al 2017). Moreover multiple layers of attention are composed in modern decoders, comparing vectors which integrates information from both source and target. These vectors cannot be considered as the representation of a single unigram from the source or from the target. The key-value convolution method 3.1.1 is not different from (Ghering et al 2017) which alternates computing convolution and attention multiple times. The query as kernel is a contribution of this work, it is highly related to the concurrent submission to ICLR on dynamic convolution \u201cPay Less Attention with Lightweight and Dynamic Convolutions\u201d. This other work however reports better empirical results over the same benchmark. On empirical results, it seems that the table does not include recent results from work on weighted transformer (Ahmed et al 2017) or relative attention (Shaw et al 2018). Also a 0.1 BLEU improvement over Vaswani et al seems brittle, is your result averaged over multiple runs, could the base transformer be better with as many parameters/updates as your model? Review Summary: This work starts from the premise that current models attends to unigram representation, which is wrong (keys and values already depends on multiple source/target positions). The empirical results are missing recent improvements. The reported empirical advantage compared to baseline is thin. The most interesting contribution is the query as kernel approach: however the concurrent submission \u201cPay Less Attention with Lightweight and Dynamic Convolutions\u201d obtains better empirical results with a similar idea. Missing references: Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. 2017. Weighted transformer network for machine translation. arxiv, 1711.02132. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. of NAACL. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful reviews . We address your comments as follows . 1/ This work motivation ignores an important aspect of neural MT : the vectors that attention compares ( \u201c queries \u201d and \u201c keys \u201d ) do not summarizes a single token/unigram . These vectors aggregate information across nearby positions ( convolutional tokens , Ghering et al 2017 ) , all previous tokens ( recurrent models , Suskever et al 2014 ) or the whole source sentence ( transformer , Vaswani et al 2017 ) . Moreover multiple layers of attention are composed in modern decoders , comparing vectors which integrates information from both source and target . These vectors can not be considered as the representation of a single unigram from the source or from the target . We are sorry if our writing gave you the wrong impression that the vectors for the tokens are computed independently . However , we did NEVER state that in our paper , rather we wrote ( in the Introduction ) : \u201c Despite the advantages , the concept of phrasal attentions has largely been neglected in NMT , as most NMT models generate translations token-by-token autoregressively , and use the token-based attention method which is order invariant . Therefore , the intuition of phrase-based translation is vague in existing NMT systems that solely depend on the underlying neural architectures ( recurrent , convolutional , or self-attention ) to incorporate compositional information. \u201d To elaborate on this , what we mean by \u201c token-based attention \u201d is described by the formula : Softmax ( QK^T ) V , which is , by definition , order-invariant and token-based . This means that if we change the order of the vectors in Q , K and V , there is no difference in the resulting attention scores and the context vectors . However , we did not say that the inputs to this formula ( Q , K , V ) are order invariant ( see Sec.2 ) .Indeed , they can be order-variant , inter-dependent and causal ( in case of the decoder ) . Different architectures have their own ways to ensure that theses inputs have such characteristics before passing to the attention . For instance , recurrent cells encode all previous tokens to the current one , convolutional layers encodes nearby tokens to the current one , self-attention encodes all ( or previous in case of decoder ) vectors from Keys and Values embedded by positional encoding . We hope this clarifies the confusion , and now let us explain why phrasal attention is needed for neural machine translation . It is true that the underlying architectures encode vector representations for tokens by aggregating information across multiple locations . But these vectors represent the respective input \u201c tokens \u201d considering their context . This is similar to ELMo/BERT representation of the tokens where the nearby vectors provide context-relevant clues only to represent the current token , and one does not use this as a representation of the phrase , rather uses it as a representation of the corresponding token . Note that we do not use ELMo/BERT representation of the tokens directly for the prediction tasks , rather they are incorporated into a model to consider the task-specific dependencies often modeled as inductive biases . For example , for NER task , these representations are fed into a bi-LSTM-CRF to model dependencies not only the in the input representations but also in the output sequence ( between NER tags ) . For SQuAD QA task , the representations are used in a bidirectional attention flow network ( BiDAF , 2017 ) to model task-relevant structures . Similarly , for textual entailment , the vectors are used in the ESIM model ( Chen et al.2017 ) that uses a bi-LSTM encoder , followed by a matrix attention layer and inference layers . The main point here is that the token representations learned by ELMo/BERT are incorporated into a model for the target task . Our phrase-based attention methods provide a model for the NMT task to incorporate phrasal alignments ( as explicit inductive bias in the NMT model ) , and this is orthogonal to the underlying representation learning neural architectures . Response continues to next comment ."}, "2": {"review_id": "r1xN5oA5tm-2", "review_text": "The authors propose to include phrases (contiguous n-grams of wordpieces) in both the self-attention and encoder-decoder attention modules of the Transformer model (Vaswani et al., 2017). In standard multi-head attention, the logits of the attention distribution of each head is computed as the dot-product between query and key representations, which are position-specific. In the phrase-based attention proposed here, a convolution is first computed over the query, key, value sequences before logits are computed as before (a few variants of this scheme are explored). Results show an improvement of up to 1.3 BLEU points compared to the baseline Transformer model. However, the lack of a controlled experiment sheds substantial doubt on the efficiacy of the model (see below). Contributions ------------------- Proposes a simple way to incorporate n-grams in the Transformer model. The implementation is straightforward and should be fully replicable in an afternoon. Having an inductive bias towards modeling of longer phrases seems intuitively useful, in particular when using subword representations, where subword units are often ambiguous. This is also motivated by the fact that prior work has shown that subword regularization, where sampling different subword segmentations during training can be useful. Improvements in BLEU scores are quite strong. Issues --------- The experiments do not control for parameter count. The phrasal attention model adds significant number of parameters (e.g., \"interleaved attention\" corresponds to 3x the number of 1D convolution parameters in the attention layer). It is well established that more parameters correspond to increased BLEU scores (e.g., the 2x parameter count in the \"big\" Transformer setting from Vaswani et al. (2017) results in over 1 BLEU point improvement). This needs to be fixed! The model is a very modest extension of the original Transformer model and so its value to the community beyond improved numbers is somewhat questionable. While an explicit inductive bias for phrases seem plausible, it may be that this can already be fully captured by multi-head attention. With positional encodings, two heads can easily attend to adjacent positions which gives the model the same capacity as the convolutional phrase model. The result in the paper that trigrams do not add anything on top of bigrams signals to me that the model is already implicitly capturing phrase-level aspects in the multi-head attention. I would urge the authors to verify this by looking at gradient information (https://arxiv.org/abs/1312.6034). There are several unsubstantiated claims: \"Without specific attention to phrases, a particular attention function has to depend entirely on the token-level softmax scores of a phrase for phrasal alignment, which is not robust and reliable, thus making it more difficult to learn the mappings.\" - The attention is positional, but not necessarily token-based. The model has capacity to represent phrases in subsequent layers. WIth h heads , a position in the k-th layer can in principle represent h^k grams (each slot in layer 2 can represent a h-gram and so on). The differences in training setup compared to Vaswani et al. (2017) needs to be explicit (\"most of the training settings\" is too handwavy). Please list any differences. The notation is somewhat cumbersome and could use some polishing. For example, the input and output symbols both range over indices in [1,n]. The multi-head attention formulas also do not match the ones from Vaswani et al. (2017) fully. Please ensure consistency and readability of symbols and formulas. The model inspection would be much improved by variance analysis. For example, the numbers in table 3 would be more useful if accompanied by variance across training runs. The particular allocation could well be an effect of random initialization. I could also see other reasons for this particular allocation than phrases being more useful in intermediate layers (e.g., positional encodings in the first layer is a strong bias towards token-to-token attention, it could be that the magnitude of convolved vectors is larger than the batch-normalized unigram encodings, so that logits are larger. Questions -------------- In \"query-as-kernel convolution\", it is unclear whether you map Q[t, :] into n x d_q x d_k convolution kernel parameters, or if each element of the window around Q[t] of width n is mapped to a convolution kernel parameter. Also what is the exact form of the transformation. Do you transform the d_q dimensional vectors in Q to a d_q x d_k matrix? Is this done by mapping to a d_q * d_k dimensional vector which is then rearranged into the convolution kernel matrix? Does the model tend to choose one particular n-gram type for a particular position, or will it select different n-gram types for the same position? \"The selection of which n-gram to assign to how many heads is arbitrary\" - How is this arbitrary? This seems a rather strong inductive bias? \"However, the homogeneity restriction may limit the model to learn interactions between different n-gram types\" - How is the case? It seems rather that the limitation is that the model cannot dynamically allocate heads to the most relevant n-gram type? I do not understand equation 14. Do you mean I_dec = I_cross = (...)? \"Phrase-to-phrase mapping helps model local agreement, e.g., between an adjective and a noun (in terms of gender, number and case) or between subject and verb (in terms of person and number).\" Is this actually verified with experiments / model inspection? \"This is especially necessary when the target language is morphologically rich, like German, whose words are usually compounded with sub-words expressing different meanings and grammatical structures\" This claim should be verified, e.g. by comparing to English-French as well as model inspection. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful reviews . We address your comments as follows . 1/ The experiments do not control for parameter count . The phrasal attention model adds significant number of parameters ( e.g. , `` interleaved attention '' corresponds to 3x the number of 1D convolution parameters in the attention layer ) . It is well established that more parameters correspond to increased BLEU scores ( e.g. , the 2x parameter count in the `` big '' Transformer setting from Vaswani et al . ( 2017 ) results in over 1 BLEU point improvement ) . This needs to be fixed ! We have now provided parameter counts of the models with the results in Table 1 in the revised version of the paper . Thanks for the suggestion . However , with due respect , the parameter counts do not match with your estimates ; the base transformer has 63M parameters , the interleaved attention ( our biggest model ) has 116M parameters , while the transformer big has 214M parameters . That is , our interleaved model has approximately 1.8x more parameters than the transformer base , while the transformer big has almost 3.4x more parameters than the base transformer . In addition , what really makes the difference in BLEU scores between Vaswani et al . ( 2017 ) and ours is the batch size ( or the number of GPUs ) used for training , which has been shown empirically to affect the results substantially . Please see the discussion https : //github.com/tensorflow/tensor2tensor/issues/444 or this material https : //ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf for more details .. We also observed the same issue for both the transformer and our phrasal attentions . The transformer used a massive setting of 8 GPUs , while we could only afford to run our experiments on a single GPU . Therefore , we compare our models with the base transformer that is trained with 1 GPU using the same settings and codes provided by the authors . This baseline gives 26.07 BLEU . We claim our gain ( 1.3 BLEU ) from this baseline instead of the reported 8-GPU result to ensure a fair comparison . Having said that , we conducted more experiments on English - Russian and Russian - English and conduct analysis and experiments on the number of parameters relative to base transformer and big transformer . For En-Ru : Base transformer : 35.64 Homogeneous QueryKernel : 36.31 Heterogeneous ConvKV with n-gram ( 1-2 ) : 36.81 Heterogenenous QueryKernel with n-gram ( 1-2-3-4 ) : 37.39 Interleaved heterogeneous QueryKernel ( 1-2 ) : 37.24 For Ru-En Base transformer : 34.56 Homogeneous ConKV : 34.75 Heterogenenous ConvKV with n-gram ( 1-2 ) : 35.10 Heterogenenous ConvKV with n-gram ( 1-2-3-4 ) : 35.91 Interleaved heterogeneous ConvKV ( 1-2 ) : 34.70 We also conducted more experiments as per your request : transformer base ( 6 layers ) : 63M params , 26.08 BLEU transformer big ( 6 layers ) : 214M params , 26.63 BLEU transformer base ( 10 layers ) : 91M params , 26.60 BLEU heterogeneous 4 layers ( ConvKV ) : 60M params , 26.63 BLEU heterogeneous 6 layers ( ConvKV ) : 80M params , 27.04 BLEU As you may see , transformer big and transformer base ( 10-layers ) have more parameters ; they should perform better than transformer base . But the margins are not significant because of limited batch size ( explained later ) . On the other hand , our heterogeneous model with just 4 layers is already on par with transformer base 10 layers or transformer big , though it has less parameters . Response continues in the next comment ."}}