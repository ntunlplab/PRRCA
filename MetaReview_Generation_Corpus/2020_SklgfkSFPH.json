{"year": "2020", "forum": "SklgfkSFPH", "title": "On PAC-Bayes Bounds for Deep Neural Networks using the Loss Curvature", "decision": "Reject", "meta_review": "The paper computes an \"approximate\" generalization bound based on loss curvature. Several expert reviewers found a long list of issues, including missing related work and a sloppy mix of formal statements and heuristics, without proper accounting of what could be gleaned from some many heuristic steps. Ultimately, the paper needs to be rewritten and re-reviewed. ", "reviews": [{"review_id": "SklgfkSFPH-0", "review_text": "The authors replace the empirical risk term in a PAC-Bayes bound by its second-order Taylor series approximation, obtaining an approximate (?) PAC-Bayes bound that depends on the Hessian. Note that the bound is likely overoptimistic unless the minimum is quadratic. They purpose to study SGD by centering the posterior at the weights learned by SGD. The posterior variance that minimizes this approximate PAC-Bayes bound can then be found analytically. They also solve for the optimal prior variance (assuming diagonal Gaussian priors/posteriors), producing a hypothetical \"best possible bound\" (at least under the particular choices of priors/posteriors, and under this approximation of the empirical risk term). The authors evaluate their approximate bound and \"best bound possible\" empirically on MNIST and CIFAR. This requires computing approximations of the Hessian for small fully connected neural networks trained on MNIST and CIFAR10. There are some nice visualizations (indeed, these may be one of the most interesting contributions.) The direction taken by the authors is potentially interesting. However, there are a few issues that would have to be addressed carefully for me to recommend acceptance. First, the comparison to (some very) related work is insufficient, and so the actual novelty is misrepresented (see detailed comments below). Further, the paper is full of questionable vague claims and miscites/attributes other work. At the moment, I think the paper is below the acceptance threshold: the authors need to read and understand (!) related work, and expand their theoretical and/or empirical results to produce a contribution of sufficient novelty/impact. DETAILED FEEDBACK. I believe the authors missed some related work by Tsuzuki, Sato and Sugiyama (2019), where a PAC-Bayes bound was derived in terms of the Hessian, via a second-order approximation. How are the results presented in this submission relate to Tsuzuki et al approach? When the posterior, Q, is a Gaussian (or any other symmetric distribution), \\eta^T H \\eta is the so-called Skilling-Hutchinson trace estimator. Thus E(\\eta^T H \\eta) is the Trace(H) scaled by the variance of \\eta. The authors seem to have completely missed this connection, which simplifies the final expression considerably. Why is the assumption that the higher order terms are negligible reasonable? Citation or experiments required. Regarding the off-diagonal Hessian approximation: how does the proposed layer-wise approximation relate to k-FAC (Martens and Grosse 2015)? IB Lagrangian: I am not sure why the authors state the result in Thm 4.2 as a lower bound on the IB Lagrangian. What\u2019s the significance of having a lower bound on IB Lagrangian? Other comments: Introduction: \u201cAt the same time neither the non-convex optimization problem solved in .. nor the compression schemes employed in \u2026 are guaranteed to converge to a global minimum.\u201d. This is true but it is really not clear what the point being made is. Essentially, so what? Note that PAC-Bayes bounds hold for all posteriors, even ones not centered at the global minimum (of any objective). The claims made in the rest of the paragraph are also questionable and their purposes are equally unclear. I would be grateful if the authors could clarify. First sentence of Section 3.1: \u201cAs the analytical solution for the KL term in 1 obviously underestimates the noise robustness of the deep neural network around the minimum...\u201d. I have no idea what is being claimed here. The statement needs to be made much less vague. Please explain. Section 4: \u201c..while we will be minimizing an upper bound on our objective we will be referring with a slight abuse of terminology to our results as a lower bound.\u201d. I would appreciate if the authors could clarify what they mean here. Section 4.1 beginning: \u201cWe make the following model assumptions...\u201d. Choosing a Gaussian prior and posterior is not an assumption. It's simply a choice. The PAC-Bayes bound is valid for any choices of Gibbs classifiers. On the other hand, it is an assumption that such distributions will yield \"tight\" bounds, related to the work of Alquier et al. Section 4.1 \u201cIn practice we perform a grid search over the parameters..\u201d. The authors should mention that such a search should be accounted for via a union bound (or otherwise). The \"cost\" of such a union bound should be discussed. The empirical risk of Q is computed using 5 MCMC samples. This seems like a very low number, as it would not even give you one decimal point of accuracy with reasonable confidence! The authors should either use more samples, or account for the error in the upper bound using a confidence interval derived from a Chernoff bound. Section 4.2: \u201cThe concept of a valid prior has been formalized under the differential privacy setting...\u201d. I am not sure what the authors mean by that. Section 5: \u201cThere is ambiguity about the size of the Hessians that can be computed exactly.\u201d What kind of ambiguity? Same paragraph in Section 5 discusses why there are few articles on Hessian computation. The authors claim that \u201cthe main problem seems to be that the relevant computations are not well supported...\u201d. This is followed by another comment that is supposed to contrast the previous claim, saying that storing the Hessian is infeasible due to memory requirements. I am not sure how this claim about memory requirements shows a contrast with the claim on computation not being supported. First sentence in Section 5.1: I believe this is only true under some conditions. Section 5.1: The authors should explain why they add a damping term, alpha, to the Hessian, and how alpha affects the results. *** Additional citation issues: The connections between variational inference, PAC-Bayes and IB Lagrangian have been pointed out in previous work (e.g. Germain, Bach, Lacoste, Lacoste-Julien (2016); Achille and Soatto 2017). In the introduction, the authors say \u201c...have been motivated simply by empirical correlations with generalization error; an argument which has been criticized \u2026\u201d (followed by a few citations). Note, that this was first criticized in Dziugaite and Roy (2017). \u201cBoth objectives in \u2026 are however difficult to optimize for anything but small scale experiments.\u201d. It seems peculiar to highlight this, since the approach that the authors are presenting is actually more computationally demanding. Citations for MNIST and CIFAR10 are missing. *** Minor: Theorem 3.1 \u201cFor any data distribution over..\u201d, I think it was meant to be \\mathcal{X} \\times (and not \\in ) Theorem 4.2: \u201cFor our choice of Gaussian prior and posterior, the following is a lower bound on the IB-Lagrangian under any Gaussian prior covariance\u201d. I assume only the mean of the Gaussian prior is fixed. Citations are misplaced (breaking the sentences, unclear when the paper of the authors are cited). There are many (!) missing commas, which makes some sentences hard to follow. *** Positive feedback: I thought the visualizations in Figure 2 and 3 were quite nice. ", "rating": "1: Reject", "reply_text": "The authors are not aware of literature where a researcher has tried to evaluate when computation of the Hessian can be done exactly , and the authors have not conducted experiments on the subject , which is also outside of the scope of the current work . In most works that we are aware of , exact computation of the Hessian is assumed to be infeasible and approximations are used instead . Recently [ 11 ] has claimed that computation of the Hessian is possible in principle but is simply not supported by current autodiff libraries . In short the authors are aware of various claims unsupported by evidence , have not conducted experiments on the subject and therefore can not comment on it ( hence the term `` ambiguity '' ) . The next sentence is meant to be complementary to the previous statements in that the authors can instead comment on how much memory an uncompressed Hessian matrix will take up in RAM . An uncompressed Hessian of moderate size , should require approximately 20GB to store , requiring almost certainly some sort of compression or clever manipulation . This is meant to highlight that dealing with the full Hessian for moderate network sizes should be challenging . The sentence in 5.1 is lifted directly from [ 11 ] which has been recently accepted in NIPS 2019 . The authors are willing to double check , whether the sentence needs to include specific conditions . We do not add a dumping term . From equation 5 in section 4.1 calculating the optimal posterior requires inverting a matrix ( H+\\beta/\\lambda \\Sigma_1^ { -1 } ) . We note that if we choose a zero mean gaussian prior with diagonal and constant covariance this corresponds to inverting a matrix of the form ( H+\\alpha I ) where \\alpha = \\beta/\\lambda . * * * Additional citation issues : We agree to include the work of ( e.g.Germain , Bach , Lacoste , Lacoste-Julien ( 2016 ) . We do not claim to be the first to find a connection between PAC-Bayes and VI . In fact [ 7 ] exploits this connection and we also cite [ 12 ] for exactly this reason . If this is not clear we can explain so in the introduction . We note that the entire paper [ 7 ] is a criticism of empirical correlations , being the first to derive non-vacuous bounds . However the works cited by the authors in this text section attack more fundamental elements of current bounds , specifically mainly uniform convergence , which [ 7 ] does not address in detail . It is for this reason that they are mentioned separately , although we had to remove the more detailed discussion due to space constraints . On `` Both objectives in \u2026 are however difficult to optimize for anything but small scale experiments . `` . We have presented our arguments and what we consider as benefits of our approach in detail earlier in this reply . [ 1 ] Li , Yingzhen , and Yarin Gal . `` Dropout inference in bayesian neural networks with alpha-divergences . '' Proceedings of the 34th International Conference on Machine Learning-Volume 70 . JMLR.org , 2017 . [ 2 ] Gal , Yarin , and Zoubin Ghahramani . `` Dropout as a bayesian approximation : Representing model uncertainty in deep learning . '' international conference on machine learning . 2016 . [ 3 ] Tsuzuku , Yusuke , Issei Sato , and Masashi Sugiyama . `` Normalized Flat Minima : Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis . '' arXiv preprint arXiv:1901.04653 ( 2019 ) . [ 4 ] Dinh , Laurent , et al . `` Sharp minima can generalize for deep nets . '' Proceedings of the 34th International Conference on Machine Learning-Volume 70 . JMLR.org , 2017 . [ 5 ] Kawaguchi , Kenji , Leslie Pack Kaelbling , and Yoshua Bengio . `` Generalization in deep learning . '' arXiv preprint arXiv:1710.05468 ( 2017 ) . [ 6 ] Dong , Xin , Shangyu Chen , and Sinno Pan . `` Learning to prune deep neural networks via layer-wise optimal brain surgeon . '' Advances in Neural Information Processing Systems . 2017 . [ 7 ] Dziugaite , Gintare Karolina , and Daniel M. Roy . `` Computing nonvacuous generalization bounds for deep ( stochastic ) neural networks with many more parameters than training data . '' arXiv preprint arXiv:1703.11008 ( 2017 ) . [ 8 ] Zhou , Wenda , et al . `` Non-vacuous generalization bounds at the imagenet scale : a PAC-bayesian compression approach . '' arXiv preprint arXiv:1804.05862 ( 2018 ) . [ 9 ] Wu , Anqi , et al . `` Deterministic variational inference for robust bayesian neural networks . '' ( 2018 ) . [ 10 ] Neyshabur , Behnam , Srinadh Bhojanapalli , and Nathan Srebro . `` A pac-bayesian approach to spectrally-normalized margin bounds for neural networks . '' arXiv preprint arXiv:1707.09564 ( 2017 ) . [ 11 ] Kunstner , Frederik , Lukas Balles , and Philipp Hennig . `` Limitations of the Empirical Fisher Approximation . '' arXiv preprint arXiv:1905.12558 ( 2019 ) . [ 12 ] Achille , Alessandro , and Stefano Soatto . `` Emergence of invariance and disentanglement in deep representations . '' The Journal of Machine Learning Research 19.1 ( 2018 ) : 1947-1980 ."}, {"review_id": "SklgfkSFPH-1", "review_text": "This paper propose a second-order approximation to the empirical loss in the PAC-Bayes bound of random neural networks. Though the idea is quite straightforward, the paper does a good job in discussing related works and motivating improvements. Two points made about the previous works on PAC-Bayesian bounds for generalization of neural networks (especially Dziugaite & Roy, 2017) are: * Despite non-vacuous, these results are obtained on \"significantly simplified\" datasets and remain \"significantly loose\" * The mean of q after optimizing the PAC-Bayes bound through variational inference is far different from the weights obtained in the original classifier. These points are valid. But it's unclear to me that the proposed method fixes any of them. My concerns are summarized below: * The inequalities are rather arbitrary and not convincing to me. BY Taylor expansion one actually get a lower bound of the right hand side, However the authors write it as first including the higher-order terms, which results in an upper bound, then throwing the higher-order term and arguing the final equation as an approximate upper bound. I believe this can be incorrect when the higher-order terms plays an nonnegligible role. * The theorems are easy algebras and better not presented as theorems. * The proposed diagonal and layer-wise approximation to hessian are very rough estimate of the original Hessian and it is not surprising that it doesn't give meaningful approximation of the original bound. * There is no explicit comparison with previous methods using the same dataset and architecture. It would be much more convincing if the authors include the results of previous works using the same style of figures as Figure 2/3. Minor: * I understand using the invalid bound (optimizing prior) as a sanity check. But the presentation in the paper could better be improved by explaining why doing this. * Do the plots in Figure 2 correspond to the invalid or valid bound? * Many papers are complaining that Hessian computation is difficult in autodff libs without noticing this is a fundamental limitation of these reverse-mode autodiff libraries and no easy fix exists. * I believe MCMC is not used and the authors are refering to MC (page 7, first paragraph). ", "rating": "3: Weak Reject", "reply_text": "Thank you for your detailed review . Major : In the paper we make a number of approximations which we do not claim are tight . We substitute the PAC-Bayesian objective with the IB-Lagrangian , we then approximate the IB-Lagrangian using a second order Taylor expansion of the loss . We then prove some formal results for the second order taylor expansion . These might or might not translate to the original PAC-Bayes objective . We believe our experiments show that to some extent the results are meaningful . Furthermore there is ample evidence in the literature that although a second order Taylor expansion of the loss around a minimum is loose it can be quite informative . In particular there has been a long line of research in the literature of DNN compression where a second order Taylor expansion of the loss has produced state of the art results in parameter number reduction . We refer to the following which include a number of well known researchers and conferences : i ) Dong , Xin , Shangyu Chen , and Sinno Pan . `` Learning to prune deep neural networks via layer-wise optimal brain surgeon . '' Advances in Neural Information Processing Systems . 2017. ii ) Wang , Chaoqi , et al . `` EigenDamage : Structured Pruning in the Kronecker-Factored Eigenbasis . '' International Conference on Machine Learning . 2019. iii ) Peng , Hanyu , et al . `` Collaborative Channel Pruning for Deep Networks . '' International Conference on Machine Learning . 2019. iv ) LeCun , Yann , John S. Denker , and Sara A. Solla . `` Optimal brain damage . '' Advances in neural information processing systems . 1990. v ) Hassibi , Babak , and David G. Stork . `` Second order derivatives for network pruning : Optimal brain surgeon . '' Advances in neural information processing systems . 1993 . .Correspondingly the approximation while almost certainly not tight has proven quite useful and meaningful . Concerning substituting the PAC-Bayes objective for the IB-Lagrangian we note that in [ 1 ] page 8 , section 6 , the authors mention that they have used the PAC Bayes bound and the IB-Lagrangian interchangably when optimising and did n't notice a difference in results . We do not object to changing the theorems to lemmas . We agree that comparison to [ 1 ] would be useful ( assuming that we fix the posterior mean and only optimise for the variance using non-convex optimisation ) however we note that , this requires careful experimentation and hyperparameter tuning which is certainly not trivial . Minor : Yes the plots correspond to the invalid prior see section 4.2 page 6 for details . We would be interested in any literature relating to limitations of reverse-mode autodiff libraries . MC is indeed the correct term . [ 1 ] Dziugaite , Gintare Karolina , and Daniel M. Roy . `` Computing nonvacuous generalization bounds for deep ( stochastic ) neural networks with many more parameters than training data . '' arXiv preprint arXiv:1703.11008 ( 2017 ) ."}, {"review_id": "SklgfkSFPH-2", "review_text": "Summary: The paper provides several approximations of PAC-Bayes generalization bounds for Gaussian prior and posterior distributions, with various restrictions on the covariance matrices. In particular, the paper: (1) Assumes that the expectation of the loss can be Taylor expanded around each point in the support, and all but the quadratic (Hessian) term can be ignored. (2) Proves a lower bound on the PAC-Bayes generalization objective. (3) Provides an upper bound on the PAC-Bayes objective via a \"layerwise\" Hessian objective. Evaluation: I found this paper extremely difficult to follow because it's sloppy in various places -- both in terms of what claims are formal, and what are heuristic approximations -- and in terms of properly defining crucial quantities. I will go in the same numbered order in which I listed the main contributions above: (1) (Taylor approximation): The equation (4) is an *approximation* -- not a lower or upper bound. Moreover, too little is said about this heuristic: note that the authors actually Taylor expand an *expectation* over \\theta -- the trivial thing to require for this Taylor approximation to hold is that it holds for *every* theta which clearly will not be true. It seems the authors want to say the distribution Q concentrates over thetas close to some local optimum \\theta^*, and over these thetas the approximation holds. At the very least something needs to be said about how much things need to concentrate and whether this is realistic in real-life settings. Also, because (4) is an approximation, it's a little disengenuous to call Theorems 4.2 and 5.2 \"theorems\", and it needs to be mentioned in the statements that they hold under some formalization of the approximation I described above. (2) The lower bound is written very oddly -- the \"prior\" for the lower bound is really dependent upon the posterior -- so it is very strange to call it an \"invalid\" prior. Moreover, I have serious problems evaluating the meaning of this lower bound -- as it uses the Taylor approximation from (1), but then decides to instantiate the prior *depending on the optimum* of this Taylor approximation. As such, *at the very least* -- some small neural net examples should be tried where the normal (un-approximated) KL bound can be evaluated, to check whether this *actually* is a lower bound most of the time. (3) The upper bound is also written rather sloppily: Q_{lj} is never defined; H_{lj} only depends on l, rather than j -- in fact, I'm fairly sure it should be H_l, and \\eta should be sampled from Q_{l} (i.e. a vector with a coordinate for each neuron in layer l) if I understood the proof correctly. ", "rating": "1: Reject", "reply_text": "Thank you for your detailed review . 1 ) The Taylor expansion of the loss is indeed an approximation in general . For the specific case of a DNN solution , assuming that we have reached a local minimum , and that the loss function is locally convex a second order expansion can be seen as an upper bound to the loss . However we do not make this formal , nor do we think that it is easy or useful to state strict conditions on whether the approximation is an upper bound . We point however to a long line of work in the DNN compression literature where a second order Taylor expansion of the loss has led to state of the art results in DNN compression . i ) Dong , Xin , Shangyu Chen , and Sinno Pan . `` Learning to prune deep neural networks via layer-wise optimal brain surgeon . '' Advances in Neural Information Processing Systems . 2017. ii ) Wang , Chaoqi , et al . `` EigenDamage : Structured Pruning in the Kronecker-Factored Eigenbasis . '' International Conference on Machine Learning . 2019. iii ) Peng , Hanyu , et al . `` Collaborative Channel Pruning for Deep Networks . '' International Conference on Machine Learning . 2019. iv ) LeCun , Yann , John S. Denker , and Sara A. Solla . `` Optimal brain damage . '' Advances in neural information processing systems . 1990. v ) Hassibi , Babak , and David G. Stork . `` Second order derivatives for network pruning : Optimal brain surgeon . '' Advances in neural information processing systems . 1993.The distribution over parameters in the Gaussian posterior case with diagonal covariance should be highly concentrated around the mean making the approximation meaningful . 2 ) We call the prior `` invalid '' in that through the way we calculate it , it depends on the posterior . This is not allowed in the PAC-Bayes framework , the prior has to be independent of the training set . Note that even though we calculate a invalid posterior based on the second order Taylor expansion of the IB-Lagrangrian , it remains invalid for the PAC-Bayes bound . In section 4.2 we make a detailed discussion , regarding the benefits and limitations of this result . In particular after one has computed an optimal posterior using equation 5 , seeing that the result is non-vacuous or loose one may be tempted to search for a better prior , for example through a separate training set . Our result using the invalid prior corresponds to a lower bound for equation 4 ( the second order taylor expansion of the IB-Lagrangian ) . We can trace a corresponding feasible region vs non-vacuous region using this lower bound ( Figure 2 ) . Thus if the two regions do n't overlap we should not be able to prove generalization even if we chose a better prior in a valid manner . To be precise what we can show is that we can not minimize the IB-Lagrangian second order approximation further . Ideally we would like these results to translate to the PAC-Bayes theorem directly ( we would like the feasible regions to be meaningful for equation 1 even though we calculated them through equation 4 ) , however we can not prove this formally and have to rely on experiments . In practice we have found the calculated feasible region to be meaningful , the baseline and the diagonal gaussian posteriors fail to cross it . The non-diagonal posterior crosses it slightly . 3 ) Q_ { lj } = \\mathcal { N } ( \\mu_ { 0lj } , \\Sigma_ { 0lj } ) and the dimension number of the multivariate Gaussian is equal to the dimensionality of the input to the layer `` l '' . While H_ { l1 } =H_ { l2 } = ... =H_ { l * } = \\frac { 1 } { N } \\sum_i z^i_ { l-1 } z^i_ { l-1 } ^T it is not true that H_ { lj } = H_ { l } . H_ { lj } is a local Hessian for each neuron therefore has a dimensions equal to layer_input_dims \\times layer_input_dims . H_ { l } is the local Hessian of the layer and therefore has dimensions equal to ( layer_input_dims * layer_output_dims ) \\times ( layer_input_dims * layer_output_dims ) . H_ { l } is block diagonal with blocks H_ { lj } . Note that the layerwise approximation of section 5.2 is only a heuristic which is independent from the rest of our analysis and is mainly used to motivate solving for the optimal posterior in an layerwise manner , which is in general a quite cheap calculation . In fact one can completely ignore section 5.2 and perform most experiments by computing a diagonal Hessian using equation 9 ."}], "0": {"review_id": "SklgfkSFPH-0", "review_text": "The authors replace the empirical risk term in a PAC-Bayes bound by its second-order Taylor series approximation, obtaining an approximate (?) PAC-Bayes bound that depends on the Hessian. Note that the bound is likely overoptimistic unless the minimum is quadratic. They purpose to study SGD by centering the posterior at the weights learned by SGD. The posterior variance that minimizes this approximate PAC-Bayes bound can then be found analytically. They also solve for the optimal prior variance (assuming diagonal Gaussian priors/posteriors), producing a hypothetical \"best possible bound\" (at least under the particular choices of priors/posteriors, and under this approximation of the empirical risk term). The authors evaluate their approximate bound and \"best bound possible\" empirically on MNIST and CIFAR. This requires computing approximations of the Hessian for small fully connected neural networks trained on MNIST and CIFAR10. There are some nice visualizations (indeed, these may be one of the most interesting contributions.) The direction taken by the authors is potentially interesting. However, there are a few issues that would have to be addressed carefully for me to recommend acceptance. First, the comparison to (some very) related work is insufficient, and so the actual novelty is misrepresented (see detailed comments below). Further, the paper is full of questionable vague claims and miscites/attributes other work. At the moment, I think the paper is below the acceptance threshold: the authors need to read and understand (!) related work, and expand their theoretical and/or empirical results to produce a contribution of sufficient novelty/impact. DETAILED FEEDBACK. I believe the authors missed some related work by Tsuzuki, Sato and Sugiyama (2019), where a PAC-Bayes bound was derived in terms of the Hessian, via a second-order approximation. How are the results presented in this submission relate to Tsuzuki et al approach? When the posterior, Q, is a Gaussian (or any other symmetric distribution), \\eta^T H \\eta is the so-called Skilling-Hutchinson trace estimator. Thus E(\\eta^T H \\eta) is the Trace(H) scaled by the variance of \\eta. The authors seem to have completely missed this connection, which simplifies the final expression considerably. Why is the assumption that the higher order terms are negligible reasonable? Citation or experiments required. Regarding the off-diagonal Hessian approximation: how does the proposed layer-wise approximation relate to k-FAC (Martens and Grosse 2015)? IB Lagrangian: I am not sure why the authors state the result in Thm 4.2 as a lower bound on the IB Lagrangian. What\u2019s the significance of having a lower bound on IB Lagrangian? Other comments: Introduction: \u201cAt the same time neither the non-convex optimization problem solved in .. nor the compression schemes employed in \u2026 are guaranteed to converge to a global minimum.\u201d. This is true but it is really not clear what the point being made is. Essentially, so what? Note that PAC-Bayes bounds hold for all posteriors, even ones not centered at the global minimum (of any objective). The claims made in the rest of the paragraph are also questionable and their purposes are equally unclear. I would be grateful if the authors could clarify. First sentence of Section 3.1: \u201cAs the analytical solution for the KL term in 1 obviously underestimates the noise robustness of the deep neural network around the minimum...\u201d. I have no idea what is being claimed here. The statement needs to be made much less vague. Please explain. Section 4: \u201c..while we will be minimizing an upper bound on our objective we will be referring with a slight abuse of terminology to our results as a lower bound.\u201d. I would appreciate if the authors could clarify what they mean here. Section 4.1 beginning: \u201cWe make the following model assumptions...\u201d. Choosing a Gaussian prior and posterior is not an assumption. It's simply a choice. The PAC-Bayes bound is valid for any choices of Gibbs classifiers. On the other hand, it is an assumption that such distributions will yield \"tight\" bounds, related to the work of Alquier et al. Section 4.1 \u201cIn practice we perform a grid search over the parameters..\u201d. The authors should mention that such a search should be accounted for via a union bound (or otherwise). The \"cost\" of such a union bound should be discussed. The empirical risk of Q is computed using 5 MCMC samples. This seems like a very low number, as it would not even give you one decimal point of accuracy with reasonable confidence! The authors should either use more samples, or account for the error in the upper bound using a confidence interval derived from a Chernoff bound. Section 4.2: \u201cThe concept of a valid prior has been formalized under the differential privacy setting...\u201d. I am not sure what the authors mean by that. Section 5: \u201cThere is ambiguity about the size of the Hessians that can be computed exactly.\u201d What kind of ambiguity? Same paragraph in Section 5 discusses why there are few articles on Hessian computation. The authors claim that \u201cthe main problem seems to be that the relevant computations are not well supported...\u201d. This is followed by another comment that is supposed to contrast the previous claim, saying that storing the Hessian is infeasible due to memory requirements. I am not sure how this claim about memory requirements shows a contrast with the claim on computation not being supported. First sentence in Section 5.1: I believe this is only true under some conditions. Section 5.1: The authors should explain why they add a damping term, alpha, to the Hessian, and how alpha affects the results. *** Additional citation issues: The connections between variational inference, PAC-Bayes and IB Lagrangian have been pointed out in previous work (e.g. Germain, Bach, Lacoste, Lacoste-Julien (2016); Achille and Soatto 2017). In the introduction, the authors say \u201c...have been motivated simply by empirical correlations with generalization error; an argument which has been criticized \u2026\u201d (followed by a few citations). Note, that this was first criticized in Dziugaite and Roy (2017). \u201cBoth objectives in \u2026 are however difficult to optimize for anything but small scale experiments.\u201d. It seems peculiar to highlight this, since the approach that the authors are presenting is actually more computationally demanding. Citations for MNIST and CIFAR10 are missing. *** Minor: Theorem 3.1 \u201cFor any data distribution over..\u201d, I think it was meant to be \\mathcal{X} \\times (and not \\in ) Theorem 4.2: \u201cFor our choice of Gaussian prior and posterior, the following is a lower bound on the IB-Lagrangian under any Gaussian prior covariance\u201d. I assume only the mean of the Gaussian prior is fixed. Citations are misplaced (breaking the sentences, unclear when the paper of the authors are cited). There are many (!) missing commas, which makes some sentences hard to follow. *** Positive feedback: I thought the visualizations in Figure 2 and 3 were quite nice. ", "rating": "1: Reject", "reply_text": "The authors are not aware of literature where a researcher has tried to evaluate when computation of the Hessian can be done exactly , and the authors have not conducted experiments on the subject , which is also outside of the scope of the current work . In most works that we are aware of , exact computation of the Hessian is assumed to be infeasible and approximations are used instead . Recently [ 11 ] has claimed that computation of the Hessian is possible in principle but is simply not supported by current autodiff libraries . In short the authors are aware of various claims unsupported by evidence , have not conducted experiments on the subject and therefore can not comment on it ( hence the term `` ambiguity '' ) . The next sentence is meant to be complementary to the previous statements in that the authors can instead comment on how much memory an uncompressed Hessian matrix will take up in RAM . An uncompressed Hessian of moderate size , should require approximately 20GB to store , requiring almost certainly some sort of compression or clever manipulation . This is meant to highlight that dealing with the full Hessian for moderate network sizes should be challenging . The sentence in 5.1 is lifted directly from [ 11 ] which has been recently accepted in NIPS 2019 . The authors are willing to double check , whether the sentence needs to include specific conditions . We do not add a dumping term . From equation 5 in section 4.1 calculating the optimal posterior requires inverting a matrix ( H+\\beta/\\lambda \\Sigma_1^ { -1 } ) . We note that if we choose a zero mean gaussian prior with diagonal and constant covariance this corresponds to inverting a matrix of the form ( H+\\alpha I ) where \\alpha = \\beta/\\lambda . * * * Additional citation issues : We agree to include the work of ( e.g.Germain , Bach , Lacoste , Lacoste-Julien ( 2016 ) . We do not claim to be the first to find a connection between PAC-Bayes and VI . In fact [ 7 ] exploits this connection and we also cite [ 12 ] for exactly this reason . If this is not clear we can explain so in the introduction . We note that the entire paper [ 7 ] is a criticism of empirical correlations , being the first to derive non-vacuous bounds . However the works cited by the authors in this text section attack more fundamental elements of current bounds , specifically mainly uniform convergence , which [ 7 ] does not address in detail . It is for this reason that they are mentioned separately , although we had to remove the more detailed discussion due to space constraints . On `` Both objectives in \u2026 are however difficult to optimize for anything but small scale experiments . `` . We have presented our arguments and what we consider as benefits of our approach in detail earlier in this reply . [ 1 ] Li , Yingzhen , and Yarin Gal . `` Dropout inference in bayesian neural networks with alpha-divergences . '' Proceedings of the 34th International Conference on Machine Learning-Volume 70 . JMLR.org , 2017 . [ 2 ] Gal , Yarin , and Zoubin Ghahramani . `` Dropout as a bayesian approximation : Representing model uncertainty in deep learning . '' international conference on machine learning . 2016 . [ 3 ] Tsuzuku , Yusuke , Issei Sato , and Masashi Sugiyama . `` Normalized Flat Minima : Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis . '' arXiv preprint arXiv:1901.04653 ( 2019 ) . [ 4 ] Dinh , Laurent , et al . `` Sharp minima can generalize for deep nets . '' Proceedings of the 34th International Conference on Machine Learning-Volume 70 . JMLR.org , 2017 . [ 5 ] Kawaguchi , Kenji , Leslie Pack Kaelbling , and Yoshua Bengio . `` Generalization in deep learning . '' arXiv preprint arXiv:1710.05468 ( 2017 ) . [ 6 ] Dong , Xin , Shangyu Chen , and Sinno Pan . `` Learning to prune deep neural networks via layer-wise optimal brain surgeon . '' Advances in Neural Information Processing Systems . 2017 . [ 7 ] Dziugaite , Gintare Karolina , and Daniel M. Roy . `` Computing nonvacuous generalization bounds for deep ( stochastic ) neural networks with many more parameters than training data . '' arXiv preprint arXiv:1703.11008 ( 2017 ) . [ 8 ] Zhou , Wenda , et al . `` Non-vacuous generalization bounds at the imagenet scale : a PAC-bayesian compression approach . '' arXiv preprint arXiv:1804.05862 ( 2018 ) . [ 9 ] Wu , Anqi , et al . `` Deterministic variational inference for robust bayesian neural networks . '' ( 2018 ) . [ 10 ] Neyshabur , Behnam , Srinadh Bhojanapalli , and Nathan Srebro . `` A pac-bayesian approach to spectrally-normalized margin bounds for neural networks . '' arXiv preprint arXiv:1707.09564 ( 2017 ) . [ 11 ] Kunstner , Frederik , Lukas Balles , and Philipp Hennig . `` Limitations of the Empirical Fisher Approximation . '' arXiv preprint arXiv:1905.12558 ( 2019 ) . [ 12 ] Achille , Alessandro , and Stefano Soatto . `` Emergence of invariance and disentanglement in deep representations . '' The Journal of Machine Learning Research 19.1 ( 2018 ) : 1947-1980 ."}, "1": {"review_id": "SklgfkSFPH-1", "review_text": "This paper propose a second-order approximation to the empirical loss in the PAC-Bayes bound of random neural networks. Though the idea is quite straightforward, the paper does a good job in discussing related works and motivating improvements. Two points made about the previous works on PAC-Bayesian bounds for generalization of neural networks (especially Dziugaite & Roy, 2017) are: * Despite non-vacuous, these results are obtained on \"significantly simplified\" datasets and remain \"significantly loose\" * The mean of q after optimizing the PAC-Bayes bound through variational inference is far different from the weights obtained in the original classifier. These points are valid. But it's unclear to me that the proposed method fixes any of them. My concerns are summarized below: * The inequalities are rather arbitrary and not convincing to me. BY Taylor expansion one actually get a lower bound of the right hand side, However the authors write it as first including the higher-order terms, which results in an upper bound, then throwing the higher-order term and arguing the final equation as an approximate upper bound. I believe this can be incorrect when the higher-order terms plays an nonnegligible role. * The theorems are easy algebras and better not presented as theorems. * The proposed diagonal and layer-wise approximation to hessian are very rough estimate of the original Hessian and it is not surprising that it doesn't give meaningful approximation of the original bound. * There is no explicit comparison with previous methods using the same dataset and architecture. It would be much more convincing if the authors include the results of previous works using the same style of figures as Figure 2/3. Minor: * I understand using the invalid bound (optimizing prior) as a sanity check. But the presentation in the paper could better be improved by explaining why doing this. * Do the plots in Figure 2 correspond to the invalid or valid bound? * Many papers are complaining that Hessian computation is difficult in autodff libs without noticing this is a fundamental limitation of these reverse-mode autodiff libraries and no easy fix exists. * I believe MCMC is not used and the authors are refering to MC (page 7, first paragraph). ", "rating": "3: Weak Reject", "reply_text": "Thank you for your detailed review . Major : In the paper we make a number of approximations which we do not claim are tight . We substitute the PAC-Bayesian objective with the IB-Lagrangian , we then approximate the IB-Lagrangian using a second order Taylor expansion of the loss . We then prove some formal results for the second order taylor expansion . These might or might not translate to the original PAC-Bayes objective . We believe our experiments show that to some extent the results are meaningful . Furthermore there is ample evidence in the literature that although a second order Taylor expansion of the loss around a minimum is loose it can be quite informative . In particular there has been a long line of research in the literature of DNN compression where a second order Taylor expansion of the loss has produced state of the art results in parameter number reduction . We refer to the following which include a number of well known researchers and conferences : i ) Dong , Xin , Shangyu Chen , and Sinno Pan . `` Learning to prune deep neural networks via layer-wise optimal brain surgeon . '' Advances in Neural Information Processing Systems . 2017. ii ) Wang , Chaoqi , et al . `` EigenDamage : Structured Pruning in the Kronecker-Factored Eigenbasis . '' International Conference on Machine Learning . 2019. iii ) Peng , Hanyu , et al . `` Collaborative Channel Pruning for Deep Networks . '' International Conference on Machine Learning . 2019. iv ) LeCun , Yann , John S. Denker , and Sara A. Solla . `` Optimal brain damage . '' Advances in neural information processing systems . 1990. v ) Hassibi , Babak , and David G. Stork . `` Second order derivatives for network pruning : Optimal brain surgeon . '' Advances in neural information processing systems . 1993 . .Correspondingly the approximation while almost certainly not tight has proven quite useful and meaningful . Concerning substituting the PAC-Bayes objective for the IB-Lagrangian we note that in [ 1 ] page 8 , section 6 , the authors mention that they have used the PAC Bayes bound and the IB-Lagrangian interchangably when optimising and did n't notice a difference in results . We do not object to changing the theorems to lemmas . We agree that comparison to [ 1 ] would be useful ( assuming that we fix the posterior mean and only optimise for the variance using non-convex optimisation ) however we note that , this requires careful experimentation and hyperparameter tuning which is certainly not trivial . Minor : Yes the plots correspond to the invalid prior see section 4.2 page 6 for details . We would be interested in any literature relating to limitations of reverse-mode autodiff libraries . MC is indeed the correct term . [ 1 ] Dziugaite , Gintare Karolina , and Daniel M. Roy . `` Computing nonvacuous generalization bounds for deep ( stochastic ) neural networks with many more parameters than training data . '' arXiv preprint arXiv:1703.11008 ( 2017 ) ."}, "2": {"review_id": "SklgfkSFPH-2", "review_text": "Summary: The paper provides several approximations of PAC-Bayes generalization bounds for Gaussian prior and posterior distributions, with various restrictions on the covariance matrices. In particular, the paper: (1) Assumes that the expectation of the loss can be Taylor expanded around each point in the support, and all but the quadratic (Hessian) term can be ignored. (2) Proves a lower bound on the PAC-Bayes generalization objective. (3) Provides an upper bound on the PAC-Bayes objective via a \"layerwise\" Hessian objective. Evaluation: I found this paper extremely difficult to follow because it's sloppy in various places -- both in terms of what claims are formal, and what are heuristic approximations -- and in terms of properly defining crucial quantities. I will go in the same numbered order in which I listed the main contributions above: (1) (Taylor approximation): The equation (4) is an *approximation* -- not a lower or upper bound. Moreover, too little is said about this heuristic: note that the authors actually Taylor expand an *expectation* over \\theta -- the trivial thing to require for this Taylor approximation to hold is that it holds for *every* theta which clearly will not be true. It seems the authors want to say the distribution Q concentrates over thetas close to some local optimum \\theta^*, and over these thetas the approximation holds. At the very least something needs to be said about how much things need to concentrate and whether this is realistic in real-life settings. Also, because (4) is an approximation, it's a little disengenuous to call Theorems 4.2 and 5.2 \"theorems\", and it needs to be mentioned in the statements that they hold under some formalization of the approximation I described above. (2) The lower bound is written very oddly -- the \"prior\" for the lower bound is really dependent upon the posterior -- so it is very strange to call it an \"invalid\" prior. Moreover, I have serious problems evaluating the meaning of this lower bound -- as it uses the Taylor approximation from (1), but then decides to instantiate the prior *depending on the optimum* of this Taylor approximation. As such, *at the very least* -- some small neural net examples should be tried where the normal (un-approximated) KL bound can be evaluated, to check whether this *actually* is a lower bound most of the time. (3) The upper bound is also written rather sloppily: Q_{lj} is never defined; H_{lj} only depends on l, rather than j -- in fact, I'm fairly sure it should be H_l, and \\eta should be sampled from Q_{l} (i.e. a vector with a coordinate for each neuron in layer l) if I understood the proof correctly. ", "rating": "1: Reject", "reply_text": "Thank you for your detailed review . 1 ) The Taylor expansion of the loss is indeed an approximation in general . For the specific case of a DNN solution , assuming that we have reached a local minimum , and that the loss function is locally convex a second order expansion can be seen as an upper bound to the loss . However we do not make this formal , nor do we think that it is easy or useful to state strict conditions on whether the approximation is an upper bound . We point however to a long line of work in the DNN compression literature where a second order Taylor expansion of the loss has led to state of the art results in DNN compression . i ) Dong , Xin , Shangyu Chen , and Sinno Pan . `` Learning to prune deep neural networks via layer-wise optimal brain surgeon . '' Advances in Neural Information Processing Systems . 2017. ii ) Wang , Chaoqi , et al . `` EigenDamage : Structured Pruning in the Kronecker-Factored Eigenbasis . '' International Conference on Machine Learning . 2019. iii ) Peng , Hanyu , et al . `` Collaborative Channel Pruning for Deep Networks . '' International Conference on Machine Learning . 2019. iv ) LeCun , Yann , John S. Denker , and Sara A. Solla . `` Optimal brain damage . '' Advances in neural information processing systems . 1990. v ) Hassibi , Babak , and David G. Stork . `` Second order derivatives for network pruning : Optimal brain surgeon . '' Advances in neural information processing systems . 1993.The distribution over parameters in the Gaussian posterior case with diagonal covariance should be highly concentrated around the mean making the approximation meaningful . 2 ) We call the prior `` invalid '' in that through the way we calculate it , it depends on the posterior . This is not allowed in the PAC-Bayes framework , the prior has to be independent of the training set . Note that even though we calculate a invalid posterior based on the second order Taylor expansion of the IB-Lagrangrian , it remains invalid for the PAC-Bayes bound . In section 4.2 we make a detailed discussion , regarding the benefits and limitations of this result . In particular after one has computed an optimal posterior using equation 5 , seeing that the result is non-vacuous or loose one may be tempted to search for a better prior , for example through a separate training set . Our result using the invalid prior corresponds to a lower bound for equation 4 ( the second order taylor expansion of the IB-Lagrangian ) . We can trace a corresponding feasible region vs non-vacuous region using this lower bound ( Figure 2 ) . Thus if the two regions do n't overlap we should not be able to prove generalization even if we chose a better prior in a valid manner . To be precise what we can show is that we can not minimize the IB-Lagrangian second order approximation further . Ideally we would like these results to translate to the PAC-Bayes theorem directly ( we would like the feasible regions to be meaningful for equation 1 even though we calculated them through equation 4 ) , however we can not prove this formally and have to rely on experiments . In practice we have found the calculated feasible region to be meaningful , the baseline and the diagonal gaussian posteriors fail to cross it . The non-diagonal posterior crosses it slightly . 3 ) Q_ { lj } = \\mathcal { N } ( \\mu_ { 0lj } , \\Sigma_ { 0lj } ) and the dimension number of the multivariate Gaussian is equal to the dimensionality of the input to the layer `` l '' . While H_ { l1 } =H_ { l2 } = ... =H_ { l * } = \\frac { 1 } { N } \\sum_i z^i_ { l-1 } z^i_ { l-1 } ^T it is not true that H_ { lj } = H_ { l } . H_ { lj } is a local Hessian for each neuron therefore has a dimensions equal to layer_input_dims \\times layer_input_dims . H_ { l } is the local Hessian of the layer and therefore has dimensions equal to ( layer_input_dims * layer_output_dims ) \\times ( layer_input_dims * layer_output_dims ) . H_ { l } is block diagonal with blocks H_ { lj } . Note that the layerwise approximation of section 5.2 is only a heuristic which is independent from the rest of our analysis and is mainly used to motivate solving for the optimal posterior in an layerwise manner , which is in general a quite cheap calculation . In fact one can completely ignore section 5.2 and perform most experiments by computing a diagonal Hessian using equation 9 ."}}