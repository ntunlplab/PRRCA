{"year": "2021", "forum": "d8Q1mt2Ghw", "title": "Emergent Road Rules In Multi-Agent Driving Environments", "decision": "Accept (Poster)", "meta_review": "This paper shows how \"road rules\" (e.g., implicit designation of fast lanes on a highway) naturally emerge in a multi-agent MDP. The paper shows that interesting traffic rules do emerge, and it presents a detailed analysis of the factors that lead to this emergence. The paper is complemented by documented source code, with the aim to encourage the community to further work on the topic.\n\nThe reviewers agreed that this is original work, and appreciated its simplicity. Two concerns that were recurrently voiced were that 1) there is no algorithmic innovation and 2) there is no comparison to baseline models, or more generally a better placement in the context of existing literature.\n\nThe authors provided a detailed and, to my eyes, convincing response. With respect to the two concerns above, I would go as far as saying that 1) (no algorithmic innovation) is a feature, not a bug. The paper is interesting exactly because it studies emergent phenomena after framing multi-agent driving as a standard RL problem. Concerning 2) (lack of baselines), it seems to me somewhat besides the point: The paper is not claiming state of the art on some benchmark for a new algorithm, but studying how certain implicit rules emerge in a given setup. In this sense, as the authors point out, rather than looking at alternative baselines, it is informative to look at which aspects of the setup contribute to rule emergence, which is what the paper does.\n\nAlthough I realize that in proposing this I am going beyond the reviewers' ratings, I found this to be an original and exciting paper, that I would strongly like to see accepted at the conference.\n", "reviews": [{"review_id": "d8Q1mt2Ghw-0", "review_text": "This paper investigate how to design simulation environments so the the agent trained with them can master social rules . Cons : 1.The paper is well written and easy to read and understand . Thanks ! 2.The experiments are solid and well defined . My major concern of this paper are : 1 . The authors seems to only considered the noise of sensors and the number of agents , and these two factors happened to induce social behaviors like following lanes and stopping at traffic signals . In other words , I am not quite convinced that with all vehicles being automated , sensor noise will cause them to formulate rules that what human drivers follow today . Since human driving interactions are complex , I do not think that sensor noise would be enough to induce them . 2.I would be happy to see how this configuration of simulation environment compares with reward guided social behaviors . For example , we can design a reward to encourage agents follow right of way . I think this way might be more direct and powerful . 2.The number of agents seems to be too small and may affect the formulation of road social rules . 3.It seems like the agents did not take traffic signal status as input for the action selection , then how did they formulate the signal control rules . 4.Figures 2 and 3 needs better explanations for readers to understand . Overall , I think this paper needs better formulation of the logics and also a deep investigation of how the simulation variations lead to those behaviors .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . We are happy to hear that the reviewer found our paper \u201c well-written \u201d and our experiments \u201c solid and well defined \u201d . As mentioned in our comment to all reviewers , we would like to emphasize that while prior work has trained agents to maximize similarity to observed human driving trajectories ( e.g.imitation learning ) , our contribution is to show that when we optimize for traffic flow , we find that agents exhibit many behaviors that have generally been assumed to require human demonstrations to learn . We address the reviewer 's comments below . 1. \u201c Since human driving interactions are complex ... \u201d - We agree with the reviewer that certain human driving behavior can not be modeled purely as emergent phenomena in a partially-observable multi-agent system in which all users seek to go from $ a\\rightarrow b $ as fast as possible without colliding . For instance , we would never expect parades or funeral processions to emerge in our environments . However , our goal in this paper was to show that standard human driving behavior such as lane following or traffic light usage are optimal under the simple reward function of minimizing the time it takes to reach randomly sampled destinations without colliding with other agents . At larger length and time scales , we expect the reward function and transition dynamics used in this paper to result in other phenomena such as the emergence of lane changes , protected lefts , and one-way roads ( which civil engineers have also determined maximize traffic flow in settings like New York City , for instance ) . Potentially , our framework could also be used to discover new driving behavior that could be adopted by human drivers to improve safety and efficiency . 2. \u201c We can design a reward to encourage agents follow the right of way \u201d - Designing complicated reward functions is what we seek to avoid in this work . Our goal is to show that the simple reward function of minimizing the time it takes agents to reach randomly sampled destinations results in much of the standard driving behavior observed in human driving systems . To clarify the purpose of the metrics plotted in our Experiment section , we use these metrics to quantitatively evaluate the extent to which the behavior of the agents trained in our MDP mimic the behavior of human drivers - not as metrics that we explicitly hope to optimize . In reality , humans themselves are unlikely to perfectly optimize for these metrics as we show in the \u201c Discussion \u201d section in which we use human driving trajectories in nuScenes to show that humans occasionally drive dangerously close to cars in front of them . Similarly , in \u201c Hide and Seek \u201d https : //arxiv.org/pdf/1909.07528.pdf , \u201c Object Movement \u201d and \u201c Agent Movement \u201d are used as heuristics for tracking emergence of different hide and seek strategies . 3. \u201c The number of agents seems to be too small \u201d - Our ablation on the number of agents in the \u201c Experiments \u201d section shows that the number of agents used ( 12 per intersection in most cases ) is enough to cause the emergence of all phenomena that we sought to analyze in this work . We do agree that for more complicated behaviors to emerge such as lane changes or protected lefts , it will be important to scale both the maps used in our experiments and the number of agents . 4. \u201c It seems like the agents did not take traffic signal status as input \u201d - Our agents perceive the traffic lights . We have updated our \u201c Problem Setting \u201d section to include details of our traffic light model . 5. \u201c Figures 2 and 3 need better explanations \u201d - We have updated the submission accordingly . 6. \u201c I think this paper needs better formulation of the logics and also a deep investigation of how the simulation variations lead to those behaviors. \u201d - The vast majority of implicit and explicit road rules that humans have developed over the last century are the result of the hard work of civil engineers seeking to define driving behavior and infrastructure that will maximize traffic flow while minimizing collisions . We find it both intuitive and exciting that in a multi-agent partially-observable driving environment , RL agents trained to optimize the same objective discover similar human-like behavior . In our experiment section , we show that the multi-agent and partially-observable nature of the driving environment are the crucial factors that lead to these behaviors . If there are specific claims in our paper that the reviewer believes we have not fully justified by our experiments , we will be happy to oblige ."}, {"review_id": "d8Q1mt2Ghw-1", "review_text": "Summary : This paper proposes a bilevel MARL method that can learn road rules and conventions implicitly without hard coding . It is quite interesting to see a simple and straightforward idea that is effective in this task setting . However , this paper needs to be further polished in terms of its delivery , completeness , and evaluation . Methodology : How is the noise introduced to the mimicked LiDAR perception results ? In Spline Model , does the trajectory shape only depends on the initial state without considering the afterward interaction ? The clarity and completeness of writing could be significantly improved . The equations , terms , and the algorithm in Section 4 need sufficient explanations . E.g. , what are vf^ { target } _t , H , K_1 , K_2 , N ? What is the complete reward list ? Experiments : It seems the route is relatively short , and the driving scenarios used for different tasks are distinct . I was wondering whether the generalization of the proposed method and the learned rules could be validated . No baselines have been evaluated or compared . Which experimental results are or the fixed track model ? What are the differences between the two models in the results ? What are the reward values during the training phase ? The captions and axis labels are difficult to read . What do the colors mean in Fig.2 ? = My rating has been updated after the rebuttal .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for their constructive and insightful feedback . We are glad that the reviewer found our approach \u201c simple \u201d , \u201c interesting \u201d , and \u201c effective \u201d . We address the reviewer \u2019 s questions and comments below . 1. \u201c How is the noise introduced to the mimicked LiDAR perception results ? \u201d - we apologize for omitting this detail . We drop p % of lidar rays to mimic lidar noise as a proxy for `` ray-dropping '' LiDAR noise identified in https : //arxiv.org/abs/2006.09348 . 2. \u201c In Spline Model , does the trajectory shape only depends on the initial state \u201d - Yes , the trajectory shape only depends on the initial state . We have polished the writing in the \u201c Policy Parameterization \u201d section to clarify this constraint . 3. \u201c It seems the route is relatively short , and the driving scenarios used for different tasks are distinct \u201d - Our maps and routes were kept purposefully short in order to probe for the emergence of different driving rules in the most direct possible way . On larger maps , we expect the same transition dynamics and reward function to result in similar emergence , but demonstrating emergence -- which is our goal in this work -- is then less targeted . We leave experiments at larger length and time scales for later work . 4. \u201c I was wondering whether the generalization of the proposed method and the learned rules could be validated \u201d - In our updated supplementary , we have added rollouts of a policy trained on a synthetic map and evaluated on a large nuScenes map . The policies transfer well to new maps without retraining . 5. \u201c No baselines have been evaluated or compared \u201d - The purpose of baselines is to offer insight on the extent to which a given approach is working relative to other methods . In the paper \u201c Hide and Seek \u201d https : //arxiv.org/abs/1909.07528 which has a similar style of contribution to ours , the central claim of the authors is that the reward function for the game of hide and seek causes tool usage to emerge . As a result , the authors provide \u201c baselines \u201d in which different \u201c curiosity-based \u201d reward functions are used instead to prove their claim . In our work , our contribution is to show that in multi-agent driving systems with noisy perception , road rules emerge . As a result , our experiment section systematically measures the emergence of different road rules as we vary the perception quality of agents and the spatial density of agents . For us , we therefore consider the case where agents have perfect perception or when there is only one agent to be \u201c baselines \u201d and show that in these cases , the agents - as expected - do not exhibit road rules . If the reviewer is not convinced that we have shown that road rules emerge in multi-agent driving environments with noisy perception because a certain ablation experiment or baseline is missing , we encourage the reviewer to let us know the experiment and we will run it . 6. \u201c Which experimental results are or the fixed track model ? \u201d - we have clarified which agent variant is being evaluated in each of the experiment subsections . 7. \u201c What are the differences between the two models in the results ? \u201d - we have added plots comparing the emergence of traffic light usage for the fixed-track and spline agents . In general , the behavior of these two agents at convergence is very similar . The benefit of the fixed track agents is that these agents allow us to factor out lane emergence from the other behaviors that we seek to probe . 8. \u201c What are the reward values during the training phase ? \u201d - we have added plots of reward vs. optimization steps to our supplementary ."}, {"review_id": "d8Q1mt2Ghw-2", "review_text": "Summary : The output of the work is an MDP model that is capable of encoding complex traffic rules including traffic signals , lanes , right of way FIFO etc . Various different traffic environments such as intersections , highways , nuScenes are considered . The MDP that results from this work is very useful for future research . Currently , there are very few simulators that encode complex traffic rules or provide the flexibility to perform research . Therefore , the possibility of new simulators resulting from this MDP is exciting and useful to encourage and promote research in autonomous driving , especially in dense and heterogeneous environments . I am glad that the authors have provided the source code * * with extensive documentation and instructions * * on how to run the code and reproduce the results . Unless there is some major flaw with this paper ( that I may have missed ) , there is no reason to reject this paper .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the kind words . We are happy to hear that the reviewer found our work \u201c exciting and useful \u201d and appreciates the value in releasing these driving environments to the public . We share the vision of the reviewer that these environments will untap new areas for research in self-driving ."}, {"review_id": "d8Q1mt2Ghw-3", "review_text": "The paper proposes to learn traffic rules ( e.g.traffic lights , speed limits ) via multi-agent RL ( MARL ) from observations rather than hardcoding the rules into the algorithms . To this end , the authors claims contributions in : -- Defining a multi-agent driving environment , where each agent has incomplete observation ( noisy LiDAR ) , and is rewarded for reaching a destination quickly without colliding . Experiments show that road rules can be learned . -- Ablation on the choice of the MDP , and insights that perception noise and spatial density of agents are important to successfully learn the environment . -- Authors promise to release a suite of 2D driving environments for future MARL research in self-driving . I must admit that I am mainly a computer vision person , and I only have limited experience with RL or MARL . However , I hope that my assessment below is still better than an educated guess . I apologize in advance if there is any obvious misunderstanding . Strengths : -- Novelty in problem statement and at a high level : The problem statement of learning hard traffic rules via observing logs seems new . The application of MARL to the problem is new too AFAIK . And as the authors stated , the majority of multi-agent behavior works have been using imitation learning . ( Maybe consider citing [ 1 ] line of work as well , which deals with multi-agent imitation learning ) . The proposed PPO-based method seems a good alternative . -- Extensive experiments : There are 7 small but specific tasks such as `` Traffic lights '' , `` Emergence of lanes '' , etc. , where the authors provided evidences to the claims that perception noise and spatial density of agents are crucial for the method 's success . [ 1 ] : Multi-Agent Generative Adversarial Imitation Learning Weaknesses : -- No comparison to prior work : The authors acknowledged imitation learning-based ( IL ) methods but did any quantitative comparison for them . I do n't see any evidence why solving the proposed problem using MARL is better than IL . So why are we doing MARL over IL ? - Usefulness : for the traffic light use case , the handling of red / green lights is essentially learned by NOT driving into other cars , which obey the traffic rules . With other words , if there are no other agents to demonstrate how to behave , the agent will always prefer to run over red . This raises the question of the usefulness of the system . -- Lack of novelty in the method itself : The paper seems to be using an off-the-shelf PPO . The centralized critic , single-step PPO , and bi-level PPO do not provide sufficient novelty in terms of methods . The centralized critic can be seen as a straightforward extension of PPO for the self-driving application . The bi-level extension is a straightforward way to optimize two objectives at the same time , which is quite common even in the ages of convex optimization ( https : //en.wikipedia.org/wiki/Biconvex_optimization ) . I think that there __is__ novelty in the method , but I am not sure if it 's enough for this venue . - Dataset : NuScenes dataset is a perception dataset , and probably does not contain many interesting interactive scenarios . Still , the method does not seem to perform very well in the experiments . E.g. , according to Fig 2. , many cars are still accelerating despite the red light ... Details : - All axes names and titles in all diagrams are way too small . No way people can decipher them if printed on paper . - No legends in the diagrams . Barely any caption . Please make diagrams self-contained if possible . Conclusion : This works provides an interesting new problem statement and explores the area of using MARL for autonomous driving . My main concerns regarding this paper are the lack of comparison to prior works in the experiments , and the work provides any usefulness and improvements over current autonomous systems , which are mostly based on imitation learning .", "rating": "5: Marginally below acceptance threshold", "reply_text": "6. \u201c NuScenes dataset is a perception dataset , and probably does not contain many interesting interactive scenarios \u201d - We are not using any information from the annotated scenarios that are part of the nuScenes dataset . We * exclusively * use crops from the HD maps that come packaged with nuScenes in order to test our model on intersections with variable real-world geometry . Images of these maps can be found in the \u201c Rollouts on Nuscenes \u201d portion of our supplementary material . We emphasize that * all * agents in our environments use policies that are trained to minimize the time it takes to reach randomly sampled destinations without colliding ; there are no fixed-trajectory agents taken from nuScenes annotations in any of our environments . 7. \u201c according to Fig 2. , many cars are still accelerating despite the red light \u201d - As noted in our response to AnonReviewer4 , our metrics for measuring the emergence of the different road rules in this paper are only heuristics for the qualitative behavior we seek to evaluate ; there are certainly scenarios where it is appropriate to accelerate when a red light is in sight despite deceleration being the correct policy the vast majority of the time . We have added Figure 3 to further visualize that the agents respect the traffic signals and that they increasingly do so when they train in environments with many other agents and noisy perception . The reviewer may also consult videos provided in our supplementary to judge qualitatively the extent to which the agents have learned a policy that follows traffic lights . 8. \u201c Details \u201d - our captions and figures have been updated to a more appropriate font size . 9. \u201c My main concerns regarding this paper are the lack of comparison to prior works in the experiments\u2026 \u201d - Our discovery in this paper is that in a simple multi-agent driving environment , road rules found in human transportation systems emerge . Since our goal is to demonstrate emergence , the bulk of our experiments , ablations , and comparisons were designed to analyze which design choices led to this emergence . If there are specific claims that the reviewer feels were not substantiated by the ablations presented in our paper , we encourage the reviewer to recommend further ablations that we have missed . 10. \u201c ... and [ whether or not ] the work provides any usefulness and improvements over current autonomous systems , which are mostly based on imitation learning. \u201d - To demonstrate the value of having a model of human driving behavior instead of relying on imitation learning , consider the case where construction or new traffic signs are added to a road network . Imitation learning requires re-collecting expert driving data in order to adapt to the change . In contrast , our agents can update policies in simulation by re-training in the updated environment . It is tremendously beneficial to be able to be able to model how changes to an environment will affect human behavior before taking actions in the real world . 11. \u201c ... which are mostly based on imitation learning. \u201d for many of the reasons included in https : //www.ri.cmu.edu/pub_files/2015/3/InvitationToImitation_3_1415.pdf , imitation learning has not been a successful approach for learning deployable driving policies ; contemporary planners for self-driving are currently rule-based with a small amount of online optimization ."}], "0": {"review_id": "d8Q1mt2Ghw-0", "review_text": "This paper investigate how to design simulation environments so the the agent trained with them can master social rules . Cons : 1.The paper is well written and easy to read and understand . Thanks ! 2.The experiments are solid and well defined . My major concern of this paper are : 1 . The authors seems to only considered the noise of sensors and the number of agents , and these two factors happened to induce social behaviors like following lanes and stopping at traffic signals . In other words , I am not quite convinced that with all vehicles being automated , sensor noise will cause them to formulate rules that what human drivers follow today . Since human driving interactions are complex , I do not think that sensor noise would be enough to induce them . 2.I would be happy to see how this configuration of simulation environment compares with reward guided social behaviors . For example , we can design a reward to encourage agents follow right of way . I think this way might be more direct and powerful . 2.The number of agents seems to be too small and may affect the formulation of road social rules . 3.It seems like the agents did not take traffic signal status as input for the action selection , then how did they formulate the signal control rules . 4.Figures 2 and 3 needs better explanations for readers to understand . Overall , I think this paper needs better formulation of the logics and also a deep investigation of how the simulation variations lead to those behaviors .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . We are happy to hear that the reviewer found our paper \u201c well-written \u201d and our experiments \u201c solid and well defined \u201d . As mentioned in our comment to all reviewers , we would like to emphasize that while prior work has trained agents to maximize similarity to observed human driving trajectories ( e.g.imitation learning ) , our contribution is to show that when we optimize for traffic flow , we find that agents exhibit many behaviors that have generally been assumed to require human demonstrations to learn . We address the reviewer 's comments below . 1. \u201c Since human driving interactions are complex ... \u201d - We agree with the reviewer that certain human driving behavior can not be modeled purely as emergent phenomena in a partially-observable multi-agent system in which all users seek to go from $ a\\rightarrow b $ as fast as possible without colliding . For instance , we would never expect parades or funeral processions to emerge in our environments . However , our goal in this paper was to show that standard human driving behavior such as lane following or traffic light usage are optimal under the simple reward function of minimizing the time it takes to reach randomly sampled destinations without colliding with other agents . At larger length and time scales , we expect the reward function and transition dynamics used in this paper to result in other phenomena such as the emergence of lane changes , protected lefts , and one-way roads ( which civil engineers have also determined maximize traffic flow in settings like New York City , for instance ) . Potentially , our framework could also be used to discover new driving behavior that could be adopted by human drivers to improve safety and efficiency . 2. \u201c We can design a reward to encourage agents follow the right of way \u201d - Designing complicated reward functions is what we seek to avoid in this work . Our goal is to show that the simple reward function of minimizing the time it takes agents to reach randomly sampled destinations results in much of the standard driving behavior observed in human driving systems . To clarify the purpose of the metrics plotted in our Experiment section , we use these metrics to quantitatively evaluate the extent to which the behavior of the agents trained in our MDP mimic the behavior of human drivers - not as metrics that we explicitly hope to optimize . In reality , humans themselves are unlikely to perfectly optimize for these metrics as we show in the \u201c Discussion \u201d section in which we use human driving trajectories in nuScenes to show that humans occasionally drive dangerously close to cars in front of them . Similarly , in \u201c Hide and Seek \u201d https : //arxiv.org/pdf/1909.07528.pdf , \u201c Object Movement \u201d and \u201c Agent Movement \u201d are used as heuristics for tracking emergence of different hide and seek strategies . 3. \u201c The number of agents seems to be too small \u201d - Our ablation on the number of agents in the \u201c Experiments \u201d section shows that the number of agents used ( 12 per intersection in most cases ) is enough to cause the emergence of all phenomena that we sought to analyze in this work . We do agree that for more complicated behaviors to emerge such as lane changes or protected lefts , it will be important to scale both the maps used in our experiments and the number of agents . 4. \u201c It seems like the agents did not take traffic signal status as input \u201d - Our agents perceive the traffic lights . We have updated our \u201c Problem Setting \u201d section to include details of our traffic light model . 5. \u201c Figures 2 and 3 need better explanations \u201d - We have updated the submission accordingly . 6. \u201c I think this paper needs better formulation of the logics and also a deep investigation of how the simulation variations lead to those behaviors. \u201d - The vast majority of implicit and explicit road rules that humans have developed over the last century are the result of the hard work of civil engineers seeking to define driving behavior and infrastructure that will maximize traffic flow while minimizing collisions . We find it both intuitive and exciting that in a multi-agent partially-observable driving environment , RL agents trained to optimize the same objective discover similar human-like behavior . In our experiment section , we show that the multi-agent and partially-observable nature of the driving environment are the crucial factors that lead to these behaviors . If there are specific claims in our paper that the reviewer believes we have not fully justified by our experiments , we will be happy to oblige ."}, "1": {"review_id": "d8Q1mt2Ghw-1", "review_text": "Summary : This paper proposes a bilevel MARL method that can learn road rules and conventions implicitly without hard coding . It is quite interesting to see a simple and straightforward idea that is effective in this task setting . However , this paper needs to be further polished in terms of its delivery , completeness , and evaluation . Methodology : How is the noise introduced to the mimicked LiDAR perception results ? In Spline Model , does the trajectory shape only depends on the initial state without considering the afterward interaction ? The clarity and completeness of writing could be significantly improved . The equations , terms , and the algorithm in Section 4 need sufficient explanations . E.g. , what are vf^ { target } _t , H , K_1 , K_2 , N ? What is the complete reward list ? Experiments : It seems the route is relatively short , and the driving scenarios used for different tasks are distinct . I was wondering whether the generalization of the proposed method and the learned rules could be validated . No baselines have been evaluated or compared . Which experimental results are or the fixed track model ? What are the differences between the two models in the results ? What are the reward values during the training phase ? The captions and axis labels are difficult to read . What do the colors mean in Fig.2 ? = My rating has been updated after the rebuttal .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewer for their constructive and insightful feedback . We are glad that the reviewer found our approach \u201c simple \u201d , \u201c interesting \u201d , and \u201c effective \u201d . We address the reviewer \u2019 s questions and comments below . 1. \u201c How is the noise introduced to the mimicked LiDAR perception results ? \u201d - we apologize for omitting this detail . We drop p % of lidar rays to mimic lidar noise as a proxy for `` ray-dropping '' LiDAR noise identified in https : //arxiv.org/abs/2006.09348 . 2. \u201c In Spline Model , does the trajectory shape only depends on the initial state \u201d - Yes , the trajectory shape only depends on the initial state . We have polished the writing in the \u201c Policy Parameterization \u201d section to clarify this constraint . 3. \u201c It seems the route is relatively short , and the driving scenarios used for different tasks are distinct \u201d - Our maps and routes were kept purposefully short in order to probe for the emergence of different driving rules in the most direct possible way . On larger maps , we expect the same transition dynamics and reward function to result in similar emergence , but demonstrating emergence -- which is our goal in this work -- is then less targeted . We leave experiments at larger length and time scales for later work . 4. \u201c I was wondering whether the generalization of the proposed method and the learned rules could be validated \u201d - In our updated supplementary , we have added rollouts of a policy trained on a synthetic map and evaluated on a large nuScenes map . The policies transfer well to new maps without retraining . 5. \u201c No baselines have been evaluated or compared \u201d - The purpose of baselines is to offer insight on the extent to which a given approach is working relative to other methods . In the paper \u201c Hide and Seek \u201d https : //arxiv.org/abs/1909.07528 which has a similar style of contribution to ours , the central claim of the authors is that the reward function for the game of hide and seek causes tool usage to emerge . As a result , the authors provide \u201c baselines \u201d in which different \u201c curiosity-based \u201d reward functions are used instead to prove their claim . In our work , our contribution is to show that in multi-agent driving systems with noisy perception , road rules emerge . As a result , our experiment section systematically measures the emergence of different road rules as we vary the perception quality of agents and the spatial density of agents . For us , we therefore consider the case where agents have perfect perception or when there is only one agent to be \u201c baselines \u201d and show that in these cases , the agents - as expected - do not exhibit road rules . If the reviewer is not convinced that we have shown that road rules emerge in multi-agent driving environments with noisy perception because a certain ablation experiment or baseline is missing , we encourage the reviewer to let us know the experiment and we will run it . 6. \u201c Which experimental results are or the fixed track model ? \u201d - we have clarified which agent variant is being evaluated in each of the experiment subsections . 7. \u201c What are the differences between the two models in the results ? \u201d - we have added plots comparing the emergence of traffic light usage for the fixed-track and spline agents . In general , the behavior of these two agents at convergence is very similar . The benefit of the fixed track agents is that these agents allow us to factor out lane emergence from the other behaviors that we seek to probe . 8. \u201c What are the reward values during the training phase ? \u201d - we have added plots of reward vs. optimization steps to our supplementary ."}, "2": {"review_id": "d8Q1mt2Ghw-2", "review_text": "Summary : The output of the work is an MDP model that is capable of encoding complex traffic rules including traffic signals , lanes , right of way FIFO etc . Various different traffic environments such as intersections , highways , nuScenes are considered . The MDP that results from this work is very useful for future research . Currently , there are very few simulators that encode complex traffic rules or provide the flexibility to perform research . Therefore , the possibility of new simulators resulting from this MDP is exciting and useful to encourage and promote research in autonomous driving , especially in dense and heterogeneous environments . I am glad that the authors have provided the source code * * with extensive documentation and instructions * * on how to run the code and reproduce the results . Unless there is some major flaw with this paper ( that I may have missed ) , there is no reason to reject this paper .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the kind words . We are happy to hear that the reviewer found our work \u201c exciting and useful \u201d and appreciates the value in releasing these driving environments to the public . We share the vision of the reviewer that these environments will untap new areas for research in self-driving ."}, "3": {"review_id": "d8Q1mt2Ghw-3", "review_text": "The paper proposes to learn traffic rules ( e.g.traffic lights , speed limits ) via multi-agent RL ( MARL ) from observations rather than hardcoding the rules into the algorithms . To this end , the authors claims contributions in : -- Defining a multi-agent driving environment , where each agent has incomplete observation ( noisy LiDAR ) , and is rewarded for reaching a destination quickly without colliding . Experiments show that road rules can be learned . -- Ablation on the choice of the MDP , and insights that perception noise and spatial density of agents are important to successfully learn the environment . -- Authors promise to release a suite of 2D driving environments for future MARL research in self-driving . I must admit that I am mainly a computer vision person , and I only have limited experience with RL or MARL . However , I hope that my assessment below is still better than an educated guess . I apologize in advance if there is any obvious misunderstanding . Strengths : -- Novelty in problem statement and at a high level : The problem statement of learning hard traffic rules via observing logs seems new . The application of MARL to the problem is new too AFAIK . And as the authors stated , the majority of multi-agent behavior works have been using imitation learning . ( Maybe consider citing [ 1 ] line of work as well , which deals with multi-agent imitation learning ) . The proposed PPO-based method seems a good alternative . -- Extensive experiments : There are 7 small but specific tasks such as `` Traffic lights '' , `` Emergence of lanes '' , etc. , where the authors provided evidences to the claims that perception noise and spatial density of agents are crucial for the method 's success . [ 1 ] : Multi-Agent Generative Adversarial Imitation Learning Weaknesses : -- No comparison to prior work : The authors acknowledged imitation learning-based ( IL ) methods but did any quantitative comparison for them . I do n't see any evidence why solving the proposed problem using MARL is better than IL . So why are we doing MARL over IL ? - Usefulness : for the traffic light use case , the handling of red / green lights is essentially learned by NOT driving into other cars , which obey the traffic rules . With other words , if there are no other agents to demonstrate how to behave , the agent will always prefer to run over red . This raises the question of the usefulness of the system . -- Lack of novelty in the method itself : The paper seems to be using an off-the-shelf PPO . The centralized critic , single-step PPO , and bi-level PPO do not provide sufficient novelty in terms of methods . The centralized critic can be seen as a straightforward extension of PPO for the self-driving application . The bi-level extension is a straightforward way to optimize two objectives at the same time , which is quite common even in the ages of convex optimization ( https : //en.wikipedia.org/wiki/Biconvex_optimization ) . I think that there __is__ novelty in the method , but I am not sure if it 's enough for this venue . - Dataset : NuScenes dataset is a perception dataset , and probably does not contain many interesting interactive scenarios . Still , the method does not seem to perform very well in the experiments . E.g. , according to Fig 2. , many cars are still accelerating despite the red light ... Details : - All axes names and titles in all diagrams are way too small . No way people can decipher them if printed on paper . - No legends in the diagrams . Barely any caption . Please make diagrams self-contained if possible . Conclusion : This works provides an interesting new problem statement and explores the area of using MARL for autonomous driving . My main concerns regarding this paper are the lack of comparison to prior works in the experiments , and the work provides any usefulness and improvements over current autonomous systems , which are mostly based on imitation learning .", "rating": "5: Marginally below acceptance threshold", "reply_text": "6. \u201c NuScenes dataset is a perception dataset , and probably does not contain many interesting interactive scenarios \u201d - We are not using any information from the annotated scenarios that are part of the nuScenes dataset . We * exclusively * use crops from the HD maps that come packaged with nuScenes in order to test our model on intersections with variable real-world geometry . Images of these maps can be found in the \u201c Rollouts on Nuscenes \u201d portion of our supplementary material . We emphasize that * all * agents in our environments use policies that are trained to minimize the time it takes to reach randomly sampled destinations without colliding ; there are no fixed-trajectory agents taken from nuScenes annotations in any of our environments . 7. \u201c according to Fig 2. , many cars are still accelerating despite the red light \u201d - As noted in our response to AnonReviewer4 , our metrics for measuring the emergence of the different road rules in this paper are only heuristics for the qualitative behavior we seek to evaluate ; there are certainly scenarios where it is appropriate to accelerate when a red light is in sight despite deceleration being the correct policy the vast majority of the time . We have added Figure 3 to further visualize that the agents respect the traffic signals and that they increasingly do so when they train in environments with many other agents and noisy perception . The reviewer may also consult videos provided in our supplementary to judge qualitatively the extent to which the agents have learned a policy that follows traffic lights . 8. \u201c Details \u201d - our captions and figures have been updated to a more appropriate font size . 9. \u201c My main concerns regarding this paper are the lack of comparison to prior works in the experiments\u2026 \u201d - Our discovery in this paper is that in a simple multi-agent driving environment , road rules found in human transportation systems emerge . Since our goal is to demonstrate emergence , the bulk of our experiments , ablations , and comparisons were designed to analyze which design choices led to this emergence . If there are specific claims that the reviewer feels were not substantiated by the ablations presented in our paper , we encourage the reviewer to recommend further ablations that we have missed . 10. \u201c ... and [ whether or not ] the work provides any usefulness and improvements over current autonomous systems , which are mostly based on imitation learning. \u201d - To demonstrate the value of having a model of human driving behavior instead of relying on imitation learning , consider the case where construction or new traffic signs are added to a road network . Imitation learning requires re-collecting expert driving data in order to adapt to the change . In contrast , our agents can update policies in simulation by re-training in the updated environment . It is tremendously beneficial to be able to be able to model how changes to an environment will affect human behavior before taking actions in the real world . 11. \u201c ... which are mostly based on imitation learning. \u201d for many of the reasons included in https : //www.ri.cmu.edu/pub_files/2015/3/InvitationToImitation_3_1415.pdf , imitation learning has not been a successful approach for learning deployable driving policies ; contemporary planners for self-driving are currently rule-based with a small amount of online optimization ."}}