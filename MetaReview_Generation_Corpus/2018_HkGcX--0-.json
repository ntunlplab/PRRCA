{"year": "2018", "forum": "HkGcX--0-", "title": "Auxiliary Guided Autoregressive Variational Autoencoders", "decision": "Reject", "meta_review": "To ensure that a VAE with a powerful autoregressive decoder does not ignore its latent variables, the authors propose adding an extra term to the ELBO, corresponding to a reconstruction with an auxiliary non-autoregressive decoder. This does indeed produce models that use latent variables and (with some tuning of the weight on the KL term) perform as well as the underlying autoregressive model alone. However, as the reviewers pointed out, the paper does not demonstrate the value of the resulting models. If the goal is learning meaningful latent representations, then the quality of the representations should be evaluated empirically. Currently it is not clear whether that the proposed approach would yield better representations than a VAE with a non-autoregressive decoder or a VAE with an autoregressive decoder trained using the \"free bits\" trick of Kingma et al. (2016). This is certainly an interesting idea, but without a proper evaluation it is impossible to judge its value.", "reviews": [{"review_id": "HkGcX--0--0", "review_text": "Summary: This paper attempts to solve the problem of meaningfully combining variational autoencoders (VAEs) and PixelCNNs. It proposes to do this by simultaneously optimizing a VAE with PixelCNN++ decoder, and a VAE with factorial decoder. The model is evaluated in terms of log-likelihood (with no improvement over a PixelCNN++) and the visual appearance of samples and reconstructions. Review: Combining density networks (like VAEs) and autoregressive models is an unsolved problem and potentially very useful. To me, the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels (below Equation 7). Unfortunately the authors were unable to make any good use of this insight, and I will explain below why I don\u2019t see any evidence of an improved generative model in this paper. As the paper is written now, it is not clear what the goal of the authors is. Is it density estimation? Then the addition of the VAE had no measurable effect on the PixelCNN++\u2019s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability. Is it representation learning? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement. Is it image synthesis (not a real application by itself), then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. Much of the authors\u2019 analysis is based on a qualitative evaluation of samples. However, samples can be very misleading. A lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the low-level statistics of natural images. In contrast to the authors, I fail to see a meaningful difference between the groups of samples in Figure 1. The VAE samples in Figure 3b) look quite smooth. Was independent Gaussian noise added to the VAE samples or are those (as is sometimes done) sampled means? If the former, what was sigma and how was it chosen? On page 7, the authors conclude that \u201cthe pixelCNN clearly takes into account the output of the VAE decoder\u201d based on the samples. Being a mixture model, a PixelCNN++ could easily represent the following mixture: p(x | z) = 0.01 \\prod_i p(x_i | x_{<i}) + 0.99 \\prod_i p(x_i | z) The first term is just like a regular PixelCNN++, ignoring the latent variables. The second term is just like a variational autoencoder with factorial decoder. The samples in this case would be dominated by the VAE, which depends on the latent state. The log-likelihood would be dominated by the first term and would be minimally effected (see Theis et al., 2016). Note that I am not saying that this is exactly what the model has learned. I am merely providing a possible counter example to the notion that the PixelCNN++ has learned to use of the latent representation in a meaningful way. What happens if the KL term is simply downweighted but the factorial decoder is not included? This seems like it would be a useful control to include. The paper is well written and clear.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed remarks and analysis of our work . We now answer your main concerns , if you want more information do not hesitate to ask . AnnonReviewer2 : `` To me , the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels ( below Equation 7 ) . Unfortunately the authors were unable to make any good use of this insight [ ... ] '' This insight is indeed one of the cornerstones of our contribution . Combining a factorial VAE model over the pixels with an conditional autoregressive model over another copy of the pixels naturally leads to two setting of lambda ( see eq.7 and paragraph just below ) , and shows that larger values of lambda also lead to valid lower bounds on the combined loss . Based on this observation , we explore models trained with different values of lambda , which improves performance from 3.2 bpd ( lambda=1 ) to 2.92 bpd ( lambda=12 ) . The latter sets a new state-of-the-art bpd level among generative models with a non-degenerate latent variable structure . We also provide quantitative and qualitative analysis of the effect different choices of balance have on the model . In that sense , we believe the insight has been put to good use . AnnonReviewer2 : \u201c As the paper is written now , it is not clear what the goal of the authors is . Is it density estimation ? Then the addition of the VAE had no measurable effect on the PixelCNN++ \u2019 s performance , i.e. , it seems like a bad idea due to the added complexity and loss of tractability . Is it representation learning ? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement . Is it image synthesis ( not a real application by itself ) , then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. \u201d Thanks for this valuable input , that will help us to clarify the message of our paper . Our goal is to learn generative models ( i.e.density estimation ) with latent variable models ( i.e.representation learning ) , for the reason stated above in response to AnnonReviewer3 . Our quantitative experimental results in terms of likelihood on held-out data ( the bpd metric ) improve over earlier latent variable models in the literature . The images sampled from our model are used as a secondary qualitative form of evaluation . The examples in Figure 3 show that our model indeed learns a meaningful latent variable structure that is conditioning the autoregressive decoder ( see also below ) . AnnonReviewer2 : \u201c The VAE samples in Figure 3b ) look quite smooth . Was independent Gaussian noise added to the VAE samples or are those ( as is sometimes done ) sampled means ? If the former , what was sigma and how was it chosen ? \u201d The images produced by the VAE and shown in Figure 3b ) are indeed the means of the output distribution , which is why no 'salt and pepper noise ' can be seen . We will clarify the text in this respect . The variance is a learned constant per color channel used across all spatial positions , and independent of the latent variable . AnnonReviewer2 : \u201c On page 7 , the authors conclude that \u201c the pixelCNN clearly takes into account the output of the VAE decoder \u201d based on the samples . Being a mixture model , a PixelCNN++ could easily represent the following mixture : p ( x | z ) = 0.01 \\prod_i p ( x_i | x_ { \u201c The end of this argument was unfortunately cut off.Can you please send an update with the complete text ? In the meantime , let us respond as follows . The samples in figures 1c , 3 and 6 show the intermediate representation f ( z ) that is computed by the VAE decoder , together with samples from the pixelCNN decoder that is conditioned on f ( z ) , see also Figure 2 for schematic overview . Let us suppose , contrary to our claim , that the pixelCNN decoder ignores the output of the VAE decoder , i.e.that the pixelCNN output is independent of the VAE output . In this case the image pairs of VAE decoder output and PixelCNN sample should not correlate at all in figures 1c , 3 and 6 . Yet , we observe in each single example a clear correspondence between the pixelCNN sample and the conditioning VAE output . The latter looks like a smoothed version of the former . Therefore , we conclude that the latter is not independent of the former , and that pixelCNN does take into account the VAE output , i.e.we succeed in conditioning the pixelCNN on the VAE output . We hope this clarifies our statement , and we will update the text accordingly ."}, {"review_id": "HkGcX--0--1", "review_text": "The proposed approach is straight forward, experimental results are good, but don\u2019t really push the state of the art. But the empirical analysis (e.g. decomposition of different cost terms) is detailed and very interesting. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your appreciation of our analysis and empirical evaluation . AnnonReviewer1 : \u201c The proposed approach is straight forward , experimental results are good , but don \u2019 t really push the state of the art . But the empirical analysis ( e.g.decomposition of different cost terms ) is detailed and very interesting. \u201d We provide justifications of why we believe that our work significantly pushes the state of the art in latent variable density modeling in our other answers , and hope these arguments are satisfying ."}, {"review_id": "HkGcX--0--2", "review_text": "The authors present Auxiliary Guided Autoregressive Variational autoEncoders (AGAVE), a hybrid approach that combines the strengths of variational autoencoders (global statistics) and autorregressive models (local statistics) for improved image modeling. This is done by controlling the capacity of the autorregressive component within an auxiliary loss function. The proposed approach is a straightforward combination of VAE and PixelCNN that although empirically better than PixelCNN, and presumably VAE, does not outperform PixelCNN++. Provided that the authors use PixelCNN++ in their approach, quantitively speaking, it is difficult to defend the value of adding a VAE component to the model. The authors do not describe how \\lambda was selected, which is critical for performance, provided the results in Figure 4. That being said, the contribution from the VAE is likely to be negligible given the performance of PixelCNN++ alone. - The KL divergence in (3) does more than simply preventing the approximation q() from becoming a point mass distribution.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your constructive review of our work . We will address the main concerns raised , please do not hesitate to ask for more detail if needed . AnnonReviewer3 : \u201c [ ... ] empirically better than PixelCNN , and presumably VAE , does not outperform PixelCNN++. \u201d \u201c Provided that the authors use PixelCNN++ [ ... ] it is difficult to defend the value of adding a VAE component to the model \u201d Our main contribution is a method based on an auxiliary loss to learn generative models that combine a non-degenerate latent variable structure with expressive autoregressive decoders . Among latent variable models , our model sets a new state-of-the-art result of 2.92 bpd . That is the same score as obtained by pixelCNN++ which does not learn a latent variable representation . Among VAE models with factored observation model of p ( x|z ) , VAE-IAF obtains the best quantitative score of 3.11 bpd on CIFAR10 . Our performance of 2.92 bpd represents an important improvement . The improvement over Lossy-VAE ( 2.95 bpd ) , former best model with latent variables , is 0.03 bpd . Our auxiliary loss allows us to use a more powerful autoregressive decoder , which allows us to improve over the Lossy-VAE result . The numbers that support these claims can be found in Table 1 . We will improve the presentation of Table 1 to highlight which models use latent variables and/or autoregressive decoders , to more easily appreciate our contribution in terms of the quantitative evaluation results . Unlike autoregressive models such as pixelCNN++ , latent variable models learn data representations which are useful for tasks such as semi-supervised learning , see e.g . ( Kingma et al. , NIPS 2014 ) . Therefore we believe that our work makes an important contribution to generative representation learning . AnnonReviewer3 : \u201c The authors do not describe how lambda was selected , which is critical for performance [ ... ] \u201d The results used when comparing with the state of the art are reported using our best configuration with lambda equal to 12 . We will clarify this in the text . In figures 3 , 4 and 5 qualitative and quantitative results are reported , as indicated , over a range of lambda values . The choice of lambda is important , but not critical , provided it is taken to be bigger than two : Figure 4 shows that beyond the first drop of 0.20 bpd when going from lambda =1 to lambda = 2 , the bpd further decreases monotonically to down to an improvement of 0.26 bpd for lambda=12 ."}], "0": {"review_id": "HkGcX--0--0", "review_text": "Summary: This paper attempts to solve the problem of meaningfully combining variational autoencoders (VAEs) and PixelCNNs. It proposes to do this by simultaneously optimizing a VAE with PixelCNN++ decoder, and a VAE with factorial decoder. The model is evaluated in terms of log-likelihood (with no improvement over a PixelCNN++) and the visual appearance of samples and reconstructions. Review: Combining density networks (like VAEs) and autoregressive models is an unsolved problem and potentially very useful. To me, the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels (below Equation 7). Unfortunately the authors were unable to make any good use of this insight, and I will explain below why I don\u2019t see any evidence of an improved generative model in this paper. As the paper is written now, it is not clear what the goal of the authors is. Is it density estimation? Then the addition of the VAE had no measurable effect on the PixelCNN++\u2019s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability. Is it representation learning? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement. Is it image synthesis (not a real application by itself), then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. Much of the authors\u2019 analysis is based on a qualitative evaluation of samples. However, samples can be very misleading. A lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the low-level statistics of natural images. In contrast to the authors, I fail to see a meaningful difference between the groups of samples in Figure 1. The VAE samples in Figure 3b) look quite smooth. Was independent Gaussian noise added to the VAE samples or are those (as is sometimes done) sampled means? If the former, what was sigma and how was it chosen? On page 7, the authors conclude that \u201cthe pixelCNN clearly takes into account the output of the VAE decoder\u201d based on the samples. Being a mixture model, a PixelCNN++ could easily represent the following mixture: p(x | z) = 0.01 \\prod_i p(x_i | x_{<i}) + 0.99 \\prod_i p(x_i | z) The first term is just like a regular PixelCNN++, ignoring the latent variables. The second term is just like a variational autoencoder with factorial decoder. The samples in this case would be dominated by the VAE, which depends on the latent state. The log-likelihood would be dominated by the first term and would be minimally effected (see Theis et al., 2016). Note that I am not saying that this is exactly what the model has learned. I am merely providing a possible counter example to the notion that the PixelCNN++ has learned to use of the latent representation in a meaningful way. What happens if the KL term is simply downweighted but the factorial decoder is not included? This seems like it would be a useful control to include. The paper is well written and clear.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed remarks and analysis of our work . We now answer your main concerns , if you want more information do not hesitate to ask . AnnonReviewer2 : `` To me , the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels ( below Equation 7 ) . Unfortunately the authors were unable to make any good use of this insight [ ... ] '' This insight is indeed one of the cornerstones of our contribution . Combining a factorial VAE model over the pixels with an conditional autoregressive model over another copy of the pixels naturally leads to two setting of lambda ( see eq.7 and paragraph just below ) , and shows that larger values of lambda also lead to valid lower bounds on the combined loss . Based on this observation , we explore models trained with different values of lambda , which improves performance from 3.2 bpd ( lambda=1 ) to 2.92 bpd ( lambda=12 ) . The latter sets a new state-of-the-art bpd level among generative models with a non-degenerate latent variable structure . We also provide quantitative and qualitative analysis of the effect different choices of balance have on the model . In that sense , we believe the insight has been put to good use . AnnonReviewer2 : \u201c As the paper is written now , it is not clear what the goal of the authors is . Is it density estimation ? Then the addition of the VAE had no measurable effect on the PixelCNN++ \u2019 s performance , i.e. , it seems like a bad idea due to the added complexity and loss of tractability . Is it representation learning ? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement . Is it image synthesis ( not a real application by itself ) , then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. \u201d Thanks for this valuable input , that will help us to clarify the message of our paper . Our goal is to learn generative models ( i.e.density estimation ) with latent variable models ( i.e.representation learning ) , for the reason stated above in response to AnnonReviewer3 . Our quantitative experimental results in terms of likelihood on held-out data ( the bpd metric ) improve over earlier latent variable models in the literature . The images sampled from our model are used as a secondary qualitative form of evaluation . The examples in Figure 3 show that our model indeed learns a meaningful latent variable structure that is conditioning the autoregressive decoder ( see also below ) . AnnonReviewer2 : \u201c The VAE samples in Figure 3b ) look quite smooth . Was independent Gaussian noise added to the VAE samples or are those ( as is sometimes done ) sampled means ? If the former , what was sigma and how was it chosen ? \u201d The images produced by the VAE and shown in Figure 3b ) are indeed the means of the output distribution , which is why no 'salt and pepper noise ' can be seen . We will clarify the text in this respect . The variance is a learned constant per color channel used across all spatial positions , and independent of the latent variable . AnnonReviewer2 : \u201c On page 7 , the authors conclude that \u201c the pixelCNN clearly takes into account the output of the VAE decoder \u201d based on the samples . Being a mixture model , a PixelCNN++ could easily represent the following mixture : p ( x | z ) = 0.01 \\prod_i p ( x_i | x_ { \u201c The end of this argument was unfortunately cut off.Can you please send an update with the complete text ? In the meantime , let us respond as follows . The samples in figures 1c , 3 and 6 show the intermediate representation f ( z ) that is computed by the VAE decoder , together with samples from the pixelCNN decoder that is conditioned on f ( z ) , see also Figure 2 for schematic overview . Let us suppose , contrary to our claim , that the pixelCNN decoder ignores the output of the VAE decoder , i.e.that the pixelCNN output is independent of the VAE output . In this case the image pairs of VAE decoder output and PixelCNN sample should not correlate at all in figures 1c , 3 and 6 . Yet , we observe in each single example a clear correspondence between the pixelCNN sample and the conditioning VAE output . The latter looks like a smoothed version of the former . Therefore , we conclude that the latter is not independent of the former , and that pixelCNN does take into account the VAE output , i.e.we succeed in conditioning the pixelCNN on the VAE output . We hope this clarifies our statement , and we will update the text accordingly ."}, "1": {"review_id": "HkGcX--0--1", "review_text": "The proposed approach is straight forward, experimental results are good, but don\u2019t really push the state of the art. But the empirical analysis (e.g. decomposition of different cost terms) is detailed and very interesting. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your appreciation of our analysis and empirical evaluation . AnnonReviewer1 : \u201c The proposed approach is straight forward , experimental results are good , but don \u2019 t really push the state of the art . But the empirical analysis ( e.g.decomposition of different cost terms ) is detailed and very interesting. \u201d We provide justifications of why we believe that our work significantly pushes the state of the art in latent variable density modeling in our other answers , and hope these arguments are satisfying ."}, "2": {"review_id": "HkGcX--0--2", "review_text": "The authors present Auxiliary Guided Autoregressive Variational autoEncoders (AGAVE), a hybrid approach that combines the strengths of variational autoencoders (global statistics) and autorregressive models (local statistics) for improved image modeling. This is done by controlling the capacity of the autorregressive component within an auxiliary loss function. The proposed approach is a straightforward combination of VAE and PixelCNN that although empirically better than PixelCNN, and presumably VAE, does not outperform PixelCNN++. Provided that the authors use PixelCNN++ in their approach, quantitively speaking, it is difficult to defend the value of adding a VAE component to the model. The authors do not describe how \\lambda was selected, which is critical for performance, provided the results in Figure 4. That being said, the contribution from the VAE is likely to be negligible given the performance of PixelCNN++ alone. - The KL divergence in (3) does more than simply preventing the approximation q() from becoming a point mass distribution.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your constructive review of our work . We will address the main concerns raised , please do not hesitate to ask for more detail if needed . AnnonReviewer3 : \u201c [ ... ] empirically better than PixelCNN , and presumably VAE , does not outperform PixelCNN++. \u201d \u201c Provided that the authors use PixelCNN++ [ ... ] it is difficult to defend the value of adding a VAE component to the model \u201d Our main contribution is a method based on an auxiliary loss to learn generative models that combine a non-degenerate latent variable structure with expressive autoregressive decoders . Among latent variable models , our model sets a new state-of-the-art result of 2.92 bpd . That is the same score as obtained by pixelCNN++ which does not learn a latent variable representation . Among VAE models with factored observation model of p ( x|z ) , VAE-IAF obtains the best quantitative score of 3.11 bpd on CIFAR10 . Our performance of 2.92 bpd represents an important improvement . The improvement over Lossy-VAE ( 2.95 bpd ) , former best model with latent variables , is 0.03 bpd . Our auxiliary loss allows us to use a more powerful autoregressive decoder , which allows us to improve over the Lossy-VAE result . The numbers that support these claims can be found in Table 1 . We will improve the presentation of Table 1 to highlight which models use latent variables and/or autoregressive decoders , to more easily appreciate our contribution in terms of the quantitative evaluation results . Unlike autoregressive models such as pixelCNN++ , latent variable models learn data representations which are useful for tasks such as semi-supervised learning , see e.g . ( Kingma et al. , NIPS 2014 ) . Therefore we believe that our work makes an important contribution to generative representation learning . AnnonReviewer3 : \u201c The authors do not describe how lambda was selected , which is critical for performance [ ... ] \u201d The results used when comparing with the state of the art are reported using our best configuration with lambda equal to 12 . We will clarify this in the text . In figures 3 , 4 and 5 qualitative and quantitative results are reported , as indicated , over a range of lambda values . The choice of lambda is important , but not critical , provided it is taken to be bigger than two : Figure 4 shows that beyond the first drop of 0.20 bpd when going from lambda =1 to lambda = 2 , the bpd further decreases monotonically to down to an improvement of 0.26 bpd for lambda=12 ."}}