{"year": "2021", "forum": "1Kxxduqpd3E", "title": "Rotograd: Dynamic Gradient Homogenization for Multitask Learning", "decision": "Reject", "meta_review": "The paper is proposing a novel representation of the GradNorm. GradNorm is presented as a Stackelberg game and its theory is used to understand and improve the convergence of the GradNorm. Moreover, in addition to the magnitude normalization, a direction normalization objective is added to the leader and a rotation matrix and a translation is used for this alignment. The paper is reviewed by three knowledgable reviewers and they unanimously agree on the rejection. Here are the major issues raised by the reviewers and the are chair:\n- The motivation behind the rotation matrix layers is not clear. It should be motivated in more detail and explained better with additional illustrations and analyses.\n- Empirical study is weak. More state of the art approaches from MTL should be included and more realistic datasets should be included.\n- The proposed method is not properly explained with respect to existing methods. There are MTL methods beyond GradNorm like PCGrad and MGDA (MTL as MOO). These methods also fix directions. Hence, it is not clear what is the relationship of the proposed method with these ones.\n\nI strongly recommend authors to improve their paper by fixing these major issues and submit to the next venue.", "reviews": [{"review_id": "1Kxxduqpd3E-0", "review_text": "In the paper , Rotograd is proposed as a new gradient-based approach for training multi-task deep neural networks based on GradNorm . GradNorm is first formulated as a Stackelberg game , where the leader aims at normalizing the gradient of different tasks and the follower aims at optimizing the collective weighted loss objective . Under this formulation , one can utilize theoretical guarantees of the Stackelberg game by making the leader have a learning rate that decays to zero faster than the follower . To further account for the different gradient directions , a learnable rotation and translation are applied to the representation of each task , such that the transformed representation match that of the single-task learning . By adding an additional term accounting for learning this rotation , the leader in the Stackelberg game will minimize the loss to homogenize both the gradient magnitude and match the representation to single-task learning as close as possible . In general , I find the direction of gradient homogenization for multi-task learning very important and interesting . The paper provides an interesting perspective through the Stackelberg game formulation , which provides a framework for selecting the learning rate of GradNorm type of gradient homogenization methods . The other contribution of the paper is a learnable task-specific rotation that aligns the task gradients with single-task learning . The proposing of a learnable rotation matrix seems an interesting idea , although I am not sure if it has been proposed previously for multi-task learning . I find the first contribution of formulating the problem as a Stackelberg game to be interesting and novel . However , in terms of the second contribution , I have some concerns about whether it makes the most sense by aligning the transformed representation with that of single-task learning . For MTL , one of the key benefits is learning a better representation by sharing it across different tasks to encourage helpful transfer between the tasks ; by constraining the transformed representation to be as close to the single-task learning representation , it might limit the transfer between tasks since the representation are constrained to be equivalent to that learned by single-task learning . I think it is helpful to think about using rotation invariant representations for aligning the gradient directions , but it is questionable to align it to that of the single-task learning . Another major concern is about the experimental results , full experiments are only conducted on one real-world dataset . The experiment on the second dataset seems to be very preliminary , which might not be sufficient to justify the proposed method empirically . Also on the second dataset , it seems the two different implementations of Rotograd have a large discrepancy in the results , which might need more investigation about why this happens . Meanwhile , many ablation studies seem to be missing . I am mostly interested to see experiments that validate the Stackelberg game formulation , for example by using different learning rates for the leader and the follower . Also , it would be interesting to see how the proposed Rotograd compares with pure GradNorm on gradient direction alignment . Overall , I feel the experiments are not complete for validating the effectiveness of the method . Some minor points : the description of d-grad method seems to be missing . Also , Yu et . al [ 2020 ] also deals with gradient aligning for MTL which could be considered as a baseline to compare with . Yu , T. , Kumar , S. , Gupta , A. , Levine , S. , Hausman , K. , & Finn , C. ( 2020 ) . Gradient surgery for multi-task learning . arXiv preprint arXiv:2001.06782 . -- After author 's response- I am not fully convinced by the explanation of the motivation behind rotation matrix , in particular why it is aligning with the single-task learning , which is counter-intuitive . The authors provided more ablation studies , however , the evaluation on datasets is still quite preliminary with some questions remaining ( such as why there is a discrepancy between the two versions of Rotograd on the second dataset ) . Therefore I am keeping my original score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "First of all , we would like to thank the reviewer for such an accurate summary of our work . We are delighted to observe interest in this novel formulation based on Stackelberg games , which we believe can shed some light on the dynamics of MTL methods such as GradNorm and Rotograd . * * 2nd contribution - Rotograd * * Up to the best of our knowledge , the closest approach to ours is `` d-grad via architecture '' , which implements using a NN-architecture an affine transformation of the output of every hidden layer in the multitask network with the aim of avoiding conflicting gradients . However , d-grad suffers from two major limitations in comparison to rotograd . First , in its formulation there is not an explicit objective on the gradient alignment , and thus , it does not provide any theoretical guarantees . As a consequence , it is not clear if the better performance shown in the empirical evaluation is due to avoiding conflicting gradients or to the additional expressiveness of the model ( as there is an additional affine transformation per layer ) . Second , d-grad does not impose restrictions on the affine transformation NNs ( or equivalently , on the individual gradient magnitudes ) , which may still result in a negative transfer between tasks due to the disparities in the individual gradient magnitudes ( problem addressed by Gradnorm and our extension , i.e. , rotograd ) . We will provide a detailed description of d-grad and its ( dis- ) similarities with rotograd in the revised version of the manuscript * * Experimental evaluation and representation aligment * * Please refer you to our general response for a detailed response on the empirical evaluation of rotograd , which includes an ablation study to validate the Stackelberg formulation , and to the question regarding the representation alignment ."}, {"review_id": "1Kxxduqpd3E-1", "review_text": "Summary : This paper proposes an MTL method that encourages the gradients on shared parameters to have similar directions across different tasks . The motivation is to reduce conflicts between gradients of different tasks , so that training can proceed more smoothly , and fit multiple tasks more easily . The paper introduces a new way of thinking about this kind of method , i.e. , through the lens of Stackelberg games , which could be useful in reasoning about the convergence of such methods . The method is shown to perform favorably against related methods , especially in regression settings . Strong points : Minimizing gradient conflict is a well-motivated way to reduce negative transfer . The algorithm description is detailed , and should be straightforward for others to implement . Stackelberg games are an interesting framework for thinking about methods like GradNorm and Rotograd that adaptively guide MTL training . Weak points : The theory is interesting at a high-level , but it is not clear that it provides insights on what makes Rotograd work . In the paper , one main takeaway from the Stackelberg games framework is that the methods converge if the leader \u2019 s learning rate is asymptotically smaller than the follower \u2019 s . This takeaway is implemented by decaying the leader \u2019 s learning rate , but it is not shown that this is a key point required for Rotograd to work . I would not be surprised if the results were unaffected if this decay were removed . If this point is really important , it should be illustrated in ablation studies . More broadly , since the point does not only apply to Rotograd , this ablation could also be done on Gradnorm and other methods . Such ablations would be one way to connect the theory to the methods . Another main takeaway from the theory is that the rotation matrices and translation vectors should be updated with gradient descent , instead of simply replacing them each step . Intuitively , the algorithm would still make sense and be simpler if R and d were simply replaced . Experiments showing that the gradient-descent update rule is necessary would help show the value of the theory . Similarly , the value of Proposition 4.1 is not clear . Is it to prove stability ? Does this have some particular connection to Rotograd , or is it a useful fact about hard parameter-sharing methods in general ? There is one ablation \u201c rotograd-sgd \u201d , but it is not clear how exactly it works : Can it simply update R and d however it wants , or is Eq.9 still used to regularize the updates in some way ? By adding the rotation matrices , it \u2019 s possible that information that would be useful to share across tasks is instead stored in these task-specific matrices . That is , conflict between tasks can beneficially lead to more general representations . Restricting R to be a rotation instead of any matrix is one step towards limiting the amount of information leakage into task-specific parameters . Is there a conceptual reason to expect that the benefits from reducing conflicts will outweigh this leakage ? The experiments are on an intentionally very small architecture , where one of the main issues is expressivity , which gives Rotograd an edge over methods that do not include an additional task-specific matrix . In Section 5.1 , does the method without Rotograd do poorly because there are no task-specific networks in that case ? Although Rotograd is motivated to reduce negative transfer , Table 1 shows that Rotograd does not reduce negative transfer , but rather improves positive transfer . That is , uniform does better than rotograd in the tasks where single-task is better than multi-task , but rotograd does better than uniform in the tasks where uniform is already better than single-task . This makes me think that the benefits of Rotograd are not coming from reducing negative transfer , but from somewhere else . Is there an explanation for why Rotograd does not work as well for multi-class classification tasks ( i.e. , performs worse than all other methods for Left and Right ) ? Is it because the task-specific heads have larger output sizes ? E.g. , could it be better to have a separate rotation matrix for each class ? Figure 4 in A.3 confirms that there is an issue here : the cosine similarity is not higher for rotograd for the classification tasks . Overall , from the limited scope of the experiments it is not clear that Rotograd would provide practical advantages over competing methods . The ChestXray experiments show that although Rotograd does not hurt much , it does not help overall compared too uniform . That said , it would be still be interesting to see whether insights from Stackelberg games could lead to practical improvements for this problem . Minor comments : The writing has some issues . These issues don \u2019 t make the work unclear , but they are a bit distracting . Some example suggestions for fixing distracting word choice : \u201c palliate \u201d - > \u201c alleviate \u201d , \u201c spoiled \u201d - > \u201c noted \u201d , \u201c we have not being able to propose Rotograd , but also to derive \u201d - > \u201c we have proposed Rotograd , and derived \u201d . There is also frequent non-standard mixing of em dashes with spaces and commas . \u201c $ [ r_k ( t ) ] ^\\alpha $ is a hyperparameter \u201d - > \u201c $ \\alpha $ is a hyperparameter \u201d The hyperparameter is \\alpha , correct ? - Update : I am very happy to see the new experiments that validate the implications of the Stackelberg games theory . The main drawback of the paper is that it is not clear that direction homogenization could lead to practical improvements for multi-task learning . The additional experiments in Table 2 are useful , and suggest that much of the benefit comes from the greater expressivity due to task-specific matrices .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for a detailed review that will help us to significantly improve the paper presentation . We will carefully revisit the manuscript to correct existing typos and improve the paper readability . We have posted a general response where we address points raised by several reviewers , including the ablation study , the `` leakage '' question , and a clarification regarding rotograd-sgd . Please refer to such a common response for details on these points . In addition , we below provide specific answers to the rest of your questions . * * Proposition 4.1 * * We recall that the original motivation of GradNorm is to equalize the magnitude contributions of the individual tasks to the gradients w.r.t.all the shared parameters $ \\Theta $ . However , for computational efficiency , the authors restrict the GradNorm solution to only a subset of the parameters ( corresponding in their experiments to one layer in the shared NN ) . In contrast , when working on the shared representation Z , we can derive a bound on the norm of the gradients w.r.t all the shared $ \\Theta $ for the individual tasks ( refer to Proposition 4.1 ) , and thus , as desired , apply GradNorm to the overall network by working on Z . Of course , by working on Z , we can not make exactly equal all the gradient magnitudes but instead force them to lay in a target interval . We will clarify this point in the final manuscript . * * Size of the architectures * * Negative transfer may occur especially in mid- and small-size architectures , as the individual tasks are `` forced '' to cooperate ( but also to compete for shared resources ) . Thus , in our experiments , we consider reduced architectures ( still with comparable accuracy compared to the original architecture ) to avoid scenarios where , due to the high number of parameters , the backbone can fit all tasks without requiring positive transfer . We would like to emphasize that , as the size of Z increases , the more likely that the gradients across tasks become orthogonal , i.e. , that different tasks use disjoint subsets of the shared intermediate representation Z . * * Rotograd on classification tasks * * We agree with the reviewer that in MNIST rotograd performs slightly worse for the classifications tasks , although significantly better for the other tasks , than the uniform approach . However , such a difference in the classification tasks decreases when a more thorough hyperparameter optimization for all the methods is performed , as shown in the following table ( which contains a summary of the new results that will be replacing Table 1 in the paper ) : | Method | Left \ud83e\udc51 | Right \ud83e\udc51 | Sum \ud83e\udc53 | Multiply \ud83e\udc53 | Density \ud83e\udc53 |\u0394 \ud83e\udc51 | | : -- | : -- : | : -- : | : : | : -- : | : : | : - : | | single task | * * 93.50 ( 00.47 ) * * | * * 90.65 ( 00.46 ) * * | 6.44 ( 4.63 ) | 159.08 ( 6.16 ) | 1.62 ( 1.78 ) | | | uniform | 90.15 ( 00.53 ) | 86.65 ( 00.41 ) | 5.14 ( 0.33 ) | 149.21 ( 5.97 ) | 0.51 ( 0.02 ) | 0.06 ( 0.11 ) | | rotograd | 89.01 ( 00.87 ) | 84.62 ( 01.19 ) | * * 4.54 ( 0.19 ) * * | * * 134.95 ( 5.92 ) * * | * * 0.23 ( 0.04 ) * * | * * 0.18 ( 0.06 ) * * | The above results correspond to a learning rate of 0.02 for the leader and an exponential decay of 0.99 per iteration . We believe that the slight deterioration in classification accuracy is due to the limited capacity of the NN , which trades-off the performance across all tasks . This can be explained by per-task learning dynamics shown in Fig.4 ( a ) of Appendix A3 , where we see that uniform aggressively optimizes both classification tasks , the other tasks are learned at a lower pace . A similar behavior is observed when looking at the cosine similarities for the individual tasks in Fig.4 ( b ) , where we can observe that while the gradient of the classification tasks is well aligned with the overall gradient evaluation ( cosine similarity approx.0.5 ) , this is not the case for the rest of tasks ( being , e.g. , the cosine similarity of the density below 0.1 ) . In contrast , rotograd forces a similar cosine similarity for all tasks , and thus eases that all tasks are learned at a similar pace ( although more slowly for the classification compared to uniform ) . In order words , rotograd makes the cosine similarity comparable across all tasks ( which means worsening the classification performance ) , whereas in uniform the density task is completely orthogonal to the others ."}, {"review_id": "1Kxxduqpd3E-2", "review_text": "This paper presents an extension of Gradnorm to address task conflicting due to discordant gradient direction . Specially it introduces a rotation matrix to rotate the hidden representation from the last shared layer . The authors put the proposed method in the context of game theory to show stability and convergence of the training , which might be of merit . The writing of the paper doesn \u2019 t meet the publication standard , needing major work to improve . There are many typos and awkward sentences , hindering understanding of their work . Also , there are many places that need clarification , for example , in Proposition 4.1 , the inverse of the gradient of Z with respective to \\theta needs to be calculated . So , what is the shape of this gradient matrix ? How it is necessarily to be a square matrix ? What ||\\Delta_ { \\theta } Z|| represents ? the F-norm ? There is lack of adequate explanation of the motivation behind the objective in Eq . ( 6 ) .By reading the paper , I have no idea about the two oracle functions , and why they are defined in the way shown in Eq. ( 8 ) .Eq . ( 3 ) is inaccurate , not aligning with that proposed in the GradNorm paper for the computation of L_ { grad } ^k . Eq . ( 9 ) is problematic . Why R_k z_i^t does not appear in the objective function of the first optimization problem ? If this is because z_i^ { k , t } = R_k z_i^t + d_k , then the objective in the second optimization problem would be just 0 . Why operating on z instead of the gradient in Gradnorm can resolve the discordant gradient issue among tasks is not properly justified . The reported empirical results are weak and do not support this method works as claimed .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Clarity of the paper * * As suggested by the reviewer , we will carefully review the writing of the paper to improve its readability and to make it more self-contained by providing all the necessary details on both the motivation and technical details of our approach . * * Proposition 4.1 and the role of Z * * The point of Prop . 4.1 is that we can bound the norm of the gradient w.r.t.the shared parameters ( the original goal of GradNorm ) by equalizing the norm of the gradient w.r.t the shared representation Z . Such bound is given in Eq.5 , which depends on the inverse of the gradient matrix . The gradient matrix is of the size of the number of parameters times the size of the shared representation Z , and thus in general case is not invertible ( as it is not squared ) . Fortunately , our theoretical results still hold when the gradient matrix is left-invertible , which does not require a squared matrix as it only requires that the rank for the gradient coincides with the dimensionality of Z ( which is in general significantly smaller than the number of parameters ) . Moreover , we point out that we do not need to restrict ourselves to a particular norm , as all norms are equivalent in finite-dimensional spaces and thus do not change the validity of our results . We will clarify this in the revised version of the paper . * * Oracle formulation ( Eq.8 ) and Leader objective ( Eq.9 ) * * There are two types of oracle functions in our formulation , which predict the next evaluation point of respectively the shared and the individual tasks ' representation and are approximated in Eq.8.These approximations correspond to a step of gradient descent ( as detailed before Eq.8 ) , and are necessary in order to be able to solve the leader objective , i.e. , to find the optimal transformation of the shared representation into the individual tasks ' representation in the next iteration of the learning algorithm . We will clarify this in the revised version of the paper . When considering the oracle approximation in Eq.8 , the leader objective in Eq.6 readily split into the two objectives in Eq.9 , plus a residual term that tends to zero when the two individual objectives in Eq.9 are solved with zero error ( refer to Eq.18 in appendix A2 for the exact relationship ) . When Eq.9 can not be perfectly solved , the solution of Eq.9 approximates ( up to a mismatch ) the solution of Eq.6.We refer the reviewer to appendix A2 for further details . * * GradNorm formulation * * We do not see any difference between Eq.3 in our paper and Eq.2 in the GradNorm paper * , except for the adaptation to our notation and for the fact that in Eq.3 we only consider one particular task ( i.e. , we have removed the summatory over tasks in Eq.2 of GradNorm ) . * https : //arxiv.org/pdf/1711.02257.pdf"}], "0": {"review_id": "1Kxxduqpd3E-0", "review_text": "In the paper , Rotograd is proposed as a new gradient-based approach for training multi-task deep neural networks based on GradNorm . GradNorm is first formulated as a Stackelberg game , where the leader aims at normalizing the gradient of different tasks and the follower aims at optimizing the collective weighted loss objective . Under this formulation , one can utilize theoretical guarantees of the Stackelberg game by making the leader have a learning rate that decays to zero faster than the follower . To further account for the different gradient directions , a learnable rotation and translation are applied to the representation of each task , such that the transformed representation match that of the single-task learning . By adding an additional term accounting for learning this rotation , the leader in the Stackelberg game will minimize the loss to homogenize both the gradient magnitude and match the representation to single-task learning as close as possible . In general , I find the direction of gradient homogenization for multi-task learning very important and interesting . The paper provides an interesting perspective through the Stackelberg game formulation , which provides a framework for selecting the learning rate of GradNorm type of gradient homogenization methods . The other contribution of the paper is a learnable task-specific rotation that aligns the task gradients with single-task learning . The proposing of a learnable rotation matrix seems an interesting idea , although I am not sure if it has been proposed previously for multi-task learning . I find the first contribution of formulating the problem as a Stackelberg game to be interesting and novel . However , in terms of the second contribution , I have some concerns about whether it makes the most sense by aligning the transformed representation with that of single-task learning . For MTL , one of the key benefits is learning a better representation by sharing it across different tasks to encourage helpful transfer between the tasks ; by constraining the transformed representation to be as close to the single-task learning representation , it might limit the transfer between tasks since the representation are constrained to be equivalent to that learned by single-task learning . I think it is helpful to think about using rotation invariant representations for aligning the gradient directions , but it is questionable to align it to that of the single-task learning . Another major concern is about the experimental results , full experiments are only conducted on one real-world dataset . The experiment on the second dataset seems to be very preliminary , which might not be sufficient to justify the proposed method empirically . Also on the second dataset , it seems the two different implementations of Rotograd have a large discrepancy in the results , which might need more investigation about why this happens . Meanwhile , many ablation studies seem to be missing . I am mostly interested to see experiments that validate the Stackelberg game formulation , for example by using different learning rates for the leader and the follower . Also , it would be interesting to see how the proposed Rotograd compares with pure GradNorm on gradient direction alignment . Overall , I feel the experiments are not complete for validating the effectiveness of the method . Some minor points : the description of d-grad method seems to be missing . Also , Yu et . al [ 2020 ] also deals with gradient aligning for MTL which could be considered as a baseline to compare with . Yu , T. , Kumar , S. , Gupta , A. , Levine , S. , Hausman , K. , & Finn , C. ( 2020 ) . Gradient surgery for multi-task learning . arXiv preprint arXiv:2001.06782 . -- After author 's response- I am not fully convinced by the explanation of the motivation behind rotation matrix , in particular why it is aligning with the single-task learning , which is counter-intuitive . The authors provided more ablation studies , however , the evaluation on datasets is still quite preliminary with some questions remaining ( such as why there is a discrepancy between the two versions of Rotograd on the second dataset ) . Therefore I am keeping my original score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "First of all , we would like to thank the reviewer for such an accurate summary of our work . We are delighted to observe interest in this novel formulation based on Stackelberg games , which we believe can shed some light on the dynamics of MTL methods such as GradNorm and Rotograd . * * 2nd contribution - Rotograd * * Up to the best of our knowledge , the closest approach to ours is `` d-grad via architecture '' , which implements using a NN-architecture an affine transformation of the output of every hidden layer in the multitask network with the aim of avoiding conflicting gradients . However , d-grad suffers from two major limitations in comparison to rotograd . First , in its formulation there is not an explicit objective on the gradient alignment , and thus , it does not provide any theoretical guarantees . As a consequence , it is not clear if the better performance shown in the empirical evaluation is due to avoiding conflicting gradients or to the additional expressiveness of the model ( as there is an additional affine transformation per layer ) . Second , d-grad does not impose restrictions on the affine transformation NNs ( or equivalently , on the individual gradient magnitudes ) , which may still result in a negative transfer between tasks due to the disparities in the individual gradient magnitudes ( problem addressed by Gradnorm and our extension , i.e. , rotograd ) . We will provide a detailed description of d-grad and its ( dis- ) similarities with rotograd in the revised version of the manuscript * * Experimental evaluation and representation aligment * * Please refer you to our general response for a detailed response on the empirical evaluation of rotograd , which includes an ablation study to validate the Stackelberg formulation , and to the question regarding the representation alignment ."}, "1": {"review_id": "1Kxxduqpd3E-1", "review_text": "Summary : This paper proposes an MTL method that encourages the gradients on shared parameters to have similar directions across different tasks . The motivation is to reduce conflicts between gradients of different tasks , so that training can proceed more smoothly , and fit multiple tasks more easily . The paper introduces a new way of thinking about this kind of method , i.e. , through the lens of Stackelberg games , which could be useful in reasoning about the convergence of such methods . The method is shown to perform favorably against related methods , especially in regression settings . Strong points : Minimizing gradient conflict is a well-motivated way to reduce negative transfer . The algorithm description is detailed , and should be straightforward for others to implement . Stackelberg games are an interesting framework for thinking about methods like GradNorm and Rotograd that adaptively guide MTL training . Weak points : The theory is interesting at a high-level , but it is not clear that it provides insights on what makes Rotograd work . In the paper , one main takeaway from the Stackelberg games framework is that the methods converge if the leader \u2019 s learning rate is asymptotically smaller than the follower \u2019 s . This takeaway is implemented by decaying the leader \u2019 s learning rate , but it is not shown that this is a key point required for Rotograd to work . I would not be surprised if the results were unaffected if this decay were removed . If this point is really important , it should be illustrated in ablation studies . More broadly , since the point does not only apply to Rotograd , this ablation could also be done on Gradnorm and other methods . Such ablations would be one way to connect the theory to the methods . Another main takeaway from the theory is that the rotation matrices and translation vectors should be updated with gradient descent , instead of simply replacing them each step . Intuitively , the algorithm would still make sense and be simpler if R and d were simply replaced . Experiments showing that the gradient-descent update rule is necessary would help show the value of the theory . Similarly , the value of Proposition 4.1 is not clear . Is it to prove stability ? Does this have some particular connection to Rotograd , or is it a useful fact about hard parameter-sharing methods in general ? There is one ablation \u201c rotograd-sgd \u201d , but it is not clear how exactly it works : Can it simply update R and d however it wants , or is Eq.9 still used to regularize the updates in some way ? By adding the rotation matrices , it \u2019 s possible that information that would be useful to share across tasks is instead stored in these task-specific matrices . That is , conflict between tasks can beneficially lead to more general representations . Restricting R to be a rotation instead of any matrix is one step towards limiting the amount of information leakage into task-specific parameters . Is there a conceptual reason to expect that the benefits from reducing conflicts will outweigh this leakage ? The experiments are on an intentionally very small architecture , where one of the main issues is expressivity , which gives Rotograd an edge over methods that do not include an additional task-specific matrix . In Section 5.1 , does the method without Rotograd do poorly because there are no task-specific networks in that case ? Although Rotograd is motivated to reduce negative transfer , Table 1 shows that Rotograd does not reduce negative transfer , but rather improves positive transfer . That is , uniform does better than rotograd in the tasks where single-task is better than multi-task , but rotograd does better than uniform in the tasks where uniform is already better than single-task . This makes me think that the benefits of Rotograd are not coming from reducing negative transfer , but from somewhere else . Is there an explanation for why Rotograd does not work as well for multi-class classification tasks ( i.e. , performs worse than all other methods for Left and Right ) ? Is it because the task-specific heads have larger output sizes ? E.g. , could it be better to have a separate rotation matrix for each class ? Figure 4 in A.3 confirms that there is an issue here : the cosine similarity is not higher for rotograd for the classification tasks . Overall , from the limited scope of the experiments it is not clear that Rotograd would provide practical advantages over competing methods . The ChestXray experiments show that although Rotograd does not hurt much , it does not help overall compared too uniform . That said , it would be still be interesting to see whether insights from Stackelberg games could lead to practical improvements for this problem . Minor comments : The writing has some issues . These issues don \u2019 t make the work unclear , but they are a bit distracting . Some example suggestions for fixing distracting word choice : \u201c palliate \u201d - > \u201c alleviate \u201d , \u201c spoiled \u201d - > \u201c noted \u201d , \u201c we have not being able to propose Rotograd , but also to derive \u201d - > \u201c we have proposed Rotograd , and derived \u201d . There is also frequent non-standard mixing of em dashes with spaces and commas . \u201c $ [ r_k ( t ) ] ^\\alpha $ is a hyperparameter \u201d - > \u201c $ \\alpha $ is a hyperparameter \u201d The hyperparameter is \\alpha , correct ? - Update : I am very happy to see the new experiments that validate the implications of the Stackelberg games theory . The main drawback of the paper is that it is not clear that direction homogenization could lead to practical improvements for multi-task learning . The additional experiments in Table 2 are useful , and suggest that much of the benefit comes from the greater expressivity due to task-specific matrices .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for a detailed review that will help us to significantly improve the paper presentation . We will carefully revisit the manuscript to correct existing typos and improve the paper readability . We have posted a general response where we address points raised by several reviewers , including the ablation study , the `` leakage '' question , and a clarification regarding rotograd-sgd . Please refer to such a common response for details on these points . In addition , we below provide specific answers to the rest of your questions . * * Proposition 4.1 * * We recall that the original motivation of GradNorm is to equalize the magnitude contributions of the individual tasks to the gradients w.r.t.all the shared parameters $ \\Theta $ . However , for computational efficiency , the authors restrict the GradNorm solution to only a subset of the parameters ( corresponding in their experiments to one layer in the shared NN ) . In contrast , when working on the shared representation Z , we can derive a bound on the norm of the gradients w.r.t all the shared $ \\Theta $ for the individual tasks ( refer to Proposition 4.1 ) , and thus , as desired , apply GradNorm to the overall network by working on Z . Of course , by working on Z , we can not make exactly equal all the gradient magnitudes but instead force them to lay in a target interval . We will clarify this point in the final manuscript . * * Size of the architectures * * Negative transfer may occur especially in mid- and small-size architectures , as the individual tasks are `` forced '' to cooperate ( but also to compete for shared resources ) . Thus , in our experiments , we consider reduced architectures ( still with comparable accuracy compared to the original architecture ) to avoid scenarios where , due to the high number of parameters , the backbone can fit all tasks without requiring positive transfer . We would like to emphasize that , as the size of Z increases , the more likely that the gradients across tasks become orthogonal , i.e. , that different tasks use disjoint subsets of the shared intermediate representation Z . * * Rotograd on classification tasks * * We agree with the reviewer that in MNIST rotograd performs slightly worse for the classifications tasks , although significantly better for the other tasks , than the uniform approach . However , such a difference in the classification tasks decreases when a more thorough hyperparameter optimization for all the methods is performed , as shown in the following table ( which contains a summary of the new results that will be replacing Table 1 in the paper ) : | Method | Left \ud83e\udc51 | Right \ud83e\udc51 | Sum \ud83e\udc53 | Multiply \ud83e\udc53 | Density \ud83e\udc53 |\u0394 \ud83e\udc51 | | : -- | : -- : | : -- : | : : | : -- : | : : | : - : | | single task | * * 93.50 ( 00.47 ) * * | * * 90.65 ( 00.46 ) * * | 6.44 ( 4.63 ) | 159.08 ( 6.16 ) | 1.62 ( 1.78 ) | | | uniform | 90.15 ( 00.53 ) | 86.65 ( 00.41 ) | 5.14 ( 0.33 ) | 149.21 ( 5.97 ) | 0.51 ( 0.02 ) | 0.06 ( 0.11 ) | | rotograd | 89.01 ( 00.87 ) | 84.62 ( 01.19 ) | * * 4.54 ( 0.19 ) * * | * * 134.95 ( 5.92 ) * * | * * 0.23 ( 0.04 ) * * | * * 0.18 ( 0.06 ) * * | The above results correspond to a learning rate of 0.02 for the leader and an exponential decay of 0.99 per iteration . We believe that the slight deterioration in classification accuracy is due to the limited capacity of the NN , which trades-off the performance across all tasks . This can be explained by per-task learning dynamics shown in Fig.4 ( a ) of Appendix A3 , where we see that uniform aggressively optimizes both classification tasks , the other tasks are learned at a lower pace . A similar behavior is observed when looking at the cosine similarities for the individual tasks in Fig.4 ( b ) , where we can observe that while the gradient of the classification tasks is well aligned with the overall gradient evaluation ( cosine similarity approx.0.5 ) , this is not the case for the rest of tasks ( being , e.g. , the cosine similarity of the density below 0.1 ) . In contrast , rotograd forces a similar cosine similarity for all tasks , and thus eases that all tasks are learned at a similar pace ( although more slowly for the classification compared to uniform ) . In order words , rotograd makes the cosine similarity comparable across all tasks ( which means worsening the classification performance ) , whereas in uniform the density task is completely orthogonal to the others ."}, "2": {"review_id": "1Kxxduqpd3E-2", "review_text": "This paper presents an extension of Gradnorm to address task conflicting due to discordant gradient direction . Specially it introduces a rotation matrix to rotate the hidden representation from the last shared layer . The authors put the proposed method in the context of game theory to show stability and convergence of the training , which might be of merit . The writing of the paper doesn \u2019 t meet the publication standard , needing major work to improve . There are many typos and awkward sentences , hindering understanding of their work . Also , there are many places that need clarification , for example , in Proposition 4.1 , the inverse of the gradient of Z with respective to \\theta needs to be calculated . So , what is the shape of this gradient matrix ? How it is necessarily to be a square matrix ? What ||\\Delta_ { \\theta } Z|| represents ? the F-norm ? There is lack of adequate explanation of the motivation behind the objective in Eq . ( 6 ) .By reading the paper , I have no idea about the two oracle functions , and why they are defined in the way shown in Eq. ( 8 ) .Eq . ( 3 ) is inaccurate , not aligning with that proposed in the GradNorm paper for the computation of L_ { grad } ^k . Eq . ( 9 ) is problematic . Why R_k z_i^t does not appear in the objective function of the first optimization problem ? If this is because z_i^ { k , t } = R_k z_i^t + d_k , then the objective in the second optimization problem would be just 0 . Why operating on z instead of the gradient in Gradnorm can resolve the discordant gradient issue among tasks is not properly justified . The reported empirical results are weak and do not support this method works as claimed .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Clarity of the paper * * As suggested by the reviewer , we will carefully review the writing of the paper to improve its readability and to make it more self-contained by providing all the necessary details on both the motivation and technical details of our approach . * * Proposition 4.1 and the role of Z * * The point of Prop . 4.1 is that we can bound the norm of the gradient w.r.t.the shared parameters ( the original goal of GradNorm ) by equalizing the norm of the gradient w.r.t the shared representation Z . Such bound is given in Eq.5 , which depends on the inverse of the gradient matrix . The gradient matrix is of the size of the number of parameters times the size of the shared representation Z , and thus in general case is not invertible ( as it is not squared ) . Fortunately , our theoretical results still hold when the gradient matrix is left-invertible , which does not require a squared matrix as it only requires that the rank for the gradient coincides with the dimensionality of Z ( which is in general significantly smaller than the number of parameters ) . Moreover , we point out that we do not need to restrict ourselves to a particular norm , as all norms are equivalent in finite-dimensional spaces and thus do not change the validity of our results . We will clarify this in the revised version of the paper . * * Oracle formulation ( Eq.8 ) and Leader objective ( Eq.9 ) * * There are two types of oracle functions in our formulation , which predict the next evaluation point of respectively the shared and the individual tasks ' representation and are approximated in Eq.8.These approximations correspond to a step of gradient descent ( as detailed before Eq.8 ) , and are necessary in order to be able to solve the leader objective , i.e. , to find the optimal transformation of the shared representation into the individual tasks ' representation in the next iteration of the learning algorithm . We will clarify this in the revised version of the paper . When considering the oracle approximation in Eq.8 , the leader objective in Eq.6 readily split into the two objectives in Eq.9 , plus a residual term that tends to zero when the two individual objectives in Eq.9 are solved with zero error ( refer to Eq.18 in appendix A2 for the exact relationship ) . When Eq.9 can not be perfectly solved , the solution of Eq.9 approximates ( up to a mismatch ) the solution of Eq.6.We refer the reviewer to appendix A2 for further details . * * GradNorm formulation * * We do not see any difference between Eq.3 in our paper and Eq.2 in the GradNorm paper * , except for the adaptation to our notation and for the fact that in Eq.3 we only consider one particular task ( i.e. , we have removed the summatory over tasks in Eq.2 of GradNorm ) . * https : //arxiv.org/pdf/1711.02257.pdf"}}