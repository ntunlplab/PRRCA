{"year": "2020", "forum": "Hye87grYDH", "title": "Sparse Transformer: Concentrated Attention Through Explicit Selection", "decision": "Reject", "meta_review": "The paper proposes a variant of Sparse Transformer where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer.\n\nWhile the proposed idea is simple, easy to implement, and it does not add additional computational or memory cost, the reviewers raised several concerns in the discussion phase, including: several baselines missing from the tables; incomplete experimental details; incorrect/misleading selection of best performing model in tables of results (e.g. In Table 1, the authors boldface their results on En-De (29.4) and De-En (35.6) but in fact, the best performance on these is achieved by competing models, respectively 29.7 and 35.7. The caption claims their model \"achieves the state-of-the-art performances in En-Vi and De-En\" but this is not true for De-En (albeit by 0.1). In Table 3, they boldface their result of 1.05 but the best result is 1.02; the text says their model beats the Transf-XL \"with an advantage\" (of 0.01) but do not point out that the advantage of Adaptive-span over their model is 3 times as large (0.03)).\n\nThis prevents me from recommending acceptance of this paper in its current form. I strongly encourage the authors to address these concerns in a future submission.", "reviews": [{"review_id": "Hye87grYDH-0", "review_text": "CONTRIBUTIONS: C1. Sparse Transformer: A modification of the Transformer, limiting attention to the top-k locations. (That is a complete statement of the proposed model.) C2. Experiments showing that, quantitatively, the Sparse Transformer out-performs the standard Transformer on translation, language modeling, and image captioning. C3. Experiments showing that, qualitatively, in translation, when generating a target word, the Sparse Transformer better focusses attention on the aligned source word RATING: Reject REASONS FOR RATING (SUMMARY). The innovativeness seems low given the several previous proposals for sparse attention, the results are not dramatic enough to compensate for the lack of originality, and the comparison to other models is wanting. REVIEW Strengths: The paper is clearly written. The question of whether the Transformer\u2019s attention is too diffuse is of interest. The proposal is admirably simple. The quantitative metrics include comparison against many alternative models. Weaknesses: A primary area of deficiency concerns the relation of the proposed model to other proposals for sparse attention: the authors cite 5 of them (and 2 more are cited in the comment by Cui). The paper should clearly identify the differences between the proposed model and earlier models: it does not discuss this at all. The deficiencies in these previous models should be clearly stated and demonstrated: they are only described as \u201ceither restricted range of attention or training difficulty\u201d (Sec 6). A rationale for why the proposal can be expected to remedy these deficiencies should be stated clearly: it is not stated at all. Experimental demonstration that the proposed innovation actually remedies the identified deficiencies should be provided, but is not. A proposal to use a top-k filter immediately raises the question of the value of k. This is not discussed at all. In particular, no empirical results are given concerning the sensitivity of the reported successes to choosing the correct value for k. We are only told that \u201ck is usually a small number such as 5 or 10\u201d (Sec 3). The experimental details in the appendix do not even state the value of k used in the models reported. It is an interesting discovery that in the translation task, attention at the top layer of the standard Transformer is strongly focused on the end of the input. This is described as an \u201cobvious problem\u201d (Sec 7). But it can\u2019t obviously be a problem because the performance of the standard Transformer is only very slightly lower than that of the Sparse Transformer: if anything is obvious, it is that processing in the standard Transformer packs a lot of information into its final encoding of the end of the input string, which functions rather like an encoding of the entire sentence. Presumably, the experimental results reported are those from a single model, since we are not told otherwise. There should be multiple tests of the models with different random initializations, with the means and variances of measures reported. It is possible, however, that limitations of computational resources made that infeasible, although the Appendix seems to indicate that no hyperparameter tuning was done, which greatly reduces computational cost. COMMENTS FOR IMPROVEMENT, NOT RELEVANT TO RATING DECISION Although the tiny sample of visualized attention weights provided is useful, a large-scale quantitative assessment of a main claim concerning translation might well be possible: that attention is in fact concentrated on the aligned word might be testable using an aligned bilingual corpus or perhaps an existing forced aligner could be used. Much space could be saved: it is not necessary to review the standard Transformer, and the modification proposed is so simple that it can be precisely stated in one sentence (see C1 above): the entire page taken up by Sec. 3 is unnecessary, as it adds only implementation details. Errors that took more than a moment to mentally correct, all on p. 12: The definition of the BPC should be E[log P(x(t+1) | h(t))]: all parentheses are missing \u201cregrad\u201d should be \u201cregard\u201d \u201cderivative\u201d should be \u201cdifferentiable\u201d in the final sentence", "rating": "1: Reject", "reply_text": "Thank you for your detailed and helpful reviewers . We have updated the pdf . Questions about the novelty and significance : In the paper Explicit sparse Transformer , the proposed method is straightforward , simple , and easy to implement . We invested a lot of time in the study of sparse transformers . In January of this year , we submitted a model to SQuAD in the name of Sparse Transformer ( but it does n't work because we do not apply the mothod to the pretrain phase at the time ) . We also thought about other complicated methods of sparsee attention , but the current method is simple and effective . Although several sparse attention methods have been applied to the sequence-to-sequence transformer model and improve the performance , detailed comparisons between the proposal and their methods based on strong baseline show that our methods are much faster in training and testing and achieve slightly better results Question about the number of experiments In the current version of the paper , for all methods on IWSLT datasets , we have experimented under three different initializations , and reported the highest results . Because of resource limits , we didn \u2019 t do this on other data sets . Question about the Alignment of Transformer : We found that the randomly selected samples of the last two layers of transformer would cause excessive attention to the end token . Question about the redundancy : We have moved the review of standard transformer into Appendix and it may help newcomers ."}, {"review_id": "Hye87grYDH-1", "review_text": "1. What is the specific question/problem tackled by the paper? The authors tackle the problem of sparse attention for various generative modeling tasks such as machine translation and image captioning. The main motivation behind studying this problem is the premise that sparse varieties of attention might generalize better than full attention. The authors propose a sparse attention mechanism based on the top-k selection where all attention values in a row are dropped if they are not higher than the k^{th} largest item in the row. Since this is a non-differentiable operation the authors propose to train this model by setting the gradients of the non-selected items to 0. The authors report results on machine translation, language modeling and image captioning. 2. Is the approach well motivated, including being well-placed in the literature? In my view the main reasons to study sparse variants of attention are either 1) scale to sequences longer than are possible with full attention (this is e.g., the motivation behind [1]) or 2) generalize better than full attention. The motivation of this work seems to be the latter as the authors claim improvements in terms of performance over full attention. The authors cite prior work on sparse attention mechanisms. 3. Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. The authors report good results on machine translation, showing that their sparse attention method improves performance on En-De to 29.4 BLEU, on De-En to 35.6 BLEU and on En-Vi to 31.1 BLEU, improving on full attention baselines. However, the authors have not submitted code for reproducing their results. The authors also do not report what choice of k is used for the top-k operation and how they made their choice of the optimal k? The paper would be well served by more ablation experiments demonstrating what the impact the choice of k has on the model performance. For example, I would expect to be able to reproduce original Transformer results using k = maximum sequence length. I am also not fully clear about how gradients are propagated through the top-k operation. It seems that if an index is not selected (i.e. it's attention value is smaller than top-k) it's gradient is set to 0. However, this seems problematic - for e.g., in the initial stages an important item might have a low attention value due to random initialization and might not make it to the top-k. Because of the way gradients are propagated it will not receive any gradient, and therefore will not be incentivized to increase its value. This doesn't seem like a good solution to me. Since the paper is mainly an empirical work, it would be improved by open-sourcing anonymized code so that it's results and claims may be verified. It would also be improved in more ablation experiments or explanations in what the optimal choice of k should be for the top-k and how that affects the results. [1] Generating Long Sequences with Sparse Transformers by Child et al (https://arxiv.org/abs/1904.10509) ", "rating": "3: Weak Reject", "reply_text": "Thank you for your valuable comments . We have empirically addressed your concerns about the optimal choice of k and the comparisons to the previous sparse attention methods in the updates . As you said , our approach is simple , so the Explicit Sparse Transformer is significantly faster in both inference and training than the previous methods of sparse attention in Transformer . For open-sourcing , we provide a simple implementation of the method in the Appendix , and we will publicly release all the code and training instructions in the near future to help replicate this work . For sparse attention methods of local attention ( OpenAI \u2019 s sparse transfomers , adaptive span ) , these methods directly ignore long-distance dependence , and they mainly work for language models but have not been proved effective on standard transformers . Therefore , we did not compare them in the experiment , but we take the variants of sparsemax into consideration because they have demonstrate improvement in standard transformer ."}, {"review_id": "Hye87grYDH-2", "review_text": "The paper proposes \"sparse self-attention\", where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer model. In general, the idea is quite simple and easy to implement. It doesn't add any computational or memory cost. The paper is well written and easy to read. The diverse experimental results show that it brings an improvement. And I think this can be combined with other improvements of Transformer. However, there are quite many baselines are missing from the tables. The sota on De-En is actually 35.7 by Fonollosa et.al. On enwik8, Transformer XL is not the best medium sized model as the authors claimed. See below: NTM En-De: - Wu et.al. Pay Less Attention with Lightweight and Dynamic Convolutions, 2019 - Ott et.al. Scaling Neural Machine Translation, 2018 NTM En-Vi: - Wang et.al. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation, 2018 NTM De-En: - Wu et.al. Pay Less Attention with Lightweight and Dynamic Convolutions, 2019 - Fonollosa et.al. Joint Source-Target Self Attention with Locality Constraints, 2019 - He et.al. Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation, 2018 LM Enwik8: - Sukhbaatar et.al, Adaptive Attention Span in Transformers, 2019 Other comments: - More experimental details are needed. What is the value K? How different K values affect performance? What is the number of parameters of NMT models. - The claim \"top layer of the vanilla Transformer focuses on the end position of the text\" can't be true generally. Probably only true for a certain task. - Where the numbers in Figure 1 come from? Is it a single attention head or average of all? - Page 4, \"the high are ...\" probably typo? - The related work is missing \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" by Rae et.al., which also uses sparse attention.", "rating": "6: Weak Accept", "reply_text": "Thanks for your careful reviews , analysis of the value of k , comparisons between previous sparse attention methods , and missing reference are all included in the newer version of the paper . The answers to the remaining questions are as follows : Q : More experimental details are needed . What is the number of parameters of NMT models . A : We have added more experimental details to the Appendix . Since our model does not increase the number of parameters , the parameter quantities of the machine translation model are the same as the Transformer base and the Transformer large respectively on the IWSLT and WMT datasets . Q : The claim `` top layer of the vanilla Transformer focuses on the end position of the text '' ca n't be true generally . Probably only true for a certain task . A : we observe similar phenomena in other tasks , such as IWSLT German to English translation . Q : Where the numbers in Figure 1 come from ? Is it a single attention head or average of all ? A : For the sake of simplicity , we show the attention score of first head ."}], "0": {"review_id": "Hye87grYDH-0", "review_text": "CONTRIBUTIONS: C1. Sparse Transformer: A modification of the Transformer, limiting attention to the top-k locations. (That is a complete statement of the proposed model.) C2. Experiments showing that, quantitatively, the Sparse Transformer out-performs the standard Transformer on translation, language modeling, and image captioning. C3. Experiments showing that, qualitatively, in translation, when generating a target word, the Sparse Transformer better focusses attention on the aligned source word RATING: Reject REASONS FOR RATING (SUMMARY). The innovativeness seems low given the several previous proposals for sparse attention, the results are not dramatic enough to compensate for the lack of originality, and the comparison to other models is wanting. REVIEW Strengths: The paper is clearly written. The question of whether the Transformer\u2019s attention is too diffuse is of interest. The proposal is admirably simple. The quantitative metrics include comparison against many alternative models. Weaknesses: A primary area of deficiency concerns the relation of the proposed model to other proposals for sparse attention: the authors cite 5 of them (and 2 more are cited in the comment by Cui). The paper should clearly identify the differences between the proposed model and earlier models: it does not discuss this at all. The deficiencies in these previous models should be clearly stated and demonstrated: they are only described as \u201ceither restricted range of attention or training difficulty\u201d (Sec 6). A rationale for why the proposal can be expected to remedy these deficiencies should be stated clearly: it is not stated at all. Experimental demonstration that the proposed innovation actually remedies the identified deficiencies should be provided, but is not. A proposal to use a top-k filter immediately raises the question of the value of k. This is not discussed at all. In particular, no empirical results are given concerning the sensitivity of the reported successes to choosing the correct value for k. We are only told that \u201ck is usually a small number such as 5 or 10\u201d (Sec 3). The experimental details in the appendix do not even state the value of k used in the models reported. It is an interesting discovery that in the translation task, attention at the top layer of the standard Transformer is strongly focused on the end of the input. This is described as an \u201cobvious problem\u201d (Sec 7). But it can\u2019t obviously be a problem because the performance of the standard Transformer is only very slightly lower than that of the Sparse Transformer: if anything is obvious, it is that processing in the standard Transformer packs a lot of information into its final encoding of the end of the input string, which functions rather like an encoding of the entire sentence. Presumably, the experimental results reported are those from a single model, since we are not told otherwise. There should be multiple tests of the models with different random initializations, with the means and variances of measures reported. It is possible, however, that limitations of computational resources made that infeasible, although the Appendix seems to indicate that no hyperparameter tuning was done, which greatly reduces computational cost. COMMENTS FOR IMPROVEMENT, NOT RELEVANT TO RATING DECISION Although the tiny sample of visualized attention weights provided is useful, a large-scale quantitative assessment of a main claim concerning translation might well be possible: that attention is in fact concentrated on the aligned word might be testable using an aligned bilingual corpus or perhaps an existing forced aligner could be used. Much space could be saved: it is not necessary to review the standard Transformer, and the modification proposed is so simple that it can be precisely stated in one sentence (see C1 above): the entire page taken up by Sec. 3 is unnecessary, as it adds only implementation details. Errors that took more than a moment to mentally correct, all on p. 12: The definition of the BPC should be E[log P(x(t+1) | h(t))]: all parentheses are missing \u201cregrad\u201d should be \u201cregard\u201d \u201cderivative\u201d should be \u201cdifferentiable\u201d in the final sentence", "rating": "1: Reject", "reply_text": "Thank you for your detailed and helpful reviewers . We have updated the pdf . Questions about the novelty and significance : In the paper Explicit sparse Transformer , the proposed method is straightforward , simple , and easy to implement . We invested a lot of time in the study of sparse transformers . In January of this year , we submitted a model to SQuAD in the name of Sparse Transformer ( but it does n't work because we do not apply the mothod to the pretrain phase at the time ) . We also thought about other complicated methods of sparsee attention , but the current method is simple and effective . Although several sparse attention methods have been applied to the sequence-to-sequence transformer model and improve the performance , detailed comparisons between the proposal and their methods based on strong baseline show that our methods are much faster in training and testing and achieve slightly better results Question about the number of experiments In the current version of the paper , for all methods on IWSLT datasets , we have experimented under three different initializations , and reported the highest results . Because of resource limits , we didn \u2019 t do this on other data sets . Question about the Alignment of Transformer : We found that the randomly selected samples of the last two layers of transformer would cause excessive attention to the end token . Question about the redundancy : We have moved the review of standard transformer into Appendix and it may help newcomers ."}, "1": {"review_id": "Hye87grYDH-1", "review_text": "1. What is the specific question/problem tackled by the paper? The authors tackle the problem of sparse attention for various generative modeling tasks such as machine translation and image captioning. The main motivation behind studying this problem is the premise that sparse varieties of attention might generalize better than full attention. The authors propose a sparse attention mechanism based on the top-k selection where all attention values in a row are dropped if they are not higher than the k^{th} largest item in the row. Since this is a non-differentiable operation the authors propose to train this model by setting the gradients of the non-selected items to 0. The authors report results on machine translation, language modeling and image captioning. 2. Is the approach well motivated, including being well-placed in the literature? In my view the main reasons to study sparse variants of attention are either 1) scale to sequences longer than are possible with full attention (this is e.g., the motivation behind [1]) or 2) generalize better than full attention. The motivation of this work seems to be the latter as the authors claim improvements in terms of performance over full attention. The authors cite prior work on sparse attention mechanisms. 3. Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. The authors report good results on machine translation, showing that their sparse attention method improves performance on En-De to 29.4 BLEU, on De-En to 35.6 BLEU and on En-Vi to 31.1 BLEU, improving on full attention baselines. However, the authors have not submitted code for reproducing their results. The authors also do not report what choice of k is used for the top-k operation and how they made their choice of the optimal k? The paper would be well served by more ablation experiments demonstrating what the impact the choice of k has on the model performance. For example, I would expect to be able to reproduce original Transformer results using k = maximum sequence length. I am also not fully clear about how gradients are propagated through the top-k operation. It seems that if an index is not selected (i.e. it's attention value is smaller than top-k) it's gradient is set to 0. However, this seems problematic - for e.g., in the initial stages an important item might have a low attention value due to random initialization and might not make it to the top-k. Because of the way gradients are propagated it will not receive any gradient, and therefore will not be incentivized to increase its value. This doesn't seem like a good solution to me. Since the paper is mainly an empirical work, it would be improved by open-sourcing anonymized code so that it's results and claims may be verified. It would also be improved in more ablation experiments or explanations in what the optimal choice of k should be for the top-k and how that affects the results. [1] Generating Long Sequences with Sparse Transformers by Child et al (https://arxiv.org/abs/1904.10509) ", "rating": "3: Weak Reject", "reply_text": "Thank you for your valuable comments . We have empirically addressed your concerns about the optimal choice of k and the comparisons to the previous sparse attention methods in the updates . As you said , our approach is simple , so the Explicit Sparse Transformer is significantly faster in both inference and training than the previous methods of sparse attention in Transformer . For open-sourcing , we provide a simple implementation of the method in the Appendix , and we will publicly release all the code and training instructions in the near future to help replicate this work . For sparse attention methods of local attention ( OpenAI \u2019 s sparse transfomers , adaptive span ) , these methods directly ignore long-distance dependence , and they mainly work for language models but have not been proved effective on standard transformers . Therefore , we did not compare them in the experiment , but we take the variants of sparsemax into consideration because they have demonstrate improvement in standard transformer ."}, "2": {"review_id": "Hye87grYDH-2", "review_text": "The paper proposes \"sparse self-attention\", where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer model. In general, the idea is quite simple and easy to implement. It doesn't add any computational or memory cost. The paper is well written and easy to read. The diverse experimental results show that it brings an improvement. And I think this can be combined with other improvements of Transformer. However, there are quite many baselines are missing from the tables. The sota on De-En is actually 35.7 by Fonollosa et.al. On enwik8, Transformer XL is not the best medium sized model as the authors claimed. See below: NTM En-De: - Wu et.al. Pay Less Attention with Lightweight and Dynamic Convolutions, 2019 - Ott et.al. Scaling Neural Machine Translation, 2018 NTM En-Vi: - Wang et.al. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation, 2018 NTM De-En: - Wu et.al. Pay Less Attention with Lightweight and Dynamic Convolutions, 2019 - Fonollosa et.al. Joint Source-Target Self Attention with Locality Constraints, 2019 - He et.al. Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation, 2018 LM Enwik8: - Sukhbaatar et.al, Adaptive Attention Span in Transformers, 2019 Other comments: - More experimental details are needed. What is the value K? How different K values affect performance? What is the number of parameters of NMT models. - The claim \"top layer of the vanilla Transformer focuses on the end position of the text\" can't be true generally. Probably only true for a certain task. - Where the numbers in Figure 1 come from? Is it a single attention head or average of all? - Page 4, \"the high are ...\" probably typo? - The related work is missing \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" by Rae et.al., which also uses sparse attention.", "rating": "6: Weak Accept", "reply_text": "Thanks for your careful reviews , analysis of the value of k , comparisons between previous sparse attention methods , and missing reference are all included in the newer version of the paper . The answers to the remaining questions are as follows : Q : More experimental details are needed . What is the number of parameters of NMT models . A : We have added more experimental details to the Appendix . Since our model does not increase the number of parameters , the parameter quantities of the machine translation model are the same as the Transformer base and the Transformer large respectively on the IWSLT and WMT datasets . Q : The claim `` top layer of the vanilla Transformer focuses on the end position of the text '' ca n't be true generally . Probably only true for a certain task . A : we observe similar phenomena in other tasks , such as IWSLT German to English translation . Q : Where the numbers in Figure 1 come from ? Is it a single attention head or average of all ? A : For the sake of simplicity , we show the attention score of first head ."}}