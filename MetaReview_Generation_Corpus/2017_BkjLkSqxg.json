{"year": "2017", "forum": "BkjLkSqxg", "title": "LipNet: End-to-End Sentence-level Lipreading", "decision": "Reject", "meta_review": "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper.", "reviews": [{"review_id": "BkjLkSqxg-0", "review_text": "UPDATE: I have read the authors' responses. I did not read the social media comments about this paper prior to reviewing it. I appreciate the authors' updates in response to the reviewer comments. Overall, however, my review stands. The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach. I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well. I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not). This is a general point and the program chairs may disagree with it, of course. I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus. ************************ ORIGINAL REVIEW: The authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus. This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort). However, this in itself is not sufficiently novel for publication at ICLR. The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language. Some specifics on what I think needs to be revised: - First, the claim of being the first to do sentence-level lipreading. As mentioned in a pre-review comment, this is not true. The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public). Ideally the title should also be changed in light of this. - The comparison with human lipreaders needs to be qualified a bit. This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints. This is great, but not a general statement about LipNet vs. humans. - The paper contains some unnecessary motivational platitudes. We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well. The McGurk effect does not show that lipreading plays a crucial role in human communication. - It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well. So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion. Some more minor comments, typos, etc.: - citations for LSTMs, CTC, etc. should be provided the first time they are mentioned. - I did not quite follow the justification for upsampling. - what is meant by \"lip-rounding vowels\"? They seem to include almost all English vowels. - Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one? Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it. - \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American). - The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c). For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/. - \"lipreading actuations\": I am not sure what \"actuations\" means in this context - \"palato-alvealoar\" --> \"palato-alveolar\" - \"Articulatorily alveolar\" --> \"Alveolar\"?", "rating": "4: Ok but not good enough - rejection", "reply_text": "1 ) We have innovated an end-to-end approach to sentence-level lipreading that outperforms the state-of-the-art on GRID . Despite the vast literature on the topic , this had not been done properly before . We respectfully , but emphatically , disagree with the subjective opinion that this work only qualifies as a workshop paper . Our line of reasoning is based on the following two facts . First , the many other papers on lip-reading , which presented hand-engineered or insufficiently general models/architectures and far worse results in comparison to ours , were worthy of publication on venues other than workshops . Ours being a simpler , more powerful , and better-performance model should therefore be held to the same standard . Second , much of the progress in speech recognition and computer vision mirrors what we have done here : the introduction of a simple , accessible , scalable , accurate , end-to-end trainable pipeline that yields better results on existing public datasets . 2 ) LipNet is the first end-to-end model that performs sentence-level sequence prediction . Previous work in the field requires either heavy preprocessing of frames to extract image features , temporal preprocessing of frames to extract video features ( e.g. , optical flow or movement detection ) , or other types of handcrafted vision pipelines . However , we agree that to reflect this we should change the title to : LipNet : End-to-End Sentence-Level Lipreading . Please see our revised paper and reply to comments concerning Neti et al 2000 and claims of prior work on sentence level prediction . 3 ) The grammar of GRID and the setup are indeed unnatural . However , the people doing the test had the grammar in front of them . Our results with people are specific to the GRID dataset and should be treated as a baseline , and no more than this . We chose to create as many baselines as possible because of the scarcity of prior work at the sentence level on GRID , and our desire to be thorough . Until we build large datasets for lipreading and real-time apps , it is not wise to make wild claims of superhuman performance . The media recently made such claims about our work , and we spent a lot of time and energy correcting and appropriately qualifying such claims . We certainly make no such claims . 4 ) The McGurk effect is frequently used to motivate audio-visual interfaces . Some citations are perhaps unnecessary , but since these are common citations in this research area , we chose to include them . 5 ) Spatiotemporal convolutions result in 2.3x lower WER , compared to Baseline-2D . This is very significant and not to be dismissed . Again , we chose to be thorough and in the new version add further baselines . 6 ) We have addressed these concerns in the revised version of our paper . 7 ) We follow the phoneme-to-viseme categorisation provided by Neti et al.2000.V1-V4 provided no interesting additional information in terms of plotting . 8 ) We agree with the reviewer that this exists in other dialects ; our analysis is based on this dataset . 9 ) This is most likely a consequence of the grammar of the dataset . /p/ and /b/ appear in several words , while /m/ appears as a letter . 10 ) Thank you for the suggestions ."}, {"review_id": "BkjLkSqxg-1", "review_text": "The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well. Overall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . Our novelty lies both in the domain as well as the architecture . In contrast to the previous state-of-the-art , which reported a \u201c 14 % performance drop \u201d using spatiotemporal convolutions instead of spatial convolution ( Chung et al.2016 ) , we present a spatiotemporal convolution architecture , coupled with a recurrent neural network , that substantially improves performance , resulting in ~2.3x lower WER . Furthermore , for a fair comparison in sentence-level predictions , our Baseline-LSTM model replicates the architecture of Wand et al.2016 , which is the previous deep learning state-of-the-art in the GRID corpus . LipNet achieves ~5x lower WER than Baseline-LSTM . To illustrate the effect of the language model , in our latest revision , we have introduced a new baseline ( Baseline-NoLM ) , which exhibits only a minor performance drop . A word-only model was never attempted , as it was not the goal of the present work . We think that sentence-level predictions are far more challenging , interesting , and useful . GRID is the largest publicly available audio-visual sentence-level dataset . Our results are a major step in end-to-end sentence-level training . Evaluating LipNet and building a larger dataset is one of our current efforts ."}, {"review_id": "BkjLkSqxg-2", "review_text": "- Proven again that end to end training with deep networks gives large gains over traditional hybrid systems with hand crafted features. The results are very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here. - Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks. - Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Please see our revised document and the responses to the questions below . We believe most of your concerns have been addressed in the latest version of our document . Thanks to the reviewing process , we have learned that the claim of sentence-level lipreading was too broad . We have therefore fixed this . In our defense , the vast literature on lipreading from video only focuses on word-level predictions . Moreover , in works doing both audio and video , video only results take a backstage role and performance tends to be poor . Given this , it is not surprising that we and many other researchers ( we are not alone in this ) had arrived at this conclusion . We are now making what we think is an accurate but still very important claim : end-to-end sentence level lipreading . We have an extensive list of citations in our paper . We have not found any reference in the literature that does end-to-end sentence-level lipreading , despite a thorough search and given the wide discussion we have had on this paper . Certainly , we have not found any works that match our performance on the popular GRID dataset . Several works have appeared after this work using similar pipelines of RNNs and CTC , but those works have failed to achieve the same performance when tested on GRID . Here , and as in most of deep learning , good engineering matters a lot as you kindly acknowledge . We only know of one work can can match the performance of our method ( https : //arxiv.org/abs/1611.05358 ) , but that work appeared two weeks after we made our paper available . They use a dataset that is not publically available for pretraining , and hence we have no means of comparing to them directly yet ."}], "0": {"review_id": "BkjLkSqxg-0", "review_text": "UPDATE: I have read the authors' responses. I did not read the social media comments about this paper prior to reviewing it. I appreciate the authors' updates in response to the reviewer comments. Overall, however, my review stands. The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach. I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well. I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not). This is a general point and the program chairs may disagree with it, of course. I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus. ************************ ORIGINAL REVIEW: The authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus. This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort). However, this in itself is not sufficiently novel for publication at ICLR. The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language. Some specifics on what I think needs to be revised: - First, the claim of being the first to do sentence-level lipreading. As mentioned in a pre-review comment, this is not true. The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public). Ideally the title should also be changed in light of this. - The comparison with human lipreaders needs to be qualified a bit. This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints. This is great, but not a general statement about LipNet vs. humans. - The paper contains some unnecessary motivational platitudes. We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well. The McGurk effect does not show that lipreading plays a crucial role in human communication. - It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well. So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion. Some more minor comments, typos, etc.: - citations for LSTMs, CTC, etc. should be provided the first time they are mentioned. - I did not quite follow the justification for upsampling. - what is meant by \"lip-rounding vowels\"? They seem to include almost all English vowels. - Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one? Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it. - \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American). - The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c). For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/. - \"lipreading actuations\": I am not sure what \"actuations\" means in this context - \"palato-alvealoar\" --> \"palato-alveolar\" - \"Articulatorily alveolar\" --> \"Alveolar\"?", "rating": "4: Ok but not good enough - rejection", "reply_text": "1 ) We have innovated an end-to-end approach to sentence-level lipreading that outperforms the state-of-the-art on GRID . Despite the vast literature on the topic , this had not been done properly before . We respectfully , but emphatically , disagree with the subjective opinion that this work only qualifies as a workshop paper . Our line of reasoning is based on the following two facts . First , the many other papers on lip-reading , which presented hand-engineered or insufficiently general models/architectures and far worse results in comparison to ours , were worthy of publication on venues other than workshops . Ours being a simpler , more powerful , and better-performance model should therefore be held to the same standard . Second , much of the progress in speech recognition and computer vision mirrors what we have done here : the introduction of a simple , accessible , scalable , accurate , end-to-end trainable pipeline that yields better results on existing public datasets . 2 ) LipNet is the first end-to-end model that performs sentence-level sequence prediction . Previous work in the field requires either heavy preprocessing of frames to extract image features , temporal preprocessing of frames to extract video features ( e.g. , optical flow or movement detection ) , or other types of handcrafted vision pipelines . However , we agree that to reflect this we should change the title to : LipNet : End-to-End Sentence-Level Lipreading . Please see our revised paper and reply to comments concerning Neti et al 2000 and claims of prior work on sentence level prediction . 3 ) The grammar of GRID and the setup are indeed unnatural . However , the people doing the test had the grammar in front of them . Our results with people are specific to the GRID dataset and should be treated as a baseline , and no more than this . We chose to create as many baselines as possible because of the scarcity of prior work at the sentence level on GRID , and our desire to be thorough . Until we build large datasets for lipreading and real-time apps , it is not wise to make wild claims of superhuman performance . The media recently made such claims about our work , and we spent a lot of time and energy correcting and appropriately qualifying such claims . We certainly make no such claims . 4 ) The McGurk effect is frequently used to motivate audio-visual interfaces . Some citations are perhaps unnecessary , but since these are common citations in this research area , we chose to include them . 5 ) Spatiotemporal convolutions result in 2.3x lower WER , compared to Baseline-2D . This is very significant and not to be dismissed . Again , we chose to be thorough and in the new version add further baselines . 6 ) We have addressed these concerns in the revised version of our paper . 7 ) We follow the phoneme-to-viseme categorisation provided by Neti et al.2000.V1-V4 provided no interesting additional information in terms of plotting . 8 ) We agree with the reviewer that this exists in other dialects ; our analysis is based on this dataset . 9 ) This is most likely a consequence of the grammar of the dataset . /p/ and /b/ appear in several words , while /m/ appears as a letter . 10 ) Thank you for the suggestions ."}, "1": {"review_id": "BkjLkSqxg-1", "review_text": "The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well. Overall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . Our novelty lies both in the domain as well as the architecture . In contrast to the previous state-of-the-art , which reported a \u201c 14 % performance drop \u201d using spatiotemporal convolutions instead of spatial convolution ( Chung et al.2016 ) , we present a spatiotemporal convolution architecture , coupled with a recurrent neural network , that substantially improves performance , resulting in ~2.3x lower WER . Furthermore , for a fair comparison in sentence-level predictions , our Baseline-LSTM model replicates the architecture of Wand et al.2016 , which is the previous deep learning state-of-the-art in the GRID corpus . LipNet achieves ~5x lower WER than Baseline-LSTM . To illustrate the effect of the language model , in our latest revision , we have introduced a new baseline ( Baseline-NoLM ) , which exhibits only a minor performance drop . A word-only model was never attempted , as it was not the goal of the present work . We think that sentence-level predictions are far more challenging , interesting , and useful . GRID is the largest publicly available audio-visual sentence-level dataset . Our results are a major step in end-to-end sentence-level training . Evaluating LipNet and building a larger dataset is one of our current efforts ."}, "2": {"review_id": "BkjLkSqxg-2", "review_text": "- Proven again that end to end training with deep networks gives large gains over traditional hybrid systems with hand crafted features. The results are very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here. - Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks. - Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Please see our revised document and the responses to the questions below . We believe most of your concerns have been addressed in the latest version of our document . Thanks to the reviewing process , we have learned that the claim of sentence-level lipreading was too broad . We have therefore fixed this . In our defense , the vast literature on lipreading from video only focuses on word-level predictions . Moreover , in works doing both audio and video , video only results take a backstage role and performance tends to be poor . Given this , it is not surprising that we and many other researchers ( we are not alone in this ) had arrived at this conclusion . We are now making what we think is an accurate but still very important claim : end-to-end sentence level lipreading . We have an extensive list of citations in our paper . We have not found any reference in the literature that does end-to-end sentence-level lipreading , despite a thorough search and given the wide discussion we have had on this paper . Certainly , we have not found any works that match our performance on the popular GRID dataset . Several works have appeared after this work using similar pipelines of RNNs and CTC , but those works have failed to achieve the same performance when tested on GRID . Here , and as in most of deep learning , good engineering matters a lot as you kindly acknowledge . We only know of one work can can match the performance of our method ( https : //arxiv.org/abs/1611.05358 ) , but that work appeared two weeks after we made our paper available . They use a dataset that is not publically available for pretraining , and hence we have no means of comparing to them directly yet ."}}