{"year": "2017", "forum": "By5e2L9gl", "title": "Trusting SVM for Piecewise Linear CNNs", "decision": "Accept (Poster)", "meta_review": "The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. \n \n Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.\n \n Thus, I recommend this paper be accepted.", "reviews": [{"review_id": "By5e2L9gl-0", "review_text": "This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities. The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. Pros: - To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis. - The paper is well-written and easy to follow. Cons: - Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. - The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, Other comments: - If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his comments , which we discuss below : Comment : \u201c Although the proposed approach can be applied in general structured prediction problem , the experiments only conduct on a simple multi-class classification task. \u201d Response : In principle the architectures used on ImageNet ( such as VGG-16 , ResNets etc . ) can be treated like the models used in our experiments , and we expect to obtain similar results on these . In addition to the current results , we are currently evaluating our method on CIFAR-100 and ImageNet , which are more challenging multi-class classification tasks . We do take note that it would be interesting to test the method on structured prediction tasks as well . Comment : \u201c The test accuracy performance on CIFAR-10 reported in the paper does n't look right . The accuracy of the best model reported in this paper is 70.2 % while existing work often reports 90+ % . \u201d Response : The best score of 70 % is obtained with a network which does not use batch normalization , which makes an important difference . The results with batch-normalization in the revised version of the paper reach around 78 % testing accuracy . Taking into account the reviewer \u2019 s comments , we are currently running experiments with deeper architectures to improve this score . Comment : \u201c If I understand correctly , BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective . In practice , the inner optimization process often stops pretty early ( i.e. , stops when duality gap is still large ) . Therefore , when putting them together , the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately . The authors should add this note when they discuss the properties of their algorithm. \u201d Response : The reviewer is correct that during the inner optimization , only the dual objective is guaranteed a monotonic improvement . In practice , we do run BCFW with a sufficient number of iterations to yield a very small duality gap . Therefore the method provides a monotonic decrease in practice . To illustrate this , we have plotted the training objective and the dual gap for an experiment presented in the paper ( Adadelta with batch-norm + LW-SVM , Figure 3 in the revised paper ) : https : //drive.google.com/file/d/0BxXMf_vDT8vCSFQwOWl0aldzeWs/view ? usp=sharing ( one data point at the end of each inner iteration of the CCCP ) . We will add a note about these points in the future revision of the paper ."}, {"review_id": "By5e2L9gl-1", "review_text": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed. Summary: \u2014\u2014\u2014 I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story. Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is reasonable albeit heuristics are required. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time. Details: \u2014\u2014\u2014\u2014 1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation. 2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016; which should probably be mentioned. 3. I think the comparison between backprop and the discussed CCCP approach is not really appropriate. Note that backprop is a mechanism to compute gradients and is not at all related to any optimization technique/algorithm. This means that backprop combined with Armijo line-search would for example result in convergence guarantees. Hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next\u2019 is according to my opinion superficial. 4. More justification regarding the argument that search for the step-size is a disadvantage seems necessary. There is evidence that noise introduced by mini-batches and inaccurate/no line search is actually beneficial. In contrast the proposed CCCP procedure may converge pre-maturely to a local optimum close to the initialization. Since mini-batches are used no guarantees are available in any case. Hence I think additional evidence for the fact that such convergence guarantees don\u2019t result in premature stopping seems necessary. 5. The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. Note that you can easily construct examples where Eq. (9) and Eq. (7) differ significantly. 6. I personally think the experimental evaluation is conducted on examples that are too small and some results are obvious. E.g., LW-SVM always improves over the solution of the SGD algorithm. If the authors were to use backtracking line search they could also improve upon LW-SVM.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the comments and provide some clarifications below . Comment : \u201c comparison between backprop and the discussed CCCP approach is not really appropriate \u201d Response : We use the term \u2018 backpropagation \u2019 to denote the overall subgradient descent ( SGD ) algorithm . This includes the forward pass and the backward pass ( which computes the subgradient ) , as well as the state of the art variants of the step size computations , namely , Adagrad , Adadelta and Adam . As our main aim is to improve the optimization of the learning objective ( 3 ) , a comparison with the existing optimization algorithms provides an appropriate baseline . Comment : \u201c mini-batch optimization alleviates any form of monotonic decrease \u201d . Response : Does the reviewer mean that by using mini-batch optimization we have lost our monotonic decrease guarantee ? If so , this is not correct . There are two distinct monotonic improvements that we guarantee here , even when using mini-batches : ( i ) a monotonic decrease in the overall objective function at each outer iteration of the CCCP algorithm . This is because we solve a convex problem and therefore obtain the optimal solution , which is at least as good as the initial point . ( ii ) a monotonic increase in the dual of the convex problem at each step . Because the dual is smooth and quadratic , we perform a coordinate ascent on the dual variables with an optimal step-size . This feature of the BCFW algorithm ( Lacoste-Julien et al. , 2013 ) guarantees a monotonic increase despite the fact that the conditional gradient is computed using samples from a mini-batch / block of coordinates . Comment : \u201c More justification regarding the argument that search for the step-size is a disadvantage seems necessary \u201d , \u201c line-search would for example result in convergence guarantees \u201d . Response : Another noteworthy advantage of our work is that we can analytically compute the optimal step-size for the conditional gradient obtained over a mini-batch . Indeed , this fact has been exploited successfully for structural SVM optimization with significant improvements over SGD methods . For backpropagation style algorithms ( that is , SGD on learning objective ( 3 ) ) , the computation of the step-size is not computationally feasible . This is due to the fact that the evaluation of the objective for each putative step-size would require an entire epoch . Furthermore , for PL-CNN , the objective ( 3 ) is only sub-differentiable . Hence , even an exhaustive line search can not guarantee a monotonic improvement . In other words , by establishing a connection between deep networks and structural SVMs , we have been able to successfully transfer the two advantages of BCFW to the deep learning domain ( monotonic improvement of dual using mini-batches , efficient computation of the optimal step-size ) . Comment : \u201c In spirit similar is work by B. Amos and J. Kolter , Input-Convex Deep Networks ( https : //openreview.net/pdf ? id=ROVmA279BsvnM0J1IpNn ) ICLR Workshop Track 2016 \u201d . Response : We thank the reviewer for the reference ( we have found a full version at https : //arxiv.org/pdf/1609.07152v2.pdf ) . While our method deals with convexity w.r.t.the parameters to improve the training task , the aforementioned work analyzes how convexity w.r.t input or the output of the network helps with the inference ( for cases with complex outputs ) . We will include this reference in the future revision of the paper . Comment : \u201c The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. \u201d Response : In our revised version of the paper , we have updated Observation 1 with a principled algorithm that is guaranteed to find the optimal solution of the convex problem . In summary , all the theoretical guarantees presented in our work are applicable , despite the use of mini-batches and the use of a modified conditional gradient algorithm ."}, {"review_id": "By5e2L9gl-2", "review_text": "This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs. Overall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps. Of course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability). The experiment is a bit weak. 1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet. 2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments , and provide some clarifications below . Comment : \u201c layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs [ ... ] It is not clear to me how the loss/gain balances each other \u201d Response : The performance impact of using coordinate descent is indeed an open question . For this work , we point out that : ( i ) LW-SVM improves over SGD solutions despite optimizing only one layer at a time , ( ii ) LW-SVM trained from scratch can also reach competitive results on CIFAR-10 , ( iii ) The connection between PL-CNNs and latent SVMs opens an interesting research direction that may further improve on LW-SVM ( for example , by extending LW-SVM to a global optimization algorithm that updates all layers simultaneously ) . Comment : \u201c This paper only compares with a crippled variant of SGD ( without batch normalization , dropout , etc ) . \u201d Response : In the revised version of the paper , we report results with batch-normalization and show similar or greater improvements of LW-SVM over the baselines . Comment : \u201c a better objective does not necessarily correspond to a good model ( e.g.due to overfitting ) . Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective ( unlike the proposed method in this paper ) , but to me , this might be a good thing since it can prevent overfitting \u201d Response : The experiments of this work suggest that the improvement made by LW-SVM on the training data generalize well in our settings . In particular , results with batch-normalization show significant improvements on the testing accuracy . A better optimization algorithm may also lead to a better understanding of the deficiencies of the current learning objective , which could in itself be of interest to the representation learning community . Comment : \u201c Only CIFAR10 is used . This is a very small dataset by today 's standard , while CNNs are typically used in large-scale datasets , such as ImageNet. \u201d Response : In principle the architectures used on ImageNet ( such as VGG-16 , ResNets etc . ) can be treated like the models used in our experiments , and we expect to obtain similar results on these . In addition to the current results , we are currently evaluating our method on CIFAR-100 and ImageNet ."}], "0": {"review_id": "By5e2L9gl-0", "review_text": "This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities. The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. Pros: - To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis. - The paper is well-written and easy to follow. Cons: - Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. - The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, Other comments: - If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his comments , which we discuss below : Comment : \u201c Although the proposed approach can be applied in general structured prediction problem , the experiments only conduct on a simple multi-class classification task. \u201d Response : In principle the architectures used on ImageNet ( such as VGG-16 , ResNets etc . ) can be treated like the models used in our experiments , and we expect to obtain similar results on these . In addition to the current results , we are currently evaluating our method on CIFAR-100 and ImageNet , which are more challenging multi-class classification tasks . We do take note that it would be interesting to test the method on structured prediction tasks as well . Comment : \u201c The test accuracy performance on CIFAR-10 reported in the paper does n't look right . The accuracy of the best model reported in this paper is 70.2 % while existing work often reports 90+ % . \u201d Response : The best score of 70 % is obtained with a network which does not use batch normalization , which makes an important difference . The results with batch-normalization in the revised version of the paper reach around 78 % testing accuracy . Taking into account the reviewer \u2019 s comments , we are currently running experiments with deeper architectures to improve this score . Comment : \u201c If I understand correctly , BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective . In practice , the inner optimization process often stops pretty early ( i.e. , stops when duality gap is still large ) . Therefore , when putting them together , the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately . The authors should add this note when they discuss the properties of their algorithm. \u201d Response : The reviewer is correct that during the inner optimization , only the dual objective is guaranteed a monotonic improvement . In practice , we do run BCFW with a sufficient number of iterations to yield a very small duality gap . Therefore the method provides a monotonic decrease in practice . To illustrate this , we have plotted the training objective and the dual gap for an experiment presented in the paper ( Adadelta with batch-norm + LW-SVM , Figure 3 in the revised paper ) : https : //drive.google.com/file/d/0BxXMf_vDT8vCSFQwOWl0aldzeWs/view ? usp=sharing ( one data point at the end of each inner iteration of the CCCP ) . We will add a note about these points in the future revision of the paper ."}, "1": {"review_id": "By5e2L9gl-1", "review_text": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed. Summary: \u2014\u2014\u2014 I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story. Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is reasonable albeit heuristics are required. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time. Details: \u2014\u2014\u2014\u2014 1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation. 2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016; which should probably be mentioned. 3. I think the comparison between backprop and the discussed CCCP approach is not really appropriate. Note that backprop is a mechanism to compute gradients and is not at all related to any optimization technique/algorithm. This means that backprop combined with Armijo line-search would for example result in convergence guarantees. Hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next\u2019 is according to my opinion superficial. 4. More justification regarding the argument that search for the step-size is a disadvantage seems necessary. There is evidence that noise introduced by mini-batches and inaccurate/no line search is actually beneficial. In contrast the proposed CCCP procedure may converge pre-maturely to a local optimum close to the initialization. Since mini-batches are used no guarantees are available in any case. Hence I think additional evidence for the fact that such convergence guarantees don\u2019t result in premature stopping seems necessary. 5. The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. Note that you can easily construct examples where Eq. (9) and Eq. (7) differ significantly. 6. I personally think the experimental evaluation is conducted on examples that are too small and some results are obvious. E.g., LW-SVM always improves over the solution of the SGD algorithm. If the authors were to use backtracking line search they could also improve upon LW-SVM.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the comments and provide some clarifications below . Comment : \u201c comparison between backprop and the discussed CCCP approach is not really appropriate \u201d Response : We use the term \u2018 backpropagation \u2019 to denote the overall subgradient descent ( SGD ) algorithm . This includes the forward pass and the backward pass ( which computes the subgradient ) , as well as the state of the art variants of the step size computations , namely , Adagrad , Adadelta and Adam . As our main aim is to improve the optimization of the learning objective ( 3 ) , a comparison with the existing optimization algorithms provides an appropriate baseline . Comment : \u201c mini-batch optimization alleviates any form of monotonic decrease \u201d . Response : Does the reviewer mean that by using mini-batch optimization we have lost our monotonic decrease guarantee ? If so , this is not correct . There are two distinct monotonic improvements that we guarantee here , even when using mini-batches : ( i ) a monotonic decrease in the overall objective function at each outer iteration of the CCCP algorithm . This is because we solve a convex problem and therefore obtain the optimal solution , which is at least as good as the initial point . ( ii ) a monotonic increase in the dual of the convex problem at each step . Because the dual is smooth and quadratic , we perform a coordinate ascent on the dual variables with an optimal step-size . This feature of the BCFW algorithm ( Lacoste-Julien et al. , 2013 ) guarantees a monotonic increase despite the fact that the conditional gradient is computed using samples from a mini-batch / block of coordinates . Comment : \u201c More justification regarding the argument that search for the step-size is a disadvantage seems necessary \u201d , \u201c line-search would for example result in convergence guarantees \u201d . Response : Another noteworthy advantage of our work is that we can analytically compute the optimal step-size for the conditional gradient obtained over a mini-batch . Indeed , this fact has been exploited successfully for structural SVM optimization with significant improvements over SGD methods . For backpropagation style algorithms ( that is , SGD on learning objective ( 3 ) ) , the computation of the step-size is not computationally feasible . This is due to the fact that the evaluation of the objective for each putative step-size would require an entire epoch . Furthermore , for PL-CNN , the objective ( 3 ) is only sub-differentiable . Hence , even an exhaustive line search can not guarantee a monotonic improvement . In other words , by establishing a connection between deep networks and structural SVMs , we have been able to successfully transfer the two advantages of BCFW to the deep learning domain ( monotonic improvement of dual using mini-batches , efficient computation of the optimal step-size ) . Comment : \u201c In spirit similar is work by B. Amos and J. Kolter , Input-Convex Deep Networks ( https : //openreview.net/pdf ? id=ROVmA279BsvnM0J1IpNn ) ICLR Workshop Track 2016 \u201d . Response : We thank the reviewer for the reference ( we have found a full version at https : //arxiv.org/pdf/1609.07152v2.pdf ) . While our method deals with convexity w.r.t.the parameters to improve the training task , the aforementioned work analyzes how convexity w.r.t input or the output of the network helps with the inference ( for cases with complex outputs ) . We will include this reference in the future revision of the paper . Comment : \u201c The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. \u201d Response : In our revised version of the paper , we have updated Observation 1 with a principled algorithm that is guaranteed to find the optimal solution of the convex problem . In summary , all the theoretical guarantees presented in our work are applicable , despite the use of mini-batches and the use of a modified conditional gradient algorithm ."}, "2": {"review_id": "By5e2L9gl-2", "review_text": "This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs. Overall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps. Of course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability). The experiment is a bit weak. 1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet. 2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments , and provide some clarifications below . Comment : \u201c layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs [ ... ] It is not clear to me how the loss/gain balances each other \u201d Response : The performance impact of using coordinate descent is indeed an open question . For this work , we point out that : ( i ) LW-SVM improves over SGD solutions despite optimizing only one layer at a time , ( ii ) LW-SVM trained from scratch can also reach competitive results on CIFAR-10 , ( iii ) The connection between PL-CNNs and latent SVMs opens an interesting research direction that may further improve on LW-SVM ( for example , by extending LW-SVM to a global optimization algorithm that updates all layers simultaneously ) . Comment : \u201c This paper only compares with a crippled variant of SGD ( without batch normalization , dropout , etc ) . \u201d Response : In the revised version of the paper , we report results with batch-normalization and show similar or greater improvements of LW-SVM over the baselines . Comment : \u201c a better objective does not necessarily correspond to a good model ( e.g.due to overfitting ) . Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective ( unlike the proposed method in this paper ) , but to me , this might be a good thing since it can prevent overfitting \u201d Response : The experiments of this work suggest that the improvement made by LW-SVM on the training data generalize well in our settings . In particular , results with batch-normalization show significant improvements on the testing accuracy . A better optimization algorithm may also lead to a better understanding of the deficiencies of the current learning objective , which could in itself be of interest to the representation learning community . Comment : \u201c Only CIFAR10 is used . This is a very small dataset by today 's standard , while CNNs are typically used in large-scale datasets , such as ImageNet. \u201d Response : In principle the architectures used on ImageNet ( such as VGG-16 , ResNets etc . ) can be treated like the models used in our experiments , and we expect to obtain similar results on these . In addition to the current results , we are currently evaluating our method on CIFAR-100 and ImageNet ."}}