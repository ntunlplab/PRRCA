{"year": "2017", "forum": "S1jmAotxg", "title": "Stick-Breaking Variational Autoencoders", "decision": "Accept (Poster)", "meta_review": "This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.", "reviews": [{"review_id": "S1jmAotxg-0", "review_text": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion. After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model. With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable. Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway. The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments , Reviewer # 2 . Our responses are below . 1 . `` After reading the paper in more detail , I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct : all latent variables are `` used '' ( which actually enable backpropagation ) but the latent variables are parametrized differently ( into $ \\pi $ ) and the decoding process is altered as to give the impression of sparsity . The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model . '' We are not sure what you mean by \u201c the latent variables are parametrized differently ( into $ \\pi $ ) and the decoding process is altered as to give the impression of sparsity \u201d . Are you referring to how the stick segments are recursively constructed ? While you are correct that the stick segments lie on the simplex and therefore look as if they could be produced by a soft-gating function , this view neglects the underlying probabilistic model and the need for sampling from the simplex ( for ELBO optimization ) . Generating differentiable samples from the simplex can not be done via existing methods and was an open problem -- -for example , see the discussion of the NVI Topic Model in my response to Review 1 . And yes , all the latent variables are used during every forward propagation , but this is commonly the case in Bayesian nonparametric models with posterior truncation . For instance , Blei and Jordan , in their seminal work on VI for Infinite Mixtures ( http : //www.cs.columbia.edu/~blei/papers/BleiJordan2004.pdf ) , use a truncated posterior , just as we do , and therefore the number of components is fixed and all are \u2018 used \u2019 in the sense that they have non-zero weight . Their generative process is still a formal Bayes nonparametric model , just as ours is . As we discuss in Section 4 , our model does work with an adaptive number of latent variables ; it \u2019 s just that optimization is slower . 2 . `` The semi-supervised results are better than a weaker version of the model used in ( Kingma et al. , 2014 ) , but as to have a fairer comparison , the results should be compared with the M1+M2 model in that paper , even if that requires also using two VAEs . '' We agree that the paper can only be improved by making the semi-supervised results more comparable to Kingma et al. \u2019 s . We will try to add these before the discussion period ends . Yet , we believe the absence of M1+M2 experiments is not a crucial flaw or deficiency . Kingma et al \u2019 s results are not SOTA any longer , and the point of our paper is not semi-supervised learning ( as Kingma et al. \u2019 s is ) . Rather , our aim is to compare SB vs Gauss latent variables in a semi-supervised setting , and we believe our experiment accurately does this ( while still having results partially comparable to Kingma et al \u2019 s , as explained in a pre-review response ) . Thanks again , Eric"}, {"review_id": "S1jmAotxg-1", "review_text": "This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper. There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \"black-box\" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML. Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts. I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention. The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer. Overall, I found this to be an interesting paper, it would be a good fit for ICLR. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your thoughts , Reviewer # 1 . Our responses are below , paired with the corresponding segment of your review . 1 . `` I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner . The pre-review questions clarified most parts . '' Thanks , this is a good point . We \u2019 ll make different subsections for the generative and inference processes in Section 4 and include and/or elaborate on our answers to your pre-review questions . 2 . `` Firstly , conditioning the model directly on the stick-breaking weights seems a little odd\u2026.To be fair , the authors discuss about this issue ( which became clearer to me after the pre-review questions ) , and explain that they 're investigating the apparently challenging problem of using a base distribution G_0 . The question is whether their relaxation is still useful . From the experiments it seems that the method is at least competitive , so the answer is yes . Hopefully an extension will come in the future , as the authors mention\u2026 . It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE . I understand that the non-parametric property induced by the prior might result in better capacity control , however I feel that this advantage ( and potentially others which are still unclear to me ) is not sufficiently explained and demonstrated . '' Yes , we agree that choosing the simplex for the latent space is one of multiple options . Other formulations -- -such as using the remaining length of the stick -- -are possible . Ultimately , we made the choice to use the simplex for the following reasons : ( 1 ) if the decoder is sufficiently powerful , the simplex should have as much representational capacity as Euclidean space does for the Gaussian VAE , ( 2 ) using the weights keeps the model closest to its natural extension to a mixture VAE , and lastly , ( 3 ) we found the formulation had some interesting properties , such as being better at classification , that seemed worthy of reporting . We believe the experiments generating MNIST samples from increasingly higher dimensional latent spaces and examining the number of latent variables used for MNIST+rot provided insight into the model \u2019 s nonparametric qualities to some degree . As for the paper \u2019 s high level motivation : our original goal was to connect SGVB and Bayesian Nonparametrics . Finding a differentiable non-centered parametrization ( DNCP ) for the Stick-Breaking process was the central challenge to this , and once we found the Kumaraswamy , we formulated the SB-VAE to test if optimization of this somewhat unusual computation graph ( i.e.the recurrent dependence across the hidden layer ) and this little known distribution was possible . It turned out that it was , and in addition , we saw some interesting properties that seemed useful for people to know . Yet , we agree that , while the SB-VAE is a VAE alternative one should try if unhappy with the Gauss-VAE \u2019 s performance , the use of the Kumaraswamy as a DNCP for Beta-like or Dirichlet-like random variables is the paper \u2019 s most widely applicable lesson . In fact , in light of recently proposed VAE extensions , the usefulness of the Kumaraswamy is more apparent now than when we began this work . For instance , current ICLR submissions such as \u201c Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders \u201d and \u201c Neural Variational Inference for Topic Models \u201d both workaround the difficulty of priors on the simplex by either , in the case of the former , just not placing priors on the mixture weights , and in the case of the latter , using a Laplace approximation . The latter work even acknowledges the problem that our paper solves : \u201c But it is difficult to handle the Dirichlet within NVI because it is difficult to develop an effective reparameterization function for the [ Reparametrization Trick ] . \u201d Moreover , the recent work from the Blei lab still does not describe a true DNCP for the Beta , their solutions being either weakly dependent on the approx . posterior parameters ( \u201c The Generalized Reparametrization Gradient , NIPS 2016 ) or requiring rejection sampling ( \u201c Rejection Sampling Variational Inference \u201d , ArXiv ) . 3 . `` Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer . '' This is a good idea to improve motivation ; we can elaborate on the connection to Nested Dropout , perhaps moving it out of the \u2018 Related Work \u2019 section . However , for experimental comparisons , we found optimization under Nested Dropout very difficult , even with unit sweeping . The models were not competitive and thus left out of experiments . Comparing against the Infinite RBM is another option , but to make comparison fair , we \u2019 d have to tie the encoder and decoder weights ( since RBMs have only one weight matrix ) , and it is not totally clear how to do this in a VAE since the networks are asymmetric . Thanks again , Eric"}, {"review_id": "S1jmAotxg-2", "review_text": "Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written. In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better. Comments: - sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc. - sec 2.2: two comma's - text flow eq 6: please refer to appendix with the closed-form KL divergence - \"The v's are sampled via\" => \"In the posterior, the v's are sampled via\". It's not clear you're talking about the posterior here, instead of the prior. - The last paragraph of section 4 is great. - Sec 7.1: \"Density estimation\" => Technically you're also doing mass estimation. - Sec 7.1: 100 IS samples is a bit on the low side. - Figure 3(f). Interesting that k-NN works so well on raw pixels. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . Our responses / comments are below . 1 . `` The fact that they do report this 'negative ' result suggests good scientific taste . '' We appreciate you recognizing this . 2 . `` sec 2.1 : There is plenty of previous work with non-Gaussian p ( z ) : DRAW , the generative ResNet paper in the IAF paper , Ladder VAEs , etc . '' Do you mean non-factorized Gaussians ? In the case of DRAW and Ladder VAEs , we still think they are using factorized Gaussians . The DRAW paper says , \u201c In our experiments the latent distribution is a diagonal Gaussian \u201d ( Section 2.1 ) , and the Ladder VAE paper states , \u201c Conditioned on the stochastic layer below each stochastic layer is specified as a fully factorized gaussian distribution \u201d ( Section 2.1 ) . The Ladder VAE paper says this when describing the regular multi-layer VAE but the same notation is used for their model . 3 . `` Figure 3 ( f ) . Interesting that k-NN works so well on raw pixels . '' Yes , this was a bit surprising to us too . This blog post reports similar results : http : //acgrama.blogspot.com/2012/09/knn-with-euclidean-distance-on-mnist.html Thanks again , Eric"}], "0": {"review_id": "S1jmAotxg-0", "review_text": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion. After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model. With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable. Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway. The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments , Reviewer # 2 . Our responses are below . 1 . `` After reading the paper in more detail , I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct : all latent variables are `` used '' ( which actually enable backpropagation ) but the latent variables are parametrized differently ( into $ \\pi $ ) and the decoding process is altered as to give the impression of sparsity . The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model . '' We are not sure what you mean by \u201c the latent variables are parametrized differently ( into $ \\pi $ ) and the decoding process is altered as to give the impression of sparsity \u201d . Are you referring to how the stick segments are recursively constructed ? While you are correct that the stick segments lie on the simplex and therefore look as if they could be produced by a soft-gating function , this view neglects the underlying probabilistic model and the need for sampling from the simplex ( for ELBO optimization ) . Generating differentiable samples from the simplex can not be done via existing methods and was an open problem -- -for example , see the discussion of the NVI Topic Model in my response to Review 1 . And yes , all the latent variables are used during every forward propagation , but this is commonly the case in Bayesian nonparametric models with posterior truncation . For instance , Blei and Jordan , in their seminal work on VI for Infinite Mixtures ( http : //www.cs.columbia.edu/~blei/papers/BleiJordan2004.pdf ) , use a truncated posterior , just as we do , and therefore the number of components is fixed and all are \u2018 used \u2019 in the sense that they have non-zero weight . Their generative process is still a formal Bayes nonparametric model , just as ours is . As we discuss in Section 4 , our model does work with an adaptive number of latent variables ; it \u2019 s just that optimization is slower . 2 . `` The semi-supervised results are better than a weaker version of the model used in ( Kingma et al. , 2014 ) , but as to have a fairer comparison , the results should be compared with the M1+M2 model in that paper , even if that requires also using two VAEs . '' We agree that the paper can only be improved by making the semi-supervised results more comparable to Kingma et al. \u2019 s . We will try to add these before the discussion period ends . Yet , we believe the absence of M1+M2 experiments is not a crucial flaw or deficiency . Kingma et al \u2019 s results are not SOTA any longer , and the point of our paper is not semi-supervised learning ( as Kingma et al. \u2019 s is ) . Rather , our aim is to compare SB vs Gauss latent variables in a semi-supervised setting , and we believe our experiment accurately does this ( while still having results partially comparable to Kingma et al \u2019 s , as explained in a pre-review response ) . Thanks again , Eric"}, "1": {"review_id": "S1jmAotxg-1", "review_text": "This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper. There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \"black-box\" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML. Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts. I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention. The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer. Overall, I found this to be an interesting paper, it would be a good fit for ICLR. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your thoughts , Reviewer # 1 . Our responses are below , paired with the corresponding segment of your review . 1 . `` I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner . The pre-review questions clarified most parts . '' Thanks , this is a good point . We \u2019 ll make different subsections for the generative and inference processes in Section 4 and include and/or elaborate on our answers to your pre-review questions . 2 . `` Firstly , conditioning the model directly on the stick-breaking weights seems a little odd\u2026.To be fair , the authors discuss about this issue ( which became clearer to me after the pre-review questions ) , and explain that they 're investigating the apparently challenging problem of using a base distribution G_0 . The question is whether their relaxation is still useful . From the experiments it seems that the method is at least competitive , so the answer is yes . Hopefully an extension will come in the future , as the authors mention\u2026 . It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE . I understand that the non-parametric property induced by the prior might result in better capacity control , however I feel that this advantage ( and potentially others which are still unclear to me ) is not sufficiently explained and demonstrated . '' Yes , we agree that choosing the simplex for the latent space is one of multiple options . Other formulations -- -such as using the remaining length of the stick -- -are possible . Ultimately , we made the choice to use the simplex for the following reasons : ( 1 ) if the decoder is sufficiently powerful , the simplex should have as much representational capacity as Euclidean space does for the Gaussian VAE , ( 2 ) using the weights keeps the model closest to its natural extension to a mixture VAE , and lastly , ( 3 ) we found the formulation had some interesting properties , such as being better at classification , that seemed worthy of reporting . We believe the experiments generating MNIST samples from increasingly higher dimensional latent spaces and examining the number of latent variables used for MNIST+rot provided insight into the model \u2019 s nonparametric qualities to some degree . As for the paper \u2019 s high level motivation : our original goal was to connect SGVB and Bayesian Nonparametrics . Finding a differentiable non-centered parametrization ( DNCP ) for the Stick-Breaking process was the central challenge to this , and once we found the Kumaraswamy , we formulated the SB-VAE to test if optimization of this somewhat unusual computation graph ( i.e.the recurrent dependence across the hidden layer ) and this little known distribution was possible . It turned out that it was , and in addition , we saw some interesting properties that seemed useful for people to know . Yet , we agree that , while the SB-VAE is a VAE alternative one should try if unhappy with the Gauss-VAE \u2019 s performance , the use of the Kumaraswamy as a DNCP for Beta-like or Dirichlet-like random variables is the paper \u2019 s most widely applicable lesson . In fact , in light of recently proposed VAE extensions , the usefulness of the Kumaraswamy is more apparent now than when we began this work . For instance , current ICLR submissions such as \u201c Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders \u201d and \u201c Neural Variational Inference for Topic Models \u201d both workaround the difficulty of priors on the simplex by either , in the case of the former , just not placing priors on the mixture weights , and in the case of the latter , using a Laplace approximation . The latter work even acknowledges the problem that our paper solves : \u201c But it is difficult to handle the Dirichlet within NVI because it is difficult to develop an effective reparameterization function for the [ Reparametrization Trick ] . \u201d Moreover , the recent work from the Blei lab still does not describe a true DNCP for the Beta , their solutions being either weakly dependent on the approx . posterior parameters ( \u201c The Generalized Reparametrization Gradient , NIPS 2016 ) or requiring rejection sampling ( \u201c Rejection Sampling Variational Inference \u201d , ArXiv ) . 3 . `` Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer . '' This is a good idea to improve motivation ; we can elaborate on the connection to Nested Dropout , perhaps moving it out of the \u2018 Related Work \u2019 section . However , for experimental comparisons , we found optimization under Nested Dropout very difficult , even with unit sweeping . The models were not competitive and thus left out of experiments . Comparing against the Infinite RBM is another option , but to make comparison fair , we \u2019 d have to tie the encoder and decoder weights ( since RBMs have only one weight matrix ) , and it is not totally clear how to do this in a VAE since the networks are asymmetric . Thanks again , Eric"}, "2": {"review_id": "S1jmAotxg-2", "review_text": "Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written. In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better. Comments: - sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc. - sec 2.2: two comma's - text flow eq 6: please refer to appendix with the closed-form KL divergence - \"The v's are sampled via\" => \"In the posterior, the v's are sampled via\". It's not clear you're talking about the posterior here, instead of the prior. - The last paragraph of section 4 is great. - Sec 7.1: \"Density estimation\" => Technically you're also doing mass estimation. - Sec 7.1: 100 IS samples is a bit on the low side. - Figure 3(f). Interesting that k-NN works so well on raw pixels. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . Our responses / comments are below . 1 . `` The fact that they do report this 'negative ' result suggests good scientific taste . '' We appreciate you recognizing this . 2 . `` sec 2.1 : There is plenty of previous work with non-Gaussian p ( z ) : DRAW , the generative ResNet paper in the IAF paper , Ladder VAEs , etc . '' Do you mean non-factorized Gaussians ? In the case of DRAW and Ladder VAEs , we still think they are using factorized Gaussians . The DRAW paper says , \u201c In our experiments the latent distribution is a diagonal Gaussian \u201d ( Section 2.1 ) , and the Ladder VAE paper states , \u201c Conditioned on the stochastic layer below each stochastic layer is specified as a fully factorized gaussian distribution \u201d ( Section 2.1 ) . The Ladder VAE paper says this when describing the regular multi-layer VAE but the same notation is used for their model . 3 . `` Figure 3 ( f ) . Interesting that k-NN works so well on raw pixels . '' Yes , this was a bit surprising to us too . This blog post reports similar results : http : //acgrama.blogspot.com/2012/09/knn-with-euclidean-distance-on-mnist.html Thanks again , Eric"}}