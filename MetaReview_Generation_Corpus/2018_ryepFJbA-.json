{"year": "2018", "forum": "ryepFJbA-", "title": "On Convergence and Stability of GANs", "decision": "Reject", "meta_review": "Pros:\nThe proposed regularization for GAN training is interesting and simple to implement.\n\nCons:\n- Reviewers agree that the methodology is incremental over the WGAN with gradient penalty and the modification is not well motivated.\n- Experimental results do not clearly demonstrate the benefits of the proposed algorithm and the paper also lacks comparisons with related works.\nGIven the pros/cons, the committee feels the paper is not ready for acceptance in its current state.", "reviews": [{"review_id": "ryepFJbA--0", "review_text": "Summary ======== The authors present a new regularization term, inspired from game theory, which encourages the discriminator's gradient to have a norm equal to one. This leads to reduce the number of local minima, so that the behavior of the optimization scheme gets closer to the optimization of a zero-sum games with convex-concave functions. Clarity ====== Overall, the paper is clear and well-written. However, the authors should motivate better the regularization introduced in section 2.3. Originality ========= The idea is novel and interesting. In addition, it is easy to implement it for any GANs since it requires only an additional regularization term. Moreover, the numerical experiments are in favor of the proposed method. Comments ========= - Why should the norm of the gradient should to be equal to 1 and not another value? Is this possible to improve the performance if we put an additional hyper-parameter instead? - Are the performances greatly impacted by other value of lambda and c (the suggested parameter values are lambda = c = 10)? - As mentioned in the paper, the regularization affects the modeling performance. Maybe the authors should add a comparison between different regularization parameters to illustrate the real impact of lambda and c on the performance. - GANs performance is usually worse on very big dataset such as Imagenet. Does this regularization trick makes their performance better? Post-rebuttal comments --------------------------------- I modified my review score, according to the problems raised by Reviewer 1 and 3. Despite the idea looks pretty simple and present some advantages, the authors should go deeper in the analysis, especially because the idea is not so novel.", "rating": "5: Marginally below acceptance threshold", "reply_text": "- A small correction in your summary . Our penalty scheme helps avoid bad local equilibria and the convex-concave case , while being simple , is quite different from the non-convex case . - We changed section 2.3 to rigorously argue that regret minimization converges to ( potentially bad ) local equilibria , added a new section 2.4 to characterize what these 'mode collapse ' equilibria look like ( D has large gradients around real samples in this case ) and demonstrate that they can be averted using gradient constraints , through new toy experiments . This provides good intuition and a strong motivation for the introduction of DRAGAN scheme . We urge you to take another look at sections 2.3 , 2.4 . - In the updated revision , we correct this arbitrary choice and use ' k ' , which should be something small . Basically , we observe that 'mode collapse ' equilibria exhibit sharp gradients of the discriminator function around real samples . So , we regularize so as to keep these gradients small . We apologize for not making it clear earlier . - You make an excellent point that by tuning ' k'/ ' c'/'lambda ' , it could be possible to get better performance but our aim here was just to demonstrate the effectiveness of our method . My intuition is that optimal configuration will depend on data domain , architecture and hence , its beyond the scope of our paper . But , this is an important topic for possibly a future work . - We only explore the performance of our penalty on MNIST , CIFAR-10 and CelebA , like most papers in this direction . I think the performance on ImageNet depends heavily on the architecture but we did not explore this aspect in our paper . It is an interesting topic to compare various methods on bigger datasets , maybe using ResNets ."}, {"review_id": "ryepFJbA--1", "review_text": "This paper addresses the well-known stability problem encountered when training GANs. As many other papers, they suggest adding a regularization penalty on the discriminator which penalizes the gradient with respect to the data, effectively linearizing the data manifold. Relevance: Although I think some of the empirical results provided in the paper are interesting, I doubt the scientific contribution of this paper is significant. First of all, the penalty the author suggest is the same as the one suggest by Gulrajani for Wasserstein GAN (there the motivation behind this penalty comes from the optimal transport plan). In this paper, the author apply the same penalty to the GAN objective with the alternative update rule which is also a lower-bound for the Wasserstein distance. Justification: The authors justify the choice of their regularization saying it linearizes the objective along the data manifold and claim it reduces the number of non-optimal fixed points. This might be true in the data space but the GAN objective is optimized over the parameter space and it is therefore not clear to me their argument hold w.r.t to the network parameters. Can you please comment on this? Regularizing the generator: Can the authors motivate their choice for regularizing the discriminator only, and not the generator? Following their reasoning of linearizing the objective, the same argument should apply to the generator. Comparison to existing work: This is not the first paper that suggests adding a regularization. Given that the theoretical aspect of the paper are rather weak, I would at least expect a comparison to existing regularization methods, e.g. Stabilizing training of generative adversarial networks through regularization. NIPS, 2017 Choice of hyper-parameters: The authors say that the suggested value for lambda is 10. Can you comment on the choice of this parameter and how it affect the results? Have you tried annealing lambda? This is a common procedure in optimization (see e.g. homotopy or continuation methods). Bogonet score: I very much like the experiment where the authors select 100 different architectures to compare their method against the vanilla GAN approach. I here have 2 questions: - Did you do a deeper examination of your results, e.g. was there some architectures for which none of the method performed well? - Did you try to run this experiment on other datasets? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Clarification regarding the importance of our theory sections : We admit that the clarity in our presentation was lacking ( especially ties to the GAN literature ) and tried to address it in the new revision . We urge you to please take another look . Specifically , our contributions are : ( reflected in updated abstract and introduction ) : - We propose to study GAN training as regret minimization . This is a completely novel contribution . In contrast , the popular view is that there is consistent divergence minimization and this is based on the unrealistic assumption that the discriminator is playing optimally at each step and making these updates in the function space . This is n't tractable ( nor ) close to what happens in practice . This forms the main motivation for our paper . - We present the analysis of artificial convex-concave case based on standard results in game theory literature . More importantly , we make explicit the connection between GAN training process ( alternating gradient updates ) and regret minimization , along the way , in section 2.2 . These are not widely known results in the GAN literature and we provide supporting references in the new revision . * * The useful outcome is that this analysis yields a novel proof for the asymptotic convergence of GAN training in the non-parametric limit and it does not require the discriminator to be optimal at each step * * The current revision reflects this message . - To explain mode collapse , we next analyze the realistic non-convex case in section 2.3 from regret minimization perspective . This is very different from the convex-concave case actually ( we apologize for the confusion ) and we cite the works of Hazan et.al to rigorously argue that convergence to potentially bad local equilibria happens using gradient updates ( under some conditions ) . Please see the updated section 2.3 . This leads us to the main hypothesis of our paper - that mode collapse is just an undesirable local equilibrium and it should be possible to avoid it . We apologize for not being clear earlier . The natural question now is how we can avoid these equilibria ? - A new section 2.4 has been added which explains how we can avoid 'mode collapse ' equilibria ( this was implicit and not clear earlier ) . Based on empirical observations , we basically characterize 'mode collapse ' equilibria with sharp gradients of the discriminator function around some real data points . This is key to fighting mode collapse and avoiding such undesirable equilibria . We provide arguments and supporting experiments for this in a new section 2.4 . This was a key transition that was missing in the earlier version . - From this motivation ( of keeping D 's gradients small in ambient data space ) , we propose DRAGAN penalty scheme . In fact , our theory also explains how other gradient penalties ( WGAN-GP/LS-GAN ) might be mitigating mode collapse . We compare and discuss its advantages over them in section 2.5 . And present the main experiments in section 3 ."}, {"review_id": "ryepFJbA--2", "review_text": "This paper contains a collection of ideas about Generative Adversarial Networks (GAN) but it is very hard for me to get the main point of this paper. I am not saying ideas are not interesting, but I think the author needs to choose the main point of the paper, and should focus on delivering in-depth studies on the main point. 1. On the game theoretic interpretations The paper, Generative Adversarial Nets, NIPS 2014, already presented the game theoretic interpretations to GANs, so it's hard for me to think what's new in the section. Best response dynamics is not used in the conventional GAN training, because it's very hard to find the global optimal of inner minimization and outer maximization. The convergence of online primal-dual gradient descent method in the minimax game is already well-known, but this analysis cannot be applied to the usual GAN setting because the objective is not convex-concave. I found this analysis would be very interesting if the authors can find the toy example when GAN becomes convex-concave by using different model parameterizations and/or different f-divergence, and conduct various studies on the convergence and stability on this problem. I also found that the hypothesis on the model collapsing has very limited connection to the convex-concave case. It is OK to form the hypothesis and present an interesting research direction, but in order to make this as a main point of the paper, the author should provide more rigorous arguments or experimental studies instead of jumping to the hypothesis in two sentences. For example, if the authors can provide the toy example where GAN becomes convex-concave vs. non-convex-concave case, and how the loss function shape or gradient dynamics are changing, that will provide very valuable insights on the problem. 2. DRAGAN As open commenters pointed out, I found it's difficult to find why we want to make the norm of the gradient to 1. Why not 2? why not 1/2? Why 1 is very special? In the WGAN paper, the gradient is clipped to a number less than 1, because it is a sufficient condition to being 1-Lipshitz, but this paper provides no justification on this number. It's OK not to have the theoretical answers to the questions but in that case the authors should provide ablation experiments. For example, sweeping gradient norm target from 10^-3, 10^-2, 10^-1, 1.0, 10.0, etc and their impact on the performance. Also scheduling regularization parameter like reducing the size of lambda exponentially would be interesting as well. Most of those studies won't be necessary if the theory is sound. However, since this paper does not provide a justification on the magic number \"1\", I think it's better to include some form of ablation studies. Note that the item 1 and item 2 are not strongly related to each other, and can be two separate papers. I recommend to choose one direction and provide in-depth study on one topic. Currently, this paper tries to present interesting ideas without very deep investigations, and I cannot recommend this paper to be published. ", "rating": "3: Clear rejection", "reply_text": "We found your review to be extremely helpful to understand where our paper was lacking . Our paper did have multiple new ideas and the presentation was n't always clear ( ties to GAN literature were missing ) . Thanks to your feedback , we chose the most important strand and strengthened it in the current revision using additional theoretical arguments and targeted experiments . The core content is still the same but the presentation has been changed significantly for improved clarity . We urge you to take another look . Specifically , the main points now read as ( reflected in updated abstract and introduction ) : - We propose to study GAN training as regret minimization ( this is a novel view ) , which is in contrast to the popular view that there is consistent divergence minimization . More about this below . - We provide a novel proof for the asymptotic convergence of GAN training in the non-parametric limit and it does not require the discriminator to be optimal at each step . - Regret minimization ( AGD ) in non-convex games converges to potentially bad local equilibria , under some conditions and we hypothesize mode collapse to be resulting from this . Please see updated section 2.3 , where we added theoretical arguments to support this . Next question is how we can avoid these equilibria ? - We characterize mode collapse equilibria with sharp gradients of the discriminator function around some real data points . We provide toy experiments and arguments in a new section 2.4 to support this . We apologize for missing this key transition earlier . - From this motivation , we propose DRAGAN penalty scheme . We compare and discuss advantages over WGAN-GP , LS-GAN in section 2.5 . And present main experiments in section 3 . Theory sections : 1 . We should have called section 2.1 as background since its reviewing original GAN paper 's formulation and we apologize for not making it clear . 2.However , studying GAN training dynamics as regret minimization is a completely novel contribution . And the popular divergence minimization hypothesis stems from D using best-response algorithm in the function space . This is n't tractable as you mention , nor close to what happens in practice . In fact , this is the main motivation for our paper . 3.We agree with you that the content in section 2.3 is well-known in game theory literature and moreover , the convex-concave is artificial . Our goal here was to simply review these results , introduce formal notions along the way , which we use in later sections and most importantly , make explicit the connection between GAN training ( alternating gradient updates ) and regret minimization . None of this is widely known in GAN literature and we support this claim with references as recent as 2017 . * * The useful outcome is that the analysis yields a novel proof for the asymptotic convergence of GAN training in the non-parametric limit and it does not require the discriminator to be optimal at each step * * The current revision reflects this message . 4.To explain mode collapse , we analyze the realistic non-convex case in section 2.3 . This is very different from convex-concave case actually ( we apologize for the confusion ) and we cite the works of Hazan et.al to rigorously argue that convergence to local equilibria can be expected using regret minimization or OGD . This leads us to the main hypothesis of our paper - that mode collapse is just an undesirable local equilibrium and it should be possible to avoid it . We apologize for not being clear earlier . 5.A new section 2.4 has been added which explains how we can avoid 'mode collapse ' equilibria . Based on empirical observations , we characterize mode collapse situation with sharp gradients of the discriminator function around some real data points . This is key to fighting mode collapse or avoiding such undesirable equilibria . We provide arguments and supporting experiments for this . From this motivation , DRAGAN penalty scheme is introduced . This was a key transition that was missing in the earlier version . DRAGAN algorithm : 1 . From the strengthened theory sections , it should be clear that as long as D ( x ) has small gradients around real data , mode collapse can be mitigated . We removed the arbitrary ' 1 ' in our scheme and used a generic ' k ' ( some small constant ) . We apologize for the jump earlier . 2.The key idea is keeping D ( x ) gradients small and this stems from our observation that 'mode collapse ' equilibria can be characterized by large gradients of D in the data space . In fact , this partly explains why WGAN-GP and LS-GAN improve stability , despite being motivated by reasoning ( divergence minimization hypothesis ) that is based on unrealistic assumptions . We urge you to take another look at sections 2.3 and 2.4 ."}], "0": {"review_id": "ryepFJbA--0", "review_text": "Summary ======== The authors present a new regularization term, inspired from game theory, which encourages the discriminator's gradient to have a norm equal to one. This leads to reduce the number of local minima, so that the behavior of the optimization scheme gets closer to the optimization of a zero-sum games with convex-concave functions. Clarity ====== Overall, the paper is clear and well-written. However, the authors should motivate better the regularization introduced in section 2.3. Originality ========= The idea is novel and interesting. In addition, it is easy to implement it for any GANs since it requires only an additional regularization term. Moreover, the numerical experiments are in favor of the proposed method. Comments ========= - Why should the norm of the gradient should to be equal to 1 and not another value? Is this possible to improve the performance if we put an additional hyper-parameter instead? - Are the performances greatly impacted by other value of lambda and c (the suggested parameter values are lambda = c = 10)? - As mentioned in the paper, the regularization affects the modeling performance. Maybe the authors should add a comparison between different regularization parameters to illustrate the real impact of lambda and c on the performance. - GANs performance is usually worse on very big dataset such as Imagenet. Does this regularization trick makes their performance better? Post-rebuttal comments --------------------------------- I modified my review score, according to the problems raised by Reviewer 1 and 3. Despite the idea looks pretty simple and present some advantages, the authors should go deeper in the analysis, especially because the idea is not so novel.", "rating": "5: Marginally below acceptance threshold", "reply_text": "- A small correction in your summary . Our penalty scheme helps avoid bad local equilibria and the convex-concave case , while being simple , is quite different from the non-convex case . - We changed section 2.3 to rigorously argue that regret minimization converges to ( potentially bad ) local equilibria , added a new section 2.4 to characterize what these 'mode collapse ' equilibria look like ( D has large gradients around real samples in this case ) and demonstrate that they can be averted using gradient constraints , through new toy experiments . This provides good intuition and a strong motivation for the introduction of DRAGAN scheme . We urge you to take another look at sections 2.3 , 2.4 . - In the updated revision , we correct this arbitrary choice and use ' k ' , which should be something small . Basically , we observe that 'mode collapse ' equilibria exhibit sharp gradients of the discriminator function around real samples . So , we regularize so as to keep these gradients small . We apologize for not making it clear earlier . - You make an excellent point that by tuning ' k'/ ' c'/'lambda ' , it could be possible to get better performance but our aim here was just to demonstrate the effectiveness of our method . My intuition is that optimal configuration will depend on data domain , architecture and hence , its beyond the scope of our paper . But , this is an important topic for possibly a future work . - We only explore the performance of our penalty on MNIST , CIFAR-10 and CelebA , like most papers in this direction . I think the performance on ImageNet depends heavily on the architecture but we did not explore this aspect in our paper . It is an interesting topic to compare various methods on bigger datasets , maybe using ResNets ."}, "1": {"review_id": "ryepFJbA--1", "review_text": "This paper addresses the well-known stability problem encountered when training GANs. As many other papers, they suggest adding a regularization penalty on the discriminator which penalizes the gradient with respect to the data, effectively linearizing the data manifold. Relevance: Although I think some of the empirical results provided in the paper are interesting, I doubt the scientific contribution of this paper is significant. First of all, the penalty the author suggest is the same as the one suggest by Gulrajani for Wasserstein GAN (there the motivation behind this penalty comes from the optimal transport plan). In this paper, the author apply the same penalty to the GAN objective with the alternative update rule which is also a lower-bound for the Wasserstein distance. Justification: The authors justify the choice of their regularization saying it linearizes the objective along the data manifold and claim it reduces the number of non-optimal fixed points. This might be true in the data space but the GAN objective is optimized over the parameter space and it is therefore not clear to me their argument hold w.r.t to the network parameters. Can you please comment on this? Regularizing the generator: Can the authors motivate their choice for regularizing the discriminator only, and not the generator? Following their reasoning of linearizing the objective, the same argument should apply to the generator. Comparison to existing work: This is not the first paper that suggests adding a regularization. Given that the theoretical aspect of the paper are rather weak, I would at least expect a comparison to existing regularization methods, e.g. Stabilizing training of generative adversarial networks through regularization. NIPS, 2017 Choice of hyper-parameters: The authors say that the suggested value for lambda is 10. Can you comment on the choice of this parameter and how it affect the results? Have you tried annealing lambda? This is a common procedure in optimization (see e.g. homotopy or continuation methods). Bogonet score: I very much like the experiment where the authors select 100 different architectures to compare their method against the vanilla GAN approach. I here have 2 questions: - Did you do a deeper examination of your results, e.g. was there some architectures for which none of the method performed well? - Did you try to run this experiment on other datasets? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Clarification regarding the importance of our theory sections : We admit that the clarity in our presentation was lacking ( especially ties to the GAN literature ) and tried to address it in the new revision . We urge you to please take another look . Specifically , our contributions are : ( reflected in updated abstract and introduction ) : - We propose to study GAN training as regret minimization . This is a completely novel contribution . In contrast , the popular view is that there is consistent divergence minimization and this is based on the unrealistic assumption that the discriminator is playing optimally at each step and making these updates in the function space . This is n't tractable ( nor ) close to what happens in practice . This forms the main motivation for our paper . - We present the analysis of artificial convex-concave case based on standard results in game theory literature . More importantly , we make explicit the connection between GAN training process ( alternating gradient updates ) and regret minimization , along the way , in section 2.2 . These are not widely known results in the GAN literature and we provide supporting references in the new revision . * * The useful outcome is that this analysis yields a novel proof for the asymptotic convergence of GAN training in the non-parametric limit and it does not require the discriminator to be optimal at each step * * The current revision reflects this message . - To explain mode collapse , we next analyze the realistic non-convex case in section 2.3 from regret minimization perspective . This is very different from the convex-concave case actually ( we apologize for the confusion ) and we cite the works of Hazan et.al to rigorously argue that convergence to potentially bad local equilibria happens using gradient updates ( under some conditions ) . Please see the updated section 2.3 . This leads us to the main hypothesis of our paper - that mode collapse is just an undesirable local equilibrium and it should be possible to avoid it . We apologize for not being clear earlier . The natural question now is how we can avoid these equilibria ? - A new section 2.4 has been added which explains how we can avoid 'mode collapse ' equilibria ( this was implicit and not clear earlier ) . Based on empirical observations , we basically characterize 'mode collapse ' equilibria with sharp gradients of the discriminator function around some real data points . This is key to fighting mode collapse and avoiding such undesirable equilibria . We provide arguments and supporting experiments for this in a new section 2.4 . This was a key transition that was missing in the earlier version . - From this motivation ( of keeping D 's gradients small in ambient data space ) , we propose DRAGAN penalty scheme . In fact , our theory also explains how other gradient penalties ( WGAN-GP/LS-GAN ) might be mitigating mode collapse . We compare and discuss its advantages over them in section 2.5 . And present the main experiments in section 3 ."}, "2": {"review_id": "ryepFJbA--2", "review_text": "This paper contains a collection of ideas about Generative Adversarial Networks (GAN) but it is very hard for me to get the main point of this paper. I am not saying ideas are not interesting, but I think the author needs to choose the main point of the paper, and should focus on delivering in-depth studies on the main point. 1. On the game theoretic interpretations The paper, Generative Adversarial Nets, NIPS 2014, already presented the game theoretic interpretations to GANs, so it's hard for me to think what's new in the section. Best response dynamics is not used in the conventional GAN training, because it's very hard to find the global optimal of inner minimization and outer maximization. The convergence of online primal-dual gradient descent method in the minimax game is already well-known, but this analysis cannot be applied to the usual GAN setting because the objective is not convex-concave. I found this analysis would be very interesting if the authors can find the toy example when GAN becomes convex-concave by using different model parameterizations and/or different f-divergence, and conduct various studies on the convergence and stability on this problem. I also found that the hypothesis on the model collapsing has very limited connection to the convex-concave case. It is OK to form the hypothesis and present an interesting research direction, but in order to make this as a main point of the paper, the author should provide more rigorous arguments or experimental studies instead of jumping to the hypothesis in two sentences. For example, if the authors can provide the toy example where GAN becomes convex-concave vs. non-convex-concave case, and how the loss function shape or gradient dynamics are changing, that will provide very valuable insights on the problem. 2. DRAGAN As open commenters pointed out, I found it's difficult to find why we want to make the norm of the gradient to 1. Why not 2? why not 1/2? Why 1 is very special? In the WGAN paper, the gradient is clipped to a number less than 1, because it is a sufficient condition to being 1-Lipshitz, but this paper provides no justification on this number. It's OK not to have the theoretical answers to the questions but in that case the authors should provide ablation experiments. For example, sweeping gradient norm target from 10^-3, 10^-2, 10^-1, 1.0, 10.0, etc and their impact on the performance. Also scheduling regularization parameter like reducing the size of lambda exponentially would be interesting as well. Most of those studies won't be necessary if the theory is sound. However, since this paper does not provide a justification on the magic number \"1\", I think it's better to include some form of ablation studies. Note that the item 1 and item 2 are not strongly related to each other, and can be two separate papers. I recommend to choose one direction and provide in-depth study on one topic. Currently, this paper tries to present interesting ideas without very deep investigations, and I cannot recommend this paper to be published. ", "rating": "3: Clear rejection", "reply_text": "We found your review to be extremely helpful to understand where our paper was lacking . Our paper did have multiple new ideas and the presentation was n't always clear ( ties to GAN literature were missing ) . Thanks to your feedback , we chose the most important strand and strengthened it in the current revision using additional theoretical arguments and targeted experiments . The core content is still the same but the presentation has been changed significantly for improved clarity . We urge you to take another look . Specifically , the main points now read as ( reflected in updated abstract and introduction ) : - We propose to study GAN training as regret minimization ( this is a novel view ) , which is in contrast to the popular view that there is consistent divergence minimization . More about this below . - We provide a novel proof for the asymptotic convergence of GAN training in the non-parametric limit and it does not require the discriminator to be optimal at each step . - Regret minimization ( AGD ) in non-convex games converges to potentially bad local equilibria , under some conditions and we hypothesize mode collapse to be resulting from this . Please see updated section 2.3 , where we added theoretical arguments to support this . Next question is how we can avoid these equilibria ? - We characterize mode collapse equilibria with sharp gradients of the discriminator function around some real data points . We provide toy experiments and arguments in a new section 2.4 to support this . We apologize for missing this key transition earlier . - From this motivation , we propose DRAGAN penalty scheme . We compare and discuss advantages over WGAN-GP , LS-GAN in section 2.5 . And present main experiments in section 3 . Theory sections : 1 . We should have called section 2.1 as background since its reviewing original GAN paper 's formulation and we apologize for not making it clear . 2.However , studying GAN training dynamics as regret minimization is a completely novel contribution . And the popular divergence minimization hypothesis stems from D using best-response algorithm in the function space . This is n't tractable as you mention , nor close to what happens in practice . In fact , this is the main motivation for our paper . 3.We agree with you that the content in section 2.3 is well-known in game theory literature and moreover , the convex-concave is artificial . Our goal here was to simply review these results , introduce formal notions along the way , which we use in later sections and most importantly , make explicit the connection between GAN training ( alternating gradient updates ) and regret minimization . None of this is widely known in GAN literature and we support this claim with references as recent as 2017 . * * The useful outcome is that the analysis yields a novel proof for the asymptotic convergence of GAN training in the non-parametric limit and it does not require the discriminator to be optimal at each step * * The current revision reflects this message . 4.To explain mode collapse , we analyze the realistic non-convex case in section 2.3 . This is very different from convex-concave case actually ( we apologize for the confusion ) and we cite the works of Hazan et.al to rigorously argue that convergence to local equilibria can be expected using regret minimization or OGD . This leads us to the main hypothesis of our paper - that mode collapse is just an undesirable local equilibrium and it should be possible to avoid it . We apologize for not being clear earlier . 5.A new section 2.4 has been added which explains how we can avoid 'mode collapse ' equilibria . Based on empirical observations , we characterize mode collapse situation with sharp gradients of the discriminator function around some real data points . This is key to fighting mode collapse or avoiding such undesirable equilibria . We provide arguments and supporting experiments for this . From this motivation , DRAGAN penalty scheme is introduced . This was a key transition that was missing in the earlier version . DRAGAN algorithm : 1 . From the strengthened theory sections , it should be clear that as long as D ( x ) has small gradients around real data , mode collapse can be mitigated . We removed the arbitrary ' 1 ' in our scheme and used a generic ' k ' ( some small constant ) . We apologize for the jump earlier . 2.The key idea is keeping D ( x ) gradients small and this stems from our observation that 'mode collapse ' equilibria can be characterized by large gradients of D in the data space . In fact , this partly explains why WGAN-GP and LS-GAN improve stability , despite being motivated by reasoning ( divergence minimization hypothesis ) that is based on unrealistic assumptions . We urge you to take another look at sections 2.3 and 2.4 ."}}