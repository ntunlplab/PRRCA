{"year": "2021", "forum": "U_mat0b9iv", "title": "Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network", "decision": "Accept (Poster)", "meta_review": "The authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets. The results are important in the general context of the \"strong\" lottery ticket hypothesis, and are of both theoretical and practical interest. Although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel.  Some further concerns of clarity and novelty were addressed by the authors.", "reviews": [{"review_id": "U_mat0b9iv-0", "review_text": "This paper propose utilizing the existing `` lottery ticket '' result for constructing binary neural networks . This work has some novelty , in the sense that I have n't seen any other papers on untrained binary neural networks . The experimental results looks good . However , I have some concerns on this paper . 1.This paper , at least the main text , is not self-contained . The writing needs significant improvement . The main contribution of the paper , section 2 , is only one-page long . Neither the theory nor the algorithm are well explained in the main text . Moreover , the algorithm relies heavily on the edge-popup algorithm , which is not explained even in the supplementary material . The title is also somewhat too long . 2.Though the proposed algorithm achieves excellent results in terms of parameter count and accuracy , I think the comparison is somewhat unfair . The subnetwork is sparse , and can be much slower on real hardware . Moreover , pruning from a larger network is known to achieve better result than training a smaller network from scratch , but the baselines does not utilize this . 3.There lacks any discussion on the real time consumption of the proposed network . 4.Time consumption of training should also be reported . Post rebuttal = Thanks the authors for clarifying and revising the paper . The updated version does look much clearer to me , so I updated my ratings . I am still wondering the difference of biprop vs. a classical quantization-aware training for ternary networks . I did read the response to R3 . From my understanding it seems that : 1. biprop does n't count 0 as a parameter , while TWN does ; 2. biprop prunes a larger network ( WRN50 ) , while TWN trains a network of the original size ( ResNet-50 ) ; I am not sure if the superiority of biprop comes from these reasons , instead of LTH itself . biprop still looks more like a QAT algorithm than a LT-finding algorithm in the sence that 1. it does not train the pruned network after finding the LT as the original LTH paper ; 2. it directly learns the binary weights . Just out of curiosity , but I think clarifying these concerns would make the paper stronger .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> However , I have some concerns on this paper . This paper , at least the main text , is not self-contained . The writing needs significant improvement . In the revised version of the paper , we have made the main text self-contained by bringing back details from the appendix to the main body . We would like to emphasize that most of these details were not missing from the paper but were pushed to the appendix to make the paper more readable ( at the cost of self-containment ) . We are grateful to the reviewer for this suggestion as the revised reorganization has made the clarity of the paper significantly better without missing any major details . If there is any specific part of the paper that reviewer feels still need more details , we can make that change . > The main contribution of the paper , section 2 , is only one-page long . We would like to bring the attention of the reviewer to the fact that the challenge we faced was not that we did not have sufficient technical results but that we made significantly more technical contribution than we could fit in the main body of the paper . Note that related full-precision NN papers are either empirical ( Frankle 2018 and Zhou 2019 ) , algorithmic ( Ramanujan 2020 ) or theoretical ( Malach 2020 ) . We have made notable progress on all three fronts : 1. proposed generalization of ( or much stronger ) lottery ticket hypothesis -- highlights the extent of the DNN compression and a new paradigm to learn binary neural nets , 2. developed high performing algorithms to find these winning tickets \u2013 our MPT-1/32 are current SOTA for BWNs and also beat their large and FP counterparts without any significant hyperparameter tuning or commonly used tricks for BNNs , and 3. presenting first theoretical result that pruned BNNs are universal approximators -- proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN . Further , these advances are not a trivial extension of the related full-precision literature ( explained later ) . > Though the proposed algorithm achieves excellent results in terms of parameter count and accuracy , I think the comparison is somewhat unfair . The subnetwork is sparse and can be much slower on real hardware . We respectfully disagree with the comment that \u201c the comparison is unfair as subnetwork is sparse and can be much slower on real hardware \u201d . This comment is not specific our approach but to any pruning-based approach . We think what reviewer is trying to say is that sparse NNs can not achieve meaningful speedup on commodity hardware ( e.g. , GPU ) built for dense matrix computations . This is indeed true and this is the precise reason that specialized accelerators are designed to exploit the sparsity . However , this does not refute our claims or make the comparison unfair in any way . The sparse model has fewer parameters and , theoretically , less computation costs and approaches such as [ 2,3 ] can be leveraged to achieve significant speed ups . However , the hardware implementation of MPTs is out of the scope of this paper and is mentioned as a worthwhile future direction in the revised manuscript . > Neither the theory nor the algorithm are well explained in the main text . We are sorry for this and thank you for giving us an opportunity to fix this . 1.In the initial version , we only provided an informal statement of our main theoretical result in the main body to improve the readability . In the revised version , we have provided more specific details on the result and also provided a proof sketch in the main body of the paper . 2.In the initial version of the paper , we provided a general pseudocode for the \u2018 biprop \u2019 framework accommodating a class of potential algorithms . In the revised version , we have instead provided a detailed algorithmic description and theoretical justification of the specific algorithm used in our experimental section . We hope that these changes have made theory and algorithm clear . We would be happy to make any further changes suggested by the reviewer ."}, {"review_id": "U_mat0b9iv-1", "review_text": "This work investigated a method of finding a subnetwork of redundant binary networks to gain an overall advantage over pruned or quantized networks . One main concern of the reviewer is the similarity between the paper 's approachtraining a mask over a binary networkand the conventional ternary network . Are the masked weights analogous to the 0 of the ternary networks , while the unmasked weights are in { -1 , 1 } ? In that case , it 's fairer to compare with ternary networks with 0 counted into the total params . Following this analogy , it is also misleading to claim that the network is `` untrained '' , as to minimize the loss , any binarized weights can be updated to 0 ( masked ) , although indeed the weight update across 0 is forbidden . The theory works of this paper are strong and prove that the expressive power of redundant binary ( or ternary ? ) networks can match their denser counterpart . Still , the question to clarify here is that whether `` subset ( lottery ticket ) + binary network '' equals to `` ternary network '' . In general , the paper is well written and the theoretical and experimental works support the authors ' claim . The reviewer would recommend accepting the paper on the condition that the authors can address the comparison with the ternary network fairly .", "rating": "7: Good paper, accept", "reply_text": "> Are the masked weights analogous to the 0 of the ternary networks , while the unmasked weights are in { -1 , 1 } ? In that case , it 's fairer to compare with ternary networks with 0 counted into the total params . Zero is not counted into the total parameters to highlight the amount of sparsity in the obtained networks . If we count ' 0 ' towards the total parameter count , the number of total parameters for all networks will be the same and this will be unable to highlight the performance of the pruning . Reporting sparsity is a well-accepted practice/metric in the pruning literature -- in fact the goal of pruning is precisely to increase the amount of sparsity ( weight values equal to \u2018 0 \u2019 ) without dropping the performance . The reason being that having sparser NNs may enable the application of sparse data structures for memory and compute efficiency . To clarify this , in the revised manuscript , we mention \u201c non-zero \u201d parameters when reporting these numbers . > Following this analogy , it is also misleading to claim that the network is `` untrained '' , as to minimize the loss , any binarized weights can be updated to 0 ( masked ) , although indeed the weight update across 0 is forbidden . > The reason for using the term was to differentiate our approach from conventional approaches that optimize over the weights as we are only dropping/pruning connections from a randomly initialized/weighted binary neural network . The reviewer correctly pointed out that we indeed optimize over the mask ( \u2018 0 \u2019 values ) . To make our usage clear , we have clarified the term \u201c untrained \u201d in the revised manuscript . However , we are open to completely removing this term if reviewer suggests so . > The theory works of this paper are strong ... redundant binary ( or ternary ? ) networks can match their denser counterpart . Still , the question to clarify here is that whether `` subset ( lottery ticket ) + binary network '' equals to `` ternary network '' . Our work proves the expressive power of pruning randomly initialized/weight binary neural networks . We would prefer using the term pruned randomly initialized binary neural networks to avoid confusing our approach with ternary neural network approaches in the literature [ 1 ] . Comparison with TNN is discussed next . > The reviewer would recommend accepting the paper on the condition that the authors can address the comparison with the ternary network fairly . Thank you for asking and giving us an opportunity to answer this excellent question . Note that our approach does not fall under any existing framework including binary and ternary neural networks learned via weight-optimization . One could claim that our setting is more restrictive than even binary neural networks/BNN ( let alone ternary neural networks ) as we learn by pruning \u201c randomly weighted \u201d binary neural network . In other words , unlike existing BNN approaches , we do not optimize over weights and learn purely by pruning \u2013 pruning mask is the only optimizable parameter . As we show in the paper , our approach can match ( or beat ) even full precision neural networks trained using weight-optimization for all considered cases except MPT-1/1 on ImageNet . Thus , they match ( or beat ) any M-ary quantization including ternary neural networks . The reviewer is correct in pointing out that weights in the original/large structure can be seen as taking ternary values as pruned weights can be consider having the value \u2018 0 \u2019 . But note the activations still take binary values not ternary in MPT-1/1 . We differ from ternary weight networks ( TWNs ) [ 1 ] due to the following reasons : 1 ) In addition to unpruned weights in MPTs ( i.e. , { +1 , -1 } ) not being trained , we allow a precise control over the sparsity , i.e. , number of weights taking the value \u2018 0 \u2019 , and 2 ) As our goal is pruning , our solutions are significantly sparser than TWNs \u2013 TWNs achieve their best solutions with sparsity < 50 % ( usually 20 % -40 % ) , and our solutions are always with sparsity > 50 % ( usually 60 % -95 % ) . This enables higher energy efficiency and more memory saving ( sometimes even when compared to BNNs ) . Informally , we could say that ( a ) in terms of memory and compute requirements : MPT $ \\leq $ BNN < TNN < FPN , and ( b ) in terms of performance/accuracy : MPT $ \\geq $ FPN > TNN > BNN ( for MPT-1/32 ) . We would further like to point you to our joint response to all the reviewers to highlight novelty , revised results , and most importantly usefulness and implications of our approach on BNNs and DNNs in general . We hope that the reviewer would agree that the comparison is fair , and our results are much stronger than initially perceived . We would be happy to clarify any further questions/concerns the reviewer might have . [ 1 ] Ternary weight networks . arXiv:1605.04711 ( 2016 ) . [ 2 ] SparseRT : Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference . https : //arxiv.org/abs/2008.11849 [ 3 ] Sparsely-connected neural networks : towards efficient vlsi implementation of deep neural networks . arXiv:1611.01427 ( 2016 ) ."}, {"review_id": "U_mat0b9iv-2", "review_text": "The paper proposes an innovate method based on lottery ticket hypothesis to prune a BNN ( parameters are only -1 ( 0 ) and +1 , it can be viewed as an extreme case of quantization ) from a dense NN . It focuses on learning a mask to prune the NN instead of the traditional method ( pruning on an already trained network ) . In addition , not only experiments but theortical proof are given and have a highly brief result . Pro : The way to find the mask iteratively is innovate and has a mathematical support . The result of MPT is amazing because the untrained network can be pruned to a BNN with comparable accrancy of some trained SOTA NN on CIFAR dataset . The experiments show it can be generized to deeper and wider network . It has better accurancy than other BNN methods but network parameters still high . Con : The main article spends little word to describe how to find the mask , and it is not a trivial way . The experiments of generization are only done on the very small NN ( e.g.Conv2/4/6/8 ) . Clarity : Very low . Pros : The authors try to express in a way that every step of logical connections in this paper can be clearly understood by readers . To reduce complexity , many parts are settled in the appendix . Cons : Many sentences in this paper are quite long and sometimes using nesting clauses , which makes the text to be obscure . Besides , since many parts are moved into the appendix , the whole structure of the main body is kind of empty and shallow . Some summative and conclusive paragraphs are the simple repetition of previous \u201c claims \u201d since the demonstrations are in the appendix . All of these make a lower clarity . Finally , I find that this paper has narrower page margins , which means each line can contain more characters . Besides , the header \u201c Under review as a conference paper at \u2026 \u201d is missing . These modifications of the submit template may volatile the conference rule and should be considered a cheating behavior . So I give the \u201c very low \u201d score on clarity . Originality : Medium . Pros : They apply the Lottery Ticket Hypothesis to quantization/BNN . They give proof of their rationality . Cons : The success essentials of their algorithm \u201c biprop \u201d contains two-part : edge-popup and gradient estimator . Both of them are take-away from other works . I regard this work as a new application of LTH to binary neural networks .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q.Finally , I find that this paper has narrower page margins , which means each line can contain more characters . Besides , the header \u201c Under review as a conference paper at \u2026 \u201d is missing . These modifications of the submit template may volatile the conference rule and should be considered a cheating behavior . A.We thank the reviewer for pointing this out . These changes were not intentional but caused by mistakenly using the \u201c fullpage \u201d package with the ICLR 21 official latex template and that caused this change . This happened when transferring our draft and used packages ( which included the \u201c fullpage \u201d package ) into the .tex file with the official ICLR 21 latex template . Further we want to highlight that this change has not given us any unfair advantage \u2013 for your reference we have uploaded a revised version with the \u201c fullpage \u201d package removed showing that the same information exists in the version with and without this issue . For these reasons , we hope that the reviewer will not take this issue into account in his/her revised score . We will respond to your other concerns very soon . Thank you !"}, {"review_id": "U_mat0b9iv-3", "review_text": "The authors propose a stronger lottery ticket hypothesis in this paper \u2013 the multi-prize lottery ticket hypothesis . In particular , the new hypothesis seeks answer to the required amount of over-parameterization for a randomly initialized network to become able to compress to a sparse untrained binary subnetwork with on-par accuracy . The authors prove the existence of such subnetwork and show the bounds on over-parameterization . The paper proposes new methods to get the binary-weight tickets and the binary-activation tickets , where binary-weight tickets are subnetworks with weights as binary , and binary-activation tickets have the activation function in the forward propagation as binary . As binary networks can largely reduce the computational complexity for inference , this work has practical importance especially for applications with constraints for memory and power . The paper has many simulation results to support the theoretical guarantees , and the proposed approach on binary-weight networks has advantages over existing methods . The paper has sufficient and novel contributions , both for the theoretical results on binary subnetworks and the empirical evaluations that reveal the efficiency of the algorithm on binary-weight subnetworks , so that I recommend this paper for publication . Here are some minor concerns . [ a ] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1 . Compared with Malach 2020 \u201c Proving the lottery ticket hypothesis : pruning is all you need \u201d , the authors can highlight what key differences are needed for proof of the binary network . [ b ] For the comparison of full precision network and the MPT from this paper , it can be useful if the computational complexity is shown , where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network . [ c ] The MPT 1/1 seems not to perform as well compared to trained binary activation network , which may reduce the quality of paper . It is good that the authors mention future work for MPT 1/1 network to attain state-of-art results of binary activation network . [ d ] Some of the references seem to lack information , e.g.Malach 2020 , Orseau 2020 , both of which do not have the venue or journal names . [ e ] A question : for the binary weights subnetworks , what does it mean by 20 million parameters ? The weights can only be -1 or 1 , so the network has 20 million of -1 or 1 , but they do not require any multiplication at the inference stage ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive review ! > [ a ] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1 . Compared with Malach 2020 \u201c Proving the lottery ticket hypothesis : pruning is all you need \u201d , the authors can highlight what key differences are needed for proof of the binary network . To the best of our knowledge ours is the first theoretical result proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN . As it has been established that real-valued DNNs are universal approximators ( Scarselli & Tsoi , 1998 ) , our result carries the implication that pruned binary-weight DNNs are universal approximators as well . In relation to the first result establishing the existence of real-valued subnetworks in a randomly weighted DNN approximating a real-valued target DNN ( Malach et al. , 2020 ) , the lower bound on the width established in Theorem 2 is better than their lower bound of $ O\\left ( \\ell^2 n^2 \\log ( \\ell n / \\delta ) /\\varepsilon^2 \\right ) $ . While the methodology of our proof is similar to ( Malach et al. , 2020 ) , proving our result for binary-weight subnetworks is not a trivial extension of their technique and require different tools . The use of binary weights in the analysis results in more complex scenarios that have to be addressed carefully . In particular , the analysis in Lemma 1 requires cleverly splitting a complex scenario involving a multinomial distribution into two simpler cases involving binomial distributions . Then , Hoeffding 's inequality is used to derive probabilistic bounds for in each case . The results are combined to yield the final probabilistic bound . This analysis is more complex than the analysis required for real-valued subnetworks as Hoeffding 's inequality was not required to establish an analogous result for real-valued subnetworks ( Malach et al. , 2020 ) . Additionally , the use of binary weights required us to identify an appropriate gain term for rescaling the binary weights in our analysis . Binary weights also result in potentially larger subnetworks being required to approximate a target network and we must carefully keep track of these requirements as we establish results for increasingly larger networks in Lemma 2 , Lemma 3 , and Theorem 2 in Appendix B . In the work on real-valued networks ( Malach et al. , 2020 ) the bound on the required number of parameters in the subnetwork is more straightforward . Furthermore , our analysis yields a tighter bound on the width of the binary network than the bound established in ( Malach et al. , 2020 ) for real-valued subnetworks . We hope that this insight into some of the complexity required when analyzing binary subnetworks in our paper illustrates how the analysis is more complex when working with binary subnetworks . > [ b ] For the comparison of full precision network and the MPT from this paper , it can be useful if the computational complexity is shown , where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network . For both MPTs and FP weight-trained baselines , we used the same number of epochs as mentioned in the Appendix A.1 . Theoretically , MPTs can be trained more efficiently than their FP counterparts . However , achieving this would require their implementation on specialized hardwares , e.g. , FPGA , and , this is out of the scope of our paper ."}], "0": {"review_id": "U_mat0b9iv-0", "review_text": "This paper propose utilizing the existing `` lottery ticket '' result for constructing binary neural networks . This work has some novelty , in the sense that I have n't seen any other papers on untrained binary neural networks . The experimental results looks good . However , I have some concerns on this paper . 1.This paper , at least the main text , is not self-contained . The writing needs significant improvement . The main contribution of the paper , section 2 , is only one-page long . Neither the theory nor the algorithm are well explained in the main text . Moreover , the algorithm relies heavily on the edge-popup algorithm , which is not explained even in the supplementary material . The title is also somewhat too long . 2.Though the proposed algorithm achieves excellent results in terms of parameter count and accuracy , I think the comparison is somewhat unfair . The subnetwork is sparse , and can be much slower on real hardware . Moreover , pruning from a larger network is known to achieve better result than training a smaller network from scratch , but the baselines does not utilize this . 3.There lacks any discussion on the real time consumption of the proposed network . 4.Time consumption of training should also be reported . Post rebuttal = Thanks the authors for clarifying and revising the paper . The updated version does look much clearer to me , so I updated my ratings . I am still wondering the difference of biprop vs. a classical quantization-aware training for ternary networks . I did read the response to R3 . From my understanding it seems that : 1. biprop does n't count 0 as a parameter , while TWN does ; 2. biprop prunes a larger network ( WRN50 ) , while TWN trains a network of the original size ( ResNet-50 ) ; I am not sure if the superiority of biprop comes from these reasons , instead of LTH itself . biprop still looks more like a QAT algorithm than a LT-finding algorithm in the sence that 1. it does not train the pruned network after finding the LT as the original LTH paper ; 2. it directly learns the binary weights . Just out of curiosity , but I think clarifying these concerns would make the paper stronger .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> However , I have some concerns on this paper . This paper , at least the main text , is not self-contained . The writing needs significant improvement . In the revised version of the paper , we have made the main text self-contained by bringing back details from the appendix to the main body . We would like to emphasize that most of these details were not missing from the paper but were pushed to the appendix to make the paper more readable ( at the cost of self-containment ) . We are grateful to the reviewer for this suggestion as the revised reorganization has made the clarity of the paper significantly better without missing any major details . If there is any specific part of the paper that reviewer feels still need more details , we can make that change . > The main contribution of the paper , section 2 , is only one-page long . We would like to bring the attention of the reviewer to the fact that the challenge we faced was not that we did not have sufficient technical results but that we made significantly more technical contribution than we could fit in the main body of the paper . Note that related full-precision NN papers are either empirical ( Frankle 2018 and Zhou 2019 ) , algorithmic ( Ramanujan 2020 ) or theoretical ( Malach 2020 ) . We have made notable progress on all three fronts : 1. proposed generalization of ( or much stronger ) lottery ticket hypothesis -- highlights the extent of the DNN compression and a new paradigm to learn binary neural nets , 2. developed high performing algorithms to find these winning tickets \u2013 our MPT-1/32 are current SOTA for BWNs and also beat their large and FP counterparts without any significant hyperparameter tuning or commonly used tricks for BNNs , and 3. presenting first theoretical result that pruned BNNs are universal approximators -- proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN . Further , these advances are not a trivial extension of the related full-precision literature ( explained later ) . > Though the proposed algorithm achieves excellent results in terms of parameter count and accuracy , I think the comparison is somewhat unfair . The subnetwork is sparse and can be much slower on real hardware . We respectfully disagree with the comment that \u201c the comparison is unfair as subnetwork is sparse and can be much slower on real hardware \u201d . This comment is not specific our approach but to any pruning-based approach . We think what reviewer is trying to say is that sparse NNs can not achieve meaningful speedup on commodity hardware ( e.g. , GPU ) built for dense matrix computations . This is indeed true and this is the precise reason that specialized accelerators are designed to exploit the sparsity . However , this does not refute our claims or make the comparison unfair in any way . The sparse model has fewer parameters and , theoretically , less computation costs and approaches such as [ 2,3 ] can be leveraged to achieve significant speed ups . However , the hardware implementation of MPTs is out of the scope of this paper and is mentioned as a worthwhile future direction in the revised manuscript . > Neither the theory nor the algorithm are well explained in the main text . We are sorry for this and thank you for giving us an opportunity to fix this . 1.In the initial version , we only provided an informal statement of our main theoretical result in the main body to improve the readability . In the revised version , we have provided more specific details on the result and also provided a proof sketch in the main body of the paper . 2.In the initial version of the paper , we provided a general pseudocode for the \u2018 biprop \u2019 framework accommodating a class of potential algorithms . In the revised version , we have instead provided a detailed algorithmic description and theoretical justification of the specific algorithm used in our experimental section . We hope that these changes have made theory and algorithm clear . We would be happy to make any further changes suggested by the reviewer ."}, "1": {"review_id": "U_mat0b9iv-1", "review_text": "This work investigated a method of finding a subnetwork of redundant binary networks to gain an overall advantage over pruned or quantized networks . One main concern of the reviewer is the similarity between the paper 's approachtraining a mask over a binary networkand the conventional ternary network . Are the masked weights analogous to the 0 of the ternary networks , while the unmasked weights are in { -1 , 1 } ? In that case , it 's fairer to compare with ternary networks with 0 counted into the total params . Following this analogy , it is also misleading to claim that the network is `` untrained '' , as to minimize the loss , any binarized weights can be updated to 0 ( masked ) , although indeed the weight update across 0 is forbidden . The theory works of this paper are strong and prove that the expressive power of redundant binary ( or ternary ? ) networks can match their denser counterpart . Still , the question to clarify here is that whether `` subset ( lottery ticket ) + binary network '' equals to `` ternary network '' . In general , the paper is well written and the theoretical and experimental works support the authors ' claim . The reviewer would recommend accepting the paper on the condition that the authors can address the comparison with the ternary network fairly .", "rating": "7: Good paper, accept", "reply_text": "> Are the masked weights analogous to the 0 of the ternary networks , while the unmasked weights are in { -1 , 1 } ? In that case , it 's fairer to compare with ternary networks with 0 counted into the total params . Zero is not counted into the total parameters to highlight the amount of sparsity in the obtained networks . If we count ' 0 ' towards the total parameter count , the number of total parameters for all networks will be the same and this will be unable to highlight the performance of the pruning . Reporting sparsity is a well-accepted practice/metric in the pruning literature -- in fact the goal of pruning is precisely to increase the amount of sparsity ( weight values equal to \u2018 0 \u2019 ) without dropping the performance . The reason being that having sparser NNs may enable the application of sparse data structures for memory and compute efficiency . To clarify this , in the revised manuscript , we mention \u201c non-zero \u201d parameters when reporting these numbers . > Following this analogy , it is also misleading to claim that the network is `` untrained '' , as to minimize the loss , any binarized weights can be updated to 0 ( masked ) , although indeed the weight update across 0 is forbidden . > The reason for using the term was to differentiate our approach from conventional approaches that optimize over the weights as we are only dropping/pruning connections from a randomly initialized/weighted binary neural network . The reviewer correctly pointed out that we indeed optimize over the mask ( \u2018 0 \u2019 values ) . To make our usage clear , we have clarified the term \u201c untrained \u201d in the revised manuscript . However , we are open to completely removing this term if reviewer suggests so . > The theory works of this paper are strong ... redundant binary ( or ternary ? ) networks can match their denser counterpart . Still , the question to clarify here is that whether `` subset ( lottery ticket ) + binary network '' equals to `` ternary network '' . Our work proves the expressive power of pruning randomly initialized/weight binary neural networks . We would prefer using the term pruned randomly initialized binary neural networks to avoid confusing our approach with ternary neural network approaches in the literature [ 1 ] . Comparison with TNN is discussed next . > The reviewer would recommend accepting the paper on the condition that the authors can address the comparison with the ternary network fairly . Thank you for asking and giving us an opportunity to answer this excellent question . Note that our approach does not fall under any existing framework including binary and ternary neural networks learned via weight-optimization . One could claim that our setting is more restrictive than even binary neural networks/BNN ( let alone ternary neural networks ) as we learn by pruning \u201c randomly weighted \u201d binary neural network . In other words , unlike existing BNN approaches , we do not optimize over weights and learn purely by pruning \u2013 pruning mask is the only optimizable parameter . As we show in the paper , our approach can match ( or beat ) even full precision neural networks trained using weight-optimization for all considered cases except MPT-1/1 on ImageNet . Thus , they match ( or beat ) any M-ary quantization including ternary neural networks . The reviewer is correct in pointing out that weights in the original/large structure can be seen as taking ternary values as pruned weights can be consider having the value \u2018 0 \u2019 . But note the activations still take binary values not ternary in MPT-1/1 . We differ from ternary weight networks ( TWNs ) [ 1 ] due to the following reasons : 1 ) In addition to unpruned weights in MPTs ( i.e. , { +1 , -1 } ) not being trained , we allow a precise control over the sparsity , i.e. , number of weights taking the value \u2018 0 \u2019 , and 2 ) As our goal is pruning , our solutions are significantly sparser than TWNs \u2013 TWNs achieve their best solutions with sparsity < 50 % ( usually 20 % -40 % ) , and our solutions are always with sparsity > 50 % ( usually 60 % -95 % ) . This enables higher energy efficiency and more memory saving ( sometimes even when compared to BNNs ) . Informally , we could say that ( a ) in terms of memory and compute requirements : MPT $ \\leq $ BNN < TNN < FPN , and ( b ) in terms of performance/accuracy : MPT $ \\geq $ FPN > TNN > BNN ( for MPT-1/32 ) . We would further like to point you to our joint response to all the reviewers to highlight novelty , revised results , and most importantly usefulness and implications of our approach on BNNs and DNNs in general . We hope that the reviewer would agree that the comparison is fair , and our results are much stronger than initially perceived . We would be happy to clarify any further questions/concerns the reviewer might have . [ 1 ] Ternary weight networks . arXiv:1605.04711 ( 2016 ) . [ 2 ] SparseRT : Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference . https : //arxiv.org/abs/2008.11849 [ 3 ] Sparsely-connected neural networks : towards efficient vlsi implementation of deep neural networks . arXiv:1611.01427 ( 2016 ) ."}, "2": {"review_id": "U_mat0b9iv-2", "review_text": "The paper proposes an innovate method based on lottery ticket hypothesis to prune a BNN ( parameters are only -1 ( 0 ) and +1 , it can be viewed as an extreme case of quantization ) from a dense NN . It focuses on learning a mask to prune the NN instead of the traditional method ( pruning on an already trained network ) . In addition , not only experiments but theortical proof are given and have a highly brief result . Pro : The way to find the mask iteratively is innovate and has a mathematical support . The result of MPT is amazing because the untrained network can be pruned to a BNN with comparable accrancy of some trained SOTA NN on CIFAR dataset . The experiments show it can be generized to deeper and wider network . It has better accurancy than other BNN methods but network parameters still high . Con : The main article spends little word to describe how to find the mask , and it is not a trivial way . The experiments of generization are only done on the very small NN ( e.g.Conv2/4/6/8 ) . Clarity : Very low . Pros : The authors try to express in a way that every step of logical connections in this paper can be clearly understood by readers . To reduce complexity , many parts are settled in the appendix . Cons : Many sentences in this paper are quite long and sometimes using nesting clauses , which makes the text to be obscure . Besides , since many parts are moved into the appendix , the whole structure of the main body is kind of empty and shallow . Some summative and conclusive paragraphs are the simple repetition of previous \u201c claims \u201d since the demonstrations are in the appendix . All of these make a lower clarity . Finally , I find that this paper has narrower page margins , which means each line can contain more characters . Besides , the header \u201c Under review as a conference paper at \u2026 \u201d is missing . These modifications of the submit template may volatile the conference rule and should be considered a cheating behavior . So I give the \u201c very low \u201d score on clarity . Originality : Medium . Pros : They apply the Lottery Ticket Hypothesis to quantization/BNN . They give proof of their rationality . Cons : The success essentials of their algorithm \u201c biprop \u201d contains two-part : edge-popup and gradient estimator . Both of them are take-away from other works . I regard this work as a new application of LTH to binary neural networks .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q.Finally , I find that this paper has narrower page margins , which means each line can contain more characters . Besides , the header \u201c Under review as a conference paper at \u2026 \u201d is missing . These modifications of the submit template may volatile the conference rule and should be considered a cheating behavior . A.We thank the reviewer for pointing this out . These changes were not intentional but caused by mistakenly using the \u201c fullpage \u201d package with the ICLR 21 official latex template and that caused this change . This happened when transferring our draft and used packages ( which included the \u201c fullpage \u201d package ) into the .tex file with the official ICLR 21 latex template . Further we want to highlight that this change has not given us any unfair advantage \u2013 for your reference we have uploaded a revised version with the \u201c fullpage \u201d package removed showing that the same information exists in the version with and without this issue . For these reasons , we hope that the reviewer will not take this issue into account in his/her revised score . We will respond to your other concerns very soon . Thank you !"}, "3": {"review_id": "U_mat0b9iv-3", "review_text": "The authors propose a stronger lottery ticket hypothesis in this paper \u2013 the multi-prize lottery ticket hypothesis . In particular , the new hypothesis seeks answer to the required amount of over-parameterization for a randomly initialized network to become able to compress to a sparse untrained binary subnetwork with on-par accuracy . The authors prove the existence of such subnetwork and show the bounds on over-parameterization . The paper proposes new methods to get the binary-weight tickets and the binary-activation tickets , where binary-weight tickets are subnetworks with weights as binary , and binary-activation tickets have the activation function in the forward propagation as binary . As binary networks can largely reduce the computational complexity for inference , this work has practical importance especially for applications with constraints for memory and power . The paper has many simulation results to support the theoretical guarantees , and the proposed approach on binary-weight networks has advantages over existing methods . The paper has sufficient and novel contributions , both for the theoretical results on binary subnetworks and the empirical evaluations that reveal the efficiency of the algorithm on binary-weight subnetworks , so that I recommend this paper for publication . Here are some minor concerns . [ a ] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1 . Compared with Malach 2020 \u201c Proving the lottery ticket hypothesis : pruning is all you need \u201d , the authors can highlight what key differences are needed for proof of the binary network . [ b ] For the comparison of full precision network and the MPT from this paper , it can be useful if the computational complexity is shown , where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network . [ c ] The MPT 1/1 seems not to perform as well compared to trained binary activation network , which may reduce the quality of paper . It is good that the authors mention future work for MPT 1/1 network to attain state-of-art results of binary activation network . [ d ] Some of the references seem to lack information , e.g.Malach 2020 , Orseau 2020 , both of which do not have the venue or journal names . [ e ] A question : for the binary weights subnetworks , what does it mean by 20 million parameters ? The weights can only be -1 or 1 , so the network has 20 million of -1 or 1 , but they do not require any multiplication at the inference stage ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive review ! > [ a ] It would be more convincing if the authors could further highlight the technical novelty of this work for the proof of Theorem 1 . Compared with Malach 2020 \u201c Proving the lottery ticket hypothesis : pruning is all you need \u201d , the authors can highlight what key differences are needed for proof of the binary network . To the best of our knowledge ours is the first theoretical result proving that pruning a randomly initialized binary-weight DNN can approximate a real-valued target DNN . As it has been established that real-valued DNNs are universal approximators ( Scarselli & Tsoi , 1998 ) , our result carries the implication that pruned binary-weight DNNs are universal approximators as well . In relation to the first result establishing the existence of real-valued subnetworks in a randomly weighted DNN approximating a real-valued target DNN ( Malach et al. , 2020 ) , the lower bound on the width established in Theorem 2 is better than their lower bound of $ O\\left ( \\ell^2 n^2 \\log ( \\ell n / \\delta ) /\\varepsilon^2 \\right ) $ . While the methodology of our proof is similar to ( Malach et al. , 2020 ) , proving our result for binary-weight subnetworks is not a trivial extension of their technique and require different tools . The use of binary weights in the analysis results in more complex scenarios that have to be addressed carefully . In particular , the analysis in Lemma 1 requires cleverly splitting a complex scenario involving a multinomial distribution into two simpler cases involving binomial distributions . Then , Hoeffding 's inequality is used to derive probabilistic bounds for in each case . The results are combined to yield the final probabilistic bound . This analysis is more complex than the analysis required for real-valued subnetworks as Hoeffding 's inequality was not required to establish an analogous result for real-valued subnetworks ( Malach et al. , 2020 ) . Additionally , the use of binary weights required us to identify an appropriate gain term for rescaling the binary weights in our analysis . Binary weights also result in potentially larger subnetworks being required to approximate a target network and we must carefully keep track of these requirements as we establish results for increasingly larger networks in Lemma 2 , Lemma 3 , and Theorem 2 in Appendix B . In the work on real-valued networks ( Malach et al. , 2020 ) the bound on the required number of parameters in the subnetwork is more straightforward . Furthermore , our analysis yields a tighter bound on the width of the binary network than the bound established in ( Malach et al. , 2020 ) for real-valued subnetworks . We hope that this insight into some of the complexity required when analyzing binary subnetworks in our paper illustrates how the analysis is more complex when working with binary subnetworks . > [ b ] For the comparison of full precision network and the MPT from this paper , it can be useful if the computational complexity is shown , where the complexity for MPT would be for the algorithm to find the mutli-prize tickets in an over-parameterized network . For both MPTs and FP weight-trained baselines , we used the same number of epochs as mentioned in the Appendix A.1 . Theoretically , MPTs can be trained more efficiently than their FP counterparts . However , achieving this would require their implementation on specialized hardwares , e.g. , FPGA , and , this is out of the scope of our paper ."}}