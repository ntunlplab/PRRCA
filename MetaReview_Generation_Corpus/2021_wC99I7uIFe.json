{"year": "2021", "forum": "wC99I7uIFe", "title": "D2p-fed:Differentially Private Federated Learning with Efficient Communication", "decision": "Reject", "meta_review": "The paper considers differentially private federated learning --- a well-motivated problem. The proposed algorithm is a simple modification to existing methods, e.g., DP-FedAvg, but uses a different DP mechanism for noise-adding.  The reviewers liked the motivation but criticized the work for its incremental nature and for somewhat overselling the contributions.\n\nPros:\n\n- The paper used advanced Renyi DP accounting to get a stronger privacy-utility tradeoff.\n\n- The experimental results improve over cp-sgd that uses Binomial mechanisms\n\nCons: \n\n- It is a bit incremental in its contribution.  The main contribution is to applying \"discrete Gaussian mechanism\" to the federated learning problem for the interest of reducing the communication cost.   Discrete Gaussian mechanism and its RDP analysis are both from existing work.\n\n- The improvement in privacy-utility tradeoff over cp-sgd seems to be due to that the discrete Gaussian mechanism has an RDP bound, which plugs right into the subsampling bound and moments accountant.    It is unclear whether the improvement is coming from the different noise or a stronger privacy accounting.  Notice that the privacy accounting of Binomial mechanism in the initial cp-sgd paper was rather crude, thus a fair comparison would be to also conduct an RDP analysis for the Binomial mechanism.  \n\nOverall, there weren't sufficient support among the reviewers and the experimental results alone are not so groundbreakingly strong to carry the paper single-handedly.\n", "reviews": [{"review_id": "wC99I7uIFe-0", "review_text": "The authors propose D2P-FED to achieve differential privacy and communication efficiency in FL tasks . The framework is interesting and the problem studied is important . However , this paper suffers from some major deficiencies . Major comments : 1 ) It is claimed that the privacy guarantee for D2P-FED is better than that of cpSGD . However , this is not substantiated after the privacy results of Theorem 5 . Furthermore , the bound here depends on D , the clipping threshold on the l_2 norm of g , which clearly affects the privacy guarantee per Theorem 5 . 2 ) The algorithm here depends on the lattice being defined as 2g^ { max } / ( k-1 ) Z . It is not clear how one can set the g^ { max } in practice . The differences of the parameters w_t are generated on the fly for each client . Thus , this algorithm has limited applicability in practice . 3 ) Similarly to point 1 ) , it is not clear in what way the communication cost is better than that of cpSGD . From the convergence result in Theorem 7 , it seems like there is no improvement . Further , g^ { max } needs to be set in a very specific way that is dependent on D and d , which is again not known in practice . So whether we can attain the convergence rate is unclear . 4 ) The authors acknowledge in their experiments that the results are sensitive to the scale of the discrete Gaussian noise . Hence , the privacy-communication cost-convergence rate tradeoff is not clearly delineated . Minor comments : 1 ) The writing is not sufficiently precise and clear . For example , before the statement of Thm 1 , it is mentioend that the proof is delayed to Appendix A . However , the proof of Thm 1 is not provided therein . Rather the proof of Thm 3 is in that appendix . The proof of Thm 4 in Appendix B is not sufficiently precise . It is not clear what the authors mean by `` expand the space a little bit '' . How much is `` a little bit '' ? Since this is a `` theorem '' , the proof should be watertight .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for the constructive and detailed comments . Please see below for our response . Q : It is claimed that the privacy guarantee for D2P-FED is better than that of cpSGD . However , this is not substantiated after the privacy results of Theorem 5 ( Corollary 3 in the latest version ) . Furthermore , the bound here depends on $ D $ , the clipping threshold on the l_2 norm of g , which clearly affects the privacy guarantee per Theorem 5 ( Corollary 3 in the latest version ) . A : Thanks for the review but there seems to be a misunderstanding about the statement due to our insufficient elaboration . First , almost all DP mechanisms for non-convex optimization are based on gradient clipping and further add noise proportional to gradient norm bound . The privacy guarantee of such mechanisms can not get away with a dependence on the gradient norm bound $ D $ . For instance , in the groundbreaking work [ 1 ] and [ 2 ] , the privacy bounds also implicitly linearly depend on $ D $ ( Theorem 1 in [ 1 ] and Corollary 3 in [ 2 ] ) . $ D $ does not explicitly show in the bounds because they assume that $ D=1 $ ( Lemma 3 in [ 1 ] and Corollary 3 in [ 2 ] ) . Furthermore , cpSGD \u2019 s privacy guarantee also depends linearly on $ D $ ( Theorem 1 and Equation ( 11 ) ) . Therefore , we do not consider the dependence on $ D $ a defect of our approach . Second , the claim that D2P-FED has a better privacy guarantee than cpSGD can be mainly justified by the following four aspects : ( 1 ) D2P-FED can achieve a stronger privacy notion than cpSGD . Specifically , D2P-FED can achieve RDP yet cpSGD can only guarantee ( $ \\epsilon , \\delta $ ) -DP and it is proved in [ 2 ] that RDP is a strictly stronger notion than ( $ \\epsilon , \\delta $ ) -DP . ( $ \\epsilon , \\delta $ ) -DP allows a small probability ( on the order of $ \\delta $ ) of complete privacy failure . For example , in cpSGD , given two neighboring datasets ( e.g. $ D_1 $ and $ D_2 $ ) differing in one entry , if the outputs of the learning algorithm on the two datasets differ by $ 1 $ ( e.g. $ 0 $ , $ 1 $ ) , then after adding noise drawn from the same binomial distribution ( e.g. $ Binom ( m , p ) $ ) , the supports of outputs also differ by $ 1 $ ( e.g . { $ 0 , \\cdots , m $ } , { $ 1 , \\cdots , m+1 $ } ) . Therefore , if the output is $ 0 $ , we can immediately tell that the input is $ D_1 $ . On the other hand , as demonstrated in [ 2 ] , RDP \u2019 s privacy guarantee \u201c degrades gracefully , such as to $ 1 $ -DP with probability $ \\frac { \\delta } { 2 } $ , to $ 2 $ -DP with probability $ \\frac { \\delta } { 4 } $ , etc \u201d . ( 2 ) D2P-FED enjoys a tighter composition compared to cpSGD . The best way so far to derive the total privacy budget for multiple rounds for cpSGD is to use advanced composition theorem , as cpSGD follows ( $ \\epsilon , \\delta $ ) -DP . On the other hand , since D2P-FED follows RDP , we can leverage analytical moments accountant [ 3 ] to calculate the total privacy budget . As shown in [ 3 ] , advanced composition in general produces much looser composition than analytical moments accountant . The reason is that if we use advanced composition for multiple-round composition , the total privacy budget has a complex dependency on the per-round privacy budget and $ \\delta $ ; hence , it is often computationally hard to find the optimal $ \\epsilon $ given a fixed $ \\delta $ . By contrast , by tracking the cumulant generating function of the composed mechanisms symbolically , analytical moments accountant allows a convenient conversion from RDP bound to ( $ \\epsilon , \\delta $ ) -DP with the smallest $ \\epsilon $ given fixed $ \\delta $ . ( 3 ) Our experimental results in Figure 1 also empirically show that D2P-FED enjoys a tighter composition than cpSGD . The total privacy budget for D2P-FED grows much more slowly than cpSGD as training proceeds . ( 4 ) Moreover , according to Figure 1 in [ 4 ] , even with the same noise scale , Gaussian noise provides stronger privacy guarantee than Binomial noise . As discrete Gaussian noise follows the same RDP bound as Gaussian noise , we believe discrete Gaussian can map the same-scale noise to lower privacy cost . We have added more details after Corollary 3 about the privacy guarantee comparison between D2P-FED and cpSGD ."}, {"review_id": "wC99I7uIFe-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper proposes the discrete Gaussian based differentially private federated learning algorithm to achieve both differential privacy and communication efficiency in federated learning . In particular , it adds discrete Gaussian noise into client updates and uses secure aggregation to prevent the server from observing the individual updates . The algorithm satisfies RDP and has lower communication cost compared to the previous method cpSGD . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I like this work . 1.The problem is critical in federated learning . The D2P-Fed algorithm has nice performance on the trade-off among privacy , utility , and communication cost . 2.It has several algorithmic novelties . To employ secure aggregate , it 's natural to use a mechanism with discrete support ( or discretize that ) . The discrete Gaussian mechanism is indeed a better candidate than the binomial mechanism . But there is much more beyond a simple combination of the discrete Gaussian mechanism and secure aggregation . They use several other techniques , such as stochastic quantization and random rotation . The mapping to a cyclic additive group is novel . 3.They provide strong theoretical guarantees and empirical validation . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor comments : 1 . In Section 3 under FL Overview , I do n't think $ k $ is clearly defined . I guess $ k=\\gamma n $ . It is also confusing in Algorithm 1 . It should be clearly defined . The same for $ d $ in Algorithm 1 . 2.The probability $ g [ j ] -b [ r ] /b [ r+1 ] -b [ r ] $ does n't seem make sense . Like if $ g [ j ] =b [ r+1 ] $ , it will be set to $ b [ r ] $ with probability 1 . Should it be $ 1- ( g [ j ] -b [ r ] /b [ r+1 ] -b [ r ] ) $ ? 3.In Figure 1 ( b ) , the x-axis should be communication cost for CIFAR10 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive feedback on our paper ! We are glad that you like the paper and find your comments really helpful . Please see below for our response . Q : In Section 3 under FL Overview , I do n't think $ k $ is clearly defined . I guess $ k=\\gamma n $ . It is also confusing in Algorithm 1 . It should be clearly defined . The same for $ d $ in Algorithm 1 . The probability $ \\frac { g [ j ] \u2212b [ r ] } { b [ r+1 ] \u2212b [ r ] } $ does n't seem make sense . Like if $ g [ j ] =b [ r+1 ] $ , it will be set to $ b [ r ] $ with probability $ 1 $ . Should it be $ 1\u2212\\frac { g [ j ] \u2212b [ r ] } { b [ r+1 ] \u2212b [ r ] } $ ? In Figure 1 ( b ) , the x-axis should be communication cost for CIFAR10 . A : Thanks for pointing this out . We have corrected the notation problems and the typos in Algorithm 1 in the latest revision . We would appreciate it if the reviewer has further suggestions ."}, {"review_id": "wC99I7uIFe-2", "review_text": "This paper proposed a privacy-preserving federated learning algorithm with efficient communication . To save communication costs , they integrate stochastic quantization and random rotation . Moreover , they apply the discrete Gaussian mechanism with Renyi DP analysis for privacy guarantee . Novelty & Significance -- Solving federated learning with a privacy guarantee is of increasing practical importance , and certainly trying to do so with efficient communication efficiency is more important . The RDP analysis for discrete Gaussian mechanism is nice , which allows a tighter composition with analytical moments accountants . Technical concern : -- I have concerns regarding the claim `` D2P-FED is more communication efficient compared to cpSGD '' due to a $ log ( 1/\\delta ) $ factor in big O . Note that constant matters in differential privacy . Without knowing other constants hidden in big O , I feel hard to follow the conclusion there . Experimental Evaluation : - My major concern is that empirical evaluation is limited . ( 1 ) In Figure 1 , three methods use different quantization . There lacks clarification on the choice of bytes . Moreover , it would be much better to add a non-private baseline without quantization . ( 2 ) Noting the privacy budget greater than 10 is no longer meaningful ; it may be better to focus on the practical privacy regimes in Figure 1 ( a ) . ( 3 ) In Figure 1 ( b ) , is the privacy budget aligned ? If not , how to set the noise scale for cpSGD and D2P-FED ? Is it possible to align both the privacy budget and communication cost ? For example , the per-round communication is the same for both algorithms with the same bytes . Set the x-axis to be the communication cost , the privacy cost is fixed for all points , and the y-axis reports the model accuracy . ( 4 ) INFIMNIST and CIFAR10 are still two toy datasets . Perhaps authors can add more experiments with real-world datasets in the next version .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the helpful comments . Please see below for our response . Q : I have concerns regarding the claim `` D2P-FED is more communication efficient compared to cpSGD '' due to a $ log ( 1/\\delta ) $ factor in big $ \\mathcal { O } $ . Note that constant matters in differential privacy . Without knowing other constants hidden in big $ \\mathcal { O } $ , I feel hard to follow the conclusion there . A : Thanks for the question ! We totally agree that constant matters in DP . The $ log ( 1/\\delta ) $ factor indicates the communication efficiency of D2P-FED in theory but can not serve as a determining evidence . However , tight constant analysis is extremely hard ( even impossible ) in most cases . Therefore we choose to demonstrate the communication efficiency using empirical evaluation just like what most relevant works do . We believe that Figure 1 ( b ) empirically illustrates the communication efficiency of D2P-FED over cpSGD ."}, {"review_id": "wC99I7uIFe-3", "review_text": "This paper proposes to use a discrete gaussian distribution the generate noise for differential privacy . While it is claimed that the discrete Gaussian distribution is better than the binomial distribution , no in-depth comparison is made . The discrete Gaussian distribution , a discretized version of the Gaussian distribution , works as expected , and provides all properties the Gaussian distribution provides ( if the discretization is sufficiently fine-grained ) . An advantage is that the communication cost can be reduced by stochastic quantization of the values to be transmitted . It is unclear whether the same stochastic quantization technique can be used for the classic Gaussian distribution . The ideas of discretizing , of stochastic quantization to reduce communication cost , and the use of discrete distributions for differential privacy are all already known , the ( limited ) novelty is in the proper combination of these ideas . The text is understandable for readers who know already the meaning of all symbols and names used , but a broader audience could be reached by properly introducing all concepts and notations in an understandable , coherent and self-contained way . This would be valuable , even if it would mean that some more technical elements would need to be moved to an appendix . In conclusion , while the idea is interesting , I have some concerns with the presentation and the insufficiently motivated novelty ( and the related lack of more in-depth comparison with existing work ) . * The paper points to a few shortcomings in the literature , but exaggerates their importance . E.g. , when the text says `` However , cpSGD faces several limitations when applied to real-world applications . Firstly , with Binomial noise , the output of a learning algorithm would have different supports when any client participates or withdraws from learning '' , it may be true this is the case when literally considering the cpSGD paper , but such problem typically can be overcome easily . When the text concludes `` can only guarantee approximate DP where the participation of the client can be completely exposed with nonzero probability . '' I guess `` approximate DP '' means ( epsilon , delta ) -differential privacy where delta is strictly larger than zero ( which is usually anyway already the case for Gaussian and binomial mechanisms ) . * Definition 3 : I wonder why the numerator and denominator of the exponent in the probability e^ { -\\pi x^2/ ( 2\\pi\\sigma^2 ) } both contain \\pi and this fraction is n't simplified . * Definition 3 : please clarify what kind of `` discrete additive subgroup '' you have in mind : is it a finite group ( where addition is possibly modulo the group order ) or is it a multiple of the set of integers \\mathbb { Z } ? ( much later in the paper , it becomes increasingly clear that the latter option is the correct one ) * Section 5.1 : please explain the meaning of all used variables at their first use . E.g. , the k in an expression log ( k ) is n't explained , until Section 6.1 cites another work for the meaning of k-level quantization ( without even explaining the gist of quantization idea briefly ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive comments . Please see below for our response . Q : The paper points to a few shortcomings in the literature , but exaggerates their importance . E.g. , when the text says `` However , cpSGD faces several limitations when applied to real-world applications . Firstly , with Binomial noise , the output of a learning algorithm would have different supports when any client participates or withdraws from learning '' , it may be true this is the case when literally considering the cpSGD paper , but such problem typically can be overcome easily . When the text concludes `` can only guarantee approximate DP where the participation of the client can be completely exposed with nonzero probability . '' I guess `` approximate DP '' means ( $ \\epsilon , \\delta $ ) -differential privacy where delta is strictly larger than zero ( which is usually anyway already the case for Gaussian and binomial mechanisms ) . A : Thanks for the review but there seems to be a misunderstanding about the statement due to our insufficient elaboration . We did not intend to use the two cited sentences ( regarding changing support and approximate DP ( aka $ \\epsilon , \\delta $ ) -DP ) for stating two separate problems of cpSGD . Indeed , the problem of cpSGD lies in the second sentence only ( i.e. , approximate DP ) and the first sentence ( i.e. , changing support ) is for specifying the cause of the problem . We \u2019 d like to first clarify that the Gaussian mechanism mentioned by the reviewer can indeed achieve a stronger privacy than approximate DP , which is RDP . So naturally , one would ask : Can RDP be achieved for federated learning with constrained communication ? Actually , cp-SGD can not get away with approximate DP in its original form because of the limited support of Binomial noise ( we will provide an example to illustrate the intuition for this later ) . Besides , we want to emphasize that the problem of changing supports is * * not easily solvable * * . We agree with the reviewer that there are several potential methods to address the issue , like clipping the noised value into a fixed support . However , these methods will introduce extra sophistication into the privacy analysis and to our best knowledge , there has not been any work that is able to prove that simple renormalization into the fixed support would lead to stronger privacy than approximate DP . To help understand why the mechanism with varying support for different input can only satisfy approximate DP , we provide the following concrete example : Assume we have two neighboring datasets $ D_1 $ and $ D_2 $ differing in only one row . Given a learning algorithm , the update vector ( for clarity we assume this is one-dimensional ) for $ D_1 $ is $ 0 $ while the update vector for $ D_2 $ is $ 1 $ . Then we add binomial noise $ Binom ( m , p ) $ to the two outputs ( for clarity we assume we directly add the noise without any shifting and zooming ) . Thus , the output support for $ D_1 $ is $ { 0 , 1 , \\cdots , m } $ while the support for $ D_2 $ is $ \\ { 1 , 2 , \\cdots , m+1\\ } $ . If the output is $ 0 $ , then we immediately know that the input is $ D_1 $ not $ D_2 $ . If we think about the above limitation theoretically , this actually means that the denominator in the pure-DP definition $ \\epsilon=\\log\\frac { Pr [ A ( D1 ) \\in S ] } { Pr [ A ( D2 ) \\in S ] } $ can be zero and thus breaks any finite $ \\epsilon $ -DP guarantee with a small probability . Therefore , Binomial mechanism is intrinsically restricted to approximate DP ( ( $ \\epsilon , \\delta $ ) -DP ) which allows a small ( but not negligible ) probability of complete privacy failure and can not follow stronger definitions such as RDP . We have rewritten this part to make it more clear in the latest revision . We would appreciate it if the reviewer has further suggestions . Q : Definition 3 : I wonder why the numerator and denominator of the exponent in the probability $ e^ { -\\pi x^2/ ( 2\\pi\\sigma^2 ) } $ both contain $ \\pi $ and this fraction is n't simplified . A : Thanks for pointing this out . We have simplified it in the newest revision . Q : Definition 3 : please clarify what kind of `` discrete additive subgroup '' you have in mind : is it a finite group ( where addition is possibly modulo the group order ) or is it a multiple of the set of integers $ \\mathbb { Z } $ ? ( much later in the paper , it becomes increasingly clear that the latter option is the correct one ) A : In Definition 3 , \u201c discrete additive group means \u201d means the latter one . We have changed the wording in definition 3 to make it more clear . Q : Section 5.1 : please explain the meaning of all used variables at their first use . E.g. , the $ k $ in an expression $ \\log ( k ) $ is n't explained , until Section 6.1 cites another work for the meaning of $ k $ -level quantization ( without even explaining the gist of quantization idea briefly ) . A : Thanks for pointing this out . We have added explanation for the notations in the latest revision ."}], "0": {"review_id": "wC99I7uIFe-0", "review_text": "The authors propose D2P-FED to achieve differential privacy and communication efficiency in FL tasks . The framework is interesting and the problem studied is important . However , this paper suffers from some major deficiencies . Major comments : 1 ) It is claimed that the privacy guarantee for D2P-FED is better than that of cpSGD . However , this is not substantiated after the privacy results of Theorem 5 . Furthermore , the bound here depends on D , the clipping threshold on the l_2 norm of g , which clearly affects the privacy guarantee per Theorem 5 . 2 ) The algorithm here depends on the lattice being defined as 2g^ { max } / ( k-1 ) Z . It is not clear how one can set the g^ { max } in practice . The differences of the parameters w_t are generated on the fly for each client . Thus , this algorithm has limited applicability in practice . 3 ) Similarly to point 1 ) , it is not clear in what way the communication cost is better than that of cpSGD . From the convergence result in Theorem 7 , it seems like there is no improvement . Further , g^ { max } needs to be set in a very specific way that is dependent on D and d , which is again not known in practice . So whether we can attain the convergence rate is unclear . 4 ) The authors acknowledge in their experiments that the results are sensitive to the scale of the discrete Gaussian noise . Hence , the privacy-communication cost-convergence rate tradeoff is not clearly delineated . Minor comments : 1 ) The writing is not sufficiently precise and clear . For example , before the statement of Thm 1 , it is mentioend that the proof is delayed to Appendix A . However , the proof of Thm 1 is not provided therein . Rather the proof of Thm 3 is in that appendix . The proof of Thm 4 in Appendix B is not sufficiently precise . It is not clear what the authors mean by `` expand the space a little bit '' . How much is `` a little bit '' ? Since this is a `` theorem '' , the proof should be watertight .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for the constructive and detailed comments . Please see below for our response . Q : It is claimed that the privacy guarantee for D2P-FED is better than that of cpSGD . However , this is not substantiated after the privacy results of Theorem 5 ( Corollary 3 in the latest version ) . Furthermore , the bound here depends on $ D $ , the clipping threshold on the l_2 norm of g , which clearly affects the privacy guarantee per Theorem 5 ( Corollary 3 in the latest version ) . A : Thanks for the review but there seems to be a misunderstanding about the statement due to our insufficient elaboration . First , almost all DP mechanisms for non-convex optimization are based on gradient clipping and further add noise proportional to gradient norm bound . The privacy guarantee of such mechanisms can not get away with a dependence on the gradient norm bound $ D $ . For instance , in the groundbreaking work [ 1 ] and [ 2 ] , the privacy bounds also implicitly linearly depend on $ D $ ( Theorem 1 in [ 1 ] and Corollary 3 in [ 2 ] ) . $ D $ does not explicitly show in the bounds because they assume that $ D=1 $ ( Lemma 3 in [ 1 ] and Corollary 3 in [ 2 ] ) . Furthermore , cpSGD \u2019 s privacy guarantee also depends linearly on $ D $ ( Theorem 1 and Equation ( 11 ) ) . Therefore , we do not consider the dependence on $ D $ a defect of our approach . Second , the claim that D2P-FED has a better privacy guarantee than cpSGD can be mainly justified by the following four aspects : ( 1 ) D2P-FED can achieve a stronger privacy notion than cpSGD . Specifically , D2P-FED can achieve RDP yet cpSGD can only guarantee ( $ \\epsilon , \\delta $ ) -DP and it is proved in [ 2 ] that RDP is a strictly stronger notion than ( $ \\epsilon , \\delta $ ) -DP . ( $ \\epsilon , \\delta $ ) -DP allows a small probability ( on the order of $ \\delta $ ) of complete privacy failure . For example , in cpSGD , given two neighboring datasets ( e.g. $ D_1 $ and $ D_2 $ ) differing in one entry , if the outputs of the learning algorithm on the two datasets differ by $ 1 $ ( e.g. $ 0 $ , $ 1 $ ) , then after adding noise drawn from the same binomial distribution ( e.g. $ Binom ( m , p ) $ ) , the supports of outputs also differ by $ 1 $ ( e.g . { $ 0 , \\cdots , m $ } , { $ 1 , \\cdots , m+1 $ } ) . Therefore , if the output is $ 0 $ , we can immediately tell that the input is $ D_1 $ . On the other hand , as demonstrated in [ 2 ] , RDP \u2019 s privacy guarantee \u201c degrades gracefully , such as to $ 1 $ -DP with probability $ \\frac { \\delta } { 2 } $ , to $ 2 $ -DP with probability $ \\frac { \\delta } { 4 } $ , etc \u201d . ( 2 ) D2P-FED enjoys a tighter composition compared to cpSGD . The best way so far to derive the total privacy budget for multiple rounds for cpSGD is to use advanced composition theorem , as cpSGD follows ( $ \\epsilon , \\delta $ ) -DP . On the other hand , since D2P-FED follows RDP , we can leverage analytical moments accountant [ 3 ] to calculate the total privacy budget . As shown in [ 3 ] , advanced composition in general produces much looser composition than analytical moments accountant . The reason is that if we use advanced composition for multiple-round composition , the total privacy budget has a complex dependency on the per-round privacy budget and $ \\delta $ ; hence , it is often computationally hard to find the optimal $ \\epsilon $ given a fixed $ \\delta $ . By contrast , by tracking the cumulant generating function of the composed mechanisms symbolically , analytical moments accountant allows a convenient conversion from RDP bound to ( $ \\epsilon , \\delta $ ) -DP with the smallest $ \\epsilon $ given fixed $ \\delta $ . ( 3 ) Our experimental results in Figure 1 also empirically show that D2P-FED enjoys a tighter composition than cpSGD . The total privacy budget for D2P-FED grows much more slowly than cpSGD as training proceeds . ( 4 ) Moreover , according to Figure 1 in [ 4 ] , even with the same noise scale , Gaussian noise provides stronger privacy guarantee than Binomial noise . As discrete Gaussian noise follows the same RDP bound as Gaussian noise , we believe discrete Gaussian can map the same-scale noise to lower privacy cost . We have added more details after Corollary 3 about the privacy guarantee comparison between D2P-FED and cpSGD ."}, "1": {"review_id": "wC99I7uIFe-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper proposes the discrete Gaussian based differentially private federated learning algorithm to achieve both differential privacy and communication efficiency in federated learning . In particular , it adds discrete Gaussian noise into client updates and uses secure aggregation to prevent the server from observing the individual updates . The algorithm satisfies RDP and has lower communication cost compared to the previous method cpSGD . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I like this work . 1.The problem is critical in federated learning . The D2P-Fed algorithm has nice performance on the trade-off among privacy , utility , and communication cost . 2.It has several algorithmic novelties . To employ secure aggregate , it 's natural to use a mechanism with discrete support ( or discretize that ) . The discrete Gaussian mechanism is indeed a better candidate than the binomial mechanism . But there is much more beyond a simple combination of the discrete Gaussian mechanism and secure aggregation . They use several other techniques , such as stochastic quantization and random rotation . The mapping to a cyclic additive group is novel . 3.They provide strong theoretical guarantees and empirical validation . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor comments : 1 . In Section 3 under FL Overview , I do n't think $ k $ is clearly defined . I guess $ k=\\gamma n $ . It is also confusing in Algorithm 1 . It should be clearly defined . The same for $ d $ in Algorithm 1 . 2.The probability $ g [ j ] -b [ r ] /b [ r+1 ] -b [ r ] $ does n't seem make sense . Like if $ g [ j ] =b [ r+1 ] $ , it will be set to $ b [ r ] $ with probability 1 . Should it be $ 1- ( g [ j ] -b [ r ] /b [ r+1 ] -b [ r ] ) $ ? 3.In Figure 1 ( b ) , the x-axis should be communication cost for CIFAR10 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive feedback on our paper ! We are glad that you like the paper and find your comments really helpful . Please see below for our response . Q : In Section 3 under FL Overview , I do n't think $ k $ is clearly defined . I guess $ k=\\gamma n $ . It is also confusing in Algorithm 1 . It should be clearly defined . The same for $ d $ in Algorithm 1 . The probability $ \\frac { g [ j ] \u2212b [ r ] } { b [ r+1 ] \u2212b [ r ] } $ does n't seem make sense . Like if $ g [ j ] =b [ r+1 ] $ , it will be set to $ b [ r ] $ with probability $ 1 $ . Should it be $ 1\u2212\\frac { g [ j ] \u2212b [ r ] } { b [ r+1 ] \u2212b [ r ] } $ ? In Figure 1 ( b ) , the x-axis should be communication cost for CIFAR10 . A : Thanks for pointing this out . We have corrected the notation problems and the typos in Algorithm 1 in the latest revision . We would appreciate it if the reviewer has further suggestions ."}, "2": {"review_id": "wC99I7uIFe-2", "review_text": "This paper proposed a privacy-preserving federated learning algorithm with efficient communication . To save communication costs , they integrate stochastic quantization and random rotation . Moreover , they apply the discrete Gaussian mechanism with Renyi DP analysis for privacy guarantee . Novelty & Significance -- Solving federated learning with a privacy guarantee is of increasing practical importance , and certainly trying to do so with efficient communication efficiency is more important . The RDP analysis for discrete Gaussian mechanism is nice , which allows a tighter composition with analytical moments accountants . Technical concern : -- I have concerns regarding the claim `` D2P-FED is more communication efficient compared to cpSGD '' due to a $ log ( 1/\\delta ) $ factor in big O . Note that constant matters in differential privacy . Without knowing other constants hidden in big O , I feel hard to follow the conclusion there . Experimental Evaluation : - My major concern is that empirical evaluation is limited . ( 1 ) In Figure 1 , three methods use different quantization . There lacks clarification on the choice of bytes . Moreover , it would be much better to add a non-private baseline without quantization . ( 2 ) Noting the privacy budget greater than 10 is no longer meaningful ; it may be better to focus on the practical privacy regimes in Figure 1 ( a ) . ( 3 ) In Figure 1 ( b ) , is the privacy budget aligned ? If not , how to set the noise scale for cpSGD and D2P-FED ? Is it possible to align both the privacy budget and communication cost ? For example , the per-round communication is the same for both algorithms with the same bytes . Set the x-axis to be the communication cost , the privacy cost is fixed for all points , and the y-axis reports the model accuracy . ( 4 ) INFIMNIST and CIFAR10 are still two toy datasets . Perhaps authors can add more experiments with real-world datasets in the next version .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the helpful comments . Please see below for our response . Q : I have concerns regarding the claim `` D2P-FED is more communication efficient compared to cpSGD '' due to a $ log ( 1/\\delta ) $ factor in big $ \\mathcal { O } $ . Note that constant matters in differential privacy . Without knowing other constants hidden in big $ \\mathcal { O } $ , I feel hard to follow the conclusion there . A : Thanks for the question ! We totally agree that constant matters in DP . The $ log ( 1/\\delta ) $ factor indicates the communication efficiency of D2P-FED in theory but can not serve as a determining evidence . However , tight constant analysis is extremely hard ( even impossible ) in most cases . Therefore we choose to demonstrate the communication efficiency using empirical evaluation just like what most relevant works do . We believe that Figure 1 ( b ) empirically illustrates the communication efficiency of D2P-FED over cpSGD ."}, "3": {"review_id": "wC99I7uIFe-3", "review_text": "This paper proposes to use a discrete gaussian distribution the generate noise for differential privacy . While it is claimed that the discrete Gaussian distribution is better than the binomial distribution , no in-depth comparison is made . The discrete Gaussian distribution , a discretized version of the Gaussian distribution , works as expected , and provides all properties the Gaussian distribution provides ( if the discretization is sufficiently fine-grained ) . An advantage is that the communication cost can be reduced by stochastic quantization of the values to be transmitted . It is unclear whether the same stochastic quantization technique can be used for the classic Gaussian distribution . The ideas of discretizing , of stochastic quantization to reduce communication cost , and the use of discrete distributions for differential privacy are all already known , the ( limited ) novelty is in the proper combination of these ideas . The text is understandable for readers who know already the meaning of all symbols and names used , but a broader audience could be reached by properly introducing all concepts and notations in an understandable , coherent and self-contained way . This would be valuable , even if it would mean that some more technical elements would need to be moved to an appendix . In conclusion , while the idea is interesting , I have some concerns with the presentation and the insufficiently motivated novelty ( and the related lack of more in-depth comparison with existing work ) . * The paper points to a few shortcomings in the literature , but exaggerates their importance . E.g. , when the text says `` However , cpSGD faces several limitations when applied to real-world applications . Firstly , with Binomial noise , the output of a learning algorithm would have different supports when any client participates or withdraws from learning '' , it may be true this is the case when literally considering the cpSGD paper , but such problem typically can be overcome easily . When the text concludes `` can only guarantee approximate DP where the participation of the client can be completely exposed with nonzero probability . '' I guess `` approximate DP '' means ( epsilon , delta ) -differential privacy where delta is strictly larger than zero ( which is usually anyway already the case for Gaussian and binomial mechanisms ) . * Definition 3 : I wonder why the numerator and denominator of the exponent in the probability e^ { -\\pi x^2/ ( 2\\pi\\sigma^2 ) } both contain \\pi and this fraction is n't simplified . * Definition 3 : please clarify what kind of `` discrete additive subgroup '' you have in mind : is it a finite group ( where addition is possibly modulo the group order ) or is it a multiple of the set of integers \\mathbb { Z } ? ( much later in the paper , it becomes increasingly clear that the latter option is the correct one ) * Section 5.1 : please explain the meaning of all used variables at their first use . E.g. , the k in an expression log ( k ) is n't explained , until Section 6.1 cites another work for the meaning of k-level quantization ( without even explaining the gist of quantization idea briefly ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive comments . Please see below for our response . Q : The paper points to a few shortcomings in the literature , but exaggerates their importance . E.g. , when the text says `` However , cpSGD faces several limitations when applied to real-world applications . Firstly , with Binomial noise , the output of a learning algorithm would have different supports when any client participates or withdraws from learning '' , it may be true this is the case when literally considering the cpSGD paper , but such problem typically can be overcome easily . When the text concludes `` can only guarantee approximate DP where the participation of the client can be completely exposed with nonzero probability . '' I guess `` approximate DP '' means ( $ \\epsilon , \\delta $ ) -differential privacy where delta is strictly larger than zero ( which is usually anyway already the case for Gaussian and binomial mechanisms ) . A : Thanks for the review but there seems to be a misunderstanding about the statement due to our insufficient elaboration . We did not intend to use the two cited sentences ( regarding changing support and approximate DP ( aka $ \\epsilon , \\delta $ ) -DP ) for stating two separate problems of cpSGD . Indeed , the problem of cpSGD lies in the second sentence only ( i.e. , approximate DP ) and the first sentence ( i.e. , changing support ) is for specifying the cause of the problem . We \u2019 d like to first clarify that the Gaussian mechanism mentioned by the reviewer can indeed achieve a stronger privacy than approximate DP , which is RDP . So naturally , one would ask : Can RDP be achieved for federated learning with constrained communication ? Actually , cp-SGD can not get away with approximate DP in its original form because of the limited support of Binomial noise ( we will provide an example to illustrate the intuition for this later ) . Besides , we want to emphasize that the problem of changing supports is * * not easily solvable * * . We agree with the reviewer that there are several potential methods to address the issue , like clipping the noised value into a fixed support . However , these methods will introduce extra sophistication into the privacy analysis and to our best knowledge , there has not been any work that is able to prove that simple renormalization into the fixed support would lead to stronger privacy than approximate DP . To help understand why the mechanism with varying support for different input can only satisfy approximate DP , we provide the following concrete example : Assume we have two neighboring datasets $ D_1 $ and $ D_2 $ differing in only one row . Given a learning algorithm , the update vector ( for clarity we assume this is one-dimensional ) for $ D_1 $ is $ 0 $ while the update vector for $ D_2 $ is $ 1 $ . Then we add binomial noise $ Binom ( m , p ) $ to the two outputs ( for clarity we assume we directly add the noise without any shifting and zooming ) . Thus , the output support for $ D_1 $ is $ { 0 , 1 , \\cdots , m } $ while the support for $ D_2 $ is $ \\ { 1 , 2 , \\cdots , m+1\\ } $ . If the output is $ 0 $ , then we immediately know that the input is $ D_1 $ not $ D_2 $ . If we think about the above limitation theoretically , this actually means that the denominator in the pure-DP definition $ \\epsilon=\\log\\frac { Pr [ A ( D1 ) \\in S ] } { Pr [ A ( D2 ) \\in S ] } $ can be zero and thus breaks any finite $ \\epsilon $ -DP guarantee with a small probability . Therefore , Binomial mechanism is intrinsically restricted to approximate DP ( ( $ \\epsilon , \\delta $ ) -DP ) which allows a small ( but not negligible ) probability of complete privacy failure and can not follow stronger definitions such as RDP . We have rewritten this part to make it more clear in the latest revision . We would appreciate it if the reviewer has further suggestions . Q : Definition 3 : I wonder why the numerator and denominator of the exponent in the probability $ e^ { -\\pi x^2/ ( 2\\pi\\sigma^2 ) } $ both contain $ \\pi $ and this fraction is n't simplified . A : Thanks for pointing this out . We have simplified it in the newest revision . Q : Definition 3 : please clarify what kind of `` discrete additive subgroup '' you have in mind : is it a finite group ( where addition is possibly modulo the group order ) or is it a multiple of the set of integers $ \\mathbb { Z } $ ? ( much later in the paper , it becomes increasingly clear that the latter option is the correct one ) A : In Definition 3 , \u201c discrete additive group means \u201d means the latter one . We have changed the wording in definition 3 to make it more clear . Q : Section 5.1 : please explain the meaning of all used variables at their first use . E.g. , the $ k $ in an expression $ \\log ( k ) $ is n't explained , until Section 6.1 cites another work for the meaning of $ k $ -level quantization ( without even explaining the gist of quantization idea briefly ) . A : Thanks for pointing this out . We have added explanation for the notations in the latest revision ."}}