{"year": "2018", "forum": "rytstxWAW", "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling", "decision": "Accept (Poster)", "meta_review": "Graph neural networks (incl. GCNs) have been shown effective on a large range of tasks. However, it has been so far hard (i.e. computationally expensive or requiring the use of heuristics) to apply them to large graphs. This paper aims to address this problem and the solution is clean and elegant. The reviewers generally find it well written and interesting. There were some concerns about the comparison to GraphSAGE (an alternative approach), but these have been addressed in a subsequent revision.\n\n+ an important problem\n+ a simple approach\n+ convincing results\n+ clear and well written\n", "reviews": [{"review_id": "rytstxWAW-0", "review_text": "Update: I have read the rebuttal and the revised manuscript. Additionally I had a brief discussion with the authors regarding some aspects of their probabilistic framework. I think that batch training of GCN is an important problem and authors have proposed an interesting solution to this problem. I appreciated all the work authors put into the revision. In this regard, I have updated my rating. However, I am not satisfied with how the probabilistic problem formulation was presented in the paper. I would appreciate if authors were more upfront about the challenges of the problem they formulated and limitations of their results. I briefly summarize the key missing points below, although I acknowledge that solution to such questions is out of scope of this work. 1. Sampling of graph nodes from P is not iid. Every subsequent node can not be equal to any of the previous nodes. Hence, the distribution changes and subsequent nodes are dependent on previous ones. However, exchangeability could be a reasonable assumption to make as order (in the joint distribution) does not matter for simple choices of P. Example: let V be {1,2,3} and P a uniform distribution. First node can be any of the {1,2,3}, second node given first (suppose first node is '2') is restricted to {1,3}. There is clearly a dependency and change of distribution. 2. Theorem 1 is proven under the assumption that it is possible to sample from P and utilize Monte Carlo type argument. However, in practice, sampling is done from a uniform distribution over observed samples. Also, authors suggest that V may be infinite. Recall that for Monte Carlo type approaches to work, sampling distribution is ought to contain support of the true distribution. Observed samples (even as sample size goes to infinity) will never be able to cover an infinite V. Hence, Theorem 1 will never be applicable (for the purposes of evaluating population loss). Also note that this is different from a more classical case of continuous distributions, where sampling from a Gaussian, for instance, will cover any domain of true distribution. In the probabilistic framework defined by the authors it is impossible to cover domain of P, unless whole V is observed. ---------------------------------------------------------------------- This work addresses a major shortcoming of recently popularized GCN. That is, when the data is equipped with the graph structure, classic SGD based methods are not straightforward to apply. Hence it is not clear how to deal with large datasets (e.g., Reddit). Proposed approach uses an adjacency based importance sampling distribution to select only a subset of nodes on each GCN layer. Resulting loss estimate is shown to be consistent and its gradient is used to perform the weight updates. Proposed approach is interesting and the direction of the work is important given recent popularity of the GCN. Nonetheless I have two major question and would be happy to revisit my score if at least one is addressed. Theory: SGD requires an unbiased estimate of the gradient to converge to the global optima in the convex loss case. Here, the loss estimate is shown to be consistent, but not guaranteed to be unbiased and nothing is said about the gradient in Algorithm 1. Could you please provide some intuition about the gradient estimate? I might not be familiar with some relevant results, but it appears to me that Algorithm 1 will not converge to the same solution as full data GD would. Practice: Per batch timings in Fig. 3 are not enough to argue that the method is faster as it might have poor convergence properties overall. Could you please show the train/test accuracies against training time for all compared methods? Some other concerns and questions: - It is not quite cleat what P is. You defined it as distribution over vertices of some (potentially infinite) population graph. Later on, sampling from P becomes equivalent to uniform sampling over the observed nodes. I don't see how you can define P over anything outside of the training nodes (without defining loss on the unobserved data), as then you would be sampling from a distribution with 0 mass on the parts of the support of P, and this would break the Monte Carlo assumptions. - Weights disappeared in the majority of the analysis. Could you please make the representation more consistent. - a(v,u) in Eq. 2 and A(v,u) in Eq. 5 are not defined. Do they both correspond to entries of the (normalized) adjacency?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate very much your critical comments . Please find our responses and summary of revisions in the following . Your reviews are cited with > > > . We hope that the edited version may clear the confusion and you enjoy the paper as other reviewers do : ) > > > Theory : > > > SGD requires an unbiased estimate of the gradient to converge to the global optima in the convex loss case . Here , the loss estimate is shown to be consistent , but not guaranteed to be unbiased and nothing is said about the gradient in Algorithm 1 . Could you please provide some intuition about the gradient estimate ? I might not be familiar with some relevant results , but it appears to me that Algorithm 1 will not converge to the same solution as full data GD would . The consistency of the gradient estimator simply follows that of the loss estimator , if the differential operator is continuous . Hence , the essential question is whether SGD converges if the gradient estimator is consistent but not unbiased . We have developed a convergence theory in the appendix ( see Section D ) for our algorithms . Generally speaking , the convergence rate is the same as the case of unbiased gradient estimator . > > > Practice : > > > Per batch timings in Fig.3 are not enough to argue that the method is faster as it might have poor convergence properties overall . Could you please show the train/test accuracies against training time for all compared methods ? We found that the convergence speed between GCN and FastGCN was empirically similar , whereas GraphSAGE appears to converge much faster . Coupled with the per-epoch cost , overall FastGCN still wins with a substantial margin . We have inserted a section in the appendix to cover the total training time as well as the accuracy . Please see Section C.1 and particularly Table 3 and Figure 4 . > > > Some other concerns and questions : > > > It is not quite clear what P is . You defined it as distribution over vertices of some ( potentially infinite ) population graph . Later on , sampling from P becomes equivalent to uniform sampling over the observed nodes . I do n't see how you can define P over anything outside of the training nodes ( without defining loss on the unobserved data ) , as then you would be sampling from a distribution with 0 mass on the parts of the support of P , and this would break the Monte Carlo assumptions . This would be a very interesting excursion . In a sampling framework that we are settling with ( all being traced back to what empirical risk minimization means for graphs ) , P is an abstract probability measure for the graph nodes . For the sake of simplicity imagine an infinite graph ( just like the usual vectorial case where the input space is d-dimensional Euclidean ) . Some graph nodes are sampled for training and some others are used for validation and testing . P is the underlying ( unknown ) probability distribution that one uses for sampling . The uniform sampling mentioned later is a separate story . Suppose that you already have a sample ( i.e. , the training set ) . Note that \u201c a sample \u201d here means a collection of data points drawn iid from a population . And you want to estimate some properties of the population ( i.e. , the expected loss ) . Bootstrapping is a scheme that subsamples the given sample for performing inference on the unknown population . This corresponds to using a mini-batch of the training set to estimate the expected loss . The most straightforward approach for bootstrapping is a uniform subsampling with or without replacement . Importance ( sub ) sampling as we use later may yield a better estimate . > > > Weights disappeared in the majority of the analysis . Could you please make the representation more consistent . We reexamined the whole paper and included the weights as appropriate . Since they are linear , the overall theory and conclusions remain valid . > > > a ( v , u ) in Eq.2 and A ( v , u ) in Eq.5 are not defined . Do they both correspond to entries of the ( normalized ) adjacency ? Yes they do . Text was edited ."}, {"review_id": "rytstxWAW-1", "review_text": "The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions. This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods. It further explores variance reduction to speed up training via importance sampling. The idea comes with theoretical support and experimental studies. Some questions are as follows: 1) could you elaborate on n/t_l in (5) that accounts for the normalization difference between matrix form (1) and the integral form (2) ? 2) In Prop.2., there seems no essential difference between the two parts, as e(v) also depends on how the u_j's are sampled. 3) what loss g is used in experiments?", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the questions . Please find our responses in the following . We hope that your confusions are now cleared . > > > could you elaborate on n/t_l in ( 5 ) that accounts for the normalization difference between matrix form ( 1 ) and the integral form ( 2 ) ? For ( 2 ) , a probability measure must integrate to unity . On other hand , for the matrix form ( 1 ) , the matrix products will explode when the matrix size becomes larger and larger . What is lacking is a factor of n that normalizes ( 1 ) . In fact , such an issue could be more principledly explained in the context of importance sampling in the subsection that follows . Note the displayed formula in Algorithm 2 . Without using importance sampling , the denominator q ( u_j^ { ( l ) } ) is simply 1/n , hence simplified to Algorithm 1 . > > > In Prop.2. , there seems no essential difference between the two parts , as e ( v ) also depends on how the u_j 's are sampled . It is true that e ( v ) is an integral in the u space . What we meant on the other hand is that if we change the way the u_j \u2019 s are sampled , the variance of G will respectively change . The specific amount of change ( compare Proposition 2 , Theorem 3 , and Proposition 4 ) happens to the second term , leaving the first term R untouched . Please see the derivation ( proof ) in the appendix . > > > what loss g is used in experiments ? Following GCN and GraphSAGE , the loss is the cross entropy ."}, {"review_id": "rytstxWAW-2", "review_text": "The paper focuses on the recently graph convolutional network (GCN) framework. They authors identify a couple of issues with GCN: the fact that both training and test data need to be present at training time, making it transductive in nature and the fact that the notion of \u2018neighborhood\u2019 grows as the signal propagates through the network. The latter implies that GCNs can have a large memory footprint, making them impractical in certain cases. The authors propose an alternative formulation that interprets the signals as vertex embedding functions; it also interprets graph convolutions as integral transforms of said functions. Starting from mini-batches consisting purely of training data (during training) each layer performs Monte Carlo sampling on the vertices to approximate the embedding functions. They show that this estimator is consistent and can be used for training the proposed architecture, FastGCN, via standard SGD. Finally, they analyze the estimator\u2019s variance and propose an importance-sampling based estimator that has minimal layer-to-layer variance. The experiments demonstrate that FastGCN is much faster than the alternatives, while suffering a small accuracy penalty. This is a very good paper. The ideas are solid, the writing is excellent and the results convincing. I have a few comments and concerns listed below. Comments: 1. I agree with the anonymous commenter that the authors should provide detailed description of their experimental setup. 2. The timing of GraphSAGE on Cora is bizarre. I\u2019m even slightly suspicious that something might have been amiss in your setup. It is by far the smallest dataset. How do you explain GraphSAGE performing so much worse on Cora than on the bigger Pubmed and Reddit datasets? It is also on Cora that GraphSAGE seems to yield subpar accuracy, while it wins the other two datasets. 3. As a concrete step towards grounding the proposed method on state of the art results, I would love to see at least one experiment with the same (original) data splits used in previous papers. I understand that semi-supervised learning is not the purpose of this paper, however matching previous results would dispel any concerns about setup/hyperparameter mismatch. 4. Another thing missing is an exploration (or at least careful discussion) as to why FastGCN performs worse than the other methods in terms of accuracy and how much that relative penalty can be. Minor comments: 5. Please add label axes to Figure 2; currently it is very hard to read. Also please label the y axis in Figure 3. 6. The notation change in Section 3.1 was well intended, however I feel like it slowed me down significantly while reading the paper. I had already absorbed the original notation and had to go back and forth to translate to the new one. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your positive comments . Please find our responses and summary of revisions in the following . Your reviews are cited with > > > . > > > I agree with the anonymous commenter that the authors should provide detailed description of their experimental setup . We have inserted details regarding the train/val/test split concerned by the anonymous commenter , in the main text . Additional experiments were included in the appendix . > > > The timing of GraphSAGE on Cora is bizarre . I \u2019 m even slightly suspicious that something might have been amiss in your setup . It is by far the smallest dataset . How do you explain GraphSAGE performing so much worse on Cora than on the bigger Pubmed and Reddit datasets ? It is also on Cora that GraphSAGE seems to yield subpar accuracy , while it wins the other two datasets . We double checked the code and reran the experiments but did not spot abnormality . We encourage the reviewer to checkout our code from the anonymous github and verify . Here are our thoughts : For training time , GraphSAGE uses sampling so the time is independent of the graph size . The times across data sets should be comparable since sample sizes are comparable . Fluctuations are normal . For accuracy , we did another round of hyperparameter tuning and found that the F1 score on Cora can be improved . The newer results were updated to the table in Figure 3 . However , these better results are still subpar compared with those of GCN and FastGCN . > > > As a concrete step towards grounding the proposed method on state of the art results , I would love to see at least one experiment with the same ( original ) data splits used in previous papers . I understand that semi-supervised learning is not the purpose of this paper , however matching previous results would dispel any concerns about setup/hyperparameter mismatch . We have included an additional experiment in the appendix ; see Section C.2 . The results for GCN are consistent with those reported by Kipf and Welling . We have not seen reported results for GraphSAGE on these data sets ; our results suggest way inferior performance . It is suspected that the model significantly overfits the data , because training accuracy is 1 . For the proposed FastGCN , it also performs inferior to GCN , probably because of the very limited number of training labels . We fork a different version , called FastGCN-transductive , which uses both training and test data for learning ( hence falling back to the transductive setting of GCN ) . The results of FastGCN-transductive match those of GCN . > > > Another thing missing is an exploration ( or at least careful discussion ) as to why FastGCN performs worse than the other methods in terms of accuracy and how much that relative penalty can be . We would argue that the accuracy results of FastGCN are quite comparable with the best of other methods . The loss of accuracy is even smaller than the difference among the several aggregators proposed for GraphSAGE . The improvement in running time outweighs such a minimal loss . > > > Minor comments : > > > Please add label axes to Figure 2 ; currently it is very hard to read . Also please label the y axis in Figure 3 . Done. > > > The notation change in Section 3.1 was well intended , however I feel like it slowed me down significantly while reading the paper . I had already absorbed the original notation and had to go back and forth to translate to the new one . It is an unfortunate compromise , because the notations developed so far have become too cumbersome . If we carry the subscripts and superscripts to the rest of the paper , the digestion of the math is possibly even harder ."}, {"review_id": "rytstxWAW-3", "review_text": "This paper addresses the memory bottleneck problem in graph neural networks and proposes a novel importance sampling scheme that is based on sampling vertices (instead of sampling local neighbors as in [1]). Experimental results demonstrate a significant speedup in per-batch training time compared to previous works while retaining similar classification accuracy on standard benchmark datasets. The paper is well-written and proposes a simple, elegant, and well-motivated solution for the memory bottleneck issue in graph neural networks. I think that this paper mostly looks solid, but I am a bit worried about the following assumption: \u201cSpecifically, we interpret that graph vertices are iid samples of some probability distribution\u201d. As graph vertices are inter-connected and inter-dependent across edges of the graph, this iid assumption might be too strong. A short comment on why the authors take this particular interpretation would be helpful. In the abstract the authors write: \u201cSuch a model [GCN], however, is transductive in nature because parameters are learned through convolutions with both training and test data.\u201d \u2014 as demonstrated in Hamilton et al. (2017) [1], this class of models admits inductive learning as well as transductive learning, so the above statement is not quite accurate. Furthermore, a comment on whether this scheme would be useful for alternative graph neural network architectures, such as the one in MoNet [2] or the generic formulation of the original graph neural net [3] (nicely summarized in Gilmer et al. (2017) [4]) would be insightful (and would make the paper even stronger). I am very happy to see that the authors provide the code together with the submission (using an anonymous GitHub repository). The authors mention that \u201cThe code of GraphSAGE is downloaded from the accompany [sic] website, whereas GCN is self implemented.\u201c - Looking at the code it looks to me, however, as if it was based on the implementation by the authors of [5]. The experimental comparison in terms of per-batch training time looks very impressive, yet it would be good to also include a comparison in terms of total training time per model (e.g. in the appendix). I quickly checked the provided implementation for FastGCN on Pubmed and compared it against the GCN implementation from [5], and it looks like the original GCN model is roughly 30% faster on my laptop (no batched training). This is not very surprising, as a fair comparison should involve batched training for both approaches. Nonetheless it would be good to include these results in the paper to avoid confusion. Minor issues: - The notation of the limit in Theorem 1 is a bit unclear. I assume the limit is taken to infinity with respect to the number of samples. - There are a number of typos throughout the paper (like \u201coppose to\u201d instead of \u201copposed to\u201d), these should be fixed in the revision. - It would be better to summarize Figure 3 (left) in a table, as the smaller values are difficult to read off the chart. Overall, I think that this paper can be accepted. The proposed scheme is a simple drop-in replacement for the way adjacency matrices are prepared in current implementations of graph neural nets and it promises to solve the memory issue of previous works while being substantially faster than the model in [1]. I expect the proposed approach to be useful for most graph neural network models. UPDATE: I would like to thank the authors for their detailed response and for adding additional experimental evaluation. My initial concerns have been addressed and I can fully recommend acceptance of this paper. [1] W.L. Hamilton, R. Ying, J. Leskovec, Inductive Representation Learning on Large Graphs, NIPS 2017 [2] F. Monti, D. Boscaini, J. Masci, E. Rodala, J. Svoboda, M.M. Bronstein, Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017 [3] F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, G. Monfardini, The Graph Neural Network Model, IEEE Transactions on Neural Networks, 2009 [4] J. Gilmer, S.S. Schoenholz, P.F. Riley, O. Vinyals, G.E. Dahl, Neural Message Passing for Quantum Chemistry, ICML 2017 [5] T.N. Kipf, M. Welling, Semi-Supervised Classification with Graph Convolutional Networks, ICLR 2017", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your encouraging comments . Please find our responses and summary of revisions in the following . Your reviews are cited with > > > . > > > I think that this paper mostly looks solid , but I am a bit worried about the following assumption : \u201c Specifically , we interpret that graph vertices are iid samples of some probability distribution \u201d . As graph vertices are inter-connected and inter-dependent across edges of the graph , this iid assumption might be too strong . A short comment on why the authors take this particular interpretation would be helpful . The iid assumption was made to be conformant with the standard learning setting that minimizes the empirical risk of iid samples . The motivation was developed at the beginning of Section 3 . > > > In the abstract the authors write : \u201c Such a model [ GCN ] , however , is transductive in nature because parameters are learned through convolutions with both training and test data. \u201d \u2014 as demonstrated in Hamilton et al . ( 2017 ) [ 1 ] , this class of models admits inductive learning as well as transductive learning , so the above statement is not quite accurate . Yes , Hamilton et al.established an extension of GCN to the task of inductive unsupervised learning . For preciseness , we edited our statement . Now it reads : \u201c This model , however , was originally designed to be learned with the presence of both training and test data. \u201d > > > Furthermore , a comment on whether this scheme would be useful for alternative graph neural network architectures , such as the one in MoNet [ 2 ] or the generic formulation of the original graph neural net [ 3 ] ( nicely summarized in Gilmer et al . ( 2017 ) [ 4 ] ) would be insightful ( and would make the paper even stronger ) . Thank you very much for suggesting generalize our work to other architectures . Indeed , the simple yet powerful idea of sampling is often applicable to models that are based on first-order neighborhoods . We extended a paragraph in the concluding section to stress this point and also suggested an avenue of future work . > > > I am very happy to see that the authors provide the code together with the submission ( using an anonymous GitHub repository ) . The authors mention that \u201c The code of GraphSAGE is downloaded from the accompany [ sic ] website , whereas GCN is self implemented. \u201c - Looking at the code it looks to me , however , as if it was based on the implementation by the authors of [ 5 ] . Yes , the codes of FastGCN are based on the implementation of [ 5 ] . We meant that we used the codes of GraphSAGE without change , but implemented our own algorithm and changed the GCN codes to adapt to our problem setting . We have modified the text to clarify the confusion . > > > The experimental comparison in terms of per-batch training time looks very impressive , yet it would be good to also include a comparison in terms of total training time per model ( e.g.in the appendix ) . I quickly checked the provided implementation for FastGCN on Pubmed and compared it against the GCN implementation from [ 5 ] , and it looks like the original GCN model is roughly 30 % faster on my laptop ( no batched training ) . This is not very surprising , as a fair comparison should involve batched training for both approaches . Nonetheless it would be good to include these results in the paper to avoid confusion . We have included additional results regarding the total training time in the appendix . Please see Section C.1 . Note that for faster convergence , the learning rate of FastGCN has been changed to 0.01 in our codes , so now it is faster than the original GCN model on Pubmed . The accuracy of FastGCN remains the same . > > > Minor issues : > > > The notation of the limit in Theorem 1 is a bit unclear . I assume the limit is taken to infinity with respect to the number of samples . Yes.Corrected . > > > There are a number of typos throughout the paper ( like \u201c oppose to \u201d instead of \u201c opposed to \u201d ) , these should be fixed in the revision . Fixed. > > > It would be better to summarize Figure 3 ( left ) in a table , as the smaller values are difficult to read off the chart . We have increased the font size to make the numbers legible . Also note that the vertical axis is modified to the log10 scale so that orders-of-magnitude improvement can be easily seen . We feel that a bar chart here may be more informative than a table ."}], "0": {"review_id": "rytstxWAW-0", "review_text": "Update: I have read the rebuttal and the revised manuscript. Additionally I had a brief discussion with the authors regarding some aspects of their probabilistic framework. I think that batch training of GCN is an important problem and authors have proposed an interesting solution to this problem. I appreciated all the work authors put into the revision. In this regard, I have updated my rating. However, I am not satisfied with how the probabilistic problem formulation was presented in the paper. I would appreciate if authors were more upfront about the challenges of the problem they formulated and limitations of their results. I briefly summarize the key missing points below, although I acknowledge that solution to such questions is out of scope of this work. 1. Sampling of graph nodes from P is not iid. Every subsequent node can not be equal to any of the previous nodes. Hence, the distribution changes and subsequent nodes are dependent on previous ones. However, exchangeability could be a reasonable assumption to make as order (in the joint distribution) does not matter for simple choices of P. Example: let V be {1,2,3} and P a uniform distribution. First node can be any of the {1,2,3}, second node given first (suppose first node is '2') is restricted to {1,3}. There is clearly a dependency and change of distribution. 2. Theorem 1 is proven under the assumption that it is possible to sample from P and utilize Monte Carlo type argument. However, in practice, sampling is done from a uniform distribution over observed samples. Also, authors suggest that V may be infinite. Recall that for Monte Carlo type approaches to work, sampling distribution is ought to contain support of the true distribution. Observed samples (even as sample size goes to infinity) will never be able to cover an infinite V. Hence, Theorem 1 will never be applicable (for the purposes of evaluating population loss). Also note that this is different from a more classical case of continuous distributions, where sampling from a Gaussian, for instance, will cover any domain of true distribution. In the probabilistic framework defined by the authors it is impossible to cover domain of P, unless whole V is observed. ---------------------------------------------------------------------- This work addresses a major shortcoming of recently popularized GCN. That is, when the data is equipped with the graph structure, classic SGD based methods are not straightforward to apply. Hence it is not clear how to deal with large datasets (e.g., Reddit). Proposed approach uses an adjacency based importance sampling distribution to select only a subset of nodes on each GCN layer. Resulting loss estimate is shown to be consistent and its gradient is used to perform the weight updates. Proposed approach is interesting and the direction of the work is important given recent popularity of the GCN. Nonetheless I have two major question and would be happy to revisit my score if at least one is addressed. Theory: SGD requires an unbiased estimate of the gradient to converge to the global optima in the convex loss case. Here, the loss estimate is shown to be consistent, but not guaranteed to be unbiased and nothing is said about the gradient in Algorithm 1. Could you please provide some intuition about the gradient estimate? I might not be familiar with some relevant results, but it appears to me that Algorithm 1 will not converge to the same solution as full data GD would. Practice: Per batch timings in Fig. 3 are not enough to argue that the method is faster as it might have poor convergence properties overall. Could you please show the train/test accuracies against training time for all compared methods? Some other concerns and questions: - It is not quite cleat what P is. You defined it as distribution over vertices of some (potentially infinite) population graph. Later on, sampling from P becomes equivalent to uniform sampling over the observed nodes. I don't see how you can define P over anything outside of the training nodes (without defining loss on the unobserved data), as then you would be sampling from a distribution with 0 mass on the parts of the support of P, and this would break the Monte Carlo assumptions. - Weights disappeared in the majority of the analysis. Could you please make the representation more consistent. - a(v,u) in Eq. 2 and A(v,u) in Eq. 5 are not defined. Do they both correspond to entries of the (normalized) adjacency?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate very much your critical comments . Please find our responses and summary of revisions in the following . Your reviews are cited with > > > . We hope that the edited version may clear the confusion and you enjoy the paper as other reviewers do : ) > > > Theory : > > > SGD requires an unbiased estimate of the gradient to converge to the global optima in the convex loss case . Here , the loss estimate is shown to be consistent , but not guaranteed to be unbiased and nothing is said about the gradient in Algorithm 1 . Could you please provide some intuition about the gradient estimate ? I might not be familiar with some relevant results , but it appears to me that Algorithm 1 will not converge to the same solution as full data GD would . The consistency of the gradient estimator simply follows that of the loss estimator , if the differential operator is continuous . Hence , the essential question is whether SGD converges if the gradient estimator is consistent but not unbiased . We have developed a convergence theory in the appendix ( see Section D ) for our algorithms . Generally speaking , the convergence rate is the same as the case of unbiased gradient estimator . > > > Practice : > > > Per batch timings in Fig.3 are not enough to argue that the method is faster as it might have poor convergence properties overall . Could you please show the train/test accuracies against training time for all compared methods ? We found that the convergence speed between GCN and FastGCN was empirically similar , whereas GraphSAGE appears to converge much faster . Coupled with the per-epoch cost , overall FastGCN still wins with a substantial margin . We have inserted a section in the appendix to cover the total training time as well as the accuracy . Please see Section C.1 and particularly Table 3 and Figure 4 . > > > Some other concerns and questions : > > > It is not quite clear what P is . You defined it as distribution over vertices of some ( potentially infinite ) population graph . Later on , sampling from P becomes equivalent to uniform sampling over the observed nodes . I do n't see how you can define P over anything outside of the training nodes ( without defining loss on the unobserved data ) , as then you would be sampling from a distribution with 0 mass on the parts of the support of P , and this would break the Monte Carlo assumptions . This would be a very interesting excursion . In a sampling framework that we are settling with ( all being traced back to what empirical risk minimization means for graphs ) , P is an abstract probability measure for the graph nodes . For the sake of simplicity imagine an infinite graph ( just like the usual vectorial case where the input space is d-dimensional Euclidean ) . Some graph nodes are sampled for training and some others are used for validation and testing . P is the underlying ( unknown ) probability distribution that one uses for sampling . The uniform sampling mentioned later is a separate story . Suppose that you already have a sample ( i.e. , the training set ) . Note that \u201c a sample \u201d here means a collection of data points drawn iid from a population . And you want to estimate some properties of the population ( i.e. , the expected loss ) . Bootstrapping is a scheme that subsamples the given sample for performing inference on the unknown population . This corresponds to using a mini-batch of the training set to estimate the expected loss . The most straightforward approach for bootstrapping is a uniform subsampling with or without replacement . Importance ( sub ) sampling as we use later may yield a better estimate . > > > Weights disappeared in the majority of the analysis . Could you please make the representation more consistent . We reexamined the whole paper and included the weights as appropriate . Since they are linear , the overall theory and conclusions remain valid . > > > a ( v , u ) in Eq.2 and A ( v , u ) in Eq.5 are not defined . Do they both correspond to entries of the ( normalized ) adjacency ? Yes they do . Text was edited ."}, "1": {"review_id": "rytstxWAW-1", "review_text": "The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions. This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods. It further explores variance reduction to speed up training via importance sampling. The idea comes with theoretical support and experimental studies. Some questions are as follows: 1) could you elaborate on n/t_l in (5) that accounts for the normalization difference between matrix form (1) and the integral form (2) ? 2) In Prop.2., there seems no essential difference between the two parts, as e(v) also depends on how the u_j's are sampled. 3) what loss g is used in experiments?", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the questions . Please find our responses in the following . We hope that your confusions are now cleared . > > > could you elaborate on n/t_l in ( 5 ) that accounts for the normalization difference between matrix form ( 1 ) and the integral form ( 2 ) ? For ( 2 ) , a probability measure must integrate to unity . On other hand , for the matrix form ( 1 ) , the matrix products will explode when the matrix size becomes larger and larger . What is lacking is a factor of n that normalizes ( 1 ) . In fact , such an issue could be more principledly explained in the context of importance sampling in the subsection that follows . Note the displayed formula in Algorithm 2 . Without using importance sampling , the denominator q ( u_j^ { ( l ) } ) is simply 1/n , hence simplified to Algorithm 1 . > > > In Prop.2. , there seems no essential difference between the two parts , as e ( v ) also depends on how the u_j 's are sampled . It is true that e ( v ) is an integral in the u space . What we meant on the other hand is that if we change the way the u_j \u2019 s are sampled , the variance of G will respectively change . The specific amount of change ( compare Proposition 2 , Theorem 3 , and Proposition 4 ) happens to the second term , leaving the first term R untouched . Please see the derivation ( proof ) in the appendix . > > > what loss g is used in experiments ? Following GCN and GraphSAGE , the loss is the cross entropy ."}, "2": {"review_id": "rytstxWAW-2", "review_text": "The paper focuses on the recently graph convolutional network (GCN) framework. They authors identify a couple of issues with GCN: the fact that both training and test data need to be present at training time, making it transductive in nature and the fact that the notion of \u2018neighborhood\u2019 grows as the signal propagates through the network. The latter implies that GCNs can have a large memory footprint, making them impractical in certain cases. The authors propose an alternative formulation that interprets the signals as vertex embedding functions; it also interprets graph convolutions as integral transforms of said functions. Starting from mini-batches consisting purely of training data (during training) each layer performs Monte Carlo sampling on the vertices to approximate the embedding functions. They show that this estimator is consistent and can be used for training the proposed architecture, FastGCN, via standard SGD. Finally, they analyze the estimator\u2019s variance and propose an importance-sampling based estimator that has minimal layer-to-layer variance. The experiments demonstrate that FastGCN is much faster than the alternatives, while suffering a small accuracy penalty. This is a very good paper. The ideas are solid, the writing is excellent and the results convincing. I have a few comments and concerns listed below. Comments: 1. I agree with the anonymous commenter that the authors should provide detailed description of their experimental setup. 2. The timing of GraphSAGE on Cora is bizarre. I\u2019m even slightly suspicious that something might have been amiss in your setup. It is by far the smallest dataset. How do you explain GraphSAGE performing so much worse on Cora than on the bigger Pubmed and Reddit datasets? It is also on Cora that GraphSAGE seems to yield subpar accuracy, while it wins the other two datasets. 3. As a concrete step towards grounding the proposed method on state of the art results, I would love to see at least one experiment with the same (original) data splits used in previous papers. I understand that semi-supervised learning is not the purpose of this paper, however matching previous results would dispel any concerns about setup/hyperparameter mismatch. 4. Another thing missing is an exploration (or at least careful discussion) as to why FastGCN performs worse than the other methods in terms of accuracy and how much that relative penalty can be. Minor comments: 5. Please add label axes to Figure 2; currently it is very hard to read. Also please label the y axis in Figure 3. 6. The notation change in Section 3.1 was well intended, however I feel like it slowed me down significantly while reading the paper. I had already absorbed the original notation and had to go back and forth to translate to the new one. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your positive comments . Please find our responses and summary of revisions in the following . Your reviews are cited with > > > . > > > I agree with the anonymous commenter that the authors should provide detailed description of their experimental setup . We have inserted details regarding the train/val/test split concerned by the anonymous commenter , in the main text . Additional experiments were included in the appendix . > > > The timing of GraphSAGE on Cora is bizarre . I \u2019 m even slightly suspicious that something might have been amiss in your setup . It is by far the smallest dataset . How do you explain GraphSAGE performing so much worse on Cora than on the bigger Pubmed and Reddit datasets ? It is also on Cora that GraphSAGE seems to yield subpar accuracy , while it wins the other two datasets . We double checked the code and reran the experiments but did not spot abnormality . We encourage the reviewer to checkout our code from the anonymous github and verify . Here are our thoughts : For training time , GraphSAGE uses sampling so the time is independent of the graph size . The times across data sets should be comparable since sample sizes are comparable . Fluctuations are normal . For accuracy , we did another round of hyperparameter tuning and found that the F1 score on Cora can be improved . The newer results were updated to the table in Figure 3 . However , these better results are still subpar compared with those of GCN and FastGCN . > > > As a concrete step towards grounding the proposed method on state of the art results , I would love to see at least one experiment with the same ( original ) data splits used in previous papers . I understand that semi-supervised learning is not the purpose of this paper , however matching previous results would dispel any concerns about setup/hyperparameter mismatch . We have included an additional experiment in the appendix ; see Section C.2 . The results for GCN are consistent with those reported by Kipf and Welling . We have not seen reported results for GraphSAGE on these data sets ; our results suggest way inferior performance . It is suspected that the model significantly overfits the data , because training accuracy is 1 . For the proposed FastGCN , it also performs inferior to GCN , probably because of the very limited number of training labels . We fork a different version , called FastGCN-transductive , which uses both training and test data for learning ( hence falling back to the transductive setting of GCN ) . The results of FastGCN-transductive match those of GCN . > > > Another thing missing is an exploration ( or at least careful discussion ) as to why FastGCN performs worse than the other methods in terms of accuracy and how much that relative penalty can be . We would argue that the accuracy results of FastGCN are quite comparable with the best of other methods . The loss of accuracy is even smaller than the difference among the several aggregators proposed for GraphSAGE . The improvement in running time outweighs such a minimal loss . > > > Minor comments : > > > Please add label axes to Figure 2 ; currently it is very hard to read . Also please label the y axis in Figure 3 . Done. > > > The notation change in Section 3.1 was well intended , however I feel like it slowed me down significantly while reading the paper . I had already absorbed the original notation and had to go back and forth to translate to the new one . It is an unfortunate compromise , because the notations developed so far have become too cumbersome . If we carry the subscripts and superscripts to the rest of the paper , the digestion of the math is possibly even harder ."}, "3": {"review_id": "rytstxWAW-3", "review_text": "This paper addresses the memory bottleneck problem in graph neural networks and proposes a novel importance sampling scheme that is based on sampling vertices (instead of sampling local neighbors as in [1]). Experimental results demonstrate a significant speedup in per-batch training time compared to previous works while retaining similar classification accuracy on standard benchmark datasets. The paper is well-written and proposes a simple, elegant, and well-motivated solution for the memory bottleneck issue in graph neural networks. I think that this paper mostly looks solid, but I am a bit worried about the following assumption: \u201cSpecifically, we interpret that graph vertices are iid samples of some probability distribution\u201d. As graph vertices are inter-connected and inter-dependent across edges of the graph, this iid assumption might be too strong. A short comment on why the authors take this particular interpretation would be helpful. In the abstract the authors write: \u201cSuch a model [GCN], however, is transductive in nature because parameters are learned through convolutions with both training and test data.\u201d \u2014 as demonstrated in Hamilton et al. (2017) [1], this class of models admits inductive learning as well as transductive learning, so the above statement is not quite accurate. Furthermore, a comment on whether this scheme would be useful for alternative graph neural network architectures, such as the one in MoNet [2] or the generic formulation of the original graph neural net [3] (nicely summarized in Gilmer et al. (2017) [4]) would be insightful (and would make the paper even stronger). I am very happy to see that the authors provide the code together with the submission (using an anonymous GitHub repository). The authors mention that \u201cThe code of GraphSAGE is downloaded from the accompany [sic] website, whereas GCN is self implemented.\u201c - Looking at the code it looks to me, however, as if it was based on the implementation by the authors of [5]. The experimental comparison in terms of per-batch training time looks very impressive, yet it would be good to also include a comparison in terms of total training time per model (e.g. in the appendix). I quickly checked the provided implementation for FastGCN on Pubmed and compared it against the GCN implementation from [5], and it looks like the original GCN model is roughly 30% faster on my laptop (no batched training). This is not very surprising, as a fair comparison should involve batched training for both approaches. Nonetheless it would be good to include these results in the paper to avoid confusion. Minor issues: - The notation of the limit in Theorem 1 is a bit unclear. I assume the limit is taken to infinity with respect to the number of samples. - There are a number of typos throughout the paper (like \u201coppose to\u201d instead of \u201copposed to\u201d), these should be fixed in the revision. - It would be better to summarize Figure 3 (left) in a table, as the smaller values are difficult to read off the chart. Overall, I think that this paper can be accepted. The proposed scheme is a simple drop-in replacement for the way adjacency matrices are prepared in current implementations of graph neural nets and it promises to solve the memory issue of previous works while being substantially faster than the model in [1]. I expect the proposed approach to be useful for most graph neural network models. UPDATE: I would like to thank the authors for their detailed response and for adding additional experimental evaluation. My initial concerns have been addressed and I can fully recommend acceptance of this paper. [1] W.L. Hamilton, R. Ying, J. Leskovec, Inductive Representation Learning on Large Graphs, NIPS 2017 [2] F. Monti, D. Boscaini, J. Masci, E. Rodala, J. Svoboda, M.M. Bronstein, Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017 [3] F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, G. Monfardini, The Graph Neural Network Model, IEEE Transactions on Neural Networks, 2009 [4] J. Gilmer, S.S. Schoenholz, P.F. Riley, O. Vinyals, G.E. Dahl, Neural Message Passing for Quantum Chemistry, ICML 2017 [5] T.N. Kipf, M. Welling, Semi-Supervised Classification with Graph Convolutional Networks, ICLR 2017", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your encouraging comments . Please find our responses and summary of revisions in the following . Your reviews are cited with > > > . > > > I think that this paper mostly looks solid , but I am a bit worried about the following assumption : \u201c Specifically , we interpret that graph vertices are iid samples of some probability distribution \u201d . As graph vertices are inter-connected and inter-dependent across edges of the graph , this iid assumption might be too strong . A short comment on why the authors take this particular interpretation would be helpful . The iid assumption was made to be conformant with the standard learning setting that minimizes the empirical risk of iid samples . The motivation was developed at the beginning of Section 3 . > > > In the abstract the authors write : \u201c Such a model [ GCN ] , however , is transductive in nature because parameters are learned through convolutions with both training and test data. \u201d \u2014 as demonstrated in Hamilton et al . ( 2017 ) [ 1 ] , this class of models admits inductive learning as well as transductive learning , so the above statement is not quite accurate . Yes , Hamilton et al.established an extension of GCN to the task of inductive unsupervised learning . For preciseness , we edited our statement . Now it reads : \u201c This model , however , was originally designed to be learned with the presence of both training and test data. \u201d > > > Furthermore , a comment on whether this scheme would be useful for alternative graph neural network architectures , such as the one in MoNet [ 2 ] or the generic formulation of the original graph neural net [ 3 ] ( nicely summarized in Gilmer et al . ( 2017 ) [ 4 ] ) would be insightful ( and would make the paper even stronger ) . Thank you very much for suggesting generalize our work to other architectures . Indeed , the simple yet powerful idea of sampling is often applicable to models that are based on first-order neighborhoods . We extended a paragraph in the concluding section to stress this point and also suggested an avenue of future work . > > > I am very happy to see that the authors provide the code together with the submission ( using an anonymous GitHub repository ) . The authors mention that \u201c The code of GraphSAGE is downloaded from the accompany [ sic ] website , whereas GCN is self implemented. \u201c - Looking at the code it looks to me , however , as if it was based on the implementation by the authors of [ 5 ] . Yes , the codes of FastGCN are based on the implementation of [ 5 ] . We meant that we used the codes of GraphSAGE without change , but implemented our own algorithm and changed the GCN codes to adapt to our problem setting . We have modified the text to clarify the confusion . > > > The experimental comparison in terms of per-batch training time looks very impressive , yet it would be good to also include a comparison in terms of total training time per model ( e.g.in the appendix ) . I quickly checked the provided implementation for FastGCN on Pubmed and compared it against the GCN implementation from [ 5 ] , and it looks like the original GCN model is roughly 30 % faster on my laptop ( no batched training ) . This is not very surprising , as a fair comparison should involve batched training for both approaches . Nonetheless it would be good to include these results in the paper to avoid confusion . We have included additional results regarding the total training time in the appendix . Please see Section C.1 . Note that for faster convergence , the learning rate of FastGCN has been changed to 0.01 in our codes , so now it is faster than the original GCN model on Pubmed . The accuracy of FastGCN remains the same . > > > Minor issues : > > > The notation of the limit in Theorem 1 is a bit unclear . I assume the limit is taken to infinity with respect to the number of samples . Yes.Corrected . > > > There are a number of typos throughout the paper ( like \u201c oppose to \u201d instead of \u201c opposed to \u201d ) , these should be fixed in the revision . Fixed. > > > It would be better to summarize Figure 3 ( left ) in a table , as the smaller values are difficult to read off the chart . We have increased the font size to make the numbers legible . Also note that the vertical axis is modified to the log10 scale so that orders-of-magnitude improvement can be easily seen . We feel that a bar chart here may be more informative than a table ."}}