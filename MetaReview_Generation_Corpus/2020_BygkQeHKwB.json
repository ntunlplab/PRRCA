{"year": "2020", "forum": "BygkQeHKwB", "title": "Walking on the Edge: Fast, Low-Distortion Adversarial Examples", "decision": "Reject", "meta_review": "In this paper the authors highlight the role of time in adversarial training and study various speed-distortion trade-offs. They introduce an attack called boundary projection BP which relies on utilizing the classification boundary. The reviewers agree that searching on the class boundary manifold, is interesting and promising but raise important concerns about evaluations on state of the art data sets. Some of the reviewers also express concern about the quality of presentation and lack of detail. While the authors have addressed some of these issues in the response, the reviewers continue to have some concerns. Overall I agree with the assessment of the reviewers and do not recommend acceptance at this time.", "reviews": [{"review_id": "BygkQeHKwB-0", "review_text": "This paper considers efficiently producing adversarial examples for deep neural networks and proposes boundary projection (BP), which quickly searches an adversarial example around the classification boundary. The BP approach is tested on three benchmark datasets and compared with existing adversarial attacking methods. The key idea of BP, searching on the class boundary manifold, is interesting and promising. However, despite the excess of the recommended 8 pages, the main parts of the proposed method are not so clearly explained. - It is not so clear which parts of the proposed method (Section 3) are mathematically justified. For example, \\gamma_i in Eq. (14) looks heuristically introduced. - Although the abstract and introduction emphasize that the main focus of BP is speed-distortion tradeoff, the experiments section does not discuss it so much and so clearly. While the operating characteristic of probability of success and distortion is mainly discussed, it is unclear which argument most demonstrate the improvement in speed-distortion tradeoff. p.5, l.7: 1(a) -> Figure 1(a) p.8, l.10: measure measure p.8, right after Eq. (21): `\"`is conditioned is\" -> \"\"is conditioned ", "rating": "3: Weak Reject", "reply_text": "Thank you for your careful and valuable comments . We address your concerns point by point below . As general feedback , we fail to see how the concerns discussed here could support a `` weak reject '' recommendation , especially given the low confidence and quick assessment as stated . Could you please let us know if there are any other issues ? Comment 1 : `` However , despite the excess of the recommended 8 pages , the main parts of the proposed method are not so clearly explained . '' Response 1 : We have put a lot of effort in describing the method with motivation , description , pseudo-code and diagrams . The other two reviewers clearly understand our method . R3 even says the paper is a good read . Could you please elaborate on what is not clear ? We are willing to make it as clear as possible . Comment 2 : `` It is not so clear which parts of the proposed method ( Section 3 ) are mathematically justified . For example , \\gamma_i in Eq . ( 14 ) looks heuristically introduced . '' Response 2 : This concern is on using heuristics rather than on clarity . The $ \\gamma_i $ heuristic ( 14 ) builds on a simpler idea of DDN ( Rony et al. , 2019 ) , where parameter $ \\gamma $ is constant across iterations . As explained , $ \\gamma_i $ controls the distortion : In stage 1 , updates are small at the beginning to keep distortion low , then larger until the attack succeeds . In stage 2 , updates are decreasing as $ \\gamma_i $ tends to 1 . It increases the distortion when the current image is correctly classified ( IN ) and decreases the distortion when the current image is adversarial ( OUT ) . All this behavior is controlled by a single parameter , which simplifies the algorithm . The fact that $ ( \\gamma_i ) _i $ is strictly increasing allows us to show that , in Stage 2 , an IN iteration ( distortion grows by $ 1/\\gamma_i > 1 $ ) followed by an OUT iteration ( distortion decays by $ \\gamma_ { i+1 } < 1 $ ) is indeed equivalent to a milder IN in the sense that the distortion grows by $ \\gamma_ { i+1 } /\\gamma_i $ which is larger than 1 but smaller than $ 1/\\gamma_i > 1 $ . Similarly , OUT followed by IN is equivalent to a mild OUT in the sense that distortion decays by $ \\gamma_i/\\gamma_ { i+1 } < 1 $ . Both cases lead towards the class boundary by a factor that tends to 1 : if the algorithm keeps alternating between OUT and IN and we only look at the OUT iterates ( remember , all attacks output the successful iterate of least distortion ) , this is equivalent to strictly decreasing distortion . This behavior is more stable than having a constant parameter $ \\gamma $ as in DDN . We shall add this jusitification . From all the possible increasing sequences $ ( \\gamma_i ) _i $ that go to 1 as i goes to the maximum number of iterations , we pick the simplest one : a linear sequence . That is the only heuristic . Comment 3 : \u201c Although the abstract and introduction emphasize that the main focus of BP is speed-distortion tradeoff , the experiments section does not discuss it so much and so clearly . While the operating characteristic of probability of success and distortion is mainly discussed , it is unclear which argument most demonstrates the improvement in speed-distortion tradeoff. \u201d Response 3 : The speed-distortion trade-off is partially addressed by reporting results for 20 and 100 iterations in Tables 1 , 2 , 3 . A more complete treatment is given in Appendix B.1 and Fig.5 , showing probability of success and distortion for more choices than 20 and 100 iterations . Following the benevolent recommendation of R3 , we will move Appendix B.1 and Fig.5 to the main body of the paper . As attacks become more and more powerful , speed becomes as important as distortion and probability of success . Comment 4 : p.5 , l.7 : 1 ( a ) - > Figure 1 ( a ) ; p.8 , l.10 : measure measure ; p.8 , right after Eq . ( 21 ) : ` `` ` is conditioned is '' - > `` `` is conditioned '' Response 4 : Thank you for spotting these mistakes . They are now corrected ."}, {"review_id": "BygkQeHKwB-1", "review_text": "This paper proposed an adversarial attack method based on optimization on the manifold. The authors claim it is a fast and effective attack even with quantization. It would better to also evaluate the method on the state of the art robust models (such as Madry et al ICLR'18) instead of only testing it on natural models. Generating adversarial examples on natural models is rather a well-solved problem and I do not think a 0.1 decrease in L2 norm is a big contribution since it is already so small that humans cannot distinguish. A better way to prove the strength would be to test it on a robust model to achieve higher success rates given a maximum distortion. I do not think the results in Table 3 are convincing or necessary. It is well-known that the FGSM is so weak that the adversarial examples produced by it are not strong enough for adversarial training. The state of the art adversarial training defense uses the adversarial examples obtained from PGD. Also, a popular way to evaluate model robustness would be to evaluate the attack success rate under a given upper bound of distortion (e.g. 0.3 for MNIST). If there is no constraint on the distortion, we can always achieve a 100% attack success rate by simply use an image from another class. So in Table 3, the authors may either make sure all attacks have a 100% success rate and compare the distortion, or set an upper bound of distortion and compare the success rate (just as in the operating characteristics plot). With the current results, I do not believe the robust training with BP can be any better than FGSM. Similar issues also exist in Table 2. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your careful and valuable comments . We address your concerns point by point . Comment 1 : \u201c It would better to also evaluate the method on the state of the art robust models ( such as Madry et al ICLR'18 ) instead of only testing it on natural models. \u201d Response 1 : All attacks keep reporting performance on natural models . For completeness , this is the first kind of evaluation every attack should have . Moreover , we do compare to robust models too . And not just that ; we also use our attack to build an even more robust model . Table 3 compares three robust models obtained by adversarial training , each using a different attack including ours . As a defense , our attack ( BP ) has a similar or better performance than DDN . According to ( Rony et al. , 2019 ) , the model obtained by adversarial training based on the DDN attack beats the robust model of ( Madry et al , ICLR \u2019 18 ) . Moreover , the training process of DDN is more efficient since standard ( clean ) model training is followed by few epochs using adversarial examples alone ; while ( Madry et al ICLR \u2019 18 ) train from scratch using a mix of clean and adversarial examples . This is why we have chosen DDN as a state of the art defense , which we further improve with our BP defense . Comment 2 : \u201c I do not think the results in Table 3 are convincing or necessary . It is well-known that the FGSM is so weak that the adversarial examples produced by it are not strong enough for adversarial training . The state of the art adversarial training defense uses the adversarial examples obtained from PGD . ... With the current results , I do not believe the robust training with BP can be any better than FGSM . Similar issues also exist in Table 2. \u201d Response 2 : According to Table 3 , adversarial training with our BP is similar or better than DDN in terms of distortion , and it also beats adversarial training with FGSM by a large margin in all cases ( higher distortion for all attacks ) . In fact , FGSM defense was included as a baseline since it was the first method used for adversarial training . We agree with the reviewer 's comment that FGSM is weak . Besides , since our attack is fast , the adversarial training is fast . Table 3 is then necessary , showing improvements in defense over DDN , which in turn improves over PGD ( Madry et al , ICLR'18 ) , which in turn improves over FGSM . For completeness , we shall add PGD and the original ( Madry et al ICLR \u2019 18 ) defenses to make Table 3 more convincing , as well as corresponding operating characteristics like those of Fig.3 ."}, {"review_id": "BygkQeHKwB-2", "review_text": "This paper introduces a parameterized approach to generate adversarial samples by balancing the speed-distortion trade-off. The method first tries to reach the boundary of classes in the classifier space, then walks on the classifier manifold to find adversarial samples that make the classifier to fail in prediction while minimizing the level of distortion in the sample. Having a limited number of iterations, the method reduces the fluctuations around the boundary and paves the classification manifold. The idea is novel, interesting and well-formulated, while the intuition could be better explained. The paper is a good read, has an adequate amount of literature review, and the results are supporting the claims of the paper: lower distortion while having comparable accuracy, the use of generated samples in fortifying the classifier, and keeping distortion to a reasonable level (qualitative results in appendix). However, one of the claims is to trade the distortion level to speed that needs verifying in the main manuscript, therefore, it is suggested that the section B.1 moves to the main manuscript and discussed more thoroughly. Also the effect of other parameters on this trade-off (such as the number of iterations K). It is also interesting to discuss how the algorithm performs in classes that are linearly separable on a toy dataset.", "rating": "6: Weak Accept", "reply_text": "Thank you for your careful and valuable comments . We address your concerns point by point . Comment 1 : \u201c However , one of the claims is to trade the distortion level to speed that needs verifying in the main manuscript , therefore , it is suggested that the section B.1 moves to the main manuscript and discussed more thoroughly. \u201d Response 1 : It is reasonable to move section B.1 to the main manuscript . This should also help with respect to point 3 of reviewer 1 . We will find the space for it and update the manuscript , but this probably means moving some other material to an Appendix or otherwise significantly shortening or removing other material . Comment 2 : \u201c Also the effect of other parameters on this trade-off ( such as the number of iterations K ) . \u201d Response 2 : The effect of the number of iterations $ K $ is exactly the same as $ \\ # grad $ as shown in Fig.5 because we only calculate one gradient per iteration . Other parameters are $ \\alpha $ and $ \\gamma_ { min } $ . We will add more results to show the effect of these parameters in an appendix . Comment 3 : \u201c It is also interesting to discuss how the algorithm performs in classes that are linearly separable on a toy dataset. \u201d Response 3 : Considering Fig.1 , if the classes were linearly separable , the boundary and all iso-contours would be straight lines , the gradient would be normal to these lines , and all algorithms would move along a line normal to the boundary . The problem would then be one-dimensional , which is not interesting to display like Fig.1.What may be more interesting is to plot the position on this line as a function of iteration . This we may attempt to include in an appendix ."}], "0": {"review_id": "BygkQeHKwB-0", "review_text": "This paper considers efficiently producing adversarial examples for deep neural networks and proposes boundary projection (BP), which quickly searches an adversarial example around the classification boundary. The BP approach is tested on three benchmark datasets and compared with existing adversarial attacking methods. The key idea of BP, searching on the class boundary manifold, is interesting and promising. However, despite the excess of the recommended 8 pages, the main parts of the proposed method are not so clearly explained. - It is not so clear which parts of the proposed method (Section 3) are mathematically justified. For example, \\gamma_i in Eq. (14) looks heuristically introduced. - Although the abstract and introduction emphasize that the main focus of BP is speed-distortion tradeoff, the experiments section does not discuss it so much and so clearly. While the operating characteristic of probability of success and distortion is mainly discussed, it is unclear which argument most demonstrate the improvement in speed-distortion tradeoff. p.5, l.7: 1(a) -> Figure 1(a) p.8, l.10: measure measure p.8, right after Eq. (21): `\"`is conditioned is\" -> \"\"is conditioned ", "rating": "3: Weak Reject", "reply_text": "Thank you for your careful and valuable comments . We address your concerns point by point below . As general feedback , we fail to see how the concerns discussed here could support a `` weak reject '' recommendation , especially given the low confidence and quick assessment as stated . Could you please let us know if there are any other issues ? Comment 1 : `` However , despite the excess of the recommended 8 pages , the main parts of the proposed method are not so clearly explained . '' Response 1 : We have put a lot of effort in describing the method with motivation , description , pseudo-code and diagrams . The other two reviewers clearly understand our method . R3 even says the paper is a good read . Could you please elaborate on what is not clear ? We are willing to make it as clear as possible . Comment 2 : `` It is not so clear which parts of the proposed method ( Section 3 ) are mathematically justified . For example , \\gamma_i in Eq . ( 14 ) looks heuristically introduced . '' Response 2 : This concern is on using heuristics rather than on clarity . The $ \\gamma_i $ heuristic ( 14 ) builds on a simpler idea of DDN ( Rony et al. , 2019 ) , where parameter $ \\gamma $ is constant across iterations . As explained , $ \\gamma_i $ controls the distortion : In stage 1 , updates are small at the beginning to keep distortion low , then larger until the attack succeeds . In stage 2 , updates are decreasing as $ \\gamma_i $ tends to 1 . It increases the distortion when the current image is correctly classified ( IN ) and decreases the distortion when the current image is adversarial ( OUT ) . All this behavior is controlled by a single parameter , which simplifies the algorithm . The fact that $ ( \\gamma_i ) _i $ is strictly increasing allows us to show that , in Stage 2 , an IN iteration ( distortion grows by $ 1/\\gamma_i > 1 $ ) followed by an OUT iteration ( distortion decays by $ \\gamma_ { i+1 } < 1 $ ) is indeed equivalent to a milder IN in the sense that the distortion grows by $ \\gamma_ { i+1 } /\\gamma_i $ which is larger than 1 but smaller than $ 1/\\gamma_i > 1 $ . Similarly , OUT followed by IN is equivalent to a mild OUT in the sense that distortion decays by $ \\gamma_i/\\gamma_ { i+1 } < 1 $ . Both cases lead towards the class boundary by a factor that tends to 1 : if the algorithm keeps alternating between OUT and IN and we only look at the OUT iterates ( remember , all attacks output the successful iterate of least distortion ) , this is equivalent to strictly decreasing distortion . This behavior is more stable than having a constant parameter $ \\gamma $ as in DDN . We shall add this jusitification . From all the possible increasing sequences $ ( \\gamma_i ) _i $ that go to 1 as i goes to the maximum number of iterations , we pick the simplest one : a linear sequence . That is the only heuristic . Comment 3 : \u201c Although the abstract and introduction emphasize that the main focus of BP is speed-distortion tradeoff , the experiments section does not discuss it so much and so clearly . While the operating characteristic of probability of success and distortion is mainly discussed , it is unclear which argument most demonstrates the improvement in speed-distortion tradeoff. \u201d Response 3 : The speed-distortion trade-off is partially addressed by reporting results for 20 and 100 iterations in Tables 1 , 2 , 3 . A more complete treatment is given in Appendix B.1 and Fig.5 , showing probability of success and distortion for more choices than 20 and 100 iterations . Following the benevolent recommendation of R3 , we will move Appendix B.1 and Fig.5 to the main body of the paper . As attacks become more and more powerful , speed becomes as important as distortion and probability of success . Comment 4 : p.5 , l.7 : 1 ( a ) - > Figure 1 ( a ) ; p.8 , l.10 : measure measure ; p.8 , right after Eq . ( 21 ) : ` `` ` is conditioned is '' - > `` `` is conditioned '' Response 4 : Thank you for spotting these mistakes . They are now corrected ."}, "1": {"review_id": "BygkQeHKwB-1", "review_text": "This paper proposed an adversarial attack method based on optimization on the manifold. The authors claim it is a fast and effective attack even with quantization. It would better to also evaluate the method on the state of the art robust models (such as Madry et al ICLR'18) instead of only testing it on natural models. Generating adversarial examples on natural models is rather a well-solved problem and I do not think a 0.1 decrease in L2 norm is a big contribution since it is already so small that humans cannot distinguish. A better way to prove the strength would be to test it on a robust model to achieve higher success rates given a maximum distortion. I do not think the results in Table 3 are convincing or necessary. It is well-known that the FGSM is so weak that the adversarial examples produced by it are not strong enough for adversarial training. The state of the art adversarial training defense uses the adversarial examples obtained from PGD. Also, a popular way to evaluate model robustness would be to evaluate the attack success rate under a given upper bound of distortion (e.g. 0.3 for MNIST). If there is no constraint on the distortion, we can always achieve a 100% attack success rate by simply use an image from another class. So in Table 3, the authors may either make sure all attacks have a 100% success rate and compare the distortion, or set an upper bound of distortion and compare the success rate (just as in the operating characteristics plot). With the current results, I do not believe the robust training with BP can be any better than FGSM. Similar issues also exist in Table 2. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your careful and valuable comments . We address your concerns point by point . Comment 1 : \u201c It would better to also evaluate the method on the state of the art robust models ( such as Madry et al ICLR'18 ) instead of only testing it on natural models. \u201d Response 1 : All attacks keep reporting performance on natural models . For completeness , this is the first kind of evaluation every attack should have . Moreover , we do compare to robust models too . And not just that ; we also use our attack to build an even more robust model . Table 3 compares three robust models obtained by adversarial training , each using a different attack including ours . As a defense , our attack ( BP ) has a similar or better performance than DDN . According to ( Rony et al. , 2019 ) , the model obtained by adversarial training based on the DDN attack beats the robust model of ( Madry et al , ICLR \u2019 18 ) . Moreover , the training process of DDN is more efficient since standard ( clean ) model training is followed by few epochs using adversarial examples alone ; while ( Madry et al ICLR \u2019 18 ) train from scratch using a mix of clean and adversarial examples . This is why we have chosen DDN as a state of the art defense , which we further improve with our BP defense . Comment 2 : \u201c I do not think the results in Table 3 are convincing or necessary . It is well-known that the FGSM is so weak that the adversarial examples produced by it are not strong enough for adversarial training . The state of the art adversarial training defense uses the adversarial examples obtained from PGD . ... With the current results , I do not believe the robust training with BP can be any better than FGSM . Similar issues also exist in Table 2. \u201d Response 2 : According to Table 3 , adversarial training with our BP is similar or better than DDN in terms of distortion , and it also beats adversarial training with FGSM by a large margin in all cases ( higher distortion for all attacks ) . In fact , FGSM defense was included as a baseline since it was the first method used for adversarial training . We agree with the reviewer 's comment that FGSM is weak . Besides , since our attack is fast , the adversarial training is fast . Table 3 is then necessary , showing improvements in defense over DDN , which in turn improves over PGD ( Madry et al , ICLR'18 ) , which in turn improves over FGSM . For completeness , we shall add PGD and the original ( Madry et al ICLR \u2019 18 ) defenses to make Table 3 more convincing , as well as corresponding operating characteristics like those of Fig.3 ."}, "2": {"review_id": "BygkQeHKwB-2", "review_text": "This paper introduces a parameterized approach to generate adversarial samples by balancing the speed-distortion trade-off. The method first tries to reach the boundary of classes in the classifier space, then walks on the classifier manifold to find adversarial samples that make the classifier to fail in prediction while minimizing the level of distortion in the sample. Having a limited number of iterations, the method reduces the fluctuations around the boundary and paves the classification manifold. The idea is novel, interesting and well-formulated, while the intuition could be better explained. The paper is a good read, has an adequate amount of literature review, and the results are supporting the claims of the paper: lower distortion while having comparable accuracy, the use of generated samples in fortifying the classifier, and keeping distortion to a reasonable level (qualitative results in appendix). However, one of the claims is to trade the distortion level to speed that needs verifying in the main manuscript, therefore, it is suggested that the section B.1 moves to the main manuscript and discussed more thoroughly. Also the effect of other parameters on this trade-off (such as the number of iterations K). It is also interesting to discuss how the algorithm performs in classes that are linearly separable on a toy dataset.", "rating": "6: Weak Accept", "reply_text": "Thank you for your careful and valuable comments . We address your concerns point by point . Comment 1 : \u201c However , one of the claims is to trade the distortion level to speed that needs verifying in the main manuscript , therefore , it is suggested that the section B.1 moves to the main manuscript and discussed more thoroughly. \u201d Response 1 : It is reasonable to move section B.1 to the main manuscript . This should also help with respect to point 3 of reviewer 1 . We will find the space for it and update the manuscript , but this probably means moving some other material to an Appendix or otherwise significantly shortening or removing other material . Comment 2 : \u201c Also the effect of other parameters on this trade-off ( such as the number of iterations K ) . \u201d Response 2 : The effect of the number of iterations $ K $ is exactly the same as $ \\ # grad $ as shown in Fig.5 because we only calculate one gradient per iteration . Other parameters are $ \\alpha $ and $ \\gamma_ { min } $ . We will add more results to show the effect of these parameters in an appendix . Comment 3 : \u201c It is also interesting to discuss how the algorithm performs in classes that are linearly separable on a toy dataset. \u201d Response 3 : Considering Fig.1 , if the classes were linearly separable , the boundary and all iso-contours would be straight lines , the gradient would be normal to these lines , and all algorithms would move along a line normal to the boundary . The problem would then be one-dimensional , which is not interesting to display like Fig.1.What may be more interesting is to plot the position on this line as a function of iteration . This we may attempt to include in an appendix ."}}