{"year": "2021", "forum": "V69LGwJ0lIN", "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper presents an interesting mix of new theoretical and empirical results showing how learning temporally extended primitive behaviors can help improve offline (batch) RL.\n\nAlthough 2/3 reviewers initially raised concerns regarding the motivation of the approach and some of the choices that were made, the authors did an excellent job at addressing these concerns in detail, and there is now a consensus towards acceptance.\n\nI consider that this work is a meaningful contribution towards better offline RL, which is definitely a very important use case in practice. The authors have given convincing explanations to motivate their approach, and made several improvements to the paper. As a result, I am recommending it for acceptance, as a poster.", "reviews": [{"review_id": "V69LGwJ0lIN-0", "review_text": "In the RL setting , this paper tackles the case where an agent may have access to large amounts of offline experience data . The objective of the work is to find an effective way to leverage this data for finding temporally extended primitive behaviors . The paper provides results that show how performing offline primitive learning can be leveraged for improving few-shot imitation learning as well as exploration and transfer on a variety of benchmark domains . The paper tackles an important question in reinforcement learning : learning temporally extended primitive behaviors from off-policy data is a very relevant question . However , I found the motivation for the approach quite vague as well as different elements that require clarification ( see below ) and because of this , I ca n't recommend acceptance . Motivation for the approach : - if the downstream policy has to stay close to the offline data distribution , it seems to me that if the offline data distribution is obtained with a bad policy , this can not lead to interesting decision-making . - can you explain why equation 2 `` motivates better generalization '' ? Other concerns : - Why can gamma not have a value of 0 ( first paragraph preliminaries ) - What does it mean `` To capture multi-modality in a dataset '' ? ( second paragraph preliminaries ) - Figure 1 : What does the ( 81.1 % ) and ( 70.3 % ) refer to ? - Table 4 : why is SAC given without standard deviation ? Some text improvements : - `` we focus on focus on '' Other comment : - The number of seeds ( 4 ) should ideally be increased .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their constructive feedback . We have updated the motivation -- both in the paper ( Section 1 ) as well as in a shared response to the reviewers , here , on openreview . We will address the reviewer \u2019 s concern as follows : * * \u201c The objective of the work is to find an effective way to leverage this data for finding temporally extended primitive behaviors. \u201d * * * * RESPONSE : * * We would say that the motivation is to use effective ( and simple ) ways to accelerate offline RL rather than finding an effective way to leverage offline data for finding temporally extended primitive behaviors . We have now made this very explicit in Section 1 of the paper . One such effective way is to first extract temporally extended primitives from offline data and then leverage those primitives to accelerate offline RL algorithms . While this might seem intuitive at first , it is actually not so intuitive , given the fact that prior works that performed controlled empirical analysis on the benefits of hierarchical RL ( Nachum et al , 2019 ) find that hierarchies work better than standard online RL because of improved exploration and there is absolutely no exploration to be done in an offline RL setting . To the best of our knowledge , ours is the first work to theoretically justify and experimentally verify the benefits of primitive learning in offline settings , showing that hierarchies can provide temporal abstraction that allows us to reduce the effect of compounding errors issue in offline RL . This is the reason behind our motivation towards demonstrating the efficacy of learning primitives in an offline RL perspective . Please see the shared response as well for clarifications on motivations of this project . * * \u201c if the downstream policy has to stay close to the offline data distribution , it seems to me that if the offline data distribution is obtained with a bad policy , this can not lead to interesting decision-making \u201d * * * * RESPONSE : * * We emphasize that our use of OPAL restricts the learned policy to \u201c stay close \u201d to the offline data distribution in the sense of restricting the learned policy to be within the support of the offline data . This is a much weaker notion than many existing offline RL algorithms that use explicit KL or other divergence regularizers to limit the deviation of the learned policy from the data-generating policy . To see why this is the case , note that by decomposing the policy into primitives , and by performing RL over the latent variables $ z $ , we are allowed to select actions that are within the support of the behavior policy but are not the mean of the behavior policy distribution , which is what a KL-divergence style constraint typically used in offline RL would do . While the support-based notion we use still restricts the expressivity of the learned policy ( as specifically analyzed by our theoretical statements ) , this is arguably unavoidable ; for example , see a concurrent submission deriving hardness results of learning policies based on offline data that is far from the optimal policy https : //openreview.net/forum ? id=30EvkP2aQLD * * \u201c can you explain why equation 2 `` motivates better generalization '' ? \u201d * * * * RESPONSE : * * Without equation 2 , the autoencoding may encourage the encoder to assign a unique z to every sub trajectory . As argued in beta-VAE paper ( Higgins et al 2017 ) , the constraint in equation 2 allows us to control the informational capacity of the latent information bottleneck , which helps with generalization Equation 2 regularizes the latent space of z and forces the distribution of z to depend on the initial state of the sub-trajectory rather than the entire sub-trajectory . * * \u201c Why can gamma not have a value of 0 ( first paragraph preliminaries ) \u201d * * * * RESPONSE : * * We are interested in long-horizon sequential decision making problems that are typically the focus of reinforcement learning algorithms . In this setting if $ \\gamma $ is set to 0 , the episode length reduces to a single time-step and the decision making problem effectively loses its sequential structure . In this work , we are interested in tasks with long horizons where we can leverage temporal abstraction . Therefore , we assume $ \\gamma $ > 0 , and we have clarified this detail in the paper . * * \u201c What does it mean `` To capture multi-modality in a dataset '' ? ( second paragraph preliminaries ) \u201d * * * * RESPONSE : * * In this work , we are focusing on multi-modal datasets ( i.e.the distribution of actions $ a $ at a given observation $ s $ in the dataset can be multimodal -- in simpler words , from the same state s , the agent can perform 2 or more different behaviors ) . To model the process of dataset $ \\mathcal { D } $ generation , we assume that an initial state is sampled first and then a policy is sampled from a distribution of policies which is then executed for $ c $ timesteps -- this data generation procedure formalizes the notion of multi-modality ."}, {"review_id": "V69LGwJ0lIN-1", "review_text": "Summary - To best leverage diverse datasets of logged experience , the authors propose to extract a space of primitive behaviors in a continuous space , and to use these for downstream learning . The primitive behaviors are learned through a VAE loss , and CQL is applied to learn a policy over the primitives . The authors claim that this approach to offline RL avoids known distribution shift and allows for temporal abstraction . The method is also applied to few-shot imitation learning , exploration and transfer to online RL . Decision -- Despite this paper being an interesting read , I feel that my concerns about the experiments lead me to not be confident in the proposed approach . As such , my preliminary rating for the paper is `` Okay but not good enough '' . The baselines chosen for the experiments do not seem representative of the problem being addressed . This also leads into the motivation , where OPAL is motivated from the offline RL perspective but does not explicitly mitigate the issues in offline RL . Only the theoretical results investigate the effect of a fixed replay-buffer , but even these claims are framed in terms of what $ \\mathcal { D } $ should be to ensure downstream RL . Originality -- The proposed approach seems limited in its novelty , combining approaches from skill-discovery , hierarchical RL and somewhat from offline RL . The theory section is interesting , however the technical arguments seem similar to that of Nachum et al . ( 2018 ) .Quality and Clarity - The overall quality of the paper is very high . The writing is clear , and the theoretical arguments are put into an easily understandable context . Strengths - OPAL leverages techniques from many areas of machine learning : unsupervised learning , hierarchical RL and offline RL . This is an interesting combination , and deserves to be investigated . It seems like the workhorse of OPAL is the unsupervised learning component however , and perhaps this should be emphasized and investigated independently . - The theoretical section is very clear and well contextualized . The claims in the paper seem correct , and they do provide much needed validation for hierarchical RL . The assumption of an optimal high-level controller seems strong , but the analysis is nonetheless interesting . Weaknesses - - Despite Figure 2 , OPAL is difficult to disentangle with many moving parts . It does n't help that there is no discussion of how hyperparameters were chosen for each component , or an ablation study to investigate different hierarchical RL approaches , fine-tuning , offline RL algorithms , or VAE losses . - The proposed method is not specifically designed to leverage anything particular in offline RL . As you show , it can be applied to online RL . However , why is this motivated from the offline perspective ? OPAL itself does not seem to mitigate distributional shift . - It would be helpful to directly compare to other skill-discovery methods that have been applied to online RL but can be adapted to offline RL . For example , what prevents the work by Campos et al . ( 2020 ) and the baselines therein to be applied in conjunction with an offline RL algorithm ? The baselines for the experiments do not seem representative of the problem you are addressing . You use standard offline RL algorithms , yet claim that the temporal abstraction of skill discovery is crucial to the results . As such , you should compare OPAL to other skill-discovery algorithms that can be combined with offline RL . Detailed Comments -- - Section 4.1 : `` Prior : \u03c1\u03c9 ( z|s0 ) tries to predict the encoded distribution of the sub-trajectory .. '' Prior does n't seem like the right word here , since it is being learned . - Section 4.2 : In what way is the task-specific policy $ \\pi_\\psi $ different from the learned prior in Section 4.1 ? Both are distributions over latent $ z $ conditioned on a state . While the prior is designed to only be conditioned on the initial state , this is still quite similar to the high-level policy $ \\pi_\\psi $ . - Section 4.2 : `` \\ [ to\\ ] ensure that the c-step transitions remain consistent \u2026 with the labelled latent action $ z_i $ '' Why is this necessary , should CQL ensure consistency at the action-level , while the latent actions are consistent by design ? - Corollary 4.1.1 : How would $ \\epsilon $ actually approach $ \\mathcal { H } _c $ in the algorithm ? The condition is never explicitly enforced because you formulate it as a constrained optimization problem and you are never able to change $ \\mathcal { H } _c $ since it is a constant . - Section 5.1 , baselines : should n't baselines be compared to different unsupervised skill discovery algorithms , paired with offline RL algorithms ? BEAR and EMAQ are offline RL algorithms without any temporal abstraction . And as you say in the results section with an ablation study , this temporal abstraction is crucial . - Section 5.1 , results : An ablation study is discussed but I can not find the corresponding results table/figure in the paper . - Section 5.2 , Table 3 : With so many 0.0 's , this leads me to believe that the baselines are quite weak , or the problem is too hard for the baselines . How were the baselines chosen . For example , why was DDCO paired with DDQN instead of SAC ? Minor Comments -- - Section 5.2 mis-capitalized We `` in this setting We use the Antmaze environ- '' - Section 5.3 , Table 4 : why are there no standard errors for SAC ? Post Rebuttal -- After discussion with the authors , I have decided to increase my score to a 6 . The authors have addressed many of my concerns with respect to motivation , theoretical analysis and empirical evidence . As it stands , I still think OPAL is hampered by the many `` moving parts '' involved . The theoretical analysis and empirical evidence suggests a very effective approach however , and should lead to further work combining variational sequence encoding techniques for primitive extraction in RL . My decision would be to `` accept the paper if there is room '' . References -- Campos , V\u00edctor , Alexander Trott , Caiming Xiong , Richard Socher , Xavier Giro-i-Nieto , and Jordi Torres . 2020. \u201c Explore , Discover and Learn : Unsupervised Discovery of State-Covering Skills. \u201d * arXiv:2002.03647 * . < http : //arxiv.org/abs/2002.03647v4 > . Nachum , Ofir , Shixiang Gu , Honglak Lee , and Sergey Levine . 2018. \u201c Near-Optimal Representation Learning for Hierarchical Reinforcement Learning. \u201d * arXiv:1810.01257 * . < http : //arxiv.org/abs/1810.01257v2 > .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201c OPAL itself does not seem to mitigate distributional shift \u201d * * * * RESPONSE : * * Intuitively OPAL partly mitigates distributional shift , by making the task policy sample primitives , which are restricted to be close to offline data distribution . Formally , OPAL significantly slows down the amount of error accumulation ( which is caused due to distributional shift and sampling error ) in offline RL . As is evident from the bound for offline policy performance in Theorem 4.2 , the curse of distributional shift is significantly reduced when OPAL is used : our bound depends on the size of the latent space and error is compounded by a smaller factor over the horizon ( due to temporal abstraction ) as compared to the standard RL counterpart of this bound which is affected by the size of the entire action space for each and every timestep in the horizon . This indicates that OPAL does help prevent distributional shift and sampling error . \u201c Section 4.2 : In what way is the task-specific policy $ \\pi_\\psi $ different from the learned prior in Section 4.1 ? Both are distributions over latent conditioned on a state . While the prior is designed to only be conditioned on the initial state , this is still quite similar to the high-level policy $ \\pi_\\psi $ . \u201d * * RESPONSE : * * The learned prior is learned using an autoencoding loss without knowledge of task ( i.e reward ) and tries to capture all the relevant skills from a state . On the other hand , the task specific policy is finetuned on the task in the RL setting , and so it focuses on skills relevant to maximizing the reward function * * \u201c The assumption of an optimal high-level controller seems strong , but the analysis is nonetheless interesting. \u201d * * * * RESPONSE : * * Our analysis in Theorem 4.2 does not assume access to an optimal high-level controller . More specifically , Theorem 4.1 shows the existence of a nearly optimal high-level controller and builds the relationship between how the ( sub ) optimality of the high level controller depends on final autoencoding loss , dimension of the primitives and length of the temporally extended primitives ( $ c $ ) . Once we have shown the existence of a near optimal high level controller , we show how to convert this into an offline RL result that learns the optimal controller in Theorem 4.2 . Our conclusion is that using a high-level controller makes the offline RL problem easier due to reduced task horizon and how it asymptotically improves the performance of offline RL . * * \u201c however the technical arguments seem similar to that of Nachum et al . ( 2018 ) \u201d * * * * RESPONSE : * * We respectfully disagree with the statement . While Theorem 4.1 tries to argue the existence of a near optimal high level controller , the underlying assumption it makes and the subsequent proof techniques is very different from that of Nachum et al . ( 2018 ) .Namely , Nachum et al . ( 2018 ) relies on the use of supremums of states in the MDP to provide a performance bound , and this limits its connection to practical algorithms . On the other hand , we prove the existence of a near optimal high level controller when underlying primitives are learned from a * stochastically sampled * offline dataset . Crucially , our derivations illuminate what properties the dataset should possess to ensure low suboptimality , and this is a key contribution of our work . Moreover , Theorem 4.2 is completely independent from Nachum et al ( 2018 ) and shows how temporally extended primitives asymptotically improve the performance of an underlying conservative offline RL method . * * \u201c Corollary 4.1.1 : How would $ \\epsilon $ approach $ -\\mathcal { H } _c $ in the algorithm ? The condition is never explicitly enforced because you formulate it as a constrained optimization problem and you are never able to change since it \u2019 s a constant. \u201d * * * * RESPONSE : * * While $ \\epsilon_c $ would decrease as a result of optimization , there \u2019 s no guarantee it would approach $ -\\mathcal { H } _c $ . We have updated the statement in the paper . * * \u201c Section 5.3 , Table 4 : why are there no standard errors for SAC ? \u201d * * * * RESPONSE : * * Those numbers are taken directly from previous work ( Yu et al ( 2019 ) ) , and they don \u2019 t include the standard deviation . But , we will run these results and update the paper in the final version as suggested by the reviewer . * * \u201c Section 4.1 : `` Prior : \u03c1\u03c9 ( z|s0 ) tries to predict the encoded distribution of the sub-trajectory .. '' Prior does n't seem like the right word here , since it is being learned. \u201d * * * * RESPONSE : * * We borrowed this terminology from Kingma et al , 2013 which stated that the prior can be either fixed or learned . However , to make the paper less confusing , we have updated the paper to include the term \u2018 Primitive Predictor \u2019 ( as it predicts z from initial state ) as well ."}, {"review_id": "V69LGwJ0lIN-2", "review_text": "# # OPAL : Offline Primitive Discovery for Accelerating Offline Reinforcement Learning # # # Summary The authors present OPAL , an offline reinforcement learning approach that distills useful common behaviors from offline transition data . They leverage a variational GRU to encode trajectories into a latent space of primitive policies . These learned primitives can later on be transferred to other tasks by learning a high-level controller over the extracted primitive latent space . Their experimental results show that OPAL compares favorably against other SOTA methods in offline , online and few-shot reinforcement learning settings . Overall , the paper is clearly written , and the approach is potentially practical for real-world applications , due to its good performance on both offline and few-shot adaptation settings . # # # Strength - The proposed method can leverage large unlabeled data without pre-defined reward functions . - The property of OPAL is mathematically analyzed . - OPAL can potentially benefit from the advancement of offline RL algorithms by simply replacing CQL . - Can be adapted to a new task through few-shot imitation learning . # # # Weakness - Training details are missing for the OPAL encoder . - Not exactly a weakness , but it would be nice to see if the method works in real-world settings ( real-world on/offline data ) .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their constructive feedback . We will address the reviewer \u2019 s concern as follows : * * \u201c Training details are missing for the OPAL encoder \u201d * * * * RESPONSE : * * The encoder is trained through equation ( 1 ) and ( 2 ) in the paper . As mentioned in the paper in Section 4.1 , we combine ( 1 ) and ( 2 ) and convert them into a $ \\\\beta $ VAE formulation . Therefore , the overall objective function becomes $ < \\\\hat { E } _ { \\\\tau \\\\sim D , z \\\\sim q_ { \\\\phi } ( z|\\\\tau ) } [ -\\\\sum_ { t=0 } ^ { c-1 } \\\\log \\\\pi_ { \\\\theta } ( a_t|s_t , z ) ] + \\\\beta \\\\hat { E } _ { \\\\tau \\\\sim D } [ D_ { KL } ( q_ { \\\\phi } ( z|\\\\tau ) ||\\\\rho_ { \\\\omega } ( z|s_0 ) ) ] > $ The encoder $ q_\\\\phi $ is trained using the above objective function through reparameterization and backpropagation . The training details for OPAL ( which includes all the architectural details and hyperparameter choices ) is provided in Appendix C. * * \u201c Not exactly a weakness , but it would be nice to see if the method works in real-world settings ( real-world on/offline data ) \u201d * * * * RESPONSE : * * We planned to do some simple real world maze navigation experiments using D \u2019 kitty robot from the ROBEL benchmark ( Ahn et al , 2019 ) . However , access to the lab has been limited due to COVID . However , we aim to add this experiment on a real-world task for the final version of this paper . * * REFERENCES : * * 1 ) ROBEL : RObotics BEnchmarks for Learning -- Low-Cost , Robust and Reproducible . Michael Ahn , Henry Zhu , Kristian Hartikainen , Hugo Ponte , Abhishek Gupta , Sergey Levine , VIKASH KUMAR"}], "0": {"review_id": "V69LGwJ0lIN-0", "review_text": "In the RL setting , this paper tackles the case where an agent may have access to large amounts of offline experience data . The objective of the work is to find an effective way to leverage this data for finding temporally extended primitive behaviors . The paper provides results that show how performing offline primitive learning can be leveraged for improving few-shot imitation learning as well as exploration and transfer on a variety of benchmark domains . The paper tackles an important question in reinforcement learning : learning temporally extended primitive behaviors from off-policy data is a very relevant question . However , I found the motivation for the approach quite vague as well as different elements that require clarification ( see below ) and because of this , I ca n't recommend acceptance . Motivation for the approach : - if the downstream policy has to stay close to the offline data distribution , it seems to me that if the offline data distribution is obtained with a bad policy , this can not lead to interesting decision-making . - can you explain why equation 2 `` motivates better generalization '' ? Other concerns : - Why can gamma not have a value of 0 ( first paragraph preliminaries ) - What does it mean `` To capture multi-modality in a dataset '' ? ( second paragraph preliminaries ) - Figure 1 : What does the ( 81.1 % ) and ( 70.3 % ) refer to ? - Table 4 : why is SAC given without standard deviation ? Some text improvements : - `` we focus on focus on '' Other comment : - The number of seeds ( 4 ) should ideally be increased .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their constructive feedback . We have updated the motivation -- both in the paper ( Section 1 ) as well as in a shared response to the reviewers , here , on openreview . We will address the reviewer \u2019 s concern as follows : * * \u201c The objective of the work is to find an effective way to leverage this data for finding temporally extended primitive behaviors. \u201d * * * * RESPONSE : * * We would say that the motivation is to use effective ( and simple ) ways to accelerate offline RL rather than finding an effective way to leverage offline data for finding temporally extended primitive behaviors . We have now made this very explicit in Section 1 of the paper . One such effective way is to first extract temporally extended primitives from offline data and then leverage those primitives to accelerate offline RL algorithms . While this might seem intuitive at first , it is actually not so intuitive , given the fact that prior works that performed controlled empirical analysis on the benefits of hierarchical RL ( Nachum et al , 2019 ) find that hierarchies work better than standard online RL because of improved exploration and there is absolutely no exploration to be done in an offline RL setting . To the best of our knowledge , ours is the first work to theoretically justify and experimentally verify the benefits of primitive learning in offline settings , showing that hierarchies can provide temporal abstraction that allows us to reduce the effect of compounding errors issue in offline RL . This is the reason behind our motivation towards demonstrating the efficacy of learning primitives in an offline RL perspective . Please see the shared response as well for clarifications on motivations of this project . * * \u201c if the downstream policy has to stay close to the offline data distribution , it seems to me that if the offline data distribution is obtained with a bad policy , this can not lead to interesting decision-making \u201d * * * * RESPONSE : * * We emphasize that our use of OPAL restricts the learned policy to \u201c stay close \u201d to the offline data distribution in the sense of restricting the learned policy to be within the support of the offline data . This is a much weaker notion than many existing offline RL algorithms that use explicit KL or other divergence regularizers to limit the deviation of the learned policy from the data-generating policy . To see why this is the case , note that by decomposing the policy into primitives , and by performing RL over the latent variables $ z $ , we are allowed to select actions that are within the support of the behavior policy but are not the mean of the behavior policy distribution , which is what a KL-divergence style constraint typically used in offline RL would do . While the support-based notion we use still restricts the expressivity of the learned policy ( as specifically analyzed by our theoretical statements ) , this is arguably unavoidable ; for example , see a concurrent submission deriving hardness results of learning policies based on offline data that is far from the optimal policy https : //openreview.net/forum ? id=30EvkP2aQLD * * \u201c can you explain why equation 2 `` motivates better generalization '' ? \u201d * * * * RESPONSE : * * Without equation 2 , the autoencoding may encourage the encoder to assign a unique z to every sub trajectory . As argued in beta-VAE paper ( Higgins et al 2017 ) , the constraint in equation 2 allows us to control the informational capacity of the latent information bottleneck , which helps with generalization Equation 2 regularizes the latent space of z and forces the distribution of z to depend on the initial state of the sub-trajectory rather than the entire sub-trajectory . * * \u201c Why can gamma not have a value of 0 ( first paragraph preliminaries ) \u201d * * * * RESPONSE : * * We are interested in long-horizon sequential decision making problems that are typically the focus of reinforcement learning algorithms . In this setting if $ \\gamma $ is set to 0 , the episode length reduces to a single time-step and the decision making problem effectively loses its sequential structure . In this work , we are interested in tasks with long horizons where we can leverage temporal abstraction . Therefore , we assume $ \\gamma $ > 0 , and we have clarified this detail in the paper . * * \u201c What does it mean `` To capture multi-modality in a dataset '' ? ( second paragraph preliminaries ) \u201d * * * * RESPONSE : * * In this work , we are focusing on multi-modal datasets ( i.e.the distribution of actions $ a $ at a given observation $ s $ in the dataset can be multimodal -- in simpler words , from the same state s , the agent can perform 2 or more different behaviors ) . To model the process of dataset $ \\mathcal { D } $ generation , we assume that an initial state is sampled first and then a policy is sampled from a distribution of policies which is then executed for $ c $ timesteps -- this data generation procedure formalizes the notion of multi-modality ."}, "1": {"review_id": "V69LGwJ0lIN-1", "review_text": "Summary - To best leverage diverse datasets of logged experience , the authors propose to extract a space of primitive behaviors in a continuous space , and to use these for downstream learning . The primitive behaviors are learned through a VAE loss , and CQL is applied to learn a policy over the primitives . The authors claim that this approach to offline RL avoids known distribution shift and allows for temporal abstraction . The method is also applied to few-shot imitation learning , exploration and transfer to online RL . Decision -- Despite this paper being an interesting read , I feel that my concerns about the experiments lead me to not be confident in the proposed approach . As such , my preliminary rating for the paper is `` Okay but not good enough '' . The baselines chosen for the experiments do not seem representative of the problem being addressed . This also leads into the motivation , where OPAL is motivated from the offline RL perspective but does not explicitly mitigate the issues in offline RL . Only the theoretical results investigate the effect of a fixed replay-buffer , but even these claims are framed in terms of what $ \\mathcal { D } $ should be to ensure downstream RL . Originality -- The proposed approach seems limited in its novelty , combining approaches from skill-discovery , hierarchical RL and somewhat from offline RL . The theory section is interesting , however the technical arguments seem similar to that of Nachum et al . ( 2018 ) .Quality and Clarity - The overall quality of the paper is very high . The writing is clear , and the theoretical arguments are put into an easily understandable context . Strengths - OPAL leverages techniques from many areas of machine learning : unsupervised learning , hierarchical RL and offline RL . This is an interesting combination , and deserves to be investigated . It seems like the workhorse of OPAL is the unsupervised learning component however , and perhaps this should be emphasized and investigated independently . - The theoretical section is very clear and well contextualized . The claims in the paper seem correct , and they do provide much needed validation for hierarchical RL . The assumption of an optimal high-level controller seems strong , but the analysis is nonetheless interesting . Weaknesses - - Despite Figure 2 , OPAL is difficult to disentangle with many moving parts . It does n't help that there is no discussion of how hyperparameters were chosen for each component , or an ablation study to investigate different hierarchical RL approaches , fine-tuning , offline RL algorithms , or VAE losses . - The proposed method is not specifically designed to leverage anything particular in offline RL . As you show , it can be applied to online RL . However , why is this motivated from the offline perspective ? OPAL itself does not seem to mitigate distributional shift . - It would be helpful to directly compare to other skill-discovery methods that have been applied to online RL but can be adapted to offline RL . For example , what prevents the work by Campos et al . ( 2020 ) and the baselines therein to be applied in conjunction with an offline RL algorithm ? The baselines for the experiments do not seem representative of the problem you are addressing . You use standard offline RL algorithms , yet claim that the temporal abstraction of skill discovery is crucial to the results . As such , you should compare OPAL to other skill-discovery algorithms that can be combined with offline RL . Detailed Comments -- - Section 4.1 : `` Prior : \u03c1\u03c9 ( z|s0 ) tries to predict the encoded distribution of the sub-trajectory .. '' Prior does n't seem like the right word here , since it is being learned . - Section 4.2 : In what way is the task-specific policy $ \\pi_\\psi $ different from the learned prior in Section 4.1 ? Both are distributions over latent $ z $ conditioned on a state . While the prior is designed to only be conditioned on the initial state , this is still quite similar to the high-level policy $ \\pi_\\psi $ . - Section 4.2 : `` \\ [ to\\ ] ensure that the c-step transitions remain consistent \u2026 with the labelled latent action $ z_i $ '' Why is this necessary , should CQL ensure consistency at the action-level , while the latent actions are consistent by design ? - Corollary 4.1.1 : How would $ \\epsilon $ actually approach $ \\mathcal { H } _c $ in the algorithm ? The condition is never explicitly enforced because you formulate it as a constrained optimization problem and you are never able to change $ \\mathcal { H } _c $ since it is a constant . - Section 5.1 , baselines : should n't baselines be compared to different unsupervised skill discovery algorithms , paired with offline RL algorithms ? BEAR and EMAQ are offline RL algorithms without any temporal abstraction . And as you say in the results section with an ablation study , this temporal abstraction is crucial . - Section 5.1 , results : An ablation study is discussed but I can not find the corresponding results table/figure in the paper . - Section 5.2 , Table 3 : With so many 0.0 's , this leads me to believe that the baselines are quite weak , or the problem is too hard for the baselines . How were the baselines chosen . For example , why was DDCO paired with DDQN instead of SAC ? Minor Comments -- - Section 5.2 mis-capitalized We `` in this setting We use the Antmaze environ- '' - Section 5.3 , Table 4 : why are there no standard errors for SAC ? Post Rebuttal -- After discussion with the authors , I have decided to increase my score to a 6 . The authors have addressed many of my concerns with respect to motivation , theoretical analysis and empirical evidence . As it stands , I still think OPAL is hampered by the many `` moving parts '' involved . The theoretical analysis and empirical evidence suggests a very effective approach however , and should lead to further work combining variational sequence encoding techniques for primitive extraction in RL . My decision would be to `` accept the paper if there is room '' . References -- Campos , V\u00edctor , Alexander Trott , Caiming Xiong , Richard Socher , Xavier Giro-i-Nieto , and Jordi Torres . 2020. \u201c Explore , Discover and Learn : Unsupervised Discovery of State-Covering Skills. \u201d * arXiv:2002.03647 * . < http : //arxiv.org/abs/2002.03647v4 > . Nachum , Ofir , Shixiang Gu , Honglak Lee , and Sergey Levine . 2018. \u201c Near-Optimal Representation Learning for Hierarchical Reinforcement Learning. \u201d * arXiv:1810.01257 * . < http : //arxiv.org/abs/1810.01257v2 > .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201c OPAL itself does not seem to mitigate distributional shift \u201d * * * * RESPONSE : * * Intuitively OPAL partly mitigates distributional shift , by making the task policy sample primitives , which are restricted to be close to offline data distribution . Formally , OPAL significantly slows down the amount of error accumulation ( which is caused due to distributional shift and sampling error ) in offline RL . As is evident from the bound for offline policy performance in Theorem 4.2 , the curse of distributional shift is significantly reduced when OPAL is used : our bound depends on the size of the latent space and error is compounded by a smaller factor over the horizon ( due to temporal abstraction ) as compared to the standard RL counterpart of this bound which is affected by the size of the entire action space for each and every timestep in the horizon . This indicates that OPAL does help prevent distributional shift and sampling error . \u201c Section 4.2 : In what way is the task-specific policy $ \\pi_\\psi $ different from the learned prior in Section 4.1 ? Both are distributions over latent conditioned on a state . While the prior is designed to only be conditioned on the initial state , this is still quite similar to the high-level policy $ \\pi_\\psi $ . \u201d * * RESPONSE : * * The learned prior is learned using an autoencoding loss without knowledge of task ( i.e reward ) and tries to capture all the relevant skills from a state . On the other hand , the task specific policy is finetuned on the task in the RL setting , and so it focuses on skills relevant to maximizing the reward function * * \u201c The assumption of an optimal high-level controller seems strong , but the analysis is nonetheless interesting. \u201d * * * * RESPONSE : * * Our analysis in Theorem 4.2 does not assume access to an optimal high-level controller . More specifically , Theorem 4.1 shows the existence of a nearly optimal high-level controller and builds the relationship between how the ( sub ) optimality of the high level controller depends on final autoencoding loss , dimension of the primitives and length of the temporally extended primitives ( $ c $ ) . Once we have shown the existence of a near optimal high level controller , we show how to convert this into an offline RL result that learns the optimal controller in Theorem 4.2 . Our conclusion is that using a high-level controller makes the offline RL problem easier due to reduced task horizon and how it asymptotically improves the performance of offline RL . * * \u201c however the technical arguments seem similar to that of Nachum et al . ( 2018 ) \u201d * * * * RESPONSE : * * We respectfully disagree with the statement . While Theorem 4.1 tries to argue the existence of a near optimal high level controller , the underlying assumption it makes and the subsequent proof techniques is very different from that of Nachum et al . ( 2018 ) .Namely , Nachum et al . ( 2018 ) relies on the use of supremums of states in the MDP to provide a performance bound , and this limits its connection to practical algorithms . On the other hand , we prove the existence of a near optimal high level controller when underlying primitives are learned from a * stochastically sampled * offline dataset . Crucially , our derivations illuminate what properties the dataset should possess to ensure low suboptimality , and this is a key contribution of our work . Moreover , Theorem 4.2 is completely independent from Nachum et al ( 2018 ) and shows how temporally extended primitives asymptotically improve the performance of an underlying conservative offline RL method . * * \u201c Corollary 4.1.1 : How would $ \\epsilon $ approach $ -\\mathcal { H } _c $ in the algorithm ? The condition is never explicitly enforced because you formulate it as a constrained optimization problem and you are never able to change since it \u2019 s a constant. \u201d * * * * RESPONSE : * * While $ \\epsilon_c $ would decrease as a result of optimization , there \u2019 s no guarantee it would approach $ -\\mathcal { H } _c $ . We have updated the statement in the paper . * * \u201c Section 5.3 , Table 4 : why are there no standard errors for SAC ? \u201d * * * * RESPONSE : * * Those numbers are taken directly from previous work ( Yu et al ( 2019 ) ) , and they don \u2019 t include the standard deviation . But , we will run these results and update the paper in the final version as suggested by the reviewer . * * \u201c Section 4.1 : `` Prior : \u03c1\u03c9 ( z|s0 ) tries to predict the encoded distribution of the sub-trajectory .. '' Prior does n't seem like the right word here , since it is being learned. \u201d * * * * RESPONSE : * * We borrowed this terminology from Kingma et al , 2013 which stated that the prior can be either fixed or learned . However , to make the paper less confusing , we have updated the paper to include the term \u2018 Primitive Predictor \u2019 ( as it predicts z from initial state ) as well ."}, "2": {"review_id": "V69LGwJ0lIN-2", "review_text": "# # OPAL : Offline Primitive Discovery for Accelerating Offline Reinforcement Learning # # # Summary The authors present OPAL , an offline reinforcement learning approach that distills useful common behaviors from offline transition data . They leverage a variational GRU to encode trajectories into a latent space of primitive policies . These learned primitives can later on be transferred to other tasks by learning a high-level controller over the extracted primitive latent space . Their experimental results show that OPAL compares favorably against other SOTA methods in offline , online and few-shot reinforcement learning settings . Overall , the paper is clearly written , and the approach is potentially practical for real-world applications , due to its good performance on both offline and few-shot adaptation settings . # # # Strength - The proposed method can leverage large unlabeled data without pre-defined reward functions . - The property of OPAL is mathematically analyzed . - OPAL can potentially benefit from the advancement of offline RL algorithms by simply replacing CQL . - Can be adapted to a new task through few-shot imitation learning . # # # Weakness - Training details are missing for the OPAL encoder . - Not exactly a weakness , but it would be nice to see if the method works in real-world settings ( real-world on/offline data ) .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their constructive feedback . We will address the reviewer \u2019 s concern as follows : * * \u201c Training details are missing for the OPAL encoder \u201d * * * * RESPONSE : * * The encoder is trained through equation ( 1 ) and ( 2 ) in the paper . As mentioned in the paper in Section 4.1 , we combine ( 1 ) and ( 2 ) and convert them into a $ \\\\beta $ VAE formulation . Therefore , the overall objective function becomes $ < \\\\hat { E } _ { \\\\tau \\\\sim D , z \\\\sim q_ { \\\\phi } ( z|\\\\tau ) } [ -\\\\sum_ { t=0 } ^ { c-1 } \\\\log \\\\pi_ { \\\\theta } ( a_t|s_t , z ) ] + \\\\beta \\\\hat { E } _ { \\\\tau \\\\sim D } [ D_ { KL } ( q_ { \\\\phi } ( z|\\\\tau ) ||\\\\rho_ { \\\\omega } ( z|s_0 ) ) ] > $ The encoder $ q_\\\\phi $ is trained using the above objective function through reparameterization and backpropagation . The training details for OPAL ( which includes all the architectural details and hyperparameter choices ) is provided in Appendix C. * * \u201c Not exactly a weakness , but it would be nice to see if the method works in real-world settings ( real-world on/offline data ) \u201d * * * * RESPONSE : * * We planned to do some simple real world maze navigation experiments using D \u2019 kitty robot from the ROBEL benchmark ( Ahn et al , 2019 ) . However , access to the lab has been limited due to COVID . However , we aim to add this experiment on a real-world task for the final version of this paper . * * REFERENCES : * * 1 ) ROBEL : RObotics BEnchmarks for Learning -- Low-Cost , Robust and Reproducible . Michael Ahn , Henry Zhu , Kristian Hartikainen , Hugo Ponte , Abhishek Gupta , Sergey Levine , VIKASH KUMAR"}}